<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A CONVERGENT INCREMENTAL GRADIENT METHOD WITH A CONSTANT STEP SIZE *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Doron</forename><surname>Blatt</surname></persName>
							<email>dblatt@drwholdings.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">DRW Trading Group</orgName>
								<address>
									<addrLine>10 South Riverside Plaza, 21st Floor</addrLine>
									<postCode>60606</postCode>
									<settlement>Chicago</settlement>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alfred</forename><forename type="middle">O</forename><surname>Hero</surname></persName>
							<email>hero@eecs.umich.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<postCode>48109</postCode>
									<settlement>Ann Arbor</settlement>
									<region>MI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>AND</roleName><forename type="first">Hillel</forename><surname>Gauchman</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Mathematics and Computer Science</orgName>
								<orgName type="institution">Eastern Illinois University</orgName>
								<address>
									<postCode>61920</postCode>
									<settlement>Charleston</settlement>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<postCode>48109</postCode>
									<settlement>Ann Arbor</settlement>
									<region>MI</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A CONVERGENT INCREMENTAL GRADIENT METHOD WITH A CONSTANT STEP SIZE *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">22D28207B289DABAEC576F2CE91C7829</idno>
					<idno type="DOI">10.1137/040615961</idno>
					<note type="submission">Received by the editors September 29, 2005; accepted for publication (in revised form) July 17, 2006;</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T04:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>incremental gradient method</term>
					<term>convergence analysis</term>
					<term>sensor networks</term>
					<term>neural networks</term>
					<term>logistic regression</term>
					<term>boosting AMS subject classifications. 90C30</term>
					<term>49M37</term>
					<term>65K05</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>An incremental aggregated gradient method for minimizing a sum of continuously differentiable functions is presented. The method requires a single gradient evaluation per iteration and uses a constant step size. For the case that the gradient is bounded and Lipschitz continuous, we show that the method visits infinitely often regions in which the gradient is small. Under certain unimodality assumptions, global convergence is established. In the quadratic case, a global linear rate of convergence is shown. The method is applied to distributed optimization problems arising in wireless sensor networks, and numerical experiments compare the new method with other incremental gradient methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction. Consider the unconstrained optimization problem minimize</head><formula xml:id="formula_0">f (x) = L l=1 f l (x), x ∈ R p , (1.1)</formula><p>where R p is the p-dimensional Euclidean space, and f l : R p → R are continuously differentiable scalar functions on R p . Our interest in this problem stems from optimization problems arising in wireless sensor networks (see, e.g., <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref>), in which f l (x) corresponds to the data collected by the lth sensor in the network. This problem also arises in neural network training, in which f l (x) corresponds to the lth training data set (see, e.g., <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b25">26]</ref>).</p><p>The iterative method proposed and analyzed in this paper for solving (1.1), which we call the incremental aggregated gradient (IAG) method, generates a sequence {x k } k≥1 as follows. Given L arbitrary initial points x 1 , x 2 , . . . , x L , an aggregated gradient, denoted by d L , is defined as L l=1 ∇f l (x l ). Possible initializations are discussed in section 3. For k ≥ L,</p><formula xml:id="formula_1">x k+1 = x k -μ 1 L d k , (1.2) d k+1 = d k -∇f (k+1) L (x k+1-L ) + ∇f (k+1) L (x k+1 ), (1.3)</formula><p>where μ is a positive constant step size chosen small enough to ensure convergence, (k) L denotes k modulo L with representative class {1, 2, . . . , L}, and the factor 1/L is explicitly included to make the approximate descent direction 1  L d k comparable in magnitude to the one used in the standard incremental gradient method to be discussed below. Thus, at every iteration a new point x k+1 is generated according to the direction of the aggregated gradient d k . Then only one of the gradient summands ∇f (k+1) L (x k+1 ) is computed to replace the previously computed ∇f (k+1) L (x k+1-L ). Note that for k ≥ L the IAG iteration (1.2)-(1.3) is equivalent to</p><formula xml:id="formula_2">x k+1 = x k -μ 1 L L-1 l=0 ∇f (k-l) L (x k-l ). (1.4)</formula><p>The IAG method is related to the large class of incremental gradient methods that has been studied extensively in the literature <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b43">44]</ref> (see also <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b31">32]</ref> and the references therein for incremental subgradient methods for nondifferentiable convex optimization). The standard incremental gradient method updates x k according to <ref type="bibr">(1.5)</ref> where μ(k) is a positive step size, possibly depending on k. Therefore, it is seen that the principal difference between the two methods is that the standard incremental gradient method uses only one of the components in order to generate an approximate descent direction, whereas the IAG method uses the average of the L previously computed gradients. This property leads to convergence of the IAG method for fixed and sufficiently small positive step size μ. This is in contrast to the standard incremental gradient method, whose convergence requires that the step size sequence μ(k) converge to zero.</p><formula xml:id="formula_3">x k+1 = x k -μ(k)∇f (k) L (x k ),</formula><p>Incremental gradient methods can be motivated by the observation that when the iterates are far from the eventual limit, the evaluation of a single gradient component is sufficient for generating an approximate descent direction. Hence, these methods lead to a significant reduction in the amount of required computations per iteration (see, e.g., <ref type="bibr">[6, sect. 1.5.2]</ref> and the discussion in <ref type="bibr" target="#b4">[5]</ref>). The drawback of these methods, when using a constant step size, is that the iterates converge to a limit cycle and oscillate around a stationary point <ref type="bibr" target="#b24">[25]</ref>, unless restrictions of the type ∇f l (x) = 0, l = 1, . . . , L, whenever ∇f (x) = 0 are imposed <ref type="bibr" target="#b43">[44]</ref>. Convergence for a diminishing step size has been established by a number of authors under different conditions <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b43">44]</ref>. However, a diminishing step size usually leads to slow convergence near the eventual limit and requires exhaustive experimentation to determine how rapidly the step size must decrease in order to prevent scenarios in which the step size becomes too small when the iterates are far from the eventual limit (e.g., determining the constants a and b in step sizes of the form μ(k) = a/(k + b)).</p><p>A hybrid between the steepest descent method and the incremental gradient method was studied in <ref type="bibr" target="#b4">[5]</ref>. The hybrid method starts as an incremental gradient method and gradually becomes the steepest descent. This method requires a tuning parameter, which controls the transition between the two methods, to gradually increase with k to ensure convergence. When the tuning parameter increases sufficiently fast with the number of iterations, it is shown that the rate of convergence is linear. However, the question of determining the rate of transition between the two methods still remains. For any fixed value of the tuning parameter, the hybrid method con-Downloaded 08/12/13 to 129.173.72.87. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php verges to a limit cycle, unless a diminishing step size is used, similar to the standard incremental gradient method.</p><p>The choice of the aggregated gradient d k (1.3) for generating an approximate descent direction was mentioned in <ref type="bibr" target="#b17">[18]</ref> in the context of adaptive step size methods, which require repeated evaluations of either the complete objective function f (x) or its gradient. This requirement renders the methods proposed in <ref type="bibr" target="#b17">[18]</ref> inapplicable to problems in sensor networks of interest to us or any other applications which require decentralized implementation, as will be explained in section 3. In addition, as noted in <ref type="bibr" target="#b45">[46]</ref>, if ∇f l (x), l = 1, . . . , L, are not necessarily zero whenever ∇f (x) = 0, the step size tends to zero, resulting in slow convergence.</p><p>The IAG method is closely related to Tseng's incremental gradient with momentum term <ref type="bibr" target="#b45">[46]</ref>, which is an incremental generalization of Polyak's heavy-ball method <ref type="bibr">[34, p. 65</ref>] (also called the steepest descent with momentum term <ref type="bibr">[7, p. 104]</ref>). Rewriting Tseng's method's update rule as</p><formula xml:id="formula_4">x k+1 = x k -μ(k) k l=0 ζ l ∇f (k-l) L (x k-l ),</formula><p>we see from <ref type="bibr">(1.4</ref>) that the IAG method is a variation of this method with a truncated sum, ζ = 1, and a constant step size. Similar to <ref type="bibr" target="#b17">[18]</ref>, the step size adaptation rule that leads to convergence in <ref type="bibr" target="#b45">[46]</ref> requires repeated evaluations of the complete objective function f (x) and its gradient. Hence, this method cannot be implemented in a distributed manner either. Furthermore, a linear convergence rate is established only under a certain growth property on the functions' gradients, which requires ∇f l (x) = 0, l = 1, . . . , L, whenever ∇f (x) = 0.</p><p>In contrast to the available methods, the IAG method has all four of the following properties: (a) it evaluates a single gradient per iteration, (b) it uses a constant step size, (c) it is convergent (Proposition 2.7), and (d) it has a global linear convergence rate for quadratic objective f (x) (Proposition 2.8).</p><p>Finally, we note that the IAG method is reminiscent of other methods in various optimization problems, such as the incremental version of the Gauss-Newton method or the extended Kalman filter <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b29">30]</ref>, the distributed EM algorithm for maximum likelihood estimation <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b32">33]</ref>, the ordered subset and incremental optimization transfer for image reconstruction <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b9">10]</ref>, and iterative methods for the convex feasibility problem <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Convergence analysis.</head><p>In this section we present convergence proofs for two different function classes: (I) restricted Lipschitz and (II) quadratic. Under a Lipschitz condition and a bounded gradient assumption on f l (x), l = 1, . . . , L (Assumptions 1 and 2), we obtain an upper bound on the limit inferior of ||∇f (x k )||, which depends linearly on the step size μ. By imposing additional restrictions on the function f (x) (Assumptions 3 and 4), we prove pointwise convergence of the method. There are many functions that satisfy Assumptions 1-4. However, one important case does not satisfy these assumptions. This is the case when f (x) and f l (x) are quadratic functions on R p . For this important case we provide a completely different convergence proof and show in addition that the convergence rate is globally linear.</p><p>For later reference, it will be useful to write <ref type="bibr">(1.4</ref>) in a form known as the "gradient Downloaded 08/12/13 to 129.173.72.87. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php method with errors" <ref type="bibr" target="#b7">[8]</ref>:</p><formula xml:id="formula_5">x k+1 = x k -μ 1 L L-1 l=0 ∇f (k-l) L (x k ) + L-1 l=0 ∇f (k-l) L (x k-l ) - L-1 l=0 ∇f (k-l) L (x k ) = x k -μ 1 L ∇f (x k ) + h k , (2.1)</formula><p>where</p><formula xml:id="formula_6">h k = L-1 l=1 ∇f (k-l) L (x k-l ) -∇f (k-l) L (x k )</formula><p>is the error term in the calculation of the gradient at x k . Also note that for all k ≥ 2L and 1 ≤ l ≤ L,</p><formula xml:id="formula_7">x k-l -x k = μ 1 L d k-1 + d k-2 + • • • + d k-l .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Case I.</head><p>Assumption 1. ∇f l (x), l = 1, . . . , L, satisfy a Lipschitz condition in R p ; i.e., there is a positive number M 1 such that for all</p><formula xml:id="formula_8">x, x ∈ R p , ||∇f l (x) -∇f l (x)|| ≤ M 1 ||x -x||, l = 1, . . . , L.</formula><p>Assumption 1 implies that ∇f (x) also satisfies a Lipschitz condition; that is, for all</p><formula xml:id="formula_9">x, x ∈ R p , ||∇f (x) -∇f (x)|| ≤ M 2 ||x -x||, where M 2 = LM 1 .</formula><p>Assumption 2. There exists a positive number M 3 such that for all </p><formula xml:id="formula_10">x ∈ R p , ||∇f l (x)|| ≤ M 3 , l = 1, . . . ,</formula><formula xml:id="formula_11">w k = cQ(w k-1 , w k-2 , . . . , w k-L+1 ) + M for k ≥ L. Since s k ≤ w k for all k, if lim k→∞ w k = M 1-c , then lim sup k→∞ s k ≤ lim sup k→∞ w k = lim k→∞ w k = M 1 -c . To show that lim k→∞ w k = M 1-c , define the sequence {v k } k≥1 by v k = s k -M 1-c for 1 ≤ k ≤ L -1 and v k = cQ(v k-1 , v k-2 , . . . , v k-L+1 )</formula><p>for k ≥ L. By this construction, </p><formula xml:id="formula_12">w L = cQ M 1 -c + v L-1 , M 1 -c + v L-2 , . . . , M 1 -c + v 1 + M = c M 1 -c + cQ(v L-1 , v L-2 , . . . , v 1 ) + M = M 1 -c + v L , Downloaded</formula><formula xml:id="formula_13">w k = M 1-c + v k for all k &gt; L. Therefore, if lim k→∞ v k = 0, then lim k→∞ w k = M 1-c . To show that lim k→∞ v k = 0, set A = max{|v 1 |, |v 2 |, . . . , |v L-1 |}. Hence, |v L | = c|Q(v L-1 , v L-2 , . . . , v 1 )| ≤ cQ(|v L-1 |, |v L-2 |, . . . , |v 1 |) ≤ cA. Similarly, |v L+1 | ≤ cA, and in general |v k | ≤ cA for all k ≥ L. Consider now v 2L . Since max{|v 2L-1 |, |v 2L-2 |, . . . , |v L+1 |} ≤ cA, we have |v 2L | = c|Q(v 2L-1 , v 2L-2 , . . . , v L+1 )| ≤ cQ(|v 2L-1 |, |v 2L-2 |, . . . , |v L+1 |) ≤ c 2 A,</formula><formula xml:id="formula_14">and in general |v k | ≤ c 2 A for all k ≥ 2L. Similarly, we obtain |v k | ≤ c n L for all k ≥ nL.</formula><p>Since 0 &lt; c &lt; 1, we have lim n→∞ c n = 0, and therefore lim k→∞ v k = 0.</p><p>Remark 1. Lemma 2.1 can also be proven using concepts from dynamical systems. The sequence w k is the output of an autoregressive linear system</p><formula xml:id="formula_15">w k = c L-1 l=1 α k w k-l + Mu(k -L),</formula><p>where u(k) is the unit step function which equals one when k ≥ 0 and zero otherwise, with initial condition w k = s k for 1 ≤ k ≤ L -1. Since the coefficients of the linear form are all positive and sum to one, and 0 &lt; c &lt; 1, it is possible to show that the system is stable (bounded input bounded output) and the steady state response is</p><formula xml:id="formula_16">M 1-c [35], i.e., lim k→∞ w k = M 1-c . Lemma 2.2. Under Assumption 1, if ||∇f (x k )|| &gt; ||h k || 1-2μM1 , and 0 &lt; 1 -2μM 1 &lt; 1, then f (x k ) &gt; f(x k+1 ). Proof. Assume that ||∇f (x k )|| &gt; ||h k || 1-2μM1 . Then ||d k || 2 = ||∇f (x k ) + h k || 2 ≤ 2||∇f (x k )|| 2 + 2||h k || 2 &lt; 2||∇f (x k )|| 2 + 2 ||h k || 2 1 -2μM 1 &lt; 4||∇f (x k )|| 2 .</formula><p>By [6, Prop. A.24], if Assumption 1 holds, then Proof.</p><formula xml:id="formula_17">f (x + y) -f (x) ≤ y ∇f (x) + 1 2 M 2 ||y|| 2 . Hence f (x k ) -f (x k+1 ) = f (x k ) -f x k -μ 1 L d k ≥ μ 1 L d k ∇f (x k ) - 1 2 M 2 μ 2 1 L 2 ||d k || 2 &gt; μ 1 L (∇f (x k ) + h k ) ∇f (x k ) - 1 2 M 2 μ 2 1 L 2 4||∇f (x k )|| 2 = μ 1 L ||∇f (x k )|| 2 + μ 1 L h k ∇f (x k ) -2M 2 μ 2 1 L 2 ||∇f (x k )|| 2 ≥ μ 1 L ||∇f (x k )|| 2 -μ 1 L ||h k || • ||∇f (x k )|| -2M 2 μ 2 1 L 2 ||∇f (x k )|| 2 = μ L ||∇f (x k )||(1 -2μM 1 ) ||∇f (x k )|| - ||h k || 1 -2μM 1 &gt; 0.</formula><formula xml:id="formula_18">||h k || ≤ L-1 l=1 ||∇f (k-l) L (x k-l ) -∇f (k-l) L (x k )|| ≤ M 1 L-1 l=1 ||x k-l -x k || = μM 1 1 L L-1 l=1 ||d k-1 + d k-2 + • • • + d k-l || ≤ μM 1 1 L L-1 l=1 ||d k-1 || + ||d k-2 || + • • • + ||d k-l || = μM 1 1 L (L -1)||d k-1 || + (L -2)||d k-2 || + • • • + ||d k-L+1 || = μM 1 1 L L(L -1) 2 (L -1)||d k-1 || + (L -2)||d k-2 || + • • • + ||d k-L+1 || L(L -1)/2 = μM 1 L -1 2 Q(||d k-1 ||, ||d k-2 ||, . . . , ||d k-L+1 ||),</formula><p>where</p><formula xml:id="formula_19">Q(||d k-1 ||, ||d k-2 ||, . . . , ||d k-L+1 ||) is a linear form in the variables ||d k-1 ||, ||d k-2 ||, . . . , ||d k-L+1 || whose coefficients, L-1 L(L-1)/2 , L-2 L(L-1)/2 , . . . , 1 L(L-1)/2 , sum to one. Next, we use ||d k || = ||∇f (x k ) + h k || ≤ ||∇f (x k )|| + ||h k || to obtain ||h k || ≤ μM 1 L -1 2 Q(||h k-1 ||, ||h k-2 ||, . . . , ||h k-L+1 ||) + μM 1 L -1 2 Q(||∇f (x k-1 )||, ||∇f (x k-2 )||, . . . , ||∇f (x k-L+1 )||) ≤ μM 1 L -1 2 Q(||h k-1 ||, ||h k-2 ||, . . . , ||h k-L+1 ||) + μM 1 L -1 2 M 3 &lt; μ M 2 2 Q(||h k-1 ||, ||h k-2 ||, . . . , ||h k-L+1 ||) + μ M 2 2 M 3 ,</formula><p>where Assumption 2 was used in the second to last inequality. Hence, by Lemma 2.1,</p><formula xml:id="formula_20">since 0 &lt; μ M2 2 &lt; 1/2, lim sup k→∞ ||h k || ≤ μ M 2 2 M3 1-μ M 2 2</formula><p>. By using μ M2 2 &lt; 1/2, we obtain lim sup k→∞ ||h k || &lt; μM 2 M 3 and the lemma follows.</p><p>Proposition 2.4. Under Assumptions</p><formula xml:id="formula_21">1 and 2, if f (x) is bounded from below and μ max{2M 1 , M 2 } &lt; 1, then lim inf k→∞ ||∇f (x k )|| ≤ 2M 2 M 3 1 -2μM 1 μ.</formula><p>Proof. The proof is similar to the proof of Theorem 2.1 in <ref type="bibr" target="#b43">[44]</ref>.</p><p>Next, by imposing two additional assumptions, we prove that the IAG method converges with a constant step size to the minimum point of f (x). Downloaded 08/12/13 to 129.173.72.87. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php Assumption 3. f (x) has a unique global minimum at x * . The Hessian ∇ 2 f (x) is continuous and positive definite at x * . Assumption 4. For any sequence</p><formula xml:id="formula_22">{t k } ∞ k=1 in R p , if lim k→∞ f (t k ) = f (x * ) or lim k→∞ ||∇f (t k )|| = 0, then lim k→∞ t k = x * .</formula><p>There is an equivalent form of Assumption 4: For each neighborhood U of x * there exists</p><formula xml:id="formula_23">η &gt; 0 such that if f (x) -f (x * ) &lt; η or ||∇f (x)|| &lt; η, then x ∈ U.</formula><p>Remark 2. Assumptions 3 and 4 are stronger than the assumptions usually made on f (x) in the literature (see <ref type="bibr" target="#b7">[8]</ref> for a summary of the available convergence proofs and the assumptions they require). However, our results hold for a constant step size and do not require that ∇f l (x) = 0, l = 1, . . . , L, whenever ∇f (x) = 0. In addition, note that there are nonconvex functions that satisfy Assumption 4. However, if f (x) is strictly convex and takes a minimum in the interior of its domain (R p ), then Assumption 4 is automatically satisfied. In particular, if f (x) satisfies Assumption 3 and is strictly convex, then Assumption 4 is satisfied. In fact, the implication lim k→∞ f <ref type="bibr" target="#b40">[41]</ref>. The implication lim k→∞ ||∇f (t k )|| = 0 ⇒ lim k→∞ t k = x * can be obtained as follows: Consider the function ∇f : R p → R p . The derivative (∇f ) of this function is the Hessian ∇ 2 f . Since f (x) satisfies Assumption 3 and is strictly convex, det(∇f ) = 0. Therefore, by the inverse function theorem, there are open neighborhoods</p><formula xml:id="formula_24">(t k ) = f (x * ) ⇒ lim k→∞ t k = x * is the statement of Corollary 27.2.2 from</formula><formula xml:id="formula_25">V of x * ∈ R p and W of 0 ∈ R p such that ∇f : V → W has a continuous inverse γ : W → V . Let {t k } ∞ k=1 be a sequence such that lim k→∞ ||∇f (t k )|| = 0. Then there exists k 0 such that ∇f (t k ) ∈ W for all k ≥ k 0 . By Theorem B on page 99 in [40], since f (x) is strictly convex, ∇f is one-to-one; i.e., if x = y, then ∇f (x) = ∇f (y). It follows that t k ∈ V for all k ≥ k 0 . Now we have lim k→∞ t k = lim k→∞ γ ∇f (t k ) = γ lim k→∞ ∇f (t k ) = γ (0) = x * .</formula><p>Remark 3. Unimodal functions which are convex in the neighborhood of their minimum and have bounded gradient are common in robust estimation <ref type="bibr" target="#b19">[20]</ref>. An example of a robust estimation objective function that satisfies Assumptions 1-4 is given in section 4.1. Another important function which satisfies Assumptions 1-4 is the objective function minimized by the LogitBoost algorithm <ref type="bibr" target="#b15">[16]</ref> (or adaptive logistic regression). To explain the components which are used to construct this objective function we include a short description (taken from <ref type="bibr" target="#b13">[14]</ref>) of the supervised learning problem, and in particular, the problem of combining weak features. Let {z l , y l } L l=1 be a set of training examples, where each instance z l takes values in an instance domain Z, and each y l , called the label, takes values in {-1, +1}. Given a set of p real-valued functions on Z, h 1 , h 2 , . . . , h p called features, the goal is to find a vector x ∈ R p for which the sign of g x (z l ) = p i=1 x i h i (z l ) is a good predictor of y l for l = 1, . . . , L. Let M be the L × p matrix whose (l, i) element is h i (z l ). The objective function f (x) : R p → R minimized by the LogitBoost algorithm <ref type="bibr" target="#b13">[14]</ref> is given by</p><formula xml:id="formula_26">f (x) = L l=1 log [1 + exp (-y l [Mx] l )] ,</formula><p>(2.2) Downloaded 08/12/13 to 129.173.72.87. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php where [Mx] l is the lth element of the vector Mx. It can be motivated as being a convex surrogate to the nonconvex and nondifferentiable 0 -1 loss function</p><formula xml:id="formula_27">f (x) = L l=1 I(g x (z l )y l ≤ 0),</formula><p>which is the number of labels that are not predicted correctly by the sign of g x (z l ), or through the maximum likelihood method for estimating the conditional probability of y l given z l . It is shown below that in the nonseparable case, i.e., when there exists no value of x for which sign(g x (z l )) = y l , for l = 1, . . . , L, and when the features are linearly independent on the training set, i.e., rank M = p, the function f (x) (2.2) satisfies Assumptions 1-4:</p><formula xml:id="formula_28">∂ ∂x j log [1 + exp (-y l [Mx] l )] = exp (-y l [Mx] l ) 1 + exp (-y l [Mx] l ) (-y l h j (z l )) ≤ |h j (z l )|.</formula><p>Hence Assumption 2 holds:</p><formula xml:id="formula_29">∂ 2 ∂x j ∂x k log [1 + exp (-y l [Mx] l )] = exp (-y l [Mx] l ) [1 + exp (-y l [Mx] l )] 2 h j (z l )h k (z l ) ≤ |h j (z l )h k (z l )|.</formula><p>Hence Assumption 1 holds. Let</p><formula xml:id="formula_30">d l (x) = exp (-y l [Mx] l ) / [1 + exp (-y l [Mx] l )] 2 &gt; 0. Then ∂ 2 f (x) ∂x j ∂x k = L l=1 d l (x)M lj M lk .</formula><p>To show that ∇f (x) is positive definite for all x, consider ζ T ∇f (x)ζ for some vector ζ ∈ R p :</p><formula xml:id="formula_31">ζ T ∇f (x)ζ = p j,k=1 L l=1 d l (x)M lk M lj ζ k ζ j = L l=1 d l (x) ([Mζ] l ) 2 ≥ 0</formula><p>with equality if and only if ζ = 0, by the assumption that rank M = p. Hence the function f (x) is strictly convex. Assume the training set {z l , y l } L l=1 is nonseparable with respect to the features h 1 , h 2 , . . . , h p ; i.e., for every x there exists at least one l for which y l [Mx] l &lt; 0. For any given x = 0 let</p><formula xml:id="formula_32">I 1 (x) = {l : y l [Mx] l &lt; 0}, I 2 (x) = {l : y l [Mx] l = 0}, and I 3 (x) = {l : y l [Mx] l &gt; 0},</formula><p>and note that I 1 (x) is nonempty by assumption. For a positive scalar c, we can write f (cx) as the sum of three summations:</p><formula xml:id="formula_33">f (cx) = l∈I1(x) log 1 + exp -cy l p i=1 x i h i (z l ) + l∈I2(x) log 2 + l∈I3(x) log 1 + exp -cy l p i=1 x i h i (z l )</formula><p>. Downloaded 08/12/13 to 129.173.72.87. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p><formula xml:id="formula_34">When c → ∞, l∈I1(x) log 1 + exp -cy l p i=1 x i h i (z l ) → ∞ and l∈I3(x) log 1 + exp -cy l p i=1 x i h i (z l ) → 0.</formula><p>Therefore, lim c→∞ f (cx) = ∞ for all x = 0. This implies that f (x) has no directions of recession. A direction of recession is a nonzero vector x 1 such that f (x 2 + cx 1 ) is a nonincreasing function of the scalar c for every choice of vector x 2 . Hence by Theorem 27.1(d) in <ref type="bibr">[41, p. 265</ref>] the minimum set of f (x) is nonempty. The minimum is unique by the strict convexity of f (x). Therefore, Assumption 3 is also satisfied, and the strict convexity, together with Assumption 3, implies Assumption 4 as well.</p><p>The following lemma is well known. Lemma 2.5. Under Assumption 3, there exists a neighborhood U of x * and positive constants A 1 , A 2 , B 1 , B 2 such that for all x ∈ U,</p><formula xml:id="formula_35">A 1 ||x -x * || 2 ≤ f (x) -f (x * ) ≤ B 1 ||x -x * || 2 , (2.3) A 2 ||x -x * || 2 ≤ ||∇f (x)|| 2 ≤ B 2 ||x -x * || 2 . (2.4)</formula><p>Let U be a neighborhood of x * for which inequalities (2.3) and (2.4) hold. By Assumption 4 there exists η &gt; 0 such that (ii) there exists a number n 2 such that ||h k || &lt; λδ for every k ≥ n 2 . Proof. First, we show that there exists k such that k ≥ n 1 and ||∇f</p><formula xml:id="formula_36">x ∈ U if f (x) -f (x * ) &lt; η or ||∇f (x)|| &lt; η. Lemma 2.6. Set M 5 = max{3 B1B2 A1A2 ,</formula><formula xml:id="formula_37">(x k )|| &lt; 2δ 1-2μM1 . In fact, if ||∇f (x k )|| ≥ 2δ 1-2μM1 for all k ≥ n 1 , then ||∇f (x k )|| &gt; 2||h k || 1-2μM1 ≥ ||h k || 1-2μM1 for all k ≥ n 1 .</formula><p>By Lemma 2.2, the sequence {f (x k )} ∞ k=n1 is decreasing. Since it is bounded from below by f (x * ), there exists lim k→∞ f (x k ). By replacing δ 0 with δ and max{K 1 , K 2 } with n 1 at the last argument of the proof of Proposition 2.4, we obtain a contradiction.</p><p>Let k 1 be the smallest natural number such that k 1 ≥ n 1 and ||∇f (x k1 )|| ≤ For every natural m, </p><formula xml:id="formula_38">||d k2m-1 || ≤ ||∇f (x k2m-1 )|| + ||h k2m-1 || ≤ 2δ 1 -2μM 1 + δ ≤ 3δ 1 -2μM 1 , ||x k2m -x k2m-1 || = μ 1 L ||d k2m-1 || ≤ 3μ/L 1 -</formula><formula xml:id="formula_39">(x k2m )|| ≤ ||∇f (x k2m ) -∇f (x k2m-1 )|| + ||∇f (x k2m-1 )|| ≤ M 2 ||x k2m -x k2m-1 || + 2δ 1 -2μM 1 ≤ M 2 3μ/L 1 -2μM 1 δ + 2 1 -2μM 1 δ = 2 + 3μM 1 1 -2μM 1 δ &lt; 3δ,</formula><p>where we used μ &lt; 1 9M1 to obtain the last inequality. Since ||∇f (x k2m )|| &lt; 3δ &lt; η, x k2m ∈ U, and we can use Lemma 2.5. We obtain</p><formula xml:id="formula_40">f (x k2m ) -f (x * ) ≤ B 1 ||x k2m -x * || ≤ B 1 A 2 ||∇f (x k2m )|| 2 &lt; B 1 A 2 9δ 2 .</formula><p>Let k be such that k 2m ≤ k &lt; k 2m+1 . Then, by Lemma 2.2,</p><formula xml:id="formula_41">f (x k ) -f (x * ) &lt; f(x k2m ) -f (x * ) &lt; 9 B 1 A 2 δ 2 . Since f (x k ) -f (x * ) &lt; 9 B1 A2 δ 2 &lt; η, x k ∈ U,</formula><p>and we can use Lemma 2.5. We obtain</p><formula xml:id="formula_42">||∇f (x k )|| 2 ≤ B 2 ||x k -x * || 2 ≤ B 2 A 1 f (x k ) -f (x * ) &lt; 9 B 1 B 2 A 1 A 2 δ 2 . Thus, if k satisfies k 2m ≤ k &lt; k 2m+1 , we have ||∇f (x k )|| &lt; 3 B1B2 A1A2 δ. If k satisfies k 2m-1 ≤ k &lt; k 2m , we have ||∇f (x k )|| &lt; 2 1-2μM1 δ. Therefore for each k ≥ k 1 , ||∇f (x k )|| &lt; M 5 δ, and therefore ||d k || ≤ ||∇f (x k )|| + ||h k || ≤ M 5 δ + δ &lt; 2M 5 δ. Thus, if k ≥ k 1 , we have ||∇f (x k )|| &lt; M 5 δ, ||d k || &lt; 2M 5 δ. (2.5)</formula><p>This proves the first part of the lemma.</p><p>To prove the second part, we take</p><formula xml:id="formula_43">n 2 = k 1 +L-1. If k ≥ n 2 ,</formula><p>then not only x k but also L -1 previous terms of the sequence {x k } satisfy inequalities (2.5). Therefore, by following the steps in the proof of Proposition 2.4, we have for k ≥ n 2  Note that the inequality μ &lt; 1 9M1 was used in the proof of Lemma 2.6, and the inequalities μ &lt; η 3M2M3 and μ &lt;</p><formula xml:id="formula_44">||h k || ≤ μM 1 1 L L-1 l=1 ||d k-1 || + ||d k-2 || + • • • + ||d k-l || &lt; μM 1 1 L 2M 5 δ L-1 l=1 l m=1 1 = μM 1 1 L 2M 5 δ L(L -1)<label>2</label></formula><formula xml:id="formula_45">&lt;</formula><formula xml:id="formula_46">|| &lt; δ 0 λ r , ||∇f (x k )|| &lt; M 5 δ 0 λ r ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">3M2M3</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2η</head><p>B1 are equivalent to 3δ 0 &lt; η and 9B1 A2 δ 2 0 &lt; η, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Case II: Quadratic case.</head><p>In <ref type="bibr" target="#b24">[25]</ref> it is shown that when applied to the objective function</p><formula xml:id="formula_47">f (x) = 1 2 (x -c 1 ) 2 + 1 2 (x -c 2 ) 2 ,</formula><p>the standard incremental gradient method with a constant step size</p><formula xml:id="formula_48">x k+1 = x k -μ∇f (k) L (x k )</formula><p>converges to a limit cycle with limit points</p><formula xml:id="formula_49">x * 1 (μ) = (1 -μ)c 1 + c 2 2 -μ , x * 2 (μ) = (1 -μ)c 2 + c 1 2 -μ whenever 0 &lt; μ &lt; 1.</formula><p>When implementing the IAG method one obtains</p><formula xml:id="formula_50">x k+1 = x k - μ 2 x k -c (k)2 + x k-1 -c (k-1)2 = x k - μ 2 x k + x k-1 -(c 1 + c 2 ) .</formula><p>Subtracting x * = (c 1 + c 2 )/2, the unique minimum of f (x), from both sides and denoting the error at the kth iteration by e k = x kx * lead to the following error form:</p><formula xml:id="formula_51">e k+1 = e k - μ 2 e k + e k-1 .</formula><p>The characteristic polynomial of this linear system is λ 2 -(1μ/2)λ + μ/2, and it is easy to show that the roots of this polynomial are inside the unit circle whenever 0 &lt; μ &lt; 2. Hence, when 0 &lt; μ &lt; 2, e k → 0; i.e., x k converges to the unique minimum, in contrast to the standard incremental gradient method. More generally, suppose that the functions f l , l = 1, . . . , L, have the following form: where Q l are given symmetric matrices, c l are given vectors, and L l=1 Q l is positive definite. Under this assumption, the function f (x) = L l=1 f l (x) is strictly convex, having its minimum point at</p><formula xml:id="formula_52">f l (x) = 1 2 x Q l x -c l x, l = 1, . . . ,</formula><formula xml:id="formula_53">x * = L l=1 Q l -1 L l=1 c l , (2.7)</formula><p>and x * is the only stationary point of f (x).</p><p>Proposition 2.8. For sufficiently small μ, lim k→∞ x k = x * , and the rate of convergence of the IAG method (1.4) is linear.</p><p>Proof. Plugging (2.6) into (1.4), the IAG method becomes</p><formula xml:id="formula_54">x k+1 = x k -μ L-1 l=0 Q (k-l) L x k-l -c (k-l) L = x k -μ L-1 l=0 Q (k-l) L x k-l + μc,</formula><p>where c = L l=1 c l , and the factor 1 L was absorbed into μ to simplify the notation. Subtracting x * (2.7) from both sides and adding and subtracting x * inside the parentheses, we obtain</p><formula xml:id="formula_55">x k+1 -x * = x k -x * -μ L-1 l=0 Q (k-l) L (x k-l -x * + x * ) + μc.</formula><p>Denoting the error at the kth iteration by e k = x kx * and the substitution of (2.7) for x * lead to the following error form:</p><formula xml:id="formula_56">e k+1 = e k -μ L-1 l=0 Q (k-l) L e k-l .</formula><p>This relation between a new error and the previous errors can be seen as a periodically time varying linear system. To analyze its stability, which will lead to the convergence result, it is useful to consider L iterations as one iteration <ref type="bibr" target="#b28">[29]</ref>. This can be seen as downsampling the original system by a factor of L, which leads to a time invariant system of a lower sampling rate. Without loss of generality, consider the case where k = NL for some integer N ; i.e., k + 1 corresponds to the first iteration of a new cycle. In this case we have</p><formula xml:id="formula_57">e k+1 = e k -μ L-1 l=0 Q (k-l) L e k-l = e k -μ Q L Q L-1 Q L-2 . . . Q 1 e k = I p -μQ L -μQ L-1 -μQ L-2 . . . -μQ 1 e k ,</formula><p>where I p is the p × p identity matrix and</p><formula xml:id="formula_58">e k = ⎡ ⎢ ⎢ ⎢ ⎣ e k e k-1</formula><p>. . . </p><formula xml:id="formula_59">e k-L+1</formula><formula xml:id="formula_60">e k+2 = e k+1 -μ L-1 l=0 Q (k+1-l)L e k+1-l = e k+1 -μ Q 1 Q L Q L-1 . . . Q 2 e k+1 = I p -μQ 1 -μQ L -μQ L-1 . . . -μQ 2 e k+1 ,</formula><p>and finally</p><formula xml:id="formula_61">e k+L = e k+L-1 -μ L-1 l=0 Q (k+L-1-l)L e k+L-1-l = e k+L-1 -μ Q L-1 Q L-2 Q L-3 . . . Q L e k+L-1 = I p -μQ L-1 -μQ L-2 -μQ L-3 . . . -μQ L e k+L-1 .</formula><p>This leads to the relation</p><formula xml:id="formula_62">e k+L = M L e k+L-1 ,</formula><p>where </p><formula xml:id="formula_63">M L = ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ I p -μQ L-1 -μQ L-2 . . . -</formula><formula xml:id="formula_64">e k+L = M L M L-1 e k+L-2 ,</formula><p>where </p><formula xml:id="formula_65">M L-1 = ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ I p -μQ L-2 -μQ L-3 . . . -</formula><formula xml:id="formula_66">e k+L = M L M L-1 . . . M 1 e k ,</formula><p>where </p><formula xml:id="formula_67">M 1 = ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ I p -μQ L -μQ L-1 . . . -</formula><formula xml:id="formula_68">M = M L M L-1 . . . M 1 ,</formula><p>we have e k+L = M e k , and in general e k+nL = M n e k . Therefore, if for sufficiently small μ &gt; 0 the eigenvalues of M are inside the unit circle, then lim n→∞ e k+nL = 0 pL×1 , where 0 pL×1 is a pL × 1 zero vector; i.e., the method converges to the minimum of the function f (x) and the convergence rate is linear.</p><p>To prove that the eigenvalues of M are inside the unit circle, set </p><formula xml:id="formula_69">A = ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ I p 0 p . . .</formula><formula xml:id="formula_70">B k = ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ Q (k-1) L Q (k-2) L . . . Q (k+1) L Q k 0 p 0 p . . . 0 p 0 p 0 p 0 p . . . 0 p 0 p . . . . . . . . . . . . . . . 0 p 0 p . . . 0 p 0 p ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ , k = 1, . . . , L, so that M k = A -μB k and M = (A -μB L )(A -μB L-1 ) . . . (A -μB 1 )</formula><p>. Hence,</p><formula xml:id="formula_71">M = A L -μ B L A L-1 + AB L-1 A L-2 + A 2 B L-2 A L-3 + • • • + A L-2 B 2 A + A L-1 B 1 + μ 2 C(μ),</formula><p>where C(μ) is an Lp × Lp matrix whose elements are polynomials in μ.</p><p>Note that premultiplying a matrix by A will duplicate the first row of p×p matrices and will shift the rest of the rows down, discarding the last p rows. Postmultiplying by A will add the second column of p × p matrices to the first one and will shift the rest of the columns to the left, inserting a block of p × p zero matrices to the last column. It follows that</p><formula xml:id="formula_72">A L = ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ I p 0 p . . . 0 p 0 p I p 0 p . . . 0 p 0 p I p 0 p . . . 0 p 0 p . . . . . . . . . . . . . . . I p 0 p . . . 0 p 0 p ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ and A L-k B k A k-1 = W 1 (k) 0 (L-k+1)p×(k-1)p 0 (k-1)p×(L-k+1)p 0 (k-1)p×(k-1)p ,</formula><p>where W 1 (k) is a (Lk + 1)p × (Lk + 1)p matrix whose elements are</p><formula xml:id="formula_73">W 1 (k) = ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ k-1 l=0 Q (l) L Q L-1 . . . Q k . . . . . . . . . k-1 l=0 Q (l) L Q L-1 . . . Q k ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦</formula><p>. Downloaded 08/12/13 to 129.173.72.87. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php Therefore, the characteristic polynomial</p><formula xml:id="formula_74">F (μ, λ) of M is F (μ, λ) = det (M -λI Lp ) = det A L -μ L k=1 A L-k B k A k-1 -λI Lp + μ 2 C(μ) . The first p columns of A L -μ L k=1 A L-k B k A k-1 -λI Lp + μ 2 C(μ) are ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ (1 -λ)I p -μ [LQ L + (L -1)Q 1 + • • • + Q L-1 ] + μ 2 C 11 I p -μ [(L -1)Q L + (L -2)Q 1 + • • • + Q L-2 ] + μ 2 C 21 I p -μ [(L -2)Q L + (L -3)Q 1 + • • • + Q L-3 ] + μ 2 C 31 . . . I p -μ (2Q L + Q 1 ) + μ 2 C L-1 1 I p -μQ L + μ 2 C L1 ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ , the second p columns are ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ -(L -1)μQ L-1 + μ 2 C 12 -(L -1)μQ L-1 -λI p + μ 2 C 22 -(L -2)μQ L-1 + μ 2 C 32 . . . -2μQ L-1 + μ 2 C L-1 2 -μQ L-1 + μ 2 C L2 ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ , the next (L -3)p columns are ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ -(L -2)μQ L-2 + μ 2 C 13 . . . -2μQ 2 + μ 2 C 1 L-1 -(L -2)μQ L-2 + μ 2 C 23 . . . -2μQ 2 + μ 2 C 2 L-1 -(L -2)μQ L-2 -λI p + μ 2 C 33 . . . -2μQ 2 + μ 2 C 3 L-1 . . . . . . -2μQ L-2 + μ 2 C L-1 3 . . . -2μQ 2 -λI p + μ 2 C L-1 L-1 -μQ L-2 + μ 2 C L3 . . . -μQ 2 + μ 2 C L L-1 ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦</formula><p>, and the last p columns are</p><formula xml:id="formula_75">⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ -μQ 1 + μ 2 C 1L -μQ 1 + μ 2 C 2L -μQ 1 + μ 2 C 3L . . . -μQ 1 + μ 2 C L-1 L -μQ 1 -λI p + μ 2 C LL ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ ,</formula><p>where C ij , i, j = 1, . . . , L, are p × p matrices whose entries are polynomials in μ.</p><p>It is easy to see that if μ = 0, then F (0, λ) = (-1) Lp λ Lp-p (λ -1) p . Hence, if μ = 0, we have an eigenvalue 0 of multiplicity Lpp and an eigenvalue 1 of multiplicity p. If μ is close enough to zero, the 0-eigenvalues will be close to the origin and therefore inside the unit circle. We need to prove that for sufficiently small positive μ, all the 1-eigenvalues will be inside the unit circle. Let λ = λ(μ) be a smooth function expressing the dependence of one of the 1-eigenvalues on μ. We will prove that dλ dμ (0 + ) &lt; 0. It will be enough for our purposes, since it will show that the Downloaded 08/12/13 to 129.173.72.87. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php trajectory λ = λ(μ) is entering the unit circle, and hence λ(μ) is inside the unit circle for sufficiently small positive μ.</p><p>By the definition of λ(μ), λ(0+) = 1 and F (μ, λ(μ)) = 0 for all μ. It follows that</p><formula xml:id="formula_76">d p F (μ, λ(μ)) dμ p = 0. (2.8)</formula><p>To calculate the left-hand side of (2.8), we use the formula for the derivative of a determinant <ref type="bibr" target="#b22">[23]</ref>. Note that substituting μ = 0 and λ = 1 into each of the first p rows of the matrix M -λI Lp leads to a row in which all of the entries are zeros, and therefore the determinant has a zero value. Therefore the only nonzero terms in</p><formula xml:id="formula_77">d p F (μ,λ(μ)) dμ p</formula><p>after substituting μ = 0 and λ = 1 (more precisely, taking μ → 0 + ) are the terms with the first derivatives in the first p rows (there are p! such terms). Hence taking the pth derivative is reduced to taking the first derivative of each of the first p rows. Substituting λ = 1 and μ → 0 + we obtain</p><formula xml:id="formula_78">d p F (μ, λ(μ)) dμ p = p! det W 2 W 3 W 4 -I (L-1)p×(L-1)p = 0,</formula><p>where</p><formula xml:id="formula_79">W 2 = -λ (0 + )I p - L-1 k=0 (L -k)Q (k) L , W 3 = -(L -1)Q L-1 -(L -2)Q L-2 . . . -2Q 2 -Q 1 ,</formula><p>and W 4 = [I p I p . . . I p ] T . Add all columns of p × p matrices to the first column of p × p matrices to obtain det W 5 W 3 0 (L-1)p×p -I (L-1)p×(L-1)p = 0,</p><p>where</p><formula xml:id="formula_80">W 5 = -λ (0 + )I p -L L k=1 Q k . Calculating the last determinant gives det L L k=1 Q k + λ (0 + )I p = 0.</formula><p>The last equation shows that -λ (0 + ) is an eigenvalue of the matrix</p><formula xml:id="formula_81">L L k=1 Q k . Since L L k=1 Q k is positive definite, -λ (0 + ) &gt; 0,</formula><p>and therefore λ (0 + ) &lt; 0. This proves that for sufficiently small μ &gt; 0 the eigenvalues of the matrix M are strictly inside the unit circle, and hence the sequence x k converges to x * , and the convergence rate is linear.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Initialization and distributed implementation.</head><p>As mentioned in section 1, the IAG method is initiated with L points, x 1 , x 2 , . . . , x L . Possible initialization strategies include setting</p><formula xml:id="formula_82">x 1 = x 2 = • • • = x L</formula><p>or generating the initial points using a single cycle of the standard incremental gradient method (1.5). Another possibility is the following. Given x 1 , compute d 1 = ∇f 1 (x 1 ). Then, for 1 ≤ k ≤ L -1,</p><formula xml:id="formula_83">x k+1 = x k -μ 1 k d k , dkk + 1 = d k + ∇f (k+1) L (x k+1 ).<label>(3.1)</label></formula><p>Therefore, after L -1 iterations we obtain x 1 , . . . , x L and d L = L l=1 ∇f l (x l ). Downloaded 08/12/13 to 129.173.72.87. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p><p>The key feature of the IAG method that makes it suitable for wireless sensor networks applications is that it can be implemented in a distributed manner. Consider a distributed system of L processors enumerated over 1, 2, . . . , L, each of which has access to one of the functions f l (x). The initialization (3.1) begins with x 1 at processor 1. Then processor 1 sets d 1 = ∇f 1 (x 1 ) and transmits x 1 and d 1 to processor 2. Upon receiving x k-1 and d k-1 from processor k -1, processor k calculates x k and d k according to (3.1) and transmits them to processor k + 1. The initialization phase is completed when processor L, upon receiving x L-1 and d L-1 from processor L -1, computes x L and d L according to (3.1) and transmits them to processor 1.</p><p>Once the initialization phase is completed, the algorithm progresses in a cyclic manner. Upon receiving x k-1 and d k-1 from processor (k -1) L , processor (k) L computes x k and d k according to (1.2) and (1.3), respectively, and transmits them to processor (k + 1) L . Note that ∇f (k) L (x k-L ) in (1.3) is available at processor (k) L , since it was the last gradient computed at that processor. Therefore, the only gradient computation at processor (k) L is ∇f (k) L (x k ). At no phase of the algorithm do the processors share information regarding the complete function f (x) or its gradient ∇f (x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Application to wireless sensor networks.</head><p>There are two motivations to use the IAG method: (a) reduced computational burden due to the evaluation of a single gradient per iteration compared to L gradients required for the steepest descent method; and (b) the possibility of a distributed implementation of the method in which each component has access to one of the functions f l (x). The second item has been shown to be very useful in the context of wireless sensor networks <ref type="bibr" target="#b37">[38]</ref>. Wireless sensor networks provide means for efficient large scale monitoring of large areas <ref type="bibr" target="#b44">[45]</ref>. Often the ultimate goal is to estimate certain parameters based on measurements that the sensors collect, giving rise to an optimization problem. If measurements from distinct sensors are modelled as statistically independent, the estimation problem takes the form of (1.1), where f l (x) is indexed by the measurements available at sensor l (see, e.g., <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37]</ref> and the references therein). When transmitting the complete set of data to a central processor is impractical due to bandwidth and power constraints, the IAG method can be implemented in a distributed manner as described in section 3. In the following sections we consider two such estimation problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Robust estimation.</head><p>One of the benefits of a wireless sensor network is the ability to deploy a large number of low cost sensors to densely monitor a certain area <ref type="bibr" target="#b44">[45]</ref>. Because low cost sensors have limited reliability, the system must be designed to be robust to the possibility of individual sensor failures. In estimation tasks, this means that some of the sensors will contribute unreliable measurements, namely outliers. In <ref type="bibr" target="#b35">[36]</ref> the authors suggest the use of robust statistics to alleviate the influence of outliers in the data (see <ref type="bibr" target="#b19">[20]</ref> or, specifically in the context of optimization, <ref type="bibr">[34, p. 347]</ref>). The robust statistics framework uses objective functions that give less weight to outliers. A common objective function used to this end is the function "Fair" <ref type="bibr">[39, p. 110]</ref>, given by</p><formula xml:id="formula_84">g(x) = c 2 |x| c -log 1 + |x| c . (4.1)</formula><p>Following <ref type="bibr" target="#b35">[36]</ref> we simulate a sensor network for measuring pollution levels and assume that a certain percentage of the sensors are damaged and provide unreliable measurements. Each sensor collects a single noisy measurement of the pollution level, Downloaded 08/12/13 to 129.173.72.87. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php and the estimate of the average pollution level is found by minimizing the objective function defined by</p><formula xml:id="formula_85">f (x) = L l=1 f l (x), (4.2)</formula><p>where x ∈ R, and</p><formula xml:id="formula_86">f l (x) = 1 L g(x -y l ),</formula><p>where y l is the measurement collected by sensor l. There were L = 50 sensors in the simulation. To reflect the possibility of faulty sensors, half of the samples were generated according to a Gaussian distribution with mean m 1 = 10 and unit variance (σ 2 1 = 1), and the other half were generated according to a Gaussian distribution with mean m 2 = 10 and ten times higher variance (σ 2 2 = 10). The coefficient c in (4.1) was chosen to be 10.</p><p>For positive x, the first derivative of g(x) is x 1+x/c , and for negative x it is x 1-x/c . Hence, g (0+) = g (0-) = 0. The continuity of g(x) implies then that it is differentiable at zero despite the term |x|. Therefore, the first derivative of g(x) is x 1+|x|/c , it is continuous, and it is bounded by c. Considering positive and negative x's separately also shows that g (0+) = g (0-) = 1 and that, in general, the second derivative of g(x) is 1 (1+|x|/c) 2 , which is bounded by 1. Hence both Assumptions 1 and 2 hold. In addition, since 1 (1+|x|/c) 2 is strictly positive, g(x) is strictly convex, and therefore f (x) is strictly convex as well. Since both lim x→∞ f (x) and lim x→-∞ f (x) diverge to ∞, f (x) has no directions of recession, and therefore, by Theorem 27.1(d) in <ref type="bibr">[41, p. 265]</ref>, the minimum set of f (x) is nonempty. The minimum is unique by the strict convexity of f (x). Since g (x) is continuous and positive everywhere, Assumption 3 is satisfied. The strict convexity of f (x) implies that Assumption 4 holds as well (see <ref type="bibr">Remark 2)</ref>.</p><p>Both the standard incremental gradient method (1.5) with a constant step size μ(k) = μ (abbreviated as IG in the figures) and the IAG method with the initialization (3.1) were implemented with several choices of step size μ. The initial point x 1 was set to 0. In Figure <ref type="figure">4</ref>.1 the trajectories of the two methods are presented. The solid straight line corresponds to the minimum point x * . It is seen that when the step size is sufficiently small, IAG increases more rapidly towards x * than the standard incremental gradient in the early iterations. Furthermore, as predicted by the theory, IAG converges to the true limit, whereas the incremental gradient method converges to a limit cycle. For a larger step size the IAG method overshoots due to its heavy ball characteristic <ref type="bibr">(1.4)</ref>. When the step size is too large, the IAG method no longer converges, but the incremental gradient method still converges to a limit cycle. We have observed this behavior for other values of the parameters m 1 , m 2 , σ 2  1 , σ 2 2 , c as well.</p><p>We also compared the IAG method with the incremental gradient method with a diminishing step size, with Bertsekas' hybrid method <ref type="bibr" target="#b4">[5]</ref>, and with Tseng's incremental gradient with momentum <ref type="bibr" target="#b45">[46]</ref> in terms of number of iterations to convergence. To optimize the performance of the incremental gradient method with a diminishing step size, a relatively large constant step size μ = 0.2 is used until convergence to a limit cycle is detected, and then the diminishing step size is μ(k) = .2μ/( kk), where k is the first iteration in which a limit cycle is detected. Convergence to a limit cycle is declared when |x kx k-L | &lt; .01 for k a multiple of L. To describe the parameters Downloaded 08/12/13 to 129.173.72.87. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php used in the hybrid method, we switch to the notation in <ref type="bibr" target="#b4">[5]</ref>. We set γ = 0.05 and α(μ) as defined in equation (47) in <ref type="bibr" target="#b4">[5]</ref>, with φ(μ) = ζ(1μ), where ζ = 2.5. The transition parameter μ is kept at zero; i.e., the iterates are identical to the incremental gradient method until convergence to a limit cycle is detected as described above. Once a limit cycle is detected, μ is updated after every cycle according to μ := 1.5μ+0.3, i.e., n = 1. These parameters seemed to optimize the performance of the hybrid method. The parameters of the incremental gradient with momentum term were set according to the recommendation in <ref type="bibr" target="#b45">[46]</ref>, which seemed to optimize the performance of the method in our application as well. In particular, we set 0 = 1, 1 = 2 = 0.00001, 3 = 1000, η = 1.5f (x 0 1 ) + 100, ρ = ∞, ω = 0.5, ζ = 0.8, and</p><formula xml:id="formula_87">λ 1 + λ 2 + • • • + λ m = 1.</formula><p>For the IAG method we set μ = 0.05. The convergence point was specified to be the first iteration for which all subsequent iterations satisfy |x kx * | &lt; . Since the IAG and the hybrid methods outperform the incremental gradient method with a diminishing step size and the incremental gradient with momentum term by a large margin, was specified to be 0.01 for the IAG and the hybrid method and 0.1 for the incremental gradient method with a diminishing step size and the incremental gradient with momentum term. The average number of iterations until convergence and its standard deviation were estimated from 100 Monte Carlo simulations and are summarized in The trajectory taken by the different methods in one of these simulations is presented in Figure <ref type="figure">4</ref>.2. It is seen that for this application, the IAG method performs best. Further experimentation required to make more general conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Source localization.</head><p>This section presents a simulation of a sensor network for localizing a source that emits acoustic waves. L sensors are distributed on the perimeter of a field at known spatial locations, denoted r l , l = 1, . . . , L, where r l ∈ R 2 . Each sensor collects a noisy measurement of the acoustic signal transmitted by the source, denoted y l , at an unknown location x. Based on a far-field assumption and an isotropic acoustic wave propagation model <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref>, the problem of estimation of source location can be formulated as a nonlinear least squares problem. Downloaded 08/12/13 to 129.173.72.87. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php The objective function is again of the form (4.2), but now</p><formula xml:id="formula_88">f l (x) = y l -g(||r l -x|| 2 )</formula><p>2 , (4.3)</p><p>x ∈ R 2 , and g(z) = A/z : z ≥ A/ , 2 -2 z/A : z &lt; A/ . <ref type="bibr">(4.4)</ref> In (4.3) g(•) models the received signal strength as a function of the squared distance. In (4.4) A is a known constant characterizing the source's signal strength. For z ≥ A/ (far-field source), the source's signal strength has isotropic attenuation as an inverse function of the squared distance, while for z &lt; A/ (near-field source), the attenuation is linear in the squared distance. It is easy to see that Assumptions 1 and 2 are satisfied, and therefore Proposition 2.4 holds. Clearly, since f (x) is multimodal in this case, Assumptions 3 and 4 cannot hold. However, it was observed in our experiments that when the source is sufficiently distant from the sensors, the objective function has a single minimum inside the observed field (see Figure <ref type="figure">4</ref>.4 for a contour plot of the objective function) and, when initiated not too far from the minimum point, the IAG method has good convergence properties. This suggests the possible application of the IAG method under weaker assumptions than those considered in this paper and motivates further investigation into its properties.</p><p>In the numerical experiment, L = 32 sensors are distributed equidistantly on the perimeter of a 100 × 100 field. The source is located at the point [60, 60] and emits a signal with strength A = 1000. The sensors' noisy measurements were generated according to a Gaussian distribution with a mean equal to the true signal power and unit variance. Both the incremental gradient method with a constant step size and the IAG method with the initialization (3.1) were initiated at the point <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b39">40]</ref> gradient method exhibits oscillations near the eventual limit, whereas the IAG method converges to the minimum. In this scenario, the IAG method outperforms the IG method at early iterations as well.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>2 1 -</head><label>1</label><figDesc>2μM1 } and λ = μM 2 M 5 . Under Assumptions 1, 3, and 4, if there exist positive numbers n 1 and δ such that ||h k || &lt; δ for every k ≥ n 1 , 3δ &lt; η, 9B1 A2 δ 2 &lt; η, and 9μM 1 &lt; 1, then (i) there exists a number k 1 such that ||∇f (x k )|| &lt; M 5 δ and ||d k || &lt; 2M 5 δ for every k ≥ k 1 , and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>2δ 1 -</head><label>1</label><figDesc>2μM1 . Without loss of generality, assume there exists k 2 , the smallest natural number such that k 2 &gt; k 1 and ||∇f (x k2 )|| &gt; 2δ 1-2μM1 . Let k 3 be the smallest natural number such that k 3 &gt; k 2 and ||∇f (x k3 )|| ≤ 2δ 1-2μM1 . Let k 4 be the smallest natural number such that k 4 &gt; k 3 and ||∇f (x k4 )|| &gt; 2δ 1-2μM1 . We define k 5 , k 6 , . . . in a similar manner.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>and ||d k || &lt; 2M 5 δ 0 λ r for k ≥ n r . The inequality μ &lt; 1 M2M5 is equivalent to 0 &lt; λ &lt; 1. Hence, lim k→∞ ||h k || = 0, lim k→∞ ||d k || = 0, and lim k→∞ ||∇f (x k )|| = 0, and by Assumption 4, lim k→∞ x k = x * .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 . 1 .</head><label>41</label><figDesc>Fig. 4.1. Trajectories taken by the IG and IAG methods for the robust "Fair" estimation problem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 . 3 .</head><label>43</label><figDesc>Fig. 4.3. Distance of IG and IAG iterates to the optimal solution x * for source localization problem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>10 Fig. 4 . 4 .</head><label>1044</label><figDesc>Fig. 4.4. Path taken by the IG and IAG methods for source localization problem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>equals one. Then lim sup k→∞ s k ≤ M 1-c . Proof. Define the sequence {w k } k≥1 by w k = s k for 1 ≤ k ≤ L -1 and</head><label></label><figDesc>L. Assumption 2 implies that for all x ∈ R p , ||∇f (x)|| ≤ M 4 , where M 4 = LM 3 .</figDesc><table><row><cell>Lemma 2.1. Let {s k } k≥1 be a sequence of nonnegative real numbers satisfying</cell></row><row><cell>for some fixed integer L &gt; 1 and all k ≥ L</cell></row><row><cell>s k ≤ cQ(s k-1 , s k-2 , . . . , s k-L+1 ) + M,</cell></row><row><cell>where 0 &lt; c &lt; 1, M is nonnegative, and Q(s k-1 , s k-2 , . . . , s k-L+1 ) is a linear form in</cell></row><row><cell>the variables s k-1 , s k-2 , . . . , s k-L+1 , whose coefficients are nonnegative and the sum</cell></row><row><cell>of the coefficients</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>08/12/13 to 129.173.72.87. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php and, by induction,</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Downloaded 08/12/13 to 129.173.72.87. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php Lemma 2.3. Set δ 0 = μM 2 M 3 . Under Assumptions 1 and 2, if μM 2 &lt; 1, there exists K such that for all k &gt; K, ||h k || &lt; δ 0 .</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>2μM 1 δ, Downloaded 08/12/13 to 129.173.72.87. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</figDesc><table><row><cell>and</cell></row><row><cell>||∇f</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>μM 2 M 5 δ = λδ. Downloaded 08/12/13 to 129.173.72.87. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php Thus ||h k || &lt; λδ. This proves the second part of Lemma 2.6. Remark 4. A direct result of Lemma 2.6 is that under Assumptions 1-4, ||h k || → 0 is a sufficient condition for the convergence of x k , generated by any gradient method with errors (2.1), to x * . then lim k→∞ x k = x * .Proof. We prove Proposition 2.7 by repeated use of Lemma 2.6. We start with δ = δ 0 . By applying Lemma 2.3, there exists K such that for all k &gt; K, ||h k || &lt; δ 0 . After applying Lemma 2.6 r times we get a number n r such that ||h k</figDesc><table><row><cell cols="2">Proposition 2.7. Under Assumptions 1, 2, 3, and 4, if μ &lt; min{ 1 9M1 , 1 M2M5 ,</cell></row><row><cell>η 3M1M3 , 1 3M2M3</cell><cell>A2η B1 },</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Downloaded 08/12/13 to 129.173.72.87. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</figDesc><table><row><cell>(2.6)</cell><cell>L,</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</figDesc><table><row><cell>Downloaded 08/12/13 to 129.173.72.87. Similarly,</cell><cell>⎤ ⎥ ⎥ ⎥ ⎦</cell><cell>.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Downloaded 08/12/13 to 129.173.72.87. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</figDesc><table><row><cell>Denoting</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>I p 0 p . . .</cell><cell>0 p I p . . .</cell><cell>. . . . . . . . .</cell><cell cols="2">μQ 2 -μQ 1 0 p 0 p 0 p 0 p . . . . . .</cell><cell>⎤ ⎥ ⎥ ⎥ ⎥ ⎦ ⎥</cell><cell>.</cell></row><row><cell>0 p</cell><cell>0 p</cell><cell>. . .</cell><cell>I p</cell><cell>0 p</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>0 p 0 p I p 0 p . . . 0 p 0 p 0 p I p . . . 0 p 0 p</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>⎤</cell></row><row><cell>. . .</cell><cell>. . .</cell><cell>. . .</cell><cell>. . .</cell><cell>. . .</cell><cell>⎥ ⎥ ⎥ ⎥ ⎥ ⎦</cell></row><row><cell cols="5">0 p 0 p . . . I p 0 p</cell><cell></cell></row><row><cell>and</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 4 . 1 .</head><label>41</label><figDesc>Downloaded 08/12/13 to 129.173.72.87. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 4 . 1</head><label>41</label><figDesc>Number of iterations to convergence.Fig. 4.2. IAG compared to IG with diminishing step size, to the hybrid method, and to IG with momentum term.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>IAG</cell><cell></cell><cell cols="2">Hybrid</cell><cell></cell><cell cols="6">IG diminishing step size</cell><cell></cell><cell cols="2">IG momentum term</cell></row><row><cell></cell><cell></cell><cell></cell><cell>= 0.01</cell><cell></cell><cell></cell><cell cols="2">= 0.01</cell><cell></cell><cell></cell><cell cols="2">= 0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>= 0.1</cell></row><row><cell></cell><cell>Mean</cell><cell></cell><cell>290</cell><cell></cell><cell></cell><cell>589</cell><cell></cell><cell></cell><cell></cell><cell cols="2">601</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2063</cell></row><row><cell></cell><cell>Std</cell><cell></cell><cell>23</cell><cell></cell><cell></cell><cell>135</cell><cell></cell><cell></cell><cell></cell><cell cols="2">258</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>919</cell></row><row><cell>12</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>12</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>x k</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>x k</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>IAG</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>IAG</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">IG with Diminishing Stepsize</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Hybrid method</cell></row><row><cell>0</cell><cell>100</cell><cell>200</cell><cell>300</cell><cell cols="2">400</cell><cell>500</cell><cell>600</cell><cell></cell><cell>700</cell><cell>0</cell><cell></cell><cell>100</cell><cell>200</cell><cell></cell><cell>300</cell><cell>400</cell><cell>500</cell><cell>600</cell><cell>700</cell></row><row><cell></cell><cell></cell><cell></cell><cell>k</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>k</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>12</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>k</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">x</cell><cell>6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>IAG</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">IG with Momentum Term</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>200</cell><cell>400</cell><cell>600</cell><cell>800</cell><cell>1000</cell><cell>1200</cell><cell>1400</cell><cell>1600</cell><cell>1800</cell><cell>2000</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>k</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Downloaded 08/12/13 to 129.173.72.87. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This author's work was supported in part by NIH/NCI grant 1P01 CA87634, by DARPA-MURI grant ARO DAAD 19-02-1-0262, and by NSF contract CCR-0325571. This author's work was supported in part by NIH/NCI grant 1P01 CA87634, by DARPA-MURI grant ARO DAAD 19-02-1-0262, and by NSF contract CCR-0325571.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Convergent incremental optimization transfer algorithms: Application to tomography</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Fessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Blatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">O</forename><surname>Hero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="283" to="296" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The iterated Kalman smoother as a Gauss-Newton method</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Bell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Optim</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="626" to="636" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The ordered subsets mirror descent optimization method with applications to tomography</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ben-Tal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Margalit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nemirovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Optim</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="79" to="108" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Incremental least squares methods and the extended Kalman filter</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Optim</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="807" to="822" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A new class of incremental gradient methods for least squares problems</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Optim</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="913" to="926" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
			<affiliation>
				<orgName type="collaboration">Athena Scientific</orgName>
			</affiliation>
		</author>
		<title level="m">Nonlinear Programming</title>
		<meeting><address><addrLine>Belmont, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>nd ed.</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Tsitsiklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neuro-Dynamic Programming</title>
		<meeting><address><addrLine>Belmont, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Gradient convergence in gradient methods with errors</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Tsitsiklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Optim</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="627" to="642" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Distributed maximum likelihood for sensor networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Blatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<meeting>the 2004 IEEE International Conference on Acoustics, Speech, and Signal Processing<address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="929" to="932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Choosing parameters in block-iterative or ordered subset reconstruction algorithms</title>
		<author>
			<persName><forename type="first">C</forename><surname>Byrne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="321" to="327" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Block-iterative algorithms with underrelaxed Bregman projections</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Censor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">T</forename><surname>Herman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Optim</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="283" to="297" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Steered sequential projections for the inconsistent convex feasibility problem</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Censor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R D</forename><surname>Pierro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaknoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nonlinear Anal</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="385" to="405" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Source localization and beamforming</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Hudson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="30" to="39" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Logistic regression, AdaBoost and Bregman distances</title>
		<author>
			<persName><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="253" to="285" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">New least-square algorithms</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">C</forename><surname>Davidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Optim. Theory Appl</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="187" to="197" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Additive logistic regression: A statistical view of Boosting</title>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Statist</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="337" to="374" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Convergence analysis of parallel backpropagation algorithm for neural Downloaded</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Gaivoronski</surname></persName>
		</author>
		<idno>08/12/13 to 129.173.72.87</idno>
		<ptr target="http://www.siam.org/journals/ojsa.phpnetworks" />
	</analytic>
	<monogr>
		<title level="j">Optim. Methods Softw</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="117" to="134" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
	<note>Redistribution subject to SIAM license or copyright</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A class of unconstrained minimization methods for neural networks training, Optim</title>
		<author>
			<persName><forename type="first">L</forename><surname>Grippo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Methods Softw</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="135" to="150" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Convergent on-line algorithms for supervised learning in neural networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Grippo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1284" to="1299" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Huber</surname></persName>
		</author>
		<title level="m">Robust Statistics</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley and Sons</publisher>
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Decomposition into functions in the minimization problem</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Kibardin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automat. Remote Control</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1311" to="1321" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Convergence of approximate and incremental subgradient methods for convex optimization</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Kiwiel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Optim</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="807" to="840" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Kreyszic</surname></persName>
		</author>
		<title level="m">Advanced Engineering Mathematics</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley and Sons</publisher>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Energy-based collaborative source localization using acoustic microsensor array</title>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP J. Appl. Signal Process</title>
		<imprint>
			<biblScope unit="page" from="321" to="337" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">On the convergence of the LMS algorithm with adaptive learning rate for linear feedforward networks</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">Q</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="226" to="245" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Analysis of an approximate gradient projection method with applications to the backpropagation algorithm</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">Q</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tseng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optim. Methods Softw</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="85" to="101" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Mathematical programming in neural networks</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">L</forename><surname>Mangasarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ORSA J. Comput</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="349" to="360" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Serial and parallel backpropagation convergence via nonmonotone perturbed minimization</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">L</forename><surname>Mangasarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Solodov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optim. Methods Softw</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="103" to="116" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A unified analysis of multirate and periodically time-varying digital filters</title>
		<author>
			<persName><forename type="first">R</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Burrus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits and Systems</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="162" to="168" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The incremental Gauss-Newton algorithm with adaptive stepsize rule</title>
		<author>
			<persName><forename type="first">H</forename><surname>Moriyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yamashita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fukushima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Optim. Appl</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="107" to="141" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A view of the EM algorithm that justifies incremental, sparse, and other variants</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning in Graphical Models</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Jordan</surname></persName>
		</editor>
		<meeting><address><addrLine>Dordrecht, The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Kluwer Academic Publishers</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="355" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Incremental subgradient methods for nondifferentiable optimization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nedić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Optim</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="109" to="138" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Distributed EM algorithms for density estimation and clustering in sensor networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Nowak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="2245" to="2253" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">T</forename><surname>Polyak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
	<note>Introduction to Optimization, Optimization Software</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Proakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Manolakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digital Signal Processing</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Prentice-Hall</publisher>
			<pubPlace>Englewood Cliffs, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Decentralized source localization and tracking</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Rabbat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Nowak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<meeting>the 2004 IEEE International Conference on Acoustics, Speech, and Signal Processing</meeting>
		<imprint>
			<publisher>Montreal</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="921" to="924" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Distributed optimization in sensor networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Rabbat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Nowak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Symposium on Information Processing in Sensor Networks</title>
		<meeting>the Third International Symposium on Information Processing in Sensor Networks<address><addrLine>Berkeley, CA; New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="page" from="20" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Quantized incremental algorithms for distributed optimization</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Rabbat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Nowak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Selected Areas in Communications</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="798" to="808" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J J</forename><surname>Rey</surname></persName>
		</author>
		<title level="m">Introduction to Robust and Quasi-Robust Statistical Methods</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Varberg</surname></persName>
		</author>
		<title level="m">Convex Functions</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Rockafeller</surname></persName>
		</author>
		<title level="m">Convex Analysis</title>
		<meeting><address><addrLine>Princeton, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Princeton University Press</publisher>
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Energy based acoustic source localization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Conference on Information Processing in Sensor Networks</title>
		<title level="s">Lecture Notes in Comput. Sci. 2634</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Feng</surname></persName>
		</editor>
		<editor>
			<persName><surname>Leonidas</surname></persName>
		</editor>
		<meeting>the Second International Conference on Information Processing in Sensor Networks<address><addrLine>Palo Alto, CA; New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2003">2003. 2003</date>
			<biblScope unit="page" from="285" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Maximum likelihood multiple-source localization using acoustic energy measurements with wireless sensor networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="44" to="53" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Incremental gradient algorithms with stepsizes bounded away from zero, Comput</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Solodov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optim. Appl</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="23" to="35" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Habitat monitoring with sensor networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Szewczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Osterweil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Polastre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mainwaring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Estrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comm. ACM</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="34" to="40" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">An incremental gradient(-projection) method with momentum term and adaptive stepsize rule</title>
		<author>
			<persName><forename type="first">P</forename><surname>Tseng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Optim</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="506" to="531" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
