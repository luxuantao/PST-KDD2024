<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Domain Adversarial for Acoustic Emotion Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Student Member, IEEE</roleName><forename type="first">Mohammed</forename><surname>Abdelwahab</surname></persName>
						</author>
						<title level="a" type="main">Domain Adversarial for Acoustic Emotion Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2395D338ACD904BB4E9810C8307FD9A9</idno>
					<idno type="DOI">10.1109/TASLP.2018.2867099</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2018.2867099, IEEE/ACM Transactions on Audio, Speech, and Language Processing</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Speech emotion recognition</term>
					<term>adversarial training</term>
					<term>unlabeled adaptation of acoustic emotional models</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The performance of speech emotion recognition is affected by the differences in data distributions between train (source domain) and test (target domain) sets used to build and evaluate the models. This is a common problem, as multiple studies have shown that the performance of emotional classifiers drop when they are exposed to data that does not match the distribution used to build the emotion classifiers. The difference in data distributions becomes very clear when the training and testing data come from different domains, causing a large performance gap between development and testing performance. Due to the high cost of annotating new data and the abundance of unlabeled data, it is crucial to extract as much useful information as possible from the available unlabeled data. This study looks into the use of adversarial multitask training to extract a common representation between train and test domains. The primary task is to predict emotional attribute-based descriptors for arousal, valence, or dominance. The secondary task is to learn a common representation where the train and test domains cannot be distinguished. By using a gradient reversal layer, the gradients coming from the domain classifier are used to bring the source and target domain representations closer. We show that exploiting unlabeled data consistently leads to better emotion recognition performance across all emotional dimensions. We visualize the effect of adversarial training on the feature representation across the proposed deep learning architecture. The analysis shows that the data representations for the train and test domains converge as the data is passed to deeper layers of the network. We also evaluate the difference in performance when we use a shallow neural network versus a deep neural network (DNN) and the effect of the number of shared layers used by the task and domain classifiers.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>I N many practical applications for speech emotion recogni- tion systems, the testing data (target domain) is different from the labeled data used to train the models (source domain). The mismatch in data distribution leads to a performance degradation of the trained models <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b4">[5]</ref>. Therefore, it is vital to develop more robust systems that are more resilient to changes in train and test conditions <ref type="bibr" target="#b5">[6]</ref>- <ref type="bibr" target="#b8">[9]</ref>. One approach to ensure that models perform well on the target domain is to use training data drawn from the same distribution. However, this approach can be expensive, since it requires enough data with emotional labels to build models specific to a new target domain. A more practical approach is to use available labeled data from similar domains along with unlabeled data from the M. Abdelwahab and C. Busso are with the Department of Electrical and Computer Engineering, The University of Texas at Dallas, Richardson TX, 75013 USA e-mail: Mohammed.Abdel-Wahab@utdallas.edu; busso@utdallas.edu Manuscript received <ref type="bibr">March 26, 2018</ref>; revised xxx xx, 2018.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>source target training testing</head><p>Fig. <ref type="figure">1</ref>: Formulation of the problem of data mismatch between train (source domain) and test (target domain) datasets. Our solution uses unlabeled data from the target domain to reduce the differences between domains.</p><p>target domain, creating models that generalize well to the new testing conditions without the need to annotate extra data with emotional labels. This study proposes an elegant solution for the problem of mismatch in train-test data distributions based on domain adversary training. We formulate the machine-learning problem as follows. We have a source domain(s) with annotated emotional data, which is used to train the models, and a large target domain with unlabeled data (see Fig. <ref type="figure">1</ref>). The testing data come from the target domain. Due to the prohibitive cost of annotating new data every time we change the target domain and the abundance of unlabeled data, we aim to use the unlabeled target data to extract useful information, reducing the differences between the source and target domains. The envisioned system generalizes better and is more robust by maximizing the performance using a shared data representation for the source and target domains. The key principle in our approach is to find a consistent feature representation for the source and target domains. Common approaches to address this problem include feature transformation methods, where the representation of the source data is mapped into a representation that resembles the features in the target domain <ref type="bibr" target="#b9">[10]</ref>, and finding a common representation between the domains, such that the features are invariant across domains <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b12">[13]</ref>. The common domaininvariant features do not necessarily contain useful information about the main task. Therefore, it is vital to constrain the learned common feature representation to ensure that it is discriminative with respect to the main task.</p><p>This paper explores the idea of finding a common representation between the source and target domains, such that data from the domains become indistinguishable while maintaining the critical information used in emotion recognition. This work is inspired by the work of Ganin et al. <ref type="bibr" target="#b13">[14]</ref>, which proposed training an adversarial multitask network. The approach searches the feature space for a representation that accurately describes the data from either domain, containing the relevant information to accurately classify the main task, in our study, the prediction of emotional attributes (arousal, valence, and dominance). The discriminative and domaininvariant features are learned by aligning the data distributions from the domains through back-propagation. This approach allows our framework to use unlabeled target data to learn a flexible representation.</p><p>This paper shows that adversarial training using unlabeled training data benefits emotion recognition. By using the abundant unlabeled target data, we gain on average 27.3% relative improvement in concordance correlation coefficient (CCC) compared to just training with the source data. We evaluate the effect of adversarial training by visualizing the similarity of the data representation learned by the network from both domains. The visualization shows that adversarial training aligns the data distributions as expected, reducing the gap between the source and target domains. The study also shows the effect of the number of shared layers between the domain classifier and the emotion predictor on the performance of the system. The size of the source domain is an important factor that determines the optimal number of shared layers in the network. This novel framework for emotion recognition provides an appealing approach to effectively leverage unlabeled data from a new domain, generalizing the models and improving the classification performance.</p><p>This paper is organized as follows. Section II discusses previous work on speech emotion recognition, with emphasis on frameworks that aim to reduce the mismatch between the train and test datasets. Section III presents our proposed model, providing the motivation and the details of the proposed framework. Section IV presents the experiment evaluation including the databases, network structure and acoustic features. Section V presents the experimental results and the analysis of the main findings. Section VI finalizes the study with conclusions and future research directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>The key challenge in speech emotion recognition is to build classifiers that perform well under various conditions. The mismatches in speech emotion recognition include speaker variability in expressing affect, the emotional context of a given interaction, the channel variability, and the acoustic differences in the environment (e.g., noise, reverberation) <ref type="bibr" target="#b0">[1]</ref>. Some of these challenges are unique to speech emotion recognition, since they are not observed on other speech-based tasks such as automatic speech recognition (ASR). For example, while the ground truth can (almost always) be objectively defined in ASR, this is not the case in speech emotion recognition. The emotional labels to train models come from perceptual evaluations, which are often noisy, reflecting the differences in emotional perception of the evaluators <ref type="bibr" target="#b14">[15]</ref>. It is a very subjective problem as different people can perceive different emotional content after listening to the same audio. Speech emotion recognition systems have to deal with larger speaker variability, beyond acoustic differences associated with variation in the feature space. We express emotional differently. One speaker may raise his/her fundamental frequency while being angry, while another may speak faster. Some speakers may partially mask their emotions due to social conventions. Furthermore, contextual information plays a key role in assessing the emotional content of a speech sample <ref type="bibr" target="#b15">[16]</ref>. For example, the emotional content in a political debate is quite different from the emotional content in a colloquial conversation between friends. All these challenges impact the performance of speech emotion recognition systems when they are tested on new domains (different speakers, different content, different channels). A clear example of this problem is the performance of cross-corpus evaluation (training in one corpus, testing on another corpus). The cross-corpora evaluation in Shami and Verhelst <ref type="bibr" target="#b16">[17]</ref> demonstrated the drop in classification performance observed when training on one emotional corpus and testing on another. Other studies have shown similar results <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b17">[18]</ref>- <ref type="bibr" target="#b21">[22]</ref>. It is important to address the drop in performance in the presence of these mismatches.</p><p>Several approaches have been proposed to solve this problem. Shami and Verhelst <ref type="bibr" target="#b16">[17]</ref> proposed to include more variability in the training data by merging emotional databases. They demonstrated that it is possible to achieve classification performance comparable to within-corpus results. More recently, Chang and Scherer <ref type="bibr" target="#b22">[23]</ref> showed that data from other domains can improve the within-corpus performance of a neural network. They employed deep convolutional generative adversarial network (DCGAN) to extract and learn useful feature representation from unlabeled data from a different domain. This led to better generalization compared to a model that did not make use of unlabeled data.</p><p>The main approach to attenuate the mismatch between train and test conditions is to minimize the differences in the feature space between both domains. Zhang et al. <ref type="bibr" target="#b23">[24]</ref> showed that by separately normalizing the features of each corpus, it is possible to minimize cross-corpus variability. Hassan et al. <ref type="bibr" target="#b24">[25]</ref> increased the weight of the train data that matches the test data distribution by using kernel mean matching (KMM), Kullback-Leibler importance estimation procedure (KLEIP), and unconstrained least-squares importance fitting (uLSIF). Zong et al. <ref type="bibr" target="#b7">[8]</ref> used least square regression (LSR) to mitigate the projected mean and covariance differences between the source data and unlabeled target samples while learning the regression coefficient matrix. Because the learned coefficient matrix depends on the samples selected from the target domain, multiple matrices were estimated and used to test new samples. Each matrix was used to predict an emotional label for a test sample, combining the results with the majority vote rule.</p><p>Studies have also explored mapping both train and test domains to a common space, where the feature representation is more robust to the variations between the domains. Deng et al. <ref type="bibr" target="#b10">[11]</ref> used auto-encoders to find a common feature representation across the domains. They trained an autoencoder such that it minimized the reconstruction error on both domains. Building upon this work, Mao et al. <ref type="bibr" target="#b25">[26]</ref> proposed to learn a shared feature representation across domains by constraining their model to share the class priors across domains. Sagha et al. <ref type="bibr" target="#b26">[27]</ref>, also motivated by the work of Deng et al. <ref type="bibr" target="#b10">[11]</ref>, used principal component analysis (PCA) along with kernel canonical correlation analysis (KCCA) to find views with the highest correlation between the source and target corpora. First, they used PCA to represent the feature space of the source and target data. Then, the features for the source and target domains were projected using the PCA in both domains. Finally, they used KCCA to select the top N dimensions that maximized the correlation between the views. Inspired by universum learning where unlabeled data is used to regularize the training process for support vector machine (SVM), Deng et al. <ref type="bibr" target="#b27">[28]</ref> proposed adding an universum loss to the reconstruction loss of an auto-encoder. The added loss function was defined as the addition of the L 2 -margin loss and the ✏-insensitive loss, making use of both labeled and unlabeled data. This approach aimed to learn an auto-encoder classifier that has low reconstruction and classification errors on both domains.</p><p>Song et al. <ref type="bibr" target="#b28">[29]</ref> proposed a couple of methods based on non-negative matrix factorization (NMF) that utilize data from both train and test domains. The proposed methods aimed to represent a matrix formed by data from both domains as two non-negative matrices whose product is an approximation of the original matrix. The factorization was regularized by maximum mean discrepancy (MMD) to ensure that the differences in the feature distributions of the two corpora were minimized. The proposed methods aim to learn a robust low dimensional feature representation using either unlabeled data or labels as hard constraints on the problem. Abdelwahab and Busso <ref type="bibr" target="#b5">[6]</ref> proposed creating an ensemble of classifiers, where each classifier focuses on a different feature space (each classifier maximized the performance for a given emotion category). The features were selected over the labeled data from the target domain obtained with active learning. This semi-supervised approach learned discriminant features for the target domain, increasing the robustness against shifts in the data distributions between domains.</p><p>Instead of finding a common representation between domains, Deng et al. <ref type="bibr" target="#b29">[30]</ref> trained a sparse auto-encoder on the target data and used it to reconstruct the source data. This approach used feature transformation in a way that exploits the underlying structure in emotional speech learned from the target data. Deng et al. <ref type="bibr" target="#b30">[31]</ref> used two denoising auto-encoders. The first auto-encoder was trained on the target data and the second auto-encoder was trained on the source data, but it was constrained to be close to the first auto-encoder. The second auto-encoder was then used to reconstruct the data from both the source and target domains.</p><p>Our proposed approach takes an innovative formulation with respect to previous work relying on domain adversarial neural network (DANN) <ref type="bibr" target="#b13">[14]</ref>. Liao et al. <ref type="bibr" target="#b31">[32]</ref> used domain adversarial training in speech enhancement to learn noise invariant features. Meng et al. <ref type="bibr" target="#b32">[33]</ref> used multi-task adversarial training to learn speaker invariant senone for ASR. They showed that adversarial training was able to align the feature representations of different speakers. Similarly, Wand and Schmidhuber <ref type="bibr" target="#b33">[34]</ref> used DANN to learn speaker invariant features for lipreading achieving an average relative improvement of 40%. Wang et al. <ref type="bibr" target="#b34">[35]</ref> showed that domain adversarial training outperforms state of the art domain adaptation methods for speaker recognition. Shinohara <ref type="bibr" target="#b35">[36]</ref> showed that the use of DANN can increase the robustness of an ASR system against certain types of noise. Sun et al. <ref type="bibr" target="#b36">[37]</ref> showed that domain adversarial training helps the model to learn features that are stable against accented speech. While these studies have showed that adversarial training can help speech based tasks, this framework has not been used with domains that have multiple forms of mismatches such as speech emotion recognition (e.g., speaker, acoustic conditions and emotional content). This is an important contribution as this framework can reduce the mismatch between train and test sets in a principled way. DANN relies on adversarial training for domain adaptation to learn a flexible representation during the training of the emotion classifier. As the training data changes, both the emotion and domain classifiers readjust their weights to find the new representation that satisfies all conditions. The domain classifier can be considered as a regularizer that prevents the main classifier from over-fitting to the source domain. The final learned representation performs well on the target domain without sacrificing the performance on the source domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED APPROACH A. Motivation</head><p>We present an unsupervised approach that reduces the mismatch between source and target domains by creating a discriminative feature representation that leverages unlabeled data from the target domain.</p><p>We aim to learn a common representation between the source and target domains, where samples from both domains are indistinguishable to a domain classifier. This approach is useful because all the knowledge learned while training the classifier on the source domain is directly applicable to the target domain data. We learn the representation by using a gradient reversal layer (GRL) where the gradient produced by the domain classifier is multiplied by a negative value when it is propagated back to the shared layers. Changing the sign of the gradient causes the feature representation of the samples from both domains to converge, reducing the gap between domains. Ideally, the performance of the domain classifiers should be at random level where both domains "look" the same. When such a representation is learned, the data distributions of both domains are aligned. This approach leads to a large performance improvement in the target domain, as demonstrated by our experimental evaluation (see Section V). A key feature of this framework is that it is unsupervised, since it does not require labeled data from the target domain. However, this framework would continue to be useful when labeled data from the target domain is available, working as a semi-supervised approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Domain Adversarial Neural Network for Emotion Recognition</head><p>Ganin et al. <ref type="bibr" target="#b13">[14]</ref>, inspired by the recent work on generative adversarial networks (GAN) <ref type="bibr" target="#b37">[38]</ref>, proposed the domain adversarial neural network (DANN). The network is trained using labeled data from the source domain and unlabeled data from the target domain. The network learns two classifiers: the main classification task, and the domain classifier, which determines whether the input sample is from the source or target domains. Both classifiers share the first few layers that determine the representation of the data used for classification. The approach introduced a GRL between the domain classifier and the feature representation layers. The layer passes the data during forward propagation and inverts the sign of the gradient during backward propagation. The network attempts to minimize the task and domain classification errors. By inverting the gradient coming from the domain classifier, the network learns a representation that maximizes the error of the domain classifier. By considering these two goals, the model ensures a discriminative representation for the main recognition task that makes the samples from either domain indistinguishable. Figure <ref type="figure" target="#fig_0">2</ref> shows an example of the DANN structure. The network is fed labeled source data and unlabeled target data in equal proportions. In our formulation, we propose to predict emotional attribute descriptors as the primary goal. We train the primary recognition task with the source data, for which we have emotional labels. For the domain classifier, we train the classifier with data from the source and target domains. Notice that the domain classifier does not require emotional labels, so we can rely on unlabeled data from the target domain. The classifiers are trained in parallel. The network's objective is defined as:</p><formula xml:id="formula_0">E(✓ f , ✓ y , ✓ d ) = 1 n n X i=1 L i y (✓ f , ✓ y ) ⇣ 1 n n X i=1 L i d (✓ f , ✓ d ) + 1 m m X i=1 L i d (✓ f , ✓ d ) ⌘ (1)</formula><p>where ✓ f represents the parameters of the shared layers providing the regularized feature representation, ✓ y represents the parameters of the layers associated with the main prediction task, and ✓ d represents the parameters of the layers of the domain classifier (n is the number of labeled training samples, m is the number of unlabeled training samples). The optimization process consists of two loss functions. L i y is the prediction loss for the main task, and L i d is the domain classification loss. The prediction loss and the domain classification loss compete against each other in an adversarial manner. The parameter is a regularization multiplier that controls the tradeoff between the losses. This is a minimax problem. It attempts to find a saddle point parametrized by ✓f , ✓y , ✓d , ( ✓f , ✓y ) = arg min</p><formula xml:id="formula_1">✓ f ,✓y E(✓ f , ✓ y , ✓d ) (2) ✓d = arg max ✓ d E( ✓f , ✓y , ✓ d )<label>(3)</label></formula><p>At the saddle point, the classification loss on the source domain is minimized and the domain classification loss is maximized. The maximization is achieved by introducing a GRL that changes the sign of the gradient going from the domain classification layers to the feature representation layers (see the white layer in Fig. <ref type="figure" target="#fig_0">2</ref>). The updates taken on the feature representation parameters are in opposite direction to the gradient. With this approach, stochastic gradient descent tries to make the features similar across domains, so what is learned from the source domain remains effective for the target domain without loss in performance.</p><p>The simple concept of choosing a representation that confuses a competent domain classifier leads to models that perform better in the target domain without impacting the performance in the source domain. This approach is particularly useful for emotion recognition, as most annotated corpora come from studio settings that greatly differs from real-world testing conditions. This unsupervised approach uses available unlabeled data to align the distributions of both domains. The aligned distributions lead to a common representation, causing the domain classifier's performance to drop to random chance levels. The common indistinguishable representation retains discriminative information learned during the training of the models with data from the source domain. We improve the classifier's performance on the target domain without having to collect new annotated data. This is an important contribution in this field, taking us one step closer toward robust speech emotion classifiers that generalize well in most testing conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL EVALUATION</head><p>We define the main task as a regression problem to estimate the emotional content conveyed in speech described by the emotional attributes arousal (calm versus activated), valence (negative versus positive), and dominance (weak versus strong). This section describes the databases (Section IV-A), the acoustic features (Section IV-B) and the specific network structures (Section IV-C) used in the experimental evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Emotional Databases</head><p>The experimental evaluation considers a multi-corpus setting with three databases. The source domain (train set) corresponds to two databases: the USC-IEMOCAP <ref type="bibr" target="#b38">[39]</ref> and MSP-IMPROV <ref type="bibr" target="#b39">[40]</ref> corpora. The target domain corresponds to the MSP-Podcast <ref type="bibr" target="#b40">[41]</ref> database.</p><p>1) The USC-IEMOCAP Corpus: The USC-IEMOCAP database is an audiovisual corpus recorded from ten actors during dyadic interactions <ref type="bibr" target="#b38">[39]</ref>. It has approximately 12 hours of recordings with detailed motion capture information carefully synchronized with audio (this study only uses the audio). The goal of the data collection was to elicit natural emotions within a controlled setting. This goal was achieved with two elicitation frameworks: emotional scripts, and improvisation of hypothetical scenarios. These approaches allowed the actors to express spontaneous emotional behaviors driven by the context, as opposed to read speech displaying prototypical emotions <ref type="bibr" target="#b41">[42]</ref>. Several dyadic interactions were recorded and manually segmented into turns. Each turn was annotated with emotional labels by at least two evaluators across emotional attributes (valence, arousal, dominance). Dimensional attributes take integer values that range from one to five. The dimensional attribute of an utterance is the average of the values given by the annotators. We linearly map the scores between 3 and 3.</p><p>2) The MSP-IMPROV Corpus: The MSP-IMPROV database is a multimodal emotional database recorded from actors interacting in dyadic sessions <ref type="bibr" target="#b39">[40]</ref>. The recordings were carefully designed to promote natural emotional behaviors, while maintaining control over lexical and emotional contents. The corpus relies on a novel elicitation scheme, where two actors improvised scenarios that lead one of them to utter target sentences. For each of these target sentences, four emotional scenarios were created to contextualize the sentence to elicit happy, angry, sad and neutral reactions, respectively. The approach allows the actor to express emotions as dictated by the scenarios, avoiding prototypical reactions that are characteristic of other acted emotional corpus. Busso et al. <ref type="bibr" target="#b39">[40]</ref> showed that the target sentences occurring within these improvised dyadic interactions were perceived as more natural than the read renditions of the same sentences. The MSP-IMPROV corpus includes not only the target sentences, but also other sentences during the improvisations that led one of the actors to utter the target sentence. It also includes the natural interactions between the actors during the breaks.</p><p>The corpus consists of 8,438 turns of emotional sentences recorded from 12 actors (over 9 hours). The sessions were manually segmented into speaking turns, which were annotated with emotional labels using perceptual evaluations conducted with crowdsourcing <ref type="bibr" target="#b42">[43]</ref>. Each turn was annotated by at least five evaluators, who annotated the emotional content in terms of the dimensional attributes arousal, valence, and dominance. Dimensional attributes take integer values that range from one to five. The consensus label assigned to each speech turn is the average value of the scores provided by the evaluators, which we linearly map between 3 and 3.</p><p>3) The MSP-Podcast Corpus: The MSP-Podcast corpus is an extensive collection of natural speech from multiple speakers appearing in Creative Commons licensed recordings downloaded from audio-sharing websites <ref type="bibr" target="#b40">[41]</ref>. Some of the key aspects of the corpus are the different conditions in which the recordings are collected, a large number of speakers, and a large variety of natural content from spontaneous conversations conveying emotional behaviors. The audio was preprocessed removing portions that contain noise, music or overlapped speech. The recordings were then segmented into speaking turns creating a big audio repository with sentences that are between 2.75 seconds and 11 seconds. Emotional models trained with existing databases are then used to retrieve speech turns with target emotional content. The candidate segments were annotated with emotional labels using an improved version of the crowdsourcing framework proposed by Burmania et al. <ref type="bibr" target="#b42">[43]</ref>.</p><p>Each speech segment was annotated by at least five raters, who provided scores for the emotional dimensions arousal, valence and dominance using seven-Likert scales. The consensus scores are the average scores assigned by the evaluators, which are shifted such that they are in the range between 3 and 3. The collection of this corpus is an ongoing effort. This study uses 14,227 labeled sentences. From this set, we use 4,283 labeled sentences coming from 50 speakers as our test set, which is consistently used across conditions. For the within corpus evaluation (i.e., training and testing in the same domain), we define a development set with 1,860 sentences from 10 speakers, and a train set with the remainder of the corpus (8,084 sentences). This study also uses 73,209 unlabeled sentences from the audio repository of segmented speech turns. The unlabeled segments are used to train the domain classifier in the DANN approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Acoustic Features</head><p>The acoustic features correspond to the set proposed for the INTERSPEECH 2013 Computational Paralinguistics Challenge (ComParE) <ref type="bibr" target="#b43">[44]</ref>. This feature set includes 6,373 acoustic features extracted at the sentence level (one feature vector per sentence). First, it extracts 65 frame-by-frame low-level descriptors (LLDs) which includes various acoustic characteristics such as Mel-frequency cepstral coefficients (MFCCs), fundamental frequency, and energy. The externalization of emotion is conveyed through different aspects of speech production so including these LLDs is important to capture emotional cues. After estimating LLDs, a list of functions are estimated for each LLD, which are referred to as high-level descriptors (HLD) features. These HLDs include standard deviation, minimum, maximum, and ranges. The acoustic features are extracted using OpenSMILE <ref type="bibr" target="#b44">[45]</ref>.</p><p>We separately normalize the features from each domain (i.e., corpus) to have zero mean and a unit standard deviation. The mean and the variance of the data is calculated considering only the values of the features within the 5% and 95% quantiles to avoid outliers skewing the values. After normalization, we ignore any value greater than 10 times its standard deviation by setting their values to zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Network Structure</head><p>As discussed in Section III-B, the DANN approach has three main components: the domain classifier layers, task classifier layers, and feature representation layers. The domain classifier layers are implemented with two layers across all the experimental evaluation. The task classifier layers are also implemented with two layers, except for the shallow network described below. The number of layers in the feature representation layers is a parameter of the network that is set on the development set. We consider different number of shared layers, evaluating the performance of the system with one, two, three or four layers. We fix the number of nodes per layers to 256 nodes. This setting provides good performance on the source domain. This parameter is consistently implemented in all the structures evaluated in this paper.</p><p>We also study whether a simple shallow network can achieve similar performance compared to the deep network. In the shallow network, the task classifier layer and the feature representation layer are implemented with one layer each. We implement the domain classifier layer with two layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Baseline Systems</head><p>We establish two baselines. The first baseline is a network trained and validated only on the source data. This condition creates a mismatch between the train and test conditions. The second baseline corresponds to within corpus evaluations, where the models are trained and tested with data from the target domain. This model assumes that training data from the target domain is available, so it corresponds to the ideal condition. The parameters of the networks are optimized using the development set. The baselines are implemented with similar architectures, serving as a fair comparison with the proposed method (e.g., number of layers, number of nodes). The key difference with the DANN models is the lack of the domain classification layers, where the feature representation layers only consider the primary classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Implementation</head><p>We train the networks using Keras <ref type="bibr" target="#b45">[46]</ref> with Tensorflow as back-end <ref type="bibr" target="#b46">[47]</ref>. We use batch normalization and dropout. The dropout rates are p=0.2 for the input layer and p=0.5 for the rest of the layers. We further regularize the models using max-norm on the weights of value four and a clip norm on the gradient of value ten. The loss function used for the main regression task is the mean square error (MSE). The loss function for the domain classification task is the categorical cross-entropy. We use Adam as an optimizer with a learning rate of 5e 4 <ref type="bibr" target="#b47">[48]</ref>. We train the models for 100 epochs with a batch size of 256 sentences. A parameter of the DANN model is in Equation <ref type="formula">1</ref>, which controls the trade-off between the task and domain classification losses. We follow a similar approach to the one proposed by Ganin et al. <ref type="bibr" target="#b13">[14]</ref>, where is initialized equal to zero for the first ten training epochs. Then, we slowly increase its value until reaching = 1 by the end of the training. We train each model twenty times to reduce the effect of initialization on the performance of the classifiers. We report the average performance across the trials.</p><p>In adversarial training, we need unlabeled data to train the domain classifier. We randomly select samples from the unlabeled portion of the target domain to be fed to the domain classifier. The number of selected samples from the audio repository of unlabeled speech turns is equal to the number of samples in the source domain, keeping the balance in the training set of the domain classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTAL RESULTS</head><p>The performance results for the baseline models and the DANN models are reported in terms of the root mean square error (RMSE), Pearson's correlation coefficient (PR), and concordance correlation coefficient (CCC) between the groundtruth labels and the estimated values. While we presented PR and RMSE, the analysis focus on CCC as the performance metric, which combines mean square error (MSE) and PR. CCC is defined as:</p><formula xml:id="formula_2">⇢ c = 2⇢ x y 2 x + 2 y + (µ x µ y ) 2<label>(4)</label></formula><p>where ⇢ is the Pearson's correlation coefficient, x and y , and µ x and µ y are the standard deviations and means of the predicted score x and the ground truth label y, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Number of Layers For the Shared Feature Representation</head><p>We first study the effect of the number of shared layers between the domain classifier and the primary regression task on the DANN model's performance (e.g., feature representation layers in Fig. <ref type="figure" target="#fig_0">2</ref>). The domain and task classifier layers are implemented with two layers each. We vary the number of shared layers between the classifiers and observe how the changes in feature representation affect the regression performance. This evaluation is conducted exclusively on the development set of the target domain (e.g., MSP-Podcast corpus).</p><p>Table <ref type="table" target="#tab_0">I</ref> shows the average RMSE, PR, and CCC for the models trained with one, two, three and four shared layers. For arousal, we consistently observe better performance (lower RMSE and higher PR and CCC) with one shared layer between the domain and task classifiers. For the CCC values, the differences are statistically significant for all the cases, except when the source domain is the MSP-IMPROV corpus and the DANN model is implemented with two layers (one-tailed t-test over the average across the twenty trials, asserting significance if p-value &lt; 0.05). The performance degrades as more shared layers are added. For valence and dominance, the number of shared layers that provides the best performance varies from one corpus to another. In most cases, two or three shared layers provide the best performance. Based on these results on the development set, we set the number of shared layers for the feature representation networks to one for arousal, two for valence and three for dominance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Regression Performance of the DANN Model</head><p>This section presents the regression results achieved by the DANN model, and the baseline models. Table <ref type="table" target="#tab_1">II</ref> lists the performance for the within-corpus evaluation where the models are trained and tested with data from the MSP-Podcast corpus (referred to as target in the table), and the crosscorpus evaluations, where the models are trained with other corpora (referred to as src on the table). These baseline results are compared with our proposed DANN method (referred to as dann on the table). The results are tabulated in terms of emotional dimensions and networks structures. These values are the average of twenty trials to account for different initializations and set of unlabeled data used to train the DANN models. For the rows denoted "All Databases", we combine all the source domain together (IEMOCAP and MSP-IMPROV corpora), treating them as a single source domain. To determine statistical differences between the src and dann conditions, we use the one-tailed t-test over the twenty trials, asserting significance if p-value &lt; 0.05. We highlight in bold when the difference between these conditions is statistically significant.</p><p>To visualize the results in Table <ref type="table" target="#tab_1">II</ref>, we create figures showing the average performance under different conditions (Figs. <ref type="figure" target="#fig_1">3</ref><ref type="figure">4</ref><ref type="figure">5</ref><ref type="figure" target="#fig_3">6</ref>). We use statistical significance tests to compare different conditions (values obtained from Table <ref type="table" target="#tab_1">II</ref>). We test the hypothesis that population means for matched conditions are different using one-tailed z-test. We assert significance if p-value &lt; 0.05. We use an asterisk in the figures to indicate if there is a statistically significant difference relative to the baseline model trained with the source domain.</p><p>Figure <ref type="figure" target="#fig_1">3</ref> shows the average concordance correlation coefficient across emotional dimensions, training sources, trials, and networks structures (three emotional dimensions ⇥ twenty trials ⇥ three sources ⇥ five structures = 900 matched conditions). The five structures correspond to four deep networks with a varying number of shared layers (one, two, three and four), and the shallow network. The average performance for the within-corpus evaluation (target) is close to double the performance for the cross-corpus evaluations (source). This result demonstrates the importance of minimizing the mismatch  between train and test conditions in emotion recognition.</p><p>The figure shows that the proposed DANN approach greatly improves the performance, achieving 6.6% gain compared to the systems trained with the source domains. As highlighted by the asterisk, the improvement is statistically significant. The proposed approach reduces the gap between within-corpus and cross-corpus emotion recognition results, effectively using unlabeled data from the target domain.</p><p>Figure <ref type="figure">4</ref> shows the average concordance correlation coefficient for each emotional dimension (twenty trials ⇥ three sources ⇥ five structures = 300 matched conditions). On average, the figure shows that models trained using DANN consistently outperform models trained with the source data.</p><p>The asterisk denotes that the differences are statistically significant across all emotional dimensions. The relative improvements over the source models are 22.8% for arousal, 33.4% for valence and 15.5% for dominance. In general, the values for CCC are lower for valence, validating findings from previous studies, which indicated that acoustic features are less discriminative for this emotional attribute <ref type="bibr" target="#b48">[49]</ref>.</p><p>Figure <ref type="figure">5</ref> shows the average concordance correlation coefficient per source domain (three emotional dimensions ⇥ twenty trials ⇥ five structures = 300 matched conditions). The figure shows the within-corpus performance (target) with a solid horizontal line. The results consistently show significant when using DANN. The relative improvements in performance over training with the source domain are 7.7% for the USC-IEMOCAP 36.4% for the MSP-IMPROV corpus, and 25% when we combine all the corpora. 5 also shows that, on average, combining all the sources into one improves the performance of the systems in recognizing emotions. Adding variability during the training of the models is important, as also demonstrated by Shami and Verhelst <ref type="bibr" target="#b16">[17]</ref>. DANN models also benefit from adding variability. By leveraging the added data representations, DANN models are able to find a common representation between the domains without sacrificing the critical features relevant for   learning the primary task.</p><p>Figure <ref type="figure" target="#fig_3">6</ref> compares the performance for deep and shallow networks (see Section IV-C). The figure summarizes the results for the within-corpus evaluations (target), crosscorpus evaluations (source) and with our proposed DANN model (three emotional dimensions ⇥ twenty trials ⇥ three sources = 180 matched conditions). For the target models, we observe significant improvements when using deep structures over shallow structures. However, the differences are not statistically significant for the source and DANN models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Performance of Domain Classifier</head><p>While the focus of this study is on the prediction of emotional attributes, studying the results on domain classi- fication can lead to further insights into the use of DANN in speech emotion recognition. We consider the performance of the domain classification during training. Figure <ref type="figure" target="#fig_5">7</ref> shows the accuracy in detecting the source and target domains during training as a function of epochs. The figure includes the results on the train set, consisting of data from the source and target domains, and on the development set, consisting of data from the source domain. Figure <ref type="figure" target="#fig_5">7a</ref> shows that the GRL behaves as expected, where the performance of the domain classifier converges to the chance level. We also re-implement the approach without the GRL so the domain classifier is trained to maximize its performance. After 20 epochs, we activate the GRL. Figure <ref type="figure" target="#fig_5">7b</ref> shows the results for the train and development sets, where the vertical line indicates the time when the GRL is activated. The figure shows that the domain classifier's performance increases during the first 20 epochs. After activating the GRL, the performance drops to the chance level, as expected.</p><p>We also report the performance of the domain classifiers on the test set. Since all the samples belong to the target domain, any sample assigned to the source domain is a mistake. The results are reported in the Table <ref type="table" target="#tab_2">III</ref>. The domain classifier recognizes data from the target domain with low confidence reaching performance below 73%. We observe that when the source domain consists of both databases (i.e., higher variability in the source domain), the domain classifier's performance is close to 50%, as expected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Data Representation</head><p>The results in Tables I and II demonstrate the benefits of using the proposed DANN framework. This section aims to understand the key aspect of the DANN approach by visualizing the feature representation created during the training process.</p><p>We use the t-SNE toolkit <ref type="bibr" target="#b49">[50]</ref> to create 2D projections of the feature distributions at different layers of the networks. t-SNE is a popular data visualization technique to project high dimensional data into a smaller subspace. The projection provides a useful tool to visually inspect feature representations learned by the model. t-SNE is built on stochastic neighbor embedding. It attempts to minimize the Kullback-Leibler divergence (KLD) between the low-dimensional embedding and the highdimensional data. This approach results in an embedding that retains the local data structure while also revealing global structures such as clusters of samples on different manifolds. Figure <ref type="figure" target="#fig_6">8</ref> shows the distributions of the data from the source domain (blue circles) and the target domain (red crosses) after projecting them in the feature representation created by two models. This example corresponds to the models trained for arousal using the USC-IEMOCAP corpus as the source domain (as explained in Section V-A, the DANN model for arousal has only one shared layer as a feature representation).</p><p>Figure <ref type="figure" target="#fig_6">8a</ref> shows the data representation at the output of the shared layer of the DANN model. Figure <ref type="figure" target="#fig_6">8b</ref> shows the data representation at the equivalent layer of the DNN model trained with the source domain (i.e., the USC-IEMOCAP database). By using adversarial domain training, the feature distributions for samples from both domains are almost indistinguishable, demonstrating that the proposed approach is able to find a common representation. Without adversarial domain training, in contrast, there are large regions in the feature space where it is easy to separate samples from the source and target domains. Figure <ref type="figure" target="#fig_6">8b</ref> suggests the presence of a sourcetarget mismatch which affects the performance of the emotion classifier.</p><p>We also explore the feature representation when the DANN model is trained with four shared layers using the t-SNE toolkit. The objective of this evaluation is to visualize the distribution of the data in each of the shared layers. This evaluation is implemented using the models for valence, using the MSP-IMPROV corpus as the source domain. Figures <ref type="figure" target="#fig_7">9a-9d</ref> show the changes in the data representation for each of the four shared layers in the DANN model. At the first layer (Fig. <ref type="figure" target="#fig_7">9a</ref>), the feature representations for the source data (blue circles) and the target data (red crosses) are dissimilar enough for the domain classifier to distinguish them. While the difference between domains in the feature representation has decreased at the second layer (Fig. <ref type="figure" target="#fig_7">9b</ref>), there are still some regions dominated by samples from one of the two domains. At the third layer (Fig. <ref type="figure" target="#fig_7">9c</ref>), the feature representations of the domains are similar enough to confuse the domain classifier. The common representation is maintained in the fourth layer (Fig. <ref type="figure" target="#fig_7">9d</ref>), where the data from the target domain is indistinguishable from the data from the source domain. This final representation is used by the emotion regression system to predict the emotional state of the target speech. For comparison, we also trained a baseline model with six layers, matching the combined number of shared and task classifier layers in the DANN model. Figures <ref type="figure" target="#fig_7">9e-9h</ref> show the feature representation of the corresponding first four layers of this model. In the DANN model, the data representation of the samples from the source and target domains become more similar in deeper layers. This trend is not observed in the model trained with only the source data. The DANN model effectively reduces the mismatch in the feature representation across domains, which leads to significant improvements in the regression models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Analysis of Speaker Mismatch</head><p>As mentioned before, speaker variability is one of the sources of mismatches in cross-corpus evaluations. The expectation is that after training the DANN model, the representation of the source data will be closer to the representation from the target data. We explore this hypothesis in Figure <ref type="figure" target="#fig_8">10</ref>, which shows the feature embeddings of the source and DANN models. After training the models, we project the samples from the test set (target domain), which are highlighted in black. These samples illustrate the data distribution of the target domain on the corresponding embeddings. We consider the data from two speakers in the development set of the source domain (USC-IEMOCAP corpus). We project these samples in both feature representations, highlighting speech segments from the speakers in red and blue, respectively. The source and DANN models have not been trained with data from this set. Figure <ref type="figure" target="#fig_8">10a</ref> shows that the data from the source domain (red and blue points) is clustered when projected on the embedding of the source domain. The clusters do not necessarily match the distribution of the target domain (black markers). Instead, Figure <ref type="figure" target="#fig_8">10b</ref> shows that the source domain data is more widespread and intertwined with the target data when using the embedding of the DANN models. These figures illustrate that the proposed approach reduces the speaker mismatch in speech emotion recognition tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>This study proposed an appealing framework for emotion recognition that exploits available unlabeled data from the target domain. The proposed approach relies on domain adversarial neural network (DANN), which creates a flexible and discriminant feature representation that reduces the gap in the feature space between the source and target domains. By using adversarial training, we learned domain invariant representations that can effectively discriminate the primary regression task. The model aims to find a balanced representation that aligns the domain distributions, while retaining crucial information for the primary regression task. The proposed adversarial training leads to significant improvements in the performance of emotion recognition classifier over models exclusively trained with data from the source domain, which was demonstrated by the experimental evaluation. We visualized the data representation of both domains by projecting the features into the shared layers of the proposed DANN model. The results showed that the model converged to a common representation, where the source and target domains became indistinguishable. The experimental evaluation also showed that the amount of labeled data from the source domain plays a small role in determining how many shared layers are needed between the domain and regression tasks. Since the number of shared layers has a strong impact on the system's performance, it is vital to identify the optimal number of shared layers, given a specific source domain. One challenging aspect in using the proposed approach is the difficulty of training adversarial networks. For example, Shinohara <ref type="bibr" target="#b35">[36]</ref> noted that in ASR problems, the improvements of DANN were large for some types of noises, but less effective for others. They suggested that tuning the parameters could lead to better results. We also observed that the framework failed to converge for certain parameters, which is common in minimax problems. It is important to investigate robust strategies to set the hyper-parameters of the model that work in most scenarios. When properly trained, however, this powerful framework can elegantly solve one of the most important problems in speech emotion recognition: reducing the mismatch between train and test domains.</p><p>There are many research directions to extend the proposed approach. The current implementation relies on hand-crafted features. The framework can be easily extended so the acoustic features are directly learned during the training of the models using convolutional neural networks (CNNs) (e.g., end-toend system). Likewise, studies have shown the benefit of jointly predicting multiple emotional attributes using multitask learning (MTL) <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b50">[51]</ref>. We hypothesize that combining these approaches with domain adversarial training would lead to further improvements.</p><p>In the case of multiple sources, our approach seems to work well when multiple sources are combined, treating them as one. This approach forces the network to learn a representation that is common across all the source domains. We hypothesize that a better approach is to use asymmetric transformations, where the model learns multiple possible representations for the test data, creating one representation for each source. During testing, the network chooses the most useful representation for each data point. Another alternative approach is to transform the available sources to match the target domains. Finally, this unsupervised approach can be easily extended to the cases where limited labeled data from the target domain is available (semi-supervised approach), creating a flexible framework to create emotion recognition systems that can generalize across domain.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Architecture of the domain adversarial neural network (DANN) proposed for emotion recognition. The network has three parts: a feature representation common to both tasks, a task classifier layer, and a domain classifier layer.</figDesc><graphic coords="4,48.96,56.07,225.95,218.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Fig.3: Average concordance correlation coefficient across conditions. The DANN models provide significant improvements over the baseline model trained with the source domain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :Fig. 5 :</head><label>45</label><figDesc>Fig. 4: Average concordance correlation coefficient across conditions for each emotional dimension.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Average concordance correlation coefficient across conditions for deep and shallow structures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(a) Domain classifier's accuracy with active GRL. (b) Domain classifier's accuracy activating the GRL after 20 epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: Domain classifier accuracy during the training process of the DANN models (arousal model trained on the USC-IEMOCAP corpus). The figures include results for the train and development sets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 :</head><label>8</label><figDesc>Fig.8: Feature representation at the last shared layer for arousal models trained on USC-IEMOCAP corpus. The figures are created with the t-SNE toolkit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 :</head><label>9</label><figDesc>Fig. 9: Feature representation at each layer using the DANN model, and the baseline DNN trained with the source domain. The figures are created with the t-SNE toolkit. The example corresponds to models for valence with four shared layers, using the MSP-IMPROV corpus as the source domain. Figures (a)-(d) report the results for DANN model. Figures (e)-(h) report the results for DNN trained with the source domain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 :</head><label>10</label><figDesc>Fig. 10: Data projection of two speakers (red and blue points) from the source domain in the embedding created by the (a) source and (b) DANN models. The figure considers arousal using the IEMOCAP corpus. The figure shows that samples are better distributed in the embedding of the DANN model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Performance of DANN framework implemented with different number of shared layers for the domain and task classifiers [Iem: USC-IEMOCAP corpus, Imp: MSP-IMPROV corpus, All: All corpora combined]</figDesc><table><row><cell></cell><cell></cell><cell>Arousal</cell><cell>Valence</cell><cell>Dominance</cell></row><row><cell>src</cell><cell>n</cell><cell>RMSE PR CCC</cell><cell>RMSE PR CCC</cell><cell>RMSE PR CCC</cell></row><row><cell></cell><cell>1</cell><cell>1.26 .412 .365</cell><cell>1.57 .140 .135</cell><cell>1.30 .151 .124</cell></row><row><cell>Iem</cell><cell>2 3</cell><cell>1.29 .379 .332 1.32 .353 .305</cell><cell>1.64 .152 .144 1.67 .150 .140</cell><cell>1.27 .160 .135 1.27 .217 .181</cell></row><row><cell></cell><cell>4</cell><cell>1.31 .347 .296</cell><cell>1.67 .147 .135</cell><cell>1.27 .192 .161</cell></row><row><cell></cell><cell>1</cell><cell>1.62 .497 .284</cell><cell>1.58 .171 .137</cell><cell>0.80 .485 .448</cell></row><row><cell>Imp</cell><cell>2 3</cell><cell>1.57 .506 .303 1.71 .399 .218</cell><cell>1.47 .203 .176 1.48 .202 .173</cell><cell>0.82 .514 .476 0.87 .478 .403</cell></row><row><cell></cell><cell>4</cell><cell>1.73 .382 .202</cell><cell>1.43 .210 .193</cell><cell>0.85 .442 .383</cell></row><row><cell></cell><cell>1</cell><cell>1.37 .432 .341</cell><cell>1.56 .181 .165</cell><cell>1.01 .395 .356</cell></row><row><cell>All</cell><cell>2 3</cell><cell>1.41 .396 .313 1.46 .369 .286</cell><cell>1.56 .173 .160 1.58 .183 .170</cell><cell>1.05 .345 .309 1.06 .339 .308</cell></row><row><cell></cell><cell>4</cell><cell>1.45 .371 .282</cell><cell>1.56 .161 .149</cell><cell>1.06 .307 .270</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>Performance of the proposed DANN approach across different structures and emotional dimensions.</figDesc><table><row><cell></cell><cell cols="2">Training Data</cell><cell></cell><cell></cell><cell cols="2">Arousal</cell><cell></cell></row><row><cell cols="2">Source</cell><cell>Approach</cell><cell cols="2">deep RMSE PR</cell><cell>CCC</cell><cell cols="2">shallow RMSE PR</cell><cell>CCC</cell></row><row><cell cols="2">MSP-Podcast</cell><cell>target</cell><cell>0.67</cell><cell>.786</cell><cell>.776</cell><cell>0.66</cell><cell>.787</cell><cell>.767</cell></row><row><cell cols="2">USC-IEMOCAP</cell><cell>src dann</cell><cell>1.01 1.10</cell><cell>.515 .503</cell><cell>.452 .489</cell><cell>1.00 1.07</cell><cell>.510 .520</cell><cell>.434 .503</cell></row><row><cell cols="2">MSP-IMPROV</cell><cell>src dann</cell><cell>1.57 1.48</cell><cell>.551 .607</cell><cell>.267 .381</cell><cell>1.53 1.46</cell><cell>.555 .614</cell><cell>.263 .381</cell></row><row><cell cols="2">All Databases</cell><cell>src dann</cell><cell>1.20 1.18</cell><cell>.496 .555</cell><cell>.386 .499</cell><cell>1.18 1.18</cell><cell>.506 .551</cell><cell>.378 .486</cell></row><row><cell></cell><cell cols="2">Training Data</cell><cell></cell><cell></cell><cell cols="2">Valence</cell><cell></cell></row><row><cell cols="2">Source</cell><cell>Approach</cell><cell cols="2">deep RMSE PR</cell><cell>CCC</cell><cell cols="2">shallow RMSE PR</cell><cell>CCC</cell></row><row><cell cols="2">MSP-Podcast</cell><cell>target</cell><cell>1.08</cell><cell>.327</cell><cell>.294</cell><cell>1.03</cell><cell>.344</cell><cell>.283</cell></row><row><cell cols="2">USC-IEMOCAP</cell><cell>src dann</cell><cell>1.30 1.44</cell><cell>.202 .218</cell><cell>.198 .215</cell><cell>1.19 1.34</cell><cell>.227 .267</cell><cell>.207 .255</cell></row><row><cell cols="2">MSP-IMPROV</cell><cell>src dann</cell><cell>1.42 1.43</cell><cell>.142 .163</cell><cell>.122 .161</cell><cell>1.29 1.37</cell><cell>.154 .180</cell><cell>.127 .178</cell></row><row><cell cols="2">All Databases</cell><cell>src dann</cell><cell>1.33 1.39</cell><cell>.214 .299</cell><cell>.201 .294</cell><cell>1.16 1.33</cell><cell>.245 .272</cell><cell>.228 .267</cell></row><row><cell></cell><cell cols="2">Training Data</cell><cell></cell><cell></cell><cell cols="2">Dominance</cell><cell></cell></row><row><cell cols="2">Source</cell><cell>Approach</cell><cell cols="2">deep RMSE PR</cell><cell>CCC</cell><cell cols="2">shallow RMSE PR</cell><cell>CCC</cell></row><row><cell cols="2">MSP-Podcast</cell><cell>target</cell><cell>0.57</cell><cell>.718</cell><cell>.697</cell><cell>0.57</cell><cell>.723</cell><cell>.704</cell></row><row><cell cols="2">USC-IEMOCAP</cell><cell>src dann</cell><cell>0.95 1.10</cell><cell>.437 .437</cell><cell>.393 .401</cell><cell>0.87 1.03</cell><cell>.461 .497</cell><cell>.426 .457</cell></row><row><cell cols="2">MSP-IMPROV</cell><cell>src dann</cell><cell>0.86 0.89</cell><cell>.563 .565</cell><cell>.368 .456</cell><cell>0.87 0.88</cell><cell>.520 .578</cell><cell>.353 .472</cell></row><row><cell cols="2">All Databases</cell><cell>src dann</cell><cell>0.85 0.92</cell><cell>.481 .526</cell><cell>.418 .499</cell><cell>0.81 0.90</cell><cell>.493 .550</cell><cell>.437 .516</cell></row><row><cell></cell><cell>0.6</cell><cell>*</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>*</cell><cell></cell></row><row><cell>ccc</cell><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Target</cell><cell></cell><cell>Source</cell><cell></cell><cell>DANN</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III :</head><label>III</label><figDesc>Performance of the domain classifier on the target domain. The results correspond to accuracies in detecting samples belonging to the target domain.</figDesc><table><row><cell></cell><cell>Source</cell><cell>Target Test</cell></row><row><cell></cell><cell>USC-IEMOCAP</cell><cell>71.2%</cell></row><row><cell>Arousal</cell><cell>MSP-IMPROV</cell><cell>70.6%</cell></row><row><cell></cell><cell>ALL</cell><cell>46.8%</cell></row><row><cell></cell><cell>USC-IEMOCAP</cell><cell>60.2%</cell></row><row><cell>Valence</cell><cell>MSP-IMPROV</cell><cell>75/0%</cell></row><row><cell></cell><cell>ALL</cell><cell>54.0%</cell></row><row><cell></cell><cell>USC-IEMOCAP</cell><cell>66.7%</cell></row><row><cell>Dominance</cell><cell>MSP-IMPROV</cell><cell>72.5%</cell></row><row><cell></cell><cell>ALL</cell><cell>61.2%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>IEEE TRANSACTIONS ON AUDIO, SPEECH AND LANGUAGE PROCESSING, VOL. XX, NO. XX, MARCH 2018</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>This study was funded by the National Science Foundation (NSF) CAREER grant IIS-1453781.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">in Social emotions in nature and artifact: emotions in human and human-computer interaction</title>
		<author>
			<persName><forename type="first">C</forename><surname>Busso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bulut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Narayanan</surname></persName>
		</author>
		<editor>J. Gratch and S. Marsella</editor>
		<imprint>
			<date type="published" when="2013-11">November 2013</date>
			<publisher>Oxford University Press</publisher>
			<biblScope unit="page" from="110" to="127" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
	<note>Toward effective automatic recognition systems of emotion in speech</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Say cheese vs. smile: Reducing speechrelated variability for facial emotion recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">Mower</forename><surname>Provost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia (MM 2014)</title>
		<meeting><address><addrLine>Orlando, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-11">November 2014</date>
			<biblScope unit="page" from="27" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automatic speech classification to five emotional states based on gender information</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ververidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kotropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Signal Processing Conference</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-09">2004. September 2004</date>
			<biblScope unit="page" from="341" to="375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Comparing feature sets for acted and spontaneous speech in view of automatic emotion recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Vogt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>André</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo (ICME 2005)</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-07">July 2005</date>
			<biblScope unit="page" from="474" to="477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Jointly predicting arousal, valence and dominance with multi-task learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Parthasarathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Busso</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-08">2017. August 2017</date>
			<biblScope unit="page" from="1103" to="1107" />
			<pubPlace>Stockholm, Sweden</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Ensemble feature selection for domain adaptation in speech emotion recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abdelwahab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Busso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-03">2017. March 2017</date>
			<biblScope unit="page" from="5000" to="5004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Incremental adaptation using active learning for acoustic emotion recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abdelwahab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Busso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-03">2017. March 2017</date>
			<biblScope unit="page" from="5160" to="5164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cross-corpus speech emotion recognition based on domain-adaptive least-squares regression</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="585" to="589" />
			<date type="published" when="2016-05">May 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Supervised domain adaptation for emotion recognition from speech</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abdelwahab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Busso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech, and Signal Processing</title>
		<meeting><address><addrLine>Brisbane, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-04">2015. April 2015</date>
			<biblScope unit="page" from="5058" to="5062" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A personalized emotion recognition system using an unsupervised feature adaptation scheme</title>
		<author>
			<persName><forename type="first">T</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Busso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech, and Signal Processing</title>
		<meeting><address><addrLine>Kyoto; Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-03">2012. March 2012</date>
			<biblScope unit="page" from="5117" to="5120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Introducing sharedhidden-layer autoencoders for transfer learning and their application in acoustic emotion recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech, and Signal Processing</title>
		<meeting><address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-05">2014. May 2014</date>
			<biblScope unit="page" from="4818" to="4822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-task deep neural network with shared hidden layers: Breaking down the wall between emotion representations</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Weninger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-03">2017. March 2017</date>
			<biblScope unit="page" from="4490" to="4494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Domain adaptation for large-scale sentiment classification: A deep learning approach</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning (ICML 2011)</title>
		<meeting><address><addrLine>Bellevue, WA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-07">June-July 2011</date>
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">59</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2016-04">April 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Formulating emotion perception as a probabilistic model with application to categorical emotion classification</title>
		<author>
			<persName><forename type="first">R</forename><surname>Lotfian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Busso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Affective Computing and Intelligent Interaction</title>
		<meeting><address><addrLine>San Antonio, TX, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-10">2017. October 2017</date>
			<biblScope unit="page" from="415" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The ordinal nature of emotions</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">N</forename><surname>Yannakakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Busso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Affective Computing and Intelligent Interaction</title>
		<meeting><address><addrLine>San Antonio, TX, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-10">2017. October 2017</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatic classification of expressiveness in speech: A multi-corpus study</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Verhelst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speaker Classification II, C. Müller</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">4441</biblScope>
			<biblScope unit="page" from="43" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fuzzy emotion recognition in natural speech dialogue</title>
		<author>
			<persName><forename type="first">A</forename><surname>Austermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Esau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kleinjohann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kleinjohann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Workshop on Robot and Human Interactive Communication</title>
		<meeting><address><addrLine>Nashville, TN, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-08">August 2005</date>
			<biblScope unit="page" from="317" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cross-corpus acoustic emotion recognition: Variances and strategies</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Vlasenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wöllmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stuhlsatz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wendemuth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rigoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="119" to="131" />
			<date type="published" when="2010-12">July-Dec 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Anger detection performances based on prosodic and acoustic cues in several corpora</title>
		<author>
			<persName><forename type="first">L</forename><surname>Vidrascu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Devillers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Second International Workshop on Emotion: Corpora for Research on Emotion and Affect, International conference on Language Resources and Evaluation (LREC 2008)</title>
		<meeting><address><addrLine>Marrakech, Morocco</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-05">May 2008</date>
			<biblScope unit="page" from="13" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Real-life emotion-related states detection in call centers: A cross-corpora study</title>
		<author>
			<persName><forename type="first">L</forename><surname>Devillers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Vaudable</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chastagnol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech 2010, Makuhari, Japan</title>
		<imprint>
			<date type="published" when="2010-09">September 2010</date>
			<biblScope unit="page" from="2350" to="2353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Emotion recognition from speech by combining databases and fusion of classifiers</title>
		<author>
			<persName><forename type="first">I</forename><surname>Lefter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J M</forename><surname>Rothkrantz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wiggers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Van Leeuwen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text, Speech and Dialogue (TSD 2010)</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Sojka</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Horák</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Kopeček</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Pala</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg; Brno, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010-09">September 2010</date>
			<biblScope unit="volume">6231</biblScope>
			<biblScope unit="page" from="353" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning representations of emotional speech with deep convolutional generative adversarial networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-03">2017. March 2017</date>
			<biblScope unit="page" from="2746" to="2750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unsupervised learning in cross-corpus acoustic emotion recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Weninger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wollmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU 2011)</title>
		<meeting><address><addrLine>Waikoloa, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-12">December 2011</date>
			<biblScope unit="page" from="523" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">On acoustic emotion recognition: compensating for covariate shift</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Damper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Niranjan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1458" to="1468" />
			<date type="published" when="2013-07">July 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Domain adaptation for speech emotion recognition by sharing priors between related source and target classes</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting><address><addrLine>Shanghai, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-03">2016. March 2016</date>
			<biblScope unit="page" from="2608" to="2612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cross lingual speech emotion recognition using canonical correlation analysis on principal component subspace</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sagha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gavryukova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting><address><addrLine>Shanghai, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-03">2016. March 2016</date>
			<biblScope unit="page" from="5800" to="5804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Universum autoencoder-based domain adaptation for speech emotion recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Frühholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="500" to="504" />
			<date type="published" when="2017-04">April 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Crosscorpus speech emotion recognition based on transfer non-negative matrix factorization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="34" to="41" />
			<date type="published" when="2016-10">October 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sparse autoencoderbased feature transfer learning for speech emotion recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Marchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Affective Computing and Intelligent Interaction (ACII 2013)</title>
		<meeting><address><addrLine>Geneva, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-09">September 2013</date>
			<biblScope unit="page" from="511" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Autoencoder-based unsupervised domain adaptation for speech emotion recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1068" to="1072" />
			<date type="published" when="2014-09">September 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Noise adaptive speech enhancement using domain adversarial training</title>
		<author>
			<persName><forename type="first">C.-F</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tsao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-M</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.07501</idno>
		<imprint>
			<date type="published" when="2018-07">July 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Speaker-invariant training via adversarial learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mazalov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-H</forename><surname>Juang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting><address><addrLine>Calgary, AB, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04">2018. April 2018</date>
			<biblScope unit="page" from="5969" to="5973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Improving speaker-independent lipreading with domain-adversarial training</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-08">2017. August 2017</date>
			<biblScope unit="page" from="3662" to="3666" />
			<pubPlace>Stockholm, Sweden</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation via domain adversarial training for speaker recognition</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Chng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting><address><addrLine>Calgary, AB, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04">2018. April 2018</date>
			<biblScope unit="page" from="4889" to="4893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Adversarial multi-task learning of deep neural networks for robust speech recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shinohara</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-09">2016. September 2016</date>
			<biblScope unit="page" from="2369" to="2372" />
			<pubPlace>San Francisco, CA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Domain adversarial training for accented speech recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-F</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-Y</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting><address><addrLine>Calgary, AB, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04">2018. April 2018</date>
			<biblScope unit="page" from="4854" to="4858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS 2014)</title>
		<meeting><address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12">December 2014</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">IEMOCAP: Interactive emotional dyadic motion capture database</title>
		<author>
			<persName><forename type="first">C</forename><surname>Busso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bulut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mower</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Language Resources and Evaluation</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="335" to="359" />
			<date type="published" when="2008-12">December 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">MSP-IMPROV: An acted corpus of dyadic interactions to study emotion perception</title>
		<author>
			<persName><forename type="first">C</forename><surname>Busso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Parthasarathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Burmania</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abdelwahab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sadoughi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">Mower</forename><surname>Provost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="67" to="80" />
			<date type="published" when="2017-03">January-March 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings</title>
		<author>
			<persName><forename type="first">R</forename><surname>Lotfian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Busso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Recording audio-visual emotional databases from actors: a closer look</title>
		<author>
			<persName><forename type="first">C</forename><surname>Busso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Second International Workshop on Emotion: Corpora for Research on Emotion and Affect, International conference on Language Resources and Evaluation (LREC 2008), Marrakech</title>
		<meeting><address><addrLine>Morocco</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-05">May 2008</date>
			<biblScope unit="page" from="17" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Increasing the reliability of crowdsourcing evaluations using online quality assessment</title>
		<author>
			<persName><forename type="first">A</forename><surname>Burmania</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Parthasarathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Busso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="374" to="388" />
			<date type="published" when="2016-12">October-December 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">The INTER-SPEECH 2013 computational paralinguistics challenge: Social signals, conflict, emotion, autism</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Steidl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Batliner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vinciarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ringeval</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chetouani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Weninger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Marchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mortillaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Salamin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Polychroniou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Valente</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013-08">2013. August 2013</date>
			<biblScope unit="page" from="148" to="152" />
			<pubPlace>Lyon, France</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">OpenSMILE: the Munich versatile and fast open-source audio feature extractor</title>
		<author>
			<persName><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wöllmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International conference on Multimedia (MM 2010)</title>
		<meeting><address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-10">October 2010</date>
			<biblScope unit="page" from="1459" to="1462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Keras: Deep learning library for Theano and TensorFlow</title>
		<author>
			<persName><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://keras.io/" />
		<imprint>
			<date type="published" when="2017-04">April 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">TensorFlow: A system for large-scale machine learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Operating Systems Design and Implementation</title>
		<meeting><address><addrLine>Savannah, GA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11">2016. November 2016</date>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Incorporating Nesterov momentum into Adam</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dozat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop track at International Conference on Learning Representations (ICLR 2015)</title>
		<meeting><address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05">May 2015</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Unveiling the acoustic properties that describe the valence dimension</title>
		<author>
			<persName><forename type="first">C</forename><surname>Busso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rahman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012-09">2012. September 2012</date>
			<biblScope unit="page" from="1179" to="1182" />
			<pubPlace>Portland, OR, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Accelerating t-SNE using tree-based algorithms</title>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3221" to="3245" />
			<date type="published" when="2014-10">October 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Ladder networks for emotion recognition: Using unsupervised auxiliary tasks to improve predictions of emotional attributes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Parthasarathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Busso</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-09">2018. September 2018</date>
			<pubPlace>Hyderabad, India</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
