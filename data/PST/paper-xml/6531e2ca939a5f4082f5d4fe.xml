<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pretraining Language Models with Text-Attributed Heterogeneous Graphs</title>
				<funder ref="#_kZRWqbA #_CWbQFBa">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_DxxfrfZ">
					<orgName type="full">Fundamental Research Funds for the Central Universities</orgName>
				</funder>
				<funder ref="#_bBKPUkE">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-10-23">23 Oct 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tao</forename><surname>Zou</surname></persName>
							<email>zoutao@buaa.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">SKLSDE Lab</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Le</forename><surname>Yu</surname></persName>
							<email>yule@buaa.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">SKLSDE Lab</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yifei</forename><surname>Huang</surname></persName>
							<email>yifeihuang@buaa.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">SKLSDE Lab</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Leilei</forename><surname>Sun</surname></persName>
							<email>leileisun@buaa.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">SKLSDE Lab</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bowen</forename><surname>Du</surname></persName>
							<email>dubowen@buaa.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">SKLSDE Lab</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Zhongguancun Laboratory</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Transportation Science and Engineering</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
						</author>
						<title level="a" type="main">Pretraining Language Models with Text-Attributed Heterogeneous Graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-10-23">23 Oct 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2310.12580v2[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In many real-world scenarios (e.g., academic networks, social platforms), different types of entities are not only associated with texts but also connected by various relationships, which can be abstracted as Text-Attributed Heterogeneous Graphs (TAHGs). Current pretraining tasks for Language Models (LMs) primarily focus on separately learning the textual information of each entity and overlook the crucial aspect of capturing topological connections among entities in TAHGs. In this paper, we present a new pretraining framework for LMs that explicitly considers the topological and heterogeneous information in TAHGs. Firstly, we define a context graph as neighborhoods of a target node within specific orders and propose a topology-aware pretraining task to predict nodes involved in the context graph by jointly optimizing an LM and an auxiliary heterogeneous graph neural network. Secondly, based on the observation that some nodes are text-rich while others have little text, we devise a text augmentation strategy to enrich textless nodes with their neighbors' texts for handling the imbalance issue. We conduct link prediction and node classification tasks on three datasets from various domains. Experimental results demonstrate the superiority of our approach over existing methods and the rationality of each design. Our code is available at https://github.com/Hope-Rita/THLM. * Equal Contribution ? Corresponding Author</p><p>masked language modeling (Devlin et al., 2019), next-token prediction (Radford et al., 2018), autoregressive blank infilling (Du et al., 2022)), PLMs can learn general contextual representations from texts in the large-scale unlabelled corpus. Keyword (textless): Brief terms Graph neural networks Paper (text-rich): Title&amp;Abstract Heterogeneous Graph Transformer. Recent years have witnessed the emerging?</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Pretrained Language Models (PLMs) <ref type="bibr" target="#b5">(Devlin et al., 2019;</ref><ref type="bibr" target="#b38">Yang et al., 2019;</ref><ref type="bibr" target="#b2">Brown et al., 2020;</ref><ref type="bibr" target="#b19">Lan et al., 2020)</ref> that built upon the Transformer architecture <ref type="bibr" target="#b32">(Vaswani et al., 2017)</ref> have been successfully applied in various downstream tasks such as automatic knowledge base construction <ref type="bibr" target="#b1">(Bosselut et al., 2019)</ref> and machine translation <ref type="bibr" target="#b11">(Herzig et al., 2020)</ref>. Due to the design of pretraining tasks (e.g., Figure <ref type="figure">1</ref>: As an instance of TAHG, an academic network contains three types of nodes (papers, authors, and keywords) with textual descriptions as well as their multi-relational connections.</p><p>In fact, texts not only carry semantic information but also are correlated with each other, which could be well represented by Text-Attributed Heterogeneous Graphs (TAHGs) that include multi-typed nodes with textual descriptions as well as relations. See Figure <ref type="figure">1</ref> for an example. Generally, TAHGs usually exhibit the following two challenges that are struggled to be handled by existing PLMs.</p><p>Abundant Topological Information (C1). Both first-and higher-order connections exist in TAHGs and can reflect rich relationships. For instance, a paper can be linked to its references via first-order citations and can also be correlated with other papers through high-order co-authorships. However, the commonly used pretraining tasks <ref type="bibr" target="#b28">(Radford et al., 2018;</ref><ref type="bibr" target="#b5">Devlin et al., 2019;</ref><ref type="bibr" target="#b41">Du et al., 2022)</ref> just learn from texts independently and thus ignore the connections among different texts. Although some recent works have attempted to make PLMs aware of graph topology <ref type="bibr" target="#b39">(Yasunaga et al., 2022;</ref><ref type="bibr" target="#b3">Chien et al., 2022)</ref>, they only consider first-order relationships and fail to handle higher-order signals.</p><p>Imbalanced Textual Descriptions of Nodes (C2). In TAHGs, nodes are heterogeneous and their carried texts are often in different magnitudes. For example, papers are described by both titles and abstracts (rich-text nodes), while authors and keywords only have names or brief terms (textless nodes). Currently, how to pretrain LMs to comprehensively capture the above characteristics of TAHGs still remains an open question.</p><p>In this paper, we propose a new pretraining framework to integrate both Topological and Heterogeneous information in TAHGs into LMs, namely THLM. To address C1, we define a context graph as the neighborhoods of the central node within K orders and design a topology-aware pretraining task (context graph prediction) to predict neighbors in the context graph. To be specific, we first obtain the contextual representation of the central node by feeding its texts into an LM and compute the structural representation of nodes in the given TAHG by an auxiliary heterogeneous graph neural network. Then, we predict which nodes are involved in the context graph based on the representations, aiming to inject the multi-order topology learning ability of graph neural networks into LMs. To tackle C2, we devise a text augmentation strategy, which enriches the semantics of textless nodes with their neighbors' texts and encodes the augmented texts by LMs. We conduct extensive experiments on three TAHGs from various domains to evaluate the model performance. Experimental results show that our approach could consistently outperform the state-of-the-art on both link prediction and node classification tasks. We also provide an in-depth analysis of the context graph prediction pretraining task and text augmentation strategy.</p><p>Our key contributions include:</p><p>? We investigate the problem of pretraining LMs on a more complicated data structure, i.e., TAHGs. Unlike most PLMs that can only learn from the textual description of each node, we present a new pretraining framework to enable LMs to capture the topological connections among different nodes.</p><p>? We introduce a topology-aware pretraining task to predict nodes in the context graph of a target node. This task jointly optimizes an LM and an auxiliary heterogeneous graph neural network, enabling the LMs to leverage both first-and high-order signals.</p><p>? We devise a text augmentation strategy to enrich the semantics of textless nodes to mitigate the text-imbalanced problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>A Pretrained Language Model (PLM) can map an input sequence X = (x 1 , x 2 , ? ? ? , x L ) of L tokens into their contextual representations H = (h 1 , h 2 , ? ? ? , h L ) with the design of pretraining tasks like masked language modeling <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref>, next-token prediction <ref type="bibr" target="#b28">(Radford et al., 2018)</ref>, autoregressive blank infilling <ref type="bibr" target="#b41">(Du et al., 2022)</ref>. In this work, we mainly focus on the encoder-only PLMs (e.g., BERT <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref>, RoBERTa <ref type="bibr" target="#b23">(Liu et al., 2019)</ref>) and leave the explorations of PLMs based on encoder-decoder or decoder-only architecture in the future.</p><p>A Text-Attributed Heterogeneous Graph (TAHG) <ref type="bibr" target="#b30">(Shi et al., 2019)</ref> usually consists of multityped nodes as well as different kinds of relations that connect the nodes. Each node is also associated with textual descriptions of varying lengths. Mathematically, a TAHG can be represented by G = (V, E, U, R, X ), where V, E, U and R denote the set of nodes, edges, node types, and edge types, respectively. Each node v ? V belongs to type ?(v) ? U and each edge e u,v has a type ?(e u,v ) ? R. X is the set of textual descriptions of nodes. Note that a TAHG should satisfy</p><formula xml:id="formula_0">|U| + |R| &gt; 2.</formula><p>Existing PLMs mainly focus on textual descriptions of each node separately, and thus fail to capture the correlations among different nodes in TAHGs (as explained in Section 1). To address this issue, we propose a new framework for pretraining LMs with TAHGs, aiming to obtain PLMs that are aware of the graph topology as well as the heterogeneous information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>Figure <ref type="figure" target="#fig_0">2</ref> shows the overall framework of our proposed approach, which mainly consists of two components: topology-aware pretraining task and text augmentation strategy. Given a TAHG, the first module extracts the context graph for a target node and predicts which nodes are involved in the context graph by jointly optimizing an LM and an auxiliary heterogeneous graph neural network. It aims to enable PLMs to capture both first-order and high-order topological information in TAHGs. Since some nodes may have little textual descriptions in TAHGs, the second component is further introduced to tackle the imbalanced textual descriptions of nodes, which enriches the semantics of textless nodes by neighbors' texts. It is worth notic-ing that after the pretraining stage, we discard the auxiliary heterogeneous graph neural network and only apply the PLM for various downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Topology-aware Pretraining Task</head><p>To tackle the drawback that most existing PLMs cannot capture the connections between nodes with textual descriptions, some recent works have been proposed <ref type="bibr" target="#b39">(Yasunaga et al., 2022;</ref><ref type="bibr" target="#b3">Chien et al., 2022)</ref>. Although insightful, these methods solely focus on the modeling of first-order connections between nodes while ignoring high-order signals, which are proved to be essential in fields like network analysis <ref type="bibr" target="#b8">(Grover and Leskovec, 2016;</ref><ref type="bibr" target="#b4">Cui et al., 2019)</ref>, graph learning <ref type="bibr" target="#b18">(Kipf and Welling, 2017;</ref><ref type="bibr" target="#b9">Hamilton et al., 2017)</ref> and recommender system <ref type="bibr" target="#b35">(Wang et al., 2019;</ref><ref type="bibr" target="#b10">He et al., 2020)</ref>. To this end, we propose a topology-aware pretraining task (namely, context graph prediction) for helping LMs capture multi-order connections among different nodes.</p><p>Context Graph Extraction. We first illustrate the definition of the context graph of a target node. Let N u be the set of first-order neighbors of node u in a given TAHG G = (V, E, U, R, X ). The context graph G K u of node u is composed of neighbors that u can reach within K orders (including node u itself) as well as their connections, which is represented by</p><formula xml:id="formula_1">G K u = (V K u , E K u ). V K u = v ? |v ? ? N v ? v ? V K-1 u ? V K-1 u is the node set of G K u and E K u = (u ? , v ? ) ? E|u ? ? V K u ? v ? ? V K u ) denotes the edge set of G K u . It is obvious that V 0 u = {u} and V 1 u = N u ? {u}.</formula><p>Based on the definition, we can extract the context graph of node u based on the given TAHG G. Note that when K ? 2, the context graph G K u will contain multi-order correlations between nodes, which provides an opportunity to capture such information by learning from G K u . Context Graph Prediction. TAHGs not only contain multiple types of nodes and relations but also involve textual descriptions of nodes. Instead of pretraining on single texts like most PLMs do, we present the Context Graph Prediction (CGP) for pretraining LMs on TAHGs to capture the rich information. Since LMs have been shown to be powerful in modeling texts <ref type="bibr" target="#b5">(Devlin et al., 2019;</ref><ref type="bibr" target="#b2">Brown et al., 2020)</ref>, the objective of CGP is to inject the graph learning ability of graph neural networks <ref type="bibr" target="#b0">(Bing et al., 2022)</ref> into LMs.</p><p>Specifically, we first utilize an auxiliary heterogeneous graph neural network to encode the input TAHG G and obtain the representations of all the nodes in V as follows,</p><formula xml:id="formula_2">H G = f HGN N (G) ? R |V|?d ,<label>(1)</label></formula><p>where f HGN N (?) can be implemented by any existing heterogeneous graph neural networks. d is the hidden dimension. Then, we encode the textual description of target node u by an LM and derive its semantic representation by</p><formula xml:id="formula_3">h u LM = MEAN(f LM (X u )) ? R d ,<label>(2)</label></formula><p>where f LM (?) can be realized by the existing LMs.</p><p>Besides, to capture the heterogeneity of node u, we introduce a projection header in the last layer of the PLM. X u denotes the textual descriptions of node u. Next, we predict the probability that node v is involved in the context graph G K u of u via a binary classification task</p><formula xml:id="formula_4">?u,v = sigmoid h u LM ? W ?(v) H G v ,<label>(3)</label></formula><p>where W ?(v) ? R d?d is a trainable transform matrix for node type ?(v) ? R. The ground truth y u,v = 1 if G K u contains v, and 0 otherwise. Pretraining Process. In this work, we use BERT <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref> and R-HGNN <ref type="bibr" target="#b41">(Yu et al., 2022)</ref> to implement f LM (?) and f HGN N (?), respectively. Since it is intractable to predict the appearing probabilities of all the nodes v ? V in Equation (3), we adopt negative sampling <ref type="bibr" target="#b26">(Mikolov et al., 2013)</ref> to jointly optimize f LM (?) and f HGN N (?). To generate positive samples, we uniformly sample k neighbors from a specific relation during each hop. The negative ones are sampled from the remaining node set V \ V K u with a negative sampling ratio of 5 (i.e., five negative samples per positive sample). In addition to the CGP task, we incorporate the widely used Masked Language Modeling (MLM) task to help LMs better handle texts. The final objective function for each node u ? V is</p><formula xml:id="formula_5">L u = L M LM u + L CGP u = -log P ( Xu |X u \ Xu )- v?V K u log ?u,v - 5 i=1 E v ? i ?Pn(V\V K u ) log 1 -?u,v ? i ,</formula><p>where Xu is the corrupted version of node u's original textual descriptions X u with a 40% masking rate following <ref type="bibr" target="#b36">(Wettig et al., 2023)</ref>  the input feature of each node for the auxiliary heterogeneous graph neural network is initialized by its semantic representation based on Equation (2)<ref type="foot" target="#foot_0">1</ref> , which is shown to be better than a randomlyinitialized trainable feature in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Text Augmentation Strategy</head><p>As discussed in Section 1, the textual descriptions of different types of nodes in TAHGs are varying with different lengths, resulting in rich-text nodes and textless nodes. The exhaustive descriptions of rich-text nodes can well reveal their characteristics, while the brief descriptions of textless nodes are insufficient to reflect their semantics and solely encoding such descriptions would lead to suboptimal performance. Therefore, we devise a text augmentation strategy to tackle the imbalance issue, which first enriches the semantics of textless nodes by combining the textual descriptions of their neighbors according to the connections in TAHGs and then computes the augmented texts by LMs. To be specific, for rich-text node u, we use its texts with special tokens <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref> as the input M u , which is denoted as [CLS] X u <ref type="bibr">[SEP]</ref>. For textless node u, we concatenate its texts and k sampled neighbors' texts as the input</p><formula xml:id="formula_6">M u , i.e., [CLS] X u [SEP] X N 1 u [SEP] ... [SEP] X N k u [SEP]</formula><p>, 2 where N i u represents the i-th sampled neighbor of u. Furthermore, in the case of nodes lacking text information, we employ the concatenation of text sequences from neighbors. This approach enables the generation of significant semantic representations for such nodes, effectively addressing the issue of text imbalance. After the augmentation of texts, we change the input of Equation (2) from X u to M u to obtain representation h u LM with more semantics. We empirically find that text augmentation strategy can bring nontrivial improvements without a significant increment of the model's complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Fine-tuning in Downstream Tasks</head><p>After the pretraining process, we discard the auxiliary heterogeneous graph neural network f HGN N (?) and solely apply the pretrained LM f LM (?) to generate the semantic representations of nodes based on Equation (2). We select two graphrelated downstream tasks for evaluation including link prediction and node classification. We employ various headers at the top of f LM (?) to make exhaustive comparisons, including MultiLayer Perceptron (MLP), <ref type="bibr">RGCN (Schlichtkrull et al., 2018)</ref>, HetSANN <ref type="bibr" target="#b12">(Hong et al., 2020)</ref>, and R-HGNN <ref type="bibr" target="#b41">(Yu et al., 2022)</ref>. For downstream tasks, f LM (?) is frozen for efficiency and only the headers can be fine-tuned. Please refer to the Appendix A.2 for detailed descriptions of the headers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Baselines</head><p>Datasets. We conduct experiments on three realworld datasets from different domains, including the academic network (OAG-Venue <ref type="bibr">(Hu et al., 2020b)</ref>), book publication (GoodReads <ref type="bibr" target="#b33">(Wan and McAuley, 2018;</ref><ref type="bibr" target="#b34">Wan et al., 2019)</ref>), and patent application (Patents<ref type="foot" target="#foot_2">3</ref> ). All the datasets have raw texts on all types of nodes, whose detailed descriptions and statistics are shown in the Appendix A.1.</p><p>Compared Methods. We compare THLM with several baselines to generate the representations of nodes and feed them into the headers for downstream tasks. In particular, we select six methods to compute the node representations: BERT <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref> and RoBERTa <ref type="bibr" target="#b23">(Liu et al., 2019)</ref> are widely used PLMs; MetaPath <ref type="bibr" target="#b6">(Dong et al., 2017)</ref> is a representative method for heterogeneous network embedding; MetaPath+BERT combines the textual and structural information as the representations, LinkBERT <ref type="bibr" target="#b39">(Yasunaga et al., 2022)</ref> and GI-ANT <ref type="bibr" target="#b3">(Chien et al., 2022)</ref> are first-order topologyaware PLMs. Besides, we apply OAG-BERT <ref type="bibr" target="#b22">(Liu et al., 2022)</ref> to compare the performance of OAG-Venue. Detailed information about baselines is shown in the Appendix A.1. It is worth noticing that LinkBERT and GIANT are designed for homogeneous graphs instead of TAHGs. Hence, we maintain the 2-order connections among rich-text nodes and remove the textless nodes to build homogeneous graphs for these two methods for evaluation. See Appendix A.6 for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Settings</head><p>Following the official configuration of BERT base (110M params, <ref type="bibr" target="#b5">(Devlin et al., 2019</ref>)), we limit the input length of the text to 512 tokens. For the context graph prediction task, the number of orders K in extracting context graphs is searched in <ref type="bibr">[1,</ref><ref type="bibr">2,</ref><ref type="bibr">3,</ref><ref type="bibr">4]</ref>. For the text augmentation strategy, we search the number of neighbors k for concatenation in <ref type="bibr">[1,</ref><ref type="bibr">2,</ref><ref type="bibr">3]</ref>. We load the weights in BERT base checkpoint released from Transformers tools 4 for initialization. For R-HGNN, we set the hidden dimension of node representations and relation representations to 786 and 64, respectively. The number of attention heads is 8. We use the two-layered R-HGNN in the experiments. To optimize THLM, we use AdamW <ref type="bibr" target="#b24">(Loshchilov and Hutter, 2019)</ref> as the optimizer with (? 1 , ? 2 ) = (0.9, 0.999), weight decay 0.01. For BERT base , we warm up the learning rate for the first 8,000 steps up to 6e-5, then linear decay it. For R-HGNN, the learning rate is set to 1e-4. We set the dropout rate <ref type="bibr" target="#b31">(Srivastava et al., 2014)</ref> of BERT base and R-HGNN to 0.1. We train for 80,000 steps, and batch sizes of 32, 48, and 64 sequences with 512 tokens for OAG-Venue, GoodReads, and Patents, and with maximize utilization while meeting the device constraints. The pretraining process took about three days on four GeForce RTX 3090 GPUs (24GB memory). For downstream tasks, please see Appendix A.4 for 4 https://huggingface.co/bert-base-cased detailed settings of various headers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation Tasks</head><p>Link Prediction. On OAG-Venue, GoodReads, and Patents, the predictions are between paperauthor, book-publisher, and patent-company, respectively. We use RMSE and MAE as evaluation metrics, whose descriptions are shown in Appendix A.3. Considering the large number of edges on the datasets, we use a sampling strategy for link prediction. Specifically, the ratio of the edges used for training, validation, and testing is 30%, 10%, and 10% in all datasets. Each edge is associated with five/one/one negative edge(s) in the training/validation/testing stage.</p><p>Node Classification. We classify the category of papers, books, and patents in OAG-Venue, GoodReads, and Patents. We use Micro-Precision, Micro-Recall, Macro-Precision, Macro-Recall, and NDCG to evaluate the performance of different models. Descriptions of the five metrics are shown in Appendix A.3. Each paper in OAG-Venue only belongs to one venue, which could be formalized as a multi-class classification problem. Each patent or each book is categorized into one or more labels, resulting in multi-label classification problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Performance Comparison</head><p>Due to space limitations, we present the performance on RMSE and MAE for link prediction, as well as Micro-Precision and Micro-Recall for node classification, in Table <ref type="table" target="#tab_1">1</ref>. For the performance on Macro-Precision, Macro-Recall, and NDCG on three datasets in the node classification task, please refer to Appendix A.5. From Table <ref type="table" target="#tab_1">1</ref> and Appendix A.5, we have the following conclusions.</p><p>Firstly, except for MetaPath, BERT and RoBERTa exhibit relatively poorer performance in link prediction across three datasets compared to other baselines. This suggests that incorporating the structural information from the graph can greatly enhance the performance of downstream link prediction tasks. Moreover, RoBERTa achieves notable performance in node classification when compared to other baselines. This implies that leveraging better linguistic representations can further improve the overall performance.</p><p>Secondly, we observe that MetaPath, which solely captures the network embeddings, performs the worst performance among the evaluated methods. However, when MetaPath is combined with semantic information, it achieves comparable or even Third, we note that LinkBERT and GIANT achieve superior results in the majority of metrics for link prediction. This highlights the advantage of learning textual representations that consider the graph structure. However, both GI-ANT and LinkBERT may not yield satisfactory results in node classification on the OAG-Venue and GoodReads. This could be attributed to two reasons: 1) these models primarily focus on first-order graph topology while overlooking the importance of high-order structures, which are crucial in these scenarios; 2) these models are designed specifically for homogeneous graphs and do not consider the presence of multiple types of relations within the graph. Consequently, their effectiveness is limited in TAHGs and may impede their performance.</p><p>Moreover, OAG-BERT demonstrates competitive results in link prediction and strong performance in node classification, thanks to its ability to capture heterogeneity and topology during pretraining. This can be attributed to its capability to learn the heterogeneity and topology of graphs. However, it should be noted that OAG-BERT primarily captures correlations between papers and their metadata, such as authors and institutions, overlooking high-order structural information. These findings highlight the importance of considering both graph structure and high-order relationships when developing models for graph-based tasks.</p><p>Finally, THLM significantly outperforms the existing models due to: 1) integrating multi-order graph topology proximity into language models, which enables the model to capture a more comprehensive understanding of the graph topology; 2) enhancing the semantic representations for textless nodes via aggregating the neighbors' textual descriptions, that generates more informative representations for textless nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Analysis of Context Graph Prediction</head><p>To explore the impact of incorporating multi-order graph topology into language models, we conduct several experiments. These experiments aim to investigate the effects of both first-and high-order topology information, as well as the model's ability to capture structural information using R-HGNN. For the remaining experiments on the analysis of different components like CGP and the text augmentation strategy, we intentionally removed the MLM task to isolate its effects in THLM, namely THLM ? in Figure <ref type="figure" target="#fig_1">3</ref> and Table <ref type="table" target="#tab_2">2</ref>.</p><p>Evaluation on Multi-order Topology Information. To assess the significance of multi-order neighbors' topology, we vary the number of orders K in extracting the context graph from 1 to 4. The corresponding results are illustrated in Figure <ref type="figure" target="#fig_1">3</ref>. Besides, to examine the impact of high-order neighbors, we solely predict the 2-order neighbors in the context graph prediction task, as indicated by w/ 2-order CGP in Table <ref type="table" target="#tab_2">2</ref>.</p><p>From the results, it is evident that THLM achieves superior performance when predicting multi-order neighbors compared to solely predicting 1-order or 2-order neighbors. This suggests that modeling both first-and high-order structures enables LMs to acquire more comprehensive graph topology. Additionally, we observe that THLM exhibits better results when K is 2 in context graph prediction. However, its performance gradually declines as we predict neighbors in higher layers, potentially due to the reduced importance of topological information in those higher-order layers.  Evaluation on Learning Informative Node Features of R-HGNN. In this work, we adopt one of the state-of-the-art HGNNs, i.e., R-HGNN with pre-initialized semantic features on nodes to obtain node representations. To examine the importance of learning informative node representations and complex graph structure in R-HGNN, we conduct experiments using two variants. Firstly, we replace R-HGNN with an MLP encoder or an alternative HGNN framework, i.e., <ref type="bibr">RGCN (Schlichtkrull et al., 2018)</ref> in this experiment, denoted as w/ MLP and w/ RGCN respectively. Secondly, we substitute the semantic node features with randomly initialized trainable features, referred to as w/ random feats.</p><p>The performance results are presented in Table <ref type="table" target="#tab_2">2</ref>.</p><p>From the obtained results, we deduce that both the initial features and effective HGNNs contribute significantly to capturing graph topology and embedding informative node representations effectively. Firstly, unlike MLP, which fails to capture the contextualized graph structure in the context graph prediction task, RGCN allows for the embedding of fine-grained graph structural information, which facilitates better learning of the graph topology. Furthermore, the utilization of effective HGNNs such as R-HGNN enables the embedding of expressive structural representations for nodes. Secondly, R-HGNN demonstrates its superior ability to learn more comprehensive graph structures from nodes compared to using randomly initialized features. These findings underscore the importance of integrating both semantic and structural information to learn informative node representations. To explore the potential of enhancing semantic information for textless nodes through our text augmentation strategy, we design three experimental variants. Firstly, we remove the text sequences of textless nodes and solely rely on the texts of their neighbors as inputs, denoted as "neighbors-only". We set the number of neighbors k as 3 for concatenation. Secondly, we only use the original text descriptions of textless nodes to derive textual embeddings, namely "textless-only". Additionally, we employ the text augmentation strategy by varying the number of neighbors for concatenation from 1 to 3, denoted as "TAS(1-Neighbor)", "TAS(2-Neighbor)", and "TAS(3-Neighbor)", respectively. For all variants, we focus exclusively on the con-text graph prediction task to isolate the effects of other factors. Due to space limitations, we present the Micro-Precision(@1) metric for node classification in the experiments. Similar trends could be observed across other metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Analysis of Text Augmentation Strategy</head><p>From Table <ref type="table">3</ref>, we observe that both neighbors and textless nodes themselves are capable of learning the semantic information for textless nodes. However, relying solely on either of them may lead to insufficient textual representations for nodes. Furthermore, it is found that using texts from more neighbors can enhance the semantic quality of textless nodes. Nevertheless, considering the limitations on the input sequence length of language models, we observe that THLM achieves similar performance when the number of k is increased beyond 2. Therefore, to strike a balance between performance and computational efficiency while accommodating sequence length limitations, we choose k as 3 for concatenation in the text augmentation strategy. To ensure the reliability of our findings, we conduct the task five times using different seeds ranging from 0 to 4. Remarkably, all obtained p-values are below 0.05, indicating statistical significance and confirming the accuracy improvement achieved by our text augmentation strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Effects of Two Pretraining Tasks</head><p>To study the importance of two pretraining tasks for downstream tasks, we use two variants of THLM to conduct the experiments, and the performance is shown in Figure <ref type="figure" target="#fig_4">4</ref>. Specifically, THLM w/o CGP removes the context graph prediction task, which does not predict the context neighbors for the input node. THLM w/o MLM reduces the masked language modeling task, which ignores the textual dependencies in the sentences and only predicts the multi-order graph topology in the pretraining process, i.e., by predicting the neighbors involved in the context graphs for input nodes.  From Figure <ref type="figure" target="#fig_4">4</ref>, we can conclude that THLM achieves the best performance when it employs both two pretraining tasks for training. Removing either of these tasks leads to a decrease in the results. In particular, the context graph prediction task significantly contributes to the overall performance, demonstrating the substantial benefits of incorporating graph topology into our LM. Additionally, the masked language modeling task helps capture the semantics within texts better and further enhances the model performance. Besides, we find that THLM w/o MLM performs better than the original BERT on two datasets, which contributes to our text augmentation strategy for textless nodes. This enhancement allows for better connectivity between the brief terms of textless nodes and their neighboring text sequences, resulting in improved contextual understanding and representation in pretraining PLMs.</p><p>5 Related work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Pretrained Language Models</head><p>The objective of PLMs is to learn general representations of texts from large and unlabeled corpora via pretraining tasks, which could be applied to a variety of downstream tasks. Pretraining tasks that most PLMs widely used include 1) masked language modeling in BERT <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref> and RoBERTa <ref type="bibr" target="#b23">(Liu et al., 2019)</ref>; 2) next token prediction in GPT models <ref type="bibr" target="#b28">(Radford et al., 2018;</ref><ref type="bibr" target="#b2">Brown et al., 2020)</ref>; and 3) autoregressive blank infilling in GLM <ref type="bibr" target="#b41">(Du et al., 2022)</ref>. However, these tasks separately focus on the modeling within single texts and ignore the correlation among multiple texts.</p><p>Recently, several works have been proposed to capture the connections between different texts <ref type="bibr" target="#b20">Levine et al. (2022)</ref>; <ref type="bibr" target="#b3">Chien et al. (2022)</ref>; <ref type="bibr" target="#b39">Yasunaga et al. (2022)</ref>. For example, <ref type="bibr" target="#b3">Chien et al. (2022)</ref> integrated the graph topology into LMs by predicting the connected neighbors of each node. <ref type="bibr" target="#b39">Yasunaga et al. (2022)</ref> designed the document relation prediction task to pretrain LMs, which aims to classify the type of relation <ref type="bibr">(contiguous, random, and linked)</ref> existing between two input text segments. Although insightful, these methods just consider the first-order connections between texts and cannot leverage high-order signals, which may lead to suboptimal performance. In this paper, we aim to present a new pretraining framework for LMs to help them comprehensively capture multi-order relationships as well as heterogeneous information in a more complicated data structure, i.e., TAHGs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Heterogeneous Graph Learning</head><p>Graph Neural Networks (GNNs) <ref type="bibr" target="#b18">(Kipf and Welling, 2017;</ref><ref type="bibr" target="#b9">Hamilton et al., 2017)</ref> have gained much progress in graph learning, which are extensively applied in modeling graph-structure data. Recently, many researchers have attempted to extend GNNs to heterogeneous graphs <ref type="bibr" target="#b42">(Zhang et al., 2019;</ref><ref type="bibr" target="#b7">Fu et al., 2020;</ref><ref type="bibr" target="#b12">Hong et al., 2020;</ref><ref type="bibr" target="#b40">Yu et al., 2020;</ref><ref type="bibr">Hu et al., 2020b;</ref><ref type="bibr" target="#b25">Lv et al., 2021)</ref>, which are powerful in handling different types of nodes and relations as well as the graph topological information. In this work, we aim to inject the graph learning ability of heterogeneous graph neural networks into PLMs via a topology-aware pretraining task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Text-rich Network Mining</head><p>Many real-world scenarios (academic networks, patent graphs) can be represented by text-rich networks, where nodes are associated with rich text descriptions. Existing methods for text-rich network mining can be divided into two categories. The first branch designs the cascade architecture to learn the textual information by Transformer <ref type="bibr" target="#b32">(Vaswani et al., 2017)</ref> and network topology by graph neural networks separately <ref type="bibr" target="#b43">(Zhu et al., 2021;</ref><ref type="bibr" target="#b21">Li et al., 2021;</ref><ref type="bibr" target="#b27">Pang et al., 2022)</ref>. Another group nests GNNs into LMs to collaboratively explore the textual and topological information <ref type="bibr" target="#b37">(Yang et al., 2021;</ref><ref type="bibr" target="#b16">Jin et al., 2022</ref><ref type="bibr">Jin et al., , 2023a,b),b)</ref>. However, these works either mainly focus on the homogeneous graph or modify the architecture of LMs by incorporating extra components. For example, Heterformers <ref type="bibr">(Jin et al., 2023b)</ref> is developed for textrich heterogeneous networks, which aims to embed nodes with rich text and their one-hop neighbors by leveraging the power of both LMs and GNNs during pretraining and downstream tasks. Different from these works, we learn about the more complicated TAHGs and employ auxiliary heterogeneous graph neural networks to assist LMs in capturing the rich information in TAHGs. After the pretraining, we discard the auxiliary networks and only apply the pretrained LMs for downstream tasks without changing their original architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we pretrained language models on more complicated text-attributed heterogeneous graphs, instead of plain texts. We proposed the context graph prediction task to inject the graph learning ability of graph neural networks into LMs, which jointly optimizes an auxiliary graph neural network and an LM to predict which nodes are involved in the context graph. To handle imbalanced textual descriptions of different nodes, a text augmentation strategy was introduced, which enriches the semantics of textless nodes by combining their neighbors' texts. Experimental results on three datasets showed that our approach could significantly and consistently outperform existing methods across two downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Limitations</head><p>In this work, we pretrained language models on TAHGs and evaluated the model performance on link prediction and node classification tasks. Although our approach yielded substantial improvements over baselines, there are still several promising directions for further investigation. Firstly, we just focused on pretraining encoder-only LMs, and it is necessary to validate whether encoder-decoder or decoder-only LMs can also benefit from the proposed pretraining task. Secondly, more downstream tasks that are related to texts (e.g., retrieval and reranking) can be compared in the experiments. Thirdly, it is interesting to explore the pretraining of LMs in larger scales on TAHGs.</p><p>predicting the relation between two segments on Wikipedia and BookCorpus. GIANT <ref type="bibr" target="#b3">(Chien et al., 2022)</ref> extracts graph-aware node embeddings from raw text data via neighborhood prediction in the graph. OAG-BERT <ref type="bibr" target="#b22">(Liu et al., 2022</ref>) is a pretrained language model specialized in academic knowledge services, allowing for the incorporation of heterogeneous entities such as authors, institutions, and keywords into paper embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Headers in Downstream Tasks</head><p>We apply four methods on downstream tasks, which could be shown as follows,</p><p>? MLP relies exclusively on node features as input and uses the multilayer perceptron for prediction, which does not consider the graph information.</p><p>? RGCN incorporates the different relationships among nodes by using transformation matrices respectively in the knowledge graphs <ref type="bibr" target="#b29">(Schlichtkrull et al., 2018)</ref>.</p><p>? HetSANN aggregates different types of relations information from neighbors with a type-aware attention mechanism <ref type="bibr" target="#b12">(Hong et al., 2020)</ref>.</p><p>? R-HGNN learns the relation-aware node representation by integrating fine-grained representation on each set of nodes within separate relations, and semantic representations across different relations <ref type="bibr" target="#b41">(Yu et al., 2022)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Evaluation Metrics</head><p>Seven metrics are adopted to comprehensively evaluate the performance of different models in link prediction and node classification. In link prediction, we use Root Mean Square Error (RMSE) and Mean Absolute Error  </p><formula xml:id="formula_7">RMSE(?, y) = 1 m m i=1 ( ?i -y i ) 2 .</formula><p>MAE measures the absolute errors between predictions and truth values. Given the prediction for all examples ? = {? 1 , ?2 , ? ? ? , ?m }, and the truth data y = {y 1 , y 2 , ? ? ? , y m }, we calculate the total MAE as follows,</p><formula xml:id="formula_8">MAE(?, y) = 1 m m i=1 | ?i -y i |.</formula><p>Micro-averaged precision measures the ability that recognizes more relevant elements than irrelevant ones in all classes. We select the top-K predicted labels as predictions for each sample. Hence, Micro-Precision@K is the proportion of positive predictions that are correct over all classes, which is calculated by,</p><formula xml:id="formula_9">Micro-Precision@K = c i ?C TP(c i ) c i ?C TP(c i ) + FP(c i ) ,</formula><p>where TP(c i ), FP(c i ) is the number of true positives, and false positives for class c i respectively.</p><p>Micro-averaged recall evaluates the model's ability in selecting all the relevant elements in all classes. We select the top-K probability predicted labels as predictions for each sample. Hence, Micro-Recall@K is the proportion of positive labels that are correctly predicted over all classes, which is calculated by,</p><formula xml:id="formula_10">Micro-Recall@K = c i ?C TP(c i ) c i ?C TP(c i ) + FN(c i ) ,</formula><p>where TP(c i ), FN(c i ) is the number of true positives, and false negatives for class c i respectively.</p><p>Macro-averaged precision reflects the average ability to recognize the relevant elements rather than irrelevant ones in each class. We select the top-K probability predicted labels as predictions. Hence, Macro-Precision@K is calculated by averaging all the precision values of all classes, Macro-Precision@K =</p><formula xml:id="formula_11">c i ?C P( ?, S, c i ) |C| ,</formula><p>where ?, S represents the predicted values and truth labels in the datasets, P( ?, S, c i ) is the precision value of class c i . Macro-averaged recall evaluates the average ability to select all the relevant elements in each class. We select the top-K probability predicted labels as predictions. Hence, Macro-Recall@K is calculated by averaging all the recall values of all classes,</p><formula xml:id="formula_12">Macro-Recall@K = c i ?C R( ?, S, c i ) |C| ,</formula><p>where ?, S represents the predicted values and truth labels in the datasets, R( ?, S, c i ) is the recall value of class c i . NDCG measures the ranking quality by considering the orders of all labels. For each sample p i , NDCG is calculated by</p><formula xml:id="formula_13">NDCG@K(p i ) = K k=1 ?( ?k i ,S i ) log 2 (k+1) min(K,|S i |) k=1 1 log 2 (k+1)</formula><p>, where ?k i denotes the k-th predicted label of example p i . ? (v, S) is 1 when element v is in set S, otherwise 0. We calculate the average NDCG of all examples as a metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Detailed Settings in Downstream Tasks</head><p>In downstream tasks, we search the hidden dimension of node representation for headers in <ref type="bibr">[32,</ref><ref type="bibr">64,</ref><ref type="bibr">128,</ref><ref type="bibr">256,</ref><ref type="bibr">512]</ref>. For methods that use attention mechanisms, (i.e., HetSANN and R-HGNN), the number of attention heads is searched in <ref type="bibr">[1,</ref><ref type="bibr">2,</ref><ref type="bibr">4,</ref><ref type="bibr">8,</ref><ref type="bibr">16]</ref>. The training process is following R-HGNN <ref type="bibr" target="#b41">(Yu et al., 2022)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Detailed Experimental Results</head><p>We show the Macro-Precision(@1) and Macro-Recall(@1) in the node classification task on three datasets in Table <ref type="table" target="#tab_6">7</ref>. Since the values of NDCG(@1) are the same as Micro-Precision(@1), we do not show duplicate results. Besides, since node classification tasks on GoodReads and Patents belong to multi-label node classification, we show the performance on five metrics when K is 3 and 5 in Table <ref type="table" target="#tab_7">8</ref> and<ref type="table" target="#tab_8">Table 9 respectively.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 LinkBERT &amp; GIANT</head><p>In our baselines, LinkBERT and GIANT are specifically designed for homogeneous text-attributed graphs, which cannot be directly applied in TAHGs.</p><p>To address this, we convert the TAHGs into homogeneous graphs that contain the set of rich-text nodes and their connections to ensure that all nodes contain rich semantic information in the graphs. For Patents and GoodReads, we extract the 2-order relationships in the graph and discard the textless nodes along with their relative edges to construct the homogeneous graphs. In the case of the OAG-Venue dataset, due to the high density of the secondorder graph, we choose to construct a homogeneous graph using a subset of crucial meta-path information to save the graph topology as much as possible. Inspired by <ref type="bibr" target="#b41">Yu et al. (2022)</ref>. we utilize the metapath "P-F-P" (Paper-Field-Paper) and the direct relation "P-P" (Paper-Paper) to build the homogeneous graph for conducting experiments.</p><p>In addition to previous experiments, we conducted another experiment to capture the first-order information in the TAHGs while preserving the graph topology as much as possible. Specifically, we discard the heterogeneity of nodes and relationships in the graph to build a homogeneous graph, and the results are shown in Table <ref type="table" target="#tab_4">5</ref>.</p><p>From Table <ref type="table" target="#tab_4">5</ref>, it is evident that pretraining LinkBERT and GIANT on TAHGs solely for 1order prediction may not yield optimal results. There are two key reasons for this observation: 1) textless nodes always lack sufficient textual content, leading to scarce semantic information. Hence, predicting relationships between textless nodes and their neighbors becomes challenging for language models. 2) Apart from first-order neighbors, highorder neighbors provide more complex structure information within the graph. By considering the relationships beyond the immediate neighbors, LMs could capture the graph topology across nodes more effectively and comprehensively. These findings highlight the importance of considering both first-order and high-order structure information in TAHGs and addressing the challenges of limited semantics on textless nodes. By tackling both problems, our model can learn better in TAHGs.  We investigate the effect of treasured structural information in the TAHGs. Specifically, we solely change the number of negative candidates for each positive entity in the context graph prediction task in <ref type="bibr">[1,</ref><ref type="bibr">3,</ref><ref type="bibr">5,</ref><ref type="bibr">7]</ref> in the pretraining stage. We present the performance of GoodReads and Patents on the Micro-Precision(@1) metric in the node classification task.</p><p>From Figure <ref type="figure" target="#fig_7">5</ref>, we could observe that the performance with a smaller number or larger number in sampling negative candidates would be worse. This observation can be explained by two factors.</p><p>Firstly, the model may receive limited structural information when selecting a smaller number of negative candidates, which hampers the model's ability to understand the underlying topology structure effectively. Secondly, sampling a larger number of negative candidates may bring noise topological information and make it difficult to distinguish meaningful patterns and relationships. Hence, the optimal performance is achieved when the number of sampled negative candidates falls within a proper range. By striking a balance between learning sufficient topological information and avoiding excessive noise, the model can effectively capture the graph structure and achieve better performance in downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.8 Performance on Large-scale Datasets</head><p>In our evaluation, we further test THLM on largescale datasets (i.e., obgn-mag dataset <ref type="bibr">(Hu et al., 2020a)</ref>) for the node classification task. The performance is shown in Table <ref type="table" target="#tab_5">6</ref>. We observe that THLM demonstrates scalability to larger datasets, outperforming baselines such as LinkBERT and GIANT. This outcome highlights the effectiveness of THLM, particularly its superior performance on the obgn-mag dataset.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Framework of the proposed approach.</figDesc><graphic url="image-15.png" coords="4,88.99,81.33,81.28,51.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Effects of learning multi-order topology information in TAHGs on node classification.</figDesc><graphic url="image-31.png" coords="7,85.87,259.65,90.74,68.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Table 3 :</head><label>3</label><figDesc>Results on the node classification task in evaluating the effectiveness of our text augmentation strategy. Neighbor) 0.7547 0.8381 0.8426 0.8475 TAS(3-Neighbor) 0.7549 0.8382 0.8425 0.8485 Patents neighbors-only 0.6971 0.7040 0.7228 0.7224 textless-only 0.6856 0.6923 0.7139 0.7164 TAS(1-Neighbor) 0.6959 0.7004 0.7211 0.7221 TAS(2-Neighbor) 0.6960 0.7050 0.7219 0.7233 TAS(3-Neighbor) 0.6948 0.7050 0.7275 0.7281</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Importance of two pretraining tasks on the node classification task.</figDesc><graphic url="image-36.png" coords="8,87.82,651.97,90.69,72.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(MAE) metrics. In node classification, we use Micro-Precision, Micro-Recall, Macro-Precision, Macro-Recall, and Normalized Discounted Cumulative Gain (NDCG) metrics for evaluation. Details of the metrics are shown below. RMSE evaluates the predicted ability for truth values, which calculates the error between prediction results and truth values. Given the prediction for all examples ? = {? 1 , ?2 , ? ? ? , ?m }, and the truth data y = {y 1 , y 2 , ? ? ? , y m }, we calculate the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Precision on node classification with different numbers in sampling negative candidates in the pretrainprocess.</figDesc><graphic url="image-42.png" coords="14,317.72,469.38,91.17,67.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>. P n (?) denotes the normal noise distribution. Additionally,</figDesc><table><row><cell>text-attributed ?</cell><cell cols="3">? ? ? ? ? (positive sample) ? ? ~?? (? \? ? (negative sample) ? )</cell><cell></cell><cell>Heterogeneous Graph Neural Network ?</cell><cell>? ? ? ? ? ? ? structural embeddings</cell><cell>+ -</cell></row><row><cell>heterogeneous graph ? extraction context graph</cell><cell>? ? textless nodes</cell><cell>? ? 1 ?</cell><cell>? ?</cell><cell></cell><cell>to ?(? ?? , ? ???? ) backpropagate gradients</cell><cell>? ? ???</cell></row><row><cell></cell><cell cols="3">Text Augmentation Strategy</cell><cell></cell><cell></cell></row><row><cell>? context graph ? ? ?</cell><cell>rich-text nodes ? ? [CLS]</cell><cell></cell><cell>[SEP]</cell><cell>? ? LM input</cell><cell>Language Model</cell><cell>textual representations:? ?? ?</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Performance of different methods on three datasets in two downstream tasks. The best and second-best performances are boldfaced and underlined. *: THLM significantly outperforms the best baseline with p-value &lt; 0.05 0.1893 * 0.1591 * 0.0614 * 0.0722 * 0.0352 * 0.2637 * 0.3409 * 0.3398 * 0.3575 * 0.2637 * 0.3409 * 0.3398 * 0.3575 * 0.1159 * 0.1000 * 0.0286 * 0.0271 * 0.0162 * 0.7769 * 0.8399 * 0.8437 * 0.8496 * 0.7459 * 0.8102 * 0.8134 * 0.8157 *</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Link Prediction</cell><cell>Node Classification</cell><cell></cell></row><row><cell>Datasets</cell><cell>Model</cell><cell>RMSE</cell><cell>MAE</cell><cell>Micro-Precision(@1)</cell><cell>Micro-Recall(@1)</cell></row><row><cell></cell><cell></cell><cell cols="4">HetSANN RGCN R-HGNN HetSANN RGCN R-HGNN MLP HetSANN RGCN R_HGNN MLP HetSANN RGCN R-HGNN</cell></row><row><cell></cell><cell>BERT</cell><cell>0.1987 0.2149 0.1802</cell><cell cols="3">0.0648 0.0886 0.0447 0.2257 0.3146 0.3136 0.3473 0.2257 0.3146 0.3136 0.3473</cell></row><row><cell></cell><cell>RoBERTa</cell><cell>0.1931 0.2152 0.1689</cell><cell cols="3">0.0635 0.0814 0.0400 0.2527 0.3193 0.3341 0.3516 0.2527 0.3193 0.3341 0.3516</cell></row><row><cell></cell><cell>MetaPath</cell><cell>0.2199 0.2415 0.1946</cell><cell cols="3">0.0842 0.0972 0.0544 0.1132 0.2693 0.2851 0.3011 0.1132 0.2693 0.2851 0.3011</cell></row><row><cell>OAG-Veune</cell><cell cols="2">MetaPath+BERT 0.2213 0.2149 0.1651 LinkBERT ? 0.1867 0.2229 0.1739</cell><cell cols="3">0.0981 0.0734 0.0377 0.2307 0.3311 0.3317 0.3472 0.2307 0.3311 0.3317 0.3472 0.0628 0.0892 0.0424 0.2278 0.3108 0.3115 0.3508 0.2278 0.3108 0.3115 0.3508</cell></row><row><cell></cell><cell>GIANT ?</cell><cell>0.2045 0.2022 0.1709</cell><cell cols="3">0.0730 0.0761 0.0408 0.2280 0.3116 0.3074 0.3274 0.2280 0.3116 0.3074 0.3274</cell></row><row><cell></cell><cell>OAG-BERT</cell><cell>0.1918 0.2030 0.1772</cell><cell>0.0634 0.0744 0.0386</cell><cell cols="2">0.2577 0.3214 0.3152 0.3425 0.2577 0.3214 0.3152 0.3425</cell></row><row><cell cols="3">THLM BERT RobERTa MetaPath 0.1857  GoodReads 0.1424 0.1738 0.1103 0.1349 0.1268 0.1044 0.1782 0.1740 0.1520 MetaPath+BERT 0.1314 0.1195 0.1403</cell><cell cols="3">0.0408 0.0586 0.0190 0.7274 0.8238 0.8240 0.8396 0.6984 0.7909 0.7911 0.8061 0.0360 0.0298 0.0189 0.7363 0.8271 0.8314 0.8404 0.7069 0.7941 0.7982 0.8069 0.0639 0.0639 0.0470 0.1492 0.6448 0.6479 0.6883 0.1432 0.6190 0.6220 0.6608 0.0325 0.0280 0.0300 0.7240 0.8258 0.8320 0.8396 0.6951 0.7928 0.7988 0.8061</cell></row><row><cell></cell><cell>LinkBERT ?</cell><cell>0.1471 0.1362 0.1135</cell><cell cols="3">0.0443 0.0396 0.0212 0.7131 0.8209 0.8259 0.8369 0.6846 0.7882 0.7930 0.8035</cell></row><row><cell></cell><cell>GIANT ?</cell><cell>0.1323 0.1179 0.1089</cell><cell cols="3">0.0375 0.0271 0.0191 0.7580 0.8250 0.8300 0.8391 0.7277 0.7921 0.7969 0.8057</cell></row><row><cell cols="3">THLM BERT RoBERTa MetaPath 0.1206  Patents 0.3274 0.3135 0.2764 0.3149 0.2926 0.2585 0.4816 0.4842 0.4842 MetaPath+BERT 0.2922 0.2840 0.2371</cell><cell cols="3">0.1945 0.1829 0.1284 0.6248 0.6603 0.6910 0.6448 0.3791 0.4006 0.4192 0.3912 0.1836 0.1545 0.1119 0.6380 0.6735 0.7022 0.6985 0.3871 0.4087 0.4261 0.4238 0.3372 0.3352 0.3353 0.1996 0.4385 0.4548 0.4654 0.1211 0.2660 0.2759 0.2824 0.1483 0.1440 0.0944 0.6243 0.6583 0.6881 0.6877 0.3788 0.3994 0.4175 0.4173</cell></row><row><cell></cell><cell>LinkBERT ?</cell><cell>0.3080 0.3033 0.2601</cell><cell cols="3">0.1803 0.1738 0.1142 0.6504 0.6749 0.7048 0.7075 0.3946 0.4095 0.4277 0.4293</cell></row><row><cell></cell><cell>GIANT ?</cell><cell>0.2734 0.2454 0.2276</cell><cell cols="3">0.1537 0.1238 0.0976 0.6508 0.6709 0.6992 0.6939 0.3949 0.4071 0.4242 0.4210</cell></row><row><cell></cell><cell>THLM</cell><cell>0.2522</cell><cell></cell><cell></cell><cell></cell></row></table><note><p>* * * 0.2513 0.2190 * 0.1233 * 0.1210 * 0.0848 * 0.7066 * 0.7159 * 0.7324 * 0.7363 * 0.4287 * 0.4344 * 0.4444 * 0.4467 * superior performance compared to RoBERTa. This highlights the importance of incorporating both structural information and textual representations for each node to enhance overall performance.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Evaluation of the ability to learn informative representations via R-HGNN on node classification</figDesc><table><row><cell>Datasets</cell><cell>GCP</cell><cell cols="4">MLP HetSANN RGCN R-HGNN</cell></row><row><cell></cell><cell>w/ MLP</cell><cell>0.2591</cell><cell>0.3195</cell><cell>0.3043</cell><cell>0.3379</cell></row><row><cell></cell><cell>w/ RGCN</cell><cell>0.2728</cell><cell>0.3323</cell><cell>0.3220</cell><cell>0.3547</cell></row><row><cell>OAG-Venue</cell><cell cols="2">w/ 2-order CGP 0.2609</cell><cell>0.3357</cell><cell>0.3121</cell><cell>0.3488</cell></row><row><cell></cell><cell cols="2">w/ random feats 0.2602</cell><cell>0.3271</cell><cell>0.3133</cell><cell>0.3487</cell></row><row><cell></cell><cell>THLM ?</cell><cell>0.2629</cell><cell>0.3383</cell><cell>0.3228</cell><cell>0.3554</cell></row><row><cell></cell><cell>w/ MLP</cell><cell>0.7528</cell><cell>0.8352</cell><cell>0.8376</cell><cell>0.8445</cell></row><row><cell></cell><cell>w/ RGCN</cell><cell>0.7608</cell><cell>0.8380</cell><cell>0.8411</cell><cell>0.8512</cell></row><row><cell>GoodReads</cell><cell cols="2">w/ 2-order CGP 0.7512</cell><cell>0.8319</cell><cell>0.8355</cell><cell>0.8431</cell></row><row><cell></cell><cell cols="2">w/ random feats 0.7523</cell><cell>0.8384</cell><cell>0.8406</cell><cell>0.8483</cell></row><row><cell></cell><cell>THLM ?</cell><cell>0.7549</cell><cell>0.8382</cell><cell>0.8425</cell><cell>0.8485</cell></row><row><cell></cell><cell>w/ MLP</cell><cell>0.6903</cell><cell>0.6963</cell><cell>0.7201</cell><cell>0.7208</cell></row><row><cell></cell><cell>w/ RGCN</cell><cell>0.6911</cell><cell>0.6986</cell><cell>0.7184</cell><cell>0.7218</cell></row><row><cell>Patents</cell><cell cols="2">w/ 2-order CGP 0.6827</cell><cell>0.6876</cell><cell>0.7057</cell><cell>0.7068</cell></row><row><cell></cell><cell cols="2">w/ random feats 0.6908</cell><cell>0.7001</cell><cell>0.7107</cell><cell>0.7198</cell></row><row><cell></cell><cell>THLM ?</cell><cell>0.6948</cell><cell>0.7050</cell><cell>0.7275</cell><cell>0.7280</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Statistics of the datasets.</figDesc><table><row><cell>Datasets</cell><cell>Nodes</cell><cell>Edges</cell><cell>Average Text Length</cell><cell>Category</cell><cell>Classification Split Sets</cell><cell>Link Prediction Split Sets</cell></row><row><cell></cell><cell># Paper (P): 167,004</cell><cell># P-F: 1,709,601</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>OAG-Venue</cell><cell># Author (A): 511,122 # Field (F): 45,775</cell><cell># P-P: 864,019 # A-I: 614,161</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell># Institution (I): 9,090</cell><cell># P-A: 480,104</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">total RMSE as follows,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Performance on node classification in LinkBERT and GIANT.</figDesc><table><row><cell>Datasets</cell><cell>Model</cell><cell>Micro-Precision(@1) MLP HetSANN RGCN R-HGNN</cell></row><row><cell></cell><cell cols="2">LinkBERT(1-order) 0.6790 0.8100 0.8044 0.8302</cell></row><row><cell>GoodReads</cell><cell cols="2">GIANT(1-order) 0.6967 0.8247 0.8284 0.8398</cell></row><row><cell></cell><cell>THLM</cell><cell>0.7769 0.8399 0.8437 0.8496</cell></row><row><cell></cell><cell cols="2">LinkBERT(1-order) 0.5972 0.6421 0.6773 0.6734</cell></row><row><cell>Patents</cell><cell cols="2">GIANT(1-order) 0.4793 0.6234 0.6323 0.6391</cell></row><row><cell></cell><cell>THLM</cell><cell>0.7066 0.7159 0.7324 0.7363</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>The accuracy results for node classification on the obgn-mag dataset.</figDesc><table><row><cell>Model</cell><cell cols="3">MLP HetSANN RGCN</cell></row><row><cell>BERT</cell><cell>0.3754</cell><cell>0.5298</cell><cell>0.5484</cell></row><row><cell cols="2">RoBERTa 0.3770</cell><cell>0.5300</cell><cell>0.5490</cell></row><row><cell cols="2">LinkBERT ? 0.3775</cell><cell>0.5230</cell><cell>0.5491</cell></row><row><cell>GIANT ?</cell><cell>0.3903</cell><cell>0.5184</cell><cell>0.5256</cell></row><row><cell>THLM</cell><cell>0.3933</cell><cell>0.5353</cell><cell>0.5517</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Performance of different methods on three datasets in node classification. The best and second-best performances are boldfaced and underlined.</figDesc><table><row><cell>Datasets</cell><cell>Model</cell><cell>Macro-Precision(@1) ? MLP HetSANN RGCN R-HGNN MLP HetSANN RGCN R-HGNN Macro-Recall(@1) ?</cell></row><row><cell></cell><cell>BERT</cell><cell>0.2110 0.3104 0.3119 0.3359 0.1992 0.3118 0.3060 0.3415</cell></row><row><cell></cell><cell>RoBERTa</cell><cell>0.2429 0.3264 0.3258 0.3598 0.2387 0.3187 0.3169 0.3412</cell></row><row><cell></cell><cell>MetaPath</cell><cell>0.0959 0.2593 0.2830 0.3005 0.0717 0.2731 0.2663 0.3019</cell></row><row><cell>OAG-Venue</cell><cell cols="2">MetaPath+BERT 0.2094 0.3202 0.3248 0.3363 0.1991 0.3150 0.3180 0.3368</cell></row><row><cell></cell><cell>LinkBERT ?</cell><cell>0.2054 0.2921 0.3014 0.3479 0.2060 0.3057 0.2902 0.3233</cell></row><row><cell></cell><cell>GIANT ?</cell><cell>0.2026 0.3078 0.3080 0.3381 0.2005 0.3097 0.2858 0.3188</cell></row><row><cell></cell><cell>THLM</cell><cell>0.2506 0.3375 0.3408 0.3562 0.2464 0.3330 0.3331 0.3537</cell></row><row><cell></cell><cell>BERT</cell><cell>0.7352 0.8273 0.8253 0.8421 0.7040 0.7960 0.7969 0.8112</cell></row><row><cell></cell><cell>RoBERTa</cell><cell>0.7420 0.8290 0.8328 0.8428 0.7134 0.7994 0.8039 0.8120</cell></row><row><cell></cell><cell>MetaPath</cell><cell>0.1786 0.6599 0.6560 0.6966 0.1371 0.6204 0.6225 0.6624</cell></row><row><cell>GoodReads</cell><cell cols="2">MetaPath+BERT 0.7285 0.8286 0.8356 0.8425 0.7015 0.7978 0.8026 0.8104</cell></row><row><cell></cell><cell>LinkBERT ?</cell><cell>0.7178 0.8239 0.8276 0.8389 0.6917 0.7932 0.7987 0.8091</cell></row><row><cell></cell><cell>GIANT ?</cell><cell>0.7622 0.8273 0.8329 0.8418 0.7331 0.7970 0.8018 0.8109</cell></row><row><cell></cell><cell>THLM</cell><cell>0.7798 0.8472 0.8493 0.8515 0.7516 0.8148 0.8184 0.8209</cell></row><row><cell></cell><cell>BERT</cell><cell>0.3526 0.3876 0.4073 0.2994 0.1587 0.1864 0.1795 0.1335</cell></row><row><cell></cell><cell>RoBERTa</cell><cell>0.3262 0.3918 0.4185 0.4227 0.1506 0.1941 0.1801 0.1846</cell></row><row><cell></cell><cell>MetaPath</cell><cell>0.0854 0.1894 0.1862 0.2059 0.0153 0.0941 0.0930 0.0946</cell></row><row><cell>Patents</cell><cell cols="2">MetaPath+BERT 0.3330 0.3827 0.4072 0.4263 0.1577 0.1866 0.1842 0.1929</cell></row><row><cell></cell><cell>LinkBERT ?</cell><cell>0.3458 0.3838 0.4182 0.4515 0.1649 0.1858 0.1884 0.1920</cell></row><row><cell></cell><cell>GIANT ?</cell><cell>0.3506 0.3904 0.4194 0.4327 0.1764 0.1995 0.1928 0.1944</cell></row><row><cell></cell><cell>THLM</cell><cell>0.4374 0.4364 0.4466 0.4974 0.2090 0.2128 0.2115 0.2281</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Performance of different methods on GoodReads in node classification. The best and second-best performances are boldfaced and underlined.</figDesc><table><row><cell>Metric</cell><cell>Model</cell><cell>K=3 MLP HetSANN RGCN R-HGNN MLP HetSANN RGCN R-HGNN K=5</cell></row><row><cell></cell><cell>BERT</cell><cell>0.3402 0.3645 0.3520 0.3637 0.2146 0.2193 0.2095 0.2136</cell></row><row><cell></cell><cell>RoBERTa</cell><cell>0.3418 0.3602 0.3552 0.3707 0.2145 0.2174 0.2104 0.2166</cell></row><row><cell></cell><cell>MetaPath</cell><cell>0.1463 0.3374 0.3283 0.3417 0.1415 0.2187 0.2107 0.2142</cell></row><row><cell>Macro-Precision</cell><cell cols="2">MetaPath+BERT 0.3377 0.3676 0.3605 0.3749 0.2138 0.2202 0.2137 0.2182</cell></row><row><cell></cell><cell>LinkBERT ?</cell><cell>0.3350 0.3688 0.3543 0.3647 0.2118 0.2209 0.2114 0.2133</cell></row><row><cell></cell><cell>GIANT ?</cell><cell>0.3526 0.3671 0.3609 0.3702 0.2186 0.2187 0.2146 0.2189</cell></row><row><cell></cell><cell>THLM</cell><cell>0.3458 0.3753 0.3647 0.3717 0.2139 0.2232 0.2125 0.2136</cell></row><row><cell></cell><cell>BERT</cell><cell>0.9368 0.9766 0.9755 0.9804 0.9814 0.9941 0.9935 0.9947</cell></row><row><cell></cell><cell>RoBERTa</cell><cell>0.9431 0.9791 0.9792 0.9819 0.9851 0.9948 0.9945 0.9952</cell></row><row><cell></cell><cell>MetaPath</cell><cell>0.3950 0.8608 0.8585 0.8863 0.6461 0.9445 0.9395 0.9554</cell></row><row><cell>Macro-Recall</cell><cell cols="2">MetaPath+BERT 0.9357 0.9762 0.9788 0.9803 0.9806 0.9939 0.9949 0.9950</cell></row><row><cell></cell><cell>LinkBERT ?</cell><cell>0.9283 0.9756 0.9760 0.9785 0.9768 0.9938 0.9935 0.9942</cell></row><row><cell></cell><cell>GIANT ?</cell><cell>0.9502 0.9766 0.9775 0.9803 0.9862 0.9947 0.9945 0.9951</cell></row><row><cell></cell><cell>THLM</cell><cell>0.9615 0.9829 0.9846 0.9836 0.9899 0.9961 0.9959 0.9957</cell></row><row><cell></cell><cell>BERT</cell><cell>0.3252 0.3391 0.3386 0.3403 0.2046 0.2071 0.2070 0.2072</cell></row><row><cell></cell><cell>RoBERTa</cell><cell>0.3274 0.3399 0.3399 0.3409 0.2053 0.2072 0.2072 0.2073</cell></row><row><cell></cell><cell>MetaPath</cell><cell>0.1438 0.2999 0.2991 0.3086 0.1410 0.1976 0.1964 0.1995</cell></row><row><cell>Micro-Precision</cell><cell cols="2">MetaPath+BERT 0.3249 0.3389 0.3399 0.3404 0.2044 0.2071 0.2073 0.2073</cell></row><row><cell></cell><cell>LinkBERT ?</cell><cell>0.3222 0.3388 0.3388 0.3397 0.2037 0.2071 0.2070 0.2071</cell></row><row><cell></cell><cell>GIANT ?</cell><cell>0.3299 0.3390 0.3393 0.3404 0.2056 0.2072 0.2072 0.2073</cell></row><row><cell></cell><cell>THLM</cell><cell>0.3338 0.3413 0.3418 0.3414 0.2063 0.2075 0.2075 0.2074</cell></row><row><cell></cell><cell>BERT</cell><cell>0.9368 0.9767 0.9753 0.9802 0.9821 0.9942 0.9936 0.9947</cell></row><row><cell></cell><cell>RoBERTa</cell><cell>0.9430 0.9790 0.9791 0.9820 0.9854 0.9948 0.9945 0.9954</cell></row><row><cell></cell><cell>MetaPath</cell><cell>0.4142 0.8638 0.8614 0.8888 0.6767 0.9484 0.9427 0.9578</cell></row><row><cell>Micro-Recall</cell><cell cols="2">MetaPath+BERT 0.9357 0.9762 0.9791 0.9805 0.9813 0.9941 0.9952 0.9952</cell></row><row><cell></cell><cell>LinkBERT ?</cell><cell>0.9281 0.9758 0.9758 0.9785 0.9777 0.9941 0.9936 0.9943</cell></row><row><cell></cell><cell>GIANT ?</cell><cell>0.9502 0.9764 0.9774 0.9804 0.9868 0.9948 0.9946 0.9953</cell></row><row><cell></cell><cell>THLM</cell><cell>0.9613 0.9831 0.9846 0.9835 0.9902 0.9962 0.9960 0.9957</cell></row><row><cell></cell><cell>BERT</cell><cell>0.8526 0.9164 0.9158 0.9252 0.8713 0.9236 0.9233 0.9312</cell></row><row><cell></cell><cell>RoBERTa</cell><cell>0.8600 0.9192 0.9209 0.9266 0.8776 0.9257 0.9273 0.9321</cell></row><row><cell></cell><cell>MetaPath</cell><cell>0.3008 0.7740 0.7735 0.8070 0.4089 0.8088 0.8072 0.8354</cell></row><row><cell>NDCG</cell><cell cols="2">MetaPath+BERT 0.8507 0.9170 0.9214 0.9253 0.8695 0.9244 0.9280 0.9314</cell></row><row><cell></cell><cell>LinkBERT ?</cell><cell>0.8413 0.9149 0.9168 0.9231 0.8617 0.9224 0.9241 0.9295</cell></row><row><cell></cell><cell>GIANT ?</cell><cell>0.8732 0.9168 0.9195 0.9252 0.8883 0.9243 0.9266 0.9313</cell></row><row><cell></cell><cell>THLM</cell><cell>0.8879 0.9288 0.9310 0.9314 0.8998 0.9342 0.9357 0.9364</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>Performance of different methods on Patents in node classification. The best and second-best performances are boldfaced and underlined.</figDesc><table><row><cell>Metric</cell><cell>Model</cell><cell>K=3 MLP HetSANN RGCN R-HGNN MLP HetSANN RGCN R-HGNN K=5</cell></row><row><cell></cell><cell>BERT</cell><cell>0.2012 0.2502 0.2634 0.2365 0.1425 0.1800 0.1883 0.1866</cell></row><row><cell></cell><cell>RoBERTa</cell><cell>0.2010 0.2421 0.2648 0.2886 0.1414 0.1725 0.1836 0.2136</cell></row><row><cell></cell><cell>MetaPath</cell><cell>0.0655 0.1321 0.1389 0.1579 0.0523 0.1062 0.1102 0.1234</cell></row><row><cell>Macro-Precision</cell><cell cols="2">MetaPath+BERT 0.2041 0.2541 0.2626 0.2914 0.1418 0.1818 0.1860 0.2098</cell></row><row><cell></cell><cell>LinkBERT ?</cell><cell>0.2144 0.2443 0.2715 0.2933 0.1490 0.1758 0.1877 0.2194</cell></row><row><cell></cell><cell>GIANT ?</cell><cell>0.2181 0.2459 0.2692 0.2854 0.1518 0.1799 0.1882 0.2137</cell></row><row><cell></cell><cell>THLM</cell><cell>0.2541 0.2671 0.2827 0.3133 0.1761 0.1864 0.1950 0.2300</cell></row><row><cell></cell><cell>BERT</cell><cell>0.3036 0.3553 0.3592 0.2765 0.3824 0.4326 0.4493 0.3619</cell></row><row><cell></cell><cell>RoBERTa</cell><cell>0.3017 0.3598 0.3603 0.3618 0.3827 0.4430 0.4560 0.4526</cell></row><row><cell></cell><cell>MetaPath</cell><cell>0.0335 0.1889 0.1949 0.1884 0.0484 0.2446 0.2543 0.2465</cell></row><row><cell>Macro-Recall</cell><cell cols="2">MetaPath+BERT 0.3027 0.3603 0.3610 0.3682 0.3810 0.4383 0.4522 0.4527</cell></row><row><cell></cell><cell>LinkBERT ?</cell><cell>0.3186 0.3597 0.3714 0.3699 0.4038 0.4483 0.4631 0.4568</cell></row><row><cell></cell><cell>GIANT ?</cell><cell>0.3344 0.3591 0.3713 0.3654 0.4131 0.4324 0.4616 0.4511</cell></row><row><cell></cell><cell>THLM</cell><cell>0.3933 0.4023 0.4067 0.4111 0.4890 0.4886 0.5038 0.4976</cell></row><row><cell></cell><cell>BERT</cell><cell>0.3502 0.3636 0.3785 0.3599 0.2426 0.2502 0.2590 0.2488</cell></row><row><cell></cell><cell>RoBERTa</cell><cell>0.3566 0.3694 0.3845 0.3826 0.2472 0.2541 0.2626 0.2615</cell></row><row><cell></cell><cell>MetaPath</cell><cell>0.1286 0.2580 0.2695 0.2729 0.0971 0.1874 0.1941 0.1962</cell></row><row><cell>Micro-Precision</cell><cell cols="2">MetaPath+BERT 0.3498 0.3646 0.3773 0.3775 0.2428 0.2507 0.2582 0.2584</cell></row><row><cell></cell><cell>LinkBERT ?</cell><cell>0.3609 0.3699 0.3841 0.3851 0.2494 0.2542 0.2625 0.2627</cell></row><row><cell></cell><cell>GIANT ?</cell><cell>0.3596 0.3656 0.3804 0.3775 0.2484 0.2505 0.2600 0.2583</cell></row><row><cell></cell><cell>THLM</cell><cell>0.3843 0.3863 0.3951 0.3959 0.2626 0.2627 0.2684 0.2686</cell></row><row><cell></cell><cell>BERT</cell><cell>0.6375 0.6618 0.6890 0.6552 0.7360 0.7591 0.7856 0.7548</cell></row><row><cell></cell><cell>RoBERTa</cell><cell>0.6491 0.6724 0.6998 0.6964 0.7499 0.7709 0.7968 0.7933</cell></row><row><cell></cell><cell>MetaPath</cell><cell>0.2341 0.4697 0.4905 0.4967 0.2945 0.5684 0.5888 0.5951</cell></row><row><cell>Micro-Recall</cell><cell cols="2">MetaPath+BERT 0.6367 0.6636 0.6868 0.6871 0.7367 0.7606 0.7834 0.7838</cell></row><row><cell></cell><cell>LinkBERT ?</cell><cell>0.6570 0.6734 0.6992 0.7010 0.7567 0.7711 0.7963 0.7970</cell></row><row><cell></cell><cell>GIANT ?</cell><cell>0.6546 0.6655 0.6923 0.6871 0.7537 0.7598 0.7888 0.7835</cell></row><row><cell></cell><cell>THLM</cell><cell>0.6996 0.7032 0.7192 0.7207 0.7967 0.7970 0.8144 0.8148</cell></row><row><cell></cell><cell>BERT</cell><cell>0.6725 0.7025 0.7297 0.6921 0.7066 0.7353 0.7610 0.7262</cell></row><row><cell></cell><cell>RoBERTa</cell><cell>0.6854 0.7140 0.7417 0.7387 0.7200 0.7470 0.7726 0.7697</cell></row><row><cell></cell><cell>MetaPath</cell><cell>0.2467 0.4902 0.5103 0.5188 0.2734 0.5296 0.5487 0.5573</cell></row><row><cell>NDCG</cell><cell cols="2">MetaPath+BERT 0.6717 0.7024 0.7272 0.7280 0.7066 0.7351 0.7586 0.7594</cell></row><row><cell></cell><cell>LinkBERT ?</cell><cell>0.6953 0.7154 0.7413 0.7444 0.7291 0.7478 0.7725 0.7748</cell></row><row><cell></cell><cell>GIANT ?</cell><cell>0.6936 0.7084 0.7354 0.7305 0.7275 0.7397 0.7665 0.7617</cell></row><row><cell></cell><cell>THLM</cell><cell>0.7442 0.7488 0.7652 0.7675 0.7752 0.7785 0.7950 0.7963</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Note that the initialization is executed only once by using the official checkpoint of BERT(Devlin et al.,  </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>2019).2 Among the neighbors in Nu, we select rich-text nodes in priority. Moreover, if the size of Nu is smaller or equal to k, we will choose all the neighbors.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://www.uspto.gov/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>https://github.com/UCLA-DM/pyHGT</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>https://sites.google.com/eng.ucsd.edu/ ucsdbookgraph/home</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5"><p>https://www.uspto.gov/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="8">Acknowledgements</head><p>This work was supported by the <rs type="funder">National Natural Science Foundation of China</rs> (<rs type="grantNumber">51991395</rs>, <rs type="grantNumber">62272023</rs>), and the <rs type="funder">Fundamental Research Funds for the Central Universities</rs> (No. <rs type="grantNumber">YWF-23-L-717</rs>, No. <rs type="grantNumber">YWF-23-L-1203</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_kZRWqbA">
					<idno type="grant-number">51991395</idno>
				</org>
				<org type="funding" xml:id="_CWbQFBa">
					<idno type="grant-number">62272023</idno>
				</org>
				<org type="funding" xml:id="_DxxfrfZ">
					<idno type="grant-number">YWF-23-L-717</idno>
				</org>
				<org type="funding" xml:id="_bBKPUkE">
					<idno type="grant-number">YWF-23-L-1203</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>A.1 Datasets and Baselines Datasets. Specific statistics of datasets are shown in Table <ref type="table">4</ref> and detailed descriptions of datasets are shown as follows.</p><p>? OAG-Venue: OAG-Venue 5 is a heterogeneous graph followed by <ref type="bibr">Hu et al. (2020b)</ref>, which includes papers (P), authors (A), fields (F) and institutions (I). Each paper is published in a single venue. We treat papers as rich-text nodes and extract the title and abstract parts as their text descriptions. Authors, fields, and institutions are regarded as textless nodes, whose text descriptions are composed of their definitions or names.</p><p>? GoodReads: Following <ref type="bibr" target="#b33">(Wan and McAuley, 2018;</ref><ref type="bibr" target="#b34">Wan et al., 2019)</ref>, we receive a subset of GoodReads 6 , which contains books (B), authors (A) and publishers (P). Each book is categorized into one or more genres. We treat books as rich-text nodes and extract brief introductions as their text descriptions. Authors and publishers are regarded as textless nodes, whose text descriptions are their names.</p><p>? Patents: Patents is a heterogeneous graph collected from the USPTO 7 , which contains patent documents (P), applicants (A) and applied companies (C). Each patent is assigned several International Patent Classification (IPC) codes. We treat patents as rich-text nodes and extract the title and abstract parts as their text descriptions. Applicants and companies use their names as text descriptions, regarded as textless nodes.</p><p>Baselines. We compare our model with the following baselines: BERT <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref> and RoBERTa <ref type="bibr" target="#b23">(Liu et al., 2019)</ref> are popular encoderonly pretraining language models. MetaPath <ref type="bibr" target="#b6">(Dong et al., 2017)</ref> leverages meta-path-based random walks in the heterogeneous graph to generate node embeddings. MetaPath+BERT combines the textual embeddings embedded from BERT base and structural representations learned from MetaPath as node features. LinkBERT <ref type="bibr" target="#b39">(Yasunaga et al., 2022)</ref> captures the dependencies across documents by</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Heterogeneous graph neural networks analysis: a survey of techniques, evaluations and applications</title>
		<author>
			<persName><forename type="first">Rui</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fanrong</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huifang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaojie</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="page" from="1" to="40" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">COMET: commonsense transformers for automatic knowledge graph construction</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaitanya</forename><surname>Malaviya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2019</title>
		<meeting><address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-07-28">2019. July 28-August 2, 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4762" to="4779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">B</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><surname>Amodei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020-12-06">2020. 2020. December 6-12, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Node feature extraction by selfsupervised multi-scale neighborhood prediction</title>
		<author>
			<persName><forename type="first">Eli</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Cheng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiang-Fu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olgica</forename><surname>Milenkovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inderjit</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2022, Virtual Event</title>
		<imprint>
			<date type="published" when="2022-04-25">2022. April 25-29, 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A survey on network embedding</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="833" to="852" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT 2019</title>
		<title level="s">Long and Short Papers</title>
		<meeting><address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-02">2019. June 2-7, 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">GLM: general language model pretraining with autoregressive blank infilling</title>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitesh</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Swami ; Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2022</title>
		<meeting><address><addrLine>Halifax, NS, Canada; Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-08-13">2017. August 13 -17, 2017. 2022. May 22-27, 2022</date>
			<biblScope unit="page" from="320" to="335" />
		</imprint>
	</monogr>
	<note>KDD</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">MAGNN: metapath aggregated graph neural network for heterogeneous graph embedding</title>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiani</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziqiao</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
		<ptr target="ACM/IW3C2" />
	</analytic>
	<monogr>
		<title level="m">WWW &apos;20: The Web Conference</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2331" to="2341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<meeting><address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016-08-13">2016. August 13-17, 2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2017</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09">2017. December 4-9, 2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Lightgcn: Simplifying and powering graph convolution network for recommendation</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuan</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong-Dong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="639" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Tapas: Weakly supervised table parsing via pre-training</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krzysztof</forename><surname>Pawel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Piccinno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2020</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">Martin Eisenschlos. 2020</date>
			<biblScope unit="page" from="4320" to="4333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An attentionbased graph neural network for heterogeneous structural learning</title>
		<author>
			<persName><forename type="first">Huiting</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hantao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yucheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jieping</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4132" to="4139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06">2020. 2020. 2020. December 6-12, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Heterogeneous graph transformer</title>
		<author>
			<persName><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<ptr target="ACM/IW3C2" />
	</analytic>
	<monogr>
		<title level="m">WWW &apos;20</title>
		<meeting><address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-20">2020. April 20-24, 2020</date>
			<biblScope unit="page" from="2704" to="2710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">2023a. Edgeformers: Graph-empowered transformers for representation learning on textual-edge networks</title>
		<author>
			<persName><forename type="first">Jin</forename><surname>Bowen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno>CoRR, abs/2302.11050</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Heterformer: A transformer architecture for node representation learning on heterogeneous text-rich networks</title>
		<author>
			<persName><forename type="first">Jin</forename><surname>Bowen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno>CoRR, abs/2205.10282</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Heterformer: Transformer-based deep node representation learning on heterogeneous text-rich networks</title>
		<author>
			<persName><forename type="first">Jin</forename><surname>Bowen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD 2023</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2023-08-06">2023. August 6-10, 2023</date>
			<biblScope unit="page" from="1020" to="1031" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semisupervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2017</title>
		<title level="s">Conference Track Proceedings. OpenReview</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24">2017. April 24-26, 2017</date>
		</imprint>
	</monogr>
	<note>net</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">ALBERT: A lite BERT for self-supervised learning of language representations</title>
		<author>
			<persName><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2020</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26">2020. April 26-30, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The inductive bias of in-context learning: Rethinking pretraining example design</title>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Wies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Jannai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Navon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yedid</forename><surname>Hoshen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amnon</forename><surname>Shashua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2022, Virtual Event</title>
		<imprint>
			<publisher>OpenReview</publisher>
			<date type="published" when="2022-04-25">2022. April 25-29, 2022</date>
		</imprint>
	</monogr>
	<note>net</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adsgnn: Behavior-graph augmented relevance modeling in sponsored search</title>
		<author>
			<persName><forename type="first">Chaozhuo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bochen</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanling</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR &apos;21, Virtual Event</title>
		<meeting><address><addrLine>Canada</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021-07-11">2021. July 11-15, 2021</date>
			<biblScope unit="page" from="223" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">OAG-BERT: towards a unified backbone language model for academic knowledge services</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingnan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingjian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;22</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2022-08-14">2022. August 14 -18, 2022</date>
			<biblScope unit="page" from="3418" to="3428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Roberta: A robustly optimized BERT pretraining approach</title>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>CoRR, abs/1907.11692</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2019</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06">2019. May 6-9, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Are we really making much progress?: Revisiting, benchmarking and refining heterogeneous graph neural networks</title>
		<author>
			<persName><forename type="first">Qingsong</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenzheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianguo</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;21, Virtual Event</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021-08-14">2021. August 14-18, 2021</date>
			<biblScope unit="page" from="1150" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tom?s</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2013</title>
		<meeting><address><addrLine>Lake Tahoe, Nevada, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improving relevance modeling via heterogeneous behavior graph learning in bing ads</title>
		<author>
			<persName><forename type="first">Bochen</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaozhuo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxun</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiwei</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;22</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2022-08-14">2022. August 14 -18, 2022</date>
			<biblScope unit="page" from="3713" to="3721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Sejr</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ESWC 2018</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">10843</biblScope>
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Discovering hypernymy in text-rich heterogeneous information network by exploiting context granularity</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naijing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinwei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengzhi</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myunghwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM 2019</title>
		<meeting><address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019-11-03">2019. November 3-7, 2019</date>
			<biblScope unit="page" from="599" to="608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2017</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09">2017. December 4-9, 2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Item recommendation on monotonic behavior chains</title>
		<author>
			<persName><forename type="first">Mengting</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><forename type="middle">J</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM Conference on Recommender Systems, RecSys 2018</title>
		<meeting>the 12th ACM Conference on Recommender Systems, RecSys 2018<address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018-10-02">2018. October 2-7, 2018</date>
			<biblScope unit="page" from="86" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fine-grained spoiler detection from large-scale review corpora</title>
		<author>
			<persName><forename type="first">Mengting</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishabh</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ndapa</forename><surname>Nakashole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><forename type="middle">J</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2019</title>
		<meeting><address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-07-28">2019. July 28-August 2, 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2605" to="2610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Neural graph collaborative filtering</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="165" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Should you mask 15% in masked language modeling?</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Wettig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zexuan</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL 2023</title>
		<meeting><address><addrLine>Dubrovnik, Croatia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023-05-02">2023. May 2-6, 2023</date>
			<biblScope unit="page" from="2977" to="2992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Graphformers: Gnn-nested transformers for representation learning on textual graph</title>
		<author>
			<persName><forename type="first">Junhan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shitao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaozhuo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Defu</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangzhong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS 2021</title>
		<imprint>
			<date type="published" when="2021-12-06">2021. December 6-14, 2021</date>
			<biblScope unit="page" from="28798" to="28810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="5754" to="5764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Linkbert: Pretraining language models with document links</title>
		<author>
			<persName><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2022</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-05-22">2022. May 22-27, 2022</date>
			<biblScope unit="page" from="8003" to="8016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Hybrid micro/macro level convolution for heterogeneous graph learning</title>
		<author>
			<persName><forename type="first">Le</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leilei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanren</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weifeng</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Xiong</surname></persName>
		</author>
		<idno>CoRR, abs/2012.14722</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Heterogeneous graph representation learning with relation awareness</title>
		<author>
			<persName><forename type="first">Le</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leilei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanren</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weifeng</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Heterogeneous graph neural network</title>
		<author>
			<persName><forename type="first">Chuxu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongjin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitesh</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;19</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="793" to="803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Textgnn: Improving text encoder via graph neural network in sponsored search</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanling</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xue</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Pelger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruofei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huasha</forename><surname>Zhao</surname></persName>
		</author>
		<ptr target="ACM/IW3C2" />
	</analytic>
	<monogr>
		<title level="m">WWW &apos;21</title>
		<meeting><address><addrLine>Ljubljana, Slovenia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-04-19">2021. April 19-23, 2021</date>
			<biblScope unit="page" from="2848" to="2857" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
