<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Layered Image Motion with Explicit Occlusions, Temporal Consistency, and Depth Ordering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Deqing</forename><surname>Sun</surname></persName>
							<email>dqsun@cs.brown.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Brown University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Erik</forename><forename type="middle">B</forename><surname>Sudderth</surname></persName>
							<email>sudderth@cs.brown.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Brown University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
							<email>black@cs.brown.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Brown University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Layered Image Motion with Explicit Occlusions, Temporal Consistency, and Depth Ordering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D3B6A0F790F4E5AD430E722018FFF72A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T02:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Layered models are a powerful way of describing natural scenes containing smooth surfaces that may overlap and occlude each other. For image motion estimation, such models have a long history but have not achieved the wide use or accuracy of non-layered methods. We present a new probabilistic model of optical flow in layers that addresses many of the shortcomings of previous approaches. In particular, we define a probabilistic graphical model that explicitly captures: 1) occlusions and disocclusions; 2) depth ordering of the layers; 3) temporal consistency of the layer segmentation. Additionally the optical flow in each layer is modeled by a combination of a parametric model and a smooth deviation based on an MRF with a robust spatial prior; the resulting model allows roughness in layers. Finally, a key contribution is the formulation of the layers using an imagedependent hidden field prior based on recent models for static scene segmentation. The method achieves state-of-the-art results on the Middlebury benchmark and produces meaningful scene segmentations as well as detected occlusion regions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Layered models of scenes offer significant benefits for optical flow estimation <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b24">25]</ref>. Splitting the scene into layers enables the motion in each layer to be defined more simply, and the estimation of motion boundaries to be separated from the problem of smooth flow estimation. Layered models also make reasoning about occlusion relationships easier. In practice, however, none of the current top performing optical flow methods use a layered approach <ref type="bibr" target="#b1">[2]</ref>. The most accurate approaches are single-layered, and instead use some form of robust smoothness assumption to cope with flow discontinuities <ref type="bibr" target="#b4">[5]</ref>. This paper formulates a new probabilistic, layered motion model that addresses the key problems of previous layered approaches. At the time of writing, it achieves the lowest average error of all tested approaches on the Middlebury optical flow benchmark <ref type="bibr" target="#b1">[2]</ref>. In particular, the accuracy at occlusion boundaries is significantly better than previous methods. By segmenting the observed scene, our model also identifies occluded and disoccluded regions.</p><p>Layered models provide a segmentation of the scene and this segmentation, because it corresponds to scene structure, should persist over time. However, this persistence is not a benefit if one is only computing flow between two frames; this is one reason that multi-layer models have not surpassed their single-layer competitors on two-frame benchmarks. Without loss of generality, here we use three-frame sequences to illustrate our method. In practice, these three frames can be constructed from an image pair by computing both the forward and backward flow. The key is that this gives two segmentations of the scene, one at each time instant, both of which must be consistent with the flow. We formulate this temporal layer consistency probabilistically. Note that the assumption of temporal layer consistency is much more realistic than previous assumptions of temporal motion consistency <ref type="bibr" target="#b3">[4]</ref>; while the scene motion can change rapidly, scene structure persists.</p><p>One of the main motivations for layered models is that, conditioned on the segmentation into layers, each layer can employ affine, planar, or other strong models of optical flow. By applying a single smooth motion across the entire layer, these models combine information over long distances and interpolate behind occlusions. Such rigid parametric assumptions, however, are too restrictive for real scenes. Instead one can model the flow within each layer as smoothly varying <ref type="bibr" target="#b25">[26]</ref>. While the resulting model is more flexible than traditional parametric models, we find that it is still not as accurate as robust single-layer models. Consequently, we formulate a hybrid model that combines a base affine motion with a robust Markov random field (MRF) model of deformations from affine <ref type="bibr" target="#b5">[6]</ref>. This roughness in layers model, which is similar in spirit to work on plane+parallax <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">19]</ref>, encourages smooth flow within layers but allows significant local deviations.</p><p>Because layers are temporally persistent, it is also possible to reason about their relative depth ordering. In general, reliable recovery of depth order requires three or more frames. Our probabilistic formulation explicitly orders layers by depth, and we show that the correct order typically produces more probable (lower energy) solutions. This also allows explicit reasoning about occlusions, which our model predicts at locations where the layer segmentations for consecutive frames disagree.</p><p>Many previous layered approaches are not truly "layered": while they segment the image into multiple regions with distinct motions, they do not model what is in front of what. For example, widely used MRF models <ref type="bibr" target="#b26">[27]</ref> encourage neighboring pixels to occupy the same region, but do not capture relationships between regions. In contrast, building on recent state-of-the-art results in static scene segmentation <ref type="bibr" target="#b20">[21]</ref>, our model determines layer support via an ordered sequence of occluding binary masks. These binary masks are generated by thresholding a series of random, continuous functions. This approach uses image-dependent Gaussian random field priors and favors partitions which accurately match the statistics of real scenes <ref type="bibr" target="#b20">[21]</ref>. Moreover, the continuous layer support functions play a key role in accurately modeling temporal layer consistency. The resulting model produces accurate layer segmentations that improve flow accuracy at occlusion boundaries, and recover meaningful scene structure.</p><p>As summarized in Figure <ref type="figure" target="#fig_0">1</ref>, our method is based on a principled, probabilistic generative model for image sequences. By combining recent advances in dense flow estimation and natural image segmentation, we develop an algorithm that simultaneously estimates accurate flow fields, detects occlusions and disocclusions, and recovers the layered structure of realistic scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Previous Work</head><p>Layered approaches to motion estimation have long been seen as elegant and promising, since spatial smoothness is separated from the modeling of discontinuities and occlusions. Darrell and Pentland <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> provide the first full approach that incorporates a Bayesian model, "support maps" for segmentation, and robust statistics. Wang and Adelson <ref type="bibr" target="#b24">[25]</ref> clearly motivate layered models of image sequences, while Jepson and Black <ref type="bibr" target="#b10">[11]</ref> formalize the problem using probabilistic mixture models. A full review of more recent methods is beyond our scope <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29]</ref>.</p><p>Early methods, which use simple parametric models of image motion within layers, are not highly accurate. Observing that rigid parametric models are too restrictive for real scenes, Weiss <ref type="bibr" target="#b25">[26]</ref> uses a more flexible Gaussian process to describe the motion within each layer. Even using modern implementation methods <ref type="bibr" target="#b21">[22]</ref> this approach does not achieve state-of-the-art results. Allocating a separate layer for every small surface discontinuity is impractical and fails to capture important global scene structure. Our approach, which allows "roughness" within layers rather than "smoothness," provides a compromise that captures coarse scene structure as well as fine within-layer details.</p><p>One key advantage of layered models is their ability to realistically model occlusion boundaries. To do this properly, however, one must know the relative depth order of the surfaces. Performing inference over the combinatorial range of possible occlusion relationships is challenging and, consequently, only a few layered flow models explicitly encode relative depth <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b29">30]</ref>. Recent work revisits the layered model to handle occlusions <ref type="bibr" target="#b8">[9]</ref>, but does not explicitly model the layer ordering or achieve state-of-the-art performance on the Middlebury benchmark. While most current optical flow methods are "two-frame," layered methods naturally extend to longer sequences <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref>.</p><p>Layered models all have some way of making either a hard or soft assignment of pixels to layers. Weiss and Adelson <ref type="bibr" target="#b26">[27]</ref> introduce spatial coherence to these layer assignments using a spatial MRF model. However, the Ising/Potts MRF they employ assigns low probability to typical segmentations of natural scenes <ref type="bibr" target="#b14">[15]</ref>. Adapting recent work on static image segmentation by Sudderth and Jordan <ref type="bibr" target="#b20">[21]</ref>, we instead generate spatially coherent, ordered layers by thresholding a series of random continuous functions. As in the single-image case, this approach realistically models the size and shape properties of real scenes. For motion estimation there are additional advantages: it allows accurate reasoning about occlusion relationships and modeling of temporal layer consistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">A Layered Motion Model</head><p>Building on this long sequence of prior work, our generative model of layered image motion is summarized in Figure <ref type="figure" target="#fig_0">1</ref>. Below we describe how the generative model captures piecewise smooth deviation of the layer motion from parametric models (Sec. 3.1), depth ordering and temporal consistency of layers (Sec. 3.2), and regions of occlusion and disocclusion (Sec. 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Roughness in Layers</head><p>Our approach is inspired by Weiss's model of smoothness in layers <ref type="bibr" target="#b25">[26]</ref>. Given a sequence of images I t , 1 ≤ t ≤ T , we model the evolution from the current frame I t , to the subsequent frame I t+1 , via K locally smooth, but potentially globally complex, flow fields. Let u tk and v tk denote the horizontal and vertical flow fields, respectively, for layer k at time t. The corresponding flow vector for pixel (i, j) is then denoted by (u ij tk , v ij tk ). Each layer's flow field is drawn from a distribution chosen to encourage piecewise smooth motion. For example, a pairwise Markov random field (MRF) would model the horizontal flow field as</p><formula xml:id="formula_0">p(u tk ) ∝ exp{-E mrf (u tk )} = exp - 1 2 (i,j) (i ′ ,j ′ )∈Γ(i,j) ρ s (u ij tk -u i ′ j ′ tk ) .<label>(1)</label></formula><p>Here, Γ(i, j) is the set of neighbors of pixel (i, j), often its four nearest neighbors. The potential ρ s (•) is some robust function <ref type="bibr" target="#b4">[5]</ref> that encourages smoothness, but allows occasional significant deviations from it. The vertical flow field v tk can then be modeled via an independent MRF prior as in Eq. ( <ref type="formula" target="#formula_0">1</ref>), as justified by the statistics of natural flow fields <ref type="bibr" target="#b17">[18]</ref>.</p><p>While such MRF priors are flexible, they capture very little dependence between pixels separated by even moderate image distances. In contrast, real scenes exhibit coherent motion over large scales, due to the motion of (partially) rigid objects in the world. To capture this, we associate an affine (or planar) motion model, with parameters θ tk , to each layer k. We then use an MRF to allow piecewise smooth deformations from the globally rigid assumptions of affine motion:</p><formula xml:id="formula_1">E aff (u tk , θ tk ) = 1 2 (i,j) (i ′ ,j ′ )∈Γ(i,j) ρ s (u ij tk -ūij θ tk ) -(u i ′ j ′ tk -ūi ′ j ′ θ tk ) .<label>(2)</label></formula><p>Here, ūij θ tk denotes the horizontal motion predicted for pixel (i, j) by an affine model with parameters θ tk . Unlike classical models that assume layers are globally well fit by a single affine motion <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b24">25]</ref>, this prior allows significant, locally smooth deviations from rigidity. Unlike the basic smoothness prior of Eq. ( <ref type="formula" target="#formula_0">1</ref>), this semiparametric construction allows effective global reasoning about non-contiguous segments of partially occluded objects. More sophisticated flow deformation priors may also be used, such as those based on robust non-local terms <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b27">28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Layer Support and Spatial Contiguity</head><p>The support for whether or not a pixel belongs to a given layer k is defined using a hidden random field g k . We associate each of the first K -1 layers at time t with a random continuous function g tk , defined over the same domain as the image. This hidden support field is illustrated in Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>We assume a single, unique layer is observable at each location and that the observed motion of that pixel is determined by its assigned layer. Analogous to level set representations, the discrete support of each layer is determined by thresholding g tk : pixel (i, j) is considered visible when g tk (i, j) ≥ 0. Let s tk (i, j) equal one if layer k is visible at pixel (i, j), and zero otherwise; note that k s tk (i, j) = 1. For pixels (i, j) for which g tk (i, j) &lt; 0, we necessarily have s tk (i, j) = 0. We define the layers to be ordered with respect to the camera, so that layer k occludes layers k ′ &gt; k.</p><p>Given the full set of support functions g tk , the unique layer k ij t * for which s tk ij t *</p><p>(i, j) = 1 is then</p><formula xml:id="formula_2">k ij t * = min ({k | 1 ≤ k ≤ K -1, g tk (i, j) ≥ 0} ∪ {K}) .<label>(3)</label></formula><p>Note that layer K is essentially a background layer that captures all pixels not assigned to the first K -1 layers. For this reason, only K -1 hidden fields g tk are needed (see Figure <ref type="figure" target="#fig_0">1</ref>).</p><p>Our use of thresholded, random continuous functions to define layer support is partially motivated by known shortcomings of discrete Ising/Potts MRF models for image partitions <ref type="bibr" target="#b14">[15]</ref>. They also provide a convenient framework for modeling the temporal and spatial coherence observed in real motion sequences. Spatial coherence is captured via a Gaussian conditional random field in which edge weights are modulated by local differences in Lab color vectors, I c t (i, j):</p><formula xml:id="formula_3">E space (g tk ) =<label>1 2</label></formula><p>(i,j) (i ′ ,j ′ )∈Γ(i,j)</p><formula xml:id="formula_4">w ij i ′ j ′ (g tk (i, j) -g tk (i ′ , j ′ )) 2 ,<label>(4)</label></formula><formula xml:id="formula_5">w ij i ′ j ′ = max exp - 1 2σ 2 c ||I c t (i, j) -I c t (i ′ , j ′ )|| 2 , δ c .<label>(5)</label></formula><p>The threshold δ c &gt; 0 adds robustness to large color changes in internal object texture. Temporal coherence of surfaces is then encouraged via a corresponding Gaussian MRF:</p><formula xml:id="formula_6">E time (g tk , g t+1,k , u tk , v tk ) = (i,j) (g tk (i, j) -g t+1,k (i + u ij tk , j + v ij tk )) 2 .<label>(6)</label></formula><p>Critically, this energy function uses the corresponding flow field to non-rigidly align the layers at subsequent frames. By allowing smooth deformation of the support functions g tk , we allow layer support to evolve over time, as opposed to transforming a single rigid template <ref type="bibr" target="#b11">[12]</ref>.</p><p>Our model of layer coherence is inspired by a recent method for image segmentation, based on spatially dependent Pitman-Yor processes <ref type="bibr" target="#b20">[21]</ref>. That work makes connections between layered occlusion processes and stick breaking representations of nonparametric Bayesian models. By assigning appropriate stochastic priors to layer thresholds, the Pitman-Yor model captures the power law statistics of natural scene partitions and infers an appropriate number of segments for each image. Existing optical flow benchmarks employ artificially constructed scenes that may have different layer-level statistics. Consequently our experiments in this paper employ a fixed number of layers K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Depth Ordering and Occlusion Reasoning</head><p>The preceding generative process defines a set of K ordered layers, with corresponding flow fields u tk , v tk and segmentation masks s tk . Recall that the layer assignment masks s are a deterministic function (threshold) of the underlying continuous layer support functions g (see Eq. ( <ref type="formula" target="#formula_2">3</ref>)). To consistently reason about occlusions, we examine the layer assignments s tk (i, j) and s t+1,k (i + u ij tk , j + v ij tk ) at locations corresponded by the underlying flow fields. This leads to a far richer occlusion model than standard spatially independent outlier processes: geometric consistency is enforced via the layered sequence of flow fields. Let I s t (i, j) denote an observed image feature for pixel (i, j); we work with a filtered version of the intensity images to provide some invariance to illumination changes. If s tk (i, j) = s t+1,k (i + u ij tk , j + v ij tk ) = 1, the visible layer for pixel (i, j) at time t remains unoccluded at time t + 1, and the image observations are modeled using a standard brightness (or, here, feature) constancy assumption. Otherwise, that pixel has become occluded, and is instead generated from a uniform distribution. The image likelihood model can then be written as</p><formula xml:id="formula_7">p(I s t | I s t+1 , u t , v t , g t , g t+1 ) ∝ exp{-E data (u t , v t , g t , g t+1 )} = exp - k (i,j) ρ d (I s t (i, j) -I s t+1 (i + u ij tk , j + v ij tk ))s tk (i, j)s t+1,k (i + u ij tk , j + v ij tk ) + λ d s tk (i, j)(1 -s t+1,k (i + u ij tk , j + v ij tk ))</formula><p>where ρ d (•) is a robust potential function and the constant λ d arises from the difference of the log normalization constants for the robust and uniform distributions. With algebraic simplifications, the data error term can be written as</p><formula xml:id="formula_8">E data (u t , v t , g t , g t+1 ) = k (i,j) ρ d (I s t (i, j) -I s t+1 (i + u ij tk , j + v ij tk )) -λ d s tk (i, j)s t+1,k (i + u ij tk , j + v ij tk )<label>(7)</label></formula><p>up to an additive, constant multiple of λ d . The shifted potential function (ρ d (•)λ d ) represents the change in energy when a pixel transitions from an occluded to an unoccluded configuration. Note that occlusions have higher likelihood only for sufficiently large discrepancies in matched image features and can only occur via a corresponding change in layer visibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Posterior Inference from Image Sequences</head><p>Considering the full generative model defined in Sec. 3, maximum a posteriori (MAP) estimation for a T frame image sequence is equivalent to minimization of the following energy function:</p><formula xml:id="formula_9">E(u, v, g, θ) = T -1 t=1 E data (u t , v t , g t , g t+1 ) + K k=1 λ a (E aff (u tk , θ tk ) + E aff (v tk , θ tk )) + K-1 k=1 λ b E space (g tk ) + λ c E time (g tk , g t+1,k , u tk , v tk ) + K-1 k=1 λ b E space (g T k ). (<label>8</label></formula><formula xml:id="formula_10">)</formula><p>Here λ a , λ b , and λ c are weights controlling the relative importance of the affine, spatial, and temporal terms respectively. Simultaneously inferring flow fields, layer support maps, and depth ordering is a challenging process; our approach is summarized below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Relaxation of the Layer Assignment Process</head><p>Due to the non-differentiability of the threshold process that determines assignments of regions to layers, direct minimization of Eq. ( <ref type="formula" target="#formula_9">8</ref>) is challenging. For a related approach to image segmentation, a mean field variational method has been proposed <ref type="bibr" target="#b20">[21]</ref>. However, that segmentation model is based on a much simpler, spatially factorized likelihood model for color and texture histogram features. Generalization to the richer flow likelihoods considered here raises significant complications.</p><p>Instead, we relax the hard threshold assignment process using the logistic function σ(g) = 1/(1 + exp(-g)). Applied to Eq. ( <ref type="formula" target="#formula_2">3</ref>), this induces the following soft layer assignments: Note that σ(-g) = 1σ(g), and K k=1 stk (i, j) = 1 for any g tk and constant λ e &gt; 0. Substituting these soft assignments stk (i, j) for s tk (i, j) in Eq. ( <ref type="formula" target="#formula_8">7</ref>), we obtain a differentiable energy function that can be optimized via gradient-based methods. A related relaxation underlies the classic backpropagation algorithm for neural network training.</p><formula xml:id="formula_11">stk (i, j) = σ(λ e g tk (i, j)) k-1 k ′ =1 σ(-λ e g tk ′ (i, j)), 1 ≤ k &lt; K, K-1 k ′ =1 σ(-λ e g tk ′ (i, j)), k = K. (<label>9</label></formula><formula xml:id="formula_12">) (a) (b) (c) (d) (e) (f) (g)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Gradient-Based Energy Minimization</head><p>We estimate the hidden fields for all the frames together, while fixing the flow fields, by optimizing an objective involving the relevant E data (•), E space (•), and E time (•) terms. We then estimate the flow fields u t , v t for each frame, while fixing those of neighboring frames and the hidden fields, via the E data (•), E aff (•), and E time (•) terms. For flow estimation, we use a standard coarse-to-fine, warpingbased technique as described in <ref type="bibr" target="#b21">[22]</ref>. For hidden field estimation, we use an implementation of conjugate gradient descent with backtracking and line search. See Supplemental Material for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head><p>We apply the proposed model to two-frame sequences and compute both the forward and backward flow fields. This enables the use of the temporal consistency term by treating one frame as both the previous and the next frame of the other <ref type="foot" target="#foot_0">1</ref> . We obtain an initial flow field using the Classic+NL method <ref type="bibr" target="#b21">[22]</ref>, cluster the flow vectors into K groups (layers), and convert the initial segmentation into the corresponding hidden fields. We then use a two-level Gaussian pyramid (downsampling factor 0.8) and perform a fairly standard incremental estimation of the flow fields for each layer. At each level, we perform 20 incremental warping steps and during each step alternately solve for the hidden fields and the flow estimates. In the end, we threshold the hidden fields to compute a hard segmentation, and obtain the final flow field by selecting the flow field from the appropriate layers.</p><p>Occluded regions are determined by inconsistencies between the hard segmentations at subsequent frames, as matched by the final flow field. We would ideally like to compare layer initializations based on all permutations of the initial flow vector clusters, but this would be computationally intensive for large K. Instead we compare two orders: a fast-to-slow order appropriate for rigid scenes, and an opposite slow-to-fast order (for variety and robustness). We illustrate automatic selection of the preferred order for the "Venus" sequence in Figure <ref type="figure" target="#fig_1">2</ref>.</p><p>The parameters for all experiments are set to λ a = 3, λ b = 30, λ c = 4, λ d = 9, λ e = 2, σ i = 12, and δ c = 0.004. A generalized Charbonnier function is used for ρ S (•) and ρ d (•) (see Supplemental Material). Optimization takes about 5 hours for the two-frame "Urban" sequence using our MATLAB implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Results on the Middlebury Benchmark</head><p>Training Set As a baseline, we implement the smoothness in layers model <ref type="bibr" target="#b25">[26]</ref> using modern techniques, and obtain an average training end-point error (EPE) of 0.487. This is reasonable but not competitive with state-of-the-art methods. The proposed model with 1 to 4 layers produces average EPEs of 0.248, 0.212, 0.200, and 0.194, respectively (see Table <ref type="table" target="#tab_0">1</ref>). The one-layer model is similar to the Classic+NL method, but has a less sophisticated (more local) model of the flow within Weighted median filtering can be interpreted as a non-local spatial smoothness term in the energy function that integrates flow field information over a larger spatial neighborhood.</p><p>The "correct" number of layers for a real scene is not well defined (consider the "Grove3" sequence, for example). We use a restricted number of layers, and model the remaining complexity of the flow within each layer via the roughness-in-layers spatial term and the WMF. As the number of layers increases, the complexity of the flow within each layer decreases, and consequently the need for WMF also decreases; note that the difference in EPE for the 4-layer model with and without WMF is insignificant. For the remaining experiments we use the version with WMF.</p><p>To test the sensitivity of the result to the initialization, we also initialized with Classic++ ("C++Init" in Table <ref type="table" target="#tab_0">1</ref>), a good, but not top, non-layered method <ref type="bibr" target="#b21">[22]</ref>. The average EPE for 1 to 4 layers increases to 0.248, 0.206, 0.203, and 0.198, respectively. While the one-layer method gets stuck in poor local minima on the "Grove3" and "Urban3" sequences, models with additional layers are more robust to the initialization. For more details and full EPE results, see the Supplemental Material.</p><p>Test Set For evaluation, we focus on a model with 3 layers (denoted "Layers++" in the Middlebury public table). On the Middlebury test set it has an average EPE of 0.270 and average angular error (AAE) of 2.556; this is the lowest among all tested methods <ref type="bibr" target="#b1">[2]</ref> at the time of writing (Oct. 2010). Table <ref type="table" target="#tab_1">2</ref> summarizes the results for individual test sequences. The layered model is particularly accurate at motion boundaries, probably due to the use of layer-specific motion models, and the explicit modeling of occlusion in E data (Eq. ( <ref type="formula" target="#formula_8">7</ref>)). For more extensive results, see the Supplemental Material.</p><p>Visual Comparison Figure <ref type="figure" target="#fig_2">3</ref> shows results for the 3-layer model on several training and test sequences. Notice that the layered model produces a motion segmentation that captures the major structure of the scene, and the layer boundaries correspond well to static image edges. It detects most occlusion regions and interpolates their motion reasonably well. Several sequences show significant improvement due to the global reasoning provided by the layered model. On the training "Grove3" sequence, the proposed method correctly identifies many holes between the branches and leaves as background. It also associates the branch at the bottom right corner with branches in the center.</p><p>As the branch moves beyond the image boundary, the layered model interpolates its motion using long-range correlation with the branches in the center. In contrast, the single-layered approach incorrectly interpolates from local background regions. The "Schefflera" result illustrates how the layered method can separate foreground objects from the background (e.g., the leaves in the top right corner), and thereby reduce errors made by single-layer approaches such as Classic+NL. "RubberWhale", "Grove3", "Urban3", "Mequon", "Schefflera", and "Grove". Left to right: First image frame, initial flow field from "Classic+NL", final flow field, motion segmentation (green front, blue middle, red back), and detected occlusions. Best viewed in color and enlarged to allow comparison of detailed motions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Discussion</head><p>We have described a new probabilistic formulation for layered image motion that explicitly occlusion and disocclusion, depth ordering of layers, and the temporal consistency of the layer segmentation. The approach allows the flow field in each layer to have piecewise smooth deformation from a parametric motion model. Layer support is modeled using an image-dependent hidden field prior that supports a model of temporal layer continuity over time. The image data error term takes into account layer occlusion relationships, resulting in increased flow accuracy near motion boundaries. Our method achieves state-of-the-art results on the Middlebury optical flow benchmark while producing meaningful segmentation and occlusion detection results.</p><p>Future work will address better inference methods, especially a better scheme to infer the layer order, and the automatic estimation of the number of layers. Computational efficiency has not been addressed, but will be important for inference on long sequences. Currently our method does not capture transparency, but this could be supported using a soft layer assignment and a different generative model. Additionally, the parameters of the model could be learned <ref type="bibr" target="#b22">[23]</ref>, but this may require more extensive and representative training sets. Finally, the parameters of the model, especially the number of layers, should adapt to the motions in a given sequence.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Left: Graphical representation for the proposed layered model. Right: Illustration of variables from the graphical model for the "Schefflera" sequence. Labeled sub-images correspond to nodes in the graph. The left column shows the flow fields for three layers, color coded as in [2]. The g and s images illustrate the reasoning about layer ownership (see text). The composite flow field (u, v) and layer labels (k) are also shown.</figDesc><graphic coords="3,270.39,81.79,225.82,150.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Results on the "Venus" sequence with 4 layers. The two background layers move faster than the two foreground layers, and the solution with the correct depth ordering has lower energy and smaller error. (a) First frame. (b-d) Fast-to-slow ordering: EPE 0.252 and energy -1.786 × 10 6 . Left to right: motion segmentation, estimated flow field, and absolute error of estimated flow field. (f-g) Slow-to-fast ordering: EPE 0.195 and energy -1.808 × 10 6 . Darker indicates larger flow field errors in (d) and (g).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Results on some Middlebury training (rows 1 to 3) and test (rows 4 to 6) sequences. Top to bottom:</figDesc><graphic coords="8,113.03,375.99,74.52,55.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Average end-point error (EPE) on the Middlebury optical flow benchmark training set.</figDesc><table><row><cell></cell><cell>Avg. EPE</cell><cell>Venus</cell><cell>Dimetrodon</cell><cell>Hydrangea</cell><cell>RubberWhale</cell><cell>Grove2</cell><cell>Grove3</cell><cell>Urban2</cell><cell>Urban3</cell></row><row><cell>Weiss [26]</cell><cell>0.487</cell><cell>0.510</cell><cell>0.179</cell><cell>0.249</cell><cell>0.236</cell><cell>0.221</cell><cell>0.608</cell><cell>0.614</cell><cell>1.276</cell></row><row><cell>Classic++</cell><cell>0.285</cell><cell>0.271</cell><cell>0.128</cell><cell>0.153</cell><cell>0.081</cell><cell>0.139</cell><cell>0.614</cell><cell>0.336</cell><cell>0.555</cell></row><row><cell>Classic+NL</cell><cell>0.221</cell><cell>0.238</cell><cell>0.131</cell><cell>0.152</cell><cell>0.073</cell><cell>0.103</cell><cell>0.468</cell><cell>0.220</cell><cell>0.384</cell></row><row><cell>1layer</cell><cell>0.248</cell><cell>0.243</cell><cell>0.144</cell><cell>0.175</cell><cell>0.095</cell><cell>0.125</cell><cell>0.504</cell><cell>0.279</cell><cell>0.422</cell></row><row><cell>2layers</cell><cell>0.212</cell><cell>0.219</cell><cell>0.147</cell><cell>0.169</cell><cell>0.081</cell><cell>0.098</cell><cell>0.376</cell><cell>0.236</cell><cell>0.370</cell></row><row><cell>3layers</cell><cell>0.200</cell><cell>0.212</cell><cell>0.149</cell><cell>0.173</cell><cell>0.073</cell><cell>0.090</cell><cell>0.343</cell><cell>0.220</cell><cell>0.338</cell></row><row><cell>4layers</cell><cell>0.194</cell><cell>0.197</cell><cell>0.148</cell><cell>0.159</cell><cell>0.068</cell><cell>0.088</cell><cell>0.359</cell><cell>0.230</cell><cell>0.300</cell></row><row><cell>3layers w/ WMF</cell><cell>0.195</cell><cell>0.211</cell><cell>0.150</cell><cell>0.161</cell><cell>0.067</cell><cell>0.086</cell><cell>0.331</cell><cell>0.210</cell><cell>0.345</cell></row><row><cell>3layers w/ WMF C++Init</cell><cell>0.203</cell><cell>0.212</cell><cell>0.151</cell><cell>0.161</cell><cell>0.066</cell><cell>0.087</cell><cell>0.339</cell><cell>0.210</cell><cell>0.396</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Average end-point error (EPE) on the Middlebury optical flow benchmark test set. It thus performs worse than the Classic+NL initialization; the performance improvements allowed by additional layers demonstrate the benefits of a layered model.Accuracy is improved by applying a 15 × 15 weighted median filter (WMF)<ref type="bibr" target="#b21">[22]</ref> to the flow fields of each layer during the iterative warping step (EPE for 1 to 4 layers: 0.231, 0.204, 0.195, and 0.193).</figDesc><table><row><cell></cell><cell>Rank</cell><cell>Average</cell><cell>Army</cell><cell>Mequon</cell><cell>Schefflera</cell><cell>Wooden</cell><cell>Grove</cell><cell>Urban</cell><cell>Yosemite</cell><cell>Teddy</cell></row><row><cell>EPE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Layers++</cell><cell>4.3</cell><cell>0.270</cell><cell>0.08</cell><cell>0.19</cell><cell>0.20</cell><cell>0.13</cell><cell>0.48</cell><cell>0.47</cell><cell>0.15</cell><cell>0.46</cell></row><row><cell>Classic+NL</cell><cell>6.5</cell><cell>0.319</cell><cell>0.08</cell><cell>0.22</cell><cell>0.29</cell><cell>0.15</cell><cell>0.64</cell><cell>0.52</cell><cell>0.16</cell><cell>0.49</cell></row><row><cell>EPE in boundary regions</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Layers++</cell><cell></cell><cell>0.560</cell><cell>0.21</cell><cell>0.56</cell><cell>0.40</cell><cell>0.58</cell><cell>0.70</cell><cell>1.01</cell><cell>0.14</cell><cell>0.88</cell></row><row><cell>Classic+NL</cell><cell></cell><cell>0.689</cell><cell>0.23</cell><cell>0.74</cell><cell>0.65</cell><cell>0.73</cell><cell>0.93</cell><cell>1.12</cell><cell>0.13</cell><cell>0.98</cell></row><row><cell>that layer.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Our model works for longer sequences. We use two frames here for fair comparison with other methods.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>DS and MJB were supported in part by the NSF Collaborative Research in Computational Neuroscience Program (IIS-0904875) and a gift from Intel Corp.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Layered representation of motion video using robust maximum-likelihood estimation of mixture models and MDL encoding</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Sawhney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1995-06">Jun 1995</date>
			<biblScope unit="page" from="777" to="784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A database and evaluation methodology for optical flow</title>
		<author>
			<persName><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint/>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multiway cut for stereo and motion with slanted surfaces</title>
		<author>
			<persName><forename type="first">S</forename><surname>Birchfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="489" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Robust dynamic motion estimation over time</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Anandan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="296" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The robust estimation of multiple motions: Parametric and piecewise-smooth flow fields</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Anandan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="75" to="104" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Estimating optical-flow in segmented images using variable-order parametric models with local deformations</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Jepson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="972" to="986" />
			<date type="published" when="1996-10">October 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Robust estimation of a multi-layered motion representation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Visual Motion</title>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="173" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cooperative robust estimation using layers of support</title>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="474" to="487" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Triangleflow: Optical flow with triangulationbased higher-order likelihoods</title>
		<author>
			<persName><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Heibel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="272" to="285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">From reference frames to reference planes: Multi-view parallax geometry and applications</title>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Anandan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weinshall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mixture models for optical flow computation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jepson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning flexible sprites in video layers</title>
		<author>
			<persName><forename type="first">N</forename><surname>Jojic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Frey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">I</biblScope>
			<biblScope unit="page" from="199" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A generative model of dense optical flow in layers</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jojic</surname></persName>
		</author>
		<idno>TR PSI-2001-11</idno>
		<imprint>
			<date type="published" when="2001-08">Aug. 2001</date>
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Shape recovery from multiple views: A parallax based approach</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Anandan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hanna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc 12th ICPR</title>
		<meeting>12th ICPR</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The Ising/Potts model is not well suited to segmentation tasks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Descombes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zerubia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Digital Signal Processing Workshop</title>
		<meeting>the IEEE Digital Signal Processing Workshop</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Motion segmentation with accurate boundaries -a tensor voting approach</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nicolescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="382" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning layered motion segmentations of video</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="301" to="319" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On the spatial statistics of optical flow</title>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="33" to="50" />
			<date type="published" when="2007-08">August 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">3D geometry from planar parallax</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Sawhney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="929" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">High resolution motion layer decomposition using dual-space graph cuts</title>
		<author>
			<persName><forename type="first">T</forename><surname>Schoenemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008-06">June 2008</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Shared segmentation of natural scenes using dependent Pitman-Yor processes</title>
		<author>
			<persName><forename type="first">E</forename><surname>Sudderth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1585" to="1592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Secrets of optical flow estimation and their principles</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning optical flow</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="83" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An integrated Bayesian approach to layer extraction from image sequences</title>
		<author>
			<persName><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Anandan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="297" to="303" />
			<date type="published" when="2001-03">Mar 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Representing moving images with layers</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="625" to="638" />
			<date type="published" when="1994-09">Sept. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Smoothness in layers: Motion segmentation using nonparametric mixture estimation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1997-06">Jun 1997</date>
			<biblScope unit="page" from="520" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A unified mixture framework for motion segmentation: Incorporating spatial coherence and estimating the number of models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1996-06">Jun 1996</date>
			<biblScope unit="page" from="321" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Motion estimation with non-local total variation regularization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Werlberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The dense estimation of motion and appearance in layers</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yalcin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fablet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Image and Video Registration</title>
		<imprint>
			<date type="published" when="2004-06">Jun 2004</date>
			<biblScope unit="page" from="777" to="784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Background layer model for object tracking through occlusion</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1079" to="1085" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
