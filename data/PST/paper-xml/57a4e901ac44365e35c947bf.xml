<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Manifold Ranking-Based Matrix Factorization for Saliency Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Dapeng</forename><surname>Tao</surname></persName>
							<email>dapeng.tao@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Jun</forename><surname>Cheng</surname></persName>
							<email>jun.cheng@siat.ac.cn</email>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Mingli</forename><surname>Song</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xu</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Engineering</orgName>
								<orgName type="institution">Yunnan University</orgName>
								<address>
									<postCode>650091</postCode>
									<settlement>Kunming</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Institutes of Advanced Technology</orgName>
								<orgName type="laboratory">Laboratory for Human Machine Control</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>518055</postCode>
									<settlement>Shenzhen, Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<postCode>310027</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Institutes of Advanced Technology</orgName>
								<orgName type="laboratory">Laboratory for Human Machine Control</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>518055</postCode>
									<settlement>Shenzhen, Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="laboratory">Hubei Key Laboratory of Intelligent Vision Based Monitoring for Hydroelectric Engineering</orgName>
								<orgName type="institution">Three Gorges University</orgName>
								<address>
									<postCode>443002</postCode>
									<settlement>Yichang</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Manifold Ranking-Based Matrix Factorization for Saliency Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">68788E7B01E04012DA7D6BFB111991C8</idno>
					<idno type="DOI">10.1109/TNNLS.2015.2461554</idno>
					<note type="submission">received September 29, 2014; revised July 21, 2015; accepted July 23, 2015.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T04:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Manifold ranking (MR)</term>
					<term>matrix factorization</term>
					<term>optimization algorithm</term>
					<term>saliency detection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Saliency detection is used to identify the most important and informative area in a scene, and it is widely used in various vision tasks, including image quality assessment, image matching, and object recognition. Manifold ranking (MR) has been used to great effect for the saliency detection, since it not only incorporates the local spatial information but also utilizes the labeling information from background queries. However, MR completely ignores the feature information extracted from each superpixel. In this paper, we propose an MR-based matrix factorization (MRMF) method to overcome this limitation. MRMF models the ranking problem in the matrix factorization framework and embeds query sample labels in the coefficients. By incorporating spatial information and embedding labels, MRMF enforces similar saliency values on neighboring superpixels and ranks superpixels according to the learned coefficients. We prove that the MRMF has good generalizability, and develops an efficient optimization algorithm based on the Nesterov method. Experiments using popular benchmark data sets illustrate the promise of MRMF compared with the other state-of-the-art saliency detection methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>most important information contained within a tremendous amount of visual data. It also has a role in determining the regions that are more attractive to humans, and the method can be considered as a selection process that locates salient regions in a scene. In tandem with the rapid development of computer vision techniques, saliency detection has become popular in a number of vision tasks, such as image quality assessment <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, image matching <ref type="bibr" target="#b2">[3]</ref>- <ref type="bibr" target="#b4">[5]</ref>, object recognition <ref type="bibr" target="#b5">[6]</ref>- <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, image superresolution <ref type="bibr" target="#b11">[12]</ref>, and visual tracking <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b15">[16]</ref>.</p><p>Saliency detection methods can be divided into two main categories: 1) the bottom-up methods and 2) the top-down methods. Most existing models fall into the bottom-up category, in which subjects' freeview, a scene and salient regions attract attention <ref type="bibr" target="#b16">[17]</ref>- <ref type="bibr" target="#b19">[20]</ref>. The bottom-up mechanism involves low-level processes, and, is driven by the intrinsic attributes of the stimuli <ref type="bibr" target="#b20">[21]</ref>- <ref type="bibr" target="#b22">[23]</ref>, adopts the different properties of the low-level visual information to compute saliency maps. In contrast, the top-down models are driven by high-level tasks, such as looking for a specific object category in a scene <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b23">[24]</ref>. The bottom-up and top-down information can be combined into a unified framework for the general visual attention analysis <ref type="bibr" target="#b24">[25]</ref>.</p><p>Although most saliency detection methods aim at detecting the center-surround contrast and have good performance, there remains room for efficiency and stability improvements. The tradition bottom-up models often pursue the object boundaries and ignore the target region uniformly, and thus these models fail to some real applications, such as visual tracking and image retrieval. Recently, Wei et al. <ref type="bibr" target="#b25">[26]</ref> considered the contribution of background priors in the saliency detection and developed a new scheme for the saliency detection. In addition, manifold ranking (MR) has successfully been applied to saliency detection, such as the effective two-stage saliency detection framework proposed in <ref type="bibr" target="#b26">[27]</ref>. Note that the models utilized the background priors can obtain a better performance in calculating the precise saliency boundary patches.</p><p>MR is an effective saliency detection technique <ref type="bibr" target="#b26">[27]</ref>, because it exploits the intrinsic graph structure and incorporates local grouping cues in graph labeling. Further analysis shows that the MR intrinsically propagates query labels along an adjacent graph, and thus the constructed graph significantly influences its performance. Since features extracted from each superpixel determine the weights between two nodes in the constructed graph, they are important cues in the saliency detection.</p><p>However, MR completely ignores such cues, and therefore, a great deal of information is not involved. In this paper, we propose a novel bottom-up model, i.e., the MR-based matrix factorization (MRMF), to overcome this problem. In particular, MRMF concatenates the features of all the superpixels into a data matrix, and then decomposes the concatenated matrix into the product of two low-rank matrices, i.e., the basis of the lower dimensional space and the coefficients of all samples <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>. By simultaneously incorporating the manifold regularization <ref type="bibr" target="#b29">[30]</ref> over coefficients and constraining the coefficients of query samples to be in the unit ball, MRMF can rank the remaining samples according to their coefficients <ref type="bibr" target="#b30">[31]</ref>. Technically, the motivation of MRMF is that the comprehensive ranking result can be used to compute the saliency map according to the MR framework. Note that the technique of matrix factorization <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref> has received a lot of attentions and is widely used in image classification <ref type="bibr" target="#b33">[34]</ref> and image processing recently. Our theoretical analysis shows that the MRMF is generalizable. However, since MRMF is nonconvex, its optimization is not efficient. To overcome this problem, we develop an alternating algorithm based on Nesterov's method <ref type="bibr" target="#b34">[35]</ref> to solve MRMF. Experiments on popular benchmark data sets demonstrate the effectiveness of MRMF for the saliency detection.</p><p>The main contributions of this paper include: 1) we develop a novel MRMF approach, which incorporates the spatial information and embeds labels; 2) we prove that the MRMF has good generalizability and develop an efficient optimization algorithm based on Nesterov's method; and 3) to demonstrate the effectiveness of MRMF, we provide extensive experimentations on the benchmark data sets to verify that the newly developed MRMF can improve the saliency detection performance. Given space constraints, those parts that are easy to implement based on the given references are not detailed <ref type="bibr" target="#b26">[27]</ref>.</p><p>The rest of this paper is organized as follows. In Section II, we briefly discuss related works of the new proposed algorithm. The newly proposed MRMF is detailed in Section III. A theoretical analysis of the generalization error bound for MRMF is provided in Section IV. Section V presents the experimental results on four representative data sets. Finally, the conclusion is drawn in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In recent years, a number of salience detection methods have been proposed, which, as noted above, can be grouped into the bottom-up and top-down models. In this section, we briefly review the most popular techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Bottom-Up Models</head><p>Based on the low-level visual characteristics used to determine the saliency measure, the bottom-up attention models can be further divided into six categories: 1) contrast-based models <ref type="bibr" target="#b35">[36]</ref>- <ref type="bibr" target="#b38">[39]</ref>; 2) information theoretical models <ref type="bibr" target="#b39">[40]</ref>- <ref type="bibr" target="#b41">[42]</ref>; 3) Bayesian models <ref type="bibr" target="#b42">[43]</ref>- <ref type="bibr" target="#b44">[45]</ref>; 4) graphical models <ref type="bibr" target="#b45">[46]</ref>- <ref type="bibr" target="#b47">[48]</ref>; 5) frequency domain-based methods <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b48">[49]</ref>- <ref type="bibr" target="#b51">[52]</ref>; and 6) supervised-learning-based models <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref>.</p><p>In general, the contrast-based models have been inspired by the feature integration theory <ref type="bibr" target="#b54">[55]</ref>, which states that visual saliency can be obtained by combining the saliencies computed from different features. These methods are also considered to be biologically plausible, because they are inspired by the behavior and the neuronal architecture of the early primate visual system. Itti et al. <ref type="bibr" target="#b35">[36]</ref> proposed a saliency model (an extension of <ref type="bibr" target="#b55">[56]</ref>) by integrating multiscale contrasts in intensity, color, and orientation features. This method has three main stages: 1) different simple features are extracted at locations over the image plane; 2) activation maps are obtained using the center-surround operators at multiple scales, resulting in the multiscale feature contrasts; and 3) the activation maps are normalized and combined into a single saliency map. The method was later extended by adding motion and flicker contrasts for the video domain <ref type="bibr" target="#b56">[57]</ref>.</p><p>Following <ref type="bibr" target="#b35">[36]</ref>, a large number of contrast-based models with different features, contrast functions, and combination rules have been developed. Le Meur et al. <ref type="bibr" target="#b37">[38]</ref> proposed a bottom-up saliency approach based on the structure of the human visual system, which includes the contrast sensitivity functions, perceptual decomposition, and center-surround interactions. In <ref type="bibr" target="#b38">[39]</ref>, the center-surround histogram contrast, which is the distance between the color histograms in a region and its surrounding regions, was used as the bottom-up cue to detect salient objects. A lot of progress has been made, since these early efforts and many off-the-shelf toolboxes exist that can be applied to many visual tasks.</p><p>The information theoretical methods define bottom-up saliency based on maximum information sampling. The most informative parts of a scene are selected as salient parts. For example, Burce and Tsotsos <ref type="bibr" target="#b39">[40]</ref> proposed the attention based on information maximization (AIM) model. AIM uses Shannon's self-information, which is inversely proportional to the likelihood of observing the feature vector at a certain position, to measure the saliency. To estimate the probability density function of local patches, they used independent component analysis to reduce the problem to estimating several 1-D density functions. Hou and Zhang <ref type="bibr" target="#b40">[41]</ref> proposed the incremental coding length method to measure the perspective entropy of each feature: the features with large coding length increments are selected as salient features. Similar to the AIM method, rare features have the highest energy and become salient. Seo and Milanfar <ref type="bibr" target="#b57">[58]</ref> proposed the self-resemblance approach, which measures the resemblance of the local image structure at each pixel to its surroundings and takes the statistical likelihood of its feature (given the features in surrounding pixels) as the saliency measure.</p><p>Bayesian models have the advantage of combining different sensory evidence and prior constraints into a unified framework. Oliva et al. <ref type="bibr" target="#b42">[43]</ref> proposed a Bayesian model that determines the joint probability of the presence and location of a pixel in the image given the observed features. The bottom-up saliency in the model is the probability of the local feature at a pixel given the global image features, which is similar to the self-information method in <ref type="bibr" target="#b39">[40]</ref>. Zhang et al. <ref type="bibr" target="#b44">[45]</ref> proposed the saliency using natural statistics (SUN) model, which defines saliency by considering what the visual system is trying to optimize when directing attention. The proposed model is a Bayesian framework, from which bottom-up saliency naturally emerges as the self-information of visual features, and the overall saliency emerges as the pointwise mutual information between the features and the target. The Bayesian models are very similar to the information theoretical methods but have the additional advantage that they can easily incorporate the top-down information.</p><p>In most of these methods, the saliency values at different locations are considered independently. With graphical models, dependence between saliencies over different spatial and temporal locations can be considered, which results in more powerful models. Harel et al. <ref type="bibr" target="#b47">[48]</ref> introduced graph-based visual saliency, in which Markov chains are defined over various image maps and the equilibrium distribution over map locations are treated as saliency values. Avraham and Lindenbaum <ref type="bibr" target="#b46">[47]</ref> introduced the extended saliency model, which uses a graphical model approximation to efficiently reveal the segments that are more likely to be salient. The model quantifies several intuitive observations, such as the greater likelihood that visually similar image regions will correspond and only a few interesting objects will be present in the scene. In general, graphical models can model complex dependence involved in the saliency computation process and can easily incorporate spatial and temporal constraints and complex priors. However, computational complexity is much higher than with the other methods.</p><p>Frequency domain-based methods model saliency in the frequency domain rather than the spatial domain. Hou and Zhang <ref type="bibr" target="#b50">[51]</ref> introduced the spectral residual model, which assumes that the statistical singularities in the spectrum are responsible for anomalous regions in the image spatial domain. The salient map is derived from the inverse Fourier transform of the residual between the Fourier amplitude spectrum of the down-sampled image and the smoothed version of the spectrum. Yang et al. <ref type="bibr" target="#b32">[33]</ref> and Guo et al. <ref type="bibr" target="#b48">[49]</ref> incorporated the phase spectrum of the Fourier transform to improve saliency predictions. Achanta et al. <ref type="bibr" target="#b51">[52]</ref> introduced a frequency-tuned approach for the saliency detection using low-level features of color and luminance; the saliency map is computed as the difference between the algorithmic mean of the image feature vector and the Gaussian-blurred version of the original image. The Fourier domain-based methods provide new insights into saliency modeling and are generally fast to compute.</p><p>While most of the existing methods use criteria based on natural priors to define saliency, the supervised-learningbased methods attempt to learn the salient mapping function from labeled salient regions. Kienzle et al. <ref type="bibr" target="#b53">[54]</ref> introduced an support vector machine (SVM)-based method <ref type="bibr" target="#b58">[59]</ref> to directly learn attention from the human eye-tracking data. This model considers saliency detection as a binary classification problem and is trained on the positive and negative samples, which are fixations and randomly sampled patches, respectively. Similar to <ref type="bibr" target="#b53">[54]</ref>, Judd et al. <ref type="bibr" target="#b52">[53]</ref> trained a linear SVM from human fixation data using a set of low-, mid-, and high-level image features. Peters and Itti <ref type="bibr" target="#b59">[60]</ref> trained a simple regression classifier to capture the task-dependent association between a given scene, and the preferred locations to gaze at while subjects were playing video games. The supervised-learning-based methods learn the saliency mapping function from the data, with few a priori assumptions. However, the learned model might be overfitted to the data used for training, and therefore, a large number of training examples must be provided. In addition, it is possible to use deep-learning techniques <ref type="bibr" target="#b60">[61]</ref> to learn cortex-like neural networks to tackle attention problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Top-Down Models</head><p>Unlike bottom-up methods, which only consider the low-level information from the images themselves, the topdown models are driven by high-level tasks, e.g., searching for particular objects or object classes. The top-down models can also incorporate bottom-up attention mechanisms to process the data more efficiently. Gao and Vasconcelos <ref type="bibr" target="#b23">[24]</ref> used classification as the specific goal for saliency, and called the saliency discriminant saliency. In this approach, the regions that are more discriminant for classification are deemed the salient regions, and saliency detection is considered as a feature selection problem by maximizing the mutual information between the features and the class labels. Similarly, the SUN model <ref type="bibr" target="#b44">[45]</ref> uses a Bayesian framework that combines bottom-up and top-down saliency and obtains the overall saliency as the pointwise mutual information between local features and the search target's features when searching for a target. Li et al. <ref type="bibr" target="#b61">[62]</ref> presented a Bayesian multitask-learning framework for visual attention in video. In this paper, bottom-up saliency modeled by multiscale wavelet decomposition was fused with different top-down components trained by a multitask-learning algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. MANIFOLD RANKING-BASED MATRIX FACTORIZATION</head><p>Given a point set V = {v 1 , . . . , v q , v q+1 , . . . , v n } ⊂ R m , the first q points are the queries, and the remainder are the points to be ranked. Let d : V × V → R denote a metric on V that assigns each pair of points, e.g., v i and v j , a distance d(v i , v j ). Since the first q points are queries, we fix their ranking scores to one. Let h : V → R denote a ranking function that assigns each nonquery point v i a ranking score h i . We can view h as a vector, i.e., h</p><formula xml:id="formula_0">= [h q+1 , . . . , h n ] T ∈ R n-q + .</formula><p>In this paper, we propose the MRMF method to compute the ranking scores h. In particular, we concatenate all data points into a matrix, i.e., V = [v 1 , . . . , v n ], and decompose V into the product of two lower rank matrices min</p><formula xml:id="formula_1">H ≥0,W V -W H 2 F (1)</formula><p>where W denotes the basis and H signifies the coefficients. To incorporate both the label and the feature information, we divide H and W into two components where W f represents the feature information, H f are the coefficients, and w signifies the axis that represents the ranking information.</p><formula xml:id="formula_2">H = H f 1 T h T (2) W = [W f , w]<label>(3)</label></formula><p>Since the ranking scores of queries are equal to one, the ranking scores of relevant points are expected to be close to one and those of irrelevant points close to zero. To this end, a closed-loop adjacent graph is constructed following <ref type="bibr" target="#b26">[27]</ref>, and the labels of queries are then propagated on this graph following the regularization theory. To achieve the closedloop graph, the image is segmented into superpixels using the simple linear iterative cluster (SLIC) algorithm <ref type="bibr" target="#b45">[46]</ref>, and a k-regular graph G is constructed to exploit the spatial relationship between the superpixels and the fact that neighboring superpixels and superpixels sharing common boundaries are likely to have a similar appearance and share saliency values. A manifold regularization is then incorporated over H and a Tikhonov regularization over h into <ref type="bibr" target="#b0">(1)</ref>.</p><p>In summary, the objective of MRMF is min</p><formula xml:id="formula_3">H ≥0,W 1 2 V -W H 2 F + γ tr(H L H T ) + β h 2 2 (<label>4</label></formula><formula xml:id="formula_4">)</formula><p>where W = [W f w], L is the graph Laplacian of G, γ &gt; 0 and β &gt; 0 are the tradeoff parameters, and H = H f 1 T h T . By minimizing tr(H L H T ), MRMF preserves the local smoothness of graph G, i.e., two superpixels close on the graph are likely to have similar saliency values. This enforces the ranking scores of relevant superpixels to be close to one. By minimizing h 2  2 , MRMF enforces the ranking scores of irrelevant superpixels to be close to zero.</p><p>Since ( <ref type="formula" target="#formula_3">4</ref>) is nonconvex, we usually alternately optimize W and H to search its local solution. W can intuitively be directly optimized using least squares</p><formula xml:id="formula_5">W * = V H † (<label>5</label></formula><formula xml:id="formula_6">)</formula><p>where † denotes the pseudoinverse operator. To optimize H , we rewrite the objective (4) as follows:</p><p>min</p><formula xml:id="formula_7">H ≥0 F(H ) = 1 2 V -W H 2 F + β h 2 2 + γ tr(H L H T ) . (<label>6</label></formula><formula xml:id="formula_8">)</formula><p>Since H is a block matrix and contains a constant block, it is difficult to deal with the third term. We therefore partition L into four blocks</p><formula xml:id="formula_9">L = L 11 L 12 L 21 L 22 (<label>7</label></formula><formula xml:id="formula_10">)</formula><p>where L 11 ∈ R q×q , and L 12 = L 21 , because L is symmetric.</p><p>According to <ref type="bibr" target="#b6">(7)</ref>, we have</p><formula xml:id="formula_11">tr(H L H T ) = tr(H f L H T f ) + tr(1 T L 11 1) + 2tr(h T L 12 1) + tr(h T L 22 h) (<label>8</label></formula><formula xml:id="formula_12">)</formula><p>where H f and h can now be optimized separately.</p><formula xml:id="formula_13">A. Optimize H f Let R f = V -w[1 T , h T ].</formula><p>The objective function of ( <ref type="formula" target="#formula_7">6</ref>) can be rewritten as min</p><formula xml:id="formula_14">H f ≥0 F(H f ) = 1 2 R f -W f H f 2 F + γ tr(H f L H T f ) (9)</formula><p>and the gradient of F f with respect to H f can be calculated as follows:</p><formula xml:id="formula_15">∇ F (H f ) = W T f W f H f -W T f R f + γ H f L. (<label>10</label></formula><formula xml:id="formula_16">)</formula><p>Equation ( <ref type="formula">9</ref>) is usually minimized using the projected gradient descent (PGD) method. PGD advances the search point based on the previous one and performs step-by-step iteration toward the optimum. However, PGD is slow to converge, because it can become trapped in local solutions. To overcome this limitation, Nesterov <ref type="bibr" target="#b34">[35]</ref> proposed the optimal gradient method (OGM) for convex optimization. Since the F(H f ) is convex and the three terms in ∇ F (H f ) are Lipschitz continuous (with Lipschitz constants W T f W f 2 , 0, and L 2 , respectively), the Nesterov method can naturally be adopted to optimize H f . OGM advances the search point based on the auxiliary point constructed by combing two neighboring search points. At the kth iteration, given two previous search points, i.e., H k-1 f and H k f , OGM constructs an auxiliary point, i.e., Y k , as follows:</p><formula xml:id="formula_17">Y k = H k f + α k -1 α k+1 H k-1 f (<label>11</label></formula><formula xml:id="formula_18">)</formula><p>where the combination coefficient α k is smartly chosen and carefully updated in each iteration <ref type="bibr" target="#b34">[35]</ref> </p><formula xml:id="formula_19">α k+1 = 1 + 4α 2 k + 1 2<label>(12)</label></formula><p>and</p><formula xml:id="formula_20">α 0 = 1. (<label>13</label></formula><formula xml:id="formula_21">)</formula><p>Note that k ≥ 1 in <ref type="bibr" target="#b7">(8)</ref> and </p><formula xml:id="formula_22">Y 0 = H 0 f when k = 0.</formula><formula xml:id="formula_23">H k+1 f = argmin H f ≥0 P(Y k , H f ) = F(Y k ) + f (Y k ), H f -Y k + L 2 H f -Y k 2 F (<label>14</label></formula><formula xml:id="formula_24">)</formula><p>where <ref type="formula" target="#formula_23">14</ref>) is a constrained optimization problem that can be solved using the Lagrange multiplier method. According to <ref type="bibr" target="#b62">[63]</ref>, H f k+1 satisfies the following Karush-Kuhn-Tucker (K.K.T.) conditions:</p><formula xml:id="formula_25">L = W T f W f 2 + γ L 2 is the Lipschitz constant of ∇ F (H f ). Equation (</formula><formula xml:id="formula_26">∇ P (H k+1 f ≥ 0 H k+1 f ≥ 0 ∇ P H k+1 f • H k+1 f = 0 (<label>15</label></formula><formula xml:id="formula_27">)</formula><p>where</p><formula xml:id="formula_28">∇ P (H k+1 f ) = ∇ F (Y k ) + L(H k+1 f -Y k ) is the gradient of P at H k+1 f</formula><p>, and • denotes the Hadamard product. Using <ref type="bibr" target="#b9">(10)</ref>, we have</p><formula xml:id="formula_29">H k+1 f = + Y k - 1 L ∇ F (Y k ) (<label>16</label></formula><formula xml:id="formula_30">)</formula><p>where ∇ F (Y k ) is defined by <ref type="bibr" target="#b9">(10)</ref> and + projects all the negative entries to zeros. By iterating ( <ref type="formula" target="#formula_17">11</ref>) and ( <ref type="formula" target="#formula_29">16</ref>), OGM can obtain the optimal solution of (9). According to <ref type="bibr" target="#b63">[64]</ref>, it is easy to verify that the OGM converges optimally at the rate O((1/k 2 )). Here, we omit the proof due to space constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Optimize h</head><p>Similarly, let R denote R = V -W f H f , and partition it into two parts, i.e., R = [R 1 , R 2 ], where R 1 ∈ R m×q . The optimization problem with respect to h can be formulated as follows:</p><formula xml:id="formula_31">min h≥0 1 2 f (h) = R 2 -wh 2 F + β h 2 2 + 2γ tr(h T L 12 1) + γ tr(h T L 22 h) . (<label>17</label></formula><formula xml:id="formula_32">)</formula><p>Let h * denote the minimum of ( <ref type="formula" target="#formula_17">11</ref>) then, according to <ref type="bibr" target="#b62">[63]</ref>, the K.K.T. conditions of ( <ref type="formula" target="#formula_31">17</ref>) are</p><formula xml:id="formula_33">⎧ ⎨ ⎩ h * ≥ 0 ∇ f (h * ) ≥ 0 h * • ∇ f (h * ) = 0 (18)</formula><p>where ∇ f (h) = hw T w-R T 2 w+γ L 21 1+γ L 22 h +βh signifies the gradient of f at h. According to <ref type="bibr" target="#b17">(18)</ref> </p><formula xml:id="formula_34">h * = + γ L 22 + w 2 2 I + β I -1 R T 2 w -γ L 21 1<label>(19)</label></formula><p>where + (•) projects variables to the positive orthant. Based on the above discussions, MRMF is summarized in Algorithm 1.</p><p>For convenience, Table <ref type="table" target="#tab_0">I</ref> lists frequently used notations and descriptions in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. MRMF for Saliency Detection</head><p>The procedure for MRMF-based saliency detection can be summarized as two stages. In the first stage, MRMF segments the input image into several nonoverlapping regions (or superpixels) using the SLIC algorithm <ref type="bibr" target="#b45">[46]</ref> and construct a closed-loop graph by connecting neighboring superpixels and superpixels sharing common image boundaries. Since superpixels on different image boundaries are dissimilar, this method separately computes specific labeled maps for each of the four sides and combines them to generate the initial saliency map. In particular, MRMF uses the superpixels on each side of image as labeled background queries and computes the saliencies of the remaining superpixels based on their relevance to those queries by ranking on the previously constructed graph. A saliency map is generated by multiplying the four-labeled maps obtained. In the second stage, MRMF considers the labeled foreground superpixels as salient queries and computes the saliency of each superpixel based on its relevance to foreground queries to produce the final map. The architecture of the proposed MRMF-based saliency detection technique is shown in Fig. <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. THEORETICAL ANALYSIS Let us first analyze the reconstruction error of the learned bases and representations using (4). Let us replace the soft constraints of tr(H L H T ) and h 2</head><p>F with the hard constraints of tr(H L H T ) ≤ λ 1 and h 2 F ≤ λ 2 , where λ 1 and λ 2 are the positive constants min</p><formula xml:id="formula_35">W ≥0,H ≥0 1 2 V -W H 2 F s.t. tr(H L H T ) ≤ λ 1 , h 2 F ≤ λ 2 . (<label>20</label></formula><formula xml:id="formula_36">)</formula><p>We further restrict H to be in the unit ball of R k×n , which is easily achieved, because there is a tradeoff between the bounds of the columns of W and the entries of H ; for example, W H = WQQ -1 1 H and Q can be set to normalize H . The empirical reconstruction error with respect to basis W can, therefore, be defined as where</p><formula xml:id="formula_37">R n (W ) = 1 n min H ∈H V -W H 2 F (21)</formula><formula xml:id="formula_38">H = {H : tr(H L H T ) ≤ λ 1 , h 2 F ≤ λ 2 , H i ≤ 1, and i = 1, . . . , n}.</formula><p>The expected reconstruction error with respect to basis W is defined as</p><formula xml:id="formula_39">R(W ) = E v R n . (<label>22</label></formula><formula xml:id="formula_40">)</formula><p>MRMF is convex with respect to either W or H, but not to both. However, given basis W, the representatives H are fixed due to convexity. Thus, the reconstruction error can be analyzed by discussing the choice of basis W. The loss function class can be defined as</p><formula xml:id="formula_41">F W = ⎧ ⎨ ⎩ f W (v) = min H ∈H v - k j =1 W j H i j 2 : W ∈ R m×k ⎫ ⎬ ⎭ .<label>(23)</label></formula><p>Then, we have</p><formula xml:id="formula_42">R n (W ) = 1 n n i=1 f W (v i )<label>(24)</label></formula><p>and</p><formula xml:id="formula_43">R(W ) = E v f W (v). (<label>25</label></formula><formula xml:id="formula_44">)</formula><p>We next discuss the reconstruction error bound of MRMF. According to (4), if a point shares the same bases of f and r with queries and, furthermore, the new representation of the point is a small distance away from those of the queries, such a point should have a high ranking score. When the basis W is given, H will be fixed due to convexity. As a result, the proposed MR crucially depends on the learned basis W. Bases that fit the query points well and the other points in the set V are needed. Assume the point set v 1 , . . . , v n are independent identically distributed We can prove that the expected reconstruction error is upper bounded by the empirical reconstruction error and is a decreasing function with respect to n, which indicates that the proposed model can be extrapolated to score a point drawn from the same distribution.</p><p>Theorem 1: Let the columns of V be upper bounded by r (that is, v ≤ r, v ∈ R m ), the columns of W are upper bounded by c, and H is in the unit ball of R k×n . For any W and H learned by (4) and any δ &gt; 0, with probability at least 1 -δ, we have</p><formula xml:id="formula_45">1 n E v V -W H 2 F ≤ 1 n V -W H 2 F + 4 √ π C(c, λ 1 , λ 2 )rk √ n + 2 √ π C 2 (c, λ 1 , λ 2 )k 2 √ n + r 2 ln 1 δ 2n</formula><p>where C(c, λ 1 , λ 2 ) is a constant depending only on the constants of c, λ 1 , and λ 2 .</p><p>The following theorem plays an important role in proving Theorem 1.</p><p>Theorem 2 <ref type="bibr" target="#b64">[65]</ref>: Let F be an [0, a]-valued function class on R m , and V = {v 1 , . . . , v n } ⊂ R m×n . For any δ &gt; 0, with probability at least 1 -δ, we have sup</p><formula xml:id="formula_46">f ∈F E v f (v) - 1 n n i=1 f (v i ) ≤ R(F) + a ln 1 δ 2n</formula><p>where R(F) denotes the Rademacher complexity</p><formula xml:id="formula_47">R(F) = E σ sup f ∈F 2 n n i=1 σ i f (v i )</formula><p>and σ 1 , . . . , σ n are independent Rademacher variables.</p><p>To provide a proof sketch to Theorem 2, the following McDiarmid's inequality is needed.</p><p>Theorem 3 (McDiarmid's Inequality): Let V = {v 1 , . . . , v n } be a sample set of independent random variables. If there exists a &gt; 0, such that the following condition satisfies:</p><formula xml:id="formula_48">| f (V) -f (V i )| ≤ a ∀i ∈ {1, . . . , n}</formula><p>where V i represents the sample set V with the i th example replaced by an independent one. Then, for any &gt; 0, the following inequality holds:</p><formula xml:id="formula_49">Pr E v f (v) - 1 n n i=1 f (v i ) ≥ ≤ exp -2n 2 a 2</formula><p>where Pr{A} denotes the probability that event A occurs.</p><p>Proof Sketch of Theorem 2:</p><formula xml:id="formula_50">Let (V) = sup f W ∈F W E v f (v) - 1 n n i=1 f (v i ) and exp -2n 2 a 2 = δ.</formula><p>It can be proven that</p><formula xml:id="formula_51">| (V) -(V i )| ≤ a n .</formula><p>Using McDiarmid's inequality, we have, with probability at least 1 -δ</p><formula xml:id="formula_52">(V) ≤ E V (V) + a ln 1 δ 2n .</formula><p>It can also be proven that</p><formula xml:id="formula_53">E V (V) ≤ R(F).</formula><p>Hence, with probability at 1 -δ, we have sup</p><formula xml:id="formula_54">f ∈F E v f (v) - 1 n n i=1 f (v i ) ≤ R(F) + a ln( 1 δ ) 2n .</formula><p>To prove Theorem 1, we first need to upper bound the Rademacher complexity R(F W ). However, this is difficult, because the reconstruction error function f W (v) has a minimum operation. We will, therefore, use the following two lemmas to upper bound R(F W ) by finding a proper Gaussian process.</p><p>Lemma 1 (Slepian's Lemma): Let and be mean zero, separable Gaussian processes indexed by a common set S, such that</p><formula xml:id="formula_55">E( s 1 -s 2 ) 2 ≤ E( s 1 -s 2 ) 2 ∀s 1 , s 2 ∈ S. Then E sup s∈S s ≤ E sup s∈S s .</formula><p>Lemma 2 <ref type="bibr" target="#b65">[66]</ref>: The Gaussian complexity is related to the Rademacher complexity as follows:</p><formula xml:id="formula_56">R(F) ≤ π 2 G(F)</formula><p>where G(F) denotes the Gaussian complexity of function class F and</p><formula xml:id="formula_57">G(F) = E γ sup f ∈F 1 n n i=1 γ i f (v i )</formula><p>where γ 1 , . . . , γ n are independent standard normal variables. Next, we are going to upper bound R(F W ). For any W learnt by (3), we have</p><formula xml:id="formula_58">R(F W ) ≤ 4 √ πcr k √ n + 2 √ π c 2 k 2 √ n .</formula><p>Proof of Lemma 1:</p><formula xml:id="formula_59">Let W = i γ i min H i v i -W H i 2 and W = √ 8 is γ is v i , W e s + √ 2 ils γ ils W e l , W e s , l, s ∈ {1, . . . , k}</formula><p>where e 1 , . . . , e k are the natural bases. We have</p><formula xml:id="formula_60">E( W 1 -W 2 ) 2 = i (min H i v i -W 1 H i 2 -min H i v i -W 2 H i 2 ) 2 ≤ i (max H i v i -W 1 H i 2 -v i -W 2 H i 2 ) 2 ≤ i ⎛ ⎝ 8 max H i s H is v i , (W 1 -W 2 )e s 2 +2 max H i ls H ls H il e l , (W 1 W 2 -W 2 W 1 )e s 2 ⎞ ⎠</formula><p>For simplicity, we use H ∈ {H :</p><formula xml:id="formula_61">H i j ≤ 1, i = 1, . . . , k, j = 1, . . . , n.} instead of H ∈ H ≤ 8 is ( v i , W 1 e s -v i , W 2 e s ) 2 + 2 ils ( W 1 e l , W 1 e s -W 1 e l , W 2 e s ) 2 = E( W 1 -W 2 ) 2 .</formula><p>The second inequality holds (a + b) 2 ≤ 2a 2 + 2b 2 . Note that, for simplicity, we have used H ∈ {H : H i j ≤ 1, i = 1, . . . , k, j = 1, . . . , n.} instead of H ∈ H to find a Gaussian process. However, according to the hard constraints that tr(H L H T ) ≤ λ 1 and h 2 F ≤ λ 2 in (4), we should have used the condition H i j ≤ C(λ 1 , λ 2 ) ≤ 1, where C(λ 1 , λ 2 ) is a constant depending on the constants of λ 1 and λ 2 . Since there is a tradeoff between the bounds of columns of W and the entries of H , we can still use H ∈ {H : Using Slepian's Lemma, we have</p><formula xml:id="formula_62">H i j ≤ 1, i = 1, . . . , k, j = 1, . . .</formula><formula xml:id="formula_63">E sup W W ≤ E sup W W ≤ √ 8E sup W is γ is v i , W e s + √ 2E sup W ils γ ils W e l , W e s ≤ √ 8C(c, λ 1 , λ 2 )E s i γ is v i + √ 2C 2 (c, λ 1 , λ 2 )E l,s i γ ils ≤ √ 8C(c, λ 1 , λ 2 )rk √ n + √ 2C 2 (c, λ 1 , λ 2 )k 2 √ n.</formula><p>The third inequality holds, since the Cauchy-Schwartz inequality is used, and the last inequality holds, since Jansen's inequality is used and the Gaussian variables are orthogonal.</p><p>Using Lemma 2, we have</p><formula xml:id="formula_64">R(F W ) ≤ √ 2π n √ 8C(c, λ 1 , λ 2 )rk √ n + √ 2C 2 (c, λ 1 , λ 2 )k 2 √ n = 4 √ πC(c, λ 1 , λ 2 )rk √ n + 2 √ πC 2 (c, λ 1 , λ 2 )k 2 √ n .</formula><p>This concludes the proof. We now can prove Theorem 1 as follows.</p><p>Proof of Theorem 1: We have proven that</p><formula xml:id="formula_65">R(F W ) ≤ 4 √ πC(c, λ 1 , λ 2 )rk √ n + 2 √ πC 2 (c, λ 1 , λ 2 )k 2 √ n .</formula><p>Since the loss function f (v) ≤ v 2 ≤ r 2 , using Theorem 2, we have sup</p><formula xml:id="formula_66">f W ∈F W E v f W (v) - 1 n n i=1 f W (v i ) ≤ 4 √ πC(c, λ 1 , λ 2 )rk √ n + 2 √ π C 2 (c, λ 1 , λ 2 )k 2 √ n + r 2 ln 1 δ 2n<label>.</label></formula><p>Thus, we have</p><formula xml:id="formula_67">1 n E v V -W H 2 F ≤ 1 n V -W H 2 F + 4 √ πC(c, λ 1 , λ 2 )rk √ n + 2 √ πC 2 (c, λ 1 , λ 2 )k 2 √ n + r 2 ln 1 δ 2n .</formula><p>This concludes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTAL RESULTS</head><p>We next conducted saliency detection experiments on four widely used data sets to validate MRMF, namely Microsoft Research Asia (MSRA)-1000 <ref type="bibr" target="#b0">[1]</ref>, Dalian University of Technology -OMRON (DUT-OMRON) <ref type="bibr" target="#b26">[27]</ref>, Complex Scene Saliency Dataset (CSSD) <ref type="bibr" target="#b66">[67]</ref>, and extended complex scene saliency dataset (ECSSD) <ref type="bibr" target="#b67">[68]</ref>. MSRA-1000 contains 1000 images and is a subset of MSRA <ref type="bibr" target="#b68">[69]</ref>; example images are shown in Fig. <ref type="figure" target="#fig_1">2</ref>. The DUT-OMRON data set was collected by Yang et al. <ref type="bibr" target="#b26">[27]</ref> and contains 5168 images. The CSSD data set contains 200 scene images collected from BSD300, the visual object  classes data set, and the Internet. The ECSSD data set further extends CSSD to 1000 images and contains many semantically meaningful, but structurally complex, images.</p><p>Performance was evaluated by assessing precision and recall. Precision is the ratio of correctly detected salient pixels to the total detected salient pixels, while recall is the ratio of correctly detected salient pixels to the ground truth salient pixels. In addition, we introduced the F-measure to evaluate the overall performance. The F-measure value is defined as the weighted harmonic mean of precision value and recall</p><formula xml:id="formula_68">F α = (1 + α)Precision × Recall αPrecision + Recall (<label>26</label></formula><formula xml:id="formula_69">)</formula><p>where α = 0.3 following <ref type="bibr" target="#b0">[1]</ref> and <ref type="bibr" target="#b26">[27]</ref>. In addition, the DUT-OMRON data set also provide the eye fixations, and thus normalized scanpath saliency (NSS) is used to compare six methods in Table <ref type="table" target="#tab_2">II</ref>. Note that NSS can describe the deviation of predicted fixation patterns from the actual fixation map.</p><p>The details of experimental setup are shown in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Baseline Methods</head><p>The effectiveness of MRMF was validated against six other representative algorithms, namely MR <ref type="bibr" target="#b26">[27]</ref>, Frequency-tuned (FT) <ref type="bibr" target="#b0">[1]</ref>, spectral residual (SR) <ref type="bibr" target="#b50">[51]</ref>, boolean map (BM) <ref type="bibr" target="#b69">[70]</ref>, and regional contrast (RC) <ref type="bibr" target="#b70">[71]</ref>. Each of these methods has its own merits and limitations. MR and MRMF are based on background priors, FT and SR are the frequency domainbased methods, while BM computes saliency maps by analyzing the topological structure of Boolean maps. RC is a regional contrast-based salient object extraction algorithm that simultaneously evaluates global contrast differences and spatially weighted coherence scores. The original parameter settings for methods of comparison were used in each case <ref type="bibr" target="#b70">[71]</ref>. For MRMF, we have some important parameters to set, such as β and γ. For fair comparison, we randomly   It can be seen that MRMF has superior performance that preserves the object boundaries and highlights salient pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>Here, we propose the MRMF method for saliency detection. MRMF incorporates three types of information in a matrix factorization framework: 1) local spatial relationships; 2) labeled background queries; and 3) features from each region extracted from the input image. Since MRMF utilizes the feature information from image regions, more accurate ranking scores can be learned than using MR alone, and the saliency detection is improved. Experimental results on popular data sets illustrate the promise of MRMF. In the future, we will apply the proposed MRMF to other applications, e.g., visual tracking, image retrieval, and image classification. In addition, the MRMF relies on the OGM that is an iterative optimization procedure. Thus, it is necessary that considering parallelized MRMF for real applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Architecture of the proposed MRMF-based saliency detection.</figDesc><graphic coords="6,80.63,59.57,449.66,271.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Example images in the MSRA-1000 data set. Top: color image. Bottom: corresponding hand-annotation image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .Fig. 4 .</head><label>34</label><figDesc>Fig. 3. Experiments on the MSRA-1000 data set. (a)The precision-recall curves of all seven methods. (b)The precision, recall, and F-measure using an adaptive threshold<ref type="bibr" target="#b0">[1]</ref> for each of the datasets</figDesc><graphic coords="9,337.71,262.90,178.98,142.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Experiments on the CSSD data set. (a)The precision-recall curves of all seven methods. (b)The precision, recall, and F-measure using an adaptive threshold<ref type="bibr" target="#b0">[1]</ref> for each of the datasets.</figDesc><graphic coords="10,338.41,252.01,181.64,145.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Experiments on the ECSSD data set. (a)The precision-recall curves of all seven methods. (b)The precision, recall, and F-measure using an adaptive threshold<ref type="bibr" target="#b0">[1]</ref> for each of the datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Some results from the evaluated methods. selected 500 samples from an external data set to form the training set. B. Experimental Results and Analysis The precision-recall curves of all seven methods on the MSRA-1000, DUT-OMRON, CSSD, and ECSSD data sets are shown in Figs. 3(a), 4(a), 5(a), and 6(a), respectively. The precision, recall, and F-measure using an adaptive threshold [1] for each of the data sets are shown in Figs. 3(b), 4(b), 5(b), and 6(b). Overall, MRMF outperforms the others in terms of precision, recall, and NSS. In addition, some results from the evaluated methods are shown in Fig. 7.</figDesc><graphic coords="10,80.99,448.13,477.98,202.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="8,134.51,58.85,342.98,284.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I IMPORTANT</head><label>I</label><figDesc>NOTATIONS USED IN THIS PAPER AND THEIR DESCRIPTION</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>OGM-Based Algorithm for MRMF Input: Data matrix: V ∈ R m×n , graph Laplacian matrix: L ∈ R n×n , Parameter: γ ∈ (0, ∞), Parameter: β∈ (0, ∞), Output: Basis matrix: W ∈ R m×r , Coefficient matrix: H ∈ R r×n .</figDesc><table><row><cell cols="2">Step 1: Repeat</cell><cell></cell></row><row><cell cols="3">Step 2: Optimize W with (5).</cell></row><row><cell cols="3">Step 3: Optimize H f with Nesterov's Method.</cell></row><row><cell cols="2">Step 3.1: Repeat</cell><cell></cell></row><row><cell>Step 3.2:</cell><cell cols="2">Construct an auxiliary point Y k by using (11);</cell></row><row><cell>Step 3.3:</cell><cell>Calculate H k+1 f</cell><cell>by using (16);</cell></row><row><cell cols="3">Step 3.4: Until {Convergence}.</cell></row><row><cell>Step 4:</cell><cell cols="2">Optimize h with (19).</cell></row><row><cell cols="3">Step 5: Until {Convergence}.</cell></row><row><cell cols="3">Return: Ranking scores are H r .</cell></row><row><cell cols="3">of F and advances the search point to the minimum of the</cell></row><row><cell cols="2">proximal function</cell><cell></cell></row></table><note><p>At the auxiliary point Y k , OGM constructs the proximal function Algorithm 1</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II PERFORMANCE</head><label>II</label><figDesc>COMPARISONS OF SIX METHODS</figDesc><table /><note><p>ON DUT-OMRON DATA SET</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>We would like to thank all anonymous reviewers for their valuable suggestions and Mr. Tongliang Liu for helping improve the theoretical study of this paper.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the Chinese Academy of Sciences (CAS) and Locality Cooperation Projects under Grant ZNGZ-2011-012, in part by the Guangdong-CAS Strategic Cooperation Program under Grant 2012-B090400044, in part by the National Natural Science Foundation of China under Grant 6140051238, in part by the Guangdong Natural Science Funds under Grant 2014A030310252, in part by the Science and Technology Service Network Initiative through CAS under Grant KFJ-EW-STS-035, in part by the Shenzhen Technology Project under Grant JCYJ20130402113127502, Grant JCYJ20140901003939001, Grant JSGG20130624154940238, and Grant JCYJ20140417113430736, in part by the Guangdong Innovative Research Team Program under Grant 201001D0104648280, and in part by the Hubei Key Laboratory of Intelligent Vision Based Monitoring for Hydroelectric Engineering Program under Grant 2014KLA01.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Mingli Song (M'06-SM <ref type="bibr">'13)</ref>  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">New strategy for image and video quality assessment</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Electron. Imag</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11019" to="11020" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Does where you gaze on an image affect your perception of quality? Applying visual attention to image quality metric</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ninassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Le Meur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Le</forename><surname>Callet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Barbba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Image Process</title>
		<meeting>Int. Conf. Image ess<address><addrLine>San Antonio, TX, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-09">Sep. 2007</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="169" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Discriminant saliency for visual recognition from cluttered scenes</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null<address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-12">Dec. 2004</date>
			<biblScope unit="page" from="481" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Biologically inspired mobile robot vision localization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Siagian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Robot</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="861" to="873" />
			<date type="published" when="2009-08">Aug. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Modeling attention to salient proto-objects</title>
		<author>
			<persName><forename type="first">D</forename><surname>Walther</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1395" to="1407" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Attentional landmarks and active gaze control for visual SLAM</title>
		<author>
			<persName><forename type="first">S</forename><surname>Frintrop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jensfelt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Robot</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1054" to="1065" />
			<date type="published" when="2008-10">Oct. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Enhanced computer vision with Microsoft Kinect sensor: A review</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1318" to="1334" />
			<date type="published" when="2013-10">Oct. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Object detection in optical remote sensing images based on weakly supervised learning and high-level feature learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3325" to="3337" />
			<date type="published" when="2015-06">Jun. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Weakly-supervised cross-domain dictionary learning for visual recognition</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="42" to="59" />
			<date type="published" when="2014-08">Aug. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An objectoriented visual saliency detection framework based on sparse coding representations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2009" to="2021" />
			<date type="published" when="2013-12">Dec. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Principal component 2-dimensional long short-term memory for font recognition on single chinese characters</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/TCYB.2015.2414920</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<date type="published" when="2015-03">Mar. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient perceptual attentive superresolution</title>
		<author>
			<persName><forename type="first">N</forename><surname>Sadaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Karam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Image Process</title>
		<meeting>Int. Conf. Image ess<address><addrLine>Cairo, Egypt</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-11">Nov. 2009</date>
			<biblScope unit="page" from="3113" to="3116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">PCA feature extraction for change detection in multidimensional unlabeled data</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">I</forename><surname>Kuncheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Faithfull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="69" to="80" />
			<date type="published" when="2014-01">Jan. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Spatiotemporal saliency in dynamic scenes</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="171" to="177" />
			<date type="published" when="2010-01">Jan. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Robust object detection at regions of interest with an application in ball recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mitri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Frintrop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Pervolz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Surmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nuchter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Robot. Antom</title>
		<meeting>Int. Conf. Robot. Antom<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-04">Apr. 2005</date>
			<biblScope unit="page" from="125" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Soft margin multiple kernel learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="749" to="761" />
			<date type="published" when="2013-05">May 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Representing and retrieving video shots in human-centric brain imaging space</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2723" to="2736" />
			<date type="published" when="2013-07">Jul. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Image visual attention computation and application via the learning of object attributes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Vis. Appl</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1671" to="1683" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Saliency, scale and image description</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kadir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brady</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="83" to="105" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Removing label ambiguity in learning-based visual saliency estimation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1513" to="1525" />
			<date type="published" when="2012-04">Apr. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">VISUAL ATTENTION: Control, representation, and time course</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">E</forename><surname>Egeth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yantis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annu. Rev. Psychol</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="269" to="297" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rees</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Tsotsos</surname></persName>
		</author>
		<title level="m">Neurobiology of Attention</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Background prior based salient object detection via deep reconstruction residual</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1309" to="1321" />
			<date type="published" when="2015-08">Aug. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bottom-up saliency is a discriminant process</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis., Rio de Janeiro</title>
		<meeting>IEEE Int. Conf. Comput. Vis., Rio de Janeiro<address><addrLine>Brazil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-10">Oct. 2007</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Integrated learning of saliency, complex features, and object detectors from cluttered scenes</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-06">Jun. 2005</date>
			<biblScope unit="page" from="282" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Geodesic saliency using background priors</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-10">Oct. 2012</date>
			<biblScope unit="page" from="29" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Saliency detection via graph-based manifold ranking</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit<address><addrLine>Portland, OR, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06">Jun. 2013</date>
			<biblScope unit="page" from="3166" to="3173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Click prediction for Web image reranking using multimodal sparse coding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2019" to="2032" />
			<date type="published" when="2014-05">May 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Exploiting click constraints and multi-view features for image re-ranking</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="159" to="168" />
			<date type="published" when="2014-01">Jan. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Semisupervised multiview distance metric learning for cartoon synthesis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4636" to="4648" />
			<date type="published" when="2012-11">Nov. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning to rank using user clicks and visual features for image retrieval</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="767" to="779" />
			<date type="published" when="2015-04">Apr. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fast and accurate matrix completion via truncated nuclear norm regularization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2117" to="2130" />
			<date type="published" when="2013-09">Sep. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Convergence analysis of graph regularized non-negative matrix factorization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2151" to="2165" />
			<date type="published" when="2014-09">Sep. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Manifold adaptive experimental design for text categorization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="707" to="719" />
			<date type="published" when="2012-04">Apr. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A method of solving a convex programming problem with convergence rate O(1/k 2 )</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Nesterov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Soviet Math. Doklady</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="372" to="376" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998-11">Nov. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Paying attention to symmetry</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kootstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nederveen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>De Boer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Brit. Mach. Vis. Conf</title>
		<meeting>Brit. Mach. Vis. Conf</meeting>
		<imprint>
			<date type="published" when="2008-09">Sep. 2008</date>
			<biblScope unit="page" from="1115" to="1125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A coherent computational approach to model bottom-up visual attention</title>
		<author>
			<persName><forename type="first">O</forename><surname>Le Meur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Le</forename><surname>Callet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Barba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Thoreau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="802" to="817" />
			<date type="published" when="2006-05">May 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Predicting visual fixations on video based on low-level visual features</title>
		<author>
			<persName><forename type="first">O</forename><surname>Le Meur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Le</forename><surname>Callet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Barba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vis. Res</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">19</biblScope>
			<biblScope unit="page" from="2483" to="2498" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Saliency based on information maximization</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D B</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Tsotsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null<address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-06">Jun. 2005</date>
			<biblScope unit="page" from="155" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Dynamic visual attention: Searching for coding length increments</title>
		<author>
			<persName><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null<address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-12">Dec. 2008</date>
			<biblScope unit="page" from="681" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Computational attention: Modelisation and application to audio and image processing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mancas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<pubPlace>Mons, Belgium</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Numediart Inst. Creative Technol., Univ. Mons</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Topdown control of visual attention in object detection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Castelhano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Image Process</title>
		<meeting>Int. Conf. Image ess<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-09">Sep. 2003</date>
			<biblScope unit="page" from="253" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Modeling global scene factors in attention</title>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Opt. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1407" to="1418" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">SUN: A Bayesian framework for saliency using natural statistics</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vis</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">SLIC superpixels compared to state-of-the-art superpixel methods</title>
		<author>
			<persName><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Süsstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2274" to="2282" />
			<date type="published" when="2012-11">Nov. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Esaliency (extended saliency): Meaningful attention using stochastic image modeling</title>
		<author>
			<persName><forename type="first">T</forename><surname>Avraham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lindenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="693" to="708" />
			<date type="published" when="2010-04">Apr. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Graph-based visual saliency</title>
		<author>
			<persName><forename type="first">J</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null<address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-12">Dec. 2006</date>
			<biblScope unit="page" from="545" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Spatio-temporal saliency detection using phase spectrum of quaternion Fourier transform</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Pattern Recognit<address><addrLine>Anchorage, AK, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-06">Jun. 2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A novel multiresolution spatiotemporal saliency detection model and its applications in image and video compression</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="185" to="198" />
			<date type="published" when="2010-01">Jan. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Saliency detection: A spectral residual approach</title>
		<author>
			<persName><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Pattern Recognit<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06">Jun. 2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Frequencytuned salient region detection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hemami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Süsstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Pattern Recognit<address><addrLine>Miami, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-06">Jun. 2009</date>
			<biblScope unit="page" from="1597" to="1604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning to predict where humans look</title>
		<author>
			<persName><forename type="first">T</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis<address><addrLine>Kyoto, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-09">Sep. 2009</date>
			<biblScope unit="page" from="2106" to="2113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Centersurround patterns emerge as optimal predictors for human saccade targets</title>
		<author>
			<persName><forename type="first">W</forename><surname>Kienzle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">O</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Wichmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vis</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2009-05">May 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A feature-integration theory of attention</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Treisman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gelade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognit. Psychol</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="97" to="136" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Shifts in selective visual attention: Towards the underlying neural circuitry</title>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human Neurobiol</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="219" to="227" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Automatic foveation for video compression using a neurobiological model of visual attention</title>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1304" to="1318" />
			<date type="published" when="2004-10">Oct. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Static and space-time visual saliency detection by self-resemblance</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vis</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Exterior-point method for support vector machines</title>
		<author>
			<persName><forename type="first">V</forename><surname>Bloom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Griva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-R</forename><surname>Wolff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1390" to="1393" />
			<date type="published" when="2014-07">Jul. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Beyond bottom-up: Incorporating taskdependent influences into a computational model of spatial attention</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06">Jun. 2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Learning deep and wide: A spectral method for learning deep networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2303" to="2308" />
			<date type="published" when="2014-12">Dec. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Probabilistic multi-task learning for visual saliency estimation in video</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="150" to="165" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
		<title level="m">Nonlinear Programming</title>
		<meeting><address><addrLine>Belmont, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Athena Scientific</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">NeNMF: An optimal gradient method for nonnegative matrix factorization</title>
		<author>
			<persName><forename type="first">N</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2882" to="2898" />
			<date type="published" when="2012-06">Jun. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Rademacher and Gaussian complexities: Risk bounds and structural results</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mendelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="463" to="482" />
			<date type="published" when="2003-03">Mar. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Ledoux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Talagrand</surname></persName>
		</author>
		<title level="m">Probability in Banach Spaces: Isoperimetry and Processes</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Hierarchical saliency detection</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit<address><addrLine>Portland, OR, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06">Jun. 2013</date>
			<biblScope unit="page" from="1155" to="1162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Hierarchical saliency detection on extended CSSD</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1408.5418" />
		<imprint>
			<date type="published" when="2014-08">Aug. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Learning to detect a salient object</title>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="353" to="367" />
			<date type="published" when="2011-02">Feb. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Saliency detection: A Boolean map approach</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-12">Dec. 2013</date>
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Global contrast based salient region detection</title>
		<author>
			<persName><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit<address><addrLine>Providence, RI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-06">Jun. 2011</date>
			<biblScope unit="page" from="409" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">as an Engineer. He has authored or co-authored over 30 scientific articles. His current research interests include machine learning, computer vision, and cloud computing</title>
	</analytic>
	<monogr>
		<title level="m">Tao has served more than ten international journals, including the IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, the IEEE TRANSACTIONS ON MULTIMEDIA, the IEEE SIGNAL PROCESSING LETTERS, and PLOS-ONE. Jun Cheng received the B.Eng. and M.Eng. degrees from the University of Science and Technology of China</title>
		<meeting><address><addrLine>Xi&apos;an, China; Guangzhou, China; Kunming, China; Hefei, China; Shenzhen, China</addrLine></address></meeting>
		<imprint>
			<publisher>Chinese University of Hong Kong, Hong Kong</publisher>
			<date type="published" when="1999">1999 and 2002. 2006</date>
		</imprint>
		<respStmt>
			<orgName>Dapeng Tao received the B.E. degree from Northwestern Polytechnical University ; South China University of Technology ; Science and Engineering, Yunnan University</orgName>
		</respStmt>
	</monogr>
	<note>He is currently with the Shenzhen Institutes of Advanced Technology. as a Professor, and the Director of the Laboratory for Human Machine Control. His current research interests include computer vision, robotics, machine intelligence, and control</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
