<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-03-03">3 Mar 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Francesco</forename><surname>Croce</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Matthias</forename><surname>Hein</surname></persName>
						</author>
						<title level="a" type="main">Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-03-03">3 Mar 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2003.01690v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The field of defense strategies against adversarial attacks has significantly grown over the last years, but progress is hampered as the evaluation of adversarial defenses is often insufficient and thus gives a wrong impression of robustness. Many promising defenses could be broken later on, making it difficult to identify the state-of-the-art. Frequent pitfalls in the evaluation are improper tuning of hyperparameters of the attacks, gradient obfuscation or masking. In this paper we first propose two extensions of the PGD-attack overcoming failures due to suboptimal step size and problems of the objective function. We then combine our novel attacks with two complementary existing ones to form a parameter-free, computationally affordable and user-independent ensemble of attacks to test adversarial robustness. We apply our ensemble to over 40 models from papers published at recent top machine learning and computer vision venues. In all except one of the cases we achieve lower robust test accuracy than reported in these papers, often by more than 10%, identifying several broken defenses.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Adversarial samples, small perturbations of the input, with respect to some distance measure, which change the decision of a classifier, are a problem for safe and robust machine learning. In particular, they are a major concern when it comes to safety-critical applications. In recent years many defenses have been proposed but with more powerful or adapted attacks most of them could be broken <ref type="bibr" target="#b5">(Carlini &amp; Wagner, 2017b;</ref><ref type="bibr" target="#b2">Athalye et al., 2018;</ref><ref type="bibr" target="#b29">Mosbach et al., 2018)</ref>. Adversarial training <ref type="bibr" target="#b25">(Madry et al., 2018)</ref> is one of the few approaches which could not be defeated so far. Recent variations are using other losses <ref type="bibr" target="#b46">(Zhang et al., 2019b)</ref> and boost robustness via generation of additional training data 1 University of Tübingen, Germany. Correspondence to: F. Croce &lt;francesco.croce@uni-tuebingen.de&gt;. <ref type="bibr" target="#b7">(Carmon et al., 2019;</ref><ref type="bibr" target="#b0">Alayrac et al., 2019)</ref> or pre-training <ref type="bibr" target="#b18">(Hendrycks et al., 2019)</ref>. Another line of work are provable defenses, either deterministic <ref type="bibr" target="#b39">(Wong et al., 2018;</ref><ref type="bibr" target="#b11">Croce et al., 2019a;</ref><ref type="bibr" target="#b27">Mirman et al., 2018;</ref><ref type="bibr" target="#b15">Gowal et al., 2018)</ref> or based on randomized smoothing <ref type="bibr" target="#b24">(Li et al., 2019;</ref><ref type="bibr" target="#b23">Lecuyer et al., 2019;</ref><ref type="bibr" target="#b9">Cohen et al., 2019)</ref>. However, these are still not competitive with the empirical robustness of adversarial training for datasets like CIFAR-10 with large perturbations.</p><p>Due to the many broken defenses, the field is currently in a state where it is very difficult to judge the value of a new defense without an independent test. This limits the progress as it is not clear how to distinguish bad from good ideas. A seminal work to mitigate this issue are the guidelines for assessing adversarial defenses by <ref type="bibr" target="#b6">(Carlini et al., 2019)</ref>. However, as we see in our experiments, even papers trying to follow these guidelines can fail in obtaining a proper evaluation. In our opinion the reason is that at the moment there is no protocol which works reliably and autonomously, and does not need the fine-tuning of parameters for every new defense. Such protocol is what we aim at in this work.</p><p>The most popular method to test adversarial robustness is the PGD (Projected Gradient Descent) attack <ref type="bibr" target="#b25">(Madry et al., 2018)</ref>, as it is computationally cheap and performs well in many cases. However, it has been shown that even PGD can fail <ref type="bibr" target="#b29">(Mosbach et al., 2018;</ref><ref type="bibr" target="#b12">Croce et al., 2019b)</ref> leading to significant overestimation of robustness: we identify i) the fixed step size and ii) the widely used cross-entropy loss as two reasons for potential failure. As remedies we propose i) a new gradient-based scheme, Auto-PGD, which does not require a step size to be chosen (Sec. 3), and ii) an alternative loss function (Sec. 4). These novel tools lead to two variants of PGD whose only free parameter is the number of iterations, while everything else is adjusted automatically: this is the first piece of the proposed evaluation protocol.</p><p>Another cause of poor evaluations is the lack of diversity among the attacks used, as most papers rely solely on the results given by PGD or weaker versions of it like FGSM <ref type="bibr" target="#b14">(Goodfellow et al., 2015)</ref>. Of different nature are for example two existing attacks: the white-box FAB-attack <ref type="bibr">(Croce &amp; Hein, 2019)</ref> and the black-box Square Attack <ref type="bibr" target="#b1">(Andriushchenko et al., 2019)</ref>. Importantly, these methods have a limited amount of parameters which are shown to general-ize well across classifiers and datasets. Then, in Sec. 5, we combine our two new versions of PGD with FAB and Square Attack to form a parameter-free, computationally affordable and user-independent ensemble of complementary attacks to estimate adversarial robustness, named AutoAttack.</p><p>We test AutoAttack in a large-scale evaluation (Sec. 6) on over 40 classifiers from more than 25 papers proposing robust models, including randomized defenses, from recent leading conferences. Although using only five restarts for each of the three white-box attacks contained in AutoAttack, in all except one case the robust test accuracy obtained by AutoAttack is lower than the one reported in the original papers. For 13 models we reduce the robust accuracy by more than 10% and identify several broken defenses.</p><p>We do not argue that AutoAttack is the ultimate adversarial attack but rather that it should become the minimal test for any new defense, since it reliably reaches good performance in all tested models, without any hyperparameter tuning and at a relatively low computational cost. At the same time our large-scale evaluation identifies the current state-of-the-art and which of the recent ideas are actually effective. To find z it is common practice to define some surrogate function L such that solving the constrained optimization problem</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Adversarial examples and PGD</head><formula xml:id="formula_0">max z∈D L(g(z), c) s. th. γ(x orig , z) ≤ , z ∈ D (1)</formula><p>enforces z not to be assigned to class c. In image classification, the most popular threat models are based on l p -distances, i.e. γ(x, z) := z − x p , and D = [0, 1] d . Since the projection on the l p -ball for p ∈ {2, ∞} is available in closed form, Problem (1) can be solved with Projected Gradient Descent (PGD). Given f : R d −→ R, S ⊂ R d and the problem max</p><formula xml:id="formula_1">x∈R d f (x) s. th. x ∈ S ,<label>(2)</label></formula><p>the iterations of PGD are defined for k = 1, . . . , N iter as</p><formula xml:id="formula_2">x (k+1) =P S x (k) + η (k) ∇f (x (k) ) ,</formula><p>where η (k) is the step size at iteration k and P S is the projection onto S. Using the cross-entropy (CE) loss as objective L, <ref type="bibr" target="#b22">(Kurakin et al., 2017;</ref><ref type="bibr" target="#b25">Madry et al., 2018)</ref> introduced the so-called PGD-attack, which is currently the most popular white-box attack. In their formulation η (k) = η for every k, i.e. the step size is fixed, and as initial point x (0) either x orig or x orig + ζ is used, where ζ is randomly sampled such that x (0) satisfies the constraints. Moreover, steepest descent is done according to the norm considered in the threat model, which means that for l ∞ one uses the sign of the gradient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Auto-PGD:</head><p>A budget-aware step size-free variant of PGD</p><p>We identify three weaknesses in the standard formulation of the PGD-attack and how it is used in the context of adversarial robustness. First, the fixed step size is suboptimal, as even for convex problems this does not guarantee convergence, and the performance of the algorithm is highly influenced by the choice of its value, see e.g. <ref type="bibr" target="#b29">(Mosbach et al., 2018)</ref>. Second, the overall scheme is in general agnostic of the budget given to the attack: as we show, the loss plateaus after a few iterations, except for extremely small step sizes, which however do not translate into better results. As a consequence, judging the strength of an attack by the number of iterations is misleading, see also <ref type="bibr" target="#b6">(Carlini et al., 2019)</ref>. Finally, the algorithm is unaware of the trend, i.e. does not consider whether the optimization is evolving successfully and is not able to react to this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Auto-PGD (APGD) algorithm</head><p>In our automatic scheme we aim at fixing these issues. The main idea is to partition the available N iter iterations in an initial exploration phase, where one searches the feasible set for good initial points, and an exploitation phase, during which one tries to maximize the knowledge so far accumulated. The transition between the two phases is managed by progressively reducing the step size. In fact, a large step size allows to move quickly in S, whereas a smaller one more eagerly maximizes the objective function locally. However, the reduction of the step size is not a priori scheduled, but rather governed by the trend of the optimization: if the value of the objective grows sufficiently fast, then the step size is most likely proper, otherwise it is reasonable to reduce it. While the update step in APGD is standard, what distinguishes our algorithm from usual PGD is the choice of the step size across iterations, which is adapted to the overall budget and to the progress of the optimization, and that, once the step size is reduced, the maximization restarts from the best point so far found. We summarize our scheme in Algorithm 1 and analyze the main features in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gradient step:</head><p>The update of APGD follows closely the classic algorithm and only adds a momentum term. Let η (k)  Algorithm 1 APGD</p><formula xml:id="formula_3">1: Input: f , S, x (0) , η, N iter , W = {w 0 , . . . , w n } 2: Output: x max , f max 3: x (1) ← P S x (0) + η∇f (x (0) ) 4: f max ← max{f (x (0) ), f (x (1) )} 5: x max ← x (0) if f max ≡ f (x (0) ) else x max ← x (1) 6: for k = 1 to N iter −1 do 7: z (k+1) ← P S x (k) + η∇f (x (k) ) 8: x (k+1) ← P S x (k) + α(z (k+1) − x (k) ) +(1 − α)(x (k) − x (k−1) ) 9:</formula><p>if f (x (k+1) ) &gt; f max then 10:</p><formula xml:id="formula_4">x max ← x (k+1) and f max ← f (x (k+1) ) 11: end if 12: if k ∈ W then 13:</formula><p>if Condition 1 or Condition 2 then 14:</p><p>η ← η/2 15:</p><p>x (k+1) ← x max 16:</p><p>end if 17:</p><p>end if 18: end for be the step size at iteration k, then the update step is</p><formula xml:id="formula_5">z (k+1) = P S x (k) + η (k) ∇f (x (k) ) x (k+1) = P S x (k) + α • (z (k+1) − x (k) ) + (1 − α) • (x (k) − x (k−1) ) , (3)</formula><p>where α ∈ [0, 1] (we use α = 0.75) regulates the influence of the previous update on the current one. Since in the early iterations of APGD the step size is particularly large, we want to keep a bias from the previous steps.</p><p>Step size selection: We start with step size η (0) at iteration 0 (we fix η (0) = 2 ), and given a budget of N iter iterations, we identify checkpoints w 0 = 0, w 1 , . . . , w n at which the algorithm decides whether it is necessary to halve the current step size. We have two conditions:</p><formula xml:id="formula_6">1. wj −1 i=wj−1 1 f (x (i+1) )&gt;f (x (i) ) &lt; ρ • (w j − w j−1 ),</formula><p>2. η (wj−1) ≡ η (wj ) and f</p><formula xml:id="formula_7">(wj−1) max ≡ f (wj ) max , where f (k)</formula><p>max is the highest objective value found in the first k iterations. If one of the conditions is true, then step size at iteration k = w j is halved and η (k) := η (wj ) /2 for every k = w j + 1, . . . , w j+1 . Condition 1: counts in how many cases since the last checkpoint w j−1 the update step has been successful in increasing f . If this happened for at least a fraction ρ of the total update steps, then the step size is kept as the optimization is proceeding properly (we use ρ = 0.75). Condition 2: holds true if the step size was not reduced at the last checkpoint and there has been no improvement in the best found objective value since the last checkpoint. This prevents getting stuck in potential cycles.</p><p>Restarts from the best point: If at a checkpoint w j the step size gets halved, then we set x (wj +1) := x max , that is we restart at the point attaining the highest objective f max so far. This makes sense as reducing η leads to a more localized search, and this should be done in a neighborhood of the current best candidate solution.</p><p>Exploration vs exploitation: We want the algorithm to transit gradually from exploring the whole feasible set S to a local optimization. This transition is regulated by progressively reducing the step size and by the choice of when to decrease it, i.e. the checkpoints w j . In practice, we want to allow a relatively long initial exploration stage and then possibly update the step size more often moving toward exploitation. In fact, with smaller step sizes the improvements in the objective function are likely more frequent but also of smaller magnitude, while the importance of taking advantage of the whole input space is testified by the success of random restarts in usual PGD-attack. We fix the checkpoints as w j = p j N iter ≤ N iter , with p j ∈ [0, 1] defined as p 0 = 0, p 1 = 0.22 and p j+1 = p j + max{p j − p j−1 − 0.03, 0.06}.</p><p>Note that the period length p j+1 − p j is reduced in each step by 0.03 but they have at least a minimum length of 0.06.</p><p>While the proposed scheme has a few parameters which could be adjusted, we fix them to the values indicated so that the only free variable is the budget N iter .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Comparison of APGD to usual PGD</head><p>We compare the performance of our APGD to PGD (discussed above) in terms of the achieved CE-loss and robust accuracy. We focus on l ∞ -attacks with perturbation size . While other norms are possible too, l ∞ -balls are the most popular threat model. In this case the sign of the gradient is used as direction for the update step and the projection on the intersection of the l ∞ -ball and [0, 1] d is done by clipping. We attack the robust models on MNIST and CIFAR-10 from <ref type="bibr" target="#b25">(Madry et al., 2018)</ref> and <ref type="bibr" target="#b46">(Zhang et al., 2019b)</ref>. We run 1000 iterations of PGD with step sizes /t with t ∈ {0.5, 1, 2, 4, 10, 25, 100}, and APGD with a budget of N iter ∈ {25, 50, 100, 200, 400, 1000} iterations, recalling that APGD does not require a step size.</p><p>In Figure <ref type="figure">1</ref> we show the evolution of the current best average cross-entropy loss and robust accuracy (i.e. the percentage of points for which the attack could not find an adversarial . PGD vs APGD: best cross-entropy loss (top) and robust accuracy (bottom) so far found as function of iterations for the models of <ref type="bibr" target="#b25">(Madry et al., 2018)</ref> and TRADES <ref type="bibr" target="#b46">(Zhang et al., 2019b)</ref> for PGD (dashed lines) with different fixed step sizes (always 1000 iterations) and APGD (solid lines) with different budgets of iterations. APGD outperforms PGD for every budget of iterations in robust accuracy.</p><p>example) for 1000 points of the respective test sets over iterations. In all cases APGD achieves the highest values of the loss (higher is better as it is a maximization problem), often giving better final results than PGD with much fewer steps. Similarly, APGD attains always the lowest (better) robust accuracy, which means that it is a stronger adversarial attack on these models. It is also possible to observe the adaptive behaviour of APGD: when the budget of iterations is larger the value of the objective (the CE loss in this case) increases more slowly, but reaches higher values in the end. This is due to the longer exploration phase, which sacrifices a greedy improvement to finally get better results.</p><p>Conversely, the runs of PGD tend to plateau at suboptimal values, regardless of the choice of the step size. We present a similar comparison of APGD to PGD with Momentum (as we use a momentum term in our algorithm) in Sec. A.1 of the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Alternative loss</head><p>We first identify problems of the CE loss which can lead to gradient masking and then show how to overcome them with our new Difference of Logits Ratio loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Properties of the cross-entropy loss</head><p>Let z = g(x) ∈ R K be the output vector (logits) of the classifier g for an input x ∈ R d and p ∈ [0, 1] K the vector of the probabilities assigned to each class defined via the Softmax function p i = e zi / K j=1 e zj . If the correct class of x has index y, the cross-entropy loss at x is</p><formula xml:id="formula_8">CE(x, y) = − log p y = −z y + log K j=1 e zj , (<label>4</label></formula><formula xml:id="formula_9">)</formula><p>which is invariant to shifts of the logits, but not to rescaling.</p><p>In particular, this can lead to issues when one computes the gradient of the CE loss wrt x which is given by</p><formula xml:id="formula_10">∇ x CE(x, y) = − ∇ x z y + K i=1 e zi K j=1 e zj ∇ x z i = (−1 + p y ) ∇ x z y + i =y p i ∇ x z i .</formula><p>If p y ≈ 1 and consequently p i ≈ 0 for i = y, we get ∇ x CE(x, y) ≈ 0, which means that maximizing the CE loss with gradient-based methods becomes ineffective <ref type="bibr" target="#b33">(Papernot et al., 2016)</ref>. Notice that one can achieve p y ≈ 1 with a classifier h = αg equivalent to g (i.e. they take the same decision for every x) but rescaled by a constant α &gt; 0. Since</p><formula xml:id="formula_11">lim α→+∞ e αzy K j=1 e αzj = 1 if z y = max i z i 0 else</formula><p>, for sufficiently large α the decisions of h can have arbitrary high confidence. The finite arithmetic makes ∇ x CE(x, y) ≈ 0 to become ∇ x CE(x, y) = 0, since typically single precision is used and only exponents roughly in <ref type="bibr">[−127, 127]</ref> in the exponential function can be expressed (recently <ref type="bibr" target="#b40">(Wong et al., 2020)</ref> proposed even half precision). Thus the sign of the gradient (which in principle is not affected by the magnitude of the values) becomes zero and one does not get meaningful ascent directions (known as gradient masking).</p><p>To exemplify this phenomenon we run 100 iterations of the l ∞ PGD-attack on the cross-entropy loss on the CIFAR-10 model from <ref type="bibr" target="#b3">(Atzmon et al., 2019)</ref>, with = 0.031, dividing the logits by a factor α ∈ {1, 10 1 , 10 2 , 10 3 }. In Figure <ref type="figure" target="#fig_2">2</ref> we show the fraction of entries in the gradients of g/α (g is the original model) equal to zero and the robust accuracy achieved by the attack in dependency on α (we use 1000 test points, the gradient statistic is computed for correctly classified points). Without rescaling (α = 1) the gradient vanishes almost for every coordinate, so that PGD is ineffective and the robustness is significantly overestimated. However, simply rescaling the logits is sufficient to bring the output of the classifier into a range where the finite precision does not hinder the computation of the gradient and one gets a much more accurate robustness assessment.</p><p>Finally, we highlight that maximizing the CE loss minimizes the confidence of the classifier in the correct class. We will show that this helps to test randomized defenses, when misclassification alone is not enough for a successful attack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Difference of Logits Ratio Loss</head><p>As seen before, the fact that the gradient of the cross-entropy loss is not scaling invariant and the limited precision of the computations can lead to the effect of gradient masking. For our new loss the goal is to avoid the numerical problems of the softmax functions and directly work on the logits z ∈ R K of the classifier. In <ref type="bibr" target="#b4">(Carlini &amp; Wagner, 2017a)</ref> the difference of the logits</p><formula xml:id="formula_12">CW(x, y) = −z y + max i =y z i ,<label>(5)</label></formula><p>has been proposed as a loss for adversarial attacks, which is now called the Carlini-Wagner (CW)-loss. In contrast to the cross-entropy loss it has a direct interpretation in terms of the decision of the classifier, in particular if an adversarial example exists, then the global maximum of the CW-loss is positive. However, the CW-loss is not scaling invariant and thus again an extreme rescaling could in principle be used to induce gradient masking.</p><p>We propose the Difference of Logits Ratio (DLR)-loss which is instead both shift and rescaling invariant:</p><formula xml:id="formula_13">DLR(x, y) = − z y − max i =y z i z π1 − z π3 , (<label>6</label></formula><formula xml:id="formula_14">)</formula><p>where π is the ordering which sorts the components of z in decreasing order. Note that the required shift-invariance of the loss is achieved having a difference of logits also in the denominator. Maximizing DLR wrt x allows to find a point classified not in class y (DLR is positive only if arg max i z i = y) and, once that is achieved, it minimizes the score of class y compared to that of the other classes.</p><p>If x is correctly classified we have π 1 ≡ y and</p><formula xml:id="formula_15">DLR(x, y) = − z π1 − z π2 z π1 − z π3 .</formula><p>Note that under this condition DLR(x, y) ∈ [−1, 0]. The role of the normalization z π1 − z π3 is to push z π2 to z y = z π1 as it prefers points for which z y ≈ z π2 &gt; z π3 and thus is biased towards changing the decision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">DLR-loss versus CW-and CE-loss</head><p>While on normally trained models the difference between the losses is typically small, some adversarial defenses generate classifiers for which the performance of PGD-based attacks varies substantially according to the objective function chosen. We exemplify this in Table <ref type="table" target="#tab_0">1</ref>, where we compare the robust accuracy achieved by 100 iterations of l ∞ -PGD on the DLR-, CW-and CE-loss with three different fixed step sizes ( /10, , 2 ). We use an l ∞ -threat model with = 8/255 for 4 models on CIFAR-10 (M1 and M2 from <ref type="bibr">(Wang &amp; Zhang, 2019)</ref>, M3 from <ref type="bibr" target="#b45">(Zhang &amp; Xu, 2020)</ref> and M4 from <ref type="bibr">(Zhang &amp; Wang, 2019)</ref>), all trained to be robust. PGD DLR and APGD DLR both outperform the CWand CE-loss in terms of robust accuracy (lower is better) and the differences are significant. We note that the fixed step size 2 for CE-and DLR-loss works for these particular models better than APGD which yields competitive results. However, such fixed step size 2 is worse than the adaptive scheme in the comparison in Table <ref type="table" target="#tab_0">1</ref>, and in general it also performs worse than smaller step sizes, as one can see in the comparison provided in Sec. C.1 of the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Robustness evaluation through an ensemble of parameter-free attacks</head><p>We propose to group our two novel extensions of PGD, APGD CE and APGD DLR , with two existing complementary attacks, FAB <ref type="bibr">(Croce &amp; Hein, 2019)</ref> and Square Attack <ref type="bibr" target="#b1">(Andriushchenko et al., 2019)</ref>, into a reliable tool to evaluate robustness, named AutoAttack, which is automatic in the sense that it does not require to specify any free parameters 1 .</p><p>A key property of AutoAttack is the diversity of its components: while APGD is a white-box attack aiming at any adversarial example within an l p -ball, FAB minimizes the norm of the perturbation necessary to achieve a misclassification, and, although it relies on the gradient of the logits, it appears to be effective also on models affected by gradient masking as shown in <ref type="bibr">(Croce &amp; Hein, 2019)</ref>. On the other hand, Square Attack is a score-based black-box attack for norm bounded perturbations which uses random search and does not exploit any gradient approximation. It outperforms other black-box attacks in terms of query efficiency and success rate and has been shown to be even competitive with white-box attacks <ref type="bibr" target="#b1">(Andriushchenko et al., 2019)</ref>. Both methods have few parameters which generalize well across models and dataset, so that we will keep them fixed for all the experiments. The diversity within the ensemble helps for two reasons: first, there exist classifiers for which some of the attacks dramatically fail, but always at least one of them works well. Second, on the same model diverse attacks might have similar robust accuracy but succeed on different points: then considering the worst case over all attacks, as we do for AutoAttack, improves the performance. We focus on l ∞ -robustness, but all four attacks have an l 2 version, so that AutoAttack can test also l 2 -robustness.</p><p>1 AutoAttack is available at https://github.com/ fra31/auto-attack.</p><p>Since we want AutoAttack to return quickly a good approximation of the robust accuracy of any classifier, we provide to each attack a relatively limited computational budget: for APGD CE , APGD DLR and FAB we use for each attack 100 iterations and 5 random restarts, for Square Attack a single run with 5000 queries. While the exact runtime depends on the size, the robustness and even the framework of the target network, APGD is the fastest one, as it just requires one forward and one backward pass per iteration. Note that the computational budget for AutoAttack is on average similar to what has been used in the evaluation of the adversarial defenses which we test. All hyperparameters of the attacks are fixed for all experiments (see Sec. B of the Appendix).</p><p>In the following we show that although having limited budget and running fully automatic, without any hyperparameter tuning, AutoAttack leads to a reliable and cost-efficient evaluation of adversarial robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>In order to test the performance of AutoAttack, but also the individual performances of APGD CE and APGD DLR , we evaluate the robustness of adversarial defenses proposed in over 25 different papers from recent conferences like ICML, NeurIPS, ICLR, ICCV, CVPR, with around 40 different models. We consider models on MNIST and CIFAR-10, as the vast majority of papers uses these datasets for comparison. First, we report the results on deterministic defenses, later we extend our analysis to randomized ones, i.e. classifiers whose output has a stochastic component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deterministic defenses:</head><p>In Table <ref type="table">2</ref> we report the results on 33 models from recent defense papers (for some of them multiple networks are considered, possibly on both datasets). When possible we used the originals models (which are either publicly available or we obtained them via personal communication from the authors). Otherwise we retrained the models with the code released by the authors. Further details about the models and papers can be found in the Appendix (Sec. C).</p><p>For each classifier we report the clean accuracy and the robust accuracy, at the specified in the table, on the whole test set obtained by the individual attacks APGD CE , APGD DLR , FAB and Square Attack, together with our ensemble Au-toAttack, which counts as a success every point on which at least one of the four attacks finds an adversarial example (worst case evaluation). Additionally, we provide the reported robust accuracy of the respective papers (please note that in some cases their statistics are computed on a subset of the test set or not the exact same network), and the difference between our robust accuracy and the reported one. The reduction is highlighted in the last column of Table <ref type="table">2</ref> in red when negative (we have a lower robust accuracy). Finally, we boldface the attack obtaining the best individual Table <ref type="table">2</ref>. Robustness evaluation of l∞-adversarial defenses by AutoAttack. We report clean test accuracy, the robust accuracy of the individual attacks as well as the combined robust accuracy of AutoAttack (AA column). We also provide the robust accuracy reported in the original papers and compute the difference to the one of AutoAttack. If negative (in red) AutoAttack provides lower (better) robust accuracy. The statistics of our attack are computed on the full test set. We use the same for the l∞-attacks as used in the papers. robust accuracy and underline those which achieve a robust accuracy lower than reported.</p><p>Notably, in all except one case AutoAttack achieves a lower robust accuracy than the one reported in the original papers, and the improvement is larger than 10% in 13 out of 33 cases, larger than 30% in 8 cases. Thus AutoAttack would have provided almost always a better estimation of the robustness of the models than what has been used in the original evaluation, without any adaptation to the specific defense. Moreover, in the only case where it does not reduce the reported robust accuracy it is only 0.62% far from it, and this result has been obtained with a variant of PGD with 180 restarts and 200 iterations (see <ref type="bibr" target="#b0">(Alayrac et al., 2019)</ref>), Table <ref type="table">3</ref>. Robustness evaluation of randomized l∞-adversarial defenses by AutoAttack. We report the clean test accuracy (mean and standard deviation over 5 runs) and the robust accuracy of the individual attacks as well as the combined on of AutoAttack (again over 5 runs). We also provide the robust accuracy reported in the respective papers and compute the difference to the one of AutoAttack (negative means that AutoAttack is better). The statistics of our attack are computed on the whole test set except for the ones of <ref type="bibr" target="#b42">(Yang et al., 2019)</ref>, which are on 1000 test points due to the computational cost of this defense. The is the same as used in the papers. Standard benchmarks: The models from <ref type="bibr" target="#b25">(Madry et al., 2018)</ref> and <ref type="bibr" target="#b46">(Zhang et al., 2019b)</ref> have become standard benchmarks to judge the strength of new attacks. Several papers have tried, with large computational budget, to break these defenses. The SOTA robust accuracy of these models is reported at the bottom of Table <ref type="table">2</ref>. AutoAttack yields the new best robust accuracy on the MNIST model of <ref type="bibr" target="#b46">(Zhang et al., 2019b)</ref> and almost matches the SOTA on the others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APGD</head><p>Randomized defenses: Another line of adversarial defenses relies on adding to a classifier some stochastic component. In this case the output (and thus the decision) of the model might change across different runs for the same input. Thus we compute in Table <ref type="table">3</ref> the mean (standard deviation in brackets) of our statistics over 5 runs. Moreover the results of AutoAttack are given considering, for each point, the attack performing better on average, i.e. across 5 runs.</p><p>In order to adapt AutoAttack to handle randomized defenses we adopt two strategies. For the models from <ref type="bibr" target="#b17">(Grathwohl et al., 2020)</ref>, we attack the model named JEM-0 with the standard version of our ensemble, since the stochastic component has little influence, and then reuse the same adversarial examples on the other models (JEM-1 and JEM-10).</p><p>For the other classifiers, the direction for the update step in APGD is computed taking the average of 20 computations of the gradient at the same point (known as Expectation over Transformation <ref type="bibr" target="#b2">(Athalye et al., 2018)</ref>). In this case we use only 1 random start instead of 5. We do not run FAB here since it returns points on or very close to the decision boundary, so that even a small variation in the classifier is likely to undo the adversarial change, as confirmed by the bad performance on the models from <ref type="bibr" target="#b17">(Grathwohl et al., 2020)</ref>. We modify Square Attack so that it accepts an update if it reduces the target loss on average over 20 forward passes. As this costs more time we use only 1000 iterations for Square Attack. Table <ref type="table">3</ref> shows that AutoAttack achieves always lower robust accuracy than reported in the respective papers, with APGD CE being the best performing attack, closely followed by APGD DLR . In 7 out of 9 cases the improvement is significant, larger than 10% (and in 3/9 cases larger than 25%). Thus AutoAttack is also suitable for the evaluation of randomized defenses.</p><p>Analysis of SOTA of adversarial defenses: While the main goal of the evaluation is to show the effectiveness of AutoAttack, at the same time it provides an assessment of the SOTA of adversarial defenses. The most robust defenses rely on variations or fine-tuning of adversarial training introduced in <ref type="bibr" target="#b25">(Madry et al., 2018)</ref>. One step forward has been made by methods which use additional data for training, like <ref type="bibr" target="#b7">(Carmon et al., 2019)</ref> and <ref type="bibr" target="#b0">(Alayrac et al., 2019)</ref> which exploit unlabeled data to improve robustness. Moreover, several defenses which claim SOTA robustness turn out to be significantly less robust than <ref type="bibr" target="#b25">(Madry et al., 2018)</ref>. While this paper contains up to our knowledge the largest independent evaluation of current adversarial defenses, this is by no means an exhaustive survey due to the large number of publications in this area. Several authors did not reply to our request or were not able to provide models (or at least code).</p><p>We thank all the authors who helped us in this evaluation.</p><p>We hope that AutoAttack contributes to a faster development of adversarial defenses due to a better evaluation.</p><p>Further experiments: In order to provide a deeper comparison of APGD to PGD and PGD with Momentum, on both CE and DLR losses, we report in Sec. C.1 of the Appendix (Tables <ref type="table" target="#tab_9">8 and 9</ref>) the performance of all the PGD-based attacks on the models in Table <ref type="table">2</ref>. Moreover, in Sec. D of the Appendix we introduce a stronger version of our ensemble, AutoAttack+, which exploits a targeted extension of the DLR loss and of FAB, used to further test the most effective defenses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Auto-PGD</head><p>In the main paper we compare in Sec. 3.2 the performance of Auto-PGD versus PGD with different fixed step sizes as a function of the iterations to show that Auto-PGD adapts automatically to a given budget of iterations and to a good step size, achieving larger (better) loss and smaller (better) robust test accuracy than PGD. In the Appendix in Sec. A.1 we show that the same holds if we compare Auto-PGD to PGD with Momentum. Moreover, we provide in Sec. C.1 a full comparison of PGD, PGD with Momentum, both with three different step sizes, and Auto-PGD. It turns out that Auto-PGD outperforms PGD with a fixed step size showing again that the automatic adaptation of Auto-PGD works well. Moreover, we provide implementation details and more details about our large-scale comparison of adversarial defenses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Comparison of APGD to PGD with Momentum</head><p>We repeat the experiment of Sec. 3.2, this time comparing the performance of APGD to that of PGD with Momentum (the same momentum as used in the update-step of APGD) for various fixed step sizes as a function of the iterations.</p><p>The results of the comparison of APGD vs PGD with Momentum are shown in Figure <ref type="figure" target="#fig_3">3</ref>. While the momentum term mostly improves the performance of plain PGD, the general behaviour remains similar, that is the best loss tends to plateau after a fraction of the total number of iterations (1000) and different step sizes work best for different models. Similar to the comparison with PGD, APGD outperforms PGD with Momentum in terms of highest loss achieved for each budget of iterations, and in most of the cases also in terms of lowest robust accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. AutoAttack: implementation details</head><p>We report all the hyperparameters used in AutoAttack. For APGD we set the momentum coefficient to α = 0.75, ρ = 0.75 (Condition 1), initial step size η (0) = 2 where is the maximum l p -norm of the perturbations. For FAB we keep the standard hyperparameters according to the implementation of Advertorch<ref type="foot" target="#foot_4">2</ref>  <ref type="bibr" target="#b13">(Ding et al., 2019)</ref>. For Square Attack we follow the original code<ref type="foot" target="#foot_5">3</ref> , using as initial value for the size of the squares p = 0.8. Moreover, we use the piecewise constant schedule for p as it is suggested for a query limit of 10000 without any rescaling (although we use only up to 5000 queries).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experiments</head><p>Deterministic defenses: We had to omit some information in our evaluation of deterministic defenses in Table <ref type="table">2</ref> in the main paper. This is reported in Table <ref type="table" target="#tab_5">5</ref> in the Appendix: the venue where each paper appeared (we add "R" to indicate that the paper was submitted at the venue but rejected) and the source of the models, that is whether they are publicly available, we got them via private communication from the authors or we retrained them according to official implementations. We also indicate the architecture of the classifiers: interestingly, larger models do not imply better robustness.</p><p>The models of <ref type="bibr" target="#b31">(Pang et al., 2019)</ref> and <ref type="bibr" target="#b32">(Pang et al., 2020)</ref> appears twice as they are evaluated both with and without the addition of adversarial training. The models of <ref type="bibr">(Wang &amp; Zhang, 2019)</ref> are the networks named "R-MOSA-LA-4" and "R-MC-LA-4".</p><p>Randomized defenses: Similar to the deterministic defenses we report additional information which had to be omitted in Table <ref type="table">3</ref>. We report in Table <ref type="table" target="#tab_6">6</ref> the venue where each paper appeared and the source of the models, that is if they are publicly available or we got them via private communication from the authors. In Table <ref type="table" target="#tab_7">7</ref> we report the mean and standard deviation of the robust accuracy achieved by each attack (note that we have a randomized defense thus the outcome is not deterministic). As mentioned in Sec. 6, to evaluate an attack we compute 5 times the classification accuracy of the target model on the same adversarial examples. For AutoAttack, we form a batch with for each test point the adversarial example among those of our different attacks which were misclassified most often in the 5 runs, and then compute the robust accuracy of this batch.</p><p>For the models of <ref type="bibr" target="#b17">(Grathwohl et al., 2020)</ref>, only plots of robust accuracy vs size of the perturbation are available in their paper. Thus the reported values are obtained by extrapolating the values in the tables from the most recent version of the paper, that is the one available at https: //arxiv.org/abs/1912.03263v2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Comparison PGD vs PGD with Momentum vs APGD</head><p>In order to compare the performance of APGD to that of standard PGD when testing adversarial defenses, we run PGD and PGD with Momentum on the models used in Sec. 6, with the same budget of APGD, i.e. 100 iterations and 5 restarts. We use three step sizes: /10, /4 and 2 , where is the threshold at which the robust accuracy is computed. We repeat the experiment with both the cross-entropy loss to be comparable to APGD CE (Table <ref type="table" target="#tab_8">8</ref>) and our new DLR loss as in APGD DLR (Table <ref type="table" target="#tab_9">9</ref>).</p><p>In Tables <ref type="table" target="#tab_9">8 and 9</ref> we see that APGD achieves with both losses the lowest (lower is better) robust accuracy in most of the cases (23 out of 33 with CE loss, 25/33 with DLR loss). Moreover, while on average PGD with Momentum and step size /4 is slightly better than the other versions of PGD, the best version of PGD varies with the classifier, and in particular there is large variance among the results obtained by different step sizes. This emphasizes again why a step size free method is essential for a reliable robustness evaluation which is as much as possible independent of the user.</p><p>For the attacks on the DLR loss, the only cases where PGD is more than 1% better than APGD consist of the models also used in the experiment in Sec. 4.3. In these cases the very large step size 2 outperforms all other step sizes, which in most of the other models works significantly worse than the smaller step sizes. It is likely that these defenses modify the loss landscape to make it unsuitable for standard gradientbased optimization, and an informed random search (as PGD with such a large step size can be seen) works better. The same happens with the CE loss, where PGD yields the best results (with a non-negligible gap to APGD) also on the models of <ref type="bibr" target="#b20">(Kim &amp; Wang, 2020)</ref> and <ref type="bibr" target="#b36">(Taghanaki et al., 2019)</ref>. However, notice that for these classifiers optimizing the DLR loss is significantly more effective than using the CE loss.</p><p>Finally, in all these cases AutoAttack achieves significantly lower robust accuracy than any version of PGD as FAB attack is less affected by this kind of gradient obfuscation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Improving AutoAttack with targeted attacks</head><p>As mentioned earlier, the model from <ref type="bibr" target="#b0">(Alayrac et al., 2019)</ref> is the only one where AutoAttack does not improve upon the robust accuracy provided in the original paper. In this case, <ref type="bibr" target="#b0">(Alayrac et al., 2019)</ref> achieved the reported robust accuracy with the attack from <ref type="bibr" target="#b16">(Gowal et al., 2019)</ref>, which runs PGD optimizing the loss</p><formula xml:id="formula_16">−z y + z t , for every t ∈ {1, . . . , K} \ {y}.</formula><p>This means that it tries to make the logit relative to the class t larger than that of the correct class y, for every possible target class t = y. However, note that they used 20 restarts per class and 200 steps.</p><p>In order to explore the possibility that for some models targeted attacks are more effective, we introduce targeted versions of APGD and FAB, namely APGD T and FAB T . Note that, despite the name, these attacks are not, strictly speaking, targeted, as we do not check whether the found adversarial examples actually belong to the target class t, but rather consider successful any misclassification, since our goal is to check general robustness.</p><p>Targeted APGD: We adapt our DLR loss to this new scenario introducing</p><formula xml:id="formula_17">Targeted-DLR(x, y) = − z y − z t z π1 − (z π3 + z π4 )/2 , (<label>7</label></formula><formula xml:id="formula_18">)</formula><p>where we keep the notation of Sec. 4. Thus, we preserve both the shift and scaling invariance of DLR loss, while aiming at getting z t &gt; z y . Moreover, we modify the denominator in (6) to ensure that the loss is never constant.</p><p>Targeted FAB: As proposed in the <ref type="bibr">(Croce &amp; Hein, 2019)</ref>, the targeted version of FAB considers only the linearization of the decision boundary between the target and the correct class, instead of all the K − 1 possible hyperplanes as in the untargeted attack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Experiments</head><p>To test this extension of AutoAttack, named AutoAttack+, we run 9 restarts (one for each target class) of APGD T and of FAB T on the top 3 entries (the most robust models) of Table <ref type="table">2</ref> and on the standard benchmarks of <ref type="bibr" target="#b25">(Madry et al., 2018)</ref> and <ref type="bibr" target="#b46">(Zhang et al., 2019b)</ref>. We report in On CIFAR-10, the targeted version helps to improve the results of AutoAttack, in particular APGD T is slightly better than AutoAttack in all the cases. AutoAttack+ reduces also the robust accuracy of the network of <ref type="bibr" target="#b0">(Alayrac et al., 2019)</ref> below the reported robust accuracy reported in their paper where the attack of <ref type="bibr" target="#b16">(Gowal et al., 2019)</ref> is granted a computational budget much larger that what we use here (we use 15 + 18 restarts each with 100 steps for AutoAttack+ plus the 5000 queries of Square Attack, whereas they use 180 restarts with 200 steps -this is roughly a 6 times higher computational budget), with an improvement of 0.91% respect to AutoAttack. Conversely, on MNIST there is almost no advantage in adding the targeted versions of APGD and FAB, which are mostly worse than the corresponding untargeted ones. However, for the MNIST model of <ref type="bibr" target="#b46">(Zhang et al., 2019b)</ref> there is a small improvement. Note that AutoAttack+ improves the SOTA on the benchmark datasets of <ref type="bibr" target="#b25">(Madry et al., 2018)</ref> and <ref type="bibr" target="#b46">(Zhang et al., 2019b)</ref> for MNIST and CIFAR-10. Moreover, AutoAttack+ (which includes AutoAttack) yields a lower robust accuracy than reported in all the evaluations in the respective papers and this for a computational cost which is similar to the one in the current state-of-the-art papers.</p><p>Again, it has to be emphasized that this is achieved without optimizing a single hyperparameter. All the attacks, in particular our new APGD, run with the mentioned parameters on all datasets and models.    <ref type="bibr" target="#b36">(Taghanaki et al., 2019)</ref> 98.86 0.00 0.00 0.00 0.00 0.00 0.00 0.00</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Given a K-class classifierg : D ⊆ R d −→ R K , taking decisions according to arg max k=1,...,K g k (•), a point x orig ∈ R dwith correct classification c, and the threat model {z ∈ D | γ(x orig , z) ≤ } (feasible set of the attack) where γ is some distance measure and &gt; 0. Then z is an adversarial sample for g at x orig wrt the threat model if arg max k=1,...,K g k (z) = c, γ(x orig , z) ≤ and z ∈ D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>(</head><label></label><figDesc>Figure1. PGD vs APGD: best cross-entropy loss (top) and robust accuracy (bottom) so far found as function of iterations for the models of<ref type="bibr" target="#b25">(Madry et al., 2018)</ref> and TRADES<ref type="bibr" target="#b46">(Zhang et al., 2019b)</ref> for PGD (dashed lines) with different fixed step sizes (always 1000 iterations) and APGD (solid lines) with different budgets of iterations. APGD outperforms PGD for every budget of iterations in robust accuracy.</figDesc><graphic url="image-5.png" coords="4,71.64,179.84,127.85,95.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Percentage of zeros in the gradients and robust accuracy, computed by PGD on the CE loss, of the classifiers g/α, where g is the CIFAR-10 model of (Atzmon et al., 2019) and α a rescaling factor. The performance of PGD depends on the scale of the logits.</figDesc><graphic url="image-9.png" coords="5,91.21,60.76,159.97,119.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>(Figure 3 .</head><label>3</label><figDesc>Figure3. PGD with Momentum vs APGD: best cross-entropy loss (top) and robust accuracy (bottom) so far found as function of iterations for the models of<ref type="bibr" target="#b25">(Madry et al., 2018)</ref> and TRADES<ref type="bibr" target="#b46">(Zhang et al., 2019b)</ref> for PGD with a momentum term (α = 0.75, as used in APGD) (dashed lines) with different fixed step sizes (always 1000 iterations) and APGD (solid lines) with different budgets of iterations. APGD outperforms PGD for every budget of iterations in robust accuracy.</figDesc><graphic url="image-14.png" coords="16,71.64,226.66,127.85,95.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Performance</figDesc><table><row><cell>attack</cell><cell>step</cell><cell>M1</cell><cell>M2</cell><cell>M3</cell><cell>M4</cell></row><row><cell></cell><cell cols="5">/10 63.4 72.7 74.3 69.9</cell></row><row><cell>PGDCE</cell><cell></cell><cell cols="4">61.8 72.3 73.1 65.7</cell></row><row><cell></cell><cell>2</cell><cell cols="4">57.6 65.1 66.8 61.5</cell></row><row><cell></cell><cell cols="5">/10 61.7 70.1 70.8 58.5</cell></row><row><cell>PGDCW</cell><cell></cell><cell cols="4">58.9 69.0 69.2 54.8</cell></row><row><cell></cell><cell>2</cell><cell cols="4">55.7 61.6 62.4 51.8</cell></row><row><cell></cell><cell cols="5">/10 65.4 58.6 62.7 58.5</cell></row><row><cell>PGDDLR</cell><cell></cell><cell cols="4">45.9 42.1 54.8 54.0</cell></row><row><cell></cell><cell>2</cell><cell cols="4">42.2 39.7 49.7 50.4</cell></row><row><cell>APGDCE</cell><cell>-</cell><cell cols="4">59.0 70.8 73.1 64.4</cell></row><row><cell>APGDDLR</cell><cell>-</cell><cell cols="4">45.9 41.0 53.5 52.7</cell></row></table><note>of PGD-type attacks when maximizing Cross Entropy (CE), Carlini-Wagner (CW) and our (DLR) losses. We report the robust accuracy on 1000 points of four models, specifically M1 and M2 from(Wang &amp; Zhang, 2019), M3 from<ref type="bibr" target="#b45">(Zhang &amp; Xu, 2020)</ref> and M4 from(Zhang &amp; Wang, 2019).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Robustness evaluation through AutoAttack+. We report the robust accuracy attained by AutoAttack (AA), the targeted attacks APGDT and FABT, and the pointwise worst-case combining them (i.e. AA+). Moreover, we show the robust accuracy reported in the original papers, and the reduction of it due to AutoAttack+.</figDesc><table><row><cell># paper</cell><cell>clean</cell><cell cols="2">AA APGDT</cell><cell>FABT</cell><cell>AA+</cell><cell>report.</cell><cell>reduct.</cell></row><row><cell>CIFAR-10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1 (Carmon et al., 2019)</cell><cell>89.69</cell><cell>59.65</cell><cell>59.54</cell><cell>60.16</cell><cell>59.50</cell><cell>62.5</cell><cell>-3.00</cell></row><row><cell>2 (Alayrac et al., 2019)</cell><cell>86.46</cell><cell>56.92</cell><cell>56.27</cell><cell>57.06</cell><cell>56.01</cell><cell>56.30</cell><cell>-0.29</cell></row><row><cell>3 (Hendrycks et al., 2019)</cell><cell>87.11</cell><cell>54.99</cell><cell>54.94</cell><cell>55.25</cell><cell>54.86</cell><cell>57.4</cell><cell>-2.54</cell></row><row><cell>4 (Zhang et al., 2019b)</cell><cell>84.92</cell><cell>53.18</cell><cell>53.10</cell><cell>53.46</cell><cell>53.04</cell><cell>56.43</cell><cell>-3.39</cell></row><row><cell>5 (Madry et al., 2018)</cell><cell>87.14</cell><cell>44.29</cell><cell>44.28</cell><cell>44.70</cell><cell>44.01</cell><cell>47.04</cell><cell>-3.03</cell></row><row><cell>MNIST</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1 (Zhang et al., 2019b)</cell><cell>99.48</cell><cell>92.76</cell><cell>93.62</cell><cell>94.68</cell><cell>92.74</cell><cell>95.60</cell><cell>-2.86</cell></row><row><cell>2 (Madry et al., 2018)</cell><cell>98.53</cell><cell>88.43</cell><cell>90.57</cell><cell>92.61</cell><cell>88.43</cell><cell>89.62</cell><cell>-1.19</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Table 4 the robust accuracy achieved by AutoAttack and by the individual targeted versions the attacks. Moreover, we compute the robust accuracy of AutoAttack+ combining the adversarial examples provided the four attacks in AutoAttack, APGD T and FAB T .</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Robustness evaluation of adversarial defenses with AutoAttack. For each model, we report architecture, source, venue, clean accuracy and combined robust accuracy given by AutoAttack (AA column). We also provide the reported robust accuracy in the original papers and compute the difference to the one of AutoAttack.</figDesc><table><row><cell># paper</cell><cell>model</cell><cell>source</cell><cell>venue</cell><cell>clean</cell><cell cols="2">AA reported</cell><cell>reduct.</cell></row><row><cell>CIFAR-10 -= 8/255</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1 (Carmon et al., 2019)</cell><cell cols="3">WideResNet-28-10 available NeurIPS 19</cell><cell>89.69</cell><cell>59.65</cell><cell>62.5</cell><cell>-2.85</cell></row><row><cell>2 (Alayrac et al., 2019)</cell><cell cols="3">WideResNet-106-8 available NeurIPS 19</cell><cell>86.46</cell><cell>56.92</cell><cell>56.30</cell><cell>0.62</cell></row><row><cell>3 (Hendrycks et al., 2019)</cell><cell cols="3">WideResNet-28-10 available ICML 19</cell><cell>87.11</cell><cell>54.99</cell><cell>57.4</cell><cell>-2.41</cell></row><row><cell>4 (Kumari et al., 2019)</cell><cell cols="3">WideResNet-34-10 available IJCAI 19</cell><cell>87.80</cell><cell>49.40</cell><cell>53.04</cell><cell>-3.64</cell></row><row><cell>5 (Mao et al., 2019)</cell><cell cols="2">WideResNet-34-10 authors</cell><cell>NeurIPS 19</cell><cell>86.21</cell><cell>47.66</cell><cell>50.03</cell><cell>-2.37</cell></row><row><cell>6 (Zhang et al., 2019a)</cell><cell cols="3">WideResNet-34-10 retrained NeurIPS 19</cell><cell>87.20</cell><cell>45.06</cell><cell>47.98</cell><cell>-2.92</cell></row><row><cell>7 (Madry et al., 2018)</cell><cell cols="3">WideResNet-34-10 available ICLR 18</cell><cell>87.14</cell><cell>44.29</cell><cell>47.04 1</cell><cell>-2.75</cell></row><row><cell>8 (Pang et al., 2020)</cell><cell>ResNet32</cell><cell cols="2">available ICLR 20</cell><cell>80.89</cell><cell>43.78</cell><cell>55.0</cell><cell>-11.22</cell></row><row><cell>9 (Wong et al., 2020)</cell><cell>ResNet18</cell><cell cols="2">available ICLR 20</cell><cell>83.34</cell><cell>43.38</cell><cell>46.06</cell><cell>-2.68</cell></row><row><cell>10 (Shafahi et al., 2019)</cell><cell cols="3">WideResNet-34-10 available NeurIPS 19</cell><cell>86.11</cell><cell>41.58</cell><cell>46.19</cell><cell>-4.61</cell></row><row><cell>11 (Zhang &amp; Wang, 2019)</cell><cell cols="3">WideResNet-28-10 available NeurIPS 19</cell><cell>89.98</cell><cell>38.78</cell><cell>60.6</cell><cell>-21.82</cell></row><row><cell cols="3">12 (Moosavi-Dezfooli et al., 2019) WideResNet-28-10 authors</cell><cell>CVPR 19</cell><cell>83.11</cell><cell>38.67</cell><cell>41.4</cell><cell>-2.73</cell></row><row><cell>13 (Zhang &amp; Xu, 2020)</cell><cell cols="3">WideResNet-28-10 available ICLR 20 R</cell><cell>90.25</cell><cell>38.57</cell><cell>68.7</cell><cell>-30.13</cell></row><row><cell>14 (Kim &amp; Wang, 2020)</cell><cell cols="3">WideResNet-34-10 available ICLR 20 R</cell><cell>91.51</cell><cell>36.10</cell><cell>57.23</cell><cell>-21.13</cell></row><row><cell>15 (Jang et al., 2019)</cell><cell>ResNet20</cell><cell cols="2">available ICCV 19</cell><cell>78.91</cell><cell>35.09</cell><cell>37.40</cell><cell>-2.31</cell></row><row><cell cols="2">16 (Moosavi-Dezfooli et al., 2019) ResNet18</cell><cell>authors</cell><cell>CVPR 19</cell><cell>80.41</cell><cell>33.76</cell><cell>36.3</cell><cell>-2.54</cell></row><row><cell>17 (Wang &amp; Zhang, 2019)</cell><cell cols="3">WideResNet-28-10 available ICCV 19</cell><cell>92.80</cell><cell>30.96</cell><cell>58.6</cell><cell>-27.64</cell></row><row><cell>18 (Wang &amp; Zhang, 2019)</cell><cell cols="3">WideResNet-28-10 available ICCV 19</cell><cell>92.82</cell><cell>29.07</cell><cell>66.9</cell><cell>-37.83</cell></row><row><cell>19 (Mustafa et al., 2019)</cell><cell>ResNet110</cell><cell cols="2">available ICCV 19</cell><cell>89.16</cell><cell>0.55</cell><cell>32.32</cell><cell>-31.77</cell></row><row><cell>20 (Chan et al., 2020)</cell><cell cols="3">WideResNet-34-10 retrained ICLR 20</cell><cell>93.79</cell><cell>0.18</cell><cell>15.5</cell><cell>-15.32</cell></row><row><cell>21 (Pang et al., 2020)</cell><cell>ResNet110</cell><cell cols="2">available ICLR 20</cell><cell>93.52</cell><cell>0.00</cell><cell>31.4</cell><cell>-31.40</cell></row><row><cell>CIFAR-10 -= 0.031</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1 (Zhang et al., 2019b)</cell><cell cols="3">WideResNet-34-10 available ICML 19</cell><cell>84.92</cell><cell>53.18</cell><cell>56.43 2</cell><cell>-3.25</cell></row><row><cell>2 (Atzmon et al., 2019)</cell><cell>ResNet18</cell><cell cols="2">available NeurIPS 19</cell><cell>81.30</cell><cell>40.61</cell><cell>43.17</cell><cell>-2.56</cell></row><row><cell>3 (Xiao et al., 2020)</cell><cell>DenseNet121</cell><cell cols="2">available ICLR 20</cell><cell>79.28</cell><cell>17.99</cell><cell>52.4</cell><cell>-34.41</cell></row><row><cell>CIFAR-10 -= 4/255</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1 (Song et al., 2019)</cell><cell>ConvNet</cell><cell cols="2">available ICLR 19</cell><cell>84.81</cell><cell>56.64</cell><cell>58.1</cell><cell>-1.46</cell></row><row><cell>CIFAR-10 -= 0.02</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1 (Pang et al., 2019)</cell><cell>ResNet20 (x3)</cell><cell cols="2">available ICML 19</cell><cell>91.22</cell><cell>2.00</cell><cell>34.0</cell><cell>-32.00</cell></row><row><cell>2 (Pang et al., 2019)</cell><cell>ResNet20 (x3)</cell><cell cols="2">available ICML 19</cell><cell>93.44</cell><cell>0.02</cell><cell>30.4</cell><cell>-30.38</cell></row><row><cell>MNIST -= 0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1 (Zhang et al., 2019b)</cell><cell></cell><cell cols="2">available ICML 19</cell><cell>99.48</cell><cell>92.76</cell><cell>95.60 3</cell><cell>-2.84</cell></row><row><cell>2 (Atzmon et al., 2019)</cell><cell></cell><cell cols="2">available NeurIPS 19</cell><cell>99.35</cell><cell>90.85</cell><cell>97.35</cell><cell>-6.50</cell></row><row><cell>3 (Madry et al., 2018)</cell><cell></cell><cell cols="2">available ICLR 18</cell><cell>98.53</cell><cell>88.43</cell><cell>89.62 4</cell><cell>-1.19</cell></row><row><cell>4 (Jang et al., 2019)</cell><cell></cell><cell cols="2">available ICCV 19</cell><cell>98.47</cell><cell>87.99</cell><cell>94.61</cell><cell>-6.62</cell></row><row><cell>5 (Wong et al., 2020)</cell><cell></cell><cell cols="2">available ICLR 20</cell><cell>98.50</cell><cell>82.88</cell><cell>88.77</cell><cell>-5.89</cell></row><row><cell>6 (Taghanaki et al., 2019)</cell><cell></cell><cell cols="2">retrained CVPR 19</cell><cell>98.86</cell><cell>0.00</cell><cell>64.25</cell><cell>-64.25</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Robustness evaluation of randomized l∞-adversarial defenses by AutoAttack. For each model, we report architecture, source, venue, clean accuracy and combined robust accuracy given by AutoAttack (AA column). We also provide the reported robust accuracy in the original papers and compute the difference to the one of AutoAttack.</figDesc><table><row><cell># paper</cell><cell>model</cell><cell>source</cell><cell>venue</cell><cell>clean</cell><cell cols="3">AutoAttack report. reduct.</cell></row><row><cell>CIFAR-10 -= 8/255</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1 (Wang et al., 2019)</cell><cell>En5RN</cell><cell>authors</cell><cell cols="2">NeurIPS 19 82.39 (0.14)</cell><cell>45.56 (0.20)</cell><cell>51.48</cell><cell>-5.9</cell></row><row><cell>2 (Yang et al., 2019)</cell><cell cols="2">with AT authors</cell><cell>ICML 19</cell><cell>84.9 (0.6)</cell><cell>26.3 (0.85)</cell><cell>52.8</cell><cell>-26.5</cell></row><row><cell>3 (Yang et al., 2019)</cell><cell>pure</cell><cell>authors</cell><cell>ICML 19</cell><cell>87.2 (0.3)</cell><cell>18.2 (0.82)</cell><cell>40.8</cell><cell>-22.6</cell></row><row><cell cols="4">4 (Grathwohl et al., 2020) JEM-10 available ICLR 20</cell><cell>90.99 (0.03)</cell><cell>9.92 (0.03)</cell><cell>47.6</cell><cell>-37.7</cell></row><row><cell cols="2">5 (Grathwohl et al., 2020) JEM-1</cell><cell cols="2">available ICLR 20</cell><cell>92.31 (0.04)</cell><cell>8.15 (0.05)</cell><cell>41.8</cell><cell>-33.6</cell></row><row><cell cols="2">6 (Grathwohl et al., 2020) JEM-0</cell><cell cols="2">available ICLR 20</cell><cell>92.82 (0.05)</cell><cell>6.36 (0.06)</cell><cell>19.8</cell><cell>-13.4</cell></row><row><cell>CIFAR-10 -= 4/255</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">1 (Grathwohl et al., 2020) JEM-10 available ICLR 20</cell><cell>91.03 (0.05)</cell><cell>47.97 (0.05)</cell><cell>72.6</cell><cell>-24.6</cell></row><row><cell cols="2">2 (Grathwohl et al., 2020) JEM-1</cell><cell cols="2">available ICLR 20</cell><cell>92.34 (0.04)</cell><cell>45.49 (0.04)</cell><cell>67.1</cell><cell>-21.6</cell></row><row><cell cols="2">3 (Grathwohl et al., 2020) JEM-0</cell><cell cols="2">available ICLR 20</cell><cell>92.82 (0.02)</cell><cell>42.55 (0.07)</cell><cell>50.8</cell><cell>-8.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 .</head><label>7</label><figDesc>Robustness evaluation of randomized l∞-adversarial defenses by AutoAttack. We report mean and standard deviation over 5 runs of evaluation of the robust accuracy achieved by the adversarial examples crafted by the different attacks, together with their combination in AutoAttack.</figDesc><table><row><cell># paper</cell><cell>model</cell><cell>APGDCE</cell><cell>APGDDLR</cell><cell>FAB</cell><cell>Square</cell><cell>AutoAttack</cell></row><row><cell>CIFAR-10 -= 8/255</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1 (Wang et al., 2019)</cell><cell>En5RN</cell><cell cols="2">48.81 (0.16) 49.37 (0.17)</cell><cell cols="3">-78.61 (0.19) 45.56 (0.20)</cell></row><row><cell>2 (Yang et al., 2019)</cell><cell>with AT</cell><cell>30.1 (0.5)</cell><cell>31.9 (0.53)</cell><cell>-</cell><cell>-</cell><cell>26.3 (0.85)</cell></row><row><cell>3 (Yang et al., 2019)</cell><cell>pure</cell><cell>21.5 (0.6)</cell><cell>24.3 (0.98)</cell><cell>-</cell><cell>-</cell><cell>18.2 (0.82)</cell></row><row><cell cols="2">4 (Grathwohl et al., 2020) JEM-10</cell><cell cols="4">11.69 (0.04) 15.88 (0.06) 63.07 (0.21) 79.32 (0.04)</cell><cell>9.92 (0.03)</cell></row><row><cell cols="2">5 (Grathwohl et al., 2020) JEM-1</cell><cell cols="4">9.15 (0.02) 13.85 (0.05) 62.71 (0.17) 79.25 (0.14)</cell><cell>8.15 (0.05)</cell></row><row><cell cols="2">6 (Grathwohl et al., 2020) JEM-0</cell><cell cols="4">7.19 (0.06) 12.63 (0.04) 66.48 (0.34) 73.12 (0.13)</cell><cell>6.36 (0.06)</cell></row><row><cell>CIFAR-10 -= 4/255</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">1 (Grathwohl et al., 2020) JEM-10</cell><cell cols="5">49.10 (0.09) 52.55 (0.06) 78.87 (0.16) 89.32 (0.06) 47.97 (0.05)</cell></row><row><cell cols="2">2 (Grathwohl et al., 2020) JEM-1</cell><cell cols="5">46.08 (0.04) 49.71 (0.05) 78.93 (0.19) 90.17 (0.03) 45.49 (0.04)</cell></row><row><cell cols="2">3 (Grathwohl et al., 2020) JEM-0</cell><cell cols="5">42.98 (0.10) 47.74 (0.08) 82.92 (0.10) 89.52 (0.13) 42.55 (0.07)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 .</head><label>8</label><figDesc>Comparison PGD vs APGD on the cross-entropy loss. We run PGD and PGD with Momentum (the same used for APGD) on the models tested in Sec. 6, with three different step sizes: /10, /4 and 2 . Similarly to APGD, we use 100 iterations and 5 restarts for PGD, and report the resulting robust accuracy on the whole test set. For each model we boldface the best attack and underline the best version of PGD (i.e. we exclude APGD).</figDesc><table><row><cell>PGD</cell><cell>PGD with Momentum</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 .</head><label>9</label><figDesc>Comparison PGD vs APGD on the DLR loss. We run PGD and PGD with Momentum (the same used for APGD) on the models tested in Sec. 6, with three different step sizes: /10, /4 and 2 . Similarly to APGD, we use 100 iterations and 5 restarts for PGD, and report the resulting robust accuracy on the whole test set. For each model we boldface the best attack and underline the best version of PGD (i.e. we exclude APGD).</figDesc><table><row><cell>PGD</cell><cell>PGD with Momentum</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">current SOTA is 44.03% in https://github.com/MadryLab/cifar10_challenge.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">current SOTA is 53.07% in https://github.com/yaodongyu/TRADES.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">current SOTA is 93.33% in https://github.com/yaodongyu/TRADES.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">current SOTA is 88.25% in https://github.com/MadryLab/mnist_challenge.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_4">https://github.com/BorealisAI/advertorch</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_5">https://github.com/max-andr/ square-attack</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We are very grateful to Alvin Chan, Chengzhi Mao, Seyed-Mohsen Moosavi-Dezfooli, Saeid Asgari Taghanaki, Bao Wang and Zhi Xu for providing code, models and answering questions on their papers. We also thank Maksym Andriushchenko for insightful discussions about this work. We acknowledge support from the German Federal Ministry of Education and Research (BMBF) through the Tbingen AI Center (FKZ: 01IS18039A). This work was also supported by the DFG Cluster of Excellence Machine Learning New Perspectives for Science, EXC 2064/1, project number 390727645, and by DFG grant 389792660 as part of TRR 248.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Are labels required for improving adversarial robustness?</title>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uesato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Stanforth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Square attack: a query-efficient black-box adversarial attack via random search</title>
		<author>
			<persName><forename type="first">M</forename><surname>Andriushchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Croce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Flammarion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.00049</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples</title>
		<author>
			<persName><forename type="first">A</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Atzmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Haim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yariv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Israelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
		<title level="m">Controlling neural level sets. In NeurIPS</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Towards evaluating the robustness of neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Security and Privacy</title>
				<imprint>
			<date type="published" when="2017">2017a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adversarial examples are not easily detected: Bypassing ten detection methods</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Workshop on Artificial Intelligence and Security</title>
		<imprint>
			<date type="published" when="2017">2017b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">On evaluating adversarial robustness</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<ptr target="https://github.com/evaluating-adversarial-robustness/adv-eval-paper" />
		<imprint>
			<date type="published" when="2019-05-14">May 14, 2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unlabeled data improves adversarial robustness</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Carmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="11190" to="11201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Jacobian adversarially regularized networks for robustness</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Certified adversarial robustness via randomized smoothing</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Minimally distorted adversarial examples with a fast adaptive boundary attack</title>
		<author>
			<persName><forename type="first">F</forename><surname>Croce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02044</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Provable robustness of relu networks via maximization of linear regions</title>
		<author>
			<persName><forename type="first">F</forename><surname>Croce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Andriushchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
				<imprint>
			<date type="published" when="2019">2019a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Scaling up the randomized gradient-free adversarial attack reveals overestimation of robustness using established attacks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Croce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019b</date>
			<publisher>International J. of Computer Vision (IJCV)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">AdverTorch v0.1: An adversarial robustness toolbox based on pytorch</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1902.07623</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">On the effectiveness of interval bound propagation for training verifiably robust models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gowal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dvijotham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Stanforth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bunel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uesato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.12715v3</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">An alternative surrogate loss for pgd-based adversarial testing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gowal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uesato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Your classifier is secretly an energy based model and you should treat it like one</title>
		<author>
			<persName><forename type="first">W</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Using pre-training can improve model robustness and uncertainty</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mazeika</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2712" to="2721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adversarial defense via learning to generate diverse attacks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Sensible adversarial learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rJlf_RVKwr" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Harnessing the vulnerability of latent layers in adversarially trained models</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kumari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Machiraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Balasubramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2779" to="2785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adversarial examples in the physical world</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Certified robustness to adversarial examples with differential privacy</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lecuyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Atlidakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Geambasu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jana</forename></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Security and Privacy (SP)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Certified adversarial robustness with additive noise</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carin</forename></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Valdu</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Metric learning for adversarial robustness</title>
		<author>
			<persName><forename type="first">C</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="478" to="489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Differentiable abstract interpretation for provably robust neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mirman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gehr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vechev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Robustness via curvature regularization, and vice versa</title>
		<author>
			<persName><forename type="first">S.-M</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uesato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Logit pairing methods can fool gradientbased attacks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mosbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Andriushchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Trost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Klakow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS 2018 Workshop on Security in Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Adversarial defense by restricting the hidden space of deep neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Improving adversarial robustness via promoting ensemble diversity</title>
		<author>
			<persName><forename type="first">T</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Rethinking softmax cross-entropy loss for adversarial robustness</title>
		<author>
			<persName><forename type="first">T</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Distillation as a defense to adversarial perturbations against deep networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Security &amp; Privacy</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Adversarial training for free! In NeurIPS</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shafahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dickerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Studer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3353" to="3364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Improving the generalization of adversarial training with domain adaptation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Hopcroft</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A kernelized manifold mapping to diminish the effect of adversarial perturbations</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Taghanaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Abhishek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Azizi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hamarneh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Resnets ensemble via the feynman-kac formalism to improve natural and robust accuracies</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Bilateral adversarial training: Towards fast training of more robust models against adversarial attacks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Scaling provable adversarial defenses</title>
		<author>
			<persName><forename type="first">E</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Fast is better than free: Revisiting adversarial training</title>
		<author>
			<persName><forename type="first">E</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Enhancing adversarial defense by k-winners-take-all</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">ME-net: Towards effective adversarial robustness with matrix estimation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Katabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">You only propagate once: Accelerating adversarial training via maximal principle</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2019">2019a</date>
			<biblScope unit="page" from="227" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Defense against adversarial attacks using feature scattering-based adversarial training</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1829" to="1839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Adversarial interpolation training: A simple approach for improving model robustness</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Syejj0NYvr" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Theoretically principled trade-off between robustness and accuracy</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Ghaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
