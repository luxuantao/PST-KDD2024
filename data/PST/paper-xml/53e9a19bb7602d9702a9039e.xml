<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bag of spatio-visual words for context inference in scene classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2012-09-05">5 September 2012</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">A</forename><surname>Bolovinou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics and Telecommunications</orgName>
								<orgName type="institution">University of Athens</orgName>
								<address>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Informatics and Telecommunications</orgName>
								<orgName type="laboratory">Computational Intelligence Laboratory</orgName>
								<orgName type="institution">National Center for Scientific Research &apos;&apos;Demokritos&apos;&apos;</orgName>
								<address>
									<postCode>153 10</postCode>
									<settlement>Athens</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">I</forename><surname>Pratikakis</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Democritus University of Thrace</orgName>
								<address>
									<postCode>GR-67100</postCode>
									<settlement>Xanthi</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">S</forename><surname>Perantonis</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute of Informatics and Telecommunications</orgName>
								<orgName type="laboratory">Computational Intelligence Laboratory</orgName>
								<orgName type="institution">National Center for Scientific Research &apos;&apos;Demokritos&apos;&apos;</orgName>
								<address>
									<postCode>153 10</postCode>
									<settlement>Athens</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Department of Informatics and Telecommunications</orgName>
								<orgName type="institution">University of Athens</orgName>
								<address>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Bag of spatio-visual words for context inference in scene classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2012-09-05">5 September 2012</date>
						</imprint>
					</monogr>
					<idno type="MD5">5BC427ED200FECE3473FC21A88C59586</idno>
					<idno type="DOI">10.1016/j.patcog.2012.07.024</idno>
					<note type="submission">Received 4 August 2010 Received in revised form 30 July 2012 Accepted 31 July 2012</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Scene classification Bag of spatio-visual words Spatial co-occurrence Contextual descriptors Ensembles&apos; learning High dimensional features&apos; clustering</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In the ''bag of visual words (BoVW)'' representation each image is represented by an unordered set of visual words. In this paper, a novel approach to encode ordered spatial configurations of visual words in order to add context in the representation is presented. The proposed method introduces a bag of spatio-visual words representation (BoSVW) obtained by clustering of visual words' correlogram ensembles. Specifically, the spherical K-means clustering algorithm is employed accounting for the large dimensionality and the sparsity of the proposed spatio-visual descriptors. Experimental results on four standard datasets show that the proposed method significantly improves a state-of-the-art BoVW model and compares favorably to existing context-based scene classification approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Automatic semantic image annotation for the management and maintenance of visual data archives has become a goal of important value in view of the evolving image large-scale collections being generated and stored by digital media worldwide <ref type="bibr">[1]</ref>. While research in object classification has achieved considerable levels of performance based solely on local appearance <ref type="bibr" target="#b0">[2]</ref><ref type="bibr" target="#b1">[3]</ref><ref type="bibr" target="#b2">[4]</ref><ref type="bibr" target="#b3">[5]</ref><ref type="bibr" target="#b4">[6]</ref><ref type="bibr" target="#b5">[7]</ref><ref type="bibr" target="#b6">[8]</ref>, classifying objects taking into account the context in a scene, remains an open issue towards improved scene understanding <ref type="bibr" target="#b7">[9]</ref>. One of the main difficulties that computational recognition approaches face by including contextual information is the lack of simple representations of context and efficient algorithms for the extraction of such information from the visual input.</p><p>In natural scene classification the objective is to classify images into pre-defined scene categories. Despite large variations in their content, images of natural scenes usually share a semantic setup per category: themes that co-exist in typical or abstract spatial configurations. For example, in the input image of Fig. <ref type="figure" target="#fig_0">1b</ref> that belongs to class ''coast'', we usually observe, in bottom-up order (spatial context), a dry land part, a sea part and a sky part (semantic context), captured from a long distance (scale context). This kind of underlying information that may influence the way a scene and the objects within it are perceived is vaguely understood as context <ref type="bibr" target="#b8">[10]</ref>. It is noted, from the beginning, that this work is focused on the whole scene classification task and not on the classification of objects within it.</p><p>Since most effective scene classification approaches are vocabulary based, the proposed approach aims at inferring scene context from visual words' co-occurrences. For that, it explores how the Biederman et al. <ref type="bibr" target="#b9">[11]</ref> notions of context (semantic as in <ref type="bibr" target="#b10">[12]</ref><ref type="bibr" target="#b11">[13]</ref><ref type="bibr" target="#b12">[14]</ref>, spatial as in <ref type="bibr" target="#b12">[14]</ref> or scale context as in <ref type="bibr" target="#b13">[15,</ref><ref type="bibr" target="#b14">16]</ref>) can be applied on top of a bag of visual words representation <ref type="bibr" target="#b2">[4,</ref><ref type="bibr" target="#b15">17,</ref><ref type="bibr" target="#b16">18]</ref>. The standard bag of visual words model (referred to as BoVW in the sequel), assigns each local feature from an image to a visual label (namely a visual word from the vocabulary) and then it represents visual labels' frequency of occurrence in an image, disregarding locality and scale information of the visual labels. The proposed method couples the spatial layout with the local co-occurrence of visual labels in a spatio-visual descriptor and constructs a bag of spatio-visual words model (referred to as BoSVW in the sequel) based on this descriptor. A variant of BoSVW model where the descriptor adapts to the scale information of visual patches is also implemented.</p><p>While the spatial layout of visual words has been partially taken into account in the works of <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b17">19,</ref><ref type="bibr" target="#b18">20]</ref>, modeling the co-location of visual words that usually co-exist within a spatial setting, is much more difficult to be achieved. This is due to the large number of the possible co-occurrence combinations and different spatial layouts of visual words in the image space. The latter objective becomes Contents lists available at SciVerse ScienceDirect journal homepage: www.elsevier.com/locate/pr more challenging in an unsupervised context. In such context, the annotation of the scenes' constituting parts, which is required to specify the location and assign the spatial relationship among scene parts, is not available as training material for the machine learning method. Currently, most approaches that deal with the colocation objective, they handle visual words up to second order cooccurrence <ref type="bibr" target="#b18">[20]</ref><ref type="bibr" target="#b19">[21]</ref><ref type="bibr" target="#b20">[22]</ref><ref type="bibr" target="#b21">[23]</ref>. Other alternatives reported in the literature comprise a feature selection technique applied upon the highdimensional spatio-visual descriptors produced, as in <ref type="bibr" target="#b22">[24,</ref><ref type="bibr" target="#b23">25]</ref> or specialized classification kernels applied upon a simple cumulative representation as in <ref type="bibr" target="#b24">[26]</ref>.</p><p>Our work addresses the above objective by the use of spatiovisual patterns derived from clustering of visual words' ensembles found in image space, without imposing constraints on the order of visual words' co-occurrence. The ensembles of visual words are encoded by a descriptor following the principle of Mori's et al. <ref type="bibr" target="#b25">[27]</ref> Shape Context. The descriptor will be called CoTrans descriptor in this work and it is derived by substituting the local low-level features appearing in <ref type="bibr" target="#b25">[27]</ref> with the corresponding visual words indexes. An illustration of the above concepts is provided in the example of Fig. <ref type="figure" target="#fig_0">1</ref>. In this figure, we take as input the image of Fig. <ref type="figure" target="#fig_0">1b</ref> and we assume a predefined toy visual vocabulary of 5 visual words, shown in Fig. <ref type="figure" target="#fig_0">1a</ref>: in this toy BoVW, different colored points represent different characteristics of local patches; we refer readers to Section 3.1 for more details in visual words' formation. Fig. <ref type="figure" target="#fig_0">1c</ref> depicts local feature extraction in the input image; in Fig. <ref type="figure" target="#fig_0">1d</ref>, each local feature is assigned to a visual word from the toy BoVW. Then, Fig. <ref type="figure" target="#fig_0">1e</ref> and f assume circular CoTrans descriptors on top of the BoVW representation and introduce the spatial and spatio-visual consideration respectively. Note that in Fig. <ref type="figure" target="#fig_0">1e</ref> the descriptors capture the spatial layout of local features but the visual labels are ignored (for visualization purposes, different color of each local feature point is replaced with gray color). Our method builds upon the more complex representation shown in Fig. <ref type="figure" target="#fig_0">1f</ref>, where visual labels (colors in the illustration) of local features are preserved.</p><p>The novel spatio-visual vocabulary model (BoSVW) will be realized by using the spherical K-means clustering algorithm <ref type="bibr" target="#b26">[28]</ref> upon the high-dimensional CoTrans descriptors. The classification image vector is derived by late fusing of the BoSVW image representation with a standard BoVW model.</p><p>As the main contribution of this paper we demonstrate that (i) automatically estimated spatio-visual words can efficiently be derived by the use of spherical K-means algorithm despite the highdimensionality and sparsity of the spatio-visual descriptors and (ii) scene classification based on the standard BoVW model can take advantage of a BoSVW model built on top of it. Three different support vector machine (SVM) kernels evaluate the performance of the proposed representation on four standard scene data sets, each comprising 15, 8, 9 and 18 categories respectively. Compared to the state-of-the-art scene classification works of <ref type="bibr" target="#b13">[15,</ref><ref type="bibr" target="#b28">30,</ref><ref type="bibr" target="#b29">31]</ref>, in which different forms of spatio-visual context are efficiently integrated, the proposed approach consistently succeeds in improving the mean classification accuracy rate. We, in particular, emphasize the fully automatic nature of our BoSVW approach and its scalability to a large number of scene classes.</p><p>This paper is organized as follows. In the next section an overview of the related literature is given. Section 3 details the proposed method. In Section 4, we present our evaluation methodology and the corresponding experimental results. Section 5 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Different approaches exploiting the spatial <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b24">26]</ref>, the scale <ref type="bibr" target="#b13">[15,</ref><ref type="bibr" target="#b14">16,</ref><ref type="bibr" target="#b30">32]</ref> and the co-occurrence <ref type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b18">20,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b31">33</ref>] information of visual words in an image have appeared, since the image histogram representation of the standard BoVW model ignores this information. Built on top of BoVW model, the related work can be broadly grouped into the three distinct categories that follow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">The use of pyramids in image or feature space</head><p>In the work of <ref type="bibr" target="#b3">[5]</ref> a spatial pyramid matching scheme inspired from <ref type="bibr" target="#b32">[34]</ref> is employed, by partitioning an image into increasingly fine sub-regions and considering each sub-region as a bag of visual words. Although this hierarchical image space partitioning can result in very large visual words codebooks, recent classification algorithms like SVM can efficiently handle them. A state-of the art work based on the former representation employing a kernel-based clustering scheme is that of <ref type="bibr" target="#b5">[7]</ref>. In <ref type="bibr" target="#b13">[15]</ref>, the authors represent visual words through different scales and create a vocabulary from the concatenation of scale-consistent vocabularies per class. A different method, which exploits a hierarchy in descriptor space, as in the original pyramid match kernel (PMK) <ref type="bibr" target="#b32">[34]</ref>, is semantic texton forest of <ref type="bibr" target="#b29">[31]</ref> which achieves significant results both for scene categorization and scene segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">The use of spatial and co-occurrence relations derived by a graphical model</head><p>Those models represent a scene as the mixture distribution of hidden topics that can essentially be considered as a semantic visual vocabulary produced out of the initial space of visual words. Liu and Shah <ref type="bibr" target="#b33">[35]</ref> used maximization of mutual information (MMI) to obtain the optimal size of the visual semantic vocabulary for action recognition. In <ref type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b17">19]</ref>, the authors use a graphical model which models pair-wise co-occurrence relationships of visual words and their spatial layout, achieving good performance in object and scene classification respectively. A spatially coherent latent topic model, where spatial consistency is imposed by an over-segmentation of the image, is introduced by <ref type="bibr" target="#b11">[13]</ref>. One step further, the work of <ref type="bibr" target="#b18">[20]</ref> considers the scene as an ordered set of labeled image patches and proposes a combination of PLSA and a conditional random field named COCRF. While PLSA models the co-occurrence relationship for every two visual words in an image, COCRF additionally models their spatial position. Although COCRF is computationally more expensive compared to previous approaches, it provides a probabilistic model to interpret the structure of scene categories. A similar approach, which infers co-occurrence of visual words and geometry knowledge, using a Markov Random Field appears in the model of <ref type="bibr" target="#b34">[36]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">The use of spatio-contextual descriptors in a model-free approach</head><p>These approaches are based on semi-local image descriptors and they constitute a powerful alternative to graphical-model representations as they are model-free (usually requiring minimal or no supervision). One of the first spatio-visual descriptors proposed was the color correlogram of <ref type="bibr" target="#b35">[37]</ref>, which expressed how the spatial correlation of pairs of colors in an image changes with distance. Applying a correlogram-like descriptor on top of the visual words representation and then performing a clustering step, many researchers choose to work on visual words doublets <ref type="bibr" target="#b19">[21,</ref><ref type="bibr" target="#b21">23]</ref> or triplets <ref type="bibr" target="#b24">[26]</ref>. These approaches belong to the family of works, where the pairwise co-occurrences constitute the cooccurrence matrix and co-occurrence patterns are often derived from spectral clustering. These methods are inspired from textdata mining techniques (the BoVW model itself was inspired by the bag of words model used in text categorization), where the use of a 2-dimensional word co-occurrence matrix ignores the spatial position of the words in a text. Indicatively we refer to the visual bi-grams of <ref type="bibr" target="#b36">[38]</ref> and its image analogous of <ref type="bibr" target="#b20">[22]</ref>. Our method works with sets of visual words that co-occur locally in an image, but it does not disregard the spatial configuration of these visual words.</p><p>Augmenting the co-occurrence matrix approach with a third spatial-distance dimension, Savarese et al. <ref type="bibr" target="#b21">[23]</ref> have suggested correlograms of rectangular spatial kernels for capturing the spatial arrangement of pairs of visual words. Furthermore, on the aggregate of correlograms per image the authors apply compact spatial modeling through the use of vector quantized correlograms, which they call correlatons. This clustering step has been previously suggested by <ref type="bibr" target="#b10">[12]</ref>. Finally, the image classification vector is obtained as the concatenated histogram of codewords and correlatons. Our approach improves this work since the order of the local co-occurrence of visual words is not restricted to be pair-wise.</p><p>Moving on to quantization-free methods, the Generalized Correlogram (GC) descriptor is proposed by <ref type="bibr" target="#b37">[39]</ref>. GC induces geometrical information describing the mutual position of color and edges features, using a log-polar r-th quantization of image spatial domain (inspired by Mori's et al. <ref type="bibr" target="#b25">[27]</ref> Shape Context). Casting the GC descriptor into the framework of BoVW model comes natural as in <ref type="bibr" target="#b22">[24]</ref>, if local features are replaced by the index of their closest visual word. In contrast with the work of Savarese et al. <ref type="bibr" target="#b21">[23]</ref>, in <ref type="bibr" target="#b22">[24]</ref> no order of co-occurrence is set from the problem. Here, the spatio-visual GC descriptors are directly employed by a feature selection model avoiding a later step of descriptors' accumulation. In addition and in contrast with our method, the absence of an unsupervised clustering step leads to the need of building several contextual models from manually labeled contextual parts of the scene. One advantage is that the GCs can be made scale and rotation invariant by scale-dependent normalization of their radius <ref type="bibr" target="#b38">[40]</ref> and by unifying theta bins to one bin resulting in a circular kernel <ref type="bibr" target="#b14">[16]</ref> respectively. Note also that the GC binary representation, though high dimensional, allows employing fast matching techniques, a feature also exploited by this work.</p><p>An interesting variant of a correlogram descriptor approach which makes use of relative distances in feature space instead of absolute distances in image space is this of <ref type="bibr" target="#b23">[25]</ref>. This work is inspired from <ref type="bibr" target="#b24">[26]</ref>, where local correlograms of visual words in the K-NN neighborhood of a reference visual word are accumulated into a cumulative distribution (called cummulogram) per image, which is then fed to a ''min'' kernel-classifier.</p><p>Handling sparse, highly nonlinear and high-dimensional data, such as the spatio-visual GC descriptors of <ref type="bibr" target="#b22">[24]</ref>, is not a trivial issue. Based on the influential work of <ref type="bibr" target="#b32">[34]</ref>, several Mercer kernels have been proposed that can efficiently process k-order spatial relationships of visual words <ref type="bibr" target="#b39">[41]</ref>. High order kernels have also been designed for many other applications, such as the string kernel <ref type="bibr" target="#b40">[42]</ref> for document classification. To overcome the order constraint when working with sets of co-occurring features, a classification kernel that can be applied on unbounded-order spatial features is proposed in <ref type="bibr" target="#b25">[27]</ref>. The kernel is computed in time that grows linearly with the number of local features in an image (as in the BoVW approach). Results on Caltech and MSRC datasets with features up to 10 th order are promising, although a further increase of the order of co-occurrence does not seem to add any information.</p><p>A different approach which influenced this work, since it suits ideally the large-dimensionality of the spatio-visual descriptors, is based on data mining algorithms. Based on the video mining method of <ref type="bibr" target="#b41">[43]</ref>, the authors in Quack et al. <ref type="bibr" target="#b31">[33]</ref> the authors define tiled neighborhoods of visual words and find frequent recurring configurations for each class by using established data mining algorithms. The method though computationally very efficient, cannot be directly exploited within a classification framework since it does not produce a final image representation. Similar limitations apply to the method of <ref type="bibr" target="#b42">[44]</ref> where itemset mining is used for vocabulary refinement.</p><p>In regard to the state-of-the-art detailed in this section, the proposed methodology belongs to the third category of the presented approaches. Inspired by the high informative nature of the spatio-visual GC descriptors <ref type="bibr" target="#b37">[39]</ref> and tackling the high dimensionality of the descriptor by an efficient clustering scheme, the proposed method makes a bag of spatio-visual words model easy to implement. Moreover, it provides context inference from visual scenes requiring minimum supervision, since only scene labels are required before hand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The proposed method</head><p>The proposed approach is outlined in the three steps below: </p><formula xml:id="formula_0">(Step I) A feature</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The BoVW representation</head><p>Most successful image representations are based on affine invariant features derived from image patches <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b2">4]</ref>. In the BoVW paradigm, the image set of local features is summarized by global statistics of the appearance of the sampled patches, as explained in the next paragraph.</p><p>First, in an offline mode, features are sampled on patches of the image, either using a multi-resolution grid, in a randomized manner, or using interest point detectors. Each patch is then described using a feature vector <ref type="bibr" target="#b2">[4]</ref>, e.g., SIFT <ref type="bibr" target="#b27">[29]</ref>. A visual vocabulary is then learned using a feature space quantization scheme by considering the center of each quantized area as a visual word. Note that quantization through clustering generates visual word vectors (cluster centers) with the same dimension as the dimension of the feature space. In this work, a kernel K-means clustering scheme using the Histogram Intersection (HI) distance is employed. This choice is due to HI being better tailored for histogram features than the popular Euclidean distance, used in classical K-means, while being equally fast <ref type="bibr" target="#b5">[7]</ref>. Then, each local feature of an image can be assigned to the closest visual words from the vocabulary producing a histogram per image. In particular, the produced histogram bears as many bins as visual words, where each bin gives the number of patches assigned to this visual word. The assignments are then aggregated over whole image to obtain an image representation. In this way, the image represented by a set of features is embedded into vector space in which an image classification model is learned.</p><p>Assignment of a visual feature to the vocabulary depends on the similarity metric (in this work the same distance metric applied for clustering) and a potential weighting scheme <ref type="bibr" target="#b43">[45,</ref><ref type="bibr" target="#b44">46]</ref>. In the case that a local feature is soft-assigned to more than one visual word, the technique is known as multi-assignment (MA) <ref type="bibr" target="#b45">[47,</ref><ref type="bibr" target="#b46">48]</ref>. A MA technique of degree 4 will be applied for both our BoVW and the BoSVW since it has been proved advantageous for vocabulary creation <ref type="bibr" target="#b44">[46,</ref><ref type="bibr" target="#b46">48]</ref>. Note that more than one BoVW models can be employed depending on the classification task in hand, in order to combine the merits of different local features (we refer the reader to Section 3.3.1 for different BoVW models used in our experiments and to Sections 4.1.2 and 4.1.4 for details on the creation of the BoVW models and the corresponding image histogram representation used in this work).</p><p>Based on the BoVW image representation described above, we continue with introducing the elements which are required in building the spatio-visual space of Step I. It should be considered that lower case variables (e.g., x, y) are used to represent local features and their associated attributes, while upper case bold italic alphabets (e.g., X, Y ) are used to represent sets of features. Finally, lower case bold variables (e.g., x) are used to represent a feature vector.</p><p>Let us consider an input image I where we detect a set of local features F I : {f} I Each feature f corresponds to a single image patch p where it was extracted and is characterized by the detection attributes [x, y, scale, ori, d] p , where:</p><p>x, y denote image coordinates and indicate the center of the patch p; scale denotes the size of the patch p (optional use, if interest point detection is used this is analogous with the scale of detection); ori denotes a measure of principal orientation of the patch p (optional use) and d denotes the feature vector that describes the patch p (i.e in broadly used SIFT <ref type="bibr" target="#b27">[29]</ref> case, the vector dimension is 128). Now, let us consider a BoVW model of size K, where each local feature f corresponding to the image patch p, is assigned to the L closest visual words (L denotes the degree of MA and L rK holds). We then define: {l n }p, that denotes the set of the L most relevant visual word labels assigned to patch p, where lAZ and n ¼1, 2,.. L.</p><p>The augmented set of [x, y, scale, ori, {l n }] p constitutes the per patch representation which will be referred to in this work as lpatch (abbreviation for labeled patch). In the l-patch, appearance is contributed by the scale, ori, {l n } attributes, while location is contributed by the x, y, attributes. In this way, a given image I can be described as ordered ensembles of visual words and the Step I has concluded. Notice that tuning the extraction of the spatiovisual descriptor according to the ''scale'' and ''ori'' inherent attributes of the l-patches (ignored in the standard BoVW), is considered only optionally in this work. Hence, in the following and unless otherwise stated, we consider the l-patch in the form of [x, y, {l n }] p. . Results on the use of the full form of l-patch are included in Section 4.2.1.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">The BoSVW representation</head><p>This section describes the core of our proposed method that is the BoSVW model construction (Step II of the method). The model is applied on images which are represented as sets of l-patches and are obtained from Step I (see Section 3.1) and it consists of two parts: (a) the spatio-visual descriptor extraction part; the employed descriptor is called CoTrans descriptor in this work and it encodes the co-occurrence of l-patches in well defined spatial regions (see Section 3.2.1) and (b) the clustering part responsible for the creation of the feature space of spatio-visual words (see Section 3.2.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">The spatio-visual descriptor extraction</head><p>Spatial configurations of sets of l-patches can be expressed through correlograms of visual words, which are semi-local, two dimensional (2-d) spatial descriptors around a set of image reference points <ref type="bibr" target="#b35">[37,</ref><ref type="bibr" target="#b37">39]</ref>. The spatial template of such descriptor defines the local contextual regions of interest while each spatial bin of the descriptor records a histogram of visual words falling inside this bin. Depending on the spatial quantization applied by the descriptor and the often large dimension of the visual vocabulary ( $10 3 ), the descriptors may result in not only highdimensional but also extremely sparse representations in the vector space. This characteristic resembles closely the transactions from data-mining literature <ref type="bibr" target="#b31">[33]</ref> and hence, the proposed descriptor will be called co-occurrence transaction (CoTrans) descriptor in this work.</p><p>More formally, let us consider an input image I described by a BoVW vocabulary of length K and let c AC I be a reference point in this image. Let us also consider a CoTrans template as in <ref type="bibr" target="#b25">[27]</ref> in which the radial axis (-r) is divided into N r bins while the polar axis (-th) is divided into N th bins. Now, for each polar spatial bin [r i , th j ] of the CoTrans c template, we define the histogram h c (i,j) of length K, that encodes the image sparse set of l-patches, {[x, y, {l n }] p } I , falling inside this bin, where indexes i, j correspond to the radial (r-) and polar (th-) quantized axes respectively. Then, the CoTrans c descriptor is derived by concatenating the h c (i,j) histograms over all spatial bins {i,j}. This results in the CoTrans c 1-d histogram vector given in definition (1). From (1), the dimensionality of the produced descriptor, D CoTrans, can be easily deducted as:</p><formula xml:id="formula_1">D CoTrans ¼#spatial bins x K ¼N r x N th x K. CoTrans c ¼ ½h c ð1, 1Þ, . . ., h c ði, jÞ, . . .h c ðN r , N th Þ,<label>ð1Þ</label></formula><p>where i, j A Z : i A ½1, N r , j A ½1, N th : Since binary CoTrans data yielded better results in our experiments (see Section 4.2.1), descriptor binarization is adopted: CoTrans c -CoTrans c-b . In this case, each h c (i,j), encodes solely the occurrence of a visual word (1: existence, 0: non-existence) and not its frequency of occurrence. CoTrans c-b is considerably more compact, while the computations that will follow become more efficient.</p><p>An illustration of a CoTrans descriptor extraction from a reference point c in the image is provided in Fig. <ref type="figure" target="#fig_1">2</ref>. The input image, which is described as a bag of visual words, appears on the left part of Fig. <ref type="figure" target="#fig_1">2</ref>. The predefined toy visual vocabulary of K¼5 visual words, used in Fig. <ref type="figure" target="#fig_0">1a</ref>, is again assumed. To simplify the visualization, note that, in this input image representation, each feature patch is assumed to be assigned with a single label from the vocabulary (depicted as a one color point). In the middle part of Fig. <ref type="figure" target="#fig_1">2</ref>, an illustration of the CoTrans descriptor around a reference point c is provided assuming a descriptor's template division of N r ¼3 in the radial axis and N th ¼4 in the polar axis. The set of the descriptor's constituting spatio-visual histograms, h c (1,1) to h c <ref type="bibr" target="#b1">(3,</ref><ref type="bibr" target="#b2">4)</ref>, and derivation of the corresponding binary CoTrans descriptor, CoTrans c-b, are presented in the right part of Fig. <ref type="figure" target="#fig_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.1.">CoTrans descriptor implementation aspects.</head><p>The log-polar spatial template of the CoTrans descriptor is defined by the radius R and the log-polar quantization. In this work, the CoTrans radius R is set to 48 pixels, N r is set to 3 and N th is set to 4. The set of CoTrans reference points, C I , are sampled from a sparse grid on image (more details in Section 4.1.3). Encoding of visual words into the CoTrans bins follows an improved version of the shape context descriptor <ref type="bibr" target="#b25">[27]</ref>, <ref type="bibr" target="#b38">[40]</ref>: each l-patch that resides close to the spatial bin's borders votes for the two closest bins in each dimension (this reduces the hard quantization effects in the creation of the h c histograms). Remarks on the binarization of the proposed CoTrans descriptor are included in the Section 4.2.1.1. A scale and rotation invariant implementation of the CoTrans descriptor has also been tested and it is included in Section 4.2.1.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">The clustering scheme</head><p>In order to produce the proposed BoSVW model an appropriate quantization scheme should be selected. While different methods have been proposed to accelerate the feature space division for the standard BoVW model <ref type="bibr" target="#b6">[8,</ref><ref type="bibr" target="#b43">45,</ref><ref type="bibr" target="#b45">47,</ref><ref type="bibr" target="#b50">52]</ref>, in this work we focus in the more fundamental issue of applying a bag of words philosophy on the space of high-dimensional and sparse image descriptors. This is due to the inefficiency (mainly in terms of memory requirements) of commonly used unsupervised schemes as the K-means or its tree-structured variations <ref type="bibr" target="#b29">[31]</ref>, the kernel-based clustering methods <ref type="bibr" target="#b5">[7,</ref><ref type="bibr" target="#b6">8]</ref> or the mean-shift <ref type="bibr" target="#b10">[12,</ref><ref type="bibr" target="#b43">45]</ref>, when those schemes are requested to address as many and such high dimensional descriptors as the CoTrans (D CoTrans $ 10 3 , an order higher than the visual descriptors used in BoVWs models).</p><p>In particular, we focus on the generic Euclidean distance (Gaussian distribution of data is assumed) that is used inside the popular K-means clustering scheme for BoVW construction. This distance, though effective in low dimensions problems, becomes useless in high dimensions, since sparsity of the vectors results in similar distances in Euclidean space. This problem can be solved by introducing a different distance metric suitable for high-dimensional, sparse and non-negative vectors.</p><p>We argue that the co-location of visual words in an image can be efficiently analyzed if we consider each of the sparse binary CoTrans c À b descriptor as document represented by D CoTrans terms. Indeed, qualitative characteristics (words' distribution) of the CoTrans vectors are similar to these of document vectors. This is sustained by the very low values of the mean of the fraction of nonzero entries (N nz ) in the term-document matrix <ref type="bibr" target="#b26">[28]</ref>: for 30 training images randomly selected from our datasets mean ratio of N nz /D CoTrans equals 0.02 (before ratio calculation, CoTrans c vectors containing less than 3 activations were removed from the dataset).</p><p>The employed quantization scheme for BoSVW generation is the online version <ref type="bibr" target="#b26">[28]</ref> of the spherical K-means algorithm <ref type="bibr" target="#b51">[53]</ref>, which to the best of the knowledge of the authors has been tested on text-mining applications only. This clustering scheme was selected based on three factors: (i) On indications from text information retrieval literature <ref type="bibr" target="#b26">[28,</ref><ref type="bibr" target="#b51">53]</ref>, where applications convincingly demonstrate cosine similarity to be the most effective measure for analyzing and clustering high-dimensional, very sparse and with non-negative entries vectors, as the text documents and the gene expressions. In this particular domain, similarity measures, such as cosine, are more effective than measures derived from Mahalanobis type distances, due to intrinsic ''directional'' characteristics of the data. The implication is that the direction of such a data vector is more important than its magnitude. This is also sustained by substantial empirical evidence provided in <ref type="bibr" target="#b51">[53]</ref>, where normalizing the data vectors helps to remove the biases induced by the length of a document and provides superior results. (ii) On the simplicity and computational efficiency of the spherical K-means algorithm <ref type="bibr" target="#b26">[28]</ref>. (iii) On the property of cosine similarity distance to degenerate to the Euclidean distance, when applied to unit-normalized vectors; this fact motivated the use of cosine similarity as most of standard BoVW models also choose the Euclidean distance for clustering visual information.</p><p>To exploit the above observations, each CoTrans descriptor x is normalized to be of unit length (JxJ ¼1) and the dot product ( Cos_Sim) is suggested as a similarity metric between the descriptors. Let X be the set of training CoTrans data, xAX and let {m 1 , y, m K } be a set of unit-length centroid vectors. Then, the spherical K-means algorithm aims to maximize the average cosine similarity objective of equations ( <ref type="formula" target="#formula_2">2</ref>) and (3) over the set X:</p><formula xml:id="formula_2">L ¼ X X x T m kðxÞ<label>ð2Þ</label></formula><p>and</p><formula xml:id="formula_3">kðxÞ ¼ arg max k x T m k :<label>ð3Þ</label></formula><p>In Eq. (3), k(x) is the index of the closest cluster centroid to x.</p><p>An interesting observation is that on a unit hyper sphere (where x and m are both unit-length vectors), maximizing (2) is equivalent to minimizing an Euclidean distance ( Eucl_dist) objective, as shown in equation 4 below:</p><formula xml:id="formula_4">Eucl_dist ¼ :wÀm: 2 ¼ :w: 2 þ :m: 2 À2w T m ¼ 2À2w T m ¼ 222nCos_Sim<label>ð4Þ</label></formula><p>The substantial difference from standard K-means is that the re-estimated mean vectors need to be normalized to unit-length and the underlying probabilistic models are not Gaussian as in most BoVW models <ref type="bibr" target="#b52">[54]</ref> (the mixture of von Mises-Fisher distributions apply here <ref type="bibr" target="#b51">[53]</ref>).</p><p>Applying online spherical k-means algorithm on the set of CoTrans vectors produce the proposed BoSVW model where cluster centers constitute the novel spatio-visual words. To obtain a histogram image representation each CoTrans descriptor from an image is assigned to the closest spatio-visual word from the BoSVW. Naturally, the assignment of the descriptors to BoSVW vocabulary follows the same similarity metric used for clustering the CoTrans data, i.e. the cosine similarity metric. In order to alleviate the hard-clustering effects in the histograms' generation, a 4N-soft-weighting method proposed in <ref type="bibr" target="#b46">[48]</ref> is applied during assignment. The method has been properly converted for the CoTrans dot product space based on Eq. (4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">The combined representation</head><p>In the final Step III of the proposed approach, a late fusion technique accomplished by vector concatenation is applied to combine the contextual information provided by the BoSVW histogram representation with the appearance information provided by a basic BoVW histogram representation. Note that image histogram representation based on BoVW can also be a concatenation of several BoVW models. Section 3.3.1 below includes information on the baseline BoVWs models employed by this work. Then, the combined image representation produced in Step III is fed into an SVM classifier for the multi-class scene classification task in hand. Details on the SVM kernels used for classification can be found in Section 4.1.5, as part of the experimental setup of this work. An overview of the proposed image representation as well as the pseudocode corresponding to the three steps described through Sections 3.1 to 3.3 are presented in Section 3.3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">Base local features</head><p>For our BoVW image representation one texture and one color-based local feature are used, which are densely sampled from the images (see Section 4.1.2): For the texture, the CENTRIST <ref type="bibr" target="#b53">[55]</ref> and for the color the Local Color Moments (LCM) <ref type="bibr" target="#b54">[56]</ref> are employed leading to the BoVW CENTRIST and the BoVW LCM vocabularies respectively.</p><p>With respect to the BoSVW vocabulary generation, a projection of the image onto a sparse-sampled SIFT <ref type="bibr" target="#b27">[29]</ref> vocabulary, namely the BoVW S SIFT , is pre-required (''S'' superscript denotes sparse sampling). This is due to the distinctiveness and repeatability properties of the interest point detection emphasized also in the works of <ref type="bibr" target="#b15">[17,</ref><ref type="bibr" target="#b48">50,</ref><ref type="bibr" target="#b49">51]</ref>. Based on our experiments, it is sustained that spatio-visual descriptors on set of l-patches work much better when visual words are assigned to sparsely sampled features based on interest point detection <ref type="bibr" target="#b2">[4]</ref> (find indicative result at the end of Section 4.2.1) instead of the widely used densely sampled ones <ref type="bibr" target="#b47">[49]</ref>. More details in base local feature sampling are included in Section 4.1.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.">System overview</head><p>An illustration of the proposed image representation as this is formed according to each of the three steps of the method's outline, provided in the introduction of Section 3, is shown in Fig. <ref type="figure" target="#fig_4">3</ref>. The pseudocode of the proposed approach is provided in Table <ref type="table">1</ref> below. Notation follows the formalism introduced in Section 3. Note with respect with the pseudocode of Table <ref type="table">1</ref>, that construction of different vocabularies or assignment to different vocabularies can be performed in parallel.</p><p>As illustrated in Fig. <ref type="figure" target="#fig_4">3</ref> Step I, the input image is assumed to be sampled in both a dense (grid) and a sparse (interest points) mode. These two modes will produce their own set of local feature vectors, {d p }, leading to the BoVW and the BoVWS vocabularies respectively. The toy visual vocabulary of K ¼5 visual words, used from Fig. <ref type="figure" target="#fig_0">1a</ref>, is assumed to be the BoVW S . In the upper-right triangle part of the Step I of Fig. <ref type="figure" target="#fig_4">3</ref>, an l-patch assigned to four visual word indexes from the BoVW S is shown: MA of degree 4 is assumed and a random white patch p i is assigned to four visual word indexes that are represented as 4 discrete colors from the toy BoVW S (including white color which is the predominant visual word label for the chosen patch p). In Step II of Fig. <ref type="figure" target="#fig_4">3</ref>, set of CoTrans c descriptors are assumed to be extracted from an image projected onto the BoVW S , as explained in Section 3.2.1. Finally, in Step III, a concatenation of the BoVW and BoSVW histogram image representations is depicted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental setup and evaluation</head><p>Evaluation of the proposed bag of spatio-visual words model is accomplished through two different tasks: Note, that, although the BoVW CENTRIST is the baseline model want to compare with, since it has been proved the best performing model in <ref type="bibr" target="#b5">[7]</ref>, results for a dense-sampled SIFT-based model (BoVW SIFT ) are also provided to allow for a straightforward comparison with other published works which in their majority are based on SIFT vocabularies. The datasets for the evaluation of the proposed method are selected based on their broadness of their use in scene classification literature within the last decade. Works that include the use of scene context and provide classification rates on scenes classes instead of objects classes will be preferred. In that sense, the difficult OT-15 scene dataset <ref type="bibr" target="#b3">[5]</ref> and the less challenging but broadly used MSRC-21 <ref type="bibr" target="#b55">[57]</ref> datasets are among the selected ones. Experiments are performed on the following four reference datasets:</p><p>(i) OT 8 color scene dataset <ref type="bibr" target="#b56">[58]</ref>   challenging, as the missing background categories (sky, water, grass) are generally easily classified (please refer to <ref type="bibr" target="#b12">[14]</ref> and <ref type="bibr" target="#b18">[20]</ref>).</p><p>(iv) MSRC-9 dataset (MSRC9): It is a subset of MSRC-18 dataset. We choose nine categories out of fifteen: {cow, airplanes, faces, cars, bikes, books, signs, sheep and chairs}, so that</p><p>Table <ref type="table">1</ref> Pseudocode of the proposed method (Steps I-III corresponds to the outline of Section 3 and the overview of Fig. <ref type="figure" target="#fig_4">3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm: BoSVW construction and its concatenation with a baseline BoVW model</head><p>Input:</p><p>-An image dataset I and an training dataset X (X ⊂ I).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>-</head><p>The {CENTRIST, LCM} I set of features on image multi-scale grid per image, extracted in a dense sampling mode (see sec 3. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Process:</head><p>Step I:</p><p>-Apply HI K-means clustering algorithm (number of centers = K) on the {SIFT} X , {CENTRIST} X and {LCM} X sets to build the BoVW to obtain the set of l-patches {[x, y, {l n }] p } I per image (n ≤ 4).</p><p>Step II:</p><p>-Construct CoTrans c template, using the log-polar (base of the logarithm is set to 2 ) quantization defined as {i, j, R∈ : Step III:</p><formula xml:id="formula_5">Ζ i ∈[1, N r ] , j ∈[1, N th ], R = R T }:</formula><p>For each image: Concatenate BoVW CENTRIST , BoVW LCM and BoSVW histograms into the final image histogram vector.</p><p>objects from different categories would not appear in the same image.</p><p>In each dataset, the available data are randomly split into a training set and a testing set based on published protocols on these datasets. For each class of OT8/15: 100 for training, rest for testing, similar to Oliva et al. <ref type="bibr" target="#b56">[58]</ref> and for MSRC8/18: 15 for training, rest for testing. The random splitting is repeated 5 times, and the average accuracy is reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">Sampling of the base local features</head><p>Dense feature sampling for the baseline BoVW SIFT/CENTRIST creation is based on local rectangular patches derived from a grid pyramid image division of 13 spatial sectors, as in <ref type="bibr" target="#b53">[55]</ref>. Dense feature sampling for BoVW LCM creation is based on local rectangular patches derived from an image grid, as in <ref type="bibr" target="#b17">[19]</ref>. Sparse feature sampling for the BoVW S SIFT , creation is based on sets of points sampled by DoG detector <ref type="bibr" target="#b27">[29]</ref>. According to Lowe's original implementation <ref type="bibr" target="#b27">[29]</ref>, a ''twin'' SIFT descriptor may be produced at each detected location and scale, if the region is assigned with a secondary orientation. For the proposed BoSVW generation, a pruned subset of the SIFT keypoints is taken into account, in order to avoid counting a feature location twice. Hence, only the SIFT keypoints representing the dominant orientation of the patch are kept by modifying the SIFT code of <ref type="bibr" target="#b57">[59]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3.">Sampling of the CoTrans descriptors</head><p>CoTrans reference points (C) are sampled from a 6 Â 6 image grid (allows for correlogram regions' overlapping) during BoSVW vocabulary creation. However, during the assignment of CoTrans descriptors to BoSVW, reference points are sampled from a 18 Â 18 grid to allow for denser sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4.">Vocabulary construction and assignment</head><p>BoVW models: For the BoVW SIFT and the BoVW CENTRIST vocabulary generation we used K¼200 visual words (resulting in BoVW CENTRIST image histogram of length <ref type="bibr">[13 Â 200-d</ref></p><formula xml:id="formula_6">] ¼2600-d and CoTrans descriptor length of [N r Â N th Â BoVW SIFT Àlength]¼ [3 Â 4 Â 200-d] ¼2400-d)</formula><p>For BoVW LCM vocabulary generation we used K 0 ¼400 visual words. Mind that the HIK clusterer of <ref type="bibr" target="#b5">[7]</ref> runs only on vectors of positive integer values and their length cannot exceed an upper bound (in our case upper bound is set to 128). In the case of the LCM feature set, only the 6-d sub-vector of LCM descriptor (RGB-mean, RGB-variance) was kept in order to avoid feeding the HIK clusterer with negative valued features (skewness part of LCM).</p><p>BoSVW model: For the BoSVW generation, we used K n ¼400 spatio-visual words.</p><p>For the assignment to both the BoVW models and the BoSVW used in this work, a MA technique of degree 4 was employed in order to alleviate the hard-clustering effects in the histograms' generation (see 4N-soft-weighting method of <ref type="bibr" target="#b46">[48]</ref> for details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.5.">SVM classification kernels</head><p>Based on the comparative study of <ref type="bibr" target="#b43">[45]</ref> and on the state-of the art work of <ref type="bibr" target="#b5">[7]</ref> we examine RBF kernel with linear (Lin) and w 2 (Chi-sq) exponential decay as well as the histogram intersection kernel (HI) <ref type="bibr" target="#b58">[60]</ref>. Note that, recent theoretical advances on kernel SVM classification place HI kernel classifier in the same order of computational cost for evaluation as linear SVM <ref type="bibr" target="#b59">[61]</ref>, while methods using this non linear kernel have demonstrated state of art results for scene <ref type="bibr" target="#b5">[7]</ref> and object classification <ref type="bibr" target="#b59">[61]</ref>. The one-against all technique was used for the multi-class classification problems in hand. For the implementation of the SVM classifier, the public LIBSVM library <ref type="bibr" target="#b60">[62]</ref> is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation</head><p>In this subsection, evaluation of the proposed BoSVW image histogram representation, on the four datasets, namely OT8, OT15, MSRC9 and MSRC18, is presented. Performance is measured as the mean classification accuracy on all categories (i.e. average of the diagonal entries in the confusion matrix) in a 5-fold cross-validation setting. For system's performance quantification the average mean classification accuracy with its standard deviation is reported as a % percentage rate. Note that the ''plus'' symbol stands for (image) histogram concatenation and it is used every time the baseline model is combined with a new proposed classification vector. For brevity, in the tables of this section, all the image classification vectors based on a vocabulary histogram are referred to with the corresponding vocabulary naming convention.</p><p>In order to compare the proposed BoSVW representation, against a naı ¨f global representation alternative, global syn-Correlogram descriptor (GSC) is implemented and is also treated as image classification vector. GSC image descriptor is defined as the aggregate of CoTrans descriptors in an image. For the GSC construction, the CoTrans descriptors (here reference points were sampled by a grid-step of 32 pixels while the radius, R, was set to 24 pixels) were based on the sparse sampled BoVW S SIFT and BoVW S LCM vocabulary representations, leading to two GSC variations namely the GSC SIFT and the GSC LCM Based on the fixed number of CoTrans spatial bins (which is 12, in our implementation) and the BoVW S SIFT , BoVW S LCM vocabulary lengths, which were fixed to K ¼80 and K ¼400 respectively, the GSC SIFT length is {12 Â 80 ¼960-d} while the GSC LCM length is {12 Â 400¼4800-d} respectively. Note that in the GSC construction, the binarization step is skipped in order to preserve all the information conveyed by the CoTrans descriptors of the image, given that this global representation is simpler.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">Task 1</head><p>First, we report on the BoSVW and the GSC SIFT image descriptors acting as stand-alone classification vectors (Table <ref type="table" target="#tab_3">2</ref>) and subsequently we report on their concatenation with the baseline BoVW CENTRIST/SIFT classification vectors (see Tables <ref type="table">3</ref> and<ref type="table" target="#tab_4">4</ref>, respectively). It is clear from Table <ref type="table" target="#tab_3">2</ref> that spatio-visual image descriptors on their own provide low accuracy rates. Note also that for vectors of relatively low dimensionality as the 400-d BoSVW classification vector, Chi-sq kernel consistently gives superior accuracy rates, while in case of vectors of moderate dimensionality like the 960-d GSC SIFT , both Chi-sq and HI kernels compete for the best kernel choice.</p><p>The main interest of this evaluation is the results from the concatenation of the BoSVW classification vector with the baseline BoVW CENTRIST/SIFT classification vector in order to prove the proposed method's complementarity with the successful but geometric-free BoVW model. As shown in Table <ref type="table" target="#tab_5">5</ref>, in this case, where dimensionality increases to 6200 þ{400}-d¼{6600}d, HI kernel has been proved better for the most difficult dataset which is the OT15. Therefore, this will be the adopted kernel for the rest of the experiments where vector concatenation is involved.</p><p>Concatenation of the BoSVW representation with the baseline BoVW CENTRIST model (Table <ref type="table">3</ref>) adds significant improvement of about 3.5% for OT8 and OT15 datasets and of about 2.5% for MSRC9 and MSRC18 datasets. On the other hand, concatenation of the BoSVW CENTRIST model with the simpler GSC SIFT representation adds improvement which is only 0.6% for OT8 and OT15 datasets and about 1% for MSRC9 and MSRC18 datasets. Hence, the proposed BoSVW classification vector, demonstrates a consistent positive effect in performance for all datasets while being very compact in terms of dimensionality. The previous conclusions agree with the fact that global descriptions, as the GSC SIFT , convey little information for contextually disambiguating a difficult scene <ref type="bibr" target="#b7">[9]</ref>. Let us also note that the case of a dense sampled vocabulary representation (case of BoVW CENTRIST ) as the basis of the BoSVW generation was tested on OT8 dataset and accuracy dropped dramatically from 90.77 to 41% (results are not presented due to space allocation issue). This result sustains the arguments of Ssection 3.3.1 which favours the interest point detection sampling for spatio-visual information inference.</p><p>In the case that both BoSVW and GSC SIFT are fused with the baseline model (Table <ref type="table">3</ref>, last column) a small additional increase in performance is observed for all datasets except for the MSRC9 dataset. Fig. <ref type="figure">4</ref>   <ref type="table">3</ref>. Notice that almost all categories benefit from the proposed BoSVW representation. Regarding the baseline models' performance, note that that BoVW CENTRIST performs always better than BoVW SIFT classification vector (compare Table <ref type="table" target="#tab_3">2</ref> with Table <ref type="table">3</ref>), as expected from the work of <ref type="bibr" target="#b5">[7]</ref>.</p><p>Table <ref type="table">3</ref> Performance of a baseline BoVW CENTRIST classification vector and its combination with the GSC SIFT and the BoSVW classification vectors (HI kernel in use).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset Model</head><p>BoVW CENTRIST (6200-d)</p><formula xml:id="formula_7">þGSC SIFT (6200 þ960-d) þ BoSVW (6200 þ400-d) þ [GSC SIFT þBoSVW] (7560-d) OT8</formula><p>87.91 72.5 88. <ref type="bibr" target="#b16">18</ref>  Performance of a baseline BoVW SIFT classification vector and its combination with the GSC SIFT and the BoSVW classification vectors (HI kernel in use). Fig. <ref type="figure">4</ref>. Average precision modification gained per category for OT-15 and MSRC-18 datasets when the proposed method is compared against a baseline BoVW model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset Model</head><formula xml:id="formula_8">BoVW SIFT þ GSC SIFT þ BoSVW þ[GSC SIFT þ BoSVW]<label>OT8</label></formula><p>4.2.1.1. On the binarization of CoTrans descriptor. Both binary and integer CoTrans descriptors were tested in order to make sure that the binary representation suits better the visual classification task. Their performance was compared in terms of mean average classification accuracy which proved that binary representation yielded a more discriminant BoSVW representation. This was relatively anticipated as an effect of the data sparsity, in the sense that when a sparse high-dimensional integer descriptor, whose integer values are relatively small, degenerates to a binary descriptor, small amount of information is lost. Additionally, with respect to the descriptors' assignment to the BoSVW, the log-idfs weights, popular in text documents representation, were tested (''idf'' term stands for inverse document frequency). Usually, unbalanced visual word frequencies are addressed by applying idf weights. However they did not prove useful for the visual data in hand (work of <ref type="bibr" target="#b45">[47]</ref> offers an explanation for that) and subsequently the corresponding results are omitted from this experimental section. First, CoTrans reference points are sampled from the first, in terms of scale, 36 DoG detected keypoints <ref type="bibr" target="#b27">[29]</ref>, substituting the 6 Â 6 sparse grid sampling of the original implementation. The choice of the points of the highest scale of detection over a random selection was motivated by <ref type="bibr" target="#b45">[47]</ref>, where points of higher scale were verified to be more stable in a feature matching framework. Then, to fulfill the invariance property setting, first each correlogram is rotated by the dominant orientation angle of the reference keypoint (referred to as BoSVW/R1 in Table <ref type="table" target="#tab_8">7</ref>) and secondly, its radius is scaled proportionally to the reference keypoint's scale of detection (referred to as BoSVW/S in Table <ref type="table" target="#tab_8">7</ref>). These steps have also been proposed previously by <ref type="bibr" target="#b25">[27,</ref><ref type="bibr" target="#b30">32]</ref> but not experimentally verified. Alternatively to the procedure described above for rotation invariance, we also test the CoTrans descriptor with N th ¼1 (the correlogram becomes a circular distance kernel), which is by construction rotationinvariant (referred to as BoSVW/R2 in Table <ref type="table" target="#tab_8">7</ref>). This set of experiments has been evaluated on 8OT and MSRC9 datasets.</p><p>As Table <ref type="table" target="#tab_8">7</ref>, column 3 shows, a great loss of performance occurs when CoTrans descriptor is rotated according to the central keypoint dominant orientation. Loss of performance also occurs in the case of a scale invariant CoTrans descriptor (column 5). In contrast, small loss of performance occurs with the second rotation invariance method which imposes N th ¼1 (column 4). This rotation invariant descriptor performs slightly worse than the fixed one, but it carries the important advantage of low dimensionality. Based on these results, the rotation and scale invariance CoTrans optional properties, though available in our implementation, remained inactive for the classification task in hand. The above empirical observations are in agreement with recent bibliographical clues where a scale and rotation consistency among scene images of the same category is recorded <ref type="bibr" target="#b2">[4,</ref><ref type="bibr" target="#b14">16,</ref><ref type="bibr" target="#b50">52]</ref>, leading thus to the conclusion that complete rotation and scale invariance may adversely affect discriminative power of the method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">Task 2</head><p>In this second task, for the sake of completeness, integration of color information within the proposed framework is tested for the three color datasets: OT8, MSRC9 and MSRC18. A basic BoVW color classification vector, BoVW LCM , and a global spatio-visual classification vector, GSC LCM , are fused with the baseline BoVW CENTRIST representation. HI kernel on the final classification vector consistently gives superior accuracy rates for all datasets and hence the presented results will not include results with the other two kernels.</p><p>As shown in Table <ref type="table" target="#tab_9">8</ref>, both BoVW LCM and GSC LCM classification vectors cause a significant improvement when concatenated with the baseline BoVW CENTRIST representation, of about 2.5% for MSRC9 and 3.5% for MSRC18 datasets. Comparing Tables <ref type="table" target="#tab_4">4</ref> and<ref type="table" target="#tab_9">8</ref> for MSRC9/21 datasets, color spatio-visual information is proved more important than texture spatio-visual information. For these two datasets we can observe that spatial relation of color occurrence, represented in this work by the GSC LCM   <ref type="table" target="#tab_4">4</ref> and<ref type="table" target="#tab_9">8</ref> are fused together in order to provide a spatio-visual descriptor taking into account both color and texture spatio-visual co-occurrences. We conducted several sets of experiments to validate the various descriptors combinations and we provide only those combinations which managed to improve the overall performance. Total accuracy is increased to 91.49, 92.1 and 78.79 for the OT8, MSRC9 and MSRC18 respectively which, to our knowledge, are the best results reported for these datasets in the literature. Summarizing the above, the OT8 dataset takes little advantage from GSC LCM and higher advantage from BoSVW classification vector; on the other hand, the MSRC9/ 18 datasets take little advantage of the BoSVW classification vector and higher advantage of the fused BoVW LCM and GSC LCM color classification vectors respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset Model</head><formula xml:id="formula_9">BoVW CENTRIST þ BoSVW LCM þ GSC LCM þ [BoSVW LCM þ GSC LCM ]<label>OT8</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3.">Comparison with the state of the art</head><p>Since this work encodes co-location relations of visual words, it is imperative that we compare our approach (results are taken form Table <ref type="table">3</ref> above) to the state-of-the-art beyond BoVW approaches that make use of visual words' relations. We consider the approaches of: <ref type="bibr" target="#b18">[20]</ref>, that uses a CRF graphical model to capture rough spatial layout (task2 in <ref type="bibr" target="#b18">[20]</ref>) and words' co-occurrence by two (task3 in <ref type="bibr" target="#b18">[20]</ref>); <ref type="bibr" target="#b13">[15]</ref>, that uses multiple vocabularies through scale in order to add scale context in the BoVW; <ref type="bibr" target="#b17">[19]</ref>, that uses a pLSA model adding spatial pyramid matching (SPM as proposed by <ref type="bibr" target="#b3">[5]</ref>) and spatial layout to the model (SP-PLSA); <ref type="bibr" target="#b4">[6]</ref>, that uses a hybrid generative/discriminative BoVW model, included for completeness although no spatio-visual information appears in it; <ref type="bibr" target="#b12">[14]</ref>, that uses a graphical model which models pair-wise cooccurrence relationships and visual words spatial layout (no direct comparison is possible due to different number of categories and different evaluation technique); <ref type="bibr" target="#b28">[30]</ref>, that uses unbounded order co-occurrence features and <ref type="bibr" target="#b29">[31]</ref>, that uses semantic forests for BoVW in MSRC21 categories using a modified Pyramid Match Kernel for classification (here, no direct comparison is possible due to different number of categories and different evaluation technique).</p><p>Results on the MSRC dataset cannot be directly compared with other published works except the work of <ref type="bibr" target="#b28">[30]</ref>, that is also evaluated on the whole image but for a smaller MSRC-9 subset (e.g., for class ''sheep'' we do not train our classifier on ''sheep'' segmented annotated regions from the images but on the entire image considering that the label of the scene is ''sheep''). Despite this fact, we do perform a comparison with the works of <ref type="bibr" target="#b29">[31]</ref> and <ref type="bibr" target="#b12">[14]</ref> on MSRC-21 dataset, in order to examine if accuracies obtained by the holistic representation offered by our BoSVW model, can compete against region-based scene classification methods. In the following tables, if a dataset is absent from a work then the corresponding cell in the table is denoted empty by a dash symbol ''-''.</p><p>First, in Table <ref type="table" target="#tab_12">10</ref>, we compare with approaches focusing on spatio-visual knowledge integration ignoring color information. As Table <ref type="table" target="#tab_12">10</ref> depicts, our method beats current state of the art on OT15 <ref type="bibr" target="#b13">[15]</ref>, MSRC9 <ref type="bibr" target="#b28">[30]</ref> and MSRC18 <ref type="bibr" target="#b12">[14]</ref> datasets. The work of <ref type="bibr" target="#b4">[6]</ref>, which holds the current state-of-the-art on OT8 dataset, cannot preserve it on the more difficult OT15 dataset. Note that, if we recall the results of Table <ref type="table" target="#tab_4">4</ref>, when the BoSVW classification vector is concatenated with the baseline BoVW CENTRIST model, total accuracy increases to 85.62 and 75.24 for the OT15 and MSRC18 dataset respectively (note that with respect to MSRC reported rates we refer on a per-image classification basis and not on a pixel-wise basis) which, to our knowledge, are the best results reported for these challenging two datasets in the literature when color information is ignored.</p><p>Additionally and in contrast with the method of <ref type="bibr" target="#b13">[15]</ref>, the proposed bag of spatio-visual words model is based on a universal visual vocabulary, to which images of all the considered classes are projected, while in the work of <ref type="bibr" target="#b13">[15]</ref> class-specific visual vocabularies are needed. Note also that task2 of <ref type="bibr" target="#b18">[20]</ref>, where spatial layout between visual words is taken into account, is almost equivalent, in terms of accuracy rate, with the baseline BoVW SIFT model (works on pyramid grid of the image taking into account rough spatial layout of visual words). On the other hand, task3 of <ref type="bibr" target="#b18">[20]</ref>, where visual words' pairwise co-occurrence enters in the model as ''edge potential'', is inferior but close in terms of accuracy rate, with our BoSVW model (see the BoSVWþ BoVW CENTRIST combination of Table <ref type="table">3</ref>).</p><p>Only few works on scene classification include color information. Note that, to the best of our knowledge, color spatio-visual information has only appeared in the work of <ref type="bibr" target="#b61">[63]</ref> which uses a color map to weight visual words assignment to a BoVW SIFT for object classification. Still, we can compare with the works of <ref type="bibr" target="#b17">[19]</ref> and <ref type="bibr" target="#b29">[31]</ref> which include color information in their basic BoVW models. As shown in Table <ref type="table">11</ref>, with respect to spatio-color information integration we achieve to improve state of the art accuracy rates in both OT8 and MSRC18 datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4.">Discussion</head><p>Since the vocabulary generation stands as a separate problem, here we discuss which contextual information was captured by the proposed method that cannot be captured by other approaches that are built on top of a BoVW model. We first focus our analysis on the type of spatial representation that can be used upon the BoVW histograms (Fig. <ref type="figure" target="#fig_8">5</ref>) and then we discuss the complexity/representation power of three different methods often encountered in the literature.</p><p>Spatial layout of visual words can be considered in the form of: Fig. <ref type="figure" target="#fig_0">1a</ref>, BoVW histograms in a multi-scale grid partition of the image as in <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b53">55]</ref>; Fig. <ref type="figure" target="#fig_0">1b</ref>, exact visual words {x, y} locations exploited usually in a pair-wise graphical model as in <ref type="bibr" target="#b0">[2,</ref><ref type="bibr" target="#b18">20]</ref>; Fig. <ref type="figure" target="#fig_0">1c</ref>, accumulation of local correlograms around keypoints for pairs of visual words as in <ref type="bibr" target="#b19">[21,</ref><ref type="bibr" target="#b21">23,</ref><ref type="bibr" target="#b22">24]</ref> or Fig. <ref type="figure" target="#fig_0">1d</ref>, bag of local correlograms extracted from a grid as in our method. The proposed method combines the advantages of both (a) and (c) representations through the use of BoVW CENTRIST and BoSVW respectively. Those two representations can be acclaimed as complementary to each other (something similar was proposed in <ref type="bibr" target="#b62">[64]</ref>) as they combine the vertical and horizontal layout (as in Fig. <ref type="figure" target="#fig_0">1b,</ref><ref type="figure">d</ref>) with the polar layout (as in Fig. <ref type="figure" target="#fig_0">1c,</ref><ref type="figure">d</ref>).</p><p>With respect to the methods that utilize a spatial information of the form of Fig. <ref type="figure" target="#fig_0">1b</ref>, note that computational demanding graphical methods like CO-CRF of <ref type="bibr" target="#b18">[20]</ref>, perform an initial arbitrary dimensionality reduction step (pLSA, LDA algorithms) in order to work on a small set of learned ''topics'' by also imposing a Gaussian distribution on the data. Topics derivation is an unsupervised procedure; however the small set of derived ''topics'' may not be able to describe the variation often present in real scenes. With respect to methods that utilize a spatial information of the form of Fig. <ref type="figure" target="#fig_0">1c</ref>, the co-occurrence matrix approach followed by clustering on the accumulated correlogram elements of the matrix, results in clusters that encode only the relevant visual words pairwise distances while the identity of the associated pair of visual words is lost as in <ref type="bibr" target="#b21">[23]</ref>. Moreover in this case, dimensionality of the matrix, which depends on the vocabulary length, poses great challenge to the clustering algorithm. Finally, with respect to methods that utilize a spatial information of the form of either Fig. <ref type="figure" target="#fig_0">1b</ref> or c, these methods will often consider pair-wise co-occurrence relationships and thus loose discriminative information of higher order interactions. Unbounded order cooccurrence kernels of <ref type="bibr" target="#b28">[30]</ref>, is the only method providing similar information with ours but for a higher computational cost.</p><p>Regarding the proposed method, the high performance rates measured on a large range of scene categories prove that the learned spatio-visual words are able to capture part of the high combinatorial nature of visual words spatial correlations in an automatic manner. The BoSVW method can be applied upon any type of BoVW representation taking advantage of new quality vocabularies as for example the model of <ref type="bibr" target="#b63">[65]</ref>. Note here that, as the method is mainly focused in handling sparse highdimensional descriptors, the spatial template employed during the descriptor extraction is of secondary importance.</p><p>In contrast with graphical-based methods, the proposed method is easily scalable for large image corpora based on the CoTrans properties which are highlighted hereafter. CoTrans descriptor extraction is relatively slow since it follows a logpolar template. That could be mitigated by the use of GPU computing for extraction stage. Binarization compacts considerably the derived descriptor and allows the use of efficient storing structures for CoTrans (bit fields in Cþþ). Additionally, CoTrans can be indexed by mature inverted index techniques applying for histogram features. Descriptor quantization is already very fast taking advantage of the fast cosine similarity metric, when applied on sparse data. In general, the vector representation of the proposed spatio-visual classification vector is advantageous for large datasets. Linear SVM kernels should be preferred in this case, since computation time in this case scales linearly with the size of the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Concluding remarks</head><p>In this work, we have used a novel bag of spatio-visual words model, namely the BoSVW, which can be combined with a standard BoVW model in order to add local context in the representation. We have experimentally shown the BoSVWbased representation superiority to existing context-driven approaches in the OT15 scene dataset as well as in an 18-class subset of the well-known MSRC21 dataset. The proposed model efficiently handles high-dimensional spatial co-occurrence descriptors, by employing the spherical K-means clustering algorithm providing discriminative spatio-visual words for image description. The compact histogram-based BoSVW representation allows the use of appropriate non-linear SVM kernels proposed for visual classification, avoiding the need for specialized SVM kernels designed for high-dimensional features <ref type="bibr" target="#b39">[41]</ref>  <ref type="bibr" target="#b28">[30]</ref>.</p><p>Three different non-linear SVM kernels evaluate the performance of the derived classification vectors. Moreover, the advantage of the bag of spatio-visual words model against a naı ¨f global spatio-visual descriptor, namely the GSC descriptor, is experimentally verified. Local color integration is considered by the proposed GSC LCM and BoVW LCM image classification vectors. To allow spatio-color descriptor to work even better, future work may integrate interest color point detection.</p><p>Due to the method's advantageous use of the binary and sparse nature of CoTrans vectors, we consider this work as a promising step towards integrating visual words co-occurrence context in much larger, ''web-scale'' image corpora. Future work will include spatio-visual vocabulary creation using alternative clustering approaches in the context of high dimensional data, where the tradeoff between model complexity (in terms of learning and training) and system's classification speed (image assignment to the model and testing) needs to be carefully tuned. Additionally, as shown by <ref type="bibr" target="#b31">[33]</ref>, mining algorithms can efficiently handle thousands of binary transactions in order to select the most frequent and discriminative ones following some criteria. In contrast with boosting methods used by <ref type="bibr" target="#b22">[24,</ref><ref type="bibr" target="#b38">40,</ref><ref type="bibr" target="#b62">64]</ref>, data mining algorithms perform feature selection with apparent computational speed. Future work in this domain could involve the co-operation of mining techniques with the BoSVW vocabulary generation method in order to produce more discriminative spatio-visual words.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. (a) Toy visual vocabulary of 5 visual words: different colored points represent different visual characteristics of local patches; (b) an input image from category ''coast''; (c) visualization of the set of SIFT [29] local features detected in the input image (magnitude of the vector denotes the scale of detection while direction of the vector denotes the principle orientation of the patch around each feature point); (d) visualization of each local feature assigned to a single visual word from the toy visual vocabulary of Fig. 1a; (e) in green, superimposed circular CoTrans descriptors capturing local spatial configurations of the local features ignoring the visual label assigned to them; (f) in green, superimposed circular CoTrans descriptors capturing local spatial configurations of the local features taking into account the visual label assigned to them. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)</figDesc><graphic coords="2,102.84,58.64,380.16,258.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Close-up example of a CoTrans descriptor of 12 spatial bins around a reference point 'c' of the input image: on the left, the input image is assumed to be projected on the toy visual vocabulary model of Fig. 1a; in the middle, the corresponding CoTrans template encoding the presence of 4 white and 2 blue visual words; on the right, the corresponding CoTrans c-b , is derived by the concatenation of the 12h c histograms of size 5 and binarization; in the CoTrans c-b , histogram illustration, rectangles filled with black or white correspond to this visual word bin assigned with the value of 1 or 0 respectively. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)</figDesc><graphic coords="5,133.93,554.53,337.32,141.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Task 1 :Task 2 :</head><label>12</label><figDesc>Augment a basic BoVW histogram representation (consider either BoVW SIFT or BoVW CENTRIST ) with the proposed BoSVW histogram representation; include in the experiments a scale and invariant BoSVW implementation. Augment the best performing scenario of task1 with the BoVW LCM color information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Overview of the proposed image representation. (Figure better viewed in color; the reader is reffered to the web version of this article).</figDesc><graphic coords="7,103.97,58.64,397.44,244.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>--</head><label></label><figDesc>The {SIFT} I set of features on the DoG patches P per image, extracted in a sparse sampling mode (see sec 3.3.1); -K, the required number of visual words for BoVW; -K * , the required number of spatio-visual words for BoSVW; -L, the MA degree for assignment to the vocabularies (L ∈ Z: L ≤ K, K * ) -The CoTrans descriptor template parameters: N r and N th and R; The set of grid-based correlogram reference points per image, C; Output: Image classification vector comprised by a BoVW and a BoSVW based histogram representation per image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>For each image described-</head><label></label><figDesc>by the set of SIFT l-patches {x, y, {l n }} p , as given by Step I For each reference point c ∈C For each CoTrans c template falling within the spatial extent of the image I For each CoTrans c spatial bin [r i , th j ] Count the occurrence of {{l n } p } c and record it into the local histograms h c (i,j) End for. For all {i,j}, concatenate all hc(i,j) into the 1-d CoTransc vector. Binarize: CoTransc CoTransc-b; Unit normalize: || CoTransc-b|| = 1Apply online spherical K-means clustering algorithm (number of centers = K * ) on the {CoTrans c-b } X , set to build the BoSVW vocabulary. Then, for each image in I: Assign each CoTrans c-b descriptor of the image to the 4 nearest spatio-visual words of the BoSVW (weighted Cosine distance adapted from [48] is used).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>shows the average improvement on percentage classification accuracy rates per category for the OT15 (left diagram) and MSRC18 (right diagram) datasets. In this Figure, red bars correspond to the baseline BoVW CENTRIST model, while green bars correspond to the augmented {BoSVWþGSC SIFT } classification vector of Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Different spatial representations that may be used on top of a BoVW model: (a) multi-scale grids; (b) sets of points; (c) sets of correlograms; (d) correlograms on a grid. Colored points in b, c, d indicate different visual words. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article).</figDesc><graphic coords="13,44.52,627.09,516.24,86.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>S SIFT , BoVW CENTRIST and BoVW LCM vocabularies. -For each image in I:</figDesc><table /><note><p>o Assign each CENTRIST p / LCM p descriptor of the image to the 4 nearest visual words of BoVW CENTRIST and BoVW LCM vocabularies, to obtain two histogram representations per image (weighted Euclidean distance as in [48] is used). o Assign each {SIFT} p feature to the 4 nearest visual words of BoVW S SIFT vocabulary,</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc>Performance of the GSC SIFT and the BoSVW classification vectors for three different SVM kernels.</figDesc><table><row><cell>Dataset</cell><cell>Model</cell></row><row><cell></cell><cell>GSC SIFT (960-d)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>7 2.5</cell><cell>90.77 71.7</cell><cell>91.027 1.3</cell></row><row><cell>OT15</cell><cell>81.87 72.8</cell><cell>82.57 2.2</cell><cell>85.58 71.1</cell><cell>85.627 2.1</cell></row><row><cell cols="2">MSRC9 86.37 72.6</cell><cell>87.85 7 2.0</cell><cell>88.88 71.9</cell><cell>88.6 7 2.3</cell></row><row><cell cols="2">MSRC18 72.56 73.4</cell><cell>73.47 2.6</cell><cell>74.6 72.6</cell><cell>75.247 1.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5</head><label>5</label><figDesc>Performance on OT15 dataset of a baseline BoVW CENTRIST classification vector and its combination with BoSVW classification vector for three different SVM kernels.</figDesc><table><row><cell></cell><cell>87.52 72.2</cell><cell>87.78 71.5</cell><cell>90.37 2.1</cell><cell>90.66 7 1.5</cell></row><row><cell>OT15</cell><cell>78.84 72.3</cell><cell>79.54 71.4</cell><cell>84.2 7 1.1</cell><cell>85.67 1.6</cell></row><row><cell>MSRC9</cell><cell>85.48 72.6</cell><cell>86.07 71.6</cell><cell>87.047 2.2</cell><cell>86.84 7 1.8</cell></row><row><cell>MSRC18</cell><cell>71.7 73.6</cell><cell>72.6 72.2</cell><cell>74.1 7 2.6</cell><cell>74.87 1.7</cell></row><row><cell>Dataset</cell><cell cols="2">Descriptor</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">BoVW CENTRIST (6200-d)</cell><cell>þBoSVW (6600-d)</cell></row><row><cell>Lin</cell><cell cols="2">77.95 7 2.2</cell><cell></cell><cell>82.84 70.9</cell></row><row><cell>Chi-sq</cell><cell cols="2">80.017 2.1</cell><cell></cell><cell>83.02 71.3</cell></row><row><cell>HI</cell><cell cols="2">81.877 1.8</cell><cell></cell><cell>85.58 71.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>4.2.1.2. Computation times. In Table6, run-time consumed for Clustering (C-rt) and for Feature Extraction and Assignment to the vocabulary (FE&amp;A-rt) is indicatively provided with respect to the baseline BoVW CENTRIST and the proposed BoSVW vocabulary (one round of the 5-fold cross validation). The numbers next to the dataset tags, OT8 and OT15, indicate the number of training images in each dataset. All routines are implemented in Cþþ using VS.Net platform. Cþþ executables ran on a 4G Ram Pentium 3 with 4 processing units. As Table6shows, clustering CoTrans data into 400 spatio-visual words with online spherical K-means algorithm of<ref type="bibr" target="#b26">[28]</ref> is a very fast procedure despite the high dimensionality of CoTrans descriptors (almost half of the time spent on BoVW CENTRIST for example).4.2.1.3. Scale/rotation invariant CoTrans descriptor.To test the BoSVW robustness when scale and rotation invariance is integrated in CoTrans extraction, a set of experiments have been conducted, in which the l-patch's scale and orientation attributes are explicitly taken into account. In this case we need to consider the l-patch representation in its full form of [x, y, scale, ori, {l n }] p and find a way to tune CoTrans extraction based on both the ''scale'' and ''ori'' attributes of the l-patches.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6</head><label>6</label><figDesc>Indicative comparative run times report for baseline BoVW CENTRIST and BoSVW vocabulary models.</figDesc><table><row><cell>Vocabulary</cell><cell>#centers</cell><cell></cell><cell>C-rt (secs)</cell><cell></cell><cell>FE&amp;A-rt,</cell></row><row><cell>abbreviation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>OT8</cell><cell>OT15</cell><cell>OT8</cell><cell>OT15</cell></row><row><cell></cell><cell></cell><cell>(800)</cell><cell>(1500)</cell><cell>(800)</cell><cell>(1500)</cell></row><row><cell>BoVW CENTRIST</cell><cell>200</cell><cell>96</cell><cell>197</cell><cell>510</cell><cell>1004</cell></row><row><cell>BoSVW</cell><cell>400</cell><cell>55</cell><cell>111</cell><cell>2344 n</cell><cell>4427 n</cell></row><row><cell cols="2">n indicates 4N assignment.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7</head><label>7</label><figDesc>Performance of the scale/rotation BoSVW model variants (BoSVW/S, BoSVW/R1, BoSVW/R2) when combined with a baseline BoVW SIFT classification vector on the OT8 and MSRC9 datasets (HI kernel in use).</figDesc><table><row><cell cols="2">Dataset Model</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>BoVW SIFT</cell><cell>þ BoSVW</cell><cell cols="3">þ BoSVW/R1 þBoSVW/R2 þBoSVW/S</cell></row><row><cell>OT8</cell><cell>87.52 7 2.2</cell><cell cols="2">90.3 7 2.1 78.67 2.1</cell><cell>89.97 1.8</cell><cell>82.3 71.8</cell></row><row><cell cols="4">MSRC9 85.48 7 2.6 87.04 7 2.2 71.67 2.1</cell><cell>86.47 1.8</cell><cell>78.1 71.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8</head><label>8</label><figDesc>Performance of the BoVW LCM and GSC LCM classification vectors when combined with the baseline BoVW CENTRIST classification vector (HI kernel in use).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9</head><label>9</label><figDesc>Performance of a baseline BoVW CENTRIST classification vector and its combination with the BoSVW/BoVW LCM and the GSC LCM classification vectors (HI kernel in use).</figDesc><table><row><cell cols="2">Dataset Model</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">BoVW CENTRIST þ [BoSVW</cell><cell>þ[BoSVWþ BoVW LCM</cell></row><row><cell></cell><cell></cell><cell>þBoVW LCM ]</cell><cell>þGSC LCM ]</cell></row><row><cell>OT8</cell><cell>87.91 72.5</cell><cell>91.497 1.8</cell><cell>90.99 7 2.1</cell></row><row><cell cols="2">MSRC9 86.37 72.6</cell><cell>91.67 1.8</cell><cell>92.987 1.9</cell></row><row><cell cols="2">MSRC18 72.56 73.4</cell><cell>77.75 7 1.6</cell><cell>79.147 1.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10</head><label>10</label><figDesc>Performance comparison on OT8 and OT15 datasets (no color information).</figDesc><table><row><cell cols="2">Dataset Authors</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Proposed</cell><cell>[19]</cell><cell></cell><cell cols="2">[15] [20]</cell><cell></cell><cell>[6]</cell><cell>[27] [14]</cell></row><row><cell></cell><cell>method</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">SPM SP-</cell><cell></cell><cell cols="2">Task2 Task3</cell></row><row><cell></cell><cell></cell><cell></cell><cell>PLSA</cell><cell></cell><cell></cell><cell></cell></row><row><cell>OT8</cell><cell>91.02</cell><cell cols="6">87.1 87.8 90.67 87.13 90.2 92.79 -</cell><cell>-</cell></row><row><cell>OT15</cell><cell>85.62</cell><cell cols="4">83.5 83.7 85.16 -</cell><cell cols="2">82.91 -</cell><cell>-</cell></row><row><cell cols="2">MSRC9 88.88</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>80.4 -</cell></row><row><cell cols="2">MSRC18 75.24</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>68.38</cell></row><row><cell>Table 11</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">Performance comparison on OT8 and OT15 dataset (color information included).</cell></row><row><cell cols="2">Dataset Authors</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Proposed</cell><cell></cell><cell>[19]</cell><cell></cell><cell>[31]</cell><cell></cell></row><row><cell></cell><cell>method</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">SPM SP-</cell><cell>Global</cell><cell></cell><cell>Per category</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>PLSA</cell><cell>kernel</cell><cell></cell><cell>kernel</cell></row><row><cell>OT8</cell><cell>91.49</cell><cell></cell><cell cols="2">91.0 91.1</cell><cell>-</cell><cell></cell></row><row><cell cols="2">MSRC18 79.14</cell><cell></cell><cell>-</cell><cell>-</cell><cell>76.3</cell><cell></cell><cell>78.3</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>A. Bolovinou et al. / Pattern Recognition 46 (2013) 1039-1053</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>Part of this research was funded by ''N.C.S.R. Demokritos'' Ph.D student scholarship.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning generative visual models from few training examples: An incremental Bayesian approach tested on 101 object categories</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="59" to="70" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Which is the best way to organize/classify images by content?</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bosch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Mun ˜oz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Marti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="778" to="791" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Local features and kernels for classification of texture and object categories: a comprehensive study</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marsza"ek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="213" to="238" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Beyond bags of features: spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CVPR</title>
		<meeting>the CVPR</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="169" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A hybrid generative/ discriminative classification framework based on free-energy terms</title>
		<author>
			<persName><forename type="first">A</forename><surname>Perina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Castellani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jojic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision ICCV 2009</title>
		<meeting>the International Conference on Computer Vision ICCV 2009<address><addrLine>Kyoto, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-10">Oct. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Beyond the Euclidean distance: creating effective visual codebooks using the histogram intersection kernel</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV 2009</title>
		<meeting>ICCV 2009<address><addrLine>Kyoto, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-10-04">Sep. 27-Oct. 4, 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Comparing compact codebooks for visual categorization</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Veenman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan-Mark</forename><surname>Geusebroek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="450" to="462" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The role of context in object recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="520" to="527" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Context-based vision: recognizing objects using information from both 2-D and 3-D imagery</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Strat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Fischler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1050" to="1065" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Scene perception: detecting and judging objects undergoing relational violations</title>
		<author>
			<persName><forename type="first">I</forename><surname>Biederman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Mezzanotte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Rabinowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="143" to="177" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multiple class segmentation using a unified framework over mean-shift patches</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Foran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CVPR</title>
		<meeting>the CVPR</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Spatially coherent latent topic model for concurrent object segmentation and classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference in Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference in Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Belongie Object categorization using coocurrence, location and appearance</title>
		<author>
			<persName><forename type="first">C</forename><surname>Galleguillos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference in Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference in Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Anchorage, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Scene categorization via contextual visual words</title>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H C</forename><surname>Yung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CVPR 2010</title>
		<meeting>the CVPR 2010</meeting>
		<imprint>
			<biblScope unit="volume">43</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Integrated feature selection and higher-order spatial feature extraction for object categorization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CVPR08</title>
		<meeting>the CVPR08</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A performance evaluation of local descriptors</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the PAMI</title>
		<meeting>the PAMI</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1615" to="1630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Visual categorization with bags of keypoints</title>
		<author>
			<persName><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dance</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Willamowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ECCV International Workshop on Statistical Learning in Computer Vision</title>
		<meeting>the ECCV International Workshop on Statistical Learning in Computer Vision</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Scene classification using a hybrid generative/discriminative approach</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bosch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Mun ˜oz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="712" to="727" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Conditional random field for natural scene image classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference<address><addrLine>Warwick</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-09">September 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Grayscale medical image annotation using local relational features</title>
		<author>
			<persName><forename type="first">L</forename><surname>Setia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Teynor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alaa</forename><surname>Halawani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hans</forename><surname>Burkhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="2039" to="2045" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Representations of keypoint-based semantic concept detection: a comprehensive study</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="42" to="53" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Discriminative object class models of appearance and shape by correlations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CVPR06</title>
		<meeting>the CVPR06</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">II</biblScope>
			<biblScope unit="page" from="2033" to="2040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Class-specific binary correlograms for object recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Amores</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Radeva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Foreground focus: unsupervised learning from partially matching images</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Proximity distribution kernels for geometric context in category recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICCV07</title>
		<meeting>the ICCV07</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Efficient shape matching using shape contexts</title>
		<author>
			<persName><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1832" to="1837" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficient online spherical K-means clustering</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Joint Conference Neural Networks (IJCNN 2005)</title>
		<meeting>the IEEE International Joint Conference Neural Networks (IJCNN 2005)<address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-08-04">July 31-August 4, 2005</date>
			<biblScope unit="page" from="3180" to="3185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Distinctive image features form scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
		<idno>91.110</idno>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Efficient kernels for identifying unbounded-order spatial features</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CVPR</title>
		<meeting>the CVPR</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Semantic texton forests for image categorization and segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE CVPR</title>
		<meeting>the IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Exploiting textons distribution on spatial hierarchy for scene classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Battiato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Frinella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gallo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ravi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EURASIP JIVP10</title>
		<meeting>the EURASIP JIVP10</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Quack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<title level="m">Efficient Mining of Frequent and Distinctive Feature Configurations ICCV</title>
		<meeting><address><addrLine>Rio de Janeiro, Brazil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The pyramid match kernel: discriminative classification with sets of image features</title>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICCV</title>
		<meeting>the ICCV</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning human action via information maximization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CVPR</title>
		<meeting>the CVPR</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Semantic segmentation of street scenes by superpixel co-occurrence and 3D geometry</title>
		<author>
			<persName><forename type="first">B</forename><surname>Micusik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kosecka</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>in: proceedings of the ObjectEvent09</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Image indexing using color correlograms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zabih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="762" to="768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The use of bigrams to enhance text categorization</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="529" to="546" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Context-based object-class recognition and retrieval by generalized correlograms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Amores</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Radeva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Flexible spatial configuration of local image features</title>
		<author>
			<persName><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Jepson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2089" to="2104" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Relaxed matching kernels for object recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the of the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Text classification using string kernels</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lodhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J C H</forename><surname>Watkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="419" to="444" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Efficient Visual Search of Videos. Toward Category-Level Object Recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Video</forename><surname>Google</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="127" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Discovery of collocation patterns: from visual words to visual phrases</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CVPR</title>
		<meeting>the CVPR</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Creating efficient codebooks for visual recognition</title>
		<author>
			<persName><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Robust scene categorization by learning image statistics in context</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Geusebroek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Veenman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CVPR Workshop on Semantic Learning Applications in Multimedia (SLAM)</title>
		<meeting>the CVPR Workshop on Semantic Learning Applications in Multimedia (SLAM)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">On the burstiness of visual elements</title>
		<author>
			<persName><forename type="first">H</forename><surname>Je ´gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE CVPR&apos;09</title>
		<meeting>the IEEE CVPR&apos;09</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Towards optimal bag-of-features for object categorization and semantic video retrieval</title>
		<author>
			<persName><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Image and Video Retrieval (CIVR&apos;07), Amsterdam</title>
		<meeting>the ACM International Conference on Image and Video Retrieval (CIVR&apos;07), Amsterdam<address><addrLine>The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-09-11">Jul. 9-11, 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Sampling strategies for bag-of-features image classification</title>
		<author>
			<persName><forename type="first">E</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ECCV</title>
		<meeting>the ECCV</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Determining patch saliency using low-level context</title>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsuhan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ECCV</title>
		<meeting>the ECCV</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Linear spatial pyramid matching using sparse coding for image classification</title>
		<author>
			<persName><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yihong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Improving bag-of-features for large scale image search</title>
		<author>
			<persName><forename type="first">H</forename><surname>Je ´gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ECCV</title>
		<meeting>the ECCV</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Frequency-sensitive competitive learning for scalable balanced clustering on high-dimensional hyperspheres</title>
		<author>
			<persName><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="702" to="719" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Veenman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Geusebroek</surname></persName>
		</author>
		<title level="m">Proceedings of the PAMI 09</title>
		<meeting>the PAMI 09</meeting>
		<imprint/>
	</monogr>
	<note>Visual word ambiguity</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">CENTRIST: A Visual Descriptor for Scene Categorization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<idno>GIT-GVU-09-05</idno>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>GVU Center, Georgia Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Natural scene image modeling using color and texture visterms</title>
		<author>
			<persName><forename type="first">P</forename><surname>Quelhas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Odobez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Image and Video Retrieval (CIVR)</title>
		<meeting>the International Conference on Image and Video Retrieval (CIVR)<address><addrLine>Phoenix AZ</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-07">July 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Textonboost for image understanding: multi-class object recognition and segmentation by jointly modeling texture, layout, and context</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IJCV07</title>
		<meeting>the IJCV07</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Modeling the shape of the scene: a holistic representation of the spatial envelope</title>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="145" to="175" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">An open source SIFT library</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hess</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Multimedia (MM)</title>
		<meeting>the ACM Multimedia (MM)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>Rob Hess</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Building kernels from binary strings for image matching</title>
		<author>
			<persName><forename type="first">F</forename><surname>Odone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Barla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Verri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="169" to="180" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Classification using intersection kernel support vector machines is efficient</title>
		<author>
			<persName><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CVPR</title>
		<meeting>the CVPR</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">LIBSVM: A Library for Support Vector Machines</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://www.csie.ntu.edu.tw/$cjlin/libsvmS" />
	</analytic>
	<monogr>
		<title level="m">Software available from</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Top-down color attention for object recognition</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vanrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICCV</title>
		<meeting>the ICCV</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<title level="m">Spatial bag-of-features</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>Proceedings of the CVPR</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<title level="m">Locality-constrained Linear Coding for Image Classification</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>Proceedings of the CVPR</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">She is currently pursuing the Ph.D. degree at the Informatics and Telecommunications department of the National Kapodistrian University of Athens in collaboration with the Institute of Informatics and Telecommunications (Computational Intelligence Laboratory) of the National Center for Scientific Research &apos;&apos;Demokritos&apos;&apos;. Her research interests include semantic scene classification and contextual feature extraction for images</title>
		<imprint/>
	</monogr>
	<note>Anastasia Bolovinou received the M.Sc. degree in electrical and computer engineering at the National Technical University of Athens in 2006</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">His research interests lie in image processing, pattern recognition, vision and graphics, and more specifically, in document image analysis and recognition, medical image analysis as well as multimedia content analysis, search and retrieval with a particular focus on visual content</title>
	</analytic>
	<monogr>
		<title level="m">International Journal of Computer Vision. He is member of the IEEE Signal Processing Society and the European Association for Computer Graphics (Eurographics)</title>
		<meeting><address><addrLine>Xanthi, Greece; Athens, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-01">January 1999. March 1999 to March 2000. January 2003 to June 2010</date>
		</imprint>
		<respStmt>
			<orgName>Department of Electrical and Computer Engineering, Democritus University of Thrace</orgName>
		</respStmt>
	</monogr>
	<note>he was working as Adjunct Researcher at the Institute of Informatics and Telecommunications in the National Centre for Scientific Research &apos;&apos;Demokritos&apos;. He has served as co-chair of the Eurographics Workshop on 3D Object Retrieval (3DOR) in 2008 and 2009 as well as Guest Editor for the Special issue on 3D Object. Also, he is member of the Board of the Hellenic Artificial Intelligence Society for the period 2010-2012</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Phil. degree in Computational Physics from the Department of Physics, University of Oxford. Since 1992 he has been with the Institute of Informatics and Telecommunications, NCSR &apos;&apos;Demokritos&apos;&apos;, where he currently holds the position of a Senior Researcher and Head of the Computational Intelligence Laboratory. His main research interests are in image processing and document image analysis</title>
		<author>
			<persName><forename type="first">Stavros</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Perantonis is the holder of his B.S. degree in Physics from the Department of Physics</title>
		<imprint/>
		<respStmt>
			<orgName>University of Athens, his M.Sc. degree in Computer Science from the Department of Computer Science, University of Liverpool and his D ; OCR and pattern recognition</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
