<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Progressive Modality Reinforcement for Human Multimodal Emotion Recognition from Unaligned Multimodal Sequences</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Fengmao</forename><surname>Lv</surname></persName>
							<email>fengmaolv@126.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Southwest Jiaotong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Center of Statistical Research</orgName>
								<orgName type="institution">Southwestern University of Finance and Economics</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiang</forename><surname>Chen</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Platform and Content Group</orgName>
								<address>
									<settlement>Tencent</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yanyong</forename><surname>Huang</surname></persName>
							<email>huangyy@swufe.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Center of Statistical Research</orgName>
								<orgName type="institution">Southwestern University of Finance and Economics</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lixin</forename><surname>Duan</surname></persName>
							<email>lxduan@gmail.com</email>
							<affiliation key="aff3">
								<orgName type="institution">University of Electronic Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
							<email>gslin@ntu.edu.sg</email>
							<affiliation key="aff4">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Progressive Modality Reinforcement for Human Multimodal Emotion Recognition from Unaligned Multimodal Sequences</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Human multimodal emotion recognition involves timeseries data of different modalities, such as natural language, visual motions, and acoustic behaviors. Due to the variable sampling rates for sequences from different modalities, the collected multimodal streams are usually unaligned. The asynchrony across modalities increases the difficulty on conducting efficient multimodal fusion. Hence, this work mainly focuses on multimodal fusion from unaligned multimodal sequences. To this end, we propose the Progressive Modality Reinforcement (PMR) approach based on the recent advances of crossmodal transformer. Our approach introduces a message hub to exchange information with each modality. The message hub sends common messages to each modality and reinforces their features via crossmodal attention. In turn, it also collects the reinforced features from each modality and uses them to generate a reinforced common message. By repeating the cycle process, the common message and the modalities' features can progressively complement each other. Finally, the reinforced features are used to make predictions for human emotion. Comprehensive experiments on different human multimodal emotion recognition benchmarks clearly demonstrate the superiority of our approach. * →C :</p><p>Ẑ[i] * →C ∈ R T C •d×1 and then process them as follows:</p><p>α [i] * →C = exp(µ [i] * →C ) * ∈{L,V,A} exp(µ [i] * →C ) , Z</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human multimodal emotion recognition focuses on recognizing the sentiment attitude of humans from video clips <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b4">5]</ref>. This task involves time-series * Corresponding authors. data of different modalities, e.g., natural language, facial gestures, and acoustic behaviors. The multimodal setting can provide rich information for thorough sentiment understanding. In practice, however, the collected multimodal streams are usually asynchronous, due to the variable sampling rates for sequences from different modalities. For example, the video frame with a depressed facial expression may relate to a negative word spoken in the past. The asynchrony across different modalities can increase the difficulty on conducting efficient multimodal fusion.</p><p>The previous works address the above issues by predefined word-level alignment <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b23">24]</ref>. To this end, the visual and acoustic sequences are first manually aligned in the resolution of the textual words. Multimodal fusion is then conducted on the aligned time steps. However, the manual word-alignment process is usually laborintensive and requires domain knowledge. Recently, <ref type="bibr">Tsai et al.</ref> propose the Multimodal Transformer (MulT) approach to fuse crossmodal information from unaligned data sequences <ref type="bibr" target="#b17">[18]</ref>. Their approach introduces the modality reinforcement unit to reinforce a target modality with information from a source modality by learning the directional pairwise attention between elements across modalities (see Fig. <ref type="figure">1(a)</ref>), based on the recent advances of transformer <ref type="bibr" target="#b19">[20]</ref>. By exploring the crossmodal interaction between elements via the crossmodal attention operations, MulT can implement multimodal fusion from asynchronous sequences without explicitly aligning the data.</p><p>In their approach, however, the modality reinforcement of each direction is performed independently and does not exchange information with each other. Hence, the multimodal fusion only appears between each directional modality pair, but not across all the modalities involved in human MRU [0]   𝑉→𝐿 MRU [1]   𝑉→𝐿 MRU [2]   𝑉→𝐿 MRU [𝐷]   𝑉→𝐿 𝑍𝐿 FFL [0]   FFL [1]   FFL [2]   FFL [𝐷]   𝑍𝑉 MRU [0]   𝐴→𝐿 MRU [1]   𝐴→𝐿 MRU [2]   𝐴→𝐿 MRU [𝐷]   𝐴→𝐿 FFL [0]   FFL [1]   FFL [2]   FFL [𝐷]   𝑍𝐴 𝑍𝐿</p><formula xml:id="formula_0">𝑍 [𝐷+1] 𝐿 (a) (b)</formula><p>MRU [0]   𝐴→𝐿 MRU [0]   𝑉→𝐴 MRU [0]   𝐿→𝐴 MRU [0]   𝑉→𝐿 MRU [1]   𝐴→𝐿 MRU [1]   𝑉→𝐴 MRU [1]   𝐿→𝐴 MRU [1]   𝑉→𝐿 MRU [2]   𝐴→𝐿 MRU [2]   𝑉→𝐴 MRU [2]   𝐿→𝐴 MRU [2]   𝑉→𝐿 MRU [0]   𝐿→𝑉 MRU [0]   𝐴→𝑉 MRU [1]   𝐿→𝑉 MRU [1]   𝐴→𝑉 MRU [2]   𝐿→𝑉 MRU [2]   𝐴→𝑉 MRU [𝐷]   𝐴→𝐿 MRU [𝐷]   𝑉→𝐴 MRU [𝐷]   𝐿→𝐴 MRU [𝐷]   𝑉→𝐿 MRU [𝐷]   𝐿→𝑉 MRU s→t represents a modality reinforcement unit, in which a source modality s reinforces a target modality t by attending to the crossmodal interaction between elements. FFL [i] represents a feed-forward layer. (a) the low-level version in which the target modality is reinforced by repeatedly attending to the low-level features of the source modality; (b) the high-level version in which the target modality is reinforced by repeatedly attending to higher-level features of the source modality. emotion recognition. It is inefficient to fuse the sequences of multiple modalities by using the pairwise manner. For example, the redundant information can be introduced by directly concatenating the visual sequence reinforced by the language modality and that reinforced by the acoustic modality. It is crucial to conduct multimodal fusion by considering the three-way interactions across all the involved modalities.</p><p>Moreover, the independent pairwise fusion approach fails to exploit the high-level features of the source modality. For each directional modality pair, as shown in Fig. <ref type="figure">1</ref>(a), the target modality is reinforced by repeatedly attending to the low-level features of the source modality. Intuitively, the deep interactions cross modalities cannot be explored via the semi-shallow structure. Their approach also notices this problem and attempts to implement crossmodal attention via the high-level features of the source modality by stacking feed-forward layers over the source modality (see Fig. <ref type="figure">1(b)</ref>). However, reduced performance is observed. This is because that the source branch does not receive clear supervision to update its feed-forward layers, since the modality reinforcement operations mainly focus on generating a reinforced target modality. As a result, it is unclear whether the high-level features of the source modality are better than the low-level features. Instead, the increased modal complexity can reduce the performance.</p><p>Motivated by the above observations, this work proposes the Progressive Modality Reinforcement (PMR) approach for multimodal fusion from unaligned multimodal sequences. Our approach introduces a message hub to exchange information with each modality. As shown in Fig. <ref type="figure">2</ref>, the message hub can send common messages to each modality in order to reinforce their features via crossmodal attention. In turn, it also collects the reinforced features from each modality and uses them to generate an improved common message. In our approach, hence, the common message and the modalities' features progressively complement each other. Moreover, we introduce a dynamic filter mechanism in the modality reinforcement unit to dynamically determine the passed proportions of the reinforced features. Compared with the prior MulT model <ref type="bibr" target="#b17">[18]</ref>, the advantage of our approach lies in two aspects. First, the common message promotes effective information flow across modalities and encourages the crossmodal attention operations to explore the element-level dependencies across all the three modalities instead of the directional pairwise dependencies. Second, the progressive reinforcement strategy provides an effective way to leverage the high-level features of the source modality for modality reinforcement. Unlike in Fig. <ref type="figure">1(b)</ref>, the feature of the source modality can receive clear supervision in the reinforcement unit where it is considered as the target modality. The superiority of our approach is verified via extensive empirical experiments on different human multimodal emotion recognition benchmarks with both the word-aligned setting and the unaligned setting.</p><p>To sum up, the contributions of this work are mainly three-fold:</p><p>• We introduce the message hub to explore the three-way interactions across all the involved modalities under the background of multimodal fusion from unaligned multimodal sequences.</p><p>• We propose the progressive strategy to leverage the high-level features of the source modality for multimodal fusion.</p><p>• Our approach can obtain better results than the existing state-of-the-art works over different human multimodal emotion recognition benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Human multimodal emotion recognition requires to infer the sentiment attitude of humans from video clips <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b4">5]</ref>. The crucial point lies in multimodal fusion from data sequences of different modalities such as natural language, video frames and acoustic signals <ref type="bibr" target="#b18">[19]</ref>. Compared to multimodal fusion from static modalities like images <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b14">15]</ref>, this task requires to fuse crossmodal information from time-series signals. The early works simply adopt the early-fusion strategy by concatenating the input sequences from different modalities <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b7">8]</ref> or the late-fusion strategy by combining the high-level information learnt from each individual modality <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17]</ref>. Furthermore, Gan et al. propose to infer the joint representations of different modalities by probabilistic graphical models <ref type="bibr" target="#b4">[5]</ref>. Although these prior works obtain better performance than learning from a single modality, they do not explicitly consider the inherent dependencies between elements of sequences from different modalities, which are crucial for efficient multimodal fusion. To this end, the recent works include a manual step to align the visual and acoustic sequences in the resolution of textual words before training <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b12">13]</ref>. These works perform multimodal fusion on the word-aligned time steps by hierarchical attention mechanism <ref type="bibr" target="#b6">[7]</ref>, nonverbal temporal interaction <ref type="bibr" target="#b20">[21]</ref>, cyclic translation <ref type="bibr" target="#b12">[13]</ref>, etc. However, the manual word-alignment process is usually labor-intensive and time-consuming. Moreover, the word-level multimodal fusion ignores the long-range dependencies between elements from different modalities.</p><p>To fuse information from unaligned multimodal sequences, the early work explores the dependencies between elements across modalities according to the maximum mutual information criterion <ref type="bibr" target="#b24">[25]</ref>. However, its performance is far from satisfactory due to the shallow learning architecture. Recently, Tsail et al. propose the crossmodal attention mechanism to learn the inherent correlations across modalities <ref type="bibr" target="#b17">[18]</ref>. Their approach repeatedly reinforces one modality with information from the other modalities through learning the directional pairwise attention between elements of different modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Progressive Modality Reinforcement</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem statement</head><p>In this work, the human emotion recognition task involves three major modalities, i.e., language (L), video (V ), and audio (A). Denote by X {L,V,A} ∈ R T {L,V,A} ×d {L,V,A} the input sequences from the corresponding modalities. T (.) and d (.) represent the sequence length and feature dimension, respectively. Our goal is to perform efficient multimodal fusion from unaligned multimodal data sequences, in order to obtain the representation that can produce desirable performance in sentiment attitude prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Preliminary -crossmodal attention.</head><p>The crossmodal attention operation reinforces the target modality with information from a source modality by learning the directional pairwise attention between them <ref type="bibr" target="#b17">[18]</ref>. Denote by X s ∈ R Ts×ds the data sequence from the source modality and X t ∈ R Tt×dt the data sequence from the target modality, where s, t ∈ {L, V, A}. Similar to the self-attention mechanism, the crossmodal attention unit involves Querys, Keys, and Values, which are defined as</p><formula xml:id="formula_1">Q t = X t W Qt with W Qt ∈ R dt×d k , K s = X s W Ks with W Ks ∈ R ds×d k , and V s = X s W Vs with W Vs ∈ R ds×dv ,</formula><p>respectively. One individual head of crossmodal attention is defined as:</p><formula xml:id="formula_2">Y t = CA s→t (X s , X t ) = softmax( Q t K T s √ d k )V s = softmax( X t W Qt W T Ks X T s √ d k )X s W Vs ,</formula><p>where Y t ∈ R Tt×dv . The full crossmodal attention operation with h heads is represented as Y t = CA mul s→t (X s , X t ), where Y t ∈ R Tt×hdv . The target modality is reinforced by encouraging the model to attend to crossmodal interaction between elements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Model overview</head><p>Our model is trained in an end-to-end manner. Following <ref type="bibr" target="#b17">[18]</ref>, we use a 1D temporal convolutional layer to process the input sequences and then augment them by the positional embedding. Denote by Z {L,V,A} ∈ R T {L,V,A} ×d the processed sequences. Note that the 1D temporal convolutional layer projects the features of different modalities to the identical dimension by controlling the kernel size used for each modality. The common message is initialized by concatenating the low-level sequence from each modality:</p><formula xml:id="formula_3">Z C = [Z L , Z V , Z A ], where Z C ∈ R T C ×d and T C = T L + L V + L A .</formula><p>Immediately, the modality reinforcement layers will repeatedly reinforce Z C and Z {L,V,A} by exploiting the correlations between elements across modalities. Fig. <ref type="figure">2</ref> displays the information flow across the modality reinforcement layers. We then concatenate the reinforced features as</p><formula xml:id="formula_4">[Z C , Z L , Z V , Z A ] ∈ R 2T C ×d</formula><p>and pass it through a transformer layer. Finally, several fully-connected layers are included to make the predictions for human emotion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Message Hub</head><p>MUM [0]  MRU [0]   𝐶→𝐴 MRU [0]   𝐶→𝐿 MRU [0]   𝐶→𝑉 MUM [1]  MRU [1]   𝐶→𝐴 MRU [1]   𝐶→𝐿 MRU [1]   𝐶→𝑉 MUM [2]  MRU [2]   𝐶→𝐴 MRU [2]   𝐶→𝐿 MRU [2]   𝐶→𝑉 MRU [𝐷]   𝐶→𝐴 MRU [𝐷]   𝐶→𝐿 MRU [𝐷]   𝐶→𝑉 MUM [𝐷]   𝑍 [0]   𝐶</p><formula xml:id="formula_5">𝑍 [0] 𝐴 𝑍 [0] 𝑉 𝑍 [0] 𝐿 𝑍 [𝐷+1] 𝐶 𝑍 [𝐷+1] 𝐴 𝑍 [𝐷+1] 𝑉 𝑍 [𝐷+1] 𝐿 Figure 2:</formula><p>The information flow across the modality reinforcement layers of the proposed model. MUM [i] represents the message update module in which the common message is reinforced by the reinforced modalities' features from the next layer.</p><formula xml:id="formula_6">MRU [i]</formula><p>C→ * represents the modality reinforcement unit in which the corresponding target modality is reinforced by the common message.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Progressive modality reinforcement</head><p>Initially, both the modalities' features Z {L,V,A} and the common message Z C do not carry information about the interactive relationship between different modalities, which is crucial for efficient multimodal fusion. In the modality reinforcement layers, Z C and Z {L,V,A} progressively complement each other by exploiting the inherent correlations between elements across modalities. To be specific, each layer includes three modality reinforcement units for updating the modalities' features Z {L,V,A} and one message update module for updating the common message Z C . Denote by MUM [i] the message update module and MRU</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>[i]</head><p>C→ * the modality reinforcement unit for the corresponding modality, where * ∈ {L, V, A}. The superscript [i] indicates the i-th modality reinforcement layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modality reinforcement unit.</head><p>The architecture of the modality reinforcement unit</p><formula xml:id="formula_7">MRU [i]</formula><p>C→ * is shown in Fig. <ref type="figure" target="#fig_2">3</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(a). It takes Z [i]</head><p>C and Z</p><p>[i] * as its inputs and outputs the reinforced features Z</p><formula xml:id="formula_8">[i+1] * : Z [i+1] * = MRU [i] C→ * (Z [i] C , Z [i] * ),</formula><p>where * ∈ {L, V, A} and Z</p><p>[i+1] * ∈ R T * ×d . Unlike in MulT, all the three modalities will participate in a single modality reinforcement unit via the common message.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Specifically, MRU</head><p>[i] C→ * reinforces Z</p><p>[i] * by two branches, including a self-attention one and a crossmodal attention one:</p><formula xml:id="formula_9">Z [i] C→ * = CA mul C→ * (LN(Z [i] C ), LN(Z [i] * )), Z [i] * = SA mul (LN(Z [i] * )),</formula><p>where Z</p><p>[i] C→ * , Z</p><p>[i] * ∈ R T * ×d , CA mul and LN represent the multi-head self-attention operation and the layer normalization operation, respectively. Immediately, the reinforced features Z</p><p>[i] * and Z</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>[i]</head><p>C→ * are processed via the following dynamic filter mechanism:</p><formula xml:id="formula_10">G [i] * = sigmoid(Z [i] * • W [i] * + Z [i] C→ * • W [i] C→ * + b [i] * ), Z [i] * = G [i] * ⊙ Z [i] * + (1 − G [i] * ) ⊙ Z [i] C→ * , where W [i] * ∈ R d×d , W [i] C→ * ∈ R d×d , and b [i] * ∈ R T * ×d .</formula><p>The passed proportions of each branch can be dynamically determined via the learnable parameters W</p><p>[i] * and b</p><p>[i] * . This operation enables to filter information produced by incorrect crossmodal interactions. Finally, as in the transformer model <ref type="bibr" target="#b19">[20]</ref>, a position-wise feed-forward layer with skip connection will process Z </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Message update module.</head><p>The reinforced features Z</p><formula xml:id="formula_11">[i+1]</formula><p>{L,V,A} will also be used to reinforce the common message</p><formula xml:id="formula_12">Z [i]</formula><p>C in the previous modality reinforcement layer. The architecture of MUM [i] is displayed in Fig. <ref type="figure" target="#fig_2">3</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(b). It takes Z</head><formula xml:id="formula_13">[i] C and Z [i+1]</formula><p>{V,L,A} as its inputs and outputs the reinforced common message</p><formula xml:id="formula_14">Z [i+1] C : Z [i+1] C = MUM [i] (Z [i+1] V , Z [i+1] L , Z [i+1] A , Z [i] C ).</formula><p>Specifically, the message update module includes three modality reinforcement units, each of which reinforces the common message Z </p><formula xml:id="formula_15">[i] * →C , Z [i]</formula><p>C is reinforced by attending to the elements of Z</p><formula xml:id="formula_16">[i+1] * : Z [i] * →C = MRU [i] * →C (Z [i+1] * , Z [i] C ),</formula><p>where Z</p><p>[i] * →C ∈ R T C ×d . The three-way interactions across all the involved modalities can be explored via the selfattention operation over Z</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>[i]</head><p>C (see Fig. <ref type="figure" target="#fig_2">3(a)</ref>). Immediately,</p><formula xml:id="formula_17">Z [i] * →C is fused into Z [i]</formula><p>C via an attention layer. To this end, we first obtain a reshaped counterpart of</p><formula xml:id="formula_18">Z [i]</formula><p>MRU [𝑖]   𝐴→𝐶 MRU [𝑖]   𝐿→𝐶 MRU [𝑖]   𝑉→𝐶 𝑍 [𝑖]   𝐶 𝑍 [𝑖]   𝐶 𝑍 [𝑖]   𝐶 𝑍 [𝑖+1]   𝐴 𝑍 [𝑖+1]   𝐿 𝑍 [𝑖+1]   𝑉 𝑍 [𝑖]   𝐴→𝐶 𝑍 [𝑖]   𝐿→𝐶 𝑍 [𝑖]   𝑉→𝐶 𝑍 [𝑖+1]   𝐶 MUM [𝑖]   CA mul   where C→ * unit. Compared with the prior MulT model <ref type="bibr" target="#b17">[18]</ref> which reinforces the target modality by repeatedly attending to the low-level features of the source modality, our approach enables Z * and Z C to progressively complement each other, i.e., Z C reinforces Z * , and in turn, the reinforced Z * reinforces Z C to produce a better common message. We also note that our approach will contain fewer modality reinforcement units at each layer if more modalities are involved (i.e., A 2 n for MulT and 2n for our approach with n indicating the modality number). Algorithm 1 displays the information flow across the modality reinforcement layers.</p><formula xml:id="formula_19">U ∈ R T C •d×1 , W [i] * →C ∈ R T C •d×T C •d and b [i] * →C ∈ R T C •d×1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental setup</head><p>We follow the common protocol of the prior works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21]</ref> and conduct experiments on the standard human multimodal emotion recognition benchmarks, including Algorithm 1 The forward propagation procedure of the modality reinforcement layers. Input: the sequences processed by 1D temporal convolution and positional embedding: Z {L,V,A} ∈ R T {L,V,A} ×d ; the layer number: D. Output: the reinforced modalities' features: Z {L,V,A} ; the reinforced common message: Z C . 1: Initialize the modalities' features:</p><formula xml:id="formula_20">Z [0] {L,V,A} = Z {L,V,A} ; 2: Initialize the common message: Z [0] C = [Z L , Z V , Z A ]; 3: i = 0; 4: while i D do 5:</formula><p>Update the modalities' features:</p><formula xml:id="formula_21">Z [i+1] * = MRU [i] C→ * (Z [i] C , Z [i] * ), where * ∈ {L, V, A}; 6:</formula><p>Update the common message:</p><formula xml:id="formula_22">Z [i+1] C = MUM [i] (Z [i+1] V , Z [i+1] L , Z [i+1] A , Z [i] C ); 7: i = i + 1; 8: end while 9: Z C = Z [D+1] C ; Z {L,V,A} = Z [D+1]</formula><p>{L,V,A} . 10: return Z C and Z {L,V,A} . CMU-MOSI <ref type="bibr" target="#b23">[24]</ref>, CMU-MOSEI <ref type="bibr" target="#b22">[23]</ref> and IEMOCAP <ref type="bibr" target="#b1">[2]</ref>. The experiments are conducted on both the word-aligned and unaligned settings.</p><p>CMU-MOSI is a dataset that contains 2,199 samples of  <ref type="bibr" target="#b23">[24]</ref> 45.0 76.9 77.0 RAVEN <ref type="bibr" target="#b20">[21]</ref> 50.0 79.1 79.5 MCTN <ref type="bibr" target="#b12">[13]</ref> 49 IEMOCAP is a dataset that contains 4,453 samples of video clips <ref type="bibr" target="#b1">[2]</ref>. Its predetermined data split includes 2,717 training samples, 798 validation samples and 938 testing samples. The acoustic and visual features are extracted at the sampling rate of 12.5 and 15 Hz, respectively. Following <ref type="bibr" target="#b20">[21]</ref>, we focus on recognizing 4 kinds of emotions (i.e., happy, sad, angry and neutral) in each video clip. Moreover, this setting is established as a multi-label task, since the sad and the angry emotions can exit in a video clip simultaneously. In agreement with the prior works <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b20">21]</ref>, we evaluate the performance by the binary classification accuracy and the F1 score for each emotion class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation details</head><p>To extract features of the textual modality, we convert the video transcripts into the pre-trained Glove model to obtain 300-dimensional word embeddings <ref type="bibr" target="#b11">[12]</ref>. For the visual modality, we process the video frames by Facet to generate 35 facial action units that represent the facial muscle movement <ref type="bibr" target="#b0">[1]</ref>. To extract features of the acoustic modality, we process the acoustic signals by COVAREP to obtain 74-dimensional features <ref type="bibr" target="#b2">[3]</ref>.</p><p>Table <ref type="table" target="#tab_2">1</ref> displays the hyperparameters used in each benchmark. The kernel size relates to the 1D temporal convolutional layer which is used to process the input sequences. In each benchmark, both the crossmodal attention operation and the self-attention operation use the same number of attention heads. The hyper-parameters are determined on the validation set. ceive clear supervision to be updated due to the independent reinforcement structure of MulT.</p><p>In the next row, we introduce the progressive reinforcement strategy for each modality pair. To this end, the reinforced target modality is in turn used to reinforce the source modality, e.g., the visual modality and the language modality progressively reinforce each other. Unlike in MulT, the source modality can receive clear supervision to be updated since it is a target modality as well. The progressive reinforcement strategy provides an efficient way to leverage the high-level features of the source modality. The performance from the third row clearly verifies the above discussion. Moreover, we introduce the message hub to exchange information with each modality. Improved performance can be observed from the fourth row. This component improves the performance by encouraging the model to explore the three-way interactions across all the modalities. Finally, we introduce the dynamic filter mechanism in the modality reinforcement unit, which can also make an effective contribution to multimodal fusion. Qualitative analysis. Fig. <ref type="figure" target="#fig_4">4</ref> displays the visualization for the crossmodal interaction between elements. The visualization sample of our approach and MulT are displayed in the upper part and the bottom part, respectively. We can see that our approach can correlate the emotion related textual word with the corresponding video clips well. Compared to MulT, our approach can encourage the model to attend to more meaningful signals across the two modalities. From the visualization sample of MulT, the correlations between the textual words and the video clips are not clear.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This work proposes the progressive modality reinforcement approach towards multimodal fusion from unaligned multimodal sequences, under the background of human multimodal emotion recognition. To this end, we introduce a message hub to exchange information with each modality. The message hub can encourage a more efficient multimodal fusion by exploring the inherent correlations across all the modalities via the common message. Moreover, the common message and the modalities' features progressively complement each other by attending to the crossmodal interaction between elements. The progressive reinforcement strategy provides an effective way to leverage the high-level features of the source modality in modality reinforcement. The experimental results over different benchmarks clearly demonstrate that our approach obtains better results than the existing state-of-the-art works.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>[i] * and generate Z[i+1] * for the next modality reinforcement layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>C</head><label></label><figDesc>by one modality. Denote by MRU[i] * →C the corresponding modality reinforcement unit, where * ∈ {V, L, A}. In MRU</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: (a) The modality reinforcement unit in which the features of a target modality are reinforced by a source modality via crossmodal attention. PFF represents the positionwise feed-forward layer. CA mul s→t the crossmodal SA mul represents the self-attention operation. (b) The message update module in which the common message is updated by the reinforced features of each modality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>are learnable parameters. The attention layer can dynamically control the passed information in Z [i] * →C and generate an informative common message. Finally, we pass Z [i] C through a position-wise feed-forward layer with skip connection and obtain the output Z [i+1] C . In the next modality reinforcement layer, Z</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Visualization for the crossmodal correlations on the CMU-MOSI benchmark. The visualization samples of our approach and MulT are displayed in the upper part and the bottom part, respectively. We conduct the visualization by observing the crossmodal attention weights of the corresponding modality reinforcement unit (i.e., MRU C→L for the proposed approach and MRU V →L for MulT) in the fourth reinforcement layer. The textual words which are closely related to human emotion recognition are displayed in red. The textual words above the video clips are the corresponding spoken words.</figDesc><graphic url="image-1.png" coords="8,67.44,75.52,460.33,204.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>The hyperparameter settings adopted in each human multimodal emotion recognition benchmark.</figDesc><table><row><cell>Setting</cell><cell>CMU-MOSEI</cell><cell>CMU-MOSI</cell><cell>IEMOCAP</cell></row><row><cell>Optimizer</cell><cell>Adam</cell><cell>Adam</cell><cell>Adam</cell></row><row><cell>Batch size</cell><cell>32</cell><cell>64</cell><cell>32</cell></row><row><cell>Learning rate</cell><cell>1e-3</cell><cell>1e-3</cell><cell>1e-3</cell></row><row><cell>Epoch number</cell><cell>100</cell><cell>120</cell><cell>60</cell></row><row><cell>Feature size d</cell><cell>40</cell><cell>40</cell><cell>40</cell></row><row><cell>Attention head h</cell><cell>10</cell><cell>8</cell><cell>8</cell></row><row><cell>Kernel size (L/V/A)</cell><cell>3/3/3</cell><cell>3/3/3</cell><cell>3/3/5</cell></row><row><cell>Reinforcement layer D</cell><cell>5</cell><cell>4</cell><cell>4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison on the CMU-MOSI benchmark under both the word-aligned setting and the unaligned setting.</figDesc><table><row><cell>Setting</cell><cell>Method</cell><cell cols="3">Acc7(%) Acc2(%) F1(%)</cell></row><row><cell></cell><cell>EF-LSTM</cell><cell>33.7</cell><cell>75.3</cell><cell>75.2</cell></row><row><cell></cell><cell>LF-LSTM</cell><cell>35.3</cell><cell>76.8</cell><cell>76.7</cell></row><row><cell></cell><cell>MFM [19]</cell><cell>36.2</cell><cell>78.1</cell><cell>78.1</cell></row><row><cell>Aligned</cell><cell>RAVEN [21]</cell><cell>33.2</cell><cell>78.0</cell><cell>76.6</cell></row><row><cell></cell><cell>MCTN [13]</cell><cell>35.6</cell><cell>79.3</cell><cell>79.1</cell></row><row><cell></cell><cell>MulT [18]</cell><cell>40.0</cell><cell>83.0</cell><cell>82.8</cell></row><row><cell></cell><cell>PMR(ours)</cell><cell>40.6</cell><cell>83.6</cell><cell>83.4</cell></row><row><cell></cell><cell>EF-LSTM</cell><cell>31.0</cell><cell>73.6</cell><cell>74.5</cell></row><row><cell></cell><cell>LF-LSTM</cell><cell>33.7</cell><cell>77.6</cell><cell>77.8</cell></row><row><cell>Unaligned</cell><cell>RAVEN [21] MCTN [13]</cell><cell>31.7 32.7</cell><cell>72.7 75.9</cell><cell>73.1 76.4</cell></row><row><cell></cell><cell>MulT [18]</cell><cell>39.1</cell><cell>81.1</cell><cell>81.0</cell></row><row><cell></cell><cell>PMR(ours)</cell><cell>40.6</cell><cell>82.4</cell><cell>82.1</cell></row><row><cell cols="5">short monologue video clips [24]. Its predetermined data</cell></row><row><cell cols="5">split includes 1,284 training samples, 229 validation sam-</cell></row><row><cell cols="5">ples and 686 testing samples. The acoustic features and</cell></row><row><cell cols="5">the visual features are extracted at the sampling rate of</cell></row><row><cell cols="5">12.5 and 15 Hz, respectively. Each multimodal sample</cell></row><row><cell cols="5">has a sentiment score which ranges from -3 (strongly neg-</cell></row><row><cell cols="5">ative) to 3 (strongly positive). In agreement with the prior</cell></row><row><cell cols="5">works [18, 19], we evaluate the performance by the follow-</cell></row><row><cell cols="5">ing metrics: 7-class accuracy (i.e., Acc 7 ), binary accuracy</cell></row><row><cell cols="2">(i.e., Acc 2 ) and F1 score.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">CMU-MOSEI is a dataset that contains 22,856 samples of</cell></row><row><cell cols="5">movie review video clips from YouTube [23]. Its prede-</cell></row><row><cell cols="5">termined data split includes 16,326 training samples, 1,871</cell></row><row><cell cols="5">validation samples and 4,659 The acoustic</cell></row><row><cell cols="5">and the visual features are extracted at the sam-</cell></row><row><cell cols="5">pling rate of 20 and 15 Hz, respectively. Likewise, each</cell></row><row><cell cols="5">multimodal sample has a sentiment score ranging from -3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparison on the CMU-MOSEI benchmark under both the word-aligned setting and the unaligned setting.</figDesc><table><row><cell>Setting</cell><cell>Method</cell><cell cols="3">Acc7(%) Acc2(%) F1(%)</cell></row><row><cell></cell><cell>EF-LSTM</cell><cell>47.4</cell><cell>78.2</cell><cell>77.9</cell></row><row><cell></cell><cell>LF-LSTM</cell><cell>48.8</cell><cell>80.6</cell><cell>80.6</cell></row><row><cell></cell><cell>G-MFN</cell><cell></cell><cell></cell></row><row><cell>Aligned</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work was supported by the National Natural Science Foundation of China (No.11829101) and the Fundamental Research Funds for the Central Universities of China (No.JBK1806002). G. Lin's participation was supported by an NTU Start-up Grant and MOE Tier-1 research grants: RG28/18 (S), RG22/19 (S) and RG95/20.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Performance comparison</head><p>The proposed approach is compared to the existing state-of-the-art baselines, including Early Fusion LSTM (EF-LSTM), Late Fusion LSTM (LF-LSTM), Multimodal Factorization Model (MFM) <ref type="bibr" target="#b18">[19]</ref>, Graph-MFN (G-MFN) <ref type="bibr" target="#b23">[24]</ref>, Recurrent Attended Variation Embedding Network (RAVEN) <ref type="bibr" target="#b20">[21]</ref>, Multimodal Cyclic Translation Network (MCTN) <ref type="bibr" target="#b12">[13]</ref>, Multimodal Transformer (MulT) <ref type="bibr" target="#b17">[18]</ref>. Of these, MulT and LF-LSTM can be applied directly to the unaligned setting. For the other methods, we include the Connectionist Temporal Classification (CTC) alignment loss <ref type="bibr" target="#b5">[6]</ref> into the learning objective, in order to make them suitable for the unaligned setting.</p><p>Word-aligned setting. This setting requires an extra step to manually align the visual and acoustic streams in the resolution of textual words. The multimodal fusion is then conducted on the word-aligned time steps. We display the experimental results of each approach in the upper part of Ta-ble 2 -4. Compared with the other baselines, our proposed approach obtains better performance on different metrics over all the three benchmarks.</p><p>Unaligned setting. This setting requires to fuse crossmodal information directly from the unaligned multimodal sequences and is more challenging than the word-aligned setting. We display the comparison of each approach in the bottom part of Table 2 -4. We can draw the following observations. First, except for MulT, most of the compared baselines obtain poor performance on the unaligned setting since their models do not consider the crossmodal interaction between elements. Second, our approach can outperform MulT on different metrics over all the three benchmarks. We can see that the performance improvement of our approach is more significant in the unaligned setting than in the word-aligned setting. This observation indicates that the technical superiority of our approach mainly lies in better capturing the dependencies between elements across modalities, which is consistent with our motivation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Analysis</head><p>Ablation study. Table <ref type="table">5</ref> displays the ablation study on the CMU-MOSEI benchmark. The first two rows display the performance of the MulT model implemented by the lowlevel version (see Fig. <ref type="figure">1(a)</ref>) and the high-level version (see Fig. <ref type="figure">1(b</ref>)), respectively. The high-level version of MulT is implemented by stacking feed-forward layers over the source modality. We can see that worse performance is obtained by the high-level version, which seems unreasonable at first sight. This can be attributed to the reason that the source modality of each directional modality pair cannot re-</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Openface: An open source facial behavior analysis toolkit</title>
		<author>
			<persName><forename type="first">Tadas</forename><surname>Baltrusaitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">IEMO-CAP: interactive emotional dyadic motion capture database</title>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Busso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murtaza</forename><surname>Bulut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi-Chun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abe</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Mower</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeannette</forename><forename type="middle">N</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungbok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shrikanth</forename><forename type="middle">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lang. Resour. Evaluation</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="335" to="359" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">COVAREP -A collaborative voice analysis repository for speech technologies</title>
		<author>
			<persName><forename type="first">Gilles</forename><surname>Degottex</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Kane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Drugman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tuomo</forename><surname>Raitio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="960" to="964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hyperdense-net: A hyper-densely connected CNN for multi-modal image segmentation</title>
		<author>
			<persName><forename type="first">Jose</forename><surname>Dolz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Gopinath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herve</forename><surname>Lombaert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Desrosiers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ismail</forename><surname>Ben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayed</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1116" to="1126" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A multimodal deep regression bayesian network for affective video content analyses</title>
		<author>
			<persName><forename type="first">Quan</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shangfei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longfei</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5123" to="5132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santiago</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faustino</forename><forename type="middle">J</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<editor>William W. Cohen and Andrew W. Moore</editor>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multimodal affective analysis using hierarchical attention strategy with word-level alignment</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kangning</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Marsic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2225" to="2235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Combining language and vision with a multimodal skipgram model</title>
		<author>
			<persName><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nghia</forename><forename type="middle">The</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="153" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Context-aware emotion recognition networks</title>
		<author>
			<persName><forename type="first">Jiyoung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungryong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungin</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kwanghoon</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="10142" to="10151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Emoticon: Context-aware multimodal emotion recognition using frege&apos;s principle</title>
		<author>
			<persName><forename type="first">Trisha</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pooja</forename><surname>Guhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uttaran</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aniket</forename><surname>Bera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinesh</forename><surname>Manocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="14222" to="14231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multimodal deep learning</title>
		<author>
			<persName><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juhan</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="689" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Found in translation: Learning robust joint representations by cyclic translations between modalities</title>
		<author>
			<persName><forename type="first">Hai</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Manzini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barnabás</forename><surname>Póczos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6892" to="6899" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multimodal emotion recognition using deep learning architectures</title>
		<author>
			<persName><forename type="first">Shayok</forename><surname>Hiranmayi Ranganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sethuraman</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><surname>Panchanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Misalignment-robust joint filter for cross-modal image pairs</title>
		<author>
			<persName><forename type="first">Takashi</forename><surname>Shibata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masayuki</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masatoshi</forename><surname>Okutomi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3315" to="3324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep spatio-temporal feature fusion with compact bilinear pooling for multimodal emotion recognition</title>
		<author>
			<persName><forename type="first">Dung</forename><surname>Nguyen Tien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kien</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sridha</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clinton</forename><surname>Fookes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Underst</title>
		<imprint>
			<biblScope unit="volume">174</biblScope>
			<biblScope unit="page" from="33" to="42" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep spatio-temporal features for multimodal emotion recognition</title>
		<author>
			<persName><forename type="first">Dung</forename><surname>Nguyen Tien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kien</forename><surname>Nguyen Thanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sridha</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Afsane</forename><surname>Ghasemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clinton</forename><surname>Fookes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1215" to="1223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multimodal transformer for unaligned multimodal language sequences</title>
		<author>
			<persName><forename type="first">Yao-Hung Hubert</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaojie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Pu Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Zico</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6558" to="6569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning factorized multimodal representations</title>
		<author>
			<persName><forename type="first">Yao-Hung Hubert</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Words can shift: Dynamically adjusting word representations using nonverbal behaviors</title>
		<author>
			<persName><forename type="first">Yansen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7216" to="7223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Memory fusion network for multi-view sequential learning</title>
		<author>
			<persName><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navonil</forename><surname>Mazumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5634" to="5641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multimodal language analysis in the wild: CMU-MOSEI dataset and interpretable dynamic fusion graph</title>
		<author>
			<persName><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2236" to="2246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multimodal sentiment intensity analysis in videos: Facial gestures and verbal messages</title>
		<author>
			<persName><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Pincus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intell. Syst</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="88" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Audio-visual affect recognition through multistream fused HMM for HCI</title>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jilin</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Pianfetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenqiu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">E</forename><surname>Levinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="967" to="972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multimodal spontaneous emotion corpus for human behavior analysis</title>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">M</forename><surname>Girard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Umur</forename><forename type="middle">A</forename><surname>Ciftci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaun</forename><forename type="middle">J</forename><surname>Canavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Reale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huiyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijun</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPP</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3438" to="3446" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
