<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Avalon: Towards QoS Awareness and Improved Utilization through Multi-Resource Management in Datacenters</title>
				<funder ref="#_Yp6CmV6">
					<orgName type="full">National R&amp;D Program of China</orgName>
				</funder>
				<funder ref="#_PfZQtFE #_U6bDjuk #_qE22QdE #_y4c6tEv">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Quan</forename><surname>Chen</surname></persName>
							<email>chen-quan@cs.sjtu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Chao</forename><surname>Li</surname></persName>
							<email>lichao@cs.sjtu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Minyi</forename><surname>Guo</surname></persName>
							<email>guo-my@cs.sjtu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Zhenning</forename><surname>Wang</surname></persName>
							<email>wangzhenning1@huawei.com</email>
						</author>
						<author>
							<persName><forename type="first">Jingwen</forename><surname>Leng</surname></persName>
							<email>leng-jw@cs.sjtu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Wenli</forename><surname>Zheng</surname></persName>
							<email>zheng-wl@cs.sjtu.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University Shanghai</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Zhenning Wang</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Shanghai Jiao Tong University Shanghai</orgName>
								<address>
									<country>China Wenli Zheng</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Shanghai Jiao Tong University Shanghai</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Minyi Guo</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="department">Institute for Advanced Communication and Data Science</orgName>
								<orgName type="institution" key="instit1">Quan Chen and Minyi Guo are the members of Shanghai</orgName>
								<orgName type="institution" key="instit2">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff8">
								<orgName type="department">ICS &apos;19</orgName>
								<address>
									<addrLine>June 26-28</addrLine>
									<postCode>2019</postCode>
									<settlement>Phoenix</settlement>
									<region>AZ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Avalon: Towards QoS Awareness and Improved Utilization through Multi-Resource Management in Datacenters</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3330345.3330370</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Spatial multitasking</term>
					<term>QoS</term>
					<term>Improved utilization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing techniques for improving datacenter utilization while guaranteeing the QoS are based on the assumption that queries have similar behaviors. However, user queries in emerging compute demanding services demonstrate significantly diverse behavior and require adaptive parallelism. Our study shows that the end-to-end latency of the compute demanding query is determined together by the system-wide load, its workload, its parallelism, contention on shared cache, and memory bandwidth. When hosting such new services, the current cross-query resource allocation results in either severe QoS violation or significant resource under-utilization.</p><p>To maximize hardware utilization while guaranteeing the QoS, we present Avalon, a runtime system that independently allocates shared resources for each query. Avalon first provides an automatic feature identification tool based on Lasso regression, to identify features that are relevant to a query's performance. Then, it establishes models that can precisely predict a query's duration under various resource configurations. Based on the accurate prediction model, Avalon proactively allocates "just-enough" cores and shared cache spaces to each query, so that the remaining resource can be assigned to execute best-effort applications. During runtime, Avalon monitors the progress of each query and mitigates any possible QoS violation due to memory bandwidth contention, occasional I/O contention, or unpredictable system interference. Our results show that Avalon improves utilization by 28.9% on average compared with state-ofthe-art techniques while achieving 99%-ile latency target.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Latency-sensitive (LS) services, such as web search <ref type="bibr" target="#b5">[6]</ref>, web service <ref type="bibr" target="#b4">[5]</ref>, and memcached <ref type="bibr">[7]</ref> are critical business workloads in datacenters. It is crucial to consistently maintain low tail latency for those user-facing services. To guarantee the Quality-of-Service (QoS), the services often run alone in the datacenter, which leads to over-provisioned hardware resources. Because services often experience diurnal pattern <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b42">44]</ref>, it is cost effective to co-locate the LS services with Best-Effort (BE) applications that have no QoS requirement. However, the co-location may result in QoS violation of LS services due to shared resource contention.</p><p>A significant amount of prior work has recognized the problem and proposed techniques to guarantee QoS and improve resource utilization <ref type="bibr" target="#b18">[20,</ref><ref type="bibr" target="#b19">21,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b36">38]</ref>. The existing solutions generally fall into two categories: profile-based, which is represented by the series of Bubble <ref type="bibr" target="#b37">[39,</ref><ref type="bibr" target="#b46">48,</ref><ref type="bibr" target="#b49">51]</ref>, profiles applications and co-locates "safe" applications; and feedback-based, such as Heracles <ref type="bibr" target="#b36">[38]</ref>, uses online history QoS information to guide resource allocation.</p><p>For queries of the traditional LS services (e.g., web search) <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b29">31]</ref> with similar workloads, both profile based and feedback based methods work effectively. However, queries in the emerging services exhibit diverse workloads and are more computation demanding. Such services include intelligent personal assistant (IPA) service <ref type="bibr" target="#b25">[27]</ref>, DNN service <ref type="bibr" target="#b24">[26]</ref>, etc. The processing time of a query in these services takes a significant portion of its end-to-end latency <ref type="bibr" target="#b23">[25,</ref><ref type="bibr" target="#b25">27,</ref><ref type="bibr" target="#b34">36]</ref>, making it necessary to choose adaptive parallelism to satisfy the QoS target. For example, running with a fixed level of parallelism, prior work <ref type="bibr" target="#b15">[17,</ref><ref type="bibr" target="#b17">19,</ref><ref type="bibr" target="#b28">30]</ref> shows that the latency of a long query can be 10? of a short one. The characteristics of the new compute demanding queries introduce new challenges in maximizing resource utilization while guaranteeing the QoS of LS services.</p><p>When hosting a computation demanding service p, profile-based methods waste co-location opportunity. Even if co-locating p with BE applications only results in QoS violation of a few long queries,  <ref type="bibr" target="#b51">[53]</ref> Bubble <ref type="bibr" target="#b46">[48,</ref><ref type="bibr" target="#b49">51]</ref> SMK QoS <ref type="bibr" target="#b44">[46]</ref> Tail-control <ref type="bibr" target="#b34">[36]</ref> Baymax <ref type="bibr" target="#b15">[17]</ref> Quasar <ref type="bibr" target="#b19">[21]</ref> Heracles <ref type="bibr" target="#b36">[38]</ref> Avalon the co-location is forbidden due to the violation of tail latency (e.g., 99%-ile latency) target, resulting in low hardware utilization. However, the QoS target can be satisfied by using more cores and more shared resources to process long queries. Similarly, we can allocate short queries with fewer cores and assign more resources for BE applications to improve their throughput. In this sense, profilebased methods are too conservative and can be improved to make co-location "safe" and improve utilization.</p><p>On the other side, feedback-based methods cause QoS violation of these new services. The principle is that, if the tail latency of queries in the past monitoring period is close to or larger than the QoS target, more resources are assigned to the LS service. Thus, if the latency of p's queries in a monitoring period is much shorter than the QoS target, fewer cores will be allocated to p in the next period, resulting in QoS violation if the following query requires long execution. Similarly, if queries in a period are long but queries in the next period are short, too many cores will be allocated to p, resulting in low utilization. Thus, the feedback-based methods will in particular suffer from queries with diverse workloads.</p><p>Recognizing the limitations of existing techniques, this paper takes the first step to proactively manage resources for each query based on both system-wide load and the query's requirements. This approach faces three major challenges. First, to satisfy the QoS target, we need to independently determine the good resource allocation for each query, which can be affected by the diverse workload, queuing delay, and networking time (i.e., system-wide load). Second, it is challenging to find an optimal multi-resource configuration (i.e., core, shared cache, and memory bandwidth) for each query. Third, the QoS violation can be caused by various reasons, such as memory bandwidth contention, occasional I/O contention (most services cache their data in the memory for reducing the end-to-end latency <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b36">38]</ref>), and unpredictable interference, but current servers lack the interface to precisely allocate memory bandwidth.</p><p>To solve the three challenges, we propose Avalon, a runtime system consisting of 1) an online performance predictor, 2) a per-query resource allocator, and 3) an on-the-fly QoS enhancer. In a datacenter, a query dispatcher routes user queries to different nodes. An Avalon instance runs on each node to manage the shared resources. Specifically, the performance predictor predicts the duration of each user query under different resource configurations using offline-trained performance models. Based on the prediction, the allocator searches "just-enough" resource configuration for each query to satisfy the QoS target. All the remaining resources are allocated to the co-located BE applications. Meanwhile, QoS enhancer monitors the progress of each query as the feedback to dynamically adjust the resource allocation. If a query runs slower than expected, more resources will be assigned to it, by preempting cores and shared cache space from BE applications.</p><p>The main contributions of this paper are as follows.</p><p>? We design performance models that predict the duration and progress speed of queries with different workloads under various resource configurations. ? We design an algorithm to identify and allocate the "justenough" configuration from a large number of candidates for a query to complete ahead the QoS target. ? To eliminate the potential QoS violation, we design an online compensation mechanism to speed up slow queries.</p><p>Our evaluation demonstrates that Avalon improves the throughput of BE applications by 28.9% compared with state-of-the-art techniques, while ensuring 99%-ile latency of LS services within their QoS target.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>There has been a large amount of prior work on improving the datacenter resource utilization while satisfying the QoS requirement of LS services <ref type="bibr" target="#b19">[21,</ref><ref type="bibr" target="#b36">38,</ref><ref type="bibr" target="#b37">39]</ref>.</p><p>Bubble-Up <ref type="bibr" target="#b37">[39,</ref><ref type="bibr" target="#b46">48]</ref> and Prophet <ref type="bibr" target="#b14">[16]</ref> identify "safe" co-locations where the performance interference would not result in QoS violation, and rely on the operating system to manage shared resources. SMiTe <ref type="bibr" target="#b49">[51]</ref> extends Bubble-Up to simultaneous multithreading (SMT) cores by analyzing the interference between threads on the same SMT core. Quasar <ref type="bibr" target="#b19">[21]</ref> classifies applications according to their characteristics and only co-locate applications when the expected interference would not result in QoS violation. These profile-based techniques are complementary to Avalon. We can use them to identify co-location pairs that would suffer from small interference, and use Avalon to manage shared resources to maximize utilization.</p><p>Heracles <ref type="bibr" target="#b36">[38]</ref> guarantees the QoS of LS services at co-location through resource management. Heracles monitors resource requirements of LS services at different system loads and updates resource allocation to serve more requests when the load is heavy. Ubik <ref type="bibr" target="#b29">[31]</ref> explores the effectiveness of cache partition with simulation. Dirigent <ref type="bibr" target="#b51">[53]</ref> profiles processing speed of LS services and utilizes DVFS to slow down BE applications for improving the performance of LS services. These feedback-based resource management techniques such as Heracles may result in either QoS violation or low resource utilization for emerging compute-demanding services.</p><p>Recent work also proposes techniques to reduce the tail latency of LS services with parallelizable queries at high system load. Haque et. al <ref type="bibr" target="#b23">[25]</ref> proposes to increase the parallelism for long queries, so that they can complete within the QoS target. Li et. al <ref type="bibr" target="#b34">[36]</ref> proposes an opposite approach that reduces the parallelism of long queries to prevent QoS violation of short queries due to the long queuing time. These techniques focus on the QoS of LS services and are not able to increase resource utilization. Llull et. al <ref type="bibr" target="#b35">[37]</ref> proposes a game-theoretic framework, called Cooper, to provide fairness while preserving the performance at co-location.</p><p>Table <ref type="table" target="#tab_0">1</ref> compares Avalon with prior work that focuses on guaranteeing the QoS of LS applications at co-location.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">REAL-SYSTEM INVESTIGATION</head><p>In this section, we first investigate the efficiency of prior work on maximizing utilization and ensuring QoS in terms of tail latency. Then, we analyze the reason that previous work fails on handling these new computation demanding LS services.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Emerging Latency-Sensitive Services</head><p>We target a new class of LS services, which we show prior work fails to handle. Traditionally, the processing time of a query (e.g., memcached <ref type="bibr">[7]</ref>) is often short and its end-to-end latency is dominated by queuing delay and scheduling overhead <ref type="bibr" target="#b29">[31,</ref><ref type="bibr" target="#b33">35,</ref><ref type="bibr" target="#b50">52]</ref>. However, our target services (e.g., Apple Siri [1], Google Now <ref type="bibr" target="#b1">[2]</ref>, Microsoft Bing <ref type="bibr" target="#b28">[30]</ref>, financial service <ref type="bibr" target="#b3">[4]</ref>) include a new class of queries that is more compute demanding <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b23">25,</ref><ref type="bibr" target="#b28">30,</ref><ref type="bibr" target="#b34">36]</ref>. We study the services in the Sirius suite <ref type="bibr" target="#b25">[27]</ref>, which is a benchmark suite for the intelligent personal assistant. Figure <ref type="figure" target="#fig_0">1</ref> shows the processing time of a query when it runs alone with a single thread normalized to its QoS target. Observed from the figure, a query's processing time with a single thread (workload) varies significantly and is already close to the QoS target. At co-location, it is necessary to use adaptive parallelism for long queries to achieve the QoS. Prior work assumes that queries have similar workloads and considers queuing time (i.e., systemwide load) as the main factor for determining the co-located BE applications, which renders them ineffective for handling new LS queries that require adaptive parallelism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Efficiency Analysis of Prior Work</head><p>We study how effective the existing scheduling policies (OS scheduling, Bubble-Up <ref type="bibr" target="#b37">[39,</ref><ref type="bibr" target="#b49">51]</ref>, and Heracles <ref type="bibr" target="#b36">[38]</ref>) can improve utilization while guaranteeing the QoS of these new services.</p><p>In this investigation, we use benchmarks in Sirius suite as LS services; and use benchmarks in PARSEC <ref type="bibr" target="#b10">[12]</ref> as BE applications. Each LS service is co-located with a BE application on 12-core 2-way SMT CPU. We equally assign threads to both applications (i.e., 12 threads for each). Same to prior work targeting traditional LS services <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b15">17,</ref><ref type="bibr" target="#b17">19,</ref><ref type="bibr" target="#b38">40]</ref>, we use 150 ms as the QoS target here. Selecting different QoS targets does not affect the effectiveness of our analysis. Avalon can handle both smaller and larger QoS targets.</p><p>Figure <ref type="figure">2</ref> shows the QoS violation for an LS service and the throughput for a BE application in each of the 7 ? 8 = 56 co-location pairs (7 LS services and 8 BE applications). The x-axis shows the co-location pairs (e.g., bs under asr means the LS service asr coruns with the BE application bs). The y-axes of Figure <ref type="figure">2</ref>(a) and (b) show the 99%-ile latency of LS services and the throughput of BE applications. Both the metrics are normalized to their solo-run numbers. The marker "?" means that the throughput is zero.</p><p>OS scheduling -As shown in Figure <ref type="figure">2</ref>(a), the OS scheduling (CentOS 6.8 with kernel 2.6.32) results in LS services in all the 56 co-locations suffering from QoS violation. The 99%-ile latency of LS services is 1.47x of the expected QoS target on average and 2.13x in the worst case, even if only a single BE application is colocated with the service in each test. The fixed resource allocation and contention on the shared resources lead to QoS violation.</p><p>Profile-based method -We implement Bubble <ref type="bibr" target="#b46">[48]</ref> as the representative profile-based method. Bubble identifies "safe" co-locations in which the BE application's interference to the LS service is within a safe margin. It guarantees the QoS of LS services by only allowing "safe" co-locations (not shown in Figure <ref type="figure">2(a)</ref>). However, as shown in Figure <ref type="figure">2</ref>(b), BE applications in 26 co-locations (e.g., the "?" marker in bs under fd) achieve zero throughput with Bubble because those co-locations are not safe and therefore not allowed.</p><p>Feedback-based method -We implement Heracles <ref type="bibr" target="#b36">[38]</ref> as the representative feedback-based method. As observed from Figure <ref type="figure">2</ref>(a), emerging services in 25 out of the 56 co-locations suffer from QoS violation with Heracles. Meanwhile, as shown in Figure <ref type="figure">2</ref>(b), Heracles enables more co-locations than Bubble (e.g., "fd" and "fs"). Heracles also achieves higher throughput for BE applications. The higher throughput can be attributed to the explicit resource allocation used in Heracles. However, the reason that Heracles causes QoS violation is that the resource allocated to a query is determined by the end-to-end latencies of historical queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Addressing the Inefficiency</head><p>The feedback-based method, such as Heracles, is effective for the traditional query because its tail latency is mainly decided by queuing delay, scheduling overhead, and networking time, as illustrated in Figure <ref type="figure">3(a)</ref>. Those factors are determined by the system-wide load (i.e., query arrival rate), which changes slowly and smoothly in a datacenter <ref type="bibr" target="#b36">[38]</ref>. As such, periodical feedback-based resource allocation in Heracles is fast enough to catch up with the load change and can eliminate QoS violation due to the variation of queuing delay.</p><p>However, the cross-query approach used by Heracles falls apart in the presence of the new compute demanding queries. Figure <ref type="figure" target="#fig_0">1</ref> and Figure <ref type="figure">3</ref>(b) show that, for a compute demanding query, the processing time instead of the queuing delay dominates the end-toend query latency. As such, the latency of those queries can vary dramatically in a short time because of their different and querydependent workloads. In this case, Heracles cannot adapt to such a fast-changing query workload, as our real-system data suggests.</p><p>An effective resource manager for the compute-demanding services must catch both the system-wide load and the resource requirement of each query to overcome the inefficiency in prior work. To this end, we propose a per-query resource manager called Avalon to maintain the QoS while maximizing system throughput for computedemanding services in which queries have different workloads.  <ref type="figure" target="#fig_1">4</ref>, a datacenter hosts multiple types of services, and each node runs a single service <ref type="bibr" target="#b36">[38,</ref><ref type="bibr" target="#b46">48]</ref>. For quick query distribution, datacenter-level query dispatcher distributes queries to the nodes that host corresponding services in a round-and-robin manner. Another way is to predict the workload of every query and assign the queries according to their workloads and the load of every node. However, predicting the workloads of all the queries at the dispatcher is too expensive for a large datacenter because scanning through all the  nodes is time-consuming. As such, it is not practical to adopt the second method in a real-world datacenter. An Avalon instance is launched to manage shared resources on each node. If the number of concurrent LS queries exceeds the concurrency threshold, newly coming queries wait in a queue. If a query runs on multiple nodes, its QoS can be satisfied by ensuring every part of the query can complete within the QoS target, because the parts on different nodes are loosely-coupled and do not interact with each other <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b36">38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">THE AVALON METHODOLOGY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Avalon Overview</head><p>As shown in the right part of Figure <ref type="figure" target="#fig_1">4</ref>, Avalon consists of an online performance predictor, a per-query resource allocator, and an on-the-fly QoS enhancer. The performance predictor predicts the duration (processing time) and progress speed (i.e., instructions-percycle, IPC) of a query with different resource configurations. The resource allocator assigns the minimum number of cores and the smallest shared cache space to each query while it can return within the QoS target. The QoS enhancer fine-tunes resource allocation for each query to mitigate QoS violation due to memory bandwidth contention or other occasional interference.</p><p>When a node receives a query q, Avalon uses performance predictor to predict q's duration and progress speed with every possible resource configuration<ref type="foot" target="#foot_0">1</ref> . The key challenge is how to predict a query's performance accurately in a short time (Section 5).</p><p>When q is scheduled to run, the resource allocator searches for the configuration that has just-enough cores and shared cache space for the query to complete within the QoS target. Queuing delay and networking time (system-wide load) are considered when searching for the configuration. If multiple LS queries are active concurrently, the resource allocator does the same thing for every query. Avalon allocates the remaining shared resources to BE applications. The key challenge is to quickly identify the "just-enough" configuration for every query (Section 6).</p><p>The on-the-fly QoS enhancer monitors q's progress and allocates it more resource if the query tends to suffer from QoS violation. The key challenge faced by the QoS enhancer is how to detect whether a query would suffer from QoS violation in advance (Section 7).</p><p>Avalon incorporates a small online backend model updater than refines the performance prediction model with new samples, because more samples are usually useful to improve accuracy. Moreover, in a datacenter, workloads often become stable after a certain time scale so that the prediction model becomes more accurate with periodical updates. Once a node receives an LS query with an unseen workload, the query is duplicated and sent to the model updater. The model updater collects new samples by profiling it under various resource configurations. Incremental update <ref type="bibr" target="#b48">[50]</ref> can then be applied with low overhead to refine the model with the new samples, which continuously improves the accuracy of the prediction model. The updated models are then sent back to the nodes that host the corresponding service. Note that, we do not use the processing time of an LS query</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Avalon Runtime System</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Perf. Predictor Resource Allocator</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>QoS Enhancer</head><p>Lat.-sensitive queries</p><formula xml:id="formula_0">q 1 q 2 q n</formula><p>Best-effort apps at co-location to update the model, because it is "polluted" by the contention on shared resources such as memory bandwidth.</p><p>Our experiment shows that the performance predictor is accurate for all the evaluated benchmarks, but still some unstable services may have queries with an unpredictable workload. If the performance predictor cannot precisely predict the workload of an LS service's queries, Avalon assumes all its queries have the largest monitored workload for the safety consideration. When scheduling these unstable services, Avalon performs similarly to Heracles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation and Discussions</head><p>We use Cache Allocation Technology (CAT) <ref type="bibr" target="#b27">[29]</ref> to allocate shared cache to a specific application. According to Intel's document <ref type="bibr" target="#b27">[29]</ref> and our measurement, CAT re-allocates cache in less than 1ms. If a query has multiple stages which are processed by various microservices, each stage will be treated as an individual query and the stage dependency can be addressed with PowerChief <ref type="bibr" target="#b47">[49]</ref>.</p><p>To support quick resource re-allocation, we use a fixed number of N threads to parallelize an LS query on an N-core node. When a query is allocated with n cores (n &lt; N), we do not change the thread number but affiliate the N threads to n cores. Hosting different numbers of threads, the cores may experience imbalanced progress speeds. As such, we run those threads in the work-stealing manner for fine-grained load balance <ref type="bibr" target="#b11">[13]</ref>. Work-stealing addresses the imbalance problem by allowing a thread to steal the workload from other threads when its assigned part completes. We port a pthread program to enable the work-stealing by modifying 3-5 lines of its source code with the Cilk Plus extension (integrated with GCC compiler by default) <ref type="bibr">[3]</ref>. Although running more threads than needed can have extra overheads due to context switch and scheduling jitter, our experiment in Section 8 shows that the overhead is small.</p><p>Avalon allows multiple LS queries to have different QoS targets to run on a node concurrently. When a query arrives, Avalon predicts the amount of required resource and preempts the required resource from BE applications without affecting other LS queries. Preempting resource from BE applications is enough for guaranteeing the QoS because of the resource over-provision. Our experiment shows that Avalon is able to identify the "just-enough" resource configuration for a query within 0.62ms (Table <ref type="table" target="#tab_3">3</ref>). In contrast, prior work <ref type="bibr" target="#b19">[21,</ref><ref type="bibr" target="#b36">38,</ref><ref type="bibr" target="#b37">39]</ref> could not handle the case that concurrent queries have different QoS targets.</p><p>Avalon can also eliminate QoS violation due to the variation of system-wide load. When the load changes, the queuing delay of a query varies. In this case, Avalon identifies the best resource configuration for each query so that it returns before the QoS target (Figure <ref type="figure">3</ref>). If the system-wide load is heavy, it is possible that BE applications receive no resource and get suspended to ensure QoS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">QUERY PERFORMANCE PREDICTION</head><p>When an LS query is received, the performance predictor predicts its duration and progress speed (we use instructions-per-cycle, IPC as the metric) under a given resource configuration using pre-trained models. The duration model is used to search for a query's optimal configuration and the progress model is used by the QoS enhancer to detect potential QoS violation in advance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Identifying Relevant Features</head><p>Although private datacenters host internal services, those services may come from various groups within a company. As such, it is still burdensome for the datacenter operators to manually select the features that are most relevant to the performance of an LS service. A naive way is to use all the potential features (e.g., all the parameters of a query, the size of a query's input, and the content of pointers) to train the duration and progress model. However, this naive method has two potential disadvantages: (1) poor accuracy and long prediction time; (2) overlook of the feature combination impact. First, irrelevant features may result in over-fitting (poor accuracy) <ref type="bibr" target="#b13">[15]</ref> and the large feature number results in long prediction time. Second, a single variable may not be relevant to the performance of a query, but the combination of multiple variables does. For instance, the source or the destination alone in the routing service does not affect the duration of a query but their combination does.</p><p>We develop an automatic feature identification tool that can find the most relevant features and their combination to a query's performance. The input to the tool includes all the features potentially related to a query's performance, as well as number of cores and size of shared cache that reflect the amount of resource used to process a query. If a server supports the explicit and precise allocation of other types of shared resources (e.g., memory bandwidth) to a specific application, the feature identification tool can be extended to incorporate it. We use polynomial feature and Lasso regression <ref type="bibr" target="#b41">[43]</ref> to overcome the aforementioned disadvantages. First, Lasso regression selects a subset of the provided covariates for the prediction model rather than using all of them. Lasso has been proved to be able to identify more relevant features (i.e., those with non-zero weights) to avoid over-fitting <ref type="bibr" target="#b6">[8,</ref><ref type="bibr" target="#b31">33,</ref><ref type="bibr" target="#b45">47]</ref>. Second, the tool adds synthesized features using polynomial feature generation <ref type="bibr" target="#b7">[9]</ref> that extends the candidates by combining multiple features into a single one. This lets us capture the impact of different feature combinations.</p><p>For the uses benchmarks, Table <ref type="table" target="#tab_2">2</ref> shows the identified most relevant features. N c , N cache , S in , MOD, N r , and N w represent number of cores, number of cache ways, input data size, used inference model, number of items in the rule list, and number of words in a question respectively. Note that, the tool may identify different features for the duration model and the progress speed model of an LS service (e.g., stem and asr in Table <ref type="table" target="#tab_2">2</ref>), and some candidate features are not relevant to the query performance (e.g., MOD for crf).</p><p>Observed from the table, the duration of stem is more relevant with S in other than N c , because stem is memory-bound. Also, the progress speeds of many benchmarks are not relevant to S in , which is also reasonable because the IPC of an application depends more on the number of involved cores. For regex, its progress speed is only relevant to N c and N 2 c , because it has high arithmetic intensity. These observations verify that the automatic feature identification tool can identify the features that are relevant to a query's performance.</p><p>Building a unified duration/progress speed model that fits all the services is not practical because services often have diverse runtime behaviors and show various performance characteristics. Services require different features for accurate performance prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Collecting Training Samples</head><p>Avalon builds a duration model and a progress speed model for each LS service. In a datacenter, it is acceptable to profile a service and build a new model before running it permanently. The profiling is done offline so it does not incur runtime overhead.</p><p>To profile an LS service, we deploy it on a datacenter node, submit queries with different inputs to the service, execute them with varying numbers of cores and ways of shared cache, and collect the corresponding duration and IPC. During the profiling, queries are executed sequentially to avoid performance interference due to the resource contention. If the node that hosts the service has N cores, an LS query is always executed with N threads no matter how many cores it is allocated to emulate the online execution scenario.</p><p>For a service in our experimental node with 12 cores and 16 ways of shared cache, we collect 16 ? 12 ? 100 = 19200 samples with 100 different inputs, 12 different numbers of cores, and 16 different numbers of cache ways. From these samples, we randomly choose 80% of the samples as the training set, and use the rest to evaluate the accuracy of the trained models (Section 5.3). In our experiment, the offline profiling for a service completes within an hour.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Determining Low Overhead Models</head><p>The QoS target of an LS query is hundreds of milliseconds to support smooth user interaction. It is crucial to choose the modeling techniques with low computation complexity and high accuracy for the online performance predictor. We evaluated a spectrum of regression models for predicting the duration and IPC of queries: K-Nearest Neighbor (KNN) regression <ref type="bibr" target="#b16">[18]</ref>, Linear Regression (LR) <ref type="bibr" target="#b39">[41]</ref>, Multilayer Perceptron Neural Network (NN) <ref type="bibr" target="#b26">[28]</ref>, and Support Vector Regression (SVR) <ref type="bibr" target="#b40">[42]</ref>. Figure <ref type="figure">5</ref> and Figure <ref type="figure">6</ref> present the prediction accuracy of the above regression models. We use coefficient of determination (R 2 ), a widely-used metric in machine learning research <ref type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b21">23,</ref><ref type="bibr" target="#b22">24]</ref>, to evaluate the accuracy of different modeling techniques. Theoretically, R 2 can be negative when fitting a nonlinear function, and the negative cases (i.e., SVR and NN models for asr) are shown as zero in Figure <ref type="figure">5</ref>. For KNN, we select K = 5. As shown in Figure <ref type="figure">5</ref> and Figure <ref type="figure">6</ref>, KNN and NN are accurate for both duration and IPC predictions; LR and SVR are not accurate when predicting the query's duration. Therefore, KNN and NN are the two best models for Avalon.</p><p>Besides accuracy, we also measure the execution time of different prediction models. The time of predicting with KNN, LR and NN is smaller than 0.04 ms, while the SVR model runs higher than 200 ms, which makes it not suitable to be used online in Avalon.</p><p>To achieve high prediction accuracy, we apply all the modeling techniques (i.e., KNN, LR, and NN) to each LS service and choose the most accurate model. Because the model selection is made offline, it does not introduce extra runtime overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">PER-QUERY RESOURCE ALLOCATION</head><p>When a query q is scheduled to run, Avalon searches the "justenough" configuration for q so that it can return within the QoS target. The decision is made based on its queuing delay, networking time, and predicted processing time under different configurations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Identifying Just-enough Configuration</head><p>As shown in Figure <ref type="figure">3</ref>, the end-to-end latency of a query is composed of its queuing delay, networking time, and actual processing time.</p><p>For query q, let T tgt , T queued , T nw , and T p represent its QoS target, its queuing delay, its networking time, and its actual processing time.</p><p>Only if T nw + T queued + T p ? T tgt , q's QoS is satisfied. T queued can be calculated as T start -T sub , where T start is the time when q starts to run, and T sub is the time when q is received. Only when Equation 1 is satisfied, q returns before its QoS target. T tgt , T start , T sub and T nw in Equation 1 are already known when q starts to run.</p><formula xml:id="formula_1">T p ? T tgt -T queued -T nw = T tgt -(T start -T sub ) -T nw (1)</formula><p>By comparing T tgt -T queued -T nw with the predicted duration of q under various configurations, Avalon identifies the "safe" configurations under which q can return before the deadline. To predict the performance of q under a configuration, Avalon calculates its corresponding features in Table <ref type="table" target="#tab_2">2</ref> and performs prediction using the trained models. A straightforward way to find the just-enough configuration is exhaustively searching the entire space.</p><p>However, the naive method is too time-consuming due to the large search space. For instance, in our platform with 12 cores and 16 ways of shared cache (CAT allocates cache in the way granularity), there are 12 ? 16 = 192 resource configurations in total. The exhaustive search would take 8 ms, and the model computation would take another 192 ? 0.04 = 7.68 ms. The root cause of the long search time is that KNN model performs prediction by calculating the Euclidean distance between the predicting sample and the 19200?80%=15360 multi-dimension training samples. As shown in the first row of Table <ref type="table" target="#tab_3">3</ref>, the total overhead can be 10.45% for a QoS target of 150 ms. The unaffordable overhead of exhaustive search is significant compared to the short QoS target. Worse, if datacenters deploy powerful servers that have more cores or a larger shared cache capacity, or allow explicit allocation of more types of resources, the overhead of exhaustive search will increase significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Minimizing Search Space</head><p>Avalon uses binary search to reduce the number of tries for identifying the "just-enough" configuration for a query q. Because both and shared cache ways can be explicitly allocated, we adopt a two-step binary search algorithm. Binary search and exhaustive search find the same optimal configuration because our investigation shows that the latency of a query has a convex relationship with cores and shared cache ways. This observation is also consistent with the findings reported in prior work <ref type="bibr" target="#b36">[38,</ref><ref type="bibr" target="#b43">45]</ref>.</p><p>Let &lt; r core , r cache &gt; represent a resource configuration, in which r core and r cache are the number of cores and number of shared cache ways allocated to q, respectively. The algorithm starts by setting r cache to its maximum value, and gradually searches the minimum safe value of r core that satisfies q's QoS using binary search. After r core is determined, the algorithm searches for the minimum safe value of r cache . In the algorithm, whether a configuration would The actual IPC of q in the current period IPC pred</p><p>The predicted IPC of q T period</p><p>The duration of a tuning period T run</p><p>The elapsed time of q W cmpl</p><p>The completed workload of q W all</p><p>The overall workload of q &lt; r core , r cache &gt;</p><p>The current resource configuration of q result in a query's QoS violation can be found out with Equation 1, in which T p is predicted with the performance predictor. Adopting this algorithm, the number of tries to find the optimal resource configuration is less than 8, which is much less than 192 tries with exhaustive search. The overhead is reduced from 8 ms to 0.3 ms. As shown in the second row of Table <ref type="table" target="#tab_3">3</ref>, the total overhead is now reduced to 0.4%. Once the "just-enough" configuration for query q is identified, the per-query resource allocator assigns resource to it according to the identified configuration.</p><p>The two-step binary search can be easily extended to future servers that have more types of allocable resources. Let a k-tuple &lt; r 1 , ..., r k &gt; represent a resource configuration with k types of allocable resources, in which r i is the amount of the i-th type of resource allocated to q. Suppose the k types of resources have N 1 , ..., N k possible values respectively. By adopting our algorithm, the time complexity of the search decreases from</p><formula xml:id="formula_2">O(N 1 ? ... ? N k ) to O(log(N 1 ) ? ... ? log(N k )).</formula><p>When multiple LS queries are active concurrently (they do not need to have the same QoS target or come from the same service), Avalon searches resource configurations for each of them. Only after all the active queries get enough resource, the remaining is allocated to the co-located BE applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">ON-THE-FLY QOS ENHANCEMENT</head><p>Running with the identified "just-enough" configuration, query q may suffer from QoS violation due to memory bandwidth contention, occasional I/O contention or unpredictable system interference. The QoS enhancer identifies whether the current configuration would result in the QoS violation and compensates slow queries periodically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Identifying Potential QoS Violation</head><p>To identify a potential QoS violation, the QoS enhancer periodically collects q's actual IPC (denoted by IPC current ), the elapsed processing time T run , and the amount of workload has been processed (denoted by W cmpl , calculated online in line 2 of Algorithm 1). Based on the above data, Equation 2 calculates the minimum IPC (denoted by IPC req ), with which q can return before the QoS target. In the equation, T tgt , T nw , and T queued are the QoS target, the networking time, and the queuing delay of q, W all = IPC pred ? T pred is q's workload, where IPC pred and T pred are q's predicted IPC and duration respectively. Our analysis and experiment in Section 5 have shown that Avalon can identify the most relevant features and predict IPC pred and T pred accurately for all the evaluated benchmarks. For potentially unstable LS services that have queries with unpredictable workloads, Avalon conservatively assumes all the queries have the largest monitored workload to ensure the QoS.</p><formula xml:id="formula_3">IPC req = W all -W cmpl T tgt -T nw -T queued -T run<label>(2)</label></formula><p>If IPC current &lt; IPC req , query q cannot return before its deadline. Besides, if IPC current &lt; IPC pred , the actual progress speed of q is slower than its expected progress speed with the current configuration. In this case, the "just-enough" configuration is not enough due to unmanaged interference. Equation 3 calculates a lag factor (denoted by F) to capture the two cases. If F &gt; 1, q would suffer from QoS violation and Avalon allocates it more resource to speed up its execution.</p><formula xml:id="formula_4">F = max{ IPC req IPC current , IPC pred IPC current }<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Eliminating QoS Violation</head><p>The QoS enhancer fine-tunes the resource allocation for each active query in the end of every tuning period. Algorithm 1 lists the algorithm of tuning resource allocation for a query. As shown in the algorithm, if F &gt; 1, we increase the number of cores as well as the number of shared cache ways allocated to q. This is because the progress speed of q would not increase significantly without scaling its shared cache space at the same time, especially when q is data intensive. Our experiment in Section 8.3 shows the necessity for tuning shared cache allocation in the QoS enhancer.</p><p>Algorithm 1 Algorithm of tuning resource allocation.</p><p>Require: The predicted IPC IPC pred , W all ? IPC pred ? T pred , the current resource configuration &lt; r core , r cache &gt;. 1: Calculate IPC current in the end of each period; 2: W cmpl ? W cmpl + IPC current ? T period ; 3: T run ? T run + T period ; 4: if T run ? T tgt -T queued -T nw then</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Allocate all the cores and cache to the LS service; 6: else 7:</p><formula xml:id="formula_5">IPC req = (W all -W cmpl )/(T tgt -T nw -T queued -T run ); 8: F = max{IPC req /IPC current , IPC pred /IPC current }; 9: if F &gt; 1 then 10: r core ? r core ? F + max{1, (F -1) ? r core }; 11: r cache ? r cache ? F + max{1, (F -1) ? r cache }; 12:</formula><p>Apply the new resource allocation;</p><p>For the fast response time, it is straightforward to linearly increase the number of cores/shared cache ways allocated to q when F &gt; 1. However, the IPC of a parallel application often increases sublinearly with the number of cores/shared cache ways according to Amdahl's Law <ref type="bibr" target="#b20">[22]</ref>. Linearly increasing the number of cores/shared cache ways allocated to q cannot increase q's IPC to IPC req . Therefore, if F &gt; 1, the QoS enhancer adopts the resource configuration &lt; r core ? F + max{1, (F -1) ? r core }, r cache ? F + max{1, (F -1) ? r cache } &gt; for query q. Because extra cores and extra ways of shared cache are added, q's new IPC is likely to reach IPC req after tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">EXPERIMENTAL EVALUATION</head><p>Because Avalon manages shared resources of each datacenter node independently, we evaluate Avalon on a node equipped with two Xeon E5-2650 V4 CPUs that support Intel Cache Allocation Technology (CAT). The detailed setups are summarized in Table <ref type="table" target="#tab_5">5</ref>. We use all the seven benchmarks in Sirius suite <ref type="bibr" target="#b25">[27]</ref> as LS services, and eight representative applications from PARSEC <ref type="bibr" target="#b10">[12]</ref> as BE applications. We port the benchmarks in Sirius suite to run in the work-stealing manner using Cilk Plus extension integrated with GCC (the modification includes only 3-5 lines of source code).</p><p>As previously explained, we run an LS query with the fixed 12 threads on our 12-core platform. It is possible that Avalon runtime assigns less than 12 cores to the query which can cause substantial performance degradation. Our experiment shows that if we consolidate 12 threads to 11 cores, the query processing time can double in the worst case. Avalon relies on the work stealing mechanism to eliminate such a significant thread scheduling overhead. Figure <ref type="figure">7</ref> shows the query processing time of consolidating 12 threads 11 cores (12T-to-11C) normalized to its processing time with 12 cores in the work stealing mode. The performance degradation is less than 12/11 = 1.09 because the performance of most benchmarks does not scale linearly. Figure <ref type="figure">7</ref> also reports the query processing time when consolidating its 12 threads to a single core (12T-to-1C). The 12T-to-1C execution time is normalized to the query's singlethread duration, which reveals the largest possible overhead caused by thread consolidation and is still less than 4.9%. In summary, multiplexing threads onto a smaller number of cores does not lead to scheduler thrashing and performance anomalies in Avalon because work-stealing can eliminate the potential load imbalance.</p><p>Prior work <ref type="bibr" target="#b36">[38]</ref> has shown that Heracles can better improve the resource utilization compared with Bubble while guaranteeing the QoS of traditional LS services such as web search and memcached. For these traditional LS services, Avalon performs similarly to Heracles because the per-query resource allocator in Avalon is sensitive to the queuing delay and networking time due to the system-wide load changes. Our experiment verifies this assertion. Due to the limited pages, we do not show the results for traditional LS services.</p><p>In our evaluation, the QoS is defined as 99%-ile latency, and the utilization is measured as the normalized throughput of BE applications. The higher the throughput is, the better the hardware is utilized. For the fair comparison, the throughput of a BE application is normalized to its solo-run throughput. We set a tuning period of the QoS enhancer as 20 ms by default.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">QoS and Throughput</head><p>We evaluate Avalon in improving the throughput of BE applications while satisfying the QoS requirement of LS services. We compare the efficiency of Avalon, Bubble, and Heracles-O. Because the original Heracles cannot guarantee the QoS of compute-demanding services whose queries have diverse workloads (Section 3, Figure <ref type="figure">2</ref>), we implement an optimized Heracles, named Heracles-O. It fine-tunes (often increases) the duration of a monitoring period for each service to capture the arrival pattern of long queries. If long queries are present in most of the monitoring periods, Heracles-O is likely to ensure their QoS target. It is not practical to deploy Heracles-O in real systems because the long query arrival patterns may change quickly.</p><p>In addition, trying different monitoring periods during servicing may result in severe QoS violation. We use Heracles-O to show the best case that Heracles achieves without modifying its key designs. Figure <ref type="figure" target="#fig_5">8</ref> and Figure <ref type="figure">9</ref> present the 99%-ile latency of LS services normalized to their QoS targets, and the normalized throughput of BE applications when they are co-located. The normalized throughput of BE applications could be greater than 1, because we co-locate multiple BE instances with each LS service to maximize utilization. It explains why the throughputs of BE applications with Bubble are higher in Figure <ref type="figure">9</ref> compared with in Figure <ref type="figure">2(b)</ref>.</p><p>In Figure <ref type="figure" target="#fig_5">8</ref>, Bubble, Heracles-O (except one co-location) and Avalon ensure the QoS of compute-demanding LS services. Better than Bubble, Avalon and Heracles-O do not require prior knowledge of whether two applications can be safely co-located. Avalon increases the 99%-ile latency of LS services slightly (still within the QoS target), trading for higher throughput of BE applications.</p><p>In Figure <ref type="figure">9</ref>, for most of the 7 ? 8 = 56 co-locations, Avalon achieves higher throughput for BE applications than Bubble and Heracles-O. Bubble deems 26 out of the 56 co-locations as not safe (i.e., zero throughput). For example, most of the BE applications cannot be safely co-located with LS application fd, fe, gmm and regex in Bubble. On the contrary, Avalon enables the co-location, because it tunes the resource allocation based on the workload of each LS query and the interference from BE applications. The perquery multi-resource management in Avalon allows BE applications to use as much resource as possible without causing QoS violation.</p><p>BE applications achieve lower throughput with Heracles-O due to its coarse-grained cross-query resource allocation. In Heracles-O, each monitoring period is relatively long to capture the arrival pattern of long queries and resources are only reallocated at the end of a monitoring period. To guarantee the QoS, Heracles-O allocates enough shared resources to LS services in each monitoring period so that the long queries tend to complete within the QoS target. However, the resources are over-provisioned for short queries in the same monitoring period, resulting in the low throughput of BE applications at co-location. On average, BE applications achieve 0.468 throughput with Avalon, which is 65.9% and 28.9% higher than the throughputs achieved with Bubble and Heracles-O.</p><p>In Figure <ref type="figure">9</ref>, BE applications achieve high throughput when they are co-located with asr, crf, and stem, but achieve low throughput when co-located with fd, fe, gmm and regex. This is mainly because LS services have different scalabilities and query workloads. fd, fe, gmm and regex have high scalabilities and high workloads, but asr and stem are not scalable, and the workload of queries in crf is small. For an LS service having scalable queries with high workloads, allocating a small number of cores or smaller shared cache space can easily result in its QoS violation. In this case, to guarantee the QoS, Avalon allocates a small number of cores/shared cache ways to the BE applications, resulting in the low throughput. On the contrary, if the average workload of queries is small or the queries are not scalable, Avalon can allocate more resources to the BE applications and improve the resource utilization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Effectiveness of the QoS Enhancer</head><p>To evaluate the effectiveness of the QoS enhancer in eliminating QoS violation due to bandwidth contention, occasional I/O contention or unpredictable system interferences, we compare Avalon with Avalon-No-M, a system that disables the QoS enhancer in Avalon.</p><p>Figure <ref type="figure" target="#fig_6">10</ref> presents the 99%-ile latency of LS services at colocation in Avalon-No-M. LS services in 21 out of the 56 co-locations suffer from QoS violation in Avalon-No-M. For instance, regex suffers from up to 2? QoS violation when it is co-located with can. The QoS violation is due to the memory bandwidth contention and occasional interferences in Avalon-No-M. For instance, almost all the LS services suffer from QoS violation when they are co-located with fs due to its high memory bandwidth usage (Figure <ref type="figure">7(b)</ref>). Even if a BE application, such as can or sc, uses moderate memory bandwidth, multiple instances can still saturate the bandwidth.</p><p>For some co-location pairs, the QoS of LS service is satisfied using Avalon-No-M, because the benchmarks in these pairs do not contend for memory bandwidth seriously. For instance, both fd and bs consume moderate memory bandwidth. In this case, with the precise performance predictor and per-query resource allocator, Avalon-No-M is possible to be enough to ensure the QoS of fd.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Effectiveness of Scaling Shared Cache</head><p>QoS enhancer increases the ways of shared cache allocated to a query with the increasing of cores. To evaluate this design choice, in Figure <ref type="figure" target="#fig_6">10</ref>, we report the 99%-ile latency of LS services at colocation with Avalon-No-C, a system that does not increases the ways of shared cache allocated to a query in Avalon accordingly.</p><p>Observed from Figure <ref type="figure" target="#fig_6">10</ref>, LS services in 23 out of the 56 colocations suffer from QoS violation with Avalon-No-C. Only increasing the number of cores allocated to a slow query is not able to effectively eliminates the QoS violation. This is mainly because there is a positive correlation between the number of cores allocated to a query and its required shared cache space. If more cores are  allocated to a "slow" query q, its threads on these cores process the data faster and issues more data access requests. In this case, q requires larger shared cache space to store the data of its threads. If QoS enhancer does not allocate larger shared cache space to q as in Avalon-No-C, the serious shared cache contention between the cores of q preventing it from achieving higher IPC. Our characterization of gmm in Figure <ref type="figure">7</ref> also backups this assertion, because the performance of gmm does not increase linearly when the size of its cache does not increase with its cores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4">Applying in a Datacenter</head><p>In this experiment, we evaluate the effectiveness of Avalon for compute-demanding services in a real-system datacenter. We build a cluster composed of seven nodes with Xeon E5-2650 V4 CPU, one node for each of the used LS services (Table <ref type="table" target="#tab_5">5</ref>). The nodes are connected via a 1Gbps switch. The query arrival rate of each service follows the Poisson distribution. The BE applications are composed of 56 instances (8 instances assigned to each node) evenly selected from PARSEC shown in Table <ref type="table" target="#tab_5">5</ref>. In the experiment, we use pair-wise co-locations, and randomly select BE applications to co-locate with each LS service. We run this experiment for 100 times and use data of the 7 ? 100 co-locations to analyze the effectiveness of Avalon. As shown in Figure <ref type="figure" target="#fig_7">11</ref>(a), Bubble, Heracles-O, and Avalon ensure the QoS of LS services for all the 700 co-locations, but LS services in the OS-managed scheduling suffer from severe QoS violation in 71.4% of the co-locations (&gt; 2X violation). Figure <ref type="figure" target="#fig_7">11</ref>(b) presents the BE applications' throughputs in the 700 co-locations. Observed from the figure, the BE applications achieve 0.836, 0.231, 0.316 and 0.397 normalized throughputs on average with OS scheduling (CentOS), Bubble, Heracles-O, and Avalon, respectively. Although the OS scheduling shows the highest throughput, it causes severe QoS violation. Compare with Bubble and Heracles-O, Avalon improves the throughput of BE applications through the per-query multi-resource management.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">CONCLUSIONS</head><p>Avalon improves the datacenter utilization without violating the QoS target of latency-sensitive services with parallelized queries. Avalon allocates just-enough resources for a query to complete within the QoS target by identifying the amount of resources needed by the query based on the precise performance prediction. Through monitoring the progress of each query, Avalon tunes the resource allocation to guarantee QoS and maximize throughput of the colocated best-effort applications. The experimental results show that Avalon improves the throughput of best-effort applications by 28.9% compared to state-of-the-art techniques while achieving the 99%-ile latency target for latency-sensitive services.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: The duration of a query when it runs alone using a single thread normalized to its QoS target.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4</head><label>4</label><figDesc>Figure 4  presents the design of Avalon. As shown in the left part of Figure4, a datacenter hosts multiple types of services, and each node runs a single service<ref type="bibr" target="#b36">[38,</ref><ref type="bibr" target="#b46">48]</ref>. For quick query distribution, datacenter-level query dispatcher distributes queries to the nodes that host corresponding services in a round-and-robin manner. Another way is to predict the workload of every query and assign the queries according to their workloads and the load of every node. However, predicting the workloads of all the queries at the dispatcher is too expensive for a large datacenter because scanning through all the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: The QoS violation of LS services and the throughput of BE applications with OS scheduling, Bubble and Heracles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Design of Avalon. A datacenter-level query dispatcher accepts user queries and distributes them to different nodes. An Avalon instance is launched on each node to manage the shared resources.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Figure5: The coefficient of determination R 2 for predicting duration on the testing set (higher is more accurate).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: The 99%-ile latency of LS services normalized to their QoS targets in the 56 co-locations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: The 99%-ile latency of LS services normalized to the QoS targets with Avalon-No-M and Avalon-No-C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: The percentage of QoS violation at different levels and throughputs of the co-located BE applications.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparing Avalon with prior related work.</figDesc><table><row><cell>QoS-aware</cell><cell>Work On</cell><cell>Improved</cell><cell>Parallelized</cell><cell>Per-query</cell></row><row><cell>Systems</cell><cell>CPU</cell><cell>Utilization</cell><cell>Queries</cell><cell>Management</cell></row><row><cell>Dirigent</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>The features that are relevant to the duration and progress speed of a benchmark. Nc * N cache , S in * Nc Progress Nc, N 2 c , Nc * N cache , S in * Nc fe Nc, N cache , S in Duration Nc, N 2 c , Nc * N cache , S in * Nc Progress Nc, N 2 c , Nc * N cache , S in * Nc</figDesc><table><row><cell>Bench.</cell><cell>Candidate features</cell><cell>Model</cell><cell>Most relevant features</cell></row><row><cell>gmm</cell><cell>Nc, N cache , S in</cell><cell>Duration Progress</cell><cell>Nc, N 2 c , Nc  *  N cache , S in  *  Nc Nc, N 2 c , Nc  *  N cache</cell></row><row><cell>crf</cell><cell>Nc, N cache , S in , MOD</cell><cell>Duration Progress</cell><cell>Nc, N 2 c , Nc  *  N cache , S in  *  Nc Nc, N 2 c , Nc  *  N cache</cell></row><row><cell>regex</cell><cell>Nc, N cache , Nr , Nw</cell><cell>Duration Progress</cell><cell>Nc, Nr , Nc  *  N cache , Nw  *  Nc Nc, N 2 c</cell></row><row><cell>stem</cell><cell>Nc, N cache , S in</cell><cell>Duration Progress</cell><cell>S in , S 2 in , S in  *  N cache , Nc Nc, N 2 c , Nc  *  N cache , S in  *  Nc</cell></row><row><cell>asr</cell><cell>Nc, N cache , S in , MOD</cell><cell>Duration Progress</cell><cell>S 2 in , S in , MOD, Nc  *  N cache , Nc Nc, N 2 c , Nc  *  N cache</cell></row><row><cell>fd</cell><cell>Nc, N cache , S in</cell><cell>Duration</cell><cell>Nc, N 2 c ,</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Overhead of searching the optimal configuration.</figDesc><table><row><cell></cell><cell>Model (ms)</cell><cell>Search (ms)</cell><cell>Total (ms)</cell><cell>Pct. (%)</cell></row><row><cell>Exhaustive Search</cell><cell>7.68</cell><cell>8</cell><cell>15.68</cell><cell>10.45</cell></row><row><cell>Binary Search</cell><cell>0.32</cell><cell>0.3</cell><cell>0.62</cell><cell>0.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Variables used in the QoS enhancement alg.</figDesc><table><row><cell>Parameter</cell><cell>Description</cell></row><row><cell>IPC current</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Hardware, software, and benchmarks.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Specifications</cell></row><row><cell cols="2">Hardware</cell><cell></cell><cell cols="2">CPU: Intel Xeon E5-2650 V4 @ 2.20GHz 12 hardware cores, 30MB of shared cache (16 ways)</cell></row><row><cell>Software</cell><cell></cell><cell></cell><cell cols="2">CentOS 6.8 x86 64 with kernel 2.6.32-642</cell></row><row><cell cols="2">Benchmark</cell><cell></cell><cell cols="2">Applications</cell></row><row><cell cols="3">Sirius suite [27]</cell><cell cols="2">dnn-asr (asr), crf, fd, fe, gmm, regex, stemmer (stem)</cell></row><row><cell cols="2">PARSEC [12]</cell><cell></cell><cell cols="2">blackscholes (bs), bodytrack (bt), facesim (fs), x264, raytrace(rt), fluidanimate(fa), canneal(can), streamcluster(sc)</cell></row><row><cell>Normalized Exec. Time</cell><cell>0 0.2 0.4 0.6 0.8 1.0 1.2</cell><cell>asr</cell><cell>crf</cell><cell>fd 12T-to-11C fe gmm regex stem avg. 12T-to-1C</cell></row><row><cell cols="5">Figure 7: Load balance and scheduling overhead. 12T-to-11C:</cell></row><row><cell cols="5">Schedule 12 threads to 11 cores (normalized to 12-core perfor-</cell></row><row><cell cols="5">mance). 12T-to-1C: Schedule 12 threads to 1 core (normalized</cell></row><row><cell cols="5">to single-thread performance).</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Figure 9: The normalized throughput of BE applications at co-location with Bubble, Heracles-O, and Avalon.</figDesc><table><row><cell>Normalized Throughput</cell><cell>0.2 0.4 0.6 0.8 1.0 1.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Avalon</cell><cell></cell><cell></cell><cell cols="5">Heracles-O</cell><cell></cell><cell></cell><cell cols="2">Bubble</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>bs</cell><cell>bt</cell><cell>fs asr fa</cell><cell>rt</cell><cell>x264</cell><cell>can</cell><cell>sc</cell><cell>bs</cell><cell>bt</cell><cell>fs crf fa</cell><cell>rt</cell><cell>x264</cell><cell>can</cell><cell>sc</cell><cell>bs</cell><cell>bt</cell><cell>fs fd fa</cell><cell>rt</cell><cell>x264</cell><cell>can</cell><cell>sc</cell><cell>bs</cell><cell>bt</cell><cell>fs fe fa</cell><cell>rt</cell><cell>x264</cell><cell>can</cell><cell>sc</cell><cell>bs</cell><cell>bt</cell><cell>fs gmm fa rt</cell><cell>x264</cell><cell>can</cell><cell>sc</cell><cell>bs</cell><cell>bt</cell><cell>fs regex fa rt x264</cell><cell>can</cell><cell>sc</cell><cell>bs</cell><cell>bt</cell><cell>fs stem fa rt</cell><cell>x264</cell><cell>can</cell><cell>sc</cell><cell>Avg.</cell></row><row><cell></cell><cell>2.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>99%-ile Latency</cell><cell>0.5 1.0 1.5 2.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Avalon-No-M</cell><cell></cell><cell></cell><cell></cell><cell cols="4">Avalon-No-C</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.0</cell><cell cols="46">bs bt fs fa rt x264 can sc bs bt fs fa rt x264 can sc bs bt fs fa rt x264 can sc bs bt fs fa rt x264 can sc bs bt fs fa rt x264 can sc bs bt fs fa rt x264 can sc bs bt fs fa rt x264 can sc Avg.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">asr</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">crf</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">fd</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">fe</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">gmm</cell><cell></cell><cell></cell><cell></cell><cell cols="2">regex</cell><cell></cell><cell></cell><cell></cell><cell cols="2">stem</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>A resource configuration contains the number of cores and the size of shared cache allocated to a query currently.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGEMENTS</head><p>This work is partially sponsored by the <rs type="funder">National R&amp;D Program of China</rs> (No. <rs type="grantNumber">2018YFB1004800</rs>), the <rs type="funder">National Natural Science Foundation of China</rs> (<rs type="grantNumber">61602301</rs>, <rs type="grantNumber">61872240</rs>, <rs type="grantNumber">61632017</rs>, <rs type="grantNumber">61702328</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Yp6CmV6">
					<idno type="grant-number">2018YFB1004800</idno>
				</org>
				<org type="funding" xml:id="_PfZQtFE">
					<idno type="grant-number">61602301</idno>
				</org>
				<org type="funding" xml:id="_U6bDjuk">
					<idno type="grant-number">61872240</idno>
				</org>
				<org type="funding" xml:id="_qE22QdE">
					<idno type="grant-number">61632017</idno>
				</org>
				<org type="funding" xml:id="_y4c6tEv">
					<idno type="grant-number">61702328</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Apple</forename><surname>Siri</surname></persName>
		</author>
		<ptr target="https://www.apple.com/ios/siri" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<ptr target="http://www.google.com/landing/now" />
		<title level="m">Google&apos;s Google Now</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Introducing Intel Cilk Plus: Extensions to simplify task and data parallelism</title>
		<ptr target="https://www.cilkplus.org/cilk-plus-tutorial" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Parallelizing a computationally intensive financial R application with zircon technology</title>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Apache HTTP Server</title>
		<ptr target="https://httpd.apache.org" />
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Apache Solr</title>
		<ptr target="http://lucene.apache.org/solr" />
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Regression-based latent factor models</title>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bee-Chung</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 15th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="19" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning With Many Irrelevant Features</title>
		<author>
			<persName><forename type="first">Hussein</forename><surname>Almuallim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="page" from="547" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Memory hierarchy for web search</title>
		<author>
			<persName><forename type="first">Grant</forename><surname>Ayers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jung</forename><surname>Ho Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Kozyrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parthasarathy</forename><surname>Ranganathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">High Performance Computer Architecture (HPCA), 2018 IEEE International Symposium</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="643" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The Case for Energy-Proportional Computing</title>
		<idno type="DOI">10.1109/MC.2007.443</idno>
		<ptr target="https://doi.org/10.1109/MC.2007.443" />
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="33" to="37" />
			<date type="published" when="2007-12">2007. Dec. 2007</date>
			<publisher>Luiz Andr? Barroso and Urs H?lzle</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The PARSEC Benchmark Suite: Characterization and Architectural Implications</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Bienia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaswinder Pal</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1145/1454115.1454128</idno>
		<ptr target="https://doi.org/10.1145/1454115.1454128" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on Parallel Architectures and Compilation Techniques (PACT &apos;08)</title>
		<meeting>the 17th International Conference on Parallel Architectures and Compilation Techniques (PACT &apos;08)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="72" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Scheduling multithreaded computations by work stealing</title>
		<author>
			<persName><forename type="first">D</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">E</forename><surname>Blumofe</surname></persName>
		</author>
		<author>
			<persName><surname>Leiserson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM (JACM)</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="720" to="748" />
			<date type="published" when="1999">1999. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An R-squared measure of goodness of fit for some common nonlinear regression models</title>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Cameron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><forename type="middle">A G</forename><surname>Windmeijer</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0304-4076(96)01818-0</idno>
		<ptr target="https://doi.org/10.1016/S0304-4076(96)01818-0" />
	</analytic>
	<monogr>
		<title level="j">Journal of Econometrics</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="329" to="342" />
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On over-fitting in model selection and subsequent selection bias in performance evaluation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gavin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicola Lc</forename><surname>Cawley</surname></persName>
		</author>
		<author>
			<persName><surname>Talbot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2079" to="2107" />
			<date type="published" when="2010-07">2010. Jul (2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Prophet: Precise QoS Prediction on Non-Preemptive Accelerators to Improve Utilization in Warehouse-Scale Computers</title>
		<author>
			<persName><forename type="first">Quan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hailong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minyi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ram</forename><surname>Srivatsa Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Mars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingjia</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Second International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the Twenty-Second International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="17" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Baymax: QoS Awareness and Increased Utilization for Non-Preemptive Accelerators in Warehouse Scale Computers</title>
		<author>
			<persName><forename type="first">Quan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hailong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Mars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingjia</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.1145/2872362.2872368</idno>
		<ptr target="https://doi.org/10.1145/2872362.2872368" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-First International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS &apos;16)</title>
		<meeting>the Twenty-First International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS &apos;16)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="681" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Nearest neighbor pattern classification</title>
		<author>
			<persName><forename type="first">T</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hart</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIT.1967.1053964</idno>
		<ptr target="https://doi.org/10.1109/TIT.1967.1053964" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="27" />
			<date type="published" when="1967-01">1967. January 1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The Tail at Scale</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luiz</forename><surname>Andr?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barroso</forename></persName>
		</author>
		<idno type="DOI">10.1145/2408776.2408794</idno>
		<ptr target="https://doi.org/10.1145/2408776.2408794" />
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="74" to="80" />
			<date type="published" when="2013-02">2013. Feb. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Paragon: QoS-aware Scheduling for Heterogeneous Datacenters</title>
		<author>
			<persName><forename type="first">Christina</forename><surname>Delimitrou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Kozyrakis</surname></persName>
		</author>
		<idno type="DOI">10.1145/2451116.2451125</idno>
		<ptr target="https://doi.org/10.1145/2451116.2451125" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS &apos;13)</title>
		<meeting>the Eighteenth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS &apos;13)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="77" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Quasar: Resource-efficient and QoS-aware Cluster Management</title>
		<author>
			<persName><forename type="first">Christina</forename><surname>Delimitrou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Kozyrakis</surname></persName>
		</author>
		<idno type="DOI">10.1145/2541940.2541941</idno>
		<ptr target="https://doi.org/10.1145/2541940.2541941" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS &apos;14)</title>
		<meeting>the 19th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS &apos;14)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="127" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Reevaluating Amdahl&apos;s law</title>
		<author>
			<persName><surname>John L Gustafson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="532" to="533" />
			<date type="published" when="1988">1988. 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An introduction to variable and feature selection</title>
		<author>
			<persName><forename type="first">Isabelle</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andre</forename><surname>Elisseeff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">0</biblScope>
			<biblScope unit="page" from="1157" to="1182" />
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Empirical validation of object-oriented metrics on open source software for fault prediction</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gyimothy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ferenc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Siket</surname></persName>
		</author>
		<idno type="DOI">10.1109/TSE.2005.112</idno>
		<ptr target="https://doi.org/10.1109/TSE.2005.112" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Software Engineering</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="897" to="910" />
			<date type="published" when="2005-10">2005. Oct 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Few-to-Many: Incremental Parallelism for Reducing Tail Latency in Interactive Services</title>
		<author>
			<persName><forename type="first">E</forename><surname>Md</surname></persName>
		</author>
		<author>
			<persName><surname>Haque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiong</forename><surname>Yong Hun Eom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameh</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Elnikety</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathryn</forename><forename type="middle">S</forename><surname>Bianchini</surname></persName>
		</author>
		<author>
			<persName><surname>Mckinley</surname></persName>
		</author>
		<idno type="DOI">10.1145/2694344.2694384</idno>
		<ptr target="https://doi.org/10.1145/2694344.2694384" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS &apos;15)</title>
		<meeting>the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS &apos;15)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="161" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Djinn and tonic: Dnn as a service and its implications for future warehouse scale computers</title>
		<author>
			<persName><forename type="first">Johann</forename><surname>Hauswald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiping</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">A</forename><surname>Laurenzano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Mudge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronald</forename><forename type="middle">G</forename><surname>Dreslinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Mars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingjia</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.1145/2749469.2749472</idno>
		<ptr target="https://doi.org/10.1145/2749469.2749472" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42Nd Annual International Symposium on Computer Architecture (ISCA &apos;15)</title>
		<meeting>the 42Nd Annual International Symposium on Computer Architecture (ISCA &apos;15)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="27" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sirius: An Open End-to-End Voice and Vision Personal Assistant and Its Implications for Future Warehouse Scale Computers</title>
		<author>
			<persName><forename type="first">Johann</forename><surname>Hauswald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">A</forename><surname>Laurenzano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Austin</forename><surname>Rovinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arjun</forename><surname>Khurana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><surname>Dreslinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Mudge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinicius</forename><surname>Petrucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingjia</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Mars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS) (ASPLOS &apos;15)</title>
		<meeting>the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS) (ASPLOS &apos;15)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multilayer feedforward networks are universal approximators</title>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Hornik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Stinchcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Halbert</forename><surname>White</surname></persName>
		</author>
		<idno type="DOI">10.1016/0893-6080(89)90020-8</idno>
		<ptr target="https://doi.org/10.1016/0893-6080(89)90020-8" />
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="359" to="366" />
			<date type="published" when="1989">1989. 1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Intel Resource Director Technology</title>
		<author>
			<persName><surname>Intel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Adaptive Parallelism for Web Search</title>
		<author>
			<persName><forename type="first">Myeongjae</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameh</forename><surname>Elnikety</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Rixner</surname></persName>
		</author>
		<idno type="DOI">10.1145/2465351.2465367</idno>
		<ptr target="https://doi.org/10.1145/2465351.2465367" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th ACM European Conference on Computer Systems (EuroSys &apos;13)</title>
		<meeting>the 8th ACM European Conference on Computer Systems (EuroSys &apos;13)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="155" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Ubik: Efficient Cache Sharing with Strict Qos for Latency-critical Workloads</title>
		<author>
			<persName><forename type="first">Harshad</forename><surname>Kasture</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Sanchez</surname></persName>
		</author>
		<idno type="DOI">10.1145/2541940.2541944</idno>
		<ptr target="https://doi.org/10.1145/2541940.2541944" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS &apos;14)</title>
		<meeting>the 19th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS &apos;14)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="729" to="742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rocksteady: Fast Migration for Low-latency In-memory Storage</title>
		<author>
			<persName><forename type="first">Chinmay</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aniraj</forename><surname>Kesavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Stutsman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Symposium on Operating Systems Principles</title>
		<meeting>the 26th Symposium on Operating Systems Principles</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="390" to="405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Efficient l?1 regularized logistic regression</title>
		<author>
			<persName><forename type="first">Su-In</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="401" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Reconciling High Server Utilization and Sub-millisecond Quality-of-service</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Leverich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Kozyrakis</surname></persName>
		</author>
		<idno type="DOI">10.1145/2592798.2592821</idno>
		<ptr target="https://doi.org/10.1145/2592798.2592821" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth European Conference on Computer Systems (EuroSys &apos;14)</title>
		<meeting>the Ninth European Conference on Computer Systems (EuroSys &apos;14)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="4" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Reconciling high server utilization and sub-millisecond quality-of-service</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Leverich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth European Conference on Computer Systems</title>
		<meeting>the Ninth European Conference on Computer Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Work Stealing for Interactive Services to Meet Target Latency</title>
		<author>
			<persName><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunal</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameh</forename><surname>Elnikety</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I-Ting Angelina</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathryn</forename><forename type="middle">S</forename><surname>Mckinley</surname></persName>
		</author>
		<idno type="DOI">10.1145/2851141.2851151</idno>
		<ptr target="https://doi.org/10.1145/2851141.2851151" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP &apos;16)</title>
		<meeting>the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP &apos;16)<address><addrLine>New York, NY, USA, Article</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Cooper: Task colocation with cooperative games</title>
		<author>
			<persName><forename type="first">Qiuyun</forename><surname>Llull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songchun</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seyed</forename><surname>Majid Zahedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">High Performance Computer Architecture (HPCA), 2017 IEEE International Symposium</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="421" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Heracles: Improving Resource Efficiency at Scale</title>
		<author>
			<persName><forename type="first">David</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqun</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rama</forename><surname>Govindaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Parthasarathy Ranganathan</surname></persName>
		</author>
		<author>
			<persName><surname>Kozyrakis</surname></persName>
		</author>
		<idno type="DOI">10.1145/2749469.2749475</idno>
		<ptr target="https://doi.org/10.1145/2749469.2749475" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42Nd Annual International Symposium on Computer Architecture (ISCA &apos;15)</title>
		<meeting>the 42Nd Annual International Symposium on Computer Architecture (ISCA &apos;15)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="450" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Bubble-Up: Increasing in Modern Warehouse Scale Computers via Sensible Co-locations</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Mars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingjia</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Hundt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Skadron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mary</forename><forename type="middle">Lou</forename><surname>Soffa</surname></persName>
		</author>
		<idno type="DOI">10.1145/2155620.2155650</idno>
		<ptr target="https://doi.org/10.1145/2155620.2155650" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO-44)</title>
		<meeting>the 44th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO-44)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="248" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Octopus-Man: QoS-driven task management for heterogeneous multicores in warehouse-scale computers</title>
		<author>
			<persName><forename type="first">V</forename><surname>Petrucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Laurenzano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Doherty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Moss?l</surname></persName>
		</author>
		<author>
			<persName><forename type="first">'</forename></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.1109/HPCA.2015.7056037</idno>
		<ptr target="https://doi.org/10.1109/HPCA.2015.7056037" />
	</analytic>
	<monogr>
		<title level="m">2015 IEEE 21st International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="246" to="258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">J</forename><surname>Seber</surname></persName>
		</author>
		<author>
			<persName><surname>Lee</surname></persName>
		</author>
		<title level="m">Linear Regression Analysis</title>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">936</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A tutorial on support vector regression</title>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<idno type="DOI">10.1023/B:STCO.0000035301.49549.88</idno>
		<idno>STCO.0000035301.49549.88</idno>
		<ptr target="https://doi.org/10.1023/B" />
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="199" to="222" />
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Regression shrinkage and selection via the lasso</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series B (Methodological)</title>
		<imprint>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1996">1996. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Worth their watts? -an empirical study of datacenter servers</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sivasubramaniam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Shimpi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sivabalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Subbiah</surname></persName>
		</author>
		<idno type="DOI">10.1109/HPCA.2010.5463056</idno>
		<ptr target="https://doi.org/10.1109/HPCA.2010.5463056" />
	</analytic>
	<monogr>
		<title level="m">HPCA -16 2010 The Sixteenth International Symposium on High-Performance Computer Architecture</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">SWAP: Effective Fine-Grain Management of Shared Last-Level Caches with Minimum Hardware Support</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Setter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Mart? Nez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="121" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Quality of Service Support for Fine-Grained Sharing on GPUs</title>
		<author>
			<persName><forename type="first">Zhenning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Melhem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruce</forename><surname>Childers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youtao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minyi</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th Annual International Symposium on Computer Architecture (ISCA &apos;17)</title>
		<meeting>the 44th Annual International Symposium on Computer Architecture (ISCA &apos;17)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Variable selection in quantile regression</title>
		<author>
			<persName><forename type="first">Yichao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufeng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistica Sinica</title>
		<imprint>
			<biblScope unit="page" from="801" to="817" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Bubbleflux: Precise Online QoS Management for Increased Utilization in Warehouse Scale Computers</title>
		<author>
			<persName><forename type="first">Hailong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Breslow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Mars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingjia</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.1145/2485922.2485974</idno>
		<ptr target="https://doi.org/10.1145/2485922.2485974" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual International Symposium on Computer Architecture (ISCA &apos;13)</title>
		<meeting>the 40th Annual International Symposium on Computer Architecture (ISCA &apos;13)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="607" to="618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">PowerChief: Intelligent Power Allocation for Multi-Stage Applications to Improve Responsiveness on Power Constrained CMP</title>
		<author>
			<persName><forename type="first">Hailong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moeiz</forename><surname>Riaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Mars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingjia</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongzhi</forename><surname>Luan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th Annual International Symposium on Computer Architecture (ISCA &apos;17)</title>
		<meeting>the 44th Annual International Symposium on Computer Architecture (ISCA &apos;17)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">High-dimensional knn joins with incremental updates</title>
		<author>
			<persName><forename type="first">Cui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaochun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Geoinformatica</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="55" to="82" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">SMiTe: Precise QoS Prediction on Real-System SMT Processors to Improve Utilization in Warehouse Scale Computers</title>
		<author>
			<persName><forename type="first">Yunqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">A</forename><surname>Laurenzano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Mars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingjia</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.1109/MICRO.2014.53</idno>
		<ptr target="https://doi.org/10.1109/MICRO.2014.53" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 47th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 47th Annual IEEE/ACM International Symposium on Microarchitecture<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="406" to="418" />
		</imprint>
	</monogr>
	<note>-47)</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Treadmill: Attributing the source of tail latency through precise load testing and statistical inference</title>
		<author>
			<persName><forename type="first">Yunqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Meisner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Mars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingjia</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Architecture (ISCA), 2016 ACM/IEEE 43rd Annual International Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="456" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Dirigent: Enforcing QoS for Latency-Critical Tasks on Shared Multicore Systems</title>
		<author>
			<persName><forename type="first">Haishan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mattan</forename><surname>Erez</surname></persName>
		</author>
		<idno type="DOI">10.1145/2954680.2872394</idno>
		<ptr target="https://doi.org/10.1145/2954680.2872394" />
	</analytic>
	<monogr>
		<title level="j">SIGOPS Oper. Syst. Rev</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="33" to="47" />
			<date type="published" when="2016-03">2016. March 2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
