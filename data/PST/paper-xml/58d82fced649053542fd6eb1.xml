<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DELVING INTO TRANSFERABLE ADVERSARIAL EX-AMPLES AND BLACK-BOX ATTACKS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yanpei</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xinyun</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Chang</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of the California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DELVING INTO TRANSFERABLE ADVERSARIAL EX-AMPLES AND BLACK-BOX ATTACKS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>An intriguing property of deep neural networks is the existence of adversarial examples, which can transfer among different architectures. These transferable adversarial examples may severely hinder deep neural network-based applications. Previous works mostly study the transferability using small scale datasets. In this work, we are the first to conduct an extensive study of the transferability over large models and a large scale dataset, and we are also the first to study the transferability of targeted adversarial examples with their target labels. We study both non-targeted and targeted adversarial examples, and show that while transferable non-targeted adversarial examples are easy to find, targeted adversarial examples generated using existing approaches almost never transfer with their target labels. Therefore, we propose novel ensemble-based approaches to generating transferable adversarial examples. Using such approaches, we observe a large proportion of targeted adversarial examples that are able to transfer with their target labels for the first time. We also present some geometric studies to help understanding the transferable adversarial examples. Finally, we show that the adversarial examples generated using ensemble-based approaches can successfully attack Clarifai.com, which is a black-box image classification system. * Work is done while visiting UC Berkeley.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Recent research has demonstrated that for a deep architecture, it is easy to generate adversarial examples, which are close to the original ones but are misclassified by the deep architecture <ref type="bibr" target="#b14">(Szegedy et al. (2013)</ref>; <ref type="bibr" target="#b2">Goodfellow et al. (2014)</ref>). The existence of such adversarial examples may have severe consequences, which hinders vision-understanding-based applications, such as autonomous driving. Most of these studies require explicit knowledge of the underlying models. It remains an open question how to efficiently find adversarial examples for a black-box model.</p><p>Several works have demonstrated that some adversarial examples generated for one model may also be misclassified by another model. Such a property is referred to as transferability, which can be leveraged to perform black-box attacks. This property has been exploited by constructing a substitute of the black-box model, and generating adversarial instances against the substitute to attack the black-box system <ref type="bibr" target="#b9">(Papernot et al. (2016a;</ref><ref type="bibr">b)</ref>). However, so far, transferability is mostly examined over small datasets, such as <ref type="bibr">MNIST (LeCun et al. (1998)</ref>) and CIFAR-10 ( <ref type="bibr" target="#b5">Krizhevsky &amp; Hinton (2009)</ref>). It has yet to be better understood transferability over large scale datasets, such as ImageNet <ref type="bibr" target="#b11">(Russakovsky et al. (2015)</ref>).</p><p>In this work, we are the first to conduct an extensive study of the transferability of different adversarial instance generation strategies applied to different state-of-the-art models trained over a large scale dataset. In particular, we study two types of adversarial examples: (1) non-targeted adversarial examples, which can be misclassified by a network, regardless of what the misclassified labels may be; and (2) targeted adversarial examples, which can be classified by a network as a target label. We examine several existing approaches searching for adversarial examples based on a single model. While non-targeted adversarial examples are more likely to transfer, we observe few targeted adversarial examples that are able to transfer with their target labels.</p><p>We further propose a novel strategy to generate transferable adversarial images using an ensemble of multiple models. In our evaluation, we observe that this new strategy can generate non-targeted adversarial instances with better transferability than other methods examined in this work. Also, for the first time, we observe a large proportion of targeted adversarial examples that are able to transfer with their target labels.</p><p>We study geometric properties of the models in our evaluation. In particular, we show that the gradient directions of different models are orthogonal to each other. We also show that decision boundaries of different models align well with each other, which partially illustrates why adversarial examples can transfer.</p><p>Last, we study whether generated adversarial images can attack Clarifai.com, a commercial company providing state-of-the-art image classification services. We have no knowledge about the training dataset and the types of models used by Clarifai.com; meanwhile, the label set of Clarifai.com is quite different from ImageNet's. We show that even in this case, both non-targeted and targeted adversarial images transfer to Clarifai.com. This is the first work documenting the success of generating both non-targeted and targeted adversarial examples for a black-box state-of-the-art online image classification system, whose model and training dataset are unknown to the attacker.</p><p>Contributions and organization. We summarize our main contributions as follows:</p><p>• For ImageNet models, we show that while existing approaches are effective to generate non-targeted transferable adversarial examples (Section 3), only few targeted adversarial examples generated by existing methods can transfer (Section 4).</p><p>• We propose novel ensemble-based approaches to generate adversarial examples (Section 5). Our approaches enable a large portion of targeted adversarial examples to transfer among multiple models for the first time.</p><p>• We are the first to present that targeted adversarial examples generated for models trained on ImageNet can transfer to a black-box system, i.e., Clarifai.com, whose model, training data, and label set is unknown to us (Section 7). In particular, Clarifai.com's label set is very different from ImageNet's.</p><p>• We conduct the first analysis of geometric properties for large models trained over Ima-geNet (Section 6), and the results reveal several interesting findings, such as the gradient directions of different models are orthogonal to each other.</p><p>In the following, we first discuss related work, and then present the background knowledge and experiment setup in Section 2. Then we present each of our experiments and conclusions in the corresponding section as mentioned above.</p><p>Related work. Transferability of adversarial examples was first examined by <ref type="bibr" target="#b14">Szegedy et al. (2013)</ref>, which studied the transferability (1) between different models trained over the same dataset; and (2) between the same or different model trained over disjoint subsets of a dataset; However, <ref type="bibr" target="#b14">Szegedy et al. (2013)</ref> only studied MNIST.</p><p>The study of transferability was followed by <ref type="bibr" target="#b2">Goodfellow et al. (2014)</ref>, which attributed the phenomenon of transferability to the reason that the adversarial perturbation is highly aligned with the weight vector of the model. Again, this hypothesis was tested using MNIST and CIFAR-10 datasets. We show that this is not the case for models trained over ImageNet. <ref type="bibr" target="#b9">Papernot et al. (2016a;</ref><ref type="bibr">b)</ref> examined constructing a substitute model to attack a black-box target model.</p><p>To train the substitute model, they developed a technique that synthesizes a training set and annotates it by querying the target model for labels. They demonstrate that using this approach, black-box attacks are feasible towards machine learning services hosted by Amazon, Google, and MetaMind. Further, <ref type="bibr" target="#b9">Papernot et al. (2016a)</ref> studied the transferability between deep neural networks and other models such as decision tree, kNN, etc.</p><p>Our work differs from <ref type="bibr" target="#b9">Papernot et al. (2016a;</ref><ref type="bibr">b)</ref>   <ref type="bibr" target="#b13">(Stallkamp et al. (2012)</ref>); in our work, we study the transferability over larger models and a larger dataset, i.e., ImageNet. Third, to attack black-box machine learning systems, we do not query the systems for constructing the substitute model ourselves.</p><p>In a concurrent and independent work, <ref type="bibr" target="#b8">Moosavi-Dezfooli et al. (2016)</ref> showed the existence of a universal perturbation for each model, which can transfer across different images. They also show that the adversarial images generated using these universal perturbations can transfer across different models on ImageNet. However, they only examine the non-targeted transferability, while our work studies both non-targeted and targeted transferability over ImageNet.</p><p>2 ADVERSARIAL DEEP LEARNING AND TRANSFERABILITY</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">THE ADVERSARIAL DEEP LEARNING PROBLEM</head><p>We assume a classifier f θ (x) outputs a category (or a label) as the prediction. Given an original image x, with ground truth label y, the adversarial deep learning problem is to seek for adversarial examples for the classifier f θ (x). Specifically, we consider two classes of adversarial examples.</p><p>A non-targeted adversarial example x is an instance that is close to x, in which case x should have the same ground truth as x, while f θ (x ) = y. For the problem to be non-trivial, we assume f θ (x) = y without loss of generality. A targeted adversarial example x is close to x and satisfies f θ (x ) = y , where y is a target label specified by the adversary, and y = y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">APPROACHES FOR GENERATING ADVERSARIAL EXAMPLES</head><p>In this work, we consider three classes of approaches for generating adversarial examples: optimization-based approaches, fast gradient approaches, and fast gradient sign approaches. Each class has non-targeted and targeted versions respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">APPROACHES FOR GENERATING NON-TARGETED ADVERSARIAL EXAMPLES</head><p>Formally, given an image x with ground truth y = f θ (x), searching for a non-targeted adversarial example can be modeled as searching for an instance x to satisfy the following constraints:</p><formula xml:id="formula_0">f θ (x ) = y (1) d(x, x ) ≤ B (2)</formula><p>where d(•, •) is a metric to quantify the distance between an original image and its adversarial counterpart, and B, called distortion, is an upper bound placed on this distance. Without loss of generality, we consider model f is composed of a network J θ (x), which outputs the probability for each category, so that f outputs the category with the highest probability.</p><p>Optimization-based approach. One approach is to approximate the solution to the following optimization problem:</p><formula xml:id="formula_1">argmin x λd(x, x ) − (1 y , J θ (x ))</formula><p>(3) where 1 y is the one-hot encoding of the ground truth label y, is a loss function to measure the distance between the prediction and the ground truth, and λ is a constant to balance constraints (2) and (1), which is empirically determined. Here, loss function is used to approximate constraint (1), and its choice can affect the effectiveness of searching for an adversarial example. In this work, we choose (u, v) = log (1 − u • v), which is shown to be effective by <ref type="bibr" target="#b0">Carlini &amp; Wagner (2016)</ref>.</p><p>Fast gradient sign (FGS). <ref type="bibr" target="#b2">Goodfellow et al. (2014)</ref> proposed the fast gradient sign (FGS) method so that the gradient needs be computed only once to generate an adversarial example. FGS can be used to generate adversarial images to meet the L ∞ norm bound. Formally, non-targeted adversarial examples are constructed as</p><formula xml:id="formula_2">x ← clip(x + Bsgn(∇ x (1 y , J θ (x))))</formula><p>Here, clip(x) is used to clip each dimension of x to the range of pixel values, i.e., <ref type="bibr">[0,</ref><ref type="bibr">255]</ref> in this work. We make a slight variation to choose (u, v) = log (1 − u • v), which is the same as used in the optimization-based approach.</p><p>Fast gradient (FG). The fast gradient approach (FG) is similar to FGS, but instead of moving along the gradient sign direction, FG moves along the gradient direction. In particular, we have</p><formula xml:id="formula_3">x ← clip(x + B ∇ x (1 y , J θ (x)) ||∇ x (1 y , J θ (x))||</formula><p>))</p><p>Here, we assume the distance metric in constraint (2), d(x, x</p><formula xml:id="formula_4">) = ||x − x || is a norm of x − x .</formula><p>The term sgn(∇ x ) in FGS is replaced by ∇x ||∇x || to meet this distance constraint. We call both FGS and FG fast gradient-based approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">APPROACHES FOR GENERATING TARGETED ADVERSARIAL EXAMPLES</head><p>A targeted adversarial image x is similar to a non-targeted one, but constraint (1) is replaced by f θ (x ) = y (4) where y is the target label given by the adversary. For the optimization-based approach, we approximate the solution by solving the following dual objective:</p><formula xml:id="formula_5">argmin x λd(x, x ) + (1 y , J θ (x ))</formula><p>(5) In this work, we choose the standard cross entropy loss</p><formula xml:id="formula_6">(u, v) = − i u i log v i .</formula><p>For FGS and FG, we construct adversarial examples as follows:</p><p>x</p><formula xml:id="formula_7">← clip(x − Bsgn(∇ x (1 y , J θ (x)))) (FGS) x ← clip(x − B ∇ x (1 y , J θ (x)) ||∇ x (1 y , J θ (x))|| ) (FG)</formula><p>where is the same as the one used for the optimization-based approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">EVALUATION METHODOLOGY</head><p>For the rest of the paper, we focus on examining the transferability among state-of-the-art models trained over ImageNet <ref type="bibr" target="#b11">(Russakovsky et al. (2015)</ref>). In this section, we detail the models to be examined, the dataset to be evaluated, and the measurements to be used. . We retrieve the pre-trained models for each network online. The performance of these models on the ILSVRC 2012 <ref type="bibr" target="#b11">(Russakovsky et al. (2015)</ref>) validation set can be found in our online technical report: <ref type="bibr" target="#b7">Liu et al. (2016)</ref>. We choose these models to study the transferability between homogeneous architectures (i.e., ResNet models) and heterogeneous architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>Dataset. It is less meaningful to examine the transferability of an adversarial image between two models which cannot classify the original image correctly. Therefore, from the ILSVRC 2012 validation set, we randomly choose 100 images, which can be classified correctly by all five models in our examination. These 100 images form our test set. To perform targeted attacks, we manually choose a target label for each image, so that its semantics is far from the ground truth. The images and target labels in our evaluation can be found on website<ref type="foot" target="#foot_3">4</ref> .</p><p>Measuring transferability. Given two models, we measure the non-targeted transferability by computing the percentage of the adversarial examples generated for one model that can be classified correctly for the other. We refer to this percentage as accuracy. A lower accuracy means better non-targeted transferability. We measure the targeted transferability by computing the percentage of the adversarial examples generated for one model that are classified as the target label by the other model. We refer to this percentage as matching rate. A higher matching rate means better targeted transferability. For clarity, the reported results are only based on top-1 accuracy. Top-5 accuracy's counterparts can be found in our online technical report: <ref type="bibr" target="#b7">Liu et al. (2016)</ref>.</p><p>Distortion. Besides transferability, another important factor is the distortion between adversarial images and the original ones. We measure the distortion by root mean square deviation, i.e., RMSD, which is computed as d(x , x) = i (x i − x i ) 2 /N , where x and x are the vector representations of an adversarial image and the original one respectively, N is the dimensionality of x and x , and x i denotes the pixel value of the i-th dimension of x, within range [0, 255], and similar for x i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">NON-TARGETED ADVERSARIAL EXAMPLES</head><p>In this section, we examine different approaches for generating non-targeted adversarial images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">OPTIMIZATION-BASED APPROACH</head><p>To apply the optimization-based approach for a single model, we initialize x to be x and use Adam Optimizer <ref type="bibr" target="#b4">(Kingma &amp; Ba (2014)</ref>) to optimize Objective (3) . We find that we can tune the RMSD by adjusting the learning rate of Adam and λ. We find that, for each model, we can use a small learning rate to generate adversarial images with small RMSD, i.e. &lt; 2, with any λ. In fact, we find that when initializing x with x, Adam Optimizer will search for an adversarial example around x, even when we set λ to be 0, i.e., not restricting the distance between x and x. Therefore, we set λ to be 0 for all experiments using optimization-based approaches throughout the paper. Although these adversarial examples with small distortions can successfully fool the target model, however, they cannot transfer well to other models (details can be found in our online technical report: Liu et al. ( <ref type="formula">2016</ref>)).</p><p>We increase the learning rate to allow the optimization algorithm to search for adversarial images with larger distortion. In particular, we set the learning rate to be 4. We run Adam Optimizer for 100 iterations to generate the adversarial images. We observe that the loss converges after 100 iterations. An alternative optimization-based approach leading to similar results can be found in our online technical report: <ref type="bibr" target="#b7">Liu et al. (2016)</ref>.</p><p>Non-targeted adversarial examples transfer. We generate non-targeted adversarial examples on one network, but evaluate them on another, and Table <ref type="table" target="#tab_2">1</ref> Panel A presents the results. From the table, we can observe that</p><p>• The diagonal contains all 0 values. This says that all adversarial images generated for one model can mislead the same model.</p><p>• A large proportion of non-targeted adversarial images generated for one model using the optimization-based approach can transfer to another. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">FAST GRADIENT-BASED APPROACHES</head><p>We then examine the effectiveness of fast gradient-based approaches. A good property of fast gradient-based approaches is that all generated adversarial examples lie in a 1-D subspace. Therefore, we can easily approximate the minimal distortion in this subspace of transferable adversarial examples between two models. In the following, we first control the RMSD to study fast gradientbased approaches' effectiveness. Second, we study the transferable minimal distortions of fast gradient-based approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">EFFECTIVENESS AND TRANSFERABILITY OF THE FAST GRADIENT-BASED</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPROACHES</head><p>Since the distortion B and the RMSD of the generated adversarial images are highly correlated, we can choose this hyperparameter B to generate adversarial images with a given RMSD. In  <ref type="table" target="#tab_2">1</ref>: Transferability of non-targeted adversarial images generated between pairs of models. The first column indicates the average RMSD of all adversarial images generated for the model in the corresponding row. The cell (i, j) indicates the accuracy of the adversarial images generated for model i (row) evaluated over model j (column). Results of top-5 accuracy can be found in our online technical report: <ref type="bibr" target="#b7">Liu et al. (2016)</ref>.</p><p>Panel B, we generate adversarial images using FG such that the average RMSD is almost the same as those generated using the optimization-based approach. We observe that the diagonal values in the table are all positive, which means that FG cannot fully mislead the models. A potential reason is that, FG can be viewed as approximating the optimization, but is tailored for speed over accuracy.</p><p>On the other hand, the values of non-diagonal cells in the table, which correspond to the accuracies of adversarial images generated for one model but evaluated on another, are comparable with or less than their counterparts in the optimization-based approach. This shows that non-targeted adversarial examples generated by FG exhibit transferability as well.</p><p>We also evaluate FGS, but the transferability of the generated images is worse than the ones generated using either FG or optimization-based approaches. The results can be found in our online technical report: <ref type="bibr" target="#b7">Liu et al. (2016)</ref>. It shows that when RMSD is around 23, the accuracies of the adversarial images generated by FGS is greater than their counterparts for FG. We hypothesize the reason why transferability of FGS is worse to this fact.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">ADVERSARIAL IMAGES WITH MINIMAL TRANSFERABLE RMSD</head><p>For an image x and two models M 1 , M 2 , we can approximate the minimal distortion B along a direction δ, such that x B = x + Bδ generated for M 1 is adversarial for both M 1 and M 2 . Here δ is the direction, i.e., sgn(∇ x ) for FGS, and ∇ x /||∇ x || for FG.</p><p>We refer to the minimal transferable RMSD from M 1 to M 2 using FG (or FGS) as the RMSD of a transferable adversarial example x B with the minimal transferable distortion B from M 1 to M 2 using FG (or FGS). The minimal transferable RMSD can illustrate the tradeoff between distortion and transferability.</p><p>In the following, we approximate the minimal transferable RMSD through a linear search by sampling B every 0.1 step. We choose the linear-search method rather than binary-search method to determine the minimal transferable RMSD because the adversarial images generated from an original image may come from multiple intervals. The experiment can be found in our online technical report: <ref type="bibr" target="#b7">Liu et al. (2016)</ref>.</p><p>Minimal transferable RMSD using FG and FGS. Figure <ref type="figure" target="#fig_1">1</ref> plots the cumulative distribution function (CDF) of the minimal transferable RMSD from VGG-16 to ResNet-152 using non-targeted FG (Figure <ref type="figure" target="#fig_1">1a</ref>) and FGS (Figure <ref type="figure" target="#fig_1">1b</ref>). From the figures, we observe that both FG and FGS can find 100% transferable adversarial images with RMSD less than 80.91 and 86.56 respectively. Further,  The matching rate of targeted adversarial images generated using the optimization-based approach. The first column indicates the average RMSD of the generated adversarial images. Cell (i, j) indicates that matching rate of the targeted adversarial images generated for model i (row) when evaluated on model j (column). The top-5 results can be found in our online technical report: <ref type="bibr" target="#b7">Liu et al. (2016)</ref>.</p><p>the FG method can generate transferable attacks with smaller RMSD than FGS. A potential reason is that while FGS minimizes the distortion's L ∞ norm, FG minimizes its L 2 norm, which is proportional to RMSD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">COMPARISON WITH RANDOM PERTURBATIONS</head><p>We also evaluate the test accuracy when we add a Gaussian noise to the 100 images in our test set. The concrete results can be found in our online technical report: <ref type="bibr" target="#b7">Liu et al. (2016)</ref>, where we show the conclusion that the "transferability" of this approach is significantly worse than either optimization-based approaches or fast gradient-based approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">TARGETED ADVERSARIAL EXAMPLES</head><p>In this section, we examine the transferability of targeted adversarial images. Table <ref type="table" target="#tab_3">2</ref> presents the results for using optimization-based approach. We observe that (1) the prediction of targeted adversarial images can match the target labels when evaluated on the same model that is used to generate the adversarial examples; but (2) the targeted adversarial images can be rarely predicted as the target labels by a different model. We call the latter that the target labels do not transfer.</p><p>Even when we increase the distortion, we still do not observe improvements on making target label transfer. Some results can be found in our online technical report: <ref type="bibr" target="#b7">Liu et al. (2016)</ref>. Even if we compute the matching rate based on top-5 accuracy, the highest matching rate is only 10%. The results can be found in our online technical report: <ref type="bibr" target="#b7">Liu et al. (2016)</ref>.</p><p>We also examine the targeted adversarial images generated by fast gradient-based approaches, and we observe that the target labels do not transfer as well. The results can be found in our online technical report: <ref type="bibr" target="#b7">Liu et al. (2016)</ref>. In fact, most targeted adversarial images cannot mislead the model, for which the adversarial images are generated, to predict the target labels, regardless of how large the distortion is used. We attribute it to the fact that the fast gradient-based approaches only search for attacks in a 1-D subspace. In this subspace, the total possible predictions may contain a small subset of all labels, which usually does not contain the target label. In Section 6, we study decision boundaries regarding this issue.</p><p>We also evaluate the matching rate of images added with Gaussian noise, as described in Section 3.3. However, we observe that the matching rate of any of the 5 models is 0%. Therefore, we conclude that by adding Gaussian noise, the attacker cannot generate successful targeted adversarial examples at all, let alone targeted transferability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">ENSEMBLE-BASED APPROACHES</head><p>We hypothesize that if an adversarial image remains adversarial for multiple models, then it is more likely to transfer to other models as well. We develop techniques to generate adversarial images for multiple models. The basic idea is to generate adversarial images for the ensemble of the models.</p><p>Formally, given k white-box models with softmax outputs being J 1 , ..., J k , an original image x, and its ground truth y, the ensemble-based approach solves the following optimization problem (for targeted attack):</p><formula xml:id="formula_8">argmin x − log ( k i=1 α i J i (x )) • 1 y + λd(x, x )<label>(6)</label></formula><p>where y is the target label specified by the adversary, α i J i (x ) is the ensemble model, and α i are the ensemble weights, k i=1 α i = 1. Note that ( <ref type="formula" target="#formula_8">6</ref>) is the targeted objective. The non-targeted counterpart can be derived similarly. In doing so, we hope the generated adversarial images remain adversarial for an additional black-box model J k+1 .</p><p>We evaluate the effectiveness of the ensemble-based approach. For each of the five models, we treat it as the black-box model to attack, and generate adversarial images for the ensemble of the rest four, which is considered as white-box. We evaluate the generated adversarial images over all five models. Throughout the rest of the paper, we refer to the approaches evaluated in Section 3 and 4 as the approaches using a single model, and to the ensemble-based approaches discussed in this section as the approaches using an ensemble model.</p><p>Optimization-based approach. We use Adam to optimize the objective (6) with equal ensemble weights across all models in the ensemble to generate targeted adversarial examples. In particular, we set the learning rate of Adam to be 8 for each model. In each iteration, we compute the Adam update for each model, sum up the four updates, and add the aggregation onto the image. We run 100 iterations of updates, and we observe that the loss converges after 100 iterations. By doing so, for the first time, we observe a large proportion of the targeted adversarial images whose target labels can transfer. The results are presented in Table <ref type="table">3</ref>. We observe that not all targeted adversarial images can be misclassified to the target labels by the models used in the ensemble. This suggests that while searching for an adversarial example for the ensemble model, there is no direct supervision to mislead any individual model in the ensemble to predict the target label. Further, from the diagonal numbers of the table, we observe that the transferability to ResNet models is better than to VGG-16 or GoogLeNet, when adversarial examples are generated against all models except the target model.</p><p>We also evaluate non-targeted adversarial images generated by the ensemble-based approach. We observe that the generated adversarial images have almost perfect transferability. We use the same procedure as for the targeted version, except the objective to generate the adversarial images. We evaluate the generated adversarial images over all models. The results are presented in Table <ref type="table">4</ref>. The generated adversarial images all have RMSDs around 17, which are lower than 22 to 23 of the optimization-based approach using a single model (See Table <ref type="table" target="#tab_2">1</ref> for comparison). When the adversarial images are evaluated over models which are not used to generate the attack, the accuracy is no greater than 6%. For a reference, the corresponding accuracies for all approaches evaluated in Section 3 using one single model are at least 12%. Our experiments demonstrate that the ensemblebased approaches can generate almost perfectly transferable adversarial images.</p><p>Fast gradient-based approach. The results for non-targeted fast gradient-based approaches applied to the ensemble can be found in our online technical report: <ref type="bibr" target="#b7">Liu et al. (2016)</ref>. We observe that the diagonal values are not zero, which is the same as we observed in the results for FG and Table <ref type="table">3</ref>: The matching rate of targeted adversarial images generated using the optimization-based approach. The first column indicates the average RMSD of the generated adversarial images. Cell (i, j) indicates that percentage of the targeted adversarial images generated for the ensemble of the four models except model i (row) is predicted as the target label by model j (column). In each row, the minus sign "−" indicates that the model of the row is not used when generating the attacks. Table <ref type="table">4</ref>: Accuracy of non-targeted adversarial images generated using the optimization-based approach. The first column indicates the average RMSD of the generated adversarial images. Cell (i, j) corresponds to the accuracy of the attack generated using four models except model i (row) when evaluated over model j (column). In each row, the minus sign "−" indicates that the model of the row is not used when generating the attacks. Results of top-5 accuracy can be found in our online technical report: <ref type="bibr" target="#b7">Liu et al. (2016)</ref>.</p><p>FGS applied to a single model. We hypothesize a potential reason is that the gradient directions of different models in the ensemble are orthogonal to each other, as we will illustrate in Section 6. In this case, the gradient direction of the ensemble is almost orthogonal to the one of each model in the ensemble. Therefore searching along this direction may require large distortion to reach adversarial examples.</p><p>For targeted adversarial examples generated using FG and FGS based on an ensemble model, their transferability is no better than the ones generated using a single model. The results can be found in our online technical report: <ref type="bibr" target="#b7">Liu et al. (2016)</ref>. We hypothesize the same reason to explain this: there are only few possible target labels in total in the 1-D subspace.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">GEOMETRIC PROPERTIES OF DIFFERENT MODELS</head><p>In this section, we show some geometric properties of the models to try to better understand transferable adversarial examples. Prior works also try to understand the geometic properties of adversarial examples theoretically <ref type="bibr" target="#b1">(Fawzi et al. (2016)</ref>) or empirically <ref type="bibr" target="#b2">(Goodfellow et al. (2014)</ref>). In this work, we examine large models trained over a large dataset with 1000 labels, whose geometric properties are never examined before. This allows us to make new observations to better understand the models and their adversarial examples.</p><p>The gradient directions of different models in our evaluation are almost orthogonal to each other. We study whether the adversarial directions of different models align with each other. We calculate cosine value of the angle between gradient directions of different models, and the results can be found in our online technical report: <ref type="bibr" target="#b7">Liu et al. (2016)</ref>. We observe that all non-diagonal values are close to 0, which indicates that for most images, their gradient directions with respect to different models are orthogonal to each other.</p><p>Decision boundaries of the non-targeted approaches using a single model. We study the decision boundary of different models to understand why adversarial examples transfer. We choose two We can observe that for all models, the region that each model can predict the image correctly is limited to the central area. Also, along the gradient direction, the classifiers are soon misled. One interesting finding is that along this gradient direction, the first misclassified label for the three ResNet models (corresponding to the light green region) is the label "orange". A more detailed study can be found in our online technical report: <ref type="bibr" target="#b7">Liu et al. (2016)</ref>. When we look at the zoomout figures, however, the labels of images that are far away from the original one are different for different models, even among ResNet models.</p><p>On the other hand, in Table <ref type="table">5</ref>, we show the total number of regions in each plane. In fact, for each plane, there are at most 21 different regions in all planes. Compared with the 1,000 total categories in ImageNet, this is only 2.1% of all categories. That means, for all other 97.9% labels, no targeted adversarial example exists in each plane. Such a phenomenon partially explains why fast gradientbased approaches can hardly find targeted adversarial images.</p><p>Further, in Figure <ref type="figure" target="#fig_3">4</ref>, we draw the decision boundaries of all models on the same plane as described above. We can observe that  • The boundaries align with each other very well. This partially explains why non-targeted adversarial images can transfer among models.</p><p>• The boundary diameters along the gradient direction is less than the ones along the random direction. A potential reason is that moving a variable along its gradient direction can change the loss function (i.e., the probability of the ground truth label) significantly. Therefore along the gradient direction it will take fewer steps to move out of the ground truth region than a random direction.</p><p>• An interesting finding is that even though we move left along the x-axis, which is equivalent to maximizing the ground truth's prediction probability, it also reaches the boundary much sooner than moving along a random direction. We attribute this to the non-linearity of the loss function: when the distortion is larger, the gradient direction also changes dramatically.</p><p>In this case, moving along the original gradient direction no longer increases the probability to predict the ground truth label (details can be found in our online technical report: Liu et al. ( <ref type="formula">2016</ref>)).</p><p>• As for VGG-16 model, there is a small hole within the region corresponding to the ground truth. This may partially explain why non-targeted adversarial images with small distortion exist, but do not transfer well. This hole does not exist in other models' decision planes. In this case, non-targeted adversarial images in this hole do not transfer.</p><p>Decision boundaries of the targeted ensemble-based approaches. In addition, we choose the targeted adversarial direction of the ensemble of all models except ResNet-101 and a random orthogonal direction, and we plot decision boundaries on the plane spanned by these two direction vectors in Figure <ref type="figure" target="#fig_4">5</ref>. We observe that the regions of images, which are predicted as the target label, align well for the four models in the ensemble. However, for the model not used to generate the adversarial image, i.e., ResNet-101, it also has a non-empty region such that the prediction is successfully misled to the target label, although the area is much smaller. Meanwhile, the region within each closed curve of the models almost has the same center.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">REAL WORLD EXAMPLE: ADVERSARIAL EXAMPLES FOR CLARIFAI.COM</head><p>Clarifai.com is a commercial company providing state-of-the-art image classification services. We have no knowledge about the dataset and types of models used behind Clarifai.com, except that we have black-box access to the services. The labels returned from Clarifai.com are also different from the categories in ILSVRC 2012. We submit all 100 original images to Clarifai.com and the returned labels are correct based on a subjective measure.</p><p>We also submit 400 adversarial images in total, where 200 of them are targeted adversarial examples, and the rest 200 are non-targeted ones. As for the 200 targeted adversarial images, 100 of them are generated using the optimization-based approach based on VGG-16 (the same ones evaluated in Table <ref type="table" target="#tab_3">2</ref>), and the rest 100 are generated using the optimization-based approach based on an ensemble of all models except ResNet-152 (the same ones evaluated in Table <ref type="table">3</ref>). The 200 nontargeted adversarial examples are generated similarly (the same ones evaluated in Table <ref type="table" target="#tab_2">1 and 4</ref>).</p><p>For non-targeted adversarial examples, we observe that for both the ones generated using VGG-16 and those generated using the ensemble, most of them can transfer to Clarifai.com.</p><p>More importantly, a large proportion of our targeted adversarial examples are misclassified by Clarifai.com as well. We observe that 57% of the targeted adversarial examples generated using VGG-16, and 76% of the ones generated using the ensemble can mislead Clarifai.com to predict labels irrelevant to the ground truth.</p><p>Further, our experiment shows that for targeted adversarial examples, 18% of those generated using the ensemble model can be predicted as labels close to the target label by Clarifai.com. The corresponding number for the targeted adversarial examples generated using VGG-16 is 2%. Considering that in the case of attacking Clarifai.com, the labels given by the target model are different from those given by our models, it is fairly surprising to see that when using the ensemble-based approach, there is still a considerable proportion of our targeted adversarial examples that can mislead this black-box model to make predictions semantically similar to our target labels. All these numbers are computed based on a subjective measure, and we include some examples in Table <ref type="table">6</ref>. More examples can be found in our online technical report:  <ref type="table">6</ref>: Original images and adversarial images evaluated over Clarifai.com. For labels returned from Clarifai.com, we sort the labels firstly by rareness: how many times a label appears in the Clarifai.com results for all adversarial images and original images, and secondly by confidence.</p><p>Only top 5 labels are provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>In this work, we are the first to conduct an extensive study of the transferability of both non-targeted and targeted adversarial examples generated using different approaches over large models and a large scale dataset. Our results confirm that the transferability for non-targeted adversarial examples are prominent even for large models and a large scale dataset. On the other hand, we find that it is hard to use existing approaches to generate targeted adversarial examples whose target labels can transfer. We develop novel ensemble-based approaches, and demonstrate that they can generate transferable targeted adversarial examples with a high success rate. Meanwhile, these new approaches exhibit better performance on generating non-targeted transferable adversarial examples than previous work. We also show that both non-targeted and targeted adversarial examples generated using our new approaches can successfully attack Clarifai.com, which is a black-box image classification system. Furthermore, we study some geometric properties to better understand the transferable adversarial examples.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>.</head><label></label><figDesc>We examine five networks, ResNet-50, ResNet-101, ResNet-152 (He et al. (2015)) 1 , GoogLeNet (Szegedy et al. (2014)) 2 , and VGG-16 (Simonyan &amp; Zisserman (2014)) 3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The CDF of the minimal transferable RMSD from VGG-16 to ResNet-152 using FG (a) and FGS (b). The green line labels the median minimal transferable RMSD, while the red line labels the minimal transferable RMSD to reach 90% percentage.</figDesc><graphic url="image-1.png" coords="7,136.35,81.86,141.30,105.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: The example image to study the decision boundary. Its ID in ILSVRC 2012 validation set is 49443, and its ground truth label is "anemone fish."</figDesc><graphic url="image-4.png" coords="10,135.91,249.90,62.70,62.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The decision boundary to separate the region within which all points are classified as the ground truth label (encircled by each closed curve) from others. The plane is the same one described in Figure 3. The origin of the coordinate plane corresponds to the original image. The units of both axises are 1 pixel values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure5: The decision boundary to separate the region within which all points are classified as the target label (encircled by each closed curve) from others. The plane is spanned by the targeted adversarial direction and a random orthogonal direction. The targeted adversarial direction is computed as the difference between the original image in Figure2and the adversarial image generated by the optimization-based approach for an ensemble. The ensemble contains all models except ResNet-101. The origin of the coordinate plane corresponds to the original image. The units of both axises are 1 pixel values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1</head><label>1</label><figDesc>RMSD </figDesc><table><row><cell>ResNet-152</cell><cell>22.83</cell><cell>0%</cell><cell>13%</cell><cell>18%</cell><cell>19%</cell><cell>11%</cell></row><row><cell>ResNet-101</cell><cell>23.81</cell><cell>19%</cell><cell>0%</cell><cell>21%</cell><cell>21%</cell><cell>12%</cell></row><row><cell>ResNet-50</cell><cell>22.86</cell><cell>23%</cell><cell>20%</cell><cell>0%</cell><cell>21%</cell><cell>18%</cell></row><row><cell>VGG-16</cell><cell>22.51</cell><cell>22%</cell><cell>17%</cell><cell>17%</cell><cell>0%</cell><cell>5%</cell></row><row><cell>GoogLeNet</cell><cell>22.58</cell><cell>39%</cell><cell>38%</cell><cell>34%</cell><cell>19%</cell><cell>0%</cell></row><row><cell></cell><cell></cell><cell cols="3">Panel A: Optimization-based approach</cell><cell></cell><cell></cell></row><row><cell></cell><cell>RMSD</cell><cell cols="5">ResNet-152 ResNet-101 ResNet-50 VGG-16 GoogLeNet</cell></row><row><cell>ResNet-152</cell><cell>23.45</cell><cell>4%</cell><cell>13%</cell><cell>13%</cell><cell>20%</cell><cell>12%</cell></row><row><cell>ResNet-101</cell><cell>23.49</cell><cell>19%</cell><cell>4%</cell><cell>11%</cell><cell>23%</cell><cell>13%</cell></row><row><cell>ResNet-50</cell><cell>23.49</cell><cell>25%</cell><cell>19%</cell><cell>5%</cell><cell>25%</cell><cell>14%</cell></row><row><cell>VGG-16</cell><cell>23.73</cell><cell>20%</cell><cell>16%</cell><cell>15%</cell><cell>1%</cell><cell>7%</cell></row><row><cell>GoogLeNet</cell><cell>23.45</cell><cell>25%</cell><cell>25%</cell><cell>17%</cell><cell>19%</cell><cell>1%</cell></row><row><cell></cell><cell></cell><cell cols="3">Panel B: Fast gradient approach</cell><cell></cell><cell></cell></row><row><cell>Table</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell cols="6">RMSD ResNet-152 ResNet-101 ResNet-50 VGG-16 GoogLeNet</cell></row><row><cell>ResNet-152</cell><cell>23.13</cell><cell>100%</cell><cell>2%</cell><cell>1%</cell><cell>1%</cell><cell>1%</cell></row><row><cell>ResNet-101</cell><cell>23.16</cell><cell>3%</cell><cell>100%</cell><cell>3%</cell><cell>2%</cell><cell>1%</cell></row><row><cell>ResNet-50</cell><cell>23.06</cell><cell>4%</cell><cell>2%</cell><cell>100%</cell><cell>1%</cell><cell>1%</cell></row><row><cell>VGG-16</cell><cell>23.59</cell><cell>2%</cell><cell>1%</cell><cell>2%</cell><cell>100%</cell><cell>1%</cell></row><row><cell>GoogLeNet</cell><cell>22.87</cell><cell>1%</cell><cell>1%</cell><cell>0%</cell><cell>1%</cell><cell>100%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Results of top-5 matching rate can be found in our online technical report:<ref type="bibr" target="#b7">Liu et al. (2016)</ref>.</figDesc><table><row><cell></cell><cell cols="6">RMSD ResNet-152 ResNet-101 ResNet-50 VGG-16 GoogLeNet</cell></row><row><cell>-ResNet-152</cell><cell>17.17</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell></row><row><cell>-ResNet-101</cell><cell>17.25</cell><cell>0%</cell><cell>1%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell></row><row><cell>-ResNet-50</cell><cell>17.25</cell><cell>0%</cell><cell>0%</cell><cell>2%</cell><cell>0%</cell><cell>0%</cell></row><row><cell>-VGG-16</cell><cell>17.80</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell><cell>6%</cell><cell>0%</cell></row><row><cell>-GoogLeNet</cell><cell>17.41</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell><cell>5%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>pug,</cell><cell></cell><cell></cell><cell>sea seal,</cell></row><row><cell></cell><cell>pug, pug-dog</cell><cell>friendship, adorable, purebred,</cell><cell>sea lion</cell><cell></cell><cell>ocean, head, sea,</cell></row><row><cell></cell><cell></cell><cell>sit</cell><cell></cell><cell></cell><cell>cute</cell></row><row><cell></cell><cell>Old</cell><cell>poodle,</cell><cell></cell><cell></cell><cell>veil,</cell></row><row><cell></cell><cell>English</cell><cell>retriever,</cell><cell></cell><cell></cell><cell>spirituality,</cell></row><row><cell></cell><cell>sheep-</cell><cell>loyalty,</cell><cell>abaya</cell><cell></cell><cell>religion,</cell></row><row><cell></cell><cell>dog,</cell><cell>sit,</cell><cell></cell><cell></cell><cell>people,</cell></row><row><cell></cell><cell>bobtail</cell><cell>two</cell><cell></cell><cell></cell><cell>illustration</cell></row><row><cell></cell><cell>maillot, tank suit</cell><cell>beach, woman, adult, wear, portrait</cell><cell>amphib-ian, amphibi-ous vehicle</cell><cell></cell><cell>transportation system, vehicle, man, print, retro</cell></row><row><cell></cell><cell>patas, hussar monkey, Erythro-cebus patas</cell><cell>primate, monkey, safari, sit, looking</cell><cell>bee eater</cell><cell></cell><cell>ornithology, avian, beak, wing, feather</cell></row><row><cell>original image</cell><cell>true label</cell><cell>Clarifai.com results of original image</cell><cell>target label</cell><cell>targeted adversarial example</cell><cell>Clarifai.com results of targeted adversarial example</cell></row><row><cell></cell><cell></cell><cell>bridge,</cell><cell></cell><cell></cell><cell>window,</cell></row><row><cell></cell><cell>viaduct</cell><cell>sight, arch, river,</cell><cell>window screen</cell><cell></cell><cell>wall, old, decoration,</cell></row><row><cell></cell><cell></cell><cell>sky</cell><cell></cell><cell></cell><cell>design</cell></row><row><cell></cell><cell></cell><cell>fruit,</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>hip, rose hip, rosehip</cell><cell>little,</cell><cell>stupa, tope</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>wildlife</cell><cell></cell><cell></cell><cell></cell></row></table><note><ref type="bibr" target="#b7">Liu et al. (2016)</ref>.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://github.com/KaimingHe/deep-residual-networks</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">https://github.com/BVLC/caffe/tree/master/models/bvlc_googlenet</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">https://gist.github.com/ksimonyan/211839e770f7b538e2d8</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">https://github.com/sunblaze-ucb/transferability-advdnn-pub</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_4">Buddha, gold, temple, celebration, artistic dogsled, dog sled, dog group together, four, sledge, sled, enjoyment hip, hip, cherry, branch, fruit, food, season</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This material is in part based upon work supported by the National Science Foundation under Grant No. TWC-1409915. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.04644</idno>
		<title level="m">Towards evaluating the robustness of neural networks</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Robustness of classifiers: from adversarial to random noise</title>
		<author>
			<persName><forename type="first">Alhussein</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seyed-Mohsen</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1624" to="1632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6572</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno>CoRR, abs/1412.6980</idno>
		<ptr target="http://arxiv.org/abs/1412.6980" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Delving into transferable adversarial examples and black-box attacks</title>
		<author>
			<persName><forename type="first">Yanpei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02770</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Universal adversarial perturbations</title>
		<author>
			<persName><forename type="first">Seyed-Mohsen</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alhussein</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omar</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Frossard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.08401</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Transferability in machine learning: from phenomena to black-box attacks using adversarial samples</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07277</idno>
		<imprint>
			<date type="published" when="2016">2016a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Practical black-box attacks against deep learning systems using adversarial examples</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Berkay Celik</surname></persName>
		</author>
		<author>
			<persName><surname>Swami</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02697</idno>
		<imprint>
			<date type="published" when="2016">2016b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-015-0816-y</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>CoRR, abs/1409.1556</idno>
		<ptr target="http://arxiv.org/abs/1409.1556" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Man vs. computer: Benchmarking machine learning algorithms for traffic sign recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Stallkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schlipsing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Salmen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Igel</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neunet.2012.02.016</idno>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0893608012000457" />
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<idno type="ISSN">0893-6080</idno>
		<imprint>
			<biblScope unit="issue">0</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6199</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno>CoRR, abs/1409.4842</idno>
		<ptr target="http://arxiv.org/abs/1409.4842" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
