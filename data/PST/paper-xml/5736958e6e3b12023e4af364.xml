<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A survey on data stream clustering and classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Hai-Long</forename><surname>Nguyen</surname></persName>
							<email>nguyenhl@i2r.a-star.edu.sg</email>
						</author>
						<author>
							<persName><forename type="first">Yew-Kwong</forename><surname>Woon</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Wee-Keong</forename><surname>Ng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">H.-L</forename><surname>Nguyen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Y.-K</forename><surname>Woon</surname></persName>
						</author>
						<author>
							<persName><forename type="first">W.-K</forename><surname>Ng</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Data Analytics Department</orgName>
								<orgName type="department" key="dep2">Institute for Infocomm Research (I2R) Agency for Science Technology and Research (A*STAR)</orgName>
								<address>
									<settlement>Singapore</settlement>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Airbus Group Innovations South Asia</orgName>
								<address>
									<settlement>Singapore</settlement>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<settlement>Singapore</settlement>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A survey on data stream clustering and classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3E7B94BAF40990D4FE5ACE8E672C8D41</idno>
					<idno type="DOI">10.1007/s10115-014-0808-1</idno>
					<note type="submission">Received: 22 August 2012 / Revised: 31 August 2014 / Accepted: 15 November 2014</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Data stream mining</term>
					<term>Clustering</term>
					<term>Classification</term>
					<term>Survey</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Nowadays, with the advance of technology, many applications generate huge amounts of data streams at very high speed. Examples include network traffic, web click streams, video surveillance, and sensor networks. Data stream mining has become a hot research topic. Its goal is to extract hidden knowledge/patterns from continuous data streams. Unlike traditional data mining where the dataset is static and can be repeatedly read many times, data stream mining algorithms face many challenges and have to satisfy constraints such as bounded memory, single-pass, real-time response, and concept-drift detection. This paper presents a comprehensive survey of the state-of-the-art data stream mining algorithms with a focus on clustering and classification because of their ubiquitous usage. It identifies mining constraints, proposes a general model for data stream mining, and depicts the relationship between traditional data mining and data stream mining. Furthermore, it analyzes the advantages as well as limitations of data stream algorithms and suggests potential areas for future research.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Traditional data mining research mostly focused on mining resident and static data repositories. However, technological developments give rise to the emergence of data streams and changed the way people store, communicate, and process data. Nowadays, many organizations generate large amounts of data at higher speed than ever. For example, on a daily basis, Google handles more than 3.5 billion searches, <ref type="foot" target="#foot_0">1</ref> NASA satellites generate around 4 TB images, <ref type="foot" target="#foot_1">2</ref> and WalMart records more than 20 million transactions. <ref type="foot" target="#foot_2">3</ref> The new "intensive data" research problem is: How can one model an infinite amount of continuous, rapid, and time-evolving data streams with a time-critical requirement?</p><p>These datasets are too large to fit in main memory and are alternatively stored in secondary storage devices. Therefore, random access to these datasets, which is commonly assumed in traditional data mining, is prohibitively expensive. One goal of data stream mining is to create a learning process that linearly increases according to the number of examples. Moreover, as data continuously arrive with new information, the model that was previously induced not only needs to incorporate new information, but also eliminates the effects of outdated data. Simply retraining the model with new examples is ineffective and inadequate; therefore, another goal of data stream mining is to update its model incrementally as each example arrives.</p><p>In this survey, we focus on clustering and classification of data stream as they are the two most frequent forms of data mining. Clustering aims to group a dataset into subsets (clusters), where data objects within a cluster are "similar" and data objects in different clusters are "dissimilar" with respect to a given similarity measure. Classification is the process of finding a general model from known data and then using this model to predict class labels for new data objects. Readers may find a good introduction of traditional data mining in a book of Jiawei Han <ref type="bibr" target="#b50">[51]</ref>.</p><p>There are some stream mining surveys that give overviews of various data stream algorithms. Gaber et al. <ref type="bibr" target="#b37">[38]</ref> introduced a survey with theoretical foundations and basic algorithms for various tasks of data stream mining, including classification, clustering, frequency counting, and time series analysis. For stream clustering, it covers STREAM algorithm that uses a divide-and-conquer strategy to incrementally cluster data streams <ref type="bibr" target="#b72">[75]</ref>, and CluStream <ref type="bibr" target="#b2">[3]</ref> that maintains its online summarized statistics and performs clustering in an offline manner. For stream classification, the survey consists of a decision tree algorithm for data streams <ref type="bibr" target="#b34">[35]</ref>, an ensemble-based classifier et al. <ref type="bibr" target="#b95">[99]</ref>, and a nearest neighbor classifier On-Demand classification <ref type="bibr" target="#b4">[5]</ref>. However, it may not be easy to understand the survey as the authors did not provide clear distinctions among these algorithms. In addition, many new and interesting algorithms have emerged since then.</p><p>Another survey of Gama and Rodrigues <ref type="bibr" target="#b41">[42]</ref> focused on illustrating data stream mining with specific algorithms and applications. The study is interesting with a real-life example of an electrical network problem. Data streams are continuously generated by around 4,000 sensors spread over the electrical network. To make hourly/daily/weekly forecasts on the electronic load, several data stream algorithms are applied, including the PiD algorithm (Partition Incremental Discretization) for data preprocessing <ref type="bibr" target="#b40">[41]</ref>, ODAC clustering algorithm with incremental correlation measure <ref type="bibr" target="#b77">[80]</ref>, and VFDT decision tree classifier <ref type="bibr" target="#b34">[35]</ref>. Although the study gives a practical viewpoint of data stream mining, it is lacking in in-depth analysis and the covered algorithms are quite outdated. Later, Aggarwal <ref type="bibr" target="#b1">[2]</ref> attempted to give a broader overview of data stream mining, where more mining tasks and real applications were discussed, but it still does not give readers a sufficient understanding of current data stream mining methods.</p><p>In a recent survey paper, Silva et al. <ref type="bibr" target="#b83">[86]</ref> introduced a taxonomy to classify data stream clustering algorithms. In addition, the authors also examined temporal aspects of data stream clustering, which may obscure the boundary separating outliers from possibly new clusters. Our work dovetails this paper by creating a framework to better understand the evolution of classical clustering algorithms into their stream versions, which will allow future algorithms to be more systematically designed to cope with specific temporal effects. We also review some newly emerged algorithms as well as stream validation methods.</p><p>In this paper, we introduce a comprehensive survey of the latest data stream clustering and classification algorithms. We present preliminaries and overview of data stream mining in Sect. 2. Then, we discuss state-of-the-art algorithms in detail, including their merits and limitations. We classify them into different categories based on their approaches and derive the relationships between traditional mining algorithms and stream mining algorithms in Sects. <ref type="bibr">3 and 4.</ref> We also analyze capabilities of each algorithm in terms of addressing constraints in a data stream setting. Finally, we discuss future research in Sect. <ref type="bibr" target="#b4">5</ref> and conclude the survey in Sect. 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Data stream applications</head><p>Here, we introduce several data stream applications and requirements of performing data analytics on this special kind of data.</p><p>-Mining query streams: Searching the web to retrieve information has become an essential activity of our everyday life. Existing search engines such as Google, Bing, and Yahoo handle millions of queries on a daily basis. Mining query streams to provide users better searching results has attracted much research work. For example, Zeng et al. <ref type="bibr" target="#b99">[103]</ref> clustered web search results to facilitate users' quick browsing through search results.</p><p>Users generally enter very short text queries, which may produce imprecise searching results due to users' own ambiguity. Chien and Immorlica <ref type="bibr" target="#b26">[27]</ref> studied temporal correlation among text queries over a period of time and refine users' search by suggesting alternative queries. -Network monitoring: Internet includes many routers that are connected and communicate with each other by send IP packets. To manage such networks, we need to analysis traffic data to discover usage patterns and unusual activities in real time. Traffic data are recorded in the form of log files at many levels, such as packet logs containing source and destination IP addresses; flow logs storing the number of packets sent, start time, end time, and protocol; and SNMP logs aggregate the number of bytes sent over each link every few minutes <ref type="bibr" target="#b70">[72]</ref>. An example of network management is to detect and prevent malicious attacks in a large Internet service provider network. A data stream classifier is required to classify in real time different kinds of attacks, such as denial-of-service (DOS), unauthorized access from a remote machine (R2L), unauthorized access to local super-user privileges (U2R), surveillance and other probing attacks. -Sensor networks: A sensor network consists of spatially distributed autonomous sensors that cooperatively monitor an environment. These sensors can sense physical values about the environment, such as temperature, sound, vibration, pressure, humidity, and light; they can cooperatively pass their data through the network to a center. Sensor networks are involved in many real-life applications, such as traffic monitoring, smart homes, habitat monitoring, and healthcare <ref type="bibr" target="#b39">[40]</ref>. For example, modern hospitals are equipped with patient monitoring system to improve healthcare quality and staff productivity. Many body sensors, which are connected to critically ill patients, can produce massive physiological data, such as temperature, electrocardiogram, pulse oximetry, and blood pressure. Since sensor devices only store updated data and human eyes cannot detect these signal subtleties, the system must analyze healthcare data streams in real time and extract meaningful information for medical professionals, such as clinical rules to identify significant events <ref type="bibr" target="#b85">[88]</ref>. -Social network streams: Online social networks (OSNs) have become more and more popular; for example, Facebook, Twitter, and LinkedIn have millions of active users. Such networks generate tremendous online data streams, such as text, multimedia, linkage, and interaction. There is much research on social network mining. For example, stream clustering methods are used to detect communities and monitor their evolution in social networks. They can explain how communities emerge, expand, shrink, and evolve in a social network <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b88">91]</ref>. Moreover, stream classification algorithm helps to classify different types of users, categorize discussion topics <ref type="bibr" target="#b30">[31]</ref>, or event detection <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b78">81]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Software for data stream mining</head><p>There is some useful, open-source software for data stream mining research.</p><p>-WEKA: <ref type="foot" target="#foot_3">4</ref> WEKA is the most well-known data mining software within the academic environment. WEKA includes a collection of learning algorithms such as data preprocessing, classification, regression, clustering, association rules, and visualization. -Massive Online Analysis (MOA): <ref type="foot" target="#foot_4">5</ref> MOA is based on the WEKA framework that is designed for data stream learning. It includes many online learning algorithms for evolving data streams, for example, very fast decision tree <ref type="bibr" target="#b34">[35]</ref> and ensemble learning <ref type="bibr" target="#b73">[76]</ref>.</p><p>Moreover, MOA provides data stream generators, such as SEA concepts, STAGGER, and rotating hyper-plane. -RapidMiner: <ref type="foot" target="#foot_5">6</ref> RapidMiner is another open-source software for data mining. RapidMiner is more powerful than WEKA as it includes all algorithms in WEKA and other advanced algorithms. Moreover, it is more intuitive as it is able define a mining process as a series of operators and provides more visualization tools.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Overview of data stream mining</head><p>We define a data stream DS as a sequence of data objects or samples: DS = {x 1 , x 2 , . . . , x t , . . .}, where x i is the i-th arrived data object. Each data object x i has a label y i ∈ Y = {y 1 , y 2 , . . . , y c } when classifying data stream, and there is no label when clustering data stream.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Mining constraints</head><p>Data streams have intrinsic characteristics, such as possibly infinite volume, chronological order, and dynamical changes. For example, Google processes million searches daily, each of which is attached with a time stamp; and these searches are changed according to different hot topics at different times.</p><p>Table <ref type="table" target="#tab_0">1</ref> shows comparisons between traditional data mining and data stream mining. Traditional data mining is able to scan datasets many times; executes with unlimited time -Single-pass: Unlike traditional data mining that may read static datasets repetitively many times, each sample in a data stream is examined at most once and cannot be backtracked <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b44">45]</ref>. The reason is I/O operations that are quite expensive than main memory operations. This constraint can be relaxed a bit in a way that an algorithm is allowed to remember examples in the short term. For example, an algorithm may buffer a batch of examples for internal data processes. However, it must discard stored data later to get ready with newly arriving data. -Real-time response: Many data stream applications such as stock market prediction require real-time response. The amount of time for processing the data and providing decision must be fast <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b95">99]</ref>. -Bounded memory: The amount of arriving data is extremely large or potentially infinite.</p><p>As we may only compute and store a small summary of the data streams and possibly throw away the rest of the data; approximate results are acceptable <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b44">45]</ref>. -Concept-drift detection: Concept drifts refer to the situation when the discovered patterns (or the underlying data distribution) change over time <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b95">99]</ref>.</p><p>Definition 1 (Concept) A concept or a data source is defined as a set of the prior probabilities for the classes P(y i ) and the class-conditional density functions (pdf) P(X |y i ), i = 1, . . . , c (definition from <ref type="bibr" target="#b55">[57]</ref>): S = {(P(y 1 ), P(X |y 1 )); . . . ; (P(y c ), P(X |y c ))}.</p><p>Traditional data mining only works with a single concept, which means the distributions of training dataset and testing dataset are the same. Meanwhile, data streams are dynamic and have many concepts. An example is the problem of preventing malicious attacks in network monitoring. The descriptions of the two class "normal" and "attack" evolve over time. Moreover, attackers are active and always try to devise new attacking methods, thereby changing the definition of class "attack." Suppose data stream DS consists of a set of k data sources S i with known distributions, i = 1, . . . , k. At a time stamp t, we have one or more "active" data sources. Let w i (t) ∈ [0, 1] are the influence of data source S i at a time stamp t, where k i=1 w i (t) = 1. The data distribution of the data stream DS at a time stamp t is characterized as follows: </p><formula xml:id="formula_0">DS (t) = {w 1 (t)S 1 , . . . , w k (t)S k } (t ) = {0.67S , 0.33S } (b) (t ) = {0.33S , 0.67S } (c) (t ) = {0S , 1S } (d) (t ) = {1S , 0S }</formula><formula xml:id="formula_1">(t) = {w 1 (t)S 1 , . . . , w k (t)S k }, where w i (t) ∈ [0, 1] and k i=1 w i (t) = 1.</formula><p>If for any two time stamps t 1 and t 2 that DS (t 1 ) = DS (t 2 ), we say there is a concept drift.</p><p>The term concept drift indicates the change of the influence of data sources over time. Figure <ref type="figure">1</ref> illustrates an example of a concept drift, where the data distribution of a data stream DS gradually changes from a data source S 1 to a data source S 2 .</p><p>To help readers understand an overview of data stream mining, we propose a general model of data stream algorithms in Fig. <ref type="figure">2</ref>. When a data stream comes, a buffer is used to store the most recent data. The stream mining engine reads the buffer to create a synopsis of the data in memory. In order to maintain the synopsis, the system may apply different time window and computational approaches. When certain criteria are triggered, for example, a user's request or after a certain time lapse; the stream mining engine will process the synopsis and output approximate results. In general, most data stream algorithms are derived and adapted from traditional mining algorithms. Lastly, stream validation methods are applied to evaluate the performance of data stream algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Time windows</head><p>As data streams are potentially infinite, it is possible to only able to process a portion of the entire data streams. This interesting portion is defined as a time window of data objects. W [i, j] = (x i , x i+1 , . . . , x j ), where i &lt; j. There are different types of time windows: landmark window, sliding window, fading window, and tilted time window.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Landmark window</head><p>In the landmark window, we are interested in the entire data stream from starting time instant 1 to the current time instant t c ; the window is <ref type="bibr" target="#b51">53,</ref><ref type="bibr" target="#b59">61,</ref><ref type="bibr" target="#b61">63,</ref><ref type="bibr" target="#b72">75,</ref><ref type="bibr" target="#b81">84]</ref>. Using the landmark window, all transactions in the window are equally important; there is no difference between past and present data. However, as data stream evolves continuously, the model built with old data objects may become inconsistent with the new ones. In order to emphasize recent data, one may apply the sliding window, tilted window, or fading window variants.</p><formula xml:id="formula_2">W [1, t c ] [35,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Sliding window</head><p>In the sliding window variant W [t c -w + 1, t c ], we are only interested in the w most recent transactions; the others are eliminated <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b71">74,</ref><ref type="bibr" target="#b73">76,</ref><ref type="bibr" target="#b79">82,</ref><ref type="bibr" target="#b94">98,</ref><ref type="bibr" target="#b100">[104]</ref><ref type="bibr" target="#b101">[105]</ref><ref type="bibr" target="#b102">[106]</ref><ref type="bibr" target="#b103">[107]</ref>. The mining result is dependent on the size of the window w. If w is too large and there is a concept drift, the window possibly contains outdated information, and the accuracy of the model decreases. If w is small, the window may have deficient data and the model over-fits and suffers from large variances. Previous work considers a fixed value for the size of the sliding window specified by users or an experimental value. Recently, there are proposals for flexible sliding windows where the size of the window changes according to the accuracy of the model <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b63">65]</ref>. When the accuracy is high, the window extends; and when the accuracy is low, the window shrinks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Fading window</head><p>In the fading window variant, each data object is assigned a different weight according to its arrival time so that new transactions receive higher weights than old ones <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b66">68,</ref><ref type="bibr" target="#b67">69,</ref><ref type="bibr" target="#b74">77,</ref><ref type="bibr" target="#b76">79,</ref><ref type="bibr" target="#b84">87,</ref><ref type="bibr" target="#b89">92,</ref><ref type="bibr" target="#b91">94,</ref><ref type="bibr" target="#b93">97]</ref>. Using the fading window, we reduce the effect (importance) of old and outdated transactions on the mining results. A decreasing exponential function f ( t) = λ t (0 &lt; λ &lt; 1) is usually used in the fading model. In this function, t is the age of a data object that is equal to time difference between the current time and its arrival time. The fading window needs to choose a suitable fading parameter λ, which is typically set in the range [0.99, 1] in real applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.4">Tilted time window</head><p>The tilted time window variant is somewhere between the fading window and sliding window variants <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5]</ref>. It applies different levels of granularity with regard to the recency of data. One is more interested in recent data at fine scale than long-term data from the past at coarse scale. Tilted time window approximately stores the entire dataset and provides a nice tradeoff between storage requirements and accuracy. However, the model may become unstable after running for a long time. For example, the tree structure in FP-Stream <ref type="bibr" target="#b28">[29]</ref> will become very large over time, and the process of updating and scanning over the tree may degrade its performance. Similarly, the microstructures in On-Demand Classification <ref type="bibr" target="#b4">[5]</ref> will become larger and larger that may give rise to the problem of low-purity clustering with large microclusters <ref type="bibr" target="#b106">[110]</ref>.</p><p>Figure <ref type="figure" target="#fig_1">3</ref> shows examples of four different time windows. For fading window, λ is set to 0.99; the weights of data objects decrease. For tilted time window, we store the four most recent quarters of an hour, then the last 24 h, and last 31 days.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Computational approaches</head><p>Besides various time window variants, there are two computational approaches to process the data streams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Incremental learning</head><p>Incremental learning is another computational approach for data streams <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b51">53,</ref><ref type="bibr" target="#b66">68,</ref><ref type="bibr" target="#b67">69,</ref><ref type="bibr" target="#b79">82,</ref><ref type="bibr" target="#b84">87,</ref><ref type="bibr" target="#b87">90]</ref>. In this approach, the model incrementally evolves to adapt to changes in incoming data. There are two schemes to update the model: by data instance and by window. For example, Street et al. <ref type="bibr" target="#b87">[90]</ref> deployed an ensemble of classifiers for data stream. It evaluated a window of incoming data and adapted the model by adjusting the weight of each classifier or replacing an old classifier with an updated one. Figure <ref type="figure" target="#fig_2">4a</ref> illustrates the incremental learning approach. It has the advantage of providing mining results instantly, but requires more computational resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Two-phase Learning</head><p>Two-phase learning, also known as online-offline learning, is a common computational approach in data streams <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b59">61,</ref><ref type="bibr" target="#b74">[77]</ref><ref type="bibr" target="#b75">[78]</ref><ref type="bibr" target="#b76">[79]</ref><ref type="bibr" target="#b81">84,</ref><ref type="bibr" target="#b93">97]</ref>. The basic idea is to divide the mining process into two phases. In the first phase (online phase), a synopsis of data is updated in a real-time manner. In the second phase (offline phase), the mining process is performed on the stored synopsis whenever a user sends a request. For example, Aggarwal et al. <ref type="bibr" target="#b2">[3]</ref> proposed an online-offline clustering method for data streams. Its online component summarizes the statistical information of the data stream in a real-time manner. Meanwhile, the offline component uses the summary statistics to perform clustering at a high level whenever required. The two-phase learning approach is depicted in Fig. <ref type="figure" target="#fig_2">4b</ref>. This approach is able to process data streams at high speed. However, its limitation is that users must wait until the mining results are available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Stream validation</head><p>In traditional data mining with limited amount of data, its validation process focuses on maximizing the use of data. Hold-out, cross-validation, and leave-one-out are standard validation methods. The holdout method randomly divides the dataset into two subsets, one for training and one for testing. Common ratios for size of training and testing datasets are ( 1 2 , 1 2 ) and ( <ref type="formula">2</ref>3 , 1 3 ). The k-fold cross-validation segments the data into k independent and equal-size subsamples. A single subsample is used for testing while the remaining (k -1) subsamples are merged and used as training data. The validation process is repeated k times so that each subsample is used exactly once as testing data. The leave-one-out method is a variant of the cross-validation method where the number of folds k is equal to the data size.</p><p>In a data stream environment, as data are potentially infinite, validation focuses on evaluating the model at various stages. A well-known approach is to plot a learning curve by recording the model's performance over time, which will show how much the model improves with additional training data and how well it adapts to concept drifts. An algorithm is said to be more superior than another if its learning curve is above the other's curve most of the time.</p><p>Hold-out and prequential are two popular approaches for stream validation. In the holdout method, data examples are collected into chunks. Each data chunk is first used as a test example and then used to update the model. The hold-out method is preferred in scenarios with concept drifts since it allows the model to adapt to latest changes of the data. Prequential (or Interleaved Test-Then-Train) is another validation method for data streams <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b42">43]</ref>. Each data instance is used for testing the model before it is used to incrementally update the model. This method can be considered as a special case of the hold-out method, where chunk size is equal to one. It has the advantage of not needing a predefined chunk size; however, it obscures the algorithm's performance at a certain time since the model's early mistakes will quickly diminish over time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1">Evaluation measures</head><p>Evaluation measures for traditional data mining can generally be applied for data stream mining. For data clustering, some widely used measures are purity, precision, sum of squared distance, and F-measure. There are some constraints in data clustering such as the cluster homogeneity constraint, cluster completeness constraint, rag bag constraint, and cluster size versus quantity constraint. Unfortunately, these clustering metrics cannot simultaneously satisfy the above constraints. The BCubed measure, which computes the average correctness over the dataset, is stated to meet the above constraints at the same time <ref type="bibr" target="#b12">[13]</ref>. Recently, Kremer et al. <ref type="bibr" target="#b60">[62]</ref> proposed an effective evaluation measure for clustering data streams, named Cluster Mapping Measure (CMM). CMM is based on the concept of connectivity between points and clusters, which indicates how well a point fits the distribution of the cluster. It is able to handle different fault types caused by evolving data streams, including cluster aging, cluster joining, and cluster diminishing.</p><p>For data classification, accuracy and 0-1 loss function are two popular measures. To continuously assess the performance of data stream classification, Game et al. <ref type="bibr" target="#b38">[39]</ref> proposed the prequential loss method cooperating with different time windows. The forgetting prequential error is proven to converge to the Bayes error in case of stationary data, which makes it useful for concept drift detection. This method can easily apply to other performance measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Data repositories</head><p>There is a shortage of available real-world data for data stream evaluation. One possible reason is that researchers from traditional data mining domains usually keep their data small enough to accommodate batch learning. Another reason is the privacy issue while publishing very large datasets; researchers often use their private data to demonstrate their systems that cannot be reproduced. To overcome this shortage, some synthetic datasets with unlimited number of examples are created, for example, Random Tree Generator <ref type="bibr" target="#b34">[35]</ref>, SEA Concepts Generator <ref type="bibr" target="#b87">[90]</ref>, and Rotating Hyperplane <ref type="bibr" target="#b51">[53]</ref>. These data generators are implemented in the MOA software package <ref type="bibr" target="#b16">[17]</ref>. We also recommend some data repositories with large datasets for stream evaluation:</p><p>-UCI Machine Learning Repository-an online repository for the empirical analysis of machine learning algorithms. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Clustering</head><p>Clustering or data segmentation is the process of grouping objects into different sets called clusters. The goal is that data objects in the same cluster are similar and are dissimilar to data objects in other clusters. Clustering is a well-studied problem, and many clustering methods have been proposed in the literature <ref type="bibr" target="#b52">[54]</ref>. As mentioned in the general model (Fig. <ref type="figure">2</ref>), data stream algorithms typically maintain synopses of data streams using different time window and computational approaches. They generally extend traditional algorithms to work for data streams with the goal to satisfy constraints, such as bounded memory, single-pass, real-time processing, and concept drifts. Similar to traditional data clustering, data stream clustering methods can be classified into five categories: partitioning methods, hierarchical methods, density-based methods, grid-based methods, and model-based methods. Moreover, it requires a measure distance between clusters as two clusters may be merged while clustering. Basically, there are four types of distance measures, including minimum distance (single-linkage), maximum distance (complete-linkage), mean distance, and average distance. Since maximum distance and average distance require expensive computation, they are rarely used in data stream settings. Minimum distance and mean distance are more popular for data streams, for example, D-Stream <ref type="bibr" target="#b23">[24]</ref>, Den-Stream <ref type="bibr" target="#b21">[22]</ref>, and MR-Stream <ref type="bibr" target="#b93">[97]</ref> use minimum distance for cluster merging, while STREAM <ref type="bibr" target="#b72">[75]</ref>, ClusStream <ref type="bibr" target="#b2">[3]</ref> use mean distance for cluster merging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Partitioning methods</head><p>A partitioning algorithm groups dataset into k clusters, where k is a predefined parameter. It iteratively reassigns objects from one group to another group in order to minimize its objective function. For traditional clustering, the most popular methods are k-means and k-medians <ref type="bibr" target="#b54">[56]</ref>.</p><p>-STREAM <ref type="bibr" target="#b72">[75]</ref>: STREAM is one of the first data stream algorithms, which extends the k-medians algorithm. To address the bounded memory and single-pass constraints, it uses a divide-and-conquer strategy and performs clustering incrementally. The STREAM algorithm breaks the data stream into chunks D 1 , . . . , D r , . . ., each of which has a manageable size m and fits into main memory. For each D i with at most m data points {x 1 , . . . , x m }, STREAM uses a k-medians algorithm to select k representatives (medians) {c 1 , . . . , c k } from D i and assigns each data point to its closest representatives. The objective function is to minimize the sum of squared distance (SSQ) measure:</p><formula xml:id="formula_3">SS Q(M, C) = k i=1 x j ⇐c i dist (x j , c i ),</formula><p>where the assignment operator is denoted by ⇐, and the distance function between two data points is denoted by dist (., .).</p><p>After a chunk has been processed, we only store k medians and their weights. The process is repeated for the next chunks. When the number of representative points exceeds m, a second level of cluster is applied to select level-2 representatives. In general, the algorithm is performed in a multi-level manner. Whenever the number of representatives at level-l reaches m, they are clustered to select k representatives of level-(l + 1). Together with a sampling method, STREAM is able to perform clustering with limited time and memory. Obviously, the STREAM algorithm is sensitive to parameter k due to the intrinsic properties of k-medians and is only able to discover spherical clusters. Moreover, it fails to detect concept drifts, where the underlying stream may evolve and change significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Hierarchical methods</head><p>A hierarchical method aims to group data objects into a hierarchical tree of clusters. Hierarchical clustering methods can be further classified as either agglomerative or divisive, where the hierarchical decompsition is formed in a bottom-up/merging or top-down/splitting fashion, respectively. Some traditional hierarchical algorithms are BIRCH <ref type="bibr" target="#b104">[108]</ref>, CURE <ref type="bibr" target="#b46">[47]</ref>, ROCK <ref type="bibr" target="#b47">[48]</ref>, and CHAMELEON <ref type="bibr" target="#b53">[55]</ref>.</p><p>-CluStream <ref type="bibr" target="#b2">[3]</ref>: CluStream extends the traditional clustering method BIRCH <ref type="bibr" target="#b104">[108]</ref> for data streams. CluStream uses micro-clusters to capture the summary information about the data streams. A micro-cluster is defined as a temporal extension of the clustering feature vector in BIRCH as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 3 (Micro-Cluster). A micro-cluster for n data points {x</head><formula xml:id="formula_4">i 1 , . . . , x i n } with time stamps {i 1 , . . . , i n } is a tuple (C F2 x , C F1 x , C F2 t , C F1 t , n),</formula><p>where C F2 x is the square sum of n data points, i.e., n j=1 (x i j ) 2 , C F1 x is the linear sum of n data points, i.e., n j=1 x i j , C F2 t is the square sum of the time stamps, i.e., n j=1 (i j ) 2 , C F1 t is the linear sum of the time stamps, i.e., n j=1 i j , and n is the number of data points. CluStream follows the online-offline approach, which is similar to the multi-phase clustering technique in BIRCH. In the online phase, CluStream continually maintains a set of q micro-clusters in the data stream. When a new micro-cluster is created, an outlier microcluster is deleted or two neighbor micro-clusters are merged. In the offline phase, it performs k-means to cluster the stored q micro-clusters. CluStream analyzes the evolution of clusters by using additional property to extract information of micro-clusters during a specific time range. Moreover, it applies the tilted time window to optimize the number of stored snapshots (the status of micro-clusters in the data stream) at differing levels of granularity.</p><p>Based on CluStream's framework, many improvements have been proposed. HPStream addresses the problem of high-dimensional data streams by deploying a projection technique to select the best attribute set for each cluster (subspace clustering) <ref type="bibr" target="#b3">[4]</ref>. Similar to CluStream, HPStream maintains micro-clusters to capture the summary information about the data stream. Furthermore, each micro-cluster consists of a set of relevant attributes, which can be considered its subspace. When a new data instance arrives, the average Manhattan distance between the new instance and each cluster is computed. Only relevant attributes of the clusters are utilized in the distance computation. Then, the new instance is assigned to the closest cluster if their distance does not exceed a limiting range, a multiple of the cluster's radius. Moreover, the statistical properties of the closest cluster are also updated. HPStream only maintains a fix number of micro-clusters. When the number of clusters reaches a maximum value, it removes the oldest cluster to give space for a new one.</p><p>SWClustering identifies a problem that the clustering results of CluStream may degrade after running for a long time <ref type="bibr" target="#b106">[110]</ref>. For example, when the center of a micro-cluster gradually shifts, CluStream maintains the micro-cluster with growing radius, instead of splitting it into many micro-clusters. To summarize data streams, SWClustering creates a temporal cluster feature (TCF) for a sliding window. The TCF is similar to a micro-cluster; the only difference is that TCF stores the latest timestamp, while micro-cluster stores the sum of timestamps. An exponential histogram of cluster feature (EHCF), a collection of TCFs, is used to capture the evolution of individual clusters. SWClustering not only produces more qualified clustering due to the fine granularity of EHCFs; it also has better performance than CluStream in terms of running time and memory usage.</p><p>E-Stream classifies cluster evolution into five categories: appearance, disappearance, self evolution, merging, and splitting. It utilizes the fading model and cluster histograms to identify the type of cluster evolution <ref type="bibr" target="#b92">[95]</ref>. ClusTree uses an R*-tree structure <ref type="bibr" target="#b14">[15]</ref> to index microclusters at different levels of granularities <ref type="bibr" target="#b58">[60]</ref>. It applies a depth first descent strategy for node insertion, which iteratively evaluates for better decisions as long as time permits. Thereby, the algorithm automatically adapts to the speed of the incoming data streams.</p><p>-REPSTREAM <ref type="bibr" target="#b67">[69]</ref>: Inspired by CHAMELEON <ref type="bibr" target="#b53">[55]</ref>, REPSTREAM is a graph-based hierarchical clustering approach for data streams. To identify clusters, REPSTREAM updates two sparse graphs that are formed by connecting each vertex to its k-nearest vertices. The first graph captures the connectivity relationship among coming data points and is used to select a set of representative vertices. The second graph of representative vertices helps to make clustering decisions at a higher level. REPSTREAM applies the fading window to diminish the effect of old data. REPSTREAM keeps track of the connectivity between the representative vertices and performs merging or splitting according to their connectivity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Density-based methods</head><p>Density-based methods build up a density profile of data for clustering purposes. Thereby, clusters are considered as dense regions of objects and are separated by sparse regions with low density in the data space. Density-based clustering is able to discover arbitrary-shaped clusters and does not require the predifined number of clusters. DBSCAN <ref type="bibr" target="#b69">[71]</ref>, OPTICS <ref type="bibr" target="#b13">[14]</ref>, and PreDeCon <ref type="bibr" target="#b17">[18]</ref> are some well-known, traditional density-based clustering methods.</p><p>-DenStream <ref type="bibr" target="#b21">[22]</ref>: DenStream is a density-based stream clustering algorithm that extends the DBSCAN algorithm. Similar to CluStream, DenStream uses micro-clusters to capture synopsis information of data streams; its online component continually updates the microclusters collection. Each micro-cluster has a center and a radius that are derived from its clustering feature vector. DenStream applies the fading model where elements of its feature vector decrease over time. Given threshold values for the weight and radius, there are three types of micro-clusters: a core micro-cluster, a potential core micro-cluster, and an outlier micro-cluster. For the offline components, it applies DBSCAN on these kinds of microclusters; a cluster is created as a group of micro-clusters that are dense and close to another.</p><p>-OPTICS-Stream <ref type="bibr" target="#b89">[92]</ref>: OPTICS-Stream is an extension of the OPTICS algorithm for data streams. Similarly to DenStream, it uses micro-clusters and the fading model to construct the synopsis. The offline component performs clustering by using the definitions of core-distance and reachability distance in OPTICS <ref type="bibr" target="#b13">[14]</ref>. A micro-cluster is a core micro-cluster if its weight is greater than or equal to μ, and its radius is less than or equal to . A micro-cluster c q is density-reachable to a core micro-cluster c p if the Euclidean between their centers is less than or equal to 3 × . By the order of data points with regard to the reachability distance, the 3-D reachability plot with one more time dimension can be used to visualize the changes of the cluster structures in the data stream over time.</p><p>-incPreDecon <ref type="bibr" target="#b61">[63]</ref>: incPreDecon is an incremental version of the PreDeCon <ref type="bibr" target="#b17">[18]</ref> algorithm, which is designed to work for dynamic data. The algorithm supports two types of updates, a single instance update and batch update. Both updating methods share the same strategy. They first identify a group of affected objects, whose properties may be changed into one of the following cases: (i) core → non-core, (ii) non-core → core, and (iii) core → core but under different preferences. Given new arriving data p, the group of affected objects consists of reachable objects from p. Properties of these objects, as well as their clusters, are updated according. The ability of incPreDecon to work for data streams is still unclear as only experiments with relatively small datasets have been performed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Grid-based methods</head><p>Grid-based clustering methods quantize data space into a multi-resolution grid structure. The grid structure contains many cells, each of which has a subspace and stores summary information of data objects within the subspace. Then, clusters are determined by dense regions of nearby dense cells. There are some popular, traditional grid-based clusters algorithms, for example, DENCLUE with a fix-sized grid <ref type="bibr" target="#b10">[11]</ref>, STING with a multiresolution grid <ref type="bibr" target="#b96">[100]</ref>,</p><p>and WaveCluster with wavelet transformation method to make the clusters more salient in the transformed space <ref type="bibr" target="#b82">[85]</ref>.</p><p>-D-Stream <ref type="bibr" target="#b23">[24]</ref>: D-Stream is a density-based clustering method for data streams. It can be considered as an extension of the DENCLUE algorithm <ref type="bibr" target="#b10">[11]</ref>. In D-Stream, each dimension is divided into p segments. With this, a grid of equal size hyper-rectangular cells is created. Similar to a micro-cluster, a grid cell in D-Stream is used to store synopsis information of data objects falling into it. As D-Stream uses the fading model to decrease the weight of cell over time, it periodically removes sparse grid cells to save memory and accelerate the mining process. D-Stream performs clustering upon a user request. A cluster is defined as a group of adjacent dense grid cells.</p><p>-MR-Stream <ref type="bibr" target="#b93">[97]</ref>: MR-Stream is a multi-resolution density-based clustering method for data stream. MR-Stream takes advantages of both D-Stream <ref type="bibr" target="#b23">[24]</ref> and STING <ref type="bibr" target="#b96">[100]</ref>. MR-Stream proposes a tree of grid cells to capture the hierarchical structure of the data space. A deeper-level tree node has higher granularity level. Similar to D-Stream, MR-Stream applies the fading model and periodically prunes sparse grid cells to save memory. Additionally, MR-Stream significantly reduces the number of tree nodes by merging sibling nodes of a parent node if they are all dense or sparse. Thus, MR-Stream preserves more memory than D-Stream and accelerates the clustering process. Moreover, MR-Stream provides a better cluster result by extending the neighborhood range concept and supports a memory sampling method that helps users to detect when concept drifts occur.</p><p>-CellTree <ref type="bibr" target="#b74">[77]</ref>: CellTree is another grid-based algorithm for data streams. It starts by partitioning the data space into a set of mutually exclusive equal size cells. When a weight of a cell is greater than a threshold value, the cell is dynamically divided into two intermediate cells using a hybrid-partition method that selects a better method between μ-partition and σ -partition methods. The μ-partition divides a dimension with the largest standard deviation, while the σ -partition method choose to split a dimension with the smallest standard deviation. To save memory, CellTree prunes sparse cells with density less than the threshold value. CellTree has been extended to a better version Cell*Tree <ref type="bibr" target="#b75">[78]</ref> that uses a B+Tree to store the synopses of data streams. The hybrid-partition method has a drawback that CellTree is not able to employ any indexing structure to access a specific grid cell immediately. In Cell*Tree, a dense grid cell is divided into a fixed number of equal size grid cells, which are indexed easily based on its order. Moreover, Cell*Tree applies the fading model to emphasize the latest change of information in a data stream on the clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Model-based methods</head><p>Model-based clustering methods attempt to optimize the likelihood between data and some statistic models. For traditional model-based clustering, Expectation-Maximization (EM) algorithm is a soft clustering method <ref type="bibr" target="#b33">[34]</ref>, and Self-Organizing Map (SOM) is a popular neural network method for clustering <ref type="bibr" target="#b57">[59]</ref>.</p><p>-SWEM <ref type="bibr" target="#b31">[32]</ref>: SWEM is an EM-based clustering algorithm for data streams using a sliding window. In SWEM, each micro-component is represented by a tuple consisting of a weight, a mean, and a covariance matrix. For the first data window, SWEM applies the EM algorithm to obtain the converged parameters. Then, in the incremental phase, SWEM utilizes the converged parameters in the previous window of data object as the initial values for the mixture models' parameters. If the two sets of parameters are significantly different, SWEM redistributes components in the entire data space by splitting those micro-components with large variance and merging neighbor micro-components. SWEM also deploys the fading model to expire the statistic summarization of the micro-components. In short, SWEM may be considered as an EM clustering using Mahalanobis<ref type="foot" target="#foot_6">7</ref> distance with fading window.</p><p>-GCPSOM <ref type="bibr" target="#b84">[87]</ref>: There are two important extensions of SOM: Growing self-organizing map (GSOM) <ref type="bibr" target="#b9">[10]</ref> and cellular probabilistic self-organizing map (CPSOM) <ref type="bibr" target="#b27">[28]</ref>. In GSOM, there is no need to prespecify the size of the output map; it dynamically grows nodes at the boundary of the map whenever its accumulated error exceeds a threshold. CPSOM is an online algorithm and is suitable for large datasets. CPSOM uses a fading window to reduce the weight of the neuron state. Thus, CPSOM may forget old patterns and adapt to new patterns as they appear. GCPSOM is a hybrid algorithm that aggregates the advantages of both the GSOM and CPSOM. Therefore, GCPSOM dynamically grows the feature map for clustering data streams and keeps track clusters as they evolve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Overall analysis</head><p>Previous sections have introduced many aspects of data stream mining, including constraints in data stream mining, time windows, and computational approaches. Many traditional and data stream clustering algorithms are also presented. Furthermore, we perform an overall analysis to summarize the above ideas and give readers a coherent view of data stream clustering. In Fig. <ref type="figure">5</ref>, we plot an intuitive diagram to outline the relationship between traditional clustering algorithms and data stream clustering algorithms. Traditional clustering methods are shown on the left of the diagram, and data stream clustering methods are on the right. Two schemata for categorizing data stream clustering methods are in the middle with two computational approaches and four time windows.</p><p>Based on this diagram, we observe that most data stream clustering methods are adapted from traditional clustering methods but apply different computational approaches and time windows. For example, STREAM extends the k-means algorithms with incremental computational approach and landmark window. The key idea of STREAM is to apply the LSEARCH technique to perform k-means incrementally and hierarchically. CluStream extends the BIRCH algorithm and applies the two-phase learning approach and tilted time window. HPStream improves CluStream to work for high-dimensional data streams; SWClustering enhances CluStream on long-term running; E-Stream extends CluStream to classify different types of concept drifts; and ClusTree indexes micro-clusters for automatical adaptation to the speed of data streams. REPSTREAM extends CHAMELEON algorithm with the incremental learning approach and fading window. Similarly, DenStream is an extension of DB-SCAN with the two-phase learning approach and fading window. incPreDeCon combines it with the preference distance measure to work with data streams. D-Stream inherits from DENCLUE; it has been extended into a multi-resolution approach with merging and splitting operations in MR-Stream. Moreover, CELL-TREE, XWAVE, SWEM, and GCPSOM are extensions of STING, WaveCluster, EM, and SOM, respectively.</p><p>Although many clustering methods have been proposed, they are often unable to address all data stream mining constraints simultaneously, which are discussed in Sect. 2. For the concept-drift constraint, we establish two levels of how the algorithms respond to concept drifts. The first level, concept-drift adaptation, means that an algorithm may update with new concepts and remove outdated concepts. Most algorithms deploy the time windows to update fresh information; therefore, they satisfy this criterion. However, those applying the landmark window do not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 5 The relationships between traditional clustering methods and stream clustering methods</head><p>The second level, concept-drift classification, indicates that an algorithm may detect and adapt properly to different types of concept drifts. For example, with cluster shifting or recurrence, we should extend the sliding window or retrieve historical information of recurrence clusters. Another example with cluster appearance/disapperance, a clustering algorithm should remove the least informative clusters (the least weight cluster or the farthest cluster from the new one), not the oldest one.</p><p>Furthermore, we evaluate whether these algorithms can work for high-dimensional data streams. Table <ref type="table" target="#tab_2">2</ref> summarizes the capabilities of previously reviewed data stream clustering techniques. We observe that only HPStream and incPreDecon can work with highdimensional data streams. E-Stream partly satisfies the concept-drift classification constraint as it can distinguish different types of cluster evolutions; however, it does not consider high dimensionality.</p><p>Besides the trade-offs of time windows and computational approaches in Sect. 2, it is worthy to note that traditional and data stream clustering algorithms share similar advantages and limitations. Table <ref type="table" target="#tab_3">3</ref> illustrates the trade-offs of each category of clustering techniques. For </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Classification</head><p>Classification is the process of finding a general model from past data to apply to new data. Classification is performed in two steps: learning step (training) and testing step. In the learning step, the system tries to learn a model from a collection of data objects, called the training set. In the testing step, the model is used to assign a class label for unlabeled data objects in the testing set. There are many data stream classification techniques in the literature, such as the decision tree, Bayesian classification, neural networks, support vector machines, k-nearest neighbor, and ensemble classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Decision Tree</head><p>Hoeffding tree is a decision tree classifier for data streams <ref type="bibr" target="#b34">[35]</ref>. Traditional decision trees need to scan the training data many times to select the splitting attribute. However, this requirement is infeasible in the data stream environment. To overcome this limitation, the Hoeffding bound is used to choose an optimal splitting attribute within a sufficient amount of receiving data objects. Given N independent observations of a random variable r with range R and computed mean r , the Hoeffding bound guarantees that the true mean of r is at least rwith probability 1 -δ, where δ is a user-specified parameter.</p><formula xml:id="formula_5">P(E[r ] ≥ (r -)) ≥ 1 -δ, = R 2 ln(1/δ) 2N</formula><p>Let G(X i ) be a heuristic measure to select a splitting attribute. After receiving N observations, X a and X b are the best and the second best splitting attribute. In case of Hoeffding tree, the variable r is now considered as</p><formula xml:id="formula_6">r = G = G(X a ) -G(X b ). If r = G = G(X a ) -G(X b ) &gt;</formula><p>, where is computed from the above equation, we say that this difference is larger than (r -) &gt; 0 with confidence 1 -δ. Then, the attribute X a is selected to build the tree.</p><p>Hoeffding tree algorithm is an incremental algorithm, which satisfies the single-pass constraint of data stream mining. For each new arriving data, Hoeffding tree algorithm uses Hoeffding bounds to check whether the best splitting attribute is confident enough to create the next level tree node.</p><p>Hoeffding tree algorithm has high accuracy and works well with large datasets. However, it is not able to handle concept drifts in data streams as no node can be changed once created. CVFDT <ref type="bibr" target="#b51">[53]</ref> is an extension of the Hoeffding tree to address concept drifts in data streams. CVFDT maintains sufficient statistics at every tree node to monitor the validity of its previous decisions. When data come, it continually updates the statistics stored in tree nodes. Using the sliding window, CVFDT removes the effect of outdated data by decreasing the corresponding statistics at the tree nodes. It periodically scans the tree nodes to detect concept drifts. If concept drift appears, CVFDT concurrently grows alternative branches with the new best attribute and removes the old branches with alternative branches if it becomes less accurate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Bayesian classification</head><p>Seidl et al. proposed a novel index-based classifier called Bayes tree <ref type="bibr" target="#b81">[84]</ref>. Adapted from the R*-tree <ref type="bibr" target="#b14">[15]</ref>, Bayes tree generates a hierarchical Gaussian-mixture tree to represent the entire dataset. Each tree node contains statistics of the data objects within, including a minimum bounding rectangle, the number of data objects, linear sum, and quadratic sum of all data objects.</p><p>To solve a multi-labeled classification problem, a single Bayes tree is constructed for each class. For each testing data object x, the algorithm tries to find a set of prefix-closed nodes E i , called frontier, in every Bayes tree. The probability that x belongs to class c i is computed as follows:</p><formula xml:id="formula_7">P(c i |x) = ⎛ ⎝ e s ∈E i n e s n g(x, μ e s , σ e s ) ⎞ ⎠ * P(c i )/P(x),</formula><p>where e s is a tree node in the frontier set E i ; n e s , μ e s and σ e s are, respectively, the number of data object, the center and the deviation of tree node e s . Testing object x is labeled with the class having the maximum probability. Bayes tree is an anytime classifier that can provide a decision after a very short initialization and later provide more accurate decisions when more time is available by selecting the more precise frontier. Bayes tree is later extended to the MC-tree <ref type="bibr" target="#b59">[61]</ref> by Kranen et al. The MC-tree combines all classes in a multi-Gaussian tree and needs only one step to refine all class models simultaneously rather than |C| steps in Bayes tree, where |C| is the number of classes. Moreover, MC-Tree optimizes tree construction by using multi-dimensional scaling (MDS) <ref type="bibr" target="#b32">[33]</ref> to transform the data space. It is claimed to outperform Bayes tree with higher accuracy of up to 15 %.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Neural Network</head><p>Leite et al. proposed an evolving granular neural network (eGNN) supported by granule-based learning algorithms to classify data streams. There are two phases in eGNN. In the first phase, eGNN uses T-S neurons to construct information granules of incoming data. Then, the neural network is built on the information granules rather than the original data in the second phase. A granule associated with a class label is defined by triangular membership functions, which are later evolved to accommodate new data. The weights are decreased by a decay constant; this process helps to reduce the degree of importance of outdated granules. Basically, eGNN uses class exemplars in the form of information granules to perform classification tasks. When testing data comes, a max-neuron selects the best-fit granules and assigns their labels as the prediction labels of the testing data. eGNN is able to tackle classification problems in continuously changing environments. However, the requirement of long training time is still a cost that limits the ability of eGNN to work with a massive dataset. Therefore, only small datasets are used to evaluate the performance of eGNN in the experiment section of this paper. The authors later extend this work to a more general and efficient semi-supervised approaches <ref type="bibr" target="#b65">[67]</ref> to work with partially labeled dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Support vector machines (SVMs)</head><p>Support vector machines have shown its prominent performance in many machine learning problems with static datasets. However, it is very expensive to use SMVs in large-scale application due to its time complexity O(N 3 ) and memory complexity O(N 2 ), where N is the number of data objects. To work with a very large dataset, Tsang et al. proposed the Core Vector Machine (CVM) algorithm that uses Minimum Enclosing Ball (MEB) to reduce its complexity <ref type="bibr" target="#b91">[94]</ref>. A MEB is a hyper-sphere that represents the set of data objects inside it. The algorithm first finds a representative MEB set that is a good approximation of the original dataset. Then, the optimization problem of finding the maximum margin is directly performed on this MEB set.</p><p>Rai et al. <ref type="bibr" target="#b76">[79]</ref> proposed StreamSVM, an extension of CVM with a single scan, to work with data streams. In StreamSVM, a MEB has a flexible radius that is increased whenever a new training is added. The algorithm is competitive by providing an approximation result to the optimal one. However, StreamSVM is still unable to detect concept drifts in data streams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">k-Nearest-Neighbor Classifier (k-NN)</head><p>On-Demand-Stream is a k-NN data stream classifier that extends the CluStream method <ref type="bibr" target="#b4">[5]</ref>. It inherits most of the good features in CluStream such as the micro-cluster structure, the tilted time window, and the online-offline approach. A micro-cluster in On-Demand-Stream is extended with a class label, and it only takes data objects with the same class label. Its offline classification process starts to find the best window of data objects, called the best time horizon. These micro-clusters in the best time horizon are extracted using the addition property of micro-clusters. On-Demand-Stream performs 1-NN classification by assigning a testing data object to the label of the closest micro-clusters.</p><p>Inspired by M-tree <ref type="bibr" target="#b29">[30]</ref>, Zhang et al. <ref type="bibr" target="#b100">[104]</ref> proposed a Lazy-tree structure to index microclusters (which are called exemplars in the paper). This helps to significantly reduce k-NN's classification time from O(N ) to O(log(N )), where N is the total number of exemplars in the Lazy-tree. The tree consists of three main operations: search, insertion, and deletion operations. The insertion and deletion operations add new nodes and remove outdated nodes in a way that guarantees that the tree is balanced. The search operation is used to classify testing data. A branch-and-bound technique based on the triangle inequality filters unnecessary checking exemplars; therefore, it minimizes the number of comparison and accelerates the searching operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Ensemble classifiers</head><p>Bagging and Boosting have shown their superior through extensive experiments on traditional dataset. Therefore, many researchers have tried to adapt the methods to work on data streams.</p><p>-Online Bagging &amp; Boosting: Oza et al. <ref type="bibr" target="#b73">[76]</ref> proposed the Online Bagging &amp; Boosting, which is one of the first work of adapting traditional bagging and boosting. From a statistical view, each training data object appears k times in training datasets of classifier members with the probability</p><formula xml:id="formula_8">P(k) = N k 1 N k 1 - 1 N N -k ,</formula><p>where k is the size of training set and N is the size of dataset. On data streams, we can assume that the number of data objects is unlimited, N → ∞. Therefore, the probability P(k) tends to a Poisson(1) distribution, where Poisson(1) = ex p(-1)/k!. Within this observation, Oza et al. proposed Online Bagging to assign each data object a weight according to Poisson(1) distribution, which is considered as a replacement sampling method. In Online Boosting, the weights of coming data objects and classifier members are adjusted according to the error rates of classifier members at the current time.</p><p>-Weighted Ensemble Classifiers: With the observation that the expiration of old data should rely on data distribution instead of their arrival time, Wang et al. <ref type="bibr" target="#b94">[98]</ref> proposed an accuracy-weighted ensemble (AWE) classifier for mining concept-drifting in data streams. The algorithm constructs and maintains a fix k number of classifiers, which can be C4.5, RIPPER, or naive Bayesian. Processing in a batch-mode manner, it use each new chunk of coming data objects to train a new classifier. Then, the ensemble is formed by selecting the k most accurate classifiers, and the weight of each classifier is set according to its accuracy. This method is stated to be better than a single data stream classifier, such as VFDT and CVFDT. However, it is quite sensitive to the chunk size and the number of classifier members k.</p><p>In real applications, data stream may contain noise where data instance may be mislabeled or have erroneous values. Zhang et al. <ref type="bibr" target="#b102">[106,</ref><ref type="bibr" target="#b103">107]</ref> proposed an aggregated ensemble algorithm to tackle the problem of learning from noisy data stream. This approach is a combination of horizontal and vertical ensemble frameworks. The horizontal framework builds a different classifier on each data chunk, while the vertical framework builds different classifiers on the up-to-date data chunk with different learning algorithms. The horizontal framework is robust to noise and can reuse historical information; however, it is unsuitable with sudden drifts, where the concepts of data stream change dramatically. On the other hand, the vertical frame work can produce good results even in case of sudden drifts; nevertheless, it is sensitive to noise. Building classifiers on different data chunks using different learning algorithms, the aggregated ensemble constitutes a Classifier Matrix. The average weighting method is used on this matrix to predict the label for testing data. The author theoretically proved that the aggregate ensemble has less or equal mean squared error of the vertical and horizontal frameworks in average. However, this framework suffers high time complexity.</p><p>Nguyen et al. <ref type="bibr" target="#b71">[74]</ref> addressed the problem of learning from high-dimensional data streams, where only a small subset of data features are important for learning process. Moreover, in this context, the definition of relevant features is temporary and limited to a certain period of time. Informative features may become irrelevant afterward, and previously insignificant features may become important features. Proposing a definition of feature drifts, i.e., a change in the set of important features, the authors combined this concept with a weighted ensemble classifier to tackle this problem. A multivariate feature selection method <ref type="bibr" target="#b64">[66]</ref> is adapted with a sliding window technique to detect feature drifts. Then, an ensemble learner consists of selected online learners and is constructed with an optimal weighting method. When a gradual drift occurs, classifier members of the ensemble are updated, and theirs weights are also adjusted according to their error rates. When a feature drift occurs, the ensemble replaces an old-fashioned learner with a updated classifier, which is trained with a new set of important features. Experiment results show that the algorithm is effective and efficient with high-dimensional data streams.</p><p>-Adapted One-vs-All Decision Trees (OVA) <ref type="bibr" target="#b79">[82]</ref>: OVA is a recent ensemble method for data streams. It learns k binary CVFDT classifiers, and each classifier is trained to classify instances between a specified class and all remaining classes. To classify a new data object, each classifier is run and the one with the highest confidence is returned. The confident of the classification is set as the proportion of the dominant class at the leaf where the testing object has reached. To archive a high accuracy, OVA aims to construct an ensemble of CVFDT classifiers with a low error correlation and a high diversity. Moreover, OVA quickly adapts to concept drifts as it only needs to update two component classifiers related to the evolving class and can work well with imbalance data streams.</p><p>-Meta-knowledge Ensemble <ref type="bibr" target="#b101">[105]</ref>: The high complexity of ensemble learning limits it to be applicable to many time-critical data stream applications in the real world. Zhang et al. proposed a meta-knowledge ensemble algorithm that selects the best suit classifier for testing data. An Ensemble-tree (E-tree) is constructed to organize base classifiers, each of which occupies a weight and a closed space in the whole data space. Similar to R-tree <ref type="bibr" target="#b48">[49]</ref>, the E-tree has three key operations, including search, insertion, and deletion operation. Hence, it is height-balanced and guarantees a logarithmic time complexity for prediction. The insert Fig. <ref type="figure">6</ref> The relationships between traditional classification methods and stream classification methods operation is used to integrate a new classifier into the ensemble. When the number of classifiers in a node exceeds a predefined value, the node is split into two nodes with a principle that the covering area of the two nodes should be minimized. The deletion operation removes outdated classifier when E-tree reaches its capacity; the tree may need to be reorganize to guarantee its balance. The search operation is used to classify a testing instance x. Classifiers whose close space contains x are invoked; a weighted voting method is applied to decide a class label for x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Overall analysis</head><p>After presenting many data stream classifiers, we introduce an overall analysis to summarize all related issues and give readers a broader view of data stream classification. Figure <ref type="figure">6</ref> shows the relationship between traditional and data stream classifiers. Traditional classifiers are on the left; data stream classifiers are on the right. To categorize data stream classifier, the two schemata, computational approaches and time windows, are in the middle.</p><p>Similarly, we observe that data stream classifiers are inherited from traditional classifiers and apply different computational approaches and time windows. For example, VFDT is an extension of the decision trees for data streams. VFDT uses Hoeffding bound to create a tree node when having a sufficient amount of data. It follows the incremental learning approach and landmark window. CVFDT is an enhanced version of VFDT that can adapt to concept drift by constructing alternative trees. Bayes tree is an extension of Bayesian classifier with the two-phase learning approach and landmark window. MC-Tree improves the Bayes tree with multi-label nodes. eGNN is a neural network algorithm that designed to work for data streams. CVM, derived from SVM classifier, follows the two-phase learning approach and fading window. StreamSVM extends CVM by making the radius of minimum enclosing balls flexible. On-Demand, an extension of k-NN classifier, applies the two-phase learning approach and tilted time window. Lazy-Tree is another improved version of k-NN classifier with the two-phase learning approach and sliding window. Many ensemble classifiers are designed to work for data streams. Online Bagging &amp; Boosting are extensions of traditional Bagging &amp; Boosting with incremental learning approach and sliding window. There are many weighted ensemble classifiers with different weighting strategies. For example, aggregated ensemble uses average weighting method (voting), while HEFT-Stream, AWE, OVA, and Ensemble-tree apply weighting method based on the accuracy of classifier members. They follow the incremental learning approach and can be loosely considered of applying sliding window as they typically remove the least accurate clarifier member.</p><p>Moreover, we evaluate capabilities of data stream classifiers into the Table <ref type="table" target="#tab_4">4</ref>. Although many classifiers have been proposed, they are typically unable to satisfy all constraints of data stream mining at the same time, which are discussed in Sect. 2. Similarly, we propose two levels of concept-drift constraint, concept-drift adaptation, and concept-drift classification. We also assess whether these classifiers can work for high-dimensional data streams. We observe that all classifiers satisfy the bounded memory and single-pass constraints. eGNN, CVM, and StreamSVM cannot response in a real-time manner, as they require much time for training process. HEFT-Stream can distinguish two kinds of concept drifts (gradual drifts and feature drifts) and adapts properly to them. VFDT, CVFDT, AWE, OVA, and Ensembletree can work for high-dimensional data streams, as they deploy the decision tree as their primitive classifiers. When a classifier follows a computational approach and time window, it will have some specific trade-offs, which is discussed in Sect. 2. Moreover, traditional and data stream classifiers also have common advantages and limitations in Table <ref type="table" target="#tab_5">5</ref>. For example, decision trees are easy to understand, robust to noise, efficient, and can remove redundant attributes; however, it suffers an over-fitting problem when the tree has many levels, and the classification rules become difficult to interpret. The lazy learner can be implemented easily, but it consumes much memory and is susceptible to high-dimensional data streams. Ensemble is highly accurate and easy to implement; however, it is mostly based on heuristics and lacks of solid theory. In high-dimensional data, not all data features (attributes) are important to the learning process. There are three common types of features: (i) irrelevant features, (ii) relevant but redundant features, and (iii) relevant and non-redundant features. The critical task of feature selection techniques is to extract the set of relevant and non-redundant features so that the learning process is more meaningful and faster. In the literature, feature selection techniques can be classified into three categories: filter, wrapper, and embedded models <ref type="bibr" target="#b68">[70]</ref>. The filter model applies an independent measure to evaluate a feature subset; thus, it only relies on the general characteristics of data. The wrapper model runs together with a learning algorithm and uses its performance to evaluate a feature subset. A hybrid model takes advantage of the above two models.</p><p>Furthermore, the importance of a feature evolves in data streams and is restricted to a certain period of time. Features that are previously considered as informative may become irrelevant and vice versa; those rejected features may become important features in the future. Thus, dynamic feature selection techniques are required to monitor the evolution of features. Figure <ref type="figure" target="#fig_3">7</ref> illustrates the dynamic nature of key features. Let us suppose that a data stream has three attributes {x,y,z} and two classes: the black and brown dots represent the positive and negative classes, respectively. At timestamp t 1 , the important feature set is {x,y} since data examples are located on the plane {x,y}. Consequently, as the data distribution evolves over time, the key feature set changes to {y,z} at timestamp t 2 .</p><p>After investigating many clustering and classification algorithms, we observe that only a few algorithms are able to work with high-dimensional data streams, and they have many limitations. For clustering, HPStream <ref type="bibr" target="#b3">[4]</ref> applies the projected approach and only removes irrelevant features but no redundant features. It requires a predefined value of the average dimensional degree and is just suitable for discovery spherical-shaped clusters. Similarly, incPreDeCon <ref type="bibr" target="#b61">[63]</ref> only takes out irrelevant features and suffers from high complexity. For classification, some classifiers work with high-dimensional data streams as they are simply derived from or deployed decision tree as primitive classifiers <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b51">53,</ref><ref type="bibr" target="#b79">82,</ref><ref type="bibr" target="#b94">98,</ref><ref type="bibr" target="#b101">105]</ref>. In this scenario, they are considered to be deploying a filter model that uses decision tree's informative measures (e.g., information gain) to select relevant features. Hence, these algorithms strictly use decision tree as primitive classifiers and are not able to remove redundant features. HEFT-Stream <ref type="bibr" target="#b71">[74]</ref> is the only ensemble classifier that removes both irrelevant and redundant features and that works with any type of classifier. However, HEFT-Stream still uses the filter model, which is independent of classifier members. Therefore, the problem of dynamic feature selection in data streams is open and requires further exploration. A dimension reduction technique that follows the hybrid model, captures features' evolution dynamically, and removes both irrelevant and redundant features is much expected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Tracking cluster evolution</head><p>In many applications, it is necessary to keep track of cluster evolution over time so that analysts can gain more valuable insights into the nature of the data streams. For example, in customer relationship management, a company may want to know whether an emerging cluster is a new group of customers or rather a shift of behavior of existing customers. Figure <ref type="figure" target="#fig_4">8</ref> illustrates an example of cluster evolution. At timestamp t 1 , there are four dynamic clusters, C1, C2, C3, and C4. The clusters C1 and C2 are merged into cluster C12 at timestamp t 2 . Meanwhile, the cluster C3 disappears and cluster C5 emerges. Moreover, cluster C4 is shifted to the right.</p><p>To address this problem, Spiliopoulou et al. proposed the MONIC algorithm to detect changes in clusters by checking their overlaps <ref type="bibr" target="#b86">[89]</ref>. There are two types of cluster transitions: internal transitions (e.g., cluster shrinks, cluster expands, cluster becomes compacter or diffuser, and cluster shifts) and external transitions (e.g., cluster survives, cluster is split into many clusters, cluster is merged with other cluster, cluster disappears, and cluster emerges). MONIC is independent of any clustering algorithm; however, it is unsuitable for data streams. It requires time to post-process clustering results; thus, it cannot show relationships among clusters in a real-time manner.</p><p>C-TREND constructs a temporal cluster graph to capture temporal order among clusters <ref type="bibr" target="#b0">[1]</ref>. In this graph, each node represents for a cluster and is labeled with the size of the cluster. Edges connect adjacent node and are labeled with a distance value (similarity) between two nodes. Althought C-TREND is able to visualize and detect trends in multiattribute temporal data, it suffers from many limitations. First, C-TREND is not suitable for data streams as it requires a predefined number of clusters. C-TREND is also computationally expensive as it deploys hierarchical clustering and constructs a dendrogram as a preprocessing step. Moreover, it only considers transitions among clusters while other internal cluster transition is ignored.</p><p>TRACDS takes advantage of both MONIC and C-TREND <ref type="bibr" target="#b49">[50]</ref>. It is independent of clustering algorithms and records transitions among clusters into a transition-count matrix. The matrix can be easily converted to a Markov Chain directed graph to learn about temporal relationship between clusters. This information, for example, can be used to predict the cluster a future record will belong to. Unlike MONIC that performs post-process clustering results to detect cluster transitions, TRACDS incrementally constructs the transition-count matrix whenever each data instance arrives and is assigned to a specified cluster. Therefore, TRACDS only needs a lightweight interface and is able to work for data streams.</p><p>OPTICS-Stream is one of the first work that attempts to visualize the temporal order of clusters <ref type="bibr" target="#b89">[92]</ref>. Inheriting the OPTICS algorithm <ref type="bibr" target="#b13">[14]</ref>, OPTICS-Stream uses micro-cluster structure to build a synopsis of data streams and further orders reachability distance among micro-clusters. It produces a 3-D reachability plot that can be used to visualize the clustering structure and structure changes in data streams. However, this plot fails to provide a clear view of borders among clusters and cannot extract external transitions clusters, such as splitting or merging clusters.</p><p>The above research work tracks evolution of clusters; however, they typically ignore the historical clustering that is important to producing better clustering results. Evolutionary clustering that considers this research issue has attracted much research work recently. Its goals is to optimize the trade-off between preserving the faithfulness of current data and preventing dramatic shifts from historical clustering to current clustering. Chakrabarti et al. first introduced the problem and proposed a framework for evolutionary clustering <ref type="bibr" target="#b22">[23]</ref>. Its objective function is as follows:</p><formula xml:id="formula_9">T t=1 sq(C t , M t ) -cp. T t=2 hc(C t-1 , C t ),</formula><p>where M t is a similarity matrix at time t, sq(C t , M t ) is a snapshot quality of the clustering C t and M t at time t, hc(C t-1 , C t ) is a history cost between C t and C t-1 , and cp is the smoothing parameter to penalize the important of historical clusters. Most evolutionary clusterings differ from snapshot quality and history cost functions. The authors deployed agglomerative hierarchical clustering and k-means as examples of the framework. In hierarchical clustering, snapshot quality is the quality of all merges performed to create C t ; history cost is the distance between two trees. In k-means clustering, snapshot quality is the sum of distance between each data instance and its center; history cost is the sum of distance between each pairing clusters between time t and (t -1). Extending this idea, Chi et al. proposed two frameworks for evolutionary spectral clustering, called Preserving Cluster Quality (PCQ) and Preserving Cluster Membership (PCM) <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>. PCQ penalizes clustering results that disagree with past similarities, while PCM penalizes clustering results that disagree with past clustering results.</p><p>Although evolutionary clustering and data stream clustering have a close relationship, they have different objectives. Working with large amounts of high-speed data, data stream clustering much focuses on single-pass and scalability issues. While evolutionary clustering aims to obtain clusters that evolve smoothly over time, most evolutionary clusterings are spectral clustering algorithms that require matrix and graph processing operations; thus, they are computationally expensive. Therefore, tracking cluster evolution in data stream setting while preserving their smooth transitions remains a challenge. Moreover, any clustering algo-rithm should support user-friendly visualization so that users can easily explore transitions among clusters and understand the major reasons that trigger these evolutionary transitions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Mining text streams</head><p>Recently, a wide range of web-related applications have generated massive amounts of text streams. For example, users in social networks continuously communicate with others via text messages; many web portals categorize and provide real-time news according to readers' interests; and web crawlers harvest millions of webpages for indexing. Mining text streams is also relevant to other important tasks, such as email spam filtering and target marketing for electronic commerce. Readers may find details about traditional text mining in a survey paper of Sebastiani <ref type="bibr" target="#b80">[83]</ref>.</p><p>In general, high-dimensional stream mining techniques can be generalized to text streams after text data are performed with preprocessing steps, including removing stop-words, stemming, mapping to internal representations (such as bags of words, TF*IDF scheme, and phrase segmentation). However, text data are more complicated than high-dimensional data as it is unstructured, contains a high level of noise, and exists on different formats. Furthermore, text streams are rich with surprising events and complex topic evolution over time so that mining text stream is still a daunting task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Clustering text streams</head><p>In the past, there are some efforts on clustering for very large document database. Van Rijsbergen proposed a single-pass clustering algorithm, which requires a single sequential scan over the sentences <ref type="bibr">[96]</ref>. The algorithm is likely to k-mean algorithm <ref type="bibr" target="#b54">[56]</ref> by assigning a sentence to its closest cluster regarding to cosine-similarity measure. Can et al. introduced an cover-coefficient-based clustering method C 3 M for text clustering, which can give a priori knowledge of the number of clusters <ref type="bibr" target="#b20">[21]</ref>. After computing the cover-coefficient matrix for each documents, C 3 M algorithm selects seed documents having highest seed power. Then, it forms clusters through grouping non-seed documents around seed documents. The author later presented an incremental version of the C 3 M algorithm, named C 2 ICM algorithm <ref type="bibr" target="#b18">[19]</ref>.</p><p>For text stream clustering, Shi Zhong proposed one of the first algorithm, referred as the Online Spherical k-Means algorithm (OSKM) <ref type="bibr" target="#b105">[109]</ref>. Similar to the STREAM algorithm <ref type="bibr" target="#b72">[75]</ref>, OSKM divides text streams into small chunks, each of which can be processed effectively in main memory. It then applied a number of k-mean iterations to cluster each chunk. The centroids with counts of previous chunks are used as inputs of the next iteration. Moreover, it applies a fading model so that old documents will be forgotten at an exponential rate. Experimental results show that OSKM can very effective in clustering text streams.</p><p>Extending the concept of micro-cluster, Aggarwal et al. <ref type="bibr" target="#b5">[6]</ref> defined a cluster droplet structure to store statistics information of text streams. A cluster droplet is a tuple of (D F2, D F1, n, w(t), l), where D F2 maintains the counts of each word pair in the cluster, D F1 maintains the count of each word, n is the number of data points, w(t) is the decayed weight of data points, and l is the time stamp of the last update. Since two vectors D F2 and D F1 are huge, the algorithm only maintains two lists of nonzero counts to save spaces. When a document comes, it would be assigned to a suitable cluster, whose cluster droplet is correspondingly updated. In the offline clustering process, the additional property is used to extract a set of droplets within a time range. Then, each droplet is treated as a pseudo point, and the k-means algorithm is performed for clustering purpose. The statistical information of cluster droplets can be used for further analysis, such as higher-level clusters and correlation analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Online event detection</head><p>Topic detection and tracking (TDT), an important research in text mining, aims to organize documents into ordered stories/topics. Since a topic is defined as "a seminal event or activity with all directly related events and activities"Ú, event detection has become an key task in TDT. There is a large number of important research work on event detection, topic tracking, and novelty detection <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b19">20]</ref>. Online event detection (OED) is more challenging since it is required to detect and trace surprising events and emerging trends over text streams in an online fashion, for example, a sequence of news articles, a sequence of messages or dialogues in social networks. This problem is closely relevant to text stream clustering as the events can be inferred from clusters of related documents.</p><p>Most popular OED algorithms are based on the initial work of Yang et al. <ref type="bibr" target="#b97">[101]</ref>. In this work, the authors proposed an agglomerative clustering to extract events in the text corpus. To be able to work with text streams, the algorithm uses an incremental version of inverse document frequency (IDF) and applied an iterative bucketing and re-clustering techniques. It also employs a decaying model to lessen the effect of old documents on the current decision. When a new document arrives, it is compared to all existing clusters. If none of their similarity exceeds a threshold, a new cluster is created, and a new event is triggered. There are several extensions of this approach, for example, using name entities <ref type="bibr" target="#b62">[64]</ref> and reweighing terms <ref type="bibr" target="#b98">[102]</ref> to improve accuracy.</p><p>Another research direction is feature-pivot approach. It first aims to identify the representative features of the hidden events; the events later are detected by clustering these presentative features. Fung et al. proposed a parameter-free, probabilistic approach to detect bursty features. The frequency of each feature is modeled with a binomial distribution. Bursty features are extracted with a threshold-based heuristic <ref type="bibr" target="#b36">[37]</ref>. The bursty events are detected by maximizing the co-occurrences of the bursty features. Similarly, He et al. utilized the Kleinberg's concept of burstiness <ref type="bibr" target="#b56">[58]</ref> to identify bursty features and then applies a standard k-means to construct the clustering <ref type="bibr">[52]</ref>. A bursty feature is defined by a 2-state finite automaton model, where the feature's frequency at a certain time point is greater or equal to (s &gt; 1) times of its average frequency.</p><p>Recently, online event detection on social networks (such as Twitter, Facebook, MySpace and various blogging sites) has become a hot research topic. Sakaki et al. <ref type="bibr" target="#b78">[81]</ref> proposed one of the earliest algorithms of online event detection from Twitter posts. They considered each Twitter user as a sensor and applied Kalman filter to estimate the location of earthquakes and typhoons. This approach monitors the burstiness of a predefined set of keywords (such as earthquake, shaking, and typhoon) and applies a support vector machine (SVM) to classify it as a real event or not. Petrovic et al. tracked events on Twitter streams by utilizing locality sensitive hashing (LSH) to limit the search space. To further reduce time complexity, they limited the number of documents inside a single bucket and the maximum number of comparisons for a new document. Based on cosine measure, similar tweets are grouped together as events.</p><p>Aggarwal et al. used a graph-based algorithm that considers both the text context and the network structure for online event detection <ref type="bibr" target="#b6">[7]</ref>. In the graph, a node is a user and an edge corresponds to an activity between two users, for example, sending message. This approach aims to discover highly dense clusters in the graph, each of which is defined by a set of nodes and a set of content-summary words. The similarity between a stream object and a cluster is a linear combination of the structural and content-based similarity values. A stream object is considered as a novel event if it is placed in a newly created cluster. With a different approach, Li et al. applied phrase segmentation for identifying the bursty features and then performed k-nearest-neighbor clustering for event detection. They further exploited Wikipedia as an external information source to identify the realistic events (i.e., phrases that appear as anchor text in Wikipedia articles are more newsworthy than those which do not).</p><p>To better understand evolving topics in text streams, Cui et al. <ref type="bibr" target="#b30">[31]</ref> proposed TextFlow, an interactive visual analysis tool for analyzing evolution patterns from multiple topics. The core component of TextFlow is a three-level Directed Acyclic Graph (DAG) whose first level corresponds to the topic flow, the second level encodes keyword bundles (critical events), and the third level represents keyword thread. First, text documents are clustered so that topics represented by document clusters and their connections are extracted. The Hierarchical Dirichlet Processes <ref type="bibr" target="#b90">[93]</ref> are used to trace spitting/merging events between topics. These critical events are likely to involve intense keyword changes as a topic is summarized by keywords. Acknowledging the limitation of stack-based graph <ref type="bibr" target="#b35">[36]</ref> that cannot well-present topic merging and splitting, TextFlow applies a river-flow-based visualization that help users understand and analyze the evolution topics easily. The publication and Bing news datasets are used to demonstrate this framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>With the proliferation of advanced data collection systems, data streaming mining has emerged as a new active and exciting area of research, which aims to extract knowledge from massive amounts of continuously generated data. Unlike traditional data mining, data stream mining is a continuous learning process while coping with time and memory limitations. Scalability and concept drift adaptation are two key issues of data stream mining. In order to achieve scalable performance, a data stream algorithm must minimize the number of passes over the data while fitting the data synopsis into the main memory. To address concept drifts, data stream algorithms need novel strategies to efficiently detect them at varying time windows.</p><p>This survey aims to provide a systematic analysis of the state-of-the-art data stream clustering and classification algorithms so as to enable a more informed decision in choosing the best algorithm to cope with today's highly dynamic data streams. Our specific focus on clustering and classification stems from their practical relevance in solving real-world problems.</p><p>First, we explain the key differences between traditional data mining and data stream mining. We identify four key constraints: single-pass, real-time response, bounded memory, and concept drift detection and propose a general model for data stream mining. Various computational approaches and their trade-offs are also presented. Next, we discuss in detail various state-of-the-art algorithms data stream clustering and classification. The inheritance of stream algorithms from traditional algorithms is intuitively illustrated so that readers can quickly understand and appreciate their relationships. We also examine their capabilities on satisfying stream mining constraints and analyze their pros and cons.</p><p>Finally, we explore future research directions which will continue to push boundaries and enhance the impact of data stream mining.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1 Fig. 1 Fig. 2 A</head><label>112</label><figDesc>Fig. 1 Example of a gradual change from a data source S 1 to a data source S 2 [73]. Class y 1 is depicted with gray dots, and class y 2 with black dots</figDesc><graphic coords="6,49.59,69.08,337.72,79.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3</head><label>3</label><figDesc>Fig. 3 Examples of time windows. a Landmark window, b sliding window, c fading window λ = 0.99, d tilted-time window</figDesc><graphic coords="8,51.09,56.03,337.96,129.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4</head><label>4</label><figDesc>Fig. 4 Two computational approaches. a Incremental learning, b two-phase learning</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 7</head><label>7</label><figDesc>Fig. 7 An example showing the dynamic nature of important features. a Time stamp t 1 , b time stamp t 2</figDesc><graphic coords="25,106.59,56.06,226.84,130.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8</head><label>8</label><figDesc>Fig. 8 An example of cluster evolution. a Timestamp t 1 , b timestamp t 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Comparisons between traditional data mining and data stream mining only one concept; and needs to produce fairly accurate results. On the other hand, data stream mining may produce approximate results and has to satisfy constraints, such as single-pass, real-time response, bounded memory, and concept-drift detection:</figDesc><table><row><cell>Characteristics/domains</cell><cell>Traditional data mining</cell><cell>Data stream mining</cell></row><row><cell>Number of passes</cell><cell>Multiple</cell><cell>Single</cell></row><row><cell>Time</cell><cell>Unlimited</cell><cell>Real-time</cell></row><row><cell>Memory</cell><cell>Unlimited</cell><cell>Bounded</cell></row><row><cell>Number of concepts</cell><cell>One</cell><cell>Multiple</cell></row><row><cell>Result</cell><cell>Accurate</cell><cell>Approximate</cell></row><row><cell>and memory; has</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>Capabilities of data stream clustering algorithms</figDesc><table><row><cell>Algorithm</cell><cell>Bounded</cell><cell>Single-pass Real-time</cell><cell>Concept-</cell><cell>Concept-</cell></row><row><cell></cell><cell>memory</cell><cell>response</cell><cell>drift adap-</cell><cell>drift classi-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>tation</cell><cell>fication</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc>Advantages and limitations of clustering approaches</figDesc><table><row><cell>Algorithm</cell><cell>Advantages</cell><cell>Limitations</cell></row><row><cell>Partitioning</cell><cell>Simple and relatively efficient Terminate</cell><cell>Need to specify the number of clusters</cell></row><row><cell></cell><cell>at a local optimum</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Unable to discover non-spherical clusters</cell></row><row><cell>Hierarchical</cell><cell>Derive more meaningful cluster</cell><cell>High complexity</cell></row><row><cell></cell><cell>structures</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Sensitive to the order of the data records</cell></row><row><cell>Density-based</cell><cell>Can find arbitrary-shape clusters Robust</cell><cell>Need many parameters: density and</cell></row><row><cell></cell><cell>to noises</cell><cell>noise thresholds</cell></row><row><cell></cell><cell></cell><cell>Difficult to detect clusters with different</cell></row><row><cell></cell><cell></cell><cell>densities</cell></row><row><cell>Grid-based</cell><cell>Fast and can discover arbitrary-shape</cell><cell>The clustering quality depends on the</cell></row><row><cell></cell><cell>clusters Robust to noises</cell><cell>grid granularity</cell></row><row><cell></cell><cell></cell><cell>Unsuitable to high-dimensional data</cell></row><row><cell>Model-based</cell><cell>Simple and can include domain</cell><cell>Depends strongly on the assumed models</cell></row><row><cell></cell><cell>knowledge</cell><cell></cell></row></table><note><p>example, partitioning clustering algorithms are simple and relatively efficient; however, they need to specify the number of clusters and are unable to discover non-spherical clusters. Gridbased clustering methods are quite fast; they are able to discover arbitrary-shaped clusters. Nevertheless, their clustering quality depends on grid granularity, and they are unsuitable for high-dimensional data streams.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc>Capabilities of data stream classification algorithms</figDesc><table><row><cell>Algorithm</cell><cell>Bounded</cell><cell>Single-</cell><cell>Real-time</cell><cell>Concept-</cell><cell>Concept-</cell></row><row><cell></cell><cell>memory</cell><cell>pass</cell><cell>response</cell><cell>drift adap-</cell><cell>drift classi-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>tation</cell><cell>fication</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5</head><label>5</label><figDesc>Advantages and limitations of classification algorithms</figDesc><table><row><cell>Algorithm</cell><cell>Advantages</cell><cell>Limitations</cell></row><row><cell>Decision tree</cell><cell>Easy to understand</cell><cell>Suffer an over-fitting problem</cell></row><row><cell></cell><cell></cell><cell>when the tree has many levels</cell></row><row><cell></cell><cell></cell><cell>and the rules become difficult</cell></row><row><cell></cell><cell></cell><cell>to comprehend</cell></row><row><cell></cell><cell>Robust to noises</cell><cell></cell></row><row><cell></cell><cell>Low computational cost, even for</cell><cell></cell></row><row><cell></cell><cell>very large training datasets</cell><cell></cell></row><row><cell></cell><cell>Can deal with redundant</cell><cell></cell></row><row><cell></cell><cell>attributes</cell><cell></cell></row><row><cell>Bayesian classifier</cell><cell>Simple, efficient, effective and</cell><cell>Independence assumption is</cell></row><row><cell></cell><cell>robust to noisy data</cell><cell>often violated in the real world</cell></row><row><cell>Neural network</cell><cell>Well-generalizing capability.</cell><cell>Inability to interpret the learned</cell></row><row><cell></cell><cell></cell><cell>model (black-box)</cell></row><row><cell></cell><cell>Can solve dynamic or nonlinear</cell><cell>High complexity</cell></row><row><cell></cell><cell>problem.</cell><cell></cell></row><row><cell>SVM</cell><cell>Provide a unique solution</cell><cell>Depend on the choice of the</cell></row><row><cell></cell><cell></cell><cell>kernel</cell></row><row><cell></cell><cell>Can solve nonlinear problem</cell><cell>High complexity</cell></row><row><cell></cell><cell>with implicit kernel</cell><cell></cell></row><row><cell></cell><cell>Work well even when training</cell><cell>Difficult to design multi-class</cell></row><row><cell></cell><cell>sample are bias</cell><cell>SVM classifiers</cell></row><row><cell>Lazy learner:</cell><cell>Easy to implement</cell><cell>Large storage requirements</cell></row><row><cell>k-NN</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>High complexity</cell></row><row><cell></cell><cell></cell><cell>Highly susceptible to</cell></row><row><cell></cell><cell></cell><cell>high-dimensional data</cell></row><row><cell>Ensemble</cell><cell>High accuracy</cell><cell>Based on heuristics and lack of</cell></row><row><cell></cell><cell></cell><cell>solid theory</cell></row><row><cell></cell><cell>Easy to implement</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>http://www.internetlivestats.com/google-search-statistics/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>http://data.nasa.gov/about/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>http://wikibon.org/blog/big-data-statistics/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>http://www.cs.waikato.ac.nz/ml/weka.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>http://moa.cs.waikato.ac.nz.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>http://rapid-i.com.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>http://en.wikipedia.org/wiki/Mahalanobis_distance.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_7"><p>VFDT<ref type="bibr" target="#b34">[35]</ref> √ √ √ √</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is partially supported by the Wattalyzer grant project of I2R, Singapore with Grant reference: NRF2012EWT-EIRP002-044.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">C-trend: temporal cluster graphs for identifying and visualizing trends in multiattribute transactional data</title>
		<author>
			<persName><forename type="first">G</forename><surname>Adomavicius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bockstedt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Knowl Data Eng</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="721" to="735" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Data streams: an overview and scientific applications</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="377" to="397" />
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A framework for clustering evolving data streams</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th international conference on very large data bases</title>
		<meeting>the 29th international conference on very large data bases</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="81" to="92" />
		</imprint>
		<respStmt>
			<orgName>VLDB Endowment</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A framework for projected clustering of high dimensional data streams</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th international conference on very large data bases</title>
		<meeting>the 13th international conference on very large data bases</meeting>
		<imprint>
			<date type="published" when="2004">2004. 1316763</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="852" to="863" />
		</imprint>
		<respStmt>
			<orgName>VLDB Endowment</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A framework for on-demand classification of evolving data streams</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Knowl Data Eng</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="577" to="589" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On clustering massive text and categorical data streams</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl Inf Syst</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="171" to="196" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Event detection in social streams</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Subbian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIAM international conference on data mining</title>
		<meeting>the SIAM international conference on data mining</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="624" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Online analysis of community evolution in data streams</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIAM international conference on data mining</title>
		<meeting>the SIAM international conference on data mining</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="56" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Novelty detection for topic tracking</title>
		<author>
			<persName><forename type="first">C</forename><surname>Aksoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Can</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kocberber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Am Soc Inf Sci Technol</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="777" to="795" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dynamic self-organizing maps with controlled growth for knowledge discovery</title>
		<author>
			<persName><forename type="first">D</forename><surname>Alahakoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Halgamuge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Neural Netw</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="601" to="614" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An efficient approach to clustering in large multimedia databases with noise</title>
		<author>
			<persName><forename type="first">H</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Er</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Daniel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the 1998 international conference knowledge discovery and data mining</title>
		<meeting>eeding of the 1998 international conference knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="58" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On-line new event detection and tracking</title>
		<author>
			<persName><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Papka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st annual international ACM SIGIR conference on research and development in information retrieval</title>
		<meeting>the 21st annual international ACM SIGIR conference on research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="37" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A comparison of extrinsic clustering evaluation metrics based on formal constraints</title>
		<author>
			<persName><forename type="first">E</forename><surname>Amigo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Artiles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf Retr</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="461" to="486" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Optics: ordering points to identify the clustering structure</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ankerst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Breunig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1999 ACM SIGMOD international conference on management of data</title>
		<meeting>the 1999 ACM SIGMOD international conference on management of data</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="49" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The R*-tree: an efficient and robust access method for points and rectangles</title>
		<author>
			<persName><forename type="first">N</forename><surname>Beckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Seeger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>ACM</publisher>
			<biblScope unit="volume">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adaptive stream mining: pattern learning and mining from evolving data streams</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bifet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the 2010 conference on adaptive stream mining: pattern learning and mining from evolving data streams</title>
		<meeting>eeding of the 2010 conference on adaptive stream mining: pattern learning and mining from evolving data streams</meeting>
		<imprint>
			<publisher>IOS Press</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Moa: massive online analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bifet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kirkby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Mach Learn Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1601" to="1604" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Density connected clustering with local subspace preferences</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bohm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kailing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kroger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th IEEE international conference on data mining</title>
		<meeting>the 4th IEEE international conference on data mining</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2004">2004. 1033433</date>
			<biblScope unit="page" from="27" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Incremental clustering for dynamic information processing</title>
		<author>
			<persName><forename type="first">F</forename><surname>Can</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans Inf Syst</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="143" to="164" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
	<note>TOIS)</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">New event detection and topic tracking in turkish</title>
		<author>
			<persName><forename type="first">F</forename><surname>Can</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kocberber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Baglioglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kardas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Ocalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Uyar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Am Soc Inf Sci Technol</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="802" to="819" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Concepts and effectiveness of the cover-coefficient-based clustering methodology for text databases</title>
		<author>
			<persName><forename type="first">F</forename><surname>Can</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Ozkarahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans Database Syst (TODS)</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="483" to="517" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Density-based clustering over an evolving data stream with noise</title>
		<author>
			<persName><forename type="first">F</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 SIAM international conference on data mining</title>
		<meeting>the 2006 SIAM international conference on data mining</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="328" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Evolutionary clustering</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tomkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the 12th ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="554" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Density-based clustering for real-time stream data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the 13th ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007. 1281210</date>
			<biblScope unit="page" from="133" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Evolutionary spectral clustering by incorporating temporal smoothness</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tseng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the 13th ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="153" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On evolutionary spectral clustering</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tseng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans Knowl Discov Data</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semantic similarity between search engine queries using temporal correlation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Immorlica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th international conference on World Wide Web</title>
		<meeting>the 14th international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005. 1060752</date>
			<biblScope unit="page" from="2" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An online cellular probabilistic self-organizing map for static and dynamic data sets</title>
		<author>
			<persName><forename type="first">Tws</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sitao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Circuits Syst I Regul Pap</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="732" to="747" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mining frequent patterns in data streams at multiple time granularities</title>
		<author>
			<persName><forename type="first">G</forename><surname>Chris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiaweihan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jianpei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xifeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Sy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Next Gener Data Min</title>
		<imprint>
			<biblScope unit="volume">212</biblScope>
			<biblScope unit="page" from="191" to="212" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">M-tree: An efficient access method for similarity search in metric spaces</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ciaccia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Patella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zezula</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on very large data bases</title>
		<meeting>the 23rd international conference on very large data bases</meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1997">1997. 671005</date>
			<biblScope unit="page" from="426" to="435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Textflow: Towards better understanding of evolving topics in text</title>
		<author>
			<persName><forename type="first">W</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Vis Comput Graph</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2412" to="2421" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">An EM-based algorithm for clustering data streams in sliding windows</title>
		<author>
			<persName><forename type="first">X</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ciptadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture notes in computer science</title>
		<imprint>
			<biblScope unit="volume">5463</biblScope>
			<biblScope unit="page" from="230" to="235" />
			<date type="published" when="2009">2009</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Applications of convex analysis to multidimensional scaling</title>
		<author>
			<persName><forename type="first">J</forename><surname>De Leeuw</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>North Holland Publishing Company</publisher>
			<biblScope unit="page" from="133" to="146" />
			<pubPlace>Amsterdam</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the em algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rubin</forename><surname>Db</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J R Stat Soc Ser B (Methodol)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Mining high-speed data streams</title>
		<author>
			<persName><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hulten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the sixth ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the sixth ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">347107</biblScope>
			<biblScope unit="page" from="71" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A visual backchannel for large-scale events</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gruen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Carpendale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Vis Comput Graph</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1129" to="1138" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Top-down specialization for information and privacy preservation</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">21st international conference on data engineering, 2005. ICDE 2005. Proceedings</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="205" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Mining data streams: a review</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Gaber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zaslavsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Krishnaswamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Sigmod Rec</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="18" to="26" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Data stream mining: the bounded rationality</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Informatica (Slovenia)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="25" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Learning from data streams-processing techniques in sensor networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Gaber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Discretization from data streams: applications to histograms and data mining</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pinto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 ACM symposium on applied computing</title>
		<meeting>the 2006 ACM symposium on applied computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="662" to="667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">An overview on mining data streams</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rodrigues</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Studies in computational intelligence</title>
		<imprint>
			<biblScope unit="volume">206</biblScope>
			<biblScope unit="page" from="29" to="45" />
			<date type="published" when="2009">2009</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">On evaluating stream learning algorithms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sebastião</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rodrigues</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach Learn</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="317" to="346" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Xwave: optimal and approximate extended wavelets</title>
		<author>
			<persName><forename type="first">S</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirtieth international conference on very large data bases</title>
		<meeting>the thirtieth international conference on very large data bases</meeting>
		<imprint>
			<date type="published" when="2004">2004. 1316716</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="288" to="299" />
		</imprint>
		<respStmt>
			<orgName>VLDB Endowment</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Clustering data streams: theory and practice</title>
		<author>
			<persName><forename type="first">S</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Meyerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>'callaghan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Knowl Data Eng</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="515" to="528" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Clustering data streams</title>
		<author>
			<persName><forename type="first">S</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>'callaghan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st annual symposium on foundations of computer science</title>
		<meeting>the 41st annual symposium on foundations of computer science</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="359" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Cure: an efficient clustering algorithm for large databases</title>
		<author>
			<persName><forename type="first">S</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1998 ACM SIGMOD international conference on management of data</title>
		<meeting>the 1998 ACM SIGMOD international conference on management of data</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998">1998. 276312</date>
			<biblScope unit="page" from="73" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Rock: a robust clustering algorithm for categorical attributes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th international conference on data engineering</title>
		<meeting>the 15th international conference on data engineering</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">512</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">R-trees: a dynamic index structure for spatial searching</title>
		<author>
			<persName><forename type="first">A</forename><surname>Guttman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1984 ACM SIGMOD international conference on management of data</title>
		<meeting>the 1984 ACM SIGMOD international conference on management of data</meeting>
		<imprint>
			<date type="published" when="1984">1984</date>
			<biblScope unit="page" from="47" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Temporal structure learning for clustering massive data streams in realtime</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hahsler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dunham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIAM conference on data mining</title>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="664" to="675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Bursty feature representation for clustering text streams</title>
		<author>
			<persName><forename type="first">J ;</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E-P</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIAM international conference on data mining</title>
		<meeting><address><addrLine>San Francisco</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2005">2005. 2007</date>
			<biblScope unit="volume">52</biblScope>
		</imprint>
	</monogr>
	<note>Data mining: concepts and techniques</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Mining time-changing data streams</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hulten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Spencer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the seventh ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the seventh ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001. 502529</date>
			<biblScope unit="page" from="97" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Data clustering: a review</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Murty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Flynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput Surv (CSUR)</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="264" to="323" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Chameleon: hierarchical clustering using dynamic modeling</title>
		<author>
			<persName><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Eui-Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="68" to="75" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Finding groups in data: an introduction to cluster analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rousseeuw</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>Wiley</publisher>
			<biblScope unit="volume">39</biblScope>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">The impact of changing populations on classifier performance</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Hand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fifth ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the fifth ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page">312285</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Bursty and hierarchical structure in streams</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Min Knowl Discov</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="373" to="397" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">The self-organizing map</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kohonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc IEEE</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1464" to="1480" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">The clustree: indexing micro-clusters for anytime stream mining</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kranen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Assent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Baldauf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Seidl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl Inf Syst</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="249" to="272" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">MC-tree: improving Bayesian anytime classification</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kranen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Seidl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture notes in computer science</title>
		<imprint>
			<biblScope unit="page" from="252" to="269" />
			<date type="published" when="2010">2010</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">An effective evaluation measure for clustering on evolving data streams</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kremer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kranen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Seidl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bifet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the 17th ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011. 2020555</date>
			<biblScope unit="page" from="868" to="876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Density based subspace clustering over dynamic data</title>
		<author>
			<persName><forename type="first">H-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kröger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ntoutsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zimek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture notes in computer science</title>
		<imprint>
			<biblScope unit="page" from="387" to="404" />
			<date type="published" when="2011">2011</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Using contextual analysis for news event detection</title>
		<author>
			<persName><forename type="first">W</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yen</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int J Intell Syst</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="525" to="546" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Online classification of nonstationary data streams</title>
		<author>
			<persName><forename type="first">M</forename><surname>Last</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intell Data Anal</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="147" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Feature selection for high-dimensional data: a fast correlation-based filter solution</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Huan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 20th international conference on machine learning</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="856" to="863" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Evolving granular neural network for semi-supervised data stream classification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Leite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gomide</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2010 international joint conference on neural networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Evolving granular classification neural networks</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Leite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gomide</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International joint conference on neural networks</title>
		<imprint>
			<date type="published" when="2009">2009. 2009. 2009</date>
			<biblScope unit="page" from="1736" to="1743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Incremental clustering of dynamic data streams using connectivity based representative points</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lühr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lazarescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Knowl Eng</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Toward integrating feature selection algorithms for classification and clustering</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Knowl Data Eng</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="491" to="502" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">A density-based algorithm for discovering clusters in large spatial databases with noise</title>
		<author>
			<persName><forename type="first">E</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hans-Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jörg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xiaowei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd international conference on knowledge discovery and data mining</title>
		<meeting>the 2nd international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="226" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">A framework for generating data to simulate changing environments</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Muthukrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Narasimhamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">I</forename><surname>Kuncheva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th IASTED international multi-conference: artificial intelligence and applications</title>
		<meeting>the 25th IASTED international multi-conference: artificial intelligence and applications</meeting>
		<imprint>
			<publisher>ACTA Press</publisher>
			<date type="published" when="2003">2003. 2007</date>
			<biblScope unit="volume">549</biblScope>
			<biblScope unit="page">389</biblScope>
		</imprint>
	</monogr>
	<note>Data streams: algorithms and applications</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Heterogeneous ensemble for feature drifts in data streams</title>
		<author>
			<persName><forename type="first">H-L</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y-K</forename><surname>Woon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W-K</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wan</forename><forename type="middle">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Pacific-Asia conference on advances in knowledge discovery and data mining</title>
		<meeting>the 16th Pacific-Asia conference on advances in knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012. 2342648</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Streaming-data algorithms for high-quality clustering</title>
		<author>
			<persName><forename type="first">L</forename><surname>O'callaghan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Meyerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th international conference on data engineering</title>
		<meeting>the 18th international conference on data engineering</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="685" to="694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Online bagging and boosting</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Oza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on systems, man and cybernetics</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005. 2005</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="2340" to="2345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Statistical grid-based clustering over data streams</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGMOD Rec</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="37" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Cell trees: an adaptive synopsis structure for clustering multi-dimensional on-line data streams</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Knowl Eng</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="528" to="549" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Streamed learning: one-pass svms</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Daum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venkatasubramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st international jont conference on artificial intelligence</title>
		<meeting>the 21st international jont conference on artificial intelligence</meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2009">2009. 1661639</date>
			<biblScope unit="page" from="1211" to="1216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Odac: hierarchical clustering of time series data streams</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rodrigues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pedroso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIAM international conference on data mining</title>
		<meeting><address><addrLine>Bethesda, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="499" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Earthquake shakes twitter users: real-time event detection by social sensors</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sakaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Okazaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Matsuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th international conference on World wide web</title>
		<meeting>the 19th international conference on World wide web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="851" to="860" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Adapted one-vs-all decision trees for data stream classification</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sattar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zahra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mohammadreza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Knowl Data Eng</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="624" to="637" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Machine learning in automated text categorization</title>
		<author>
			<persName><forename type="first">F</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput Surv (CSUR)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Indexing density models for incremental learning and anytime classification on data streams</title>
		<author>
			<persName><forename type="first">T</forename><surname>Seidl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Assent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kranen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Krieger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Herrmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th international conference on extending database technology: advances in database technology</title>
		<meeting>the 12th international conference on extending database technology: advances in database technology</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009. 1516397</date>
			<biblScope unit="page" from="311" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Wavecluster: a wavelet-based clustering approach for spatial data in very large databases</title>
		<author>
			<persName><forename type="first">G</forename><surname>Sheikholeslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLDB J</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="289" to="304" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Data stream clustering: a survey</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Faria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Barros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Hruschka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carvalho</forename><surname>Acd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput Surv (CSUR)</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Growing self-organizing map for online continuous clustering</title>
		<author>
			<persName><forename type="first">T</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Alahakoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Studies in computational intelligence</title>
		<imprint>
			<biblScope unit="volume">204</biblScope>
			<biblScope unit="page" from="49" to="83" />
			<date type="published" when="2009">2009</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Body sensor data processing using stream computing</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Biem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blount</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ebling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Verscheure</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the international conference on multimedia information retrieval</title>
		<meeting>the international conference on multimedia information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010. 1743465</date>
			<biblScope unit="page" from="449" to="458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Monic: modeling and monitoring cluster transitions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Spiliopoulou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ntoutsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Theodoridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the 12th ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="706" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">A streaming ensemble algorithm (sea) for large-scale classification</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">N</forename><surname>Street</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the seventh ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the seventh ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001. 502568</date>
			<biblScope unit="page" from="377" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Graphscope: parameter-free mining of large timeevolving graphs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Papadimitriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the 13th ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="687" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Visualising the cluster structure of data streams</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Tasoulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th international conference on intelligent data analysis</title>
		<meeting>the 7th international conference on intelligent data analysis</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007. 1771633</date>
			<biblScope unit="page" from="81" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Hierarchical dirichlet processes</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Beal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Am Stat Assoc</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">476</biblScope>
			<biblScope unit="page" from="1566" to="1581" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Simpler core vector machines with enclosing balls</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kocsor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on machine learning</title>
		<meeting>the 24th international conference on machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007. 1273611</date>
			<biblScope unit="page" from="911" to="918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">E-stream: evolution-based technique for stream clustering</title>
		<author>
			<persName><forename type="first">K</forename><surname>Udommanetanakit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rakthanmanon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Waiyamai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture notes in computer science</title>
		<imprint>
			<biblScope unit="volume">4632</biblScope>
			<biblScope unit="page" from="605" to="615" />
			<date type="published" when="1979">2007. 1979</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin 96. van Rijsbergen C; Butterworths, Massachusetts</pubPlace>
		</imprint>
	</monogr>
	<note>Information retrieval, 2nd edn</note>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Density-based clustering of data streams at multiple resolutions</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">K</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">H</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans Knowl Discov Data (TKDD)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="28" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Mining concept-drifting data streams using ensemble classifiers</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><forename type="middle">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ninth ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the ninth ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">956778</biblScope>
			<biblScope unit="page" from="226" to="235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">Mining data streams</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="777" to="792" />
			<pubPlace>US</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Sting: a statistical information grid approach to spatial data mining</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Muntz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the international conference on very large data bases</title>
		<meeting>the international conference on very large data bases</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="186" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">A study of retrospective and on-line event detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st annual international ACM SIGIR conference on research and development in information retrieval</title>
		<meeting>the 21st annual international ACM SIGIR conference on research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="28" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Topic-conditioned novelty detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><forename type="middle">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighth ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the eighth ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="688" to="693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Learning to cluster web search results</title>
		<author>
			<persName><forename type="first">H-J</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q-C</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W-Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th annual international ACM SIGIR conference on research and development in information retrieval</title>
		<meeting>the 27th annual international ACM SIGIR conference on research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004. 1009030</date>
			<biblScope unit="page" from="210" to="217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Enabling fast lazy learning for data streams</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th IEEE international conference on data mining (ICDM-11)</title>
		<meeting>the 11th IEEE international conference on data mining (ICDM-11)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="932" to="941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Enabling fast prediction for ensemble models on data streams</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the 17th ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011. 2020442</date>
			<biblScope unit="page" from="177" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Robust ensemble learning for mining noisy data streams</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Decis Support Syst</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="469" to="479" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">An aggregate ensemble for mining concept drifting data streams with noise</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture notes in computer science</title>
		<imprint>
			<biblScope unit="volume">5476</biblScope>
			<biblScope unit="page" from="1021" to="1029" />
			<date type="published" when="2009">2009</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Birch: an efficient data clustering method for very large databases</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Livny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1996 ACM SIGMOD international conference on management of data</title>
		<meeting>the 1996 ACM SIGMOD international conference on management of data</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1996">1996. 233324</date>
			<biblScope unit="page" from="103" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Efficient streaming text clustering</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="790" to="798" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Tracking clusters in evolving data streams over sliding windows</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><forename type="middle">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl Inf Syst</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="181" to="214" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
