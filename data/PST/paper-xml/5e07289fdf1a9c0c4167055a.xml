<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-Paced Collaborative and Adversarial Network for Unsupervised Domain Adaptation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Weichen</forename><surname>Zhang</surname></persName>
							<email>weichen.zhang@sydney.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Weichen Zhang</orgName>
								<orgName type="department" key="dep2">Dong Xu and Wanli Ouyang are with the School of Electrical and Information Engineering</orgName>
								<orgName type="institution" key="instit1">The University of Sydney</orgName>
								<orgName type="institution" key="instit2">NSW</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE, Wanli Ouyang, Senior Member, IEEE</roleName><forename type="first">Dong</forename><surname>Xu</surname></persName>
							<email>dong.xu@sydney.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Weichen Zhang</orgName>
								<orgName type="department" key="dep2">Dong Xu and Wanli Ouyang are with the School of Electrical and Information Engineering</orgName>
								<orgName type="institution" key="instit1">The University of Sydney</orgName>
								<orgName type="institution" key="instit2">NSW</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wen</forename><surname>Li</surname></persName>
							<email>liwen@vison.ee.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Weichen Zhang</orgName>
								<orgName type="department" key="dep2">Dong Xu and Wanli Ouyang are with the School of Electrical and Information Engineering</orgName>
								<orgName type="institution" key="instit1">The University of Sydney</orgName>
								<orgName type="institution" key="instit2">NSW</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Computer Vision Laboratory</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<region>Switzerlan</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Self-Paced Collaborative and Adversarial Network for Unsupervised Domain Adaptation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7A57659A4CB0FE3A9B8E93E3CFB62000</idno>
					<idno type="DOI">10.1109/TPAMI.2019.2962476</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2019.2962476, IEEE Transactions on Pattern Analysis and Machine Intelligence received XX XX 2018 …… Input Image Domain Specific Feature Domain Invariant Feature Domain Collaborative Learning Domain Adversarial Learning This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2019.2962476, IEEE Transactions on Pattern Analysis and Machine Intelligence</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>domain adaptation</term>
					<term>transfer learning</term>
					<term>deep learning</term>
					<term>adversarial learning</term>
					<term>self-paced learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper proposes a new unsupervised domain adaptation approach called Collaborative and Adversarial Network (CAN), which uses the domain-collaborative and domain-adversarial learning strategy for training the neural network. The domain-collaborative learning strategy aims to learn domain specific feature representation to preserve the discriminability for the target domain, while the domain adversarial learning strategy aims to learn domain invariant feature representation to reduce the domain distribution mismatch between the source and target domains. We show that these two learning strategies can be uniformly formulated as domain classifier learning with positive or negative weights on the losses. We then design a collaborative and adversarial training scheme, which automatically learns domain specific representations from lower blocks in CNNs through collaborative learning and domain invariant representations from higher blocks through adversarial learning. Moreover, to further enhance the discriminability in the target domain, we propose Self-Paced CAN (SPCAN), which progressively selects pseudo-labeled target samples for re-training the classifiers. We employ a self-paced learning strategy such that we can select the highly confident pseudo-labeled target samples as easy samples in the early iterations and lower-confident harder samples at the subsequent iterations. Additionally, we build upon the popular two stream approach and extend our domain adaptation approach for more challenging video action recognition task, which additionally considers the cooperation between the RGB stream and the optical flow stream. We propose the Cooperative SPCAN (CoSPCAN) method to select and reweigh the pseudo labeled target samples of one stream (RGB/Flow) based on the information from another stream (Flow/RGB) in a cooperative way. As a result, our CoSPCAN model is able to exchange the information between the two streams. Comprehensive experiments on different benchmark datasets, Office-31, ImageCLEF-DA and VISDA-2017 for the object recognition task, and UCF101-10 and HMDB51-10 for the video action recognition task, show our newly proposed approach achieves the state-of-the-art performance, which clearly demonstrates the effectiveness of our proposed approaches for unsupervised domain adaptation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>I N many visual recognition tasks, the training data and the testing data often have different distributions. In order to enhance the generalization capability of the models learnt from training data to the testing data, many domain adaptation technologies were proposed <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref> for different visual tasks, such as object recognition, video event recognition, and semantic segmentation by explicitly reducing the data distribution mismatch between the training samples in the source domain and the testing samples in the target domain.</p><p>The advancement of deep learning methods <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> have driven significant progress in a wide variety of computer vision tasks. However, modern deep learning methods are often based on the availability of large volumes of labeled data. To overcome this data limitation difficulty, deep transfer learning methods <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref> are commonly used to learn domain invariant features by exploiting the knowledge from the source domain to the target domain, in which the labeled samples are limited or even do not Fig. <ref type="figure" target="#fig_2">1</ref>: Motivation of our CAN. We aim to learn domain specific features at lower layers and domain invariant features at higher layers, such that the learnt feature representation is domain invariant and the discriminability of features can also be well preserved.</p><p>exist. Recently, a few deep transferring methods were proposed, which achieve promising results. These deep transfer learning approaches can be roughly categorized as statistical approaches <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b26">[27]</ref>, which exploit regularizers, such as Maximum Mean Discrepancy (MMD) <ref type="bibr" target="#b27">[28]</ref>, and adversarial learning based approaches <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, which learn new representations through the adversarial learning processes. To learn domain-invariant representations for different domains, the recent work Domain Adversarial Training of Neural Network (DANN) <ref type="bibr" target="#b18">[19]</ref> added a domain classifier and used reversed gradient layer to update the feature extractor shared with the image/video classifier. Please refer to Section 2 for a brief review of the existing domain adaptation approaches.</p><p>In this paper, we propose new unsupervised domain adaptation methods, in which all the source domain samples are fully annotated and none of the target domain sample is annotated. We apply our approaches in two common visual tasks, object recognition in images and action recognition in videos.</p><p>For the object recognition task, we propose a new deep transfer learning method called Collaborative and Adversarial Network (CAN), which integrates a set of domain classifiers (also called as domain discriminators) into multiple blocks. Each block consists of several CNN layers and each domain classifier is connected to one block. As shown in Fig. <ref type="figure" target="#fig_2">1</ref>, our method CAN is based on the motivation that some characteristic information from target domain data may be lost after learning domain-invariant features with the recent method DANN <ref type="bibr" target="#b18">[19]</ref>. Meanwhile, the representations at lower blocks (shallow layers) are often low-level features, such as corners and edges, which are expected to be specific for distinguishing images from different domains. As the result, we use collaborative and adversarial learning to learn discriminant features including domain-specific features in shallow layers and domain-invariant features in deep layers to reduce the data distribution mismatch. Inspired by Self-Paced learning <ref type="bibr" target="#b28">[29]</ref>, we additionally propose SPCAN based on an easy-to-hard progressive learning scheme. SPCAN selects pseudo-labeled target samples starting from easy pseudo-labeled target samples with high confidence scores and gradually selects more pseudo-labeled target samples with higher confidence scores. SPCAN aims at, (1) gradually improving the image classification accuracy; and (2) gradually improving the domain classifier and reducing the data distribution mismatch between the source and target domains. For the first goal (see Fig. <ref type="figure" target="#fig_0">2(a)</ref>), the features and image/video classifier of SPCAN are additionally trained by adding a small amount of pseudo-labeled target sample with higher confidence scores at the beginning, and gradually adding harder samples with lower confidence scores in the subsequent iterations. For the second goal, as shown in Fig. <ref type="figure" target="#fig_0">2</ref>(b), domain classifiers are gradually learnt from easy samples to harder ones, which can additionally improve the performance. In addition, by reversing the gradients of the easily-classified samples, the features of these samples would become more domain-invariant and harder to be classified by the domain classifiers. The two processes of SPCAN are performed alternatively.</p><p>For the action recognition task, we additionally propose CoSPCAN to learn better video-level feature representations under the unsupervised domain adaptation setting. We utilize the temporal segment network(TSN) <ref type="bibr" target="#b29">[30]</ref> as the base network, which fuses the RGB stream and the optical flow stream to predict the action classes. We propose a twostream domain adaptation network, Cooperative SPCAN (CoSPCAN), for both RGB and optical flow streams. Due to complementary representations in the two streams, in CoSPCAN, the more reliable pseudo-labeled target samples of one stream (RGB/Flow) are used for another stream (Flow/RGB), which helps both streams to select more reliable pseudo-labeled target samples and substantially boost the domain adaptation results.</p><p>Comprehensive experiments on several benchmark datasets Office-31, ImageCLEF-DA and VISDA-2017 for the A preliminary version of this paper was presented in <ref type="bibr" target="#b30">[31]</ref>. This paper extends the work in <ref type="bibr" target="#b30">[31]</ref> by additionally proposing two new methods the SPCAN and CoSPCAN for domain adaptation as well as new experiments to evaluate the effectiveness of SPCAN and CoSPCAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Domain Adaptation</head><p>Classical domain adaptation methods can be roughly categorized as feature (transform) based approaches <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, which aims to seek new domain-invariant features or learn new feature transforms for domain adaptation, and classifier based approaches <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, which directly learn the target classifiers (e.g., the SVM based classifiers) for domain adaptation.</p><p>Moreover, several deep transfer learning methods were proposed recently based on the convolutional neural net-works (CNNs), which can be roughly categorized as statistic-based approaches <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref> and adversarial learning based approaches <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref>. The statistic-based approaches usually employ statisticbased metrics to model the domain difference. For example, Deep Correlation Alignment(CORAL) <ref type="bibr" target="#b21">[22]</ref> utilized the mean and variance of the different domains, while Deep Adaptation Network <ref type="bibr" target="#b14">[15]</ref> adopt the MMD <ref type="bibr" target="#b27">[28]</ref> as the domain distance. On the other hand, the adversarial learning based approaches used Generative Adversarial Networks (GANs) <ref type="bibr" target="#b50">[51]</ref> to learn domain-invariant representation, which can be seen as minimizing the H-divergence <ref type="bibr" target="#b51">[52]</ref> or the Jensen-Shannon divergence <ref type="bibr" target="#b52">[53]</ref> between two domains.</p><p>Our work is more related to adversarial learning based approaches. The adversarial learning strategy has been widely used in domain adaptation for learning domain invariant representation. For example, the works in <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref> introduced different domain loss terms to learn domaininvariant representation by adding additional classifiers or using the adversarial learning strategy. As a result, it is difficult to distinguish which domain each sample belongs to. Besides learning domain invariant representation, a few works have been proposed to preserve certain representation differences between different domains for better extracting features in the target domain. In particular, Domain Separation Network (DSN) <ref type="bibr" target="#b22">[23]</ref> introduced one shared encoder and two private encoders for two domains by using the similarity loss and the difference loss. As a result, the shared and private representation components are pushed apart, whereas the shared representation components are enforced to be similar. The works in <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref> used the two-stream network for the source and target domains, in which two different regularization terms are introduced to learn the domain-invariant representation, respectively. The approach in <ref type="bibr" target="#b46">[47]</ref> proposed an image-to-image translation framework to disentangle the domain-invariant features and the domain-specific features (attributes) and then use both types of features to translate the labelled images from the source domain to the target domain for training the classifier for domain adaptation. Different from these works, we propose a new collaborative and adversarial learning strategy to not only learn more domain-invariant representations in deeper layers through domain adversarial learning, but also learn domain-specific representations in shallower layers through domain collaborative learning. We used a shared model for both source and target domains and automatically learn the domain-specific and domaininvariant features by using different domain classifiers and learning the corresponding weights for different layers. In this way, the discriminability of the low-level features can be well preserved, while the common high-level semantics features can also be well extracted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Progressive Training for Domain Adaptation</head><p>A few works have also proposed to progressively select pseudo-labeled target samples for improving unsupervised domain adaptation. In <ref type="bibr" target="#b36">[37]</ref>, Bruzzone et al. proposed the Domain Adaptation Support Vector Machine (DASVM) method to iteratively select the unlabeled target domain data while simultaneously remove some labeled source samples. Chen et al. <ref type="bibr" target="#b53">[54]</ref> proposed a features selection approach to split the target features into two views, and use cotraining <ref type="bibr" target="#b54">[55]</ref> to leverage target domain pseudo-labeled samples in a semi-supervised learning fashion. More recently, Saito et al. <ref type="bibr" target="#b55">[56]</ref>, equally leveraged three classifiers with an asymmetric tri-training network, where they selected pseudo-labeled target samples by the agreement of the two network classifiers, and then use these samples to retrain the third classifier to learn the target-discriminative features. In our preliminary work, we propose a method called iCAN <ref type="bibr" target="#b30">[31]</ref> in which the domain classifier and the image/video classifier are both used to guide selecting more reliable samples. However, due to the large domain difference, the progressive training procedure may not work well when the labels of the selected pseudo-labeled samples are wrongly predicted.</p><p>Different from those works, in this work, we propose a SPCAN model inspired by the Curriculum Learning (CL) <ref type="bibr" target="#b56">[57]</ref> and Self-Paced Learning <ref type="bibr" target="#b28">[29]</ref> to select target pseudolabeled samples in an easy to hard mode, thus improving the stability of progressive training. There were several works incorporated self-paced learning for object detection and semantic segmentation <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b59">[60]</ref>, however, we aim to solve the general unsupervised domain adaptation task with a focus on object and action recognition, and correspondingly design a new self-paced sample selection module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Multi-View Domain Adaptation</head><p>Recently, a few multi-view domain adaptation methods are proposed due to the increased amount of distinct features, viewpoints and sensors for different visual applications. In <ref type="bibr" target="#b60">[61]</ref>, zhang et al. proposed a tranfer learning framework called Multi-View Transfer Learning with a Large Margin Approach (MVTL-LM) to integrate features of the source domain and the target domains from different views. The work in <ref type="bibr" target="#b61">[62]</ref> incorporated multiple views of data in a perceptive transfer learning framework and proposed a domain adaptation approach called Multi-view Discriminant Transfer (MDT) to find the optimal discriminant weight vectors for each view by maximizing the correlation of the vector projections of two-view data and minimising the domain discrepancy and view disagreement simultaneously. Niu et al. proposed a multi-view domain generalization (MVDG) method <ref type="bibr" target="#b62">[63]</ref>, which used exemplar SVM and alternating optimization algorithm to exploit the manifold structure of unlabeled target domain samples for domain adaptation. The work in <ref type="bibr" target="#b63">[64]</ref> formulated a unified multi-view domain adaptation framework and conducted a comprehensive discussion across both multi-view and domain adaptation problems. In contrast to these works, our newly proposed twostream unsupervised domain adaptation approach CoSP-CAN focuses on progressively exchanging complementary information between the RGB and optical flow streams for the action recognition task under the unsupervised domain adaptation setting in an end-to-end training fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Domain Adaptation for Action Recognition</head><p>Convolutional Neural Networks(CNNs) have been extensively exploited for the video-based action recognition task <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b64">[65]</ref>, <ref type="bibr" target="#b65">[66]</ref>, <ref type="bibr" target="#b66">[67]</ref>, <ref type="bibr" target="#b67">[68]</ref>, <ref type="bibr" target="#b68">[69]</ref>, <ref type="bibr" target="#b69">[70]</ref>, <ref type="bibr" target="#b70">[71]</ref>, <ref type="bibr" target="#b71">[72]</ref>. Recently, a deep learning based approach called Temporal Segment Networks(TSN) <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b71">[72]</ref> has been proposed to learn the video features based on the two stream framework (RGB and Optical flow), which used the sparse sampling strategy and fused several action prediction scores for each stream.</p><p>The problem of domain shift between videos from different datasets is rarely explored. Recently, several transfer learning methods <ref type="bibr" target="#b72">[73]</ref>, <ref type="bibr" target="#b73">[74]</ref>, <ref type="bibr" target="#b74">[75]</ref>, <ref type="bibr" target="#b75">[76]</ref> have been applied for the action recognition tasks. Niu et al. <ref type="bibr" target="#b72">[73]</ref> has proposed a multiple-domain adaptation method for video action recognition by leveraging a large number of web images from different sources. HiGAN <ref type="bibr" target="#b73">[74]</ref> used the two-level adversarial learning approach to find domain-invariant feature representation from source images and target action videos. Jamal et al. <ref type="bibr" target="#b74">[75]</ref> presented a domain adaptation method by combining adversarial learning for extracting action features under a 3D CNN network. In <ref type="bibr" target="#b75">[76]</ref>, Busto et al. proposed a domain adaptation framework for the action recognition task, where the videos in the target domain contains instances of categories that are not present in the source domain. However, they mainly treat features of a video as a single representation, while our CoSPCAN incorporates co-training into our Self-Paced CAN to take advantage of the complementary information of RGB and optical flow streams in an iterative fashion, which boosts the adaptation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">COLLABORATIVE AND ADVERSARIAL NETWORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Unsupervised domain adaptation in Object Recognition</head><p>In unsupervised domain adaptation, we are given a set of labeled source samples and a set of unlabeled target samples, where the source and target data are different in distribution. The goal of the task is to learn a classifier which can accurately categorize the unlabeled target samples into the given classes.</p><p>Formally, denote X s = {(x s i , y s i )| N s i=1 } as the set of images from the source domain, where x s i is the i-th source domain image, y s i is its category label, and N s is the number of images in the source domain. Similarly, we denote X t = {x t i | N t i=1 } as the set of images from target domain, where x t i is the i-th target domain image, and N t is the number of images in the target domain. For convenience, we also use X</p><formula xml:id="formula_0">= X s ∪ X t = {(x i , d i )| N</formula><p>i=1 } to denote the training images from both domains, where N is the total number of images, d i ∈ {0, 1} is the domain label for the i-th image with d i = 1 as the source domain, and d i = 0 as the target domain.</p><p>There are many deep domain adaption approaches, which learn the domain invariant representations for the two domains <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b18">[19]</ref>. Usually, their methods are employed with a multi-task learning scheme, which aim to minimize the classification loss on the labeled source data, and also align the source and target domain distribution with a domain discrepancy loss (e.g., H-divergence or MMD).</p><p>However, since the network aims to minimize the source domain classification loss, domain specific features that are discriminative for image classification might be lost in training process, making the learnt representation less discriminative for classifying images in the target domain. To this end, we propose a new Collaborative and Adversarial Network (CAN) model, which simultaneously learns domain-specific and domain-invariant features through domain collaborative and domain adversarial learning. Empirically, the final feature representation learnt by the CAN model is more discriminative for image classification. Our new CAN model is described step by step with three parts as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Domain Specific Feature Learning</head><p>Domain specific features are useful for distinguishing which domain each sample belongs to. In a general deep neural network, features learnt from lower layers are mostly lowlevel details, e.g. corners and edges, and features learnt in higher layers are semantic features that describe the objects or attributes. The low-level features are often domain specific that are useful for distinguishing not only images from different domains but also images from different classes.</p><p>Therefore, different from the other method <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b18">[19]</ref> which learns domain invariant features for the whole network, CAN makes these low-level features distinguishable from different domains. To keep the domain specific features when training the whole network, we propose to incorporate several domain classifiers, where each of the domain classifiers is applied on each block (including several CNN layers).</p><p>In particular, given an image x, let us denote its feature representation extracted from a certain layer (e.g., "conv3") as f . We also denote the feature extraction network before this layer (inclusive) as F , then f can be deemed as the output of F ,( i.e., f = F (x; θ)), where θ are the parameters for F .</p><p>To let f encode as much target information as possible, we propose to learn a domain classifier D : f → 0, 1, which is used to predict whether the input image x belongs to source domain <ref type="bibr" target="#b0">(1)</ref> or target domain (0). Intuitively, if the feature representation f can be used to well distinguish which domain x comes from, sufficient target information should be encoded within f . Then, the objective for learning domain classifier D can be written as,</p><formula xml:id="formula_1">min θ,w 1 N N i=1 L D (D(F (x i ; θ); w), d i ) , (<label>1</label></formula><formula xml:id="formula_2">)</formula><p>where w is the parameters for the domain classifier D, d i is the domain label, and L D is the classification loss, which the binary cross entropy loss is used for this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Domain Invariant Feature Learning</head><p>domain invariant features cannot clearly distinguish whether the sample is from the source or the target domain. To learn the domain invariant representations at the final layer for image classification, other deep transfer approaches <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b18">[19]</ref> designed various domain alignment approaches to handle this problem. Our work uses domain adversarial learning, which learns a domain classifier and reversely back-propagate the domain gradient to confuse the domain classifier through a Gradient Reversal Layer <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. In this way, the shared features learnt in the  Fig. <ref type="figure">3</ref>: The pipeline of our SPCAN method for image classification, which can also be similarly used for video classification. This pipeline consists of the CAN module (a) and the sample selection module (b). For the CAN module, we add multiple domain classifiers at different blocks of the CNN structure. We then train the domain classifiers by learning positive or negative weights on the losses to automatically extract domain-specific and domain-invariant features. As a result, the features learnt through domain collaborative learning in the lower layers will become more domain specific, while the features learnt through domain adversarial learning in the higher layers will become more domain invariant. For our sample selection module, based on the image classifier, we gradually select the set S c from fewer target samples to more target samples with higher confidence scores; while based on the domain classifier, we gradually select the set S d from fewer target samples to more target samples that are likely from the target domain. We use both selected sets S c and S d to retrain the CAN model in the next training epoch. As a result, the model gradually learns more discriminant features for image classification as well as aligning the distributions of two domains. domain adversarial way is able to reduce the distribution mismatch between the source and target domain. With this domain adversarial learning, the learnt image/video classifier performs better on the target domain.</p><p>Specifically, by following the domain adversarial training strategy in DANN <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, the objective for learning the domain invariant representations can be written as</p><formula xml:id="formula_3">max θ min w 1 N N i=1 L D (D(F (x i ; θ); w), d i ) ,<label>(2)</label></formula><p>where the difference between the above equation and Eqn.</p><p>(1) is that the network F is trained by minimizing the domain classification loss L D in Eqn. (1) for domain specific features, while F is trained by maximizing L D in Eqn. (2) for domain invariant features. Thus, the network can be easily trained by minimizing the loss with the conventional optimization techniques like the stochastic gradient descent method.</p><p>Intuitively, optimizing Eqn. ( <ref type="formula" target="#formula_1">1</ref>) is to distinguish the two domains, such that the feature representation generated from F is domain specific or domain specific. Instead, optimizing Eqn. (2) tends to remove domain specific information, such that two domains are similar to each other using the feature representations generated from F . Thus, optimizing Eqn. (2) inevitably causes loss of target specific features, since such representation does not help to confuse the discriminator. To learn domain specific representation at lower layers and domain invariant representation at higher layers, we propose a collaborative and adversarial learning scheme below, which accommodates the opposite tasks in Eqn. (1) and Eqn. ( <ref type="formula" target="#formula_3">2</ref>) into one framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Collaborative and Adversarial Feature Learning Scheme</head><p>In the collaborative and adversarial learning scheme, we encourage the feature representation of the model to keep as much domain specific feature as possible in lower layers, while enforce the feature representation to be domain invariant in higher layers. The two tasks are applied to multiple layers with different weights, such that feature representation goes smoothly from domain informative to invariant when the samples are forwarded in the network from lower layers to higher layers.</p><p>The architecture of our CAN model is illustrated in the left part of Fig 3 . We divide the whole CNN network into several groups, which are called blocks (i.e., the blue cubes). Each block consists of several consecutive CNN layers. Suppose in total m blocks are used, we build a domain discriminator after the last layer of each block, leading to m domain discriminators (i.e., the green rectangles). We assign a weight λ l (l = 1, . . . , m) for each domain discriminator, and automatically optimize the weights when we backpropagate the losses. Intuitively, a higher λ l indicates the network tends to learn domain specific features at the l-th block. When λ l &lt; 0, the network tends to learn domain invariant features at the l-th block.</p><p>For example, for a ResNet50 network, we divide the base model into four blocks, and add four discriminators after each block, where the sizes of the receptive fields are changed. Then, we add a domain discriminator and its corresponding weight to each of the block to learn domain specific and invariant features through the domain collaborative and adversarial learning.</p><p>Formally, denote θ l (l = 1, . . . , m) as the network parameters before the l-th block (inclusive), and denote w l as the parameters of the domain discriminator at the l-th block. For m blocks, we denote W = {w 1 , . . . , w m }, Θ F = {θ 1 , . . . , θ m }. We also denote the loss term from a domain discriminator as</p><formula xml:id="formula_4">L D (θ, w) = 1 N N i=1 L D (D(F (x i ; θ); w), d i ).</formula><p>The objective of the collaborative and adversarial learning scheme can be written as follows:</p><formula xml:id="formula_5">min Θ F ,λ L CA = m l=1 λ l min W L D (θ l , w l ),<label>(3)</label></formula><formula xml:id="formula_6">s.t. m l=1 λ l = -1, |λ l | ≤ 1.</formula><p>where the set of the domain discriminator weight λ = {λ 1 , . . . , λ m }. DANN <ref type="bibr" target="#b18">[19]</ref> only used one domain classifier and employed a gradient reversal layer to change the sign of the gradient back-propagated from the domain classifier (i.e., by multiplying by -1). In this work, we extend the DANN method to use multiple domain classifiers at different blocks, and automatically learn the weights for different domain classifiers. In order to be consistent with DANN <ref type="bibr" target="#b18">[19]</ref>, we set the sum of all weights λ l 's to be -1.</p><p>Automatically optimizing the loss weights λ l 's reduces the number of hyper-parameters, and more importantly, also allows multiple specific feature learning tasks to well collaborate with each other. When λ l ≥ 0, the corresponding sub-problem is similar to the optimization problem in Eqn.</p><p>(1), so we disable the gradient reverse layer, and encourage the corresponding discriminator to learn domain specific features as performing the Collaborative learning. On the other hand, when λ l &lt; 0, the corresponding sub-problem is similar to the max-min problem in Eqn. ( <ref type="formula" target="#formula_3">2</ref>), so we enable the gradient reverse layer, and encourage the discriminator to learn domain invariant features as performing the Adversarial learning. We can incorporate the loss L CA in (3) into any popular deep convolutional neural networks (CNNs) architecture (e.g., AlexNet, VGG, ResNet, DenseNet, etc) to learn robust features for unsupervised domain adaptation. Therefore, we jointly optimize the above loss with the conventional classification loss. Let a image classifier be C : f → ỹi . The image classification loss can be denoted as</p><formula xml:id="formula_7">L src = 1 N s N s i=1 L C (C(F (x s i ; Θ F ); c), y s i ), (<label>4</label></formula><formula xml:id="formula_8">)</formula><p>where c is the parameters for the classifier C and L C is the cross entropy loss for the classification task. The final objective for our collaborative and adversarial network (CAN) can be written as</p><formula xml:id="formula_9">min Θ F ,c,W,λ l ∈Λ L CAN = λL CA + L src ,<label>(5)</label></formula><p>where</p><formula xml:id="formula_10">Λ = {λ l | m l=1 λ l = λ 0 , |λ l | ≤ λ 0 , l = 1, .</formula><p>. . , m} is the feasible set of λ l 's. While we set l λ l = -1 (see the constraint in Eqn. ( <ref type="formula" target="#formula_5">3</ref>)), we additionally introduce the tradeoff parameter λ in order to provide more flexibility.</p><p>As discussed in <ref type="bibr" target="#b18">[19]</ref>, the domain adversarial training in Eqn. ( <ref type="formula" target="#formula_3">2</ref>) can be seen as to minimize the H-divergence between two domains. Formally, given two domain distributions D s of X s and D t of X t , and a hypothesis class H(a set of binary classifiers h : X → [0, 1]), the H-divergence for the two distributions d H (D s , D t ) is then defined as <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b76">[77]</ref>,</p><formula xml:id="formula_11">d H (D s , D t ) = 2 sup h∈H P x∼D s [h(x) = 1] -P x∼D t [h(x) = 1] .</formula><p>It has been shown in <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b76">[77]</ref> that when H is a symmetric hypothesis class, the H-divergence can be empirically computed by,</p><formula xml:id="formula_12">dH (X s , X t ) = 2 1 -min h∈H 1 N s N s i=1 I h(x s i ) = 0 + 1 N t N t i=1 I h(x t i ) = 1 ,<label>(6)</label></formula><p>where I(a) is the binary indicator function. It can be easily observed that it is equivalent to optimize the objective in Eqn. ( <ref type="formula" target="#formula_3">2</ref>) or find a hypothesis h that minimizes dH (X s , X t ).</p><p>The DANN <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref> approach is designed to simultaneously minimize the classification error on the labeled samples in the source domain (i.e., Eqn. ( <ref type="formula" target="#formula_7">4</ref>)), and the empirical H-divergence between two domains (i.e., Eqn. ( <ref type="formula" target="#formula_3">2</ref>)) when training neural networks. To achieve this, DANN adds a domain discriminator at the final layer, and reversely backpropagates the gradient learnt from the domain discriminator to make the feature extracted from the final layer become domain indistinguishable.</p><p>Similarly, in our CAN, we incorporate a domain discriminator at each layer, and minimize the domain discrepancies measured at all layers. Therefore, the DANN approach can be seen as a special case of our CAN when we only have one domain discriminator at the final layer. By jointly optimizing domain discriminators at different levels with the constrained weight, we obtain a lower bound to the Hdivergence, which is described as follows, Proposition 1. The optimum of Eqn. (3) is smaller than or equal to the optimum of Eqn. <ref type="bibr" target="#b1">(2)</ref>.</p><p>Proof. The objective in Eqn. (2) can be seen as a special case of Eqn. (3) when setting λ m = -1, and λ l = 0 for l = 1, . . . , m -1. Thus, by optimizing Eqn. (3), we expect to obtain a lower minimum than that of Eqn. (2). We complete the proof.</p><p>By imposing discriminators at different layers, and jointly optimizing all discriminators and their ensemble weights, we expect to achieve a lower domain discrepancy than DANN, thus we can better reduce the distribution mismatch between two domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SELF-PACED CAN (SPCAN)</head><p>To effectively use the unlabeled target samples for learning better representations, in this section, we further propose a sample selection strategy to progressively select a set of pseudo-labeled target samples for training the model. A simple strategy based on thresholding was proposed in our preliminary work <ref type="bibr" target="#b30">[31]</ref>. Inspired by the self-paced learning <ref type="bibr" target="#b28">[29]</ref>, we present a more effective approach to select pseudolabeled samples, referred to as Self-Paced CAN (SPCAN).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Unsupervised Domain Adaptation with Self-Paced Learning</head><p>Self-paced learning was originally designed for the supervised learning task. Let us denote by X = {(x i , y i )| N i=1 } the training data, where x i is the i-th training sample, y i is its corresponding label, and N is the total number of samples. We also denote by g(x i , w sp ) the prediction function with w sp being the parameters to be learnt. Let L(y i , g(x i , w sp )) be the loss of the i-th sample, the goal of self-paced learning is to jointly learn the parameter w sp and the latent weight variable s = {s i | N i=1 } for each sample by minimizing the objective function:</p><formula xml:id="formula_13">min wsp∈W,s∈[0,1] n N i=1 s i L(y i , g(x i , w sp )) - N i=1 s i T sp ,<label>(7)</label></formula><p>where T sp is the threshold (i.e. age parameter) that controls the learning pace, and -N i=1 s i T sp is the self-paced regularizer.</p><p>Using the alternating convex optimization strategy <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b77">[78]</ref>, for fixed w sp , the solution s * = {s * i | N i=1 } can be easily calculated as,</p><formula xml:id="formula_14">s * i = 1, L(y i , g(x i , w sp )) &lt; T sp . 0, otherwise.<label>(8)</label></formula><p>Intuitively, when updating the weight variable s with fixed w sp , a sample is selected (s * i = 1) when its loss is smaller than the threshold age parameter T sp , in which the selected sample is also regarded as an "easy" sample for small T sp . We will gradually select more samples from easy to hard, by gradually increasing the age parameter T sp .</p><p>In unsupervised domain adaptation, the goal is to train a model for the target domain. Thus the easiness of a training sample should be dependent on two aspects: 1) easy for classification, which shares the same spirit as self-paced learning; 2) easy for adaptation, which means the samples should be confidently discriminated to their corresponding domain, and further reversed their gradients to correctly update the feature extractor. From the above motivations, we respectively design a self-paced sample selection module based on the image classifier and the domain classifier, and integrate the selected pseudo-labeled target samples for boosting the model performance. The detail of the two selection schemes are provided below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Image Classifier Based Sample Selection (CSS)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Image Classification Confidence Score</head><p>We follow the idea of self-paced learning to gradually select easy target samples determined by the image classifier, which is referred to as the image classifier based sample selection (CSS) module. However, the major challenge in unsupervised domain adaptation is that we have only unlabeled samples in the target domain. Since ground-truth labels for these unlabeled samples are absent, we cannot measure the easiness of these samples based on classification loss as in the original self-pace learning work <ref type="bibr" target="#b28">[29]</ref>. Thus, we instead determine the easiness of target samples according to their class classification confidence scores.</p><p>Specifically, let us denote {p cl (x t i )| N cl cl=1 } as the output from the softmax layer of the , in which each p cl (x t i ) is the probability that x t i belongs to the cl-th category, and N cl is the total number of categories. The pseudo-label ỹt i of x t i can be obtained by choosing the category with the highest probability, i.e., ỹt i = arg max cl p cl (x t i ). We refer to the probability p ỹt i (x t i ) as the class classification confidence score. Intuitively, the higher the classification confidence score is, the more likely the target sample is correctly predicted, in other words, we suppose that the sample would be relatively easy for classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Selection Strategy</head><p>Considering that the predicted pseudo-labels for some target samples may be incorrect, we only select the highly confident samples to retrain our model for better performance. In our preliminary conference paper <ref type="bibr" target="#b30">[31]</ref>, we presented a selection strategy based on thresholding during the whole training process. However, prediction of CNN model often fluctuates during the training procedure due to usage of mini-batches. It becomes even more obvious when additionally incorporating the adversarial training in unsupervised domain adaptation tasks. Nevertheless, we note that "easy" samples and difficult samples are actually decided relatively. Thus, we turn to select easiest samples instead with a ranking strategy.</p><p>Specifically, at each epoch, we use the image classifier to predict all the target samples. Then the classification confidence scores are used for sorting the target samples. After the sorting procedure, we select r c % pseudo-labeled target samples with the highest classification confidence scores, where r c ∈ [0, 100] is a proportion parameter updated at each epoch to control the learning pace. r c plays a similar role to the threshold age parameter T sp as in self-paced learning. To ensure the quality of selected pseudo-labeled samples, we dynamically adjust the proportion parameter r c based on the classification accuracy of learnt model, which is validated by using the source domain samples. When the classification accuracy drops in two consecutive epochs, the proportion parameter r c is decreased to prevent model collapse by using low quality pseudo-labeled target samples. Otherwise, r c would be increased.</p><p>Formally, let A i be the classification accuracy on source samples at the i-th epoch, and Āi be the average accuracy of the first i epochs (i.e., Āi = 1 i i j=1 A j ). For the total T training epochs, the proportion parameter r c at the e-th epoch is then adjusted according to,</p><formula xml:id="formula_15">r c = e i=1 η i T , η i = -1, if A i &lt; Āi and A i-1 &lt; Āi-1 , 1, otherwise.<label>(9)</label></formula><p>Let us denote S c as the set of selected pseudo-labeled target samples by using the classification confidence score. Then, after selecting pseudo-labeled target samples, we add them into the training set, and re-train the image classifier in next epoch. Moreover, considering the low-confident samples are more likely to have incorrect pseudo labels, we assign different weights to different target samples according to their classification probabilities, i.e., w c (x t i ) = p ỹt i (x t i ). For convenience of presentation, let us define an indicator function s c (x t i ) which produces 1 if x t i is selected (i.e., x t i ∈ S c ) and 0 otherwise. Then, the loss of image classifier for the selected pseudo-labeled target samples can be written as,</p><formula xml:id="formula_16">L c tar = 1 N t N t i=1 s c (x t i )w c (x t i )L C (C(F (x t i ; Θ F ); c), ỹt i ),<label>(10)</label></formula><p>where the pseudo-label ỹt i is treated as the label of the sample x t i for computing the loss L C ( * ) and learning the parameters Θ F .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Domain Classifier Based Sample Selection (DSS)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Domain Classification Confidence Score</head><p>We also gradually improve the domain classifiers by retraining the model with the selected target samples based on the domain classification confidence scores in an easy to hard manner, which is referred to as domain classifier based sample selection (DSS) module.</p><p>For a target sample x t i , let us denote d(x t i ) as the predicted probability from the last domain classifier trained in our CAN model, where the domain label is equal to 1 when the sample is from the source domain, and 0 when the sample is from the target domain. We define 1 -d(x t i ) as the domain classification confidence score. If the domain classification confidence score is higher, it is easier for the target domain sample to be classified by the domain classifier, which should be selected and reweighed with higher weights when learning the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Selection Strategy</head><p>Similarly to our approach for the image classifier, we also define a proportion parameter r d for selecting samples with highest domain classification confidence scores. The proportion parameter r d is gradually increased at each epoch to include more samples for learning the domain classifier. At the e-th epoch, r d is adjusted as,</p><formula xml:id="formula_17">r d = e T ,<label>(11)</label></formula><p>where T is the total number of epochs.</p><p>Let us denote S d as the set of target samples selected by using their domain classification confidence scores. We also apply the weights on the selected target samples based on their predicted domain probabilities, which is defined as</p><formula xml:id="formula_18">w d (x t i ) = 2(1 -d(x t i )).<label>(12)</label></formula><p>In this way, when the CNN parameters are learnt by using the gradient reversal layer, lower weights (i.e., w d (x t i ) &lt; 1) are assigned to the selected pseudo-labeled target samples which are already predicted to be "source", and higher weights (i.e., w d (x t i ) &gt; 1) are assigned to the samples which are still predicted to be "target", to encourage more target samples to be classified to "source".</p><p>Empirically, we also find that the results are improved if we re-train the domain classifier by including the selected pseudo-labeled target samples in the set S c , which were used for training the image classifier. A possible explanation is that we emphasize the importance of high-confident samples when learning the domain classifiers, and also further reduce the data distribution mismatch for these samples.  Select the pseudo-labeled target samples based on s c (x t i ) by using the image classifier, and select the target samples based on s d (x t i ) by using the latest domain classifier, which respectively construct the set S c containing target samples with both pseudo-labels ỹt i and domain labels "0", and the set S d containing target samples with domain labels "0" only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Calculate the weights w c (x t i ) and w d (x t i ) for the image classifier and the domain classifiers, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>Update the image classifier c and the parameters Θ from the CNN feature extraction blocks based on the classification loss of the source samples using Eqn. ( <ref type="formula" target="#formula_7">4</ref>) and the selected pseudo-labeled target samples in the set S c using Eqn. <ref type="bibr" target="#b9">(10)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>Update the domain classfiers W, the weights of domain classifiers Λ, and the parameters from the CNN feature extraction blocks Θ based on the loss of all the samples from both domains using Eqn. ( <ref type="formula" target="#formula_5">3</ref>) and also the loss of selected target samples in the set S c and S d using Eqn. <ref type="bibr" target="#b12">(13)</ref>.</p><p>As the result, the loss of the selected pseudo-labeled target samples for the domain classifier part can be written as,</p><formula xml:id="formula_19">min Θ F ,W,λ l ∈Λ L d tar = 1 N t N t i=1 ((s c (x t i ) + s d (x t i ))w d (x t i )L CA ),(<label>13</label></formula><formula xml:id="formula_20">)</formula><p>where s c (x t i ) is the selection indicator from the image classifier defined in 4.2.2, and s d (x t i ) is the selection indicator from the domain classifier which equals to 1 if x t i is selected (i.e., x t i ∈ S d ) and 0 otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">SPCAN Summary</head><p>The objective of our SPCAN is to jointly optimize four losses, which include L CA defined in Eqn. ( <ref type="formula" target="#formula_5">3</ref>), L src defined in Eqn. ( <ref type="formula" target="#formula_7">4</ref>), L c tar defined in Eqn. <ref type="bibr" target="#b9">(10)</ref>, and L d tar defined in Eqn. <ref type="bibr" target="#b12">(13)</ref>. The selected pseudo labelled target samples are only used in L c tar and L d tar . All labelled source samples are only used in L src , while all unlabelled samples from both domain are used in L CA . The total objective of SPCAN can be written as follow,</p><formula xml:id="formula_21">min Θ F ,c,W,z,λ l ∈Λ L SP CAN =L src + L c tar + λ(L CA + L d tar ). (<label>14</label></formula><formula xml:id="formula_22">)</formula><p>The whole pipeline of SPCAN is illustrated in With this initial training strategy, the image classifier and domain classifiers are able to provide reasonable prediction results so that we can use them for selecting pseudo-labeled target samples. For the second stage, we separately select and reweigh different pseudo-labeled target samples for the image classifier and the domain classifiers, and use the gradients from their corresponding losses to update the classifiers and CNN parameters respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Cooperated SPCAN for Video Action Recognition</head><p>We further extend our SPCAN model to the video action recognition task under the unsupervised domain adaptation setting. In the video action recognition task, both spatial information and temporal information are important for classifying actions. Many Two-Stream ConvNets were proposed <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b71">[72]</ref>, in which two convolutional networks trained for the RGB stream and optical flow stream are finally fused for action recognition. Inspired by the co-training approach <ref type="bibr" target="#b54">[55]</ref>, we further exploit the complementary information in two streams, and encourage the networks of two streams to help each other in the self-paced learning procedure, which is referred to as Cooperated SPCAN (CoSPCAN).</p><p>For the training procedure of CoSPCAN, we train a CAN model for the RGB stream and train another CAN model for the optical flow stream. And the total loss for one stream is as follows:</p><formula xml:id="formula_23">L M CoSP CAN = L M src + L c,M tar + λ(L M CA + L d,M tar ). (<label>15</label></formula><formula xml:id="formula_24">)</formula><p>The loss function above is very similar to the objective of SPCAN in Eqn. <ref type="bibr" target="#b13">(14)</ref>. Since there are two streams, we use M as the index, i.e., M = f low for the optical flow stream and M = rgb for the RGB stream.</p><p>For each stream M , we have a cross-entropy loss L M src for the video classification task as in Eqn. ( <ref type="formula" target="#formula_7">4</ref>) and the loss L M CA for the collaborative and adversarial loss as in Eqn. <ref type="bibr" target="#b2">(3)</ref>. For the loss functions L c,M tar and L d,M tar that utilize psuedo-labeled data, we also calculate the sample selection indicators and sample weights, but in a way different from SPCAN.</p><p>More formally, let us denote p rgb i (resp., p f low i ) as the score for the i-th video V i predicted by the networks on the RGB (resp., optical flow) stream. The loss function for L c,M tar and L d,M tar can be defined similar to <ref type="bibr" target="#b9">(10)</ref> and ( <ref type="formula" target="#formula_19">13</ref>) by replacing the indicators with the cross-stream selection indicators defined as follows:</p><formula xml:id="formula_25">s c rgb (V i ) = 1, p f low i &gt; T c f low . 0, otherwise. s c f low (V i ) = 1, p rgb i &gt; T c rgb . 0, otherwise. (<label>16</label></formula><formula xml:id="formula_26">)</formula><p>where s c rgb (V i ) and s c f low (V i ) are the sample selection indicators for the video classifiers of RGB and optical flow streams, respectively. T c rgb and T c f low are the sample selection thresholds as in SPCAN using the classification confidence scores from the RGB stream and the optical flow stream, respectively. The two sets of selected samples by using the two indicators for the RGB and optical flow streams are then denoted as S c rgb and S c f low , respectively. Analysis. In SPCAN, the sample selection indicators and sample weights calculated from one stream are used by the stream itself. In CoSPCAN, however, inspired by co-training <ref type="bibr" target="#b54">[55]</ref>, the selection indicators and weights calculated from one stream are used for training the networks of another stream. In this way, we exchange the selected target samples and their corresponding weights within the RGB and flow streams, which can exchange the information of the two streams so that the deep models of the two streams help each other during the learning process.</p><p>The details of the selected pseudo-labeled target samples exchange process between the RGB stream and the optical flow stream is illustrated in Fig. <ref type="bibr" target="#b3">(4)</ref>. For example, firstly in the RGB stream, we use the video classifier of this individual stream to predict pseudo-labels of each videos and use the classification confidence scores and target domain discriminator scores of this individual stream to select pseudolabeled target samples (set S c rgb and S d rgb ) for the optical flow stream. Then, for the optical flow stream, we use the target samples from the RGB stream to re-train and improve the deep model of the optical flow stream, and use the updated model to predict pseudo-labels and select target samples for the RGB stream. Again, the selected target samples from the optical flow stream are used to re-train the deep model of the RGB stream in the next training epoch. This selection and re-training procedure is iteratively performed until the models of the two streams converge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section, we evaluate our proposed methods for the unsupervised domain adaptation problem under two different tasks, object recognition and video action recognition. Then, we also investigate different components of our methods in details.</p><p>The source code of our preliminary work iCAN <ref type="bibr" target="#b30">[31]</ref> is available online 1 . We will release the code of this work upon acceptance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experiments for the object recognition tasks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Datasets</head><p>For object recognition tasks, we evaluate our methods on three benchmark datasets, Office-31, ImageCLEF-DA and VISDA-2017.</p><p>Office-31 dataset <ref type="bibr" target="#b78">[79]</ref> is a benchmark dataset for object recognition in domain adaptation, which contains 4,110 images from 31 classes. It contains three domains: Amazon (A), Webcam (W) and DSLR (D).</p><p>ImageCLEF-DA dataset <ref type="bibr" target="#b16">[17]</ref> is built for ImageCLEF 2014 domain adaptation challenge 2 . It contains 4 subsets, including Caltech-256 (C), ImageNet ILSVRC 2012 (I), Bing (B) and Pascal VOC 2012 (P). Each of the subset contains 12 classes and each class has 50 images, which results in a total number of 600 images for one subset. For each dataset, we follow <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b16">[17]</ref> to utilize the common evaluation protocol under all 6 domain adaptation settings.</p><p>VISDA-2017 <ref type="bibr" target="#b79">[80]</ref> is also a domain adaptation challenge dataset 3 . It focuses on the domain adaptation from synthetic images to real-world images. The dataset contains two domains. The source domain is the synthetic domain (i.e. the training set), which contains rendered 3D CAD model images from different angles and lightning conditions. The target domain is the domain with real-world, which contains natural images cropped from the COCO dataset <ref type="bibr" target="#b80">[81]</ref> (i.e. the validation set). The whole dataset contains 152,397 images for the training set and 55,388 images for the validation set, which are all from 12 categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Implementation Details</head><p>We implement our proposed methods based on the PyTorch framework 4 .</p><p>We use the ImageNet pre-trained ResNet50 model as the backbone network. The learning rate for discriminators and image classifier are set as 10 times of the backbone network(feature extractor layers). In the network, the feature extractor layers are grouped into four blocks. We add four domain discriminators after the 2nd, 3rd, 4th and 5th pooling layers (after 10th(res2b), 22th(res3d), 40th(res4f) and 49th layer(res5c)). The image classifier is placed at the last ResNet50 layer, after the 5th pooling layer. We use minibatch stochastic gradient descent(SGD) for optimization.</p><p>The learning rate of SPCAN is 0.0015 initially and decreases gradually after each iteration. We use the same INV learning rate decrease strategy as in DANN <ref type="bibr" target="#b18">[19]</ref>. Following the setting in DANN <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, an adaptation factor is used for controlling the learning rate of the domain discriminator. In our experiment, we set the batch size, momentum, and weight decay as 16, 0.9 and 3 × 10 -4 , respectively. We set 1. https://github.com/zhangweichen2006/iCAN 2. http://imageclef.org/2014/adaptation 3. http://ai.bu.edu/visda-2017/ 4. https://github.com/pytorch/pytorch the trade-off parameter λ = 0.4, and use λ m = -2 in order to have more stabilized training for all tasks.</p><p>In each domain adaptation task, we utilize all samples from both source and target domains. Due to different dataset size in different tasks, we utilize different total training epoch lengths and different number of pseudo-labeled target samples in one batch. For training epoch length, we decide it based on the fixed total iteration for different tasks, similar to other settings in <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b18">[19]</ref>.</p><p>Similar to the iCAN <ref type="bibr" target="#b30">[31]</ref> model, for image classification, we use the same number of labelled and unlabelled samples in one batch. In one training epoch, let us denote N s and N p as the total number of source samples and the number of target samples selected by the CSS module, respectively. Let us also denote N t and N q as the total number of target samples, and the number of target samples selected by the DSS module, respectively. For convenience of presentation, we denote β = N p N s +N p as the ratio of the number of target samples selected by the CSS module over the total number of source samples and target samples selected by the CSS module. To train the image classifier and domain classifiers with all selected target samples, we balance the number of target samples selected by the CSS module and the DSS module with the source samples in one mini-batch. For the image classification part, when one mini-batch has eight samples, we use 8(1 -β) source samples with the category label and 8β pseudo-labelled target samples selected by the CSS module to train the network. For the domain classification part, when one mini-batch has sixteen samples, we use 8 source samples, 8β target samples selected by the CSS module, 8(1 -β) N q N t target samples selected by the DSS module, and the remaining samples are randomly selected target samples. The domain labels for the source samples and the target samples are "1" and "0", respectively. In this way, the numbers of training samples from the source and target domains are the same, which is helpful for learning the model that is balanced for both source and target domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Experimental Results</head><p>We compare our method with the basic deep learning method (ResNet50) and the existing deep domain adaptation learning methods based on ResNet50. For the basic deep learning method, we use only source samples to finetune the ResNet50 <ref type="bibr" target="#b7">[8]</ref> model that is pre-trained based on the ImageNet dataset. For deep transfer learning methods, we report the results of Deep Domain Confusion (DDC) <ref type="bibr" target="#b11">[12]</ref>, Deep Adaptation Network (DAN) <ref type="bibr" target="#b14">[15]</ref>, Residual Transfer Network (RTN) <ref type="bibr" target="#b15">[16]</ref>, Joint Adaptation Network (JAN) <ref type="bibr" target="#b16">[17]</ref>, Multi-Adversarial Domain Adaptation(MADA) <ref type="bibr" target="#b81">[82]</ref>, Similarity Learning (SimNet) <ref type="bibr" target="#b41">[42]</ref>, Generate To Adapt (GTA) <ref type="bibr" target="#b42">[43]</ref>, Deep Adversarial Attention Alignment (DAAA) <ref type="bibr" target="#b25">[26]</ref>, Adversarial dropout regularization (ADR) <ref type="bibr" target="#b82">[83]</ref>, and our work CAN and SPCAN. In addition, we also report the results of Domain Adversarial Training of Neural Network (DANN) <ref type="bibr" target="#b18">[19]</ref> using our own implementation.</p><p>The results on three datasets, Office-31, ImageCLEF-DA and VISDA-2017 are reported in Table <ref type="table" target="#tab_2">1</ref>, Table <ref type="table" target="#tab_3">2</ref> and<ref type="table">Table 3</ref>, respectively. In terms of the average accuracy, our proposed SPCAN achieves the best results on all three datasets. In the Office-31 dataset, SPCAN gains prominent improvements from the two more difficult tasks, (i.e., W→A and D→A), when compared to other state-of-the-art approaches. SP-CAN also performs better than our preliminary work iCAN <ref type="bibr" target="#b30">[31]</ref> on all three datasets, in which the average accuracies are 87.2% on Office-31, 87.4% on Image-CLEF and 76.9% on VISDA-2017, respectively.</p><p>There are 152,397 synthetic images from 12 categories in the VISDA-2017 dataset. However, a few images from the same 3D model and similar viewpoints are redundant for model training. Due to such redundancy issue, we randomly use 1000 images from each class to form a small subset of VISDA-2017, which are used as the source training data to evaluate our methods CAN and SPCAN. Surprisingly, we observe that the results of our methods CAN and SPCAN using a small proportion of training data are already sufficient to demonstrate the effectiveness of our SPCAN, which also outperform other state-of-the-art methods (e.g., <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b82">[83]</ref>) using the whole training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.4">Analysis on the learnt weights λ l 's</head><p>In our method CAN, we follow the existing method DANN <ref type="bibr" target="#b18">[19]</ref> and set the weight at the last layer/block as a negative value to learn domain invariant representations at higher layers/blocks. In order to empirically justify why it is useful to learn domain specific representations at lower layers/blocks, we also conduct new experiments by using different fixed weights (e.g., λ l = 1  3 , λ l = -1 3 , λ l = 1, λ l = -1, l = 1, 2, 3) instead of learning the optimal weights with our method SPCAN, which is referred to as sSPCAN( <ref type="formula" target="#formula_1">1</ref>3 ), sSPCAN(-1</p><p>3 ), sSPCAN <ref type="bibr" target="#b0">(1)</ref>, and sSPCAN(-1), respectively. The results are reported in Table <ref type="table">4</ref>. From the results in Table <ref type="table">4</ref>, we observe that our SPCAN outperforms sSPCAN for all tasks, which demonstrates that it is beneficial to learn the TABLE <ref type="table">3</ref> Average Accuracies (%) of different methods on the VISDA-2017 dataset. All methods use ResNet50 as the backbone network except ADR <ref type="bibr" target="#b82">[83]</ref>, which uses ResNet101.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VISDA-2017</head><p>ResNet50 <ref type="bibr" target="#b7">[8]</ref> 50.6 DAN <ref type="bibr" target="#b14">[15]</ref> 55.0 RTN <ref type="bibr" target="#b15">[16]</ref> 57.3 DANN <ref type="bibr" target="#b18">[19]</ref> 57.8 JAN <ref type="bibr" target="#b16">[17]</ref> 61.8 GTA <ref type="bibr" target="#b42">[43]</ref> 69.5 SimNet <ref type="bibr" target="#b41">[42]</ref> 69.6 ADR <ref type="bibr" target="#b82">[83]</ref> 73.5 CAN(Ours) 64.1 SPCAN(Ours) 82.6</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE 4</head><p>Accuracies (%) of the simplified version of SPCAN (referred to as sSPCAN) by using different sets of fixed λ l 's on the Office-31 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>A→W W→A A→D D→A W→D D→W Avg.</p><p>sSPCAN(  optimal weights for different blocks. For sSPCAN, we also observe that sSPCAN achieves the best results when setting λ l = 1 3 , l = 1, 2, 3, which also indicates that it is beneficial to learn domain specific features in the shallower layers.</p><formula xml:id="formula_27">λ 1 λ 2 λ 3 λ 4 -2 -1 0 1 (a) A → W λ 1 λ 2 λ 3 λ 4 -2 -1<label>0</label></formula><p>We also observe that the learnt λ l 's at lower blocks (e.g., λ 1 ) are often larger than those at higher blocks (e.g., λ 3 ). We take the Office-31 dataset A→W and W→D as two examples to illustrate the λ l 's learnt at different blocks in Figure <ref type="figure">5</ref>. The results show that the learnt representations can be gradually changed from domain specific representations at lower blocks to domain invariant representations at higher blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.5">Results using different CNN architectures</head><p>In our proposed methods CAN and SPCAN, we use ResNet50 <ref type="bibr" target="#b7">[8]</ref> as our default backbone network. However, different CNN architectures <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b83">[84]</ref>, <ref type="bibr" target="#b84">[85]</ref> can be readily used as the backbone networks in our work. In Table <ref type="table" target="#tab_5">5</ref>, we take the Office-31 dataset as an example to report more results of our method SPCAN when using VGG19 <ref type="bibr" target="#b83">[84]</ref>, ResNet152 <ref type="bibr" target="#b7">[8]</ref> and DenseNet161 <ref type="bibr" target="#b84">[85]</ref> as the backbone networks. Similar to our default model using ResNet50 as the backbone  network, we add four domain discriminators after the last pooling layers of the four blocks when using different CNN architectures. For comparison, the results of the baseline method DANN <ref type="bibr" target="#b18">[19]</ref> based on VGG19 <ref type="bibr" target="#b83">[84]</ref>, ResNet152 <ref type="bibr" target="#b7">[8]</ref> and DenseNet161 <ref type="bibr" target="#b84">[85]</ref> are also reported in Table <ref type="table" target="#tab_5">5</ref>. In terms of the average accuracies over all six settings on the Office-31 dataset, we observe that SPCAN outperforms DANN when using VGG19, ResNet152 and DenseNet161 as the backbone networks, which demonstrates the effectiveness and the robustness of our method SPCAN for unsupervised domain adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.6">Ablation study</head><p>To investigate the benefits of different modules in SPCAN, in this section, we conduct ablation study on SPCAN using the Office-31 dataset. In particular, we study different variants of SPCAN, including: The results of all variants are reported in Table <ref type="table" target="#tab_6">6</ref>. In terms of the average accuracy, our CAN model without using any sample selection modules gives 82.4%. Moreover, we also observe that the results of SPCAN w/o CSS achieves only similar results with the CAN model. On the other hand, if we use only the CSS module in SPCAN (i.e., SPCAN w/o DSS), the average accuracy can be improved by 5.7% to 88.1%. The above results indicate that it is crucial to use image classifier to select pseudo-labeled target samples. Finally, our SPCAN module achieves the average accuracy of 89.1%, which demonstrates that it is beneficial to integrate the new self-paced sample selection module for improving the domain adaptation performance.</p><p>Furthermore, we also show that our self-paced sample selection module can also be applied to other adversarial training based models to further improve their performance for domain adaptation. For example, when combining our self-paced sample selection module with DANN, the average accuracy is improved from 80.9% to 86.2% (see the results of DANN and SPDANN in Table <ref type="table" target="#tab_6">6</ref>). However, it is still inferior to our SPCAN, which also validates the effectiveness of our collaborative and adversarial training scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.7">Qualitative Analysis</head><p>We take the Office-31 dataset (A → W) as an example to visualize the features extracted by using the baseline method DANN <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, our methods CAN and SPCAN. For better visualization, we follow the existing domain adaptation methods <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b16">[17]</ref> to generate the 2D representations of all source and target domain samples from all 31 classes. The visualization results are shown in Figure <ref type="figure">6</ref>. When comparing our method CAN and SPCAN with the baseline method DANN, we observe that the distributions of both source domain data and target domain data have more overlaps than DANN, and the samples from different classes are also grouped better for CAN <ref type="bibr" target="#b30">[31]</ref> and SPCAN. The results clearly demonstrate that the features extracted by using our methods CAN <ref type="bibr" target="#b30">[31]</ref> and SPCAN are not only domaininvariant but also discriminant for the image classification task. Compared with CAN, SPCAN is able to learn more discriminant and domain-invariant features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experiments for the video action recognition task</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Datasets</head><p>For the video action recognition task, there are hardly any benchmark action recognition dataset with multiple distinct video subsets that share the same action class. Generally, videos in different datasets usually have different distributions. As the result, we evaluate our models on two commonly used action recognition datasets, UCF101 <ref type="bibr" target="#b86">[87]</ref> and HMDB51 <ref type="bibr" target="#b87">[88]</ref>, which contain 10 shared categories for both datasets.</p><p>UCF101 dataset <ref type="bibr" target="#b86">[87]</ref> is a benchmark dataset for action recognition with trimmed videos. It contains 101 action classes of 13,320 video clips. HMDB51 dataset <ref type="bibr" target="#b87">[88]</ref> is a large video collection from web videos and movies. It consists of 6766 annotated videos from 51 different actions.</p><p>From these two datasets, we select all video clips from all the categories that are shared by both datasets 5 for domain 5. The ten selected categories include: Archery(Shoot Bow), Basketball(Shoot Ball), Biking(Ride Bike), Diving, Fencing, Golf Swing(Golf), Horse Riding(Ride Horse), Pull Ups, Punch, Push Ups. Fig. <ref type="figure">6</ref>: Visualization of the source and target domain samples for the Office-31 (A→W) in the 2D space by using the t-SNE embedding method <ref type="bibr" target="#b85">[86]</ref>, where the representations are learnt by using the baseline method DANN (a), our method CAN (b) and SPCAN (c), respectively. The samples from the source and the target domains are shown in blue and red colors, respectively. Best viewed in color.</p><p>adaptation. The two subsets are referred to as UCF101-10 and HMDB51-10 in this work, which contains 1339 videos and 1172 videos, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Implementation Details</head><p>In this task, we also implement our methods in the Pytorch framework. The pre-trained BN-Inception <ref type="bibr" target="#b88">[89]</ref> model based on the ImageNet dataset is used as the backbone network. Similarly as for the object recognition task, the learning rate for the domain discriminators and the video classifier is set as ten times of the backbone network. We also add four domain discriminators for the BN-Inception model, after the 2nd, 3rd, 4th and 5th pooling layer. The video classifier is added after the 5th pooling layer. Mini-batch stochastic gradient descent (SGD) is also used for optimization.</p><p>We set the initial learning rate as 0.001 for the CoSPCAN model and gradually decrease the learning rate after each iteration as in SPCAN. We set the batch size as 48 for both streams and utilize the same setting of other hyperparameters as our SPCAN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Experimental Results</head><p>The results for the video action recognition task under the domain adaptation setting between UCF101-10 and HMDB51-10 datasets are reported in Table <ref type="table">7</ref>. We compare our proposed approach with the state-of-the-art methods, including the ImageNet fine-tuned TSN <ref type="bibr" target="#b29">[30]</ref> and our reimplemented DANN <ref type="bibr" target="#b18">[19]</ref>. The TSN network is trained by using source domain labeled samples only without considering domain shift, while for DANN we further add a domain discriminator on top of TSN for each stream. To validate the effectiveness of our pseudo label exchanging strategy, we also include our CAN and SPCAN models for comparison. For all methods, the predictions on two streams are fused with the same weights as in TSN <ref type="bibr" target="#b29">[30]</ref>.</p><p>The results are summarized in Table <ref type="table">7</ref>. We observe that all domain adaption approaches improve the classification accuracy when compared with the baseline TSN model, and our CoSPCAN achieves the best performance. In particular, DANN improves TSN by 2.4% with domain adversarial learning, while our CAN model further boosts this margin to 5.1%, which validates the effectiveness of our collaborative and adversarial learning scheme. SPCAN</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE 7</head><p>Comparison of different unsupervised domain adaptation methods using BN-Inception backbone <ref type="bibr" target="#b88">[89]</ref> for video action recognition on the UCF101-10 (U) and HMDB51-10 (H) datasets. further improves the margin to 6.8%, showing the benefit of selecting pseudo-labeled target samples with our proposed two modules. Finally, our CoSPCAN gains an improvement of 8.2% and achieves the best average accuracy of 93.5%, which demonstrates the effectiveness of our CoSPCAN by jointly incorporating the complementary information from different streams to learn robust models for cross-domain video action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we have proposed a new deep learning method called Collaborative and Adversarial Network (CAN) for unsupervised domain adaptation. Different from the existing works that learn only domain invariant representations through domain adversarial learning, our method CAN additionally learns domain specific representations through domain collaborative learning. To effectively explore the unlabeled target samples, we have further proposed a Self-Paced CAN (SPCAN) model, in which a selfpaced learning scheme is developed to iteratively select pseudo-labeled target samples, and enlarge the training set for learning a more robust model. Observing that the twostream framework is commonly used for video action recognition, we have also developed the unsupervised domain adaptation method Cooperated SPCAN (CoSPCAN) for video action recognition, in which we further incorporate the complementary information from different views (i.e., RGB and Flow clues) by using the selected pseudo-labeled samples from one view to help the model training process on the other view. Extensive experiments on multiple benchmark datasets have demonstrated the effectiveness of our proposed models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Motivation of our Self-Paced CAN (SPCAN). In order to enhance the discriminability of the learnt model and inspired by Self-paced Learning, we re-train the CAN model by iteratively selecting pseudo-labeled target samples in an easy-to-hard learning scheme for both image/video classifier (a) and domain classifier (b). Based on the image/video classifier, our SPCAN gradually selects from fewer target samples to more target samples with high confidence scores. Based on the domain classifier, our SPCAN gradually selects from fewer target samples to more target samples that are likely from the target domain.</figDesc><graphic coords="2,312.00,43.70,252.00,301.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 1</head><label>1</label><figDesc>Self-Paced Collaborative and Adversarial Network (SPCAN).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Overview of the pseudo-labeled target sample selection process in our CoSPCAN. We use the video classifier and last domain classifier of one stream (RGB/Flow) to select the target samples for another stream (Flow/RGB). S c rgb and S c f low are the selected pseudo-labelled target domain samples for the video classifiers of the RGB and flow stream, respectively. S d rgb and S d f low are the selected target domain samples for the domain classifiers of the RGB and flow stream, respectively. C rgb and C f low are the video classifiers for the RGB and flow stream, respectively. D rgb and D f low are the domain classifiers for the RGB and flow stream, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>1 (Fig. 5 :</head><label>15</label><figDesc>Fig. 5: Different λ l 's learnt by using our SPCAN on the Office-31 dataset (a) A→W and (b) W→D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>( 1 )</head><label>1</label><figDesc>CAN corresponds to the proposed CAN model without using selected target samples for re-training the image classifier or the domain classifiers. (2) SPCAN w/o CSS is the SPCAN method without using the selected target samples for re-training the image classifier. (3) SPCAN w/o DSS is the (SPCAN) method without using the selected target samples for re-training the domain classifiers. (4) SPCAN is our final model, in which both the image classifier and the domain classifier are re-trained using the selected target samples. (5) DANN corresponds to our re-implemented DANN model in [19], which does not use the selected target samples for re-training the image classifier or the domain classifiers. (6) SPDANN applies the same sample selection strategy as in SPCAN model on DANN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="13,48.00,43.70,516.01,135.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 1</head><label>1</label><figDesc>Accuracies (%) of different methods on the Office-31 dataset.</figDesc><table><row><cell>Model</cell><cell>A→W W→A A→D D→A W→D D→W Avg.</cell></row><row><cell cols="2">ResNet50 [8] 73.5 59.8 76.5 56.7 99.0 93.6 76.5</cell></row><row><cell>DDC [12]</cell><cell>76.0 63.7 77.5 67.0 98.2 94.8 79.5</cell></row><row><cell>DAN [15]</cell><cell>80.5 62.8 78.6 63.6 99.6 97.1 80.4</cell></row><row><cell>RTN [16]</cell><cell>84.5 64.8 77.5 66.2 99.4 96.8 81.6</cell></row><row><cell cols="2">DANN [19] 79.3 63.2 80.7 65.3 99.6 97.3 80.9</cell></row><row><cell>JAN [17]</cell><cell>86.0 70.7 85.1 69.2 99.7 96.7 84.6</cell></row><row><cell cols="2">MADA [82] 90.0 66.4 87.8 70.3 99.6 97.4 85.2</cell></row><row><cell cols="2">SimNet [42] 88.6 71.8 85.3 73.4 99.7 98.2 86.2</cell></row><row><cell>GTA [43]</cell><cell>89.5 71.4 87.7 72.8 99.8 97.9 86.5</cell></row><row><cell>DAAA [26]</cell><cell>86.8 73.9 88.8 74.3 100.0 99.3 87.2</cell></row><row><cell cols="2">CAN(Ours) 81.5 63.4 85.5 65.9 99.7 98.2 82.4</cell></row><row><cell cols="2">SPCAN(Ours) 92.4 74.5 91.2 77.1 100.0 99.2 89.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 2</head><label>2</label><figDesc>Accuracies (%) of different methods on the ImageCLEF-DA dataset.</figDesc><table><row><cell>Model</cell><cell>I→P P→I I→C C→I C→P P→C Avg.</cell></row><row><cell cols="2">ResNet50 [8] 74.6 82.9 91.2 79.8 66.8 86.9 80.4</cell></row><row><cell>DAN [15]</cell><cell>74.5 82.2 92.8 86.3 69.2 89.8 82.5</cell></row><row><cell>RTN [16]</cell><cell>74.6 85.8 94.3 85.9 71.7 91.2 83.9</cell></row><row><cell cols="2">DANN [19] 75.6 84.0 93.0 86.0 71.7 87.5 83.0</cell></row><row><cell>JAN [17]</cell><cell>76.8 88.0 89.7 74.2 91.7 85.8</cell></row><row><cell cols="2">MADA [82] 75.0 87.9 96.0 88.8 75.2 92.2 85.8</cell></row><row><cell cols="2">CAN(Ours) 78.2 87.5 94.2 89.5 75.8 89.2 85.7</cell></row><row><cell cols="2">SPCAN(Ours) 79.0 91.1 95.5 92.9 79.4 91.3 88.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>74.5 91.2 77.1 100.0 99.2 89.1</head><label></label><figDesc></figDesc><table><row><cell cols="2">1 3 ) 89.5 68.6 91.1 74.3 100.0 98.9 87.1 sSPCAN(-1 3 ) 89.4 68.8 90.3 68.8 99.8 98.4 85.9 sSPCAN(1) 89.2 72.2 87.8 70.4 99.8 98.3 86.3</cell></row><row><cell cols="2">sSPCAN(-1) 88.7 70.8 88.5 71.0 99.6 98.0 86.1</cell></row><row><cell>SPCAN</cell><cell>92.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 5</head><label>5</label><figDesc>Accuracy (%) comparison between our SPCAN and the baseline method DANN when using different CNN architectures as the backbone networks on the Office-31 dataset.</figDesc><table><row><cell>Model</cell><cell>A→W W→A A→D D→A W→D D→W Avg.</cell></row><row><cell>DANN(VGG19)</cell><cell>75.3 60.7 77.6 60.1 99.3 97.6 78.4</cell></row><row><cell>DANN(ResNet50)</cell><cell>79.3 63.2 80.7 65.3 99.6 97.3 80.9</cell></row><row><cell>DANN(ResNet152)</cell><cell>79.8 63.9 82.6 65.9 99.7 97.4 81.6</cell></row><row><cell cols="2">DANN(DenseNet161) 83.1 65.1 84.7 66.0 99.6 97.6 82.7</cell></row><row><cell>SPCAN(VGG19)</cell><cell>90.3 72.5 89.0 74.9 100.0 98.2 87.5</cell></row><row><cell>SPCAN(ResNet50)</cell><cell>92.4 74.5 91.2 77.1 100.0 99.2 89.1</cell></row><row><cell cols="2">SPCAN(ResNet152) 92.8 77.3 92.0 78.6 100.0 99.3 90.0</cell></row><row><cell cols="2">SPCAN(DenseNet161) 93.8 76.8 93.6 76.9 100.0 99.3 90.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 6</head><label>6</label><figDesc>Ablation study of different variants of our SPCAN model on theOffice-31 dataset.</figDesc><table><row><cell>Model</cell><cell>A→W W→A A→D D→A W→D D→W Avg.</cell></row><row><cell>CAN</cell><cell>81.5 63.4 85.5 65.9 99.7 98.2 82.4</cell></row><row><cell cols="2">SPCAN w/o CSS 81.8 63.6 84.9 66.0 99.8 98.0 82.4</cell></row><row><cell cols="2">SPCAN w/o DSS 90.3 73.8 90.2 75.4 99.9 98.9 88.1</cell></row><row><cell>SPCAN</cell><cell>92.4 74.5 91.2 77.1 100.0 99.2 89.1</cell></row><row><cell>DANN</cell><cell>79.3 63.2 80.7 65.3 99.6 97.3 80.9</cell></row><row><cell>SPDANN</cell><cell>88.6 70.1 89.2 71.3 99.8 98.3 86.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>89.4 97.6 93.5</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>U→H H→U Avg.</cell></row><row><cell>TSN [30]</cell><cell>80.5 90.1 85.3</cell></row><row><cell>DANN [19]</cell><cell>83.6 91.7 87.7</cell></row><row><cell>CAN(Ours)</cell><cell>85.7 95.1 90.4</cell></row><row><cell cols="2">SPCAN(Ours) 87.6 96.6 92.1</cell></row><row><cell>CoSPCAN(Ours)</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE , VOL.XX, NO.XX, XXXX XXXX</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Domain adaptation via transfer component analysis</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-NN</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="199" to="210" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by domain invariant projection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Baktashmotlagh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Lovell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="769" to="776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Domain adaptation from multiple sources: A domain-dependent regularization approach</title>
		<author>
			<persName><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-H</forename><surname>Tsang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-NNLS</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="504" to="518" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised visual domain adaptation using subspace alignment</title>
		<author>
			<persName><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Habrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sebban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2960" to="2967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Visual event recognition in videos by learning from web data</title>
		<author>
			<persName><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-H</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-PAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1667" to="1680" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<title level="m">Fcns in the wild: Pixellevel adversarial and constraint-based adaptation</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Deep domain confusion: Maximizing for domain invariance</title>
		<author>
			<persName><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3474</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Simultaneous deep transfer across domains and tasks</title>
		<author>
			<persName><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4068" to="4076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep transfer metric learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-P</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="325" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="97" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaptation with residual transfer networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="136" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep transfer learning with joint adaptation networks</title>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2208" to="2217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1180" to="1189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Domainadversarial training of neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">59</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2962" to="2971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep learning of scene-specific classifier for pedestrian detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="472" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep coral: Correlation alignment for deep domain adaptation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="443" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Domain separation networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="343" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Coupled generative adversarial networks</title>
		<author>
			<persName><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="469" to="477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unsupervised pixel-level domain adaptation with generative adversarial networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3722" to="3731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep adversarial attention alignment for unsupervised domain adaptation: the benefit of target expectation maximization</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Conditional adversarial domain adaptation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1647" to="1657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A kernel two-sample test</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="723" to="773" />
			<date type="published" when="2012-03">Mar. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Self-paced learning for latent variable models</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1189" to="1197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Collaborative and adversarial network for unsupervised domain adaptation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3801" to="3809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">What you saw is not what you get: Domain adaptation using asymmetric kernel transforms</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1785" to="1792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Domain adaptation for object recognition: An unsupervised approach</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="page" from="999" to="1006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Geodesic flow kernel for unsupervised domain adaptation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2066" to="2073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Direct importance estimation with model selection and its application to covariate shift adaptation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nakajima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kashima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Buenau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kawanabe</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1433" to="1440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Correcting sample selection bias by unlabeled data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="601" to="608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Domain adaptation problems: A DASVM classification technique and a circular validation strategy</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bruzzone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marconcini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="770" to="787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Discriminative learning for differing training and test distributions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Br</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Scheffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="81" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Domain generalization and adaptation using low rank exemplar SVMs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-PAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1114" to="1127" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Boosting domain adaptation by discovering latent domains</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mancini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Bul Ò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Autodial: Automatic domain alignment layers</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Carlucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Bul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5077" to="5085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with similarity learning</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Element</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Generate to adapt: Aligning domains using generative adversarial networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Duplex generative adversarial network for unsupervised domain adaptation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1498" to="1507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">From source to target and back: symmetric bi-directional adaptive gan</title>
		<author>
			<persName><forename type="first">P</forename><surname>Russo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Carlucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Cycada: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Diverse image-to-image translation via disentangled representations</title>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Maximum classifier discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Residual parameter transfer for deep domain adaptation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rozantsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4339" to="4348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Beyond sharing weights for deep domain adaptation</title>
	</analytic>
	<monogr>
		<title level="m">T-PAMI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A theory of learning from different domains</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="151" to="175" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5767" to="5777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Co-training for domain adaptation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2456" to="2464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Combining labeled and unlabeled data with co-training</title>
		<author>
			<persName><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eleventh annual conference on Computational learning theory</title>
		<meeting>the eleventh annual conference on Computational learning theory</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="92" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Asymmetric tri-training for unsupervised domain adaptation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML. JMLR.org</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2988" to="2997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual international conference on machine learning</title>
		<meeting>the 26th annual international conference on machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Shifting weights: Adapting object detectors from image to video</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="638" to="646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Curriculum domain adaptation for semantic segmentation of urban scenes</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2020" to="2030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via class-balanced self-training</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Vijaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="289" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Multi-view transfer learning with a large margin approach</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lawrence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1208" to="1216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Multi-view discriminant transfer learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Multi-view domain generalization for visual recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4193" to="4201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Robust multi-view representation: a unified perspective from multi-view learning to domain adaption</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5434" to="5440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3551" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">3d convolutional neural networks for human action recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">T-PAMI</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="221" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yue-Hei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4694" to="4702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Action recognition with trajectorypooled deep-convolutional descriptors</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4305" to="4314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Convolutional twostream network fusion for video action recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1933" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Temporal segment networks for action recognition in videos</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Exploiting privileged information from web data for action and event recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="130" to="150" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Exploiting images for video recognition with hierarchical generative adversarial networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Deep domain adaptation in action space</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jamal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Namboodiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Deodhare1</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Venkatesh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Open set domain adaptation for image and action recognition</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Busto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Analysis of representations for domain adaptation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="137" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Bazaraa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">D</forename><surname>Sherali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Shetty</surname></persName>
		</author>
		<title level="m">Nonlinear programming: theory and algorithms</title>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Adapting visual category models to new domains</title>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="213" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Visda: A synthetic-to-real benchmark for visual domain adaptation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Usman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kaushik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Roynard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-E</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Hayes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR-W</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2021" to="2026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Multi-adversarial domain adaptation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Adversarial dropout regularization</title>
		<author>
			<persName><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11">Nov. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Hmdb: a large video database for human motion recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2556" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Batch normalization: accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
