<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ALT: Breaking the Wall between Graph and Operator Level Optimizations for Deep Learning Compilation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-10-22">22 Oct 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhiying</forename><surname>Xu</surname></persName>
							<email>zyxu@smail.nju.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Haipeng</forename><surname>Dai</surname></persName>
							<email>haipengdai@nju.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Jiafan</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hongding</forename><surname>Peng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xiaoliang</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Haoran</forename><surname>Wan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yixu</forename><surname>Xu</surname></persName>
							<email>xuyixu@huawei.com</email>
						</author>
						<author>
							<persName><forename type="first">Hao</forename><surname>Cheng</surname></persName>
							<email>chenghao49@hisilicon.com</email>
						</author>
						<author>
							<persName><forename type="first">Kun</forename><surname>Wang</surname></persName>
							<email>kun.wang1981@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Guihai</forename><surname>Chen</surname></persName>
							<email>gchen@nju.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Jiafan Xu</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country>China Hongding Peng</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Wei Wang</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="department">Xiaoliang Wang</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country>China Haoran Wan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff8">
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff9">
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff10">
								<orgName type="department">Yixu Xu</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff11">
								<orgName type="institution">Huawei Technologies</orgName>
								<address>
									<settlement>Hao Cheng</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff12">
								<address>
									<country>Huawei Technologies China Kun Wang</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff13">
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff14">
								<orgName type="department">Guihai Chen</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff15">
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ALT: Breaking the Wall between Graph and Operator Level Optimizations for Deep Learning Compilation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-10-22">22 Oct 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2210.12415v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning models rely on highly optimized tensor libraries for efficient inference on heterogeneous hardware. Current deep compilers typically predetermine layouts of tensors and then optimize loops of operators. However, such unidirectional and one-off workflow strictly separates graph-level optimization and operator-level optimization into different system layers, missing opportunities for unified tuning.</p><p>This paper proposes ALT, a compiler that performs joint graphand operator-level optimizations for deep models. ALT provides a generic transformation module to manipulate layouts and loops with easy-to-use primitive functions. ALT further integrates an auto-tuning module that jointly optimizes graph-level data layouts and operator-level loops while guaranteeing efficiency. Experimental results show that ALT significantly outperforms state-of-the-art compilers (e.g., Ansor) in terms of both single operator performance (e.g., 1.5? speedup on average) and end-to-end inference performance (e.g., 1.4? speedup on average).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Deep learning has become one of the essential building blocks for emerging applications, such as machine translation and autonomous driving systems. To provide ubiquitous services, developers craft high-performance programs supporting various tensor operators (e.g., 2-D convolution and matrix multiplication) on different hardware platforms (e.g., NVIDIA GPU and ARM CPU). However, current vendor libraries (e.g., MKL-DNN <ref type="bibr" target="#b32">[33]</ref> and cuDNN <ref type="bibr" target="#b11">[12]</ref>) typically demand significant engineering effort on manual optimization. Moreover, the hand-tuning approach can hardly catch up with the fast evolution of deep learning techniques that constantly introduce new tensor operators <ref type="bibr" target="#b31">[32]</ref> and new hardware (e.g., neural processing units). Therefore, researchers develop deep compilers <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b80">81]</ref> to achieve automatic performance optimization by auto-tuning and code generation techniques.</p><p>Two key categories of optimizations during compilation are graph-level optimization and operator-level optimization. Graphlevel optimization represents operators as nodes and tensors as edges in a computational graph and rewrites nodes or edges to obtain a more efficient graph for inference. For instance, data layout optimization rewrites the tensor storage format to improve memory accessing performance <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b68">69]</ref>. Constant folding <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b56">57]</ref> and common subexpression elimination <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b56">57]</ref> removes redundant nodes. Operator-level optimization, which mainly involves loop optimization, transforms the nested loops in the source code of each operator to schedule the execution of instructions <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b54">55]</ref>. In this work, we focus on data layout optimization and loop optimization because they yield significant performance improvements and their tuning strongly correlates with operator and hardware characteristics.</p><p>Unfortunately, existing deep compilers (e.g., TVM <ref type="bibr" target="#b9">[10]</ref>, Tensor Comprehension <ref type="bibr" target="#b69">[70]</ref>, Tiramisu <ref type="bibr" target="#b5">[6]</ref>, AKG <ref type="bibr" target="#b80">[81]</ref>) and auto-tuning techniques (e.g., AutoTVM <ref type="bibr" target="#b10">[11]</ref>, NeoCPU <ref type="bibr" target="#b43">[44]</ref>, FlexTensor <ref type="bibr" target="#b89">[89]</ref> and Ansor <ref type="bibr" target="#b82">[83]</ref>), fail to combine data layout and loop optimizations effectively. These systems first predetermine tensor layouts either manually or via setting a hyper-parameter from a predefined template and then perform loop optimization based on these layouts. There are three major limitations in this unidirectional and oneoff workflow. First, manual layout selection implies that only a limited number of layout choices can be explored, hence prone to be suboptimal. Second, altering the tensor layout demands the time-consuming re-implementation of operators that access the tensor. Third, layout optimization and loop optimization are separated into different system layers. Such strict boundary seriously compromises the performance of the generated tensor programs. For instance, we observe that optimizing loops based on the best of three candidate layouts for 2-D convolutional operators can improve the performance by 55.9% on average on the Intel CPU. Moreover, the performance of a specific data layout is sensitive to operator configurations (e.g., tensor shapes) and hardware. Thus it is hard to determine data layouts for each workload without feedback from loop optimization.</p><p>This paper proposes ALT, a deep compiler that jointly performs graph-level and operator-level optimizations for deep models. The design of ALT originates from the following insight. Graph-level data layout optimization and operator-level loop optimization could benefit from each other. In the meanwhile, the root cause of the inability to perform cross-layer joint tuning is the coupling between data storage and operator implementation in prior arts, such that altering the data layout requires re-implementing operators. Such high cost for changing layouts further leads to the unidirectional and one-off optimization flow. Therefore, ALT abstracts layout manipulation as easy-to-use primitive functions, such that the task of re-implementing operators can be delegated to a compilation pass without human interference. After reducing the cost, ALT further incorporates layout and loop optimizations into a unified auto-tuning framework, which breaks the boundary between graphand operator-level optimizations to open new opportunities.</p><p>It is not trivial to achieve our goals. We need to strike the following challenges. Challenge 1: How can we eliminate the overhead of layout transformation? We discover two types of potential overhead when altering the tensor layouts: layout-conversion overhead and fusion-conflict overhead. First, operators along the data stream may require different tensor layouts to reach their optimal performance. To transform the layouts of tensors produced by other operators at runtime, directly inserting conversion operators will incur extra overhead on data movements. Second, altering the output tensor layout of an operator needs the reconstruction of its loop nest. Such reconstruction may inhibit the operator from being fused with its consumer, which is an important loop optimization technique to improve inter-operator data locality. Challenge 2: How can we prevent inefficiency due to the search space reconstruction during joint tuning? Changing the output layout of an operator will induce the loop nest reconstruction, which will further lead to the variation of the loop tuning space. For joint tuning, such space variation prohibits a direct iterative exploration. Otherwise, the points we have searched in the last iteration may be invalid in the changing space. This leads to inefficient tuning for most search methods, including genetic and learning-based algorithms, since the accumulated knowledge of the search space structure cannot be further exploited in the newly reconstructed space. Challenge 3: How can we improve efficiency given the search space explosion with the combination of layout and loop tuning? Along with the joint tuning, the combined search space will be tremendously large, hence inefficient to explore directly. For example, in a typical 2-D convolutional operator, the loop transformation space can contain up to ? (10 7 ) points for its seven nested loops. After combining the layout transformation, the joint search space can be at a scale of ? (10 19 ) considering three tensors, each of which further involves four dimensions. Moreover, end-to-end optimization is more challenging due to the inter-dependency of many operators and tensors.</p><p>To eliminate the two types of overhead brought by layout transformation, we propose a layout propagation mechanism. Suppose we have chosen a different layout for the input tensor of an operator. We let the upstream operator, which is the producer of this tensor, directly yield elements based on this new layout, hence no conversion operator is required. To promote operator fusion, we propagate the new layout downstream along the computational graph to let the consumer operator trigger the same loop reconstruction, which helps to align loop nests of multiple operators for fusion. As such, we can safely transform data layouts with minimal overhead, and without sabotaging loop optimization.</p><p>To alleviate the search space reconstruction issue in the cotuning, our solution is two folds. First, we divide the co-tuning into two stages: joint stage and loop-only stage. The joint stage searches for optimal tensor layouts, while the loop-only stage only performs loop tuning with the searched layouts remaining unchanged. Second, we propose a cross-exploration architecture for the joint stage, rather than the direct exploration. For a new feasible layout, we reconstruct the loop space and then perform multiple rounds of loop optimization to assess the new layout. This design avoids inefficient loop space reconstruction since the loop-only stage keeps layouts unchanged. It also achieves the expected bidirectional and unified tuning flow in the joint stage, because each candidate layout is evaluated based on feedback from loop optimization through our novel tuning architecture.</p><p>To avoid the search space explosion due to the combination of layout and loop tuning, we prune the space at two levels. First, we only build layout transformation spaces for tensors accessed by complex operators. In this work, we take convolutions and general matrix multiplication as complex operators, the performance of which are layout sensitive. For other tensors, we further exploit the layout propagation mechanism to propagate the searched layouts onto them without more searching. Second, we identify a promising subspace by tailoring a tuning template for each tensor accessed by complex operators. These templates are constructed based on our analysis of layout optimization considering both operator and hardware characteristics.</p><p>By addressing these challenges, ALT achieves joint and efficient graph-level data layout optimization and operator-level loop optimization automatically.</p><p>We comprehensively evaluate ALT on Intel CPU, NVIDIA GPU, and ARM CPU. Compared with state-of-the-art vendor libraries (e.g., MKL-DNN <ref type="bibr" target="#b32">[33]</ref>, cuDNN <ref type="bibr" target="#b11">[12]</ref>, and XNNPACK <ref type="bibr" target="#b26">[27]</ref>) and autotuning frameworks (e.g., Ansor <ref type="bibr" target="#b82">[83]</ref>), ALT achieves an average of 1.5? speedup in terms of single operator performance, and 1.4? speedup in terms of end-to-end inference performance. Our evaluation also shows that ALT can find data layouts that are not explored in prior arts. Additionally, we have deployed ALT in production environments for four months, boosting a broad spectrum of real workloads (e.g., speech recognition and super resolution).</p><p>In summary, we make the following contributions:   ? We reveal the necessity of joint graph-and operator-level optimizations for deep learning compilation, and that the root cause of the inefficient unidirectional and one-off optimization flow in prior arts lies in the high cost of layout manipulation. ? We design an easy-to-use generic infrastructure that covers a rich layout transformation space. It allows users to manipulate layouts without soiling the hands for re-implementation, and without extra overhead via the layout propagation mechanism during end-to-end optimization. ? We devise a joint layout and loop auto-tuning framework. Via effective space pruning and judicious exploration design, it not only achieves a bidirectional and unified optimization flow but also guarantees tuning efficiency. ? Our extensive evaluation shows that, without human interference, ALT improves performance over state-of-the-art baselines significantly, which also verifies the effectiveness of the proposed techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND AND MOTIVATION</head><p>A deep compiler typically compiles a neural network with multistage lowering and optimization. The compiler takes a model that can be generated by other frameworks (e.g., TensorFlow <ref type="bibr" target="#b0">[1]</ref>) as input. It then resolves the model to a computational graph where operators and tensors are represented as nodes and edges, respectively. Data layout optimization <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b68">69]</ref> is to rewrite the tensor storage format (i.e., the attributes of an edge) to alleviate memory accessing overhead for operators that access the tensor. Thus, data layout optimization is often classified as graph-level optimization. The storage format refers to the arrangement of tensor dimensions. Take the 2-D convolution (C2D) operator as an example. Popular data layouts for the output tensor of C2D include ???? , ? ?? ?, and ?? ?? , where ? , ?, ?,? represent the batch size, the number of output channels, the output tensor height, and the output tensor width, respectively. ???? is widely used on GPU <ref type="bibr" target="#b52">[53]</ref>, ? ?? ? is the default layout on CPU in TensorFlow <ref type="bibr" target="#b0">[1]</ref>, and ?? ?? is used in digital signal processing.</p><p>After graph-level optimization, the compiler will lower each node in the computational graph to operator-level representation. An operator can typically be represented as deeply nested loops. As the major part of operator-level optimization, loop optimization (e.g., loop tiling, vectorization, etc.) <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b54">55]</ref> is to transform the loop nest to schedule the execution of statements of each operator.</p><p>The motivation for this work is as follows. Observation 1: It is beneficial to jointly perform data layout optimization and loop optimization. We illustrate the benefits</p><formula xml:id="formula_0">KH -1 H H / 2 W W + (KW -1)</formula><p>Output tensor Input tensor tile overlap H + (KH -1) tile stride by an experiment that optimizes loops of C2D based on ???? , ? ?? ?, and ?? ?? layouts, respectively. Our platforms include 32-core Intel Xeon Silver 4110 CPU@2.1GHz, NVIDIA RTX 2080Ti GPU, and Kirin 990 ARM SoC. We report the performance in Fig. <ref type="figure" target="#fig_2">1</ref>, where the latency axis is in log scale, and each hardware involves multiple operator configurations (different number of channels, convolutional strides, etc.) to cover rich workloads. We observe that the best layout could improve the performance of loop optimization by 55.9%, 87.2%, and 48.8% on average on Intel CPU, NVIDIA GPU, and ARM CPU, respectively. On the converse, making a choice among different layouts is not easy when there is no feedback from loop optimization, due to the highly divergent performance with regard to operator configurations and platforms. For example, although ? ?? ? often outperforms ???? and ?? ?? on CPUs, especially when the number of input channels is small, there is still no clear rule that can fit all configurations. Observation 2: Existing solutions cannot effectively perform joint tuning due to the high cost of layout manipulation. Existing systems <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b69">70]</ref> typically couple the tensor storage with the implementation of operators, thus changing layouts requires reimplementation. Such a high cost of layout manipulation limits the number of layout choices that can be explored, and further leads to the unidirectional optimization flow. While there are works using special layouts to improve versatility, e.g., ? ? ? ? ?? ? ? where ? ? is a tiling parameter that can be changed without re-implementation <ref type="bibr" target="#b43">[44]</ref>, they still only cover a small layout optimization space. Moreover, switching to another category of layouts still requires reimplementing operators and even rewriting loop-tuning templates.</p><p>We use a more versatile layout as a motivating example. This layout is outside the tuning space of ? ? ? ? ?? ? ? and is hard to be discovered manually or without joint tuning. It can achieve performance improvement of 32.4% over ? ? ? ? ?? ? ? . Besides tiling the channel dimension, this layout further tiles the spatial dimensions (the height and the width) of the output tensor into four blocks. for io in range(o_t): Each spatial tile of the output tensor has shape ? 2 ? ? 2 . For a C2D with convolutional stride 1, the height and the width of the input tensor are ? + (?? -1) and ? + (?? -1), where ?? and ?? are the height and the width of the convolutional window. Due to the sliding-window operation of C2D that has natural overlaps, each output tile requires a ? 2 + (?? -1) ? ? 2 + (?? -1) tile of the input tensor for convolution. This leads to the layout in Fig. <ref type="figure" target="#fig_3">2</ref>, where each colored area denotes a tile, and the overlap between tiles along the input tensor height is exactly (?? -1). After the layout transformation, the generated loop nest is shown in Fig. <ref type="figure">3</ref>, where ? is the number of input channels, ????, ???, and ??? are the output tensor, input tensor, and weight tensor, respectively. In Fig. <ref type="figure">3</ref>, we also tile the output channels by ? ? to achieve multidimensional layout tiling. Besides, the corresponding loop ?? is placed as the innermost loop to improve locality, as a showcase for joint layout and loop optimization. The shape of ???? in Fig. <ref type="figure">3</ref> is</p><formula xml:id="formula_1">Conv[n][oh][ow][oo][ih][iw][io] += \ Inp[n][oh][ow][i][ih+rh][iw+rw]\ * Ker[oo][i][rh][rw][io]</formula><formula xml:id="formula_2">? ? 2 ? 2 ? ? ? ? ? ? 2 ? ? 2 ? ? ? .</formula><p>Such multi-dimensional tiling with overlaps promotes data locality and cache utilization. We defer the detailed profiling results on various layouts in Section 7.3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SYSTEM OVERVIEW</head><p>ALT is a deep compiler that achieves joint graph-level layout optimization and operator-level loop optimization to generate highperformance tensor programs for heterogeneous platforms automatically. The system overview of ALT is depicted in Fig. <ref type="figure">4</ref>, which incorporates two major modules: auto-tuning and transformation. The transformation module is a generic infrastructure that achieves low-cost layout and loop manipulation by easy-to-use primitive functions. Based on it, the auto-tuning module performs joint data layout and loop optimization by searching in the parameter spaces of the primitive functions. The workflow of ALT is as follows.</p><p>First, the user provides the computational graph of a deep model, which a domain-specific language (e.g., a subset of Python) can express. It can also be constructed from a model file generated by other frameworks (e.g., TensorFlow <ref type="bibr" target="#b0">[1]</ref>).</p><p>Second, the auto-tuning module builds search space for tensors and operators and explores the space jointly. To reduce the tuning time, it uses a cost model to minimize time-consuming on-device measurements. When the exploration completes, it decodes the best performant point found in the space into a sequence of layout and loop primitives. Then, it delivers these primitives to the transformation module.</p><p>Third, the layout propagation submodule propagates layout primitives. Then, the transformation module applies all primitives to perform layout and loop transformation to generate an optimized tensor program. Finally, we deploy the program on different hardware for inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">TRANSFORMATION</head><p>We first introduce the transformation module of ALT, which is a generic infrastructure for manipulating data layouts and loops. The transformation module consists of three submodules: layout transformation, layout propagation, and loop transformation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Layout Transformation</head><p>To achieve low-cost layout manipulation and easy layout tuning, we devise various primitive functions to transform data layouts: split, reorder, fuse, unfold, pad, and store_at. Among them, split, reorder, and fuse are basic primitives and the others are advanced primitives. These primitives lift the data layout transformation from the black-box compiler level to the source level to facilitate leaner control with domain-specific knowledge. We will temporarily cache the operation each time a primitive is applied on a tensor. During program generation, as a compilation pass, we will actually transform the data shapes and alter the corresponding accessing statements in the program. Thus, no human interference is required for re-implementing the operators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Basic Layout Primitives.</head><p>The basic primitives perform one-toone transformations. Given an ? dimensional tensor ? with original data layout of ? 1 ? 2 ...? ? and accessing expressions of ? 1 , ? 2 , ..., ? ? , we summarize basic primitives in Table <ref type="table" target="#tab_1">1</ref>, where 1 ? ? ? ? is an index to dimensions, ? ? is an integer denoting the splitting factor, ? is a permutation vector with ? (?) as its ?-th element, and ? 2?? is an abbreviation for ? ?=2 ? ? (? ???+? is similar). For instance, to get the ? ? ? ? ?? ? ? layout from ???? , we can apply the following primitive sequence:</p><formula xml:id="formula_3">split(T, dim=2, factors=[O // o_t, o_t]) reorder(T, perm=[1, 2, 4, 5, 3])</formula><p>Alternatively, to pack the layout into spatial blocks, we can transform ? ?? ? through another primitive sequence: </p><formula xml:id="formula_4">fuse(T, dims=[2, 3, 4]) split(T, dim=2, factors=[O // 4, 4, H * W]) reorder(T, perm=[1, 2, 4, 3])</formula><formula xml:id="formula_5">[?] [?] [?] [?]</formula><p>in the code, it will be transformed as follows:</p><p>(</p><formula xml:id="formula_6">1) T[n][?(? ?) + ?? + ?], and let ? = ?(? ?) + ?? + ? (2) T[n][ ? ?? 4 ][ ? (?? ) mod 4][? mod (?? )] (3) T[n][ ? ?? 4 ][? mod (?? )][ ? ?? mod 4] . 4.1.2 Advanced Layout Primitives.</formula><p>The above examples show the versatility of basic primitives. However, there are cases that cannot be covered, such as the overlapped tiling in Fig. <ref type="figure" target="#fig_3">2</ref>. To achieve such special transformations, we abstract advanced layout primitives: unfold, pad, and store_at.</p><p>unfold: This primitive performs overlapped tiling. It accepts a tile_size parameter, and a stride parameter which is the interval between two tiles: unfold(tensor, dimension, tile_size, stride)</p><p>We denote ????_???? as ? and ?????? as ?. If the original size for a dimension is ?, this primitive will generate two new dimensions with sizes of ? ?-? ? ? + 1 and ?. For instance, an array {1, 2, 3, 4, 5} can be unfolded to a 2-D array {{1, 2, 3}, {3, 4, 5}} by setting ? = 3 and ? = 2. For the input tensor layout in Fig. <ref type="figure" target="#fig_3">2</ref>, we can set ? = ? 2 + (?? -1), ? = ? 2 for the height dimension, and the width is similar.</p><p>The unfold primitive is useful for sliding-window computational patterns, e.g., convolutional layers. They have the memory access pattern of ? ? + ? , where ? is the constant convolutional stride, ? is the window index, and ? is a reduction iterator for the offset inside a window. In the following, we use ? to denote the window size (e.g., ? will be equal to ?? and ?? for the two patterns ?? + ?? and ?? + ?? in Fig. <ref type="figure">3</ref>, respectively). Then, the original accessing statement ? [? ? + ? ] will be transformed to</p><formula xml:id="formula_7">? ? ? ?-? ? ? +1 ? ? +? -? ? ? ?-? ? ? +1 . (<label>1</label></formula><formula xml:id="formula_8">)</formula><p>Besides unfold, we also propose pad and store_at primitives. The pad primitive is to append zeros for a selected dimension, which is useful to align data in memory and alleviate bank conflicts on the shared memory of the NVIDIA GPU. The store_at primitive allows fusing two tensors together by attaching one to another to improve inter-tensor data locality. For example, in a fully connected layer, it can attach each element of the bias vector to each column of the weight matrix. Subsequently, the inner product and the bias addition in general matrix multiplication (GMM) may be computed together by accessing the weight column and the bias element in the   same cache line. Additionally, all three primitives have their inverse counterparts, namely fold, unpad, and decouple_at, to transform layouts back and forth.</p><formula xml:id="formula_9">Conv[n][ht][w][o][hi] += Inp[...]*Ker[...] for n, o, h, w in range(N, O, H, W): ReLU[n][o][h][w] = max(Conv[n][h//4][w][o][h%4],0)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Layout Propagation</head><p>The layout primitives working at the local tensor level could incur overhead when performing joint or end-to-end optimization on a computational graph. Specifically, we discover two types of such overhead: layout-conversion overhead and fusion-conflict overhead.</p><p>In this subsection, we will analyze the overhead and propose the layout propagation mechanism to address this issue. Given a C2D, if it requires a different layout for the weight tensor, we can transform it offline without any runtime overhead because the weight tensor is a constant. Unfortunately, if the C2D requests a different input layout X ? , it can only be achieved either by <ref type="bibr" target="#b0">(1)</ref> inserting an operator performing runtime layout conversion (Fig. <ref type="figure" target="#fig_6">5a</ref>) or (2) letting the producer operator yield each element based on the new layout directly (Fig. <ref type="figure" target="#fig_6">5b</ref>). Inserting layout conversion operators will incur extra overhead due to runtime data movements. So, we prefer the second way, which is called layout propagation. After propagation, the padding operator actually performs two tasks at runtime: padding zeros and converting the layout. Similarly, for the output tensor of C2D, we can let its consumer operator access the new layout directly, rather than inserting another conversion operator next to C2D.</p><p>Besides the layout-conversion overhead, another delicate issue emerges when incorporating operator fusion. Operator fusion is a loop-tuning technique to promote inter-operator data locality by  letting the downstream operator consume the intermediate data immediately before spilling out of the cache. Consider two operators: C2D and ReLU, and the original output layouts of them are both ???? . Suppose we transform the output layout of the C2D to ? ? 4 ? ?4 through split and reorder primitives. Then, the generated program is shown in Fig. <ref type="figure" target="#fig_7">6</ref>. The loop nest of the C2D is reconstructed accordingly due to the output layout transformation. Different from the original case, we cannot perform loop tiling on the two loop nests with the same tile sizes and then fuse the two nests. Since fusion is an effective technique, reducing the chance of fusion due to the reconstructed loop nest will result in performance loss.</p><p>To eliminate such fusion-conflict overhead induced by layout transformation, we extend the layout propagation mechanism such that the same layout can be shared among multiple tensors. Layout propagation can be implemented easily by duplicating the primitive sequence of the source tensor for the target tensor. For instance, we replicate the primitives from tensor ???? in Fig. <ref type="figure" target="#fig_7">6</ref>, i.e., split and reorder primitives, for tensor ???? . Then ReLU will trigger the same loop nest reconstruction, hence aligned perfectly with that of C2D. Consequently, the fusion-after-tiling in loop tuning will be the same as the normal case, as illustrated in Fig. <ref type="figure" target="#fig_9">7</ref>.</p><p>Although layout propagation helps to eliminate the overhead incurred by layout transformation, it has three constraints. First, we only propagate primitives along a path with only element-wise operators and among tensors with the same shape. Given an operator ? [?] = ? (? [?]), there exists an element-wise data mapping between the output tensor ? and the input tensor ? . We can propagate the layout of ? to ? , or vice versa. This constraint is introduced because the parameters of primitives are shape-dependent. Second, we will not propagate a primitive sequence if it contains non-trivial advanced primitives. This is because advanced primitives will induce data expansion. Instead, we will insert conversion operators when they arise, as in Fig. <ref type="figure" target="#fig_6">5a</ref>. Third, the layout tuning for each complex operator will be performed independently. This constraint is to eschew the overhead of layout propagation itself, because the optimal layout of a complex operator may lead to inferior performance for another. For example, given two consecutive C2Ds, we will insert a conversion operator between them if needed rather than letting the output tensor of the former C2D and the input tensor of the latter C2D share the same layout. Notably, no conversion operator is necessary when other simple operators exist between the two C2Ds (e.g., we can propagate a layout onto the padding operator like in Fig. <ref type="figure" target="#fig_6">5b</ref> and let it perform the actual layout conversion).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Loop Transformation</head><p>We perform loop transformation via reusing the loop primitives of TVM <ref type="bibr" target="#b9">[10]</ref>: split, reorder (same names as layout ones, but distinct functions), vectorize, unroll, cache_read/write, parallel, inline, and compute_at. Most loop-tuning techniques, including loop tiling, vectorization, and operator fusion, can be realized by combining these primitives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">AUTO-TUNING</head><p>Even with the transformation module, optimization is still painful because it requires numerous manual trials. The combination of layout and loop tuning further exacerbates the problem. Thus, in the auto-tuning module, we devise a unified framework to jointly optimize layouts and loops to generate high-performance programs automatically.</p><p>Our joint tuning comprehends three steps: 1) we build the layout tuning space for tensors and loop tuning space for operators, each point in the space can be decoded as a primitive sequence; 2) we explore the tuning space to find the best performant point; 3) we decode this point as instantiated primitives and deliver them to the transformation module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Space Building</head><p>Auto-tuning is to search for the code with the best performance in the tuning space. With our transformation module, we only need to find the best parameters to apply primitives. Thus, the tuning space is equivalent to the parameter space for primitives. For now, we only consider layout split, reorder, and unfold primitives in the layout space. Also, we will omit details on the loop space, which is similar to <ref type="bibr" target="#b82">[83,</ref><ref type="bibr" target="#b89">89]</ref>, e.g., space of loop split factors for each operator.</p><p>The layout space to be built should be pruned, otherwise, it will be infinitely large because the number of primitives that can be applied is infinite. As in Section 1, we only perform layout tuning for complex operators and propagate their results to reduce the number of tuning tasks. Further, we craft a layout tuning template for each tensor that is accessed by complex operators. Each template only exposes a subset of parameters of primitives as tunable options. The templates are crafted based on the following observations on how data layouts influence performance considering intra-operator data dependency and hardware characteristics.</p><p>First, data layout influences the data reuse strategy, <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b46">47]</ref>. For most architectures, data reuse is vital to reducing the number of memory accesses and improving software pipeline. Consider the C2D as an example, each output element requires (?? ) ? (?? ) ?? input elements for reduction. Without data reuse, we need totally ? ? ? ?? ? ? ? (?? ) ? (?? ) ? ? load instructions for the input tensor. Fortunately, an input element is required by at most (?? ) ? (?? ) ?? output elements. Thus, we can reuse an input element to accumulate on ?? ? ?? spatial positions or ? channels before spilling it out of the cache. Besides, sequential data accesses can be bundled by SIMD instructions. With these two aspects, we can also explain why ? ?? ? layout often performs better than ???? layout <ref type="bibr" target="#b82">[83]</ref>: 1) an input element can be reused to accumulate on many (at most ?) output channels and ? is typically large, hence a high reuse rate; 2) output channels can be loaded with SIMD instructions easily since ? is the last dimension.</p><p>Second, data layout influences cache utilization. Both layout and loop tiling can be exploited to let a data block fit in cache <ref type="bibr" target="#b63">[64]</ref>. Besides, we also observe that layout tiling can further prevent cache misses by facilitating hardware prefetching <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b47">48]</ref>. To verify  <ref type="table" target="#tab_2">2</ref>, where we also present our predictions based on hardware prefetching in the second column. We observe that the CPU is very likely to fetch four contiguous cache lines when a miss event is triggered. For example, the prediction for tile size 512 ? 4 is calculated as 512?4 16?4 = 32. From Table <ref type="table" target="#tab_2">2</ref>, layout tiling is preferable to loop tiling to improve cache utilization via hardware prefetching. Most importantly, the cache performance after layout tiling is always better than in other cases.</p><p>The second observation indicates that layout tiling improves cache utilization even though loop tiling has been exploited. Thus, our layout tuning template is a tiling template, with tiling sizes as basic tunable options. For most dimensions, the tiling can be achieved with split primitives. For height and width dimensions of convolutions, it can be achieved with the unfold primitives to enable the overlapped tiling. After splits and unfolds, based on the first observation, we let the tiled channel dimension be the last dimension to promote data reuse and SIMD. Consequently, our data layout tuning template for C2D has the following form: In the above templates, uppercase letters represent the original dimensions, while lowercase letters with a subscript ? denote the tiled parameters correspondingly. We do not need to tune the unfolded dimensions for the input tensor, because they are directly related to the tiling of the output tensor. Suppose the tuner splits the ? dimension of the output tensor as ? ? ? ? ? ? . It then applies the following unfold primitive on the input tensor directly: unfold(Inp, Inp height, h_t + (KH -1), h_t) This is the same as the case in Fig. <ref type="figure" target="#fig_3">2</ref> where ? ? = ? 2 . In summary, the pruned layout space for C2D consists of six tunable parameters (i.e., at a scale of ? (10  for tiling ?, ? of the weight tensor. For other convolutions (e.g., 3-D case), the template is similar.</p><formula xml:id="formula_10">?</formula><p>For a GMM ? = ? ?, where ?? , ??, ?? are the original layouts of the three matrices, the search space is much smaller due to fewer dimensions. Thus our template consists of split parameters for all dimensions. Then, based on the first observation, the reorder after splits is determined without tuning: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>? ? ? ? ? ? for ?. Finally, there are three tunable parameters (i.e., up to ? (10 3 ) points): ? ? , ? ? , ? ? , in the layout space for GMM.</p><p>The above templates only perform one-level multi-dimen-sional layout tiling. We can expand them to multi-level cases easily, which can be configured in ALT for scalability. For example, we can use two-level layout tiling templates for ALT, where the template for the output tensor of C2D can be defined as</p><formula xml:id="formula_11">? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? .</formula><p>Without our template-based pruning, the search space, especially the parameter space for the reorder primitive, will be too large to explore. The only concern after pruning is whether the subspace contains good points. We verify the effectiveness of pruning through experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Exploration &amp; Cost Model</head><p>To explore the search space, we need to: (1) visit points efficiently; (2) evaluate visited points rapidly. We resort to the PPO algorithm <ref type="bibr" target="#b59">[60]</ref> from reinforcement learning (RL) to explore the space. Compared with heuristic algorithms (e.g., genetic algorithm) and other RL algorithms, PPO is learning-based and more stable <ref type="bibr" target="#b30">[31]</ref>, which is introduced in <ref type="bibr" target="#b1">[2]</ref> to speed up the tuning space exploration. To speed up the evaluation, we develop a cost model to predict the performance to reduce the number of time-consuming on-device measurements.</p><p>In RL, an agent will respond (referred to as action) to environments based on its observation, which is composed of the state of the current environment and feedback given by the environment called reward. PPO employs two neural networks: actor and critic. The actor gives actions while the critic judges each action, i.e., fitting the real rewards.</p><p>Even with PPO, exploring layout and loop spaces simultaneously is challenging. Consider the C2D as an example, we need to rebuild its loop space every time given a new layout, because the loop nest relies on the output layout, like ?, ?, ?, ? in Fig. <ref type="figure" target="#fig_7">6</ref>. The reconstructed , , Zhiying Xu, Jiafan Xu, Hongding Peng, Wei Wang, Xiaoliang Wang, Haoran Wan, Haipeng Dai, Yixu Xu, Hao Cheng, Kun Wang, and Guihai Chen loop space further leads to that the points searched previously will be invalid in the new space, hence inefficient exploration.</p><p>As in Section 1, our solution to this issue involves two aspects. We first divide the performance tuning into two stages: the joint stage and the loop-only stage. We then propose a cross-exploration architecture, as shown in Fig. <ref type="figure" target="#fig_10">8</ref>, for the joint stage. The cross-exploration repeats the following process: determining a layout through the layout PPO actor, performing multiple rounds of loop tuning via loop PPO actors, and feeding the best performance back as the reward for the current layout. Consequently, we achieve a bidirectional and unified optimization flow in the joint stage to find better layouts. We also prevent inefficient loop tuning, since the loop reconstruction will not occur in the loop-only stage.</p><p>In the following, we will only elaborate on the design of RL action, state, and reward for the joint stage based on the crossexploration architecture. The loop-only stage can be achieved by removing layout-related searches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Layout Space Exploration.</head><p>Since the pruned layout space only involves tunable split parameters, we here develop a generic actor to explore the parameter space of the layout split primitive. Then, the final layout will be determined by a sequence of actions. Take the C2D in Fig. <ref type="figure" target="#fig_7">6</ref> as an example, the action sequence for resolving the output layout of ???? consists of: split ? , split ? , split ?, and reorder them to ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? . The split actor only provides the factors to split ?,? , ?, while the reorder is determined in the template in Section 5.1. Similarly, replacing the first two splits with unfolds forms the action sequence for the input layout.</p><p>Consider the dimension with a size of ? in a tensor. To obtain a generic split actor, we map its output action ? ? to a contiguous interval (0, 1). Then, the splitting factor ? is calculated as follows:</p><formula xml:id="formula_12">? = ?(? ? ? ? ) .</formula><p>(</p><formula xml:id="formula_13">)<label>2</label></formula><p>Assume the tensor ???? in Fig. <ref type="figure" target="#fig_7">6</ref> has ? = 32. The actor gives one action ? ? = 0.5. Then we derive two split dimensions : ? ? = ?(32 * 0.5) = 16, ? ? ? = ?(32/16) = 2. The state for the actor is given by the concatenation of the current states of all primitives for all tensors of the complex operator (e.g., ???, ??? , ???? in a C2D). For instance, when unfolding the height of ??? in Fig. <ref type="figure" target="#fig_3">2</ref> into two parts, the current state of the unfold primitive is changed to [2, ? 2 + (?? -1)], while the initial state was [1, ? + ?? -1]. Similarly, the current state for the split primitive is composed of factors, e.g., <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b15">16]</ref> for ? = 32 (initial state was <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b31">32]</ref>). Then the final state is the concatenation of all such sub-states.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Loop Space Exploration.</head><p>The exploration for loop space follows a similar random-walk design as <ref type="bibr" target="#b89">[89]</ref>. We first sample a batch of points in the loop space and choose the best one as the starting point, then each actor gives a direction for some parameter space. After that, we arrive at the next point by walking along that direction, as shown in Fig. <ref type="figure" target="#fig_10">8</ref>.</p><p>Including the layout split actor, we have a lot of actors now. To model the interference among subspaces/primitives, we deploy a global shared critic network for all actors (not shown in Fig. <ref type="figure" target="#fig_10">8</ref> for simplicity).</p><p>The reward ? for all RL agents is the same:</p><formula xml:id="formula_14">? = ? -? , (<label>3</label></formula><formula xml:id="formula_15">)</formula><p>where ? is a constant and ? is the latency of some point. For layout RL agents, ? is chosen as the best latency after several rounds of loop exploration given the current layout.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Cost Model.</head><p>To evaluate points rapidly, we estimate the performance by developing a cost model for each hardware. The cost model is a tree ensemble from XGBoost <ref type="bibr" target="#b8">[9]</ref>, similar to that of Ansor <ref type="bibr" target="#b82">[83]</ref>. For some point, we decode it as primitives and apply them to generate the optimized tensor program. Then we feed the features of the program (e.g., loop structures and accessing expressions) to the cost model to estimate the throughput. During exploration, we only measure the top-? points of a batch or an episode of RL trajectories, which are predicted by the cost model, on the target hardware. These measurements are also used for training the cost model online.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">IMPLEMENTATION</head><p>We implemented ALT based on TVM (v0.8dev1) <ref type="bibr" target="#b9">[10]</ref> with 19K LoC of Python and 2K LoC of C++.</p><p>To implement the layout transformation, we insert a pass before lowering the tensor expression (TE) of TVM to TVMIR. This pass will rewrite the indices of all tensor accesses in TE when layouts change. With regard to an operator ? = ? (? ) where the output tensor ? is of shape ? 1 ? 2 ..? ? , in TE this operator has ? nested spatial loops, each corresponding to a dimension of ? (one-to-one mapping). We denote the loop variables as ? = {? 1 , ? 2 , ..., ? ? }. Assume ALT caches a set of primitive sequences S either provided manually or by the auto-tuning module automatically. Our pass will first transform accesses for the output tensor ? , and then other tensors. We denote the primitive sequence for ? as S(? ) (abbreviated as ? ? ). Our pass first deducts the final layout of ? by applying each primitive function in ? ? . Assuming the new layout has ? dimensions with shape ? ? 1 ? ? 2 ..? ? ? , the loop structure will then be reconstructed by TE as ? ? = {? ? 1 , ? ? 2 , ..., ? ? ? }. Given the one-to-one mapping between a dimension of the output tensor and a loop variable, we will also have ? ? = ? ? (?). With this, we can transform accesses for tensor ? while ensuring validity. Specifically, the accesses of ? must first be remapped with the newly reconstructed loop variables. The remapping is done in two steps: 1) calculating the inverse primitive sequence of ? ? , denoted as ? -1 ? ; 2) replacing all old loop variables ? by ? -1 ? (? ? ) in all access indices of ? . After this remapping, the tensor accesses of ? can be safely transformed to ? ? (? -1 ? (? ? )) by applying S(? ). To implement the layout propagation, we copy the primitive sequence of the source tensor for the destination tensor. The joint stage of ALT sequentially tunes each complex operator following the topological order and propagates the resulting layouts. A special case is that an operator can have multiple consumers or producers. In the case of multiple consumers, ALT will propagate the layout of the source tensor to all consumers. For the case of multiple producers, consider</p><formula xml:id="formula_16">? [?] = ? (? 0 [?], ? 1 [?], ? 2 [ ?])</formula><p>, where there are element-wise mappings between ? 0 and ? , and between ? 1 and ? . When the layouts of ? 0 and ? 1 are both tuned, ALT will heuristically choose ? 0 for propagation onto ? . Conversely, if the layout of ? is tuned first (i.e., there is no complex operator prior ? 0 or ? 1 that can propagate layouts to them), ALT will propagate the layout of ? to both ? 0 and ? 1 .  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">EVALUATION</head><p>In this section, we evaluate ALT on various hardware platforms, including 40-core Intel Xeon Gold 6248 CPU@2.5GHz (443GB memory), NVIDIA Tesla V100 (CUDA v11.0), and Kirin 990 SoC (Android v10). We compare ALT with state-of-the-art frameworks and compilers: Torch (v1.7) <ref type="bibr" target="#b52">[53]</ref>, AutoTVM (v0.8dev1) <ref type="bibr" target="#b10">[11]</ref>, FlexTensor <ref type="bibr" target="#b89">[89]</ref>,</p><p>and Ansor <ref type="bibr" target="#b82">[83]</ref>. Torch is a reference point for vendor libraries, which was evaluated by using MKL-DNN library <ref type="bibr" target="#b32">[33]</ref> for Intel CPU, cuDNN (v8.0.4) library <ref type="bibr" target="#b11">[12]</ref> for NVIDIA GPU, and XNNPACK library <ref type="bibr" target="#b26">[27]</ref> for ARM CPU. AutoTVM, FlexTensor, and Ansor are three widely used auto-tuning frameworks. Besides, Ansor outperforms Tensorflow Lite <ref type="bibr" target="#b0">[1]</ref> and other hardware-specific compilers <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b82">83]</ref> such as OpenVINO <ref type="bibr" target="#b33">[34]</ref> and TensorRT <ref type="bibr" target="#b51">[52]</ref>. Thus, we do not include them as baselines here.</p><p>For ALT, if not specified, we use one-level layout tiling templates for layout space building. For loop space exploration, we set the sampling batch size and the episode length to 128, and measure the top-8 points predicted by the cost model on the target hardware. In addition, we take the total number of such on-device measurements as a metric of the search budget for all auto-tuning methods. Thus, a batch or an episode of points in ALT will cost a budget of 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Single Operator Benchmark</head><p>We first present the results on single operators. We consider 9 operators, including C2D, Group-wise C2D (GRP), Depth-wise C2D (DEP), Dilated C2D (DIL), 3-D convolution (C3D), 1-D convolution (C1D), GMM, Transposed C2D (T2D), Transposed C3D (T3D). Each operator is evaluated using 10 random configurations with different batch sizes, kernel sizes, etc. For instance, the value of batch size is selected from <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16]</ref>, and the number of input channels is uniformly sampled from <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr">512,</ref><ref type="bibr">960,</ref><ref type="bibr">1280]</ref>. We generate 90 test cases for each device. The result is normalized based on the geometric mean of speedups over the worst latency of each test case. For C1D, C2D/T2D, and C3D/T3D and their variants, we test ??? /?? ? for C1D, ???? /? ?? ? for C2D/T2D, and ????? /? ??? ? (? is the depth dimension) for C3D/T3D and report the best for baselines except Torch (it only supports ??? , ???? , ????? ). We set the search budget to 1000 for all auto-tuning methods, which is suggested by Ansor. For ALT, the budget for the joint stage and the loop-only stage is 300 and 700 respectively.</p><p>As shown in Fig. <ref type="figure" target="#fig_12">9a</ref>, on Intel CPU ALT achieves 9.5?, 9.9?, 9.8?, and 1.6? speedups in comparison with Torch, AutoTVM, FlexTensor, and Ansor respectively. Among all operators, DIL and DEP have lower operational intensity (the ratio of the number of computational instructions to the number of memory access instructions), and thus they are more likely to be memory-bound. For DIL and DEP, ALT outperforms other baselines with a large margin because layout tuning can effectively reduce memory accessing overheads. Even for operators that are typically compute-bound, e.g., C2D and C3D, ALT still achieves notable speedups. This is because the operational intensity depends on tensor shapes. ALT can tailor the tensor layouts toward each specific shape and hardware platform.</p><p>We achieve similar results on NVIDIA GPU and ARM CPU. Compared with Ansor, ALT achieves an average of 1.5? speedup on NVIDIA GPU (Fig. <ref type="figure" target="#fig_12">9b</ref>), and 1.4? speedup on ARM CPU (Fig. <ref type="figure" target="#fig_12">9c</ref>). We do not include the results of FlexTensor for ARM CPU since it does not support ARM backends. Generally, auto-tuning methods can outperform Torch because non-typical operator configurations are often less optimized in vendor libraries. Further, AutoTVM suffers from small tuning space and FlexTensor has no cost model, thus both demonstrate inferior performance than Ansor and ALT. Additionally, compared with Ansor, ALT can effectively tune data layouts with feedback from operator-level optimization and hence illustrate significant improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">End-to-End Benchmark</head><p>We then evaluate the end-to-end performance of ALT with five neural networks, including applications of 1) image processing: ResNet-18 (R18) <ref type="bibr" target="#b29">[30]</ref>, MobileNet-V2 (MV2) <ref type="bibr" target="#b58">[59]</ref>, 2) natural language processing: BERT-base (BB) <ref type="bibr" target="#b18">[19]</ref>, BERT-tiny (BT) <ref type="bibr" target="#b19">[20]</ref>, and 3) video processing: ResNet3D-18 (R3D) <ref type="bibr" target="#b28">[29]</ref>. For Intel CPU and NVIDIA GPU, the benchmarks use batch sizes of 1 and 16. For ARM CPU, we set the batch size to 1 due to the limited resource.</p><p>For convolutional networks, the input tensor is of shape ? ? 3 ? 224 ? 224 (image processing) and ? ? 3 ? 16 ? 112 ? 112 (video processing), respectively. For BERT, the shape of the input tensor is ? ? 128. For auto-tuning baselines, we set the search budget as 20,000 (which is suggested by Ansor <ref type="bibr" target="#b82">[83]</ref>). We set the budget for the joint stage to 8,000 and the budget for the loop-only stage to 12,000 in ALT. Additionally, Torch uses ???? /????? layouts while AutoTVM and Ansor use ? ? ? ? ?? ? ? /? ? ? ? ??? ? ? after integrating NeoCPU <ref type="bibr" target="#b43">[44]</ref>.</p><p>We illustrate the speedup ratio of all methods over Torch in Fig. <ref type="figure" target="#fig_13">10</ref>, where ?1 denotes batch size 1 and ?16 denotes batch size 16. The number on top of each bar demonstrates the latency in milliseconds. To verify the effectiveness of the joint tuning and layout propagation, we define two variants of ALT: (1) ALT-OL, which only involves loop optimization without the joint stage based on ? ?? ?/? ??? ? layouts; (2) ALT-WP, which only eliminates conversion operators between adjacent operators, as that shown  in Fig. <ref type="figure" target="#fig_6">5b</ref>. Compared with Ansor 1 , ALT achieves 1.47?, 1.39?, and 1.46? speedups on Intel CPU, NVIDIA GPU, and ARM CPU, respectively. For R3D, most of its operators are compute-bound, thus ALT achieves similar results with Ansor. For MV2, which is a lightweight network with lower operational intensity, ALT outperforms the baselines significantly. Notice that ALT-OL achieves similar performance as Ansor because both of them mainly involve loop tuning. When incorporating layout tuning and basic layout propagation, ALT-WP shows 1.1? speedup over ALT-OL in general and no improvement in a few cases. ALT achieves 1.3? speedup on average compared with ALT-WP. This is because operator fusion is a critical loop-tuning technique to improve performance, while ALT-WP cannot combine layout tuning and loop tuning effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Micro Benchmark</head><p>We dive into the details to achieve a better understanding of the system design. First, we present the overhead of layout propagation. We then study the parameter sensitivity of ALT in the end-to-end optimization. We will also conduct a case study to help understand the searched layouts and loops. Finally, we present more observations to provide hints for deep compiler optimization. Notably, we do not give more experiments on cost model <ref type="bibr" target="#b82">[83]</ref> and the PPO exploration method <ref type="bibr" target="#b1">[2]</ref> because they are not our major contributions. 1 Ansor performs better than Torch <ref type="bibr" target="#b52">[53]</ref> and AutoTVM <ref type="bibr" target="#b10">[11]</ref>. We omit the results of Torch and AutoTVM due to the lack of space.   </p><formula xml:id="formula_17">P U M V 2 -b 1 -C P U B B -b 1 -C P U R 3 D -b 1 -C P U R 1 8 -b 1 -G P U M V 2 -b 1 -G P U B B -b 1 -G P U R 3 D -b 1 -G P U 0.0 0.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.1">Layout Propagation Overhead:</head><p>We here study the overhead of layout propagation to show the necessity of the introduced constraints in Section 4.2. We evaluate two subgraphs on 48-core Intel(R) Xeon(R) Gold 5117 CPU @2.0GHz and NVIDIA RTX GPU. Each subgraph consists of three operators: padding (padding is 1), C2D (?? = ?? = 3, ?????? = 1), C2D (?? = ?? = 1, ?????? = 1). The input height/width of subgraph#1 is 7, while it is 14 for subgraph#2. Besides, all the numbers of input/output channels are 512, except that the number of output channels of the latter C2D (?? = ?? = 1) in subgraph#2 is 2048. We conduct two variants of ALT: ALT-FP and ALT-BP. ALT-FP will first tune C2D (?? = ?? = 3) and propagate its output layout to the input tensor of the latter C2D (?? = ?? = 1). While ALT-BP will first tune C2D (?? = ?? = 1) and propagate its input layout to the output tensor of the former C2D (?? = ?? = 3). Instead, ALT will tune the two C2Ds separately and insert a layout conversion operator between them according to the third constraint in Section 4.2.</p><p>The profiling results are reported in Fig. <ref type="figure" target="#fig_14">11</ref>, where we use Ansor as a reference point. We observe that ALT outperforms ALT-FP and ALT-WP. In other words, the best output layout of the C2D (?? = ?? = 3) is sub-optimal for the second C2D (?? = ?? = 1), and vice versa. Independent layout tuning for each complex operator brings more benefits while the layout conversion only incurs low overhead (2 microseconds for GPU and 8 microseconds for CPU). Combined with the results of ALT-WP in Fig. <ref type="figure" target="#fig_13">10</ref>, the fusion conflicts incur more overhead than layout conversions when performing layout transformation. We alleviate such two kinds of overheads by layout propagation and eschew the overhead of propagation itself by introducing necessary constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.2">Parameter sensitivity:</head><p>We study the parameter sensitivity by comparing the performance given different budget settings and search space sizes. We include three variants here: 1) two-level tiling templates with 20,000 budget; 2) two-level tiling templates but with 30,000 budget; 3) one-level layout tiling templates with 20,000 budget as the baseline (i.e., same as Section 7.2).</p><p>The end-to-end performance in different settings is shown in Fig. <ref type="figure" target="#fig_15">12</ref>. The first variant expands the search space size while keeping the budget unchanged. Compared with it, the baseline illustrates 15% performance improvement on average. By contrast, after setting the budget to 30,000, the second variant improves about 6% performance over the baseline. Also, more improvements can be obtained if given a larger budget, since one-level tiling templates constitute a subset of the two-level variant. For the budget of 20,000 in Section 7.2, one-level layout tiling templates yield a more effective trade-off between the final performance and the search space size. The budget of 20,000 to optimize a network typically costs 12-16 hours. But, it is affordable for practitioners as they only need to execute ALT once. Additionally, these results demonstrate the scalability of the tuning space, which is hard to achieve in prior auto-tuning works. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>? ? ? ? ? ? ? ? on Intel CPU. We profiled a small computational graph, which contains several operators: padding (after padding, the tensor will have ? = 1, ? = 3, ? = ? = 230), C2D (? = 64, ?? = ?? = 7, convolutional stride is 2), bias addition, and ReLU. This small graph is also the first layer of R18-b1. We set ? ? = 16 for ? ? ? ? ?? ? ? (? ? = 3 for the input tensor), while the searched layout has ? ? = 4, ? ? = 16, ? ? = 16 for ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? (? ? = 1 for the input tensor). The platform is 48-core Intel(R) Xeon(R) Gold 5117 CPU @2.0GHz. The results are summarized in Table <ref type="table" target="#tab_9">3</ref>, where we abbreviate (?? ) (?? ) to ?? for the weight tensor ??? . The latency (Lat.) is recorded in milliseconds and others are on a scale of 10 6 . We observe that for all layouts, except ???? , their optimized loop nests prefer reusing input values by computing multiple output channels once with SIMD, thus reporting fewer instructions and fewer cache loads/stores than ???? . Compared with ? ? ? ? ?? ? ? , ? ?? ? shows better data locality due to the larger tile size for the output channel. Specifically, ? = 64 in ? ?? ? yields a higher reuse rate than ? ? = 16 in ? ? ? ? ?? ? ? , as analyzed in Section 5.1. Further, ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? achieves more efficient cache utilization (only 2% misses) than ? ?? ?, due to the contiguous storage of intra-tile data elements after layout tiling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.4">Other observations:</head><p>Besides the profiled results, we observe that the ? ? parameter in the templates to tile output channels is often tuned as twice as the number of vector lanes that the platform supports when the spatial dimensions are not tiled. Specifically, we observe that ? ? = 32 on Intel CPU, ? ? = 8 on NVIDIA GPU, and ? ? = 8 on ARM CPU frequently arise, although the number of vector lanes with float32 data types is 16 for AVX-512, 4 for CUDA, and 4 for NEON. This is different from many hand-tuned libraries. However, these results are not applicable to all configurations or platforms. By contrast, the methodology in our micro-benchmarks could help understand the optimized layout, and similar analysis can be conducted for other cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">RELATED WORK</head><p>Deep learning compiler. A variety of deep compilers have been developed. Halide <ref type="bibr" target="#b54">[55]</ref> and TVM <ref type="bibr" target="#b9">[10]</ref> decouple the operator description and schedule to simplify loop optimization. XLA <ref type="bibr" target="#b38">[39]</ref>, Glow <ref type="bibr" target="#b57">[58]</ref>, nGraph <ref type="bibr" target="#b17">[18]</ref>, and Relay <ref type="bibr" target="#b56">[57]</ref> develop graph-level representations to support layout selection, constant folding, etc. Rammer <ref type="bibr" target="#b45">[46]</ref> supports fine-grained operator fusion. CODE <ref type="bibr" target="#b66">[67]</ref> speeds up the ensemble of deep models. Cortex <ref type="bibr" target="#b22">[23]</ref>, Nimble <ref type="bibr" target="#b61">[62]</ref>, DietCode <ref type="bibr" target="#b81">[82]</ref>, and CoRa <ref type="bibr" target="#b23">[24]</ref> focus on optimizing recursive/dynamic networks. TASO <ref type="bibr" target="#b34">[35]</ref>, Tensat <ref type="bibr" target="#b75">[76]</ref>, PET <ref type="bibr" target="#b70">[71]</ref>, Unity <ref type="bibr" target="#b67">[68]</ref>, and Ollie <ref type="bibr" target="#b86">[86]</ref> perform subgraph substitutions to obtain a more efficient computational graph. Tensor Comprehension (TC) <ref type="bibr" target="#b69">[70]</ref>, Tiramisu <ref type="bibr" target="#b5">[6]</ref>, MLIR <ref type="bibr" target="#b37">[38]</ref>, and AKG <ref type="bibr" target="#b80">[81]</ref> integrate polyhedral techniques. Bolt <ref type="bibr" target="#b74">[75]</ref> provides support for tensor core by integrating CUTLASS <ref type="bibr" target="#b50">[51]</ref>. SoyBean <ref type="bibr" target="#b71">[72]</ref> and Alpa <ref type="bibr" target="#b84">[84]</ref> provide auto-tuning support for inter-and intraoperator parallelism in distributed scenarios. UNIT <ref type="bibr" target="#b73">[74]</ref>, AMOS <ref type="bibr" target="#b88">[88]</ref>, and TensorIR <ref type="bibr" target="#b24">[25]</ref> provide support for tensorization on tensor accelerators. SparTA <ref type="bibr" target="#b87">[87]</ref> and SparseTIR <ref type="bibr" target="#b76">[77]</ref> introduce representation for sparse tensors. Compared with ALT, the layout auto-tuning, together with the joint data layout and loop optimization, is limited in these works. For instance, TC and Tiramisu require developers to transform data buffers manually. Although Relay and TVM can insert layout conversion operators between C2Ds with different predefined layouts (e.g., ???? , ? ?? ?, etc.), each layout combination requires a manual re-implementation of operators. By contrast, ALT supports generic graph-level layout auto-tuning with feedback from operator-level optimization.</p><p>Layout and loop tuning. Many systems try to improve the performance with layout transformation <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b82">83]</ref>. For instance, <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b78">79]</ref> optimize data layouts for FPGA design. <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b53">54]</ref> suggests to choose layouts among ? ?? ?, ???? , etc. <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b42">43]</ref> tightly couples it with the sparse computation. Compared with ALT, they lack versatility and are limited to a few tuning options. By contrast, the systems in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b82">83]</ref> can typically set the ? ? parameter in ? ? ? ? ?? ? ? layout after integrating NeoCPU <ref type="bibr" target="#b43">[44]</ref>. However, they have limitations: 1) switching to another kind of layout (e.g., a different reorder, or the overlapped tiling in ALT) still requires manually rewriting operators and even the templates of loop tuning, due to the coupling among data storage, operator implementation, and the loop-tuning templates; 2) ? ? is typically predetermined, while in Ansor <ref type="bibr" target="#b82">[83]</ref> is set via resolving the loop tiling configurations after loop tuning, as a packing technique and only for constant tensors, hence no joint tuning. ALT addresses the two limitations via 1) the generic layout transformation submodule, which requires no re-implementation, and is also independent of the loop transformation to achieve the decoupling; 2) an autotuning module at a higher level to orchestrate the cross-layer joint tuning while guaranteeing efficiency. As for recent loop optimization techniques <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b79">80,</ref><ref type="bibr" target="#b85">85,</ref><ref type="bibr" target="#b89">[89]</ref><ref type="bibr" target="#b90">[90]</ref><ref type="bibr" target="#b91">[91]</ref>, such as delicate cost models <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b72">73]</ref>, aggressive operator fusion <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b79">80,</ref><ref type="bibr" target="#b90">90]</ref>, and micro-kernel construction <ref type="bibr" target="#b91">[91]</ref>, they are complementary to ALT.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: C2D latency with different data layouts on different hardware platforms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Layout with overlapped tiling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: Program based on the layout in Fig. 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Ways to achieve runtime layout conversion. for n in range(N): for ht in range(H // 4): for w, o in range(W, O): for hi in range(4): Conv[n][ht][w][o][hi] = 0.0 for ri, rh, rw in range(I, KH, KW): Conv[n][ht][w][o][hi] += Inp[...]*Ker[...] for n, o, h, w in range(N, O, H, W): ReLU[n][o][h][w] = max(Conv[n][h//4][w][o][h%4],0)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Loop nests without propagation and fusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>for n, ht, w, o, hi in range(N, H // 4, W, O, 4): Conv[n][ht][w][o][hi] = 0.0 for ri, rh, rw in range(I, KH, KW): Conv[n][ht][w][o][hi] += Inp[...] * Ker[...] ReLU[n][ht][w][o][hi] = max(Conv[...], 0)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Loop nests with propagation and fusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Cross exploration architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Single Operator on ARM CPU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Single operator performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: End-to-end inference performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: The overhead of layout propagation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: End-to-end performance of different settings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Basic layout primitives. ? 1 , ..., ? ? ...? ?-1 ? 1 ...? ? ? ?+1 ... ..., ? ?-1 , ? ? ? 2?? , ..., ? ? ? ? mod ? ?-1 , ? ? mod ? ? , ? ?+1 , ... reorder permutation vector ? ? ? (1) ? ? (2) ...? ? (?) ? ? (1) , ? ? (2) , ..., ? ? (?) fuse ?, ? + 1, ..., ? + ? ...? ?-1 (? ???+? )? ?+?+1 ... ..., ? ?-1 , (? ? ? 2?? + ? ?+1 ? 3?? + ... + ? ?+? ), ? ?+?+1 , ... During program generation, the first fuse primitive produces shape ? (?? ?), the second gives ? ( ? 4 )4(?? ), and the final reorder generates ? ( ? 4 ) (?? )4, based on Table 1. Assuming the original accessing statement ?</figDesc><table><row><cell cols="2">Primitive Parameter</cell><cell>Transformed Shape</cell><cell>Transformed Accessing Expressions</cell></row><row><cell>split</cell><cell>?,</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Profiled L1 data cache misses.</figDesc><table><row><cell cols="3">Tile Size #L1-mis / Pred. (1st F.) #L1-mis (2nd F.)</cell></row><row><cell>512 ? 4</cell><cell>32 / 32</cell><cell>208</cell></row><row><cell>512 ? 16</cell><cell>96 / 128</cell><cell>262</cell></row><row><cell>512 ? 64</cell><cell>501 / 512</cell><cell>785</cell></row><row><cell>512 ? 256</cell><cell>2037 / 2048</cell><cell>2952</cell></row><row><cell cols="3">this, we conduct an experiment on a Cortex-A76 CPU, which is</cell></row><row><cell cols="3">a big core on Kirin 990 SoC, the L1 data cache line size of which</cell></row><row><cell cols="3">is float32x16 (i.e., 64 bytes). We profile two functions and both of</cell></row><row><cell cols="3">which only load a 2-D data block once from memory with NEON</cell></row><row><cell cols="3">instructions. The data elements for the first function are stored</cell></row><row><cell cols="3">contiguously in memory, i.e., layout tiling case. By contrast, the</cell></row><row><cell cols="3">elements for the second function are stored row by row, i.e., loop</cell></row><row><cell cols="3">tiling case without changing data placements. The profiled L1 cache</cell></row><row><cell cols="2">misses are reported in Table</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>output tensor ????: ? ? ? ? ? ? ? , where ? ? , ? ? , and ? ? are three tunable split parameters for tiling ? , ? , and ?. ? + ?? -1) (? ? + ?? -1) ? ? , where (? ? + ?? -1) and (? ? + ?? -1) are the unfolded dimensions, and ? ? is the only tunable split parameter for tiling ? .</figDesc><table><row><cell>? ? ? ? ? ? input tensor ???: ? ? ? ? ? ? ? ? ? ? ? ? ? ? (? ? weight tensor ??? : ? ? ? ? ? ? ? ? (?? )(?? )? ? ? ? ? ? , where ? ? ? and ? ? ? are two</cell></row><row><cell>tunable split parameters for tiling ? and ?.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>6 )): ? ? , ? ? , ? ? for tiling ?,? , ? of the output tensor, ? ? for tiling ? of the input tensor, ? ? ? , ? ?</figDesc><table><row><cell>padding</cell><cell cols="2">C2D</cell><cell>ReLU</cell></row><row><cell cols="4">complex operator to optimize layouts</cell></row><row><cell>initial layout state</cell><cell></cell><cell></cell><cell>loop state</cell></row><row><cell></cell><cell></cell><cell></cell><cell>random walk</cell></row><row><cell cols="2">Actor action sequence</cell><cell>feedback</cell><cell>Actors</cell></row><row><cell>final layout state</cell><cell></cell><cell></cell><cell>update state</cell></row><row><cell>Layout Agent</cell><cell></cell><cell>layout</cell><cell>Loop Agents</cell></row><row><cell>?</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>? ? ? ? ? ? ? ? ? ? for ?,</figDesc><table><row><cell>? ? ?</cell><cell>? ? ?</cell><cell>?</cell></row></table><note><p>? ? ? for ?, and ? ? ?</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>7.3.3 Case study:To understand how the joint tuning improves the loop performance, we perform loop optimization based on ? ?? ?, ???? , ? ? ? ? ?? ? ? , and ? ?</figDesc><table><row><cell></cell><cell>?</cell></row><row><cell>? ?</cell><cell>? ?</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 3 :</head><label>3</label><figDesc>Profiling results based on several layouts.</figDesc><table><row><cell cols="4">Layout (Conv &amp; Ker)</cell><cell cols="2">#Inst. #L1-lds #L1-mis #L1-sts Lat.</cell></row><row><cell cols="3">? ?? ? &amp; ????</cell><cell></cell><cell>509.4 166.4</cell><cell>9.7</cell><cell>103.6 0.34</cell></row><row><cell cols="3">? ??? &amp; ????</cell><cell></cell><cell>626.9 206.6</cell><cell>4.5</cell><cell>121.3 0.49</cell></row><row><cell cols="3">? ? ?? ?? ? ? &amp; ? ??</cell><cell cols="2">? ?? ???? 567.6 193.6</cell><cell>9.9</cell><cell>112.9 0.37</cell></row><row><cell>? ? ??</cell><cell>? ??</cell><cell cols="3">? ?? ? ? ? ? ? ? &amp; ... 550.5 174.3</cell><cell>3.9</cell><cell>106.2 0.25</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="9">CONCLUSION</head><p>In this paper, we propose ALT, a compiler that jointly performs graph-level data layout optimization and operator-level loop optimization for deep models. ALT provides a generic transformation module for low-cost layout and loop manipulation. It further integrates an auto-tuning module for bidirectional and unified layout and loop tuning. Experiments show that ALT outperforms state-ofthe-art vendor libraries and auto-tuning frameworks.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName><forename type="first">Mart?n</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th USENIX Symposium on Operating Systems Design and Implementation</title>
		<meeting>the 12th USENIX Symposium on Operating Systems Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Chameleon: Adaptive code optimization for expedited deep neural network compilation</title>
		<author>
			<persName><forename type="first">Prannoy</forename><surname>Byung Hoon Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Pilligundla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hadi</forename><surname>Yazdanbakhsh</surname></persName>
		</author>
		<author>
			<persName><surname>Esmaeilzadeh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08743</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">An asymptotic cost model for autoscheduling sparse tensor programs</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Ahrens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fredrik</forename><surname>Kjolstad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saman</forename><surname>Amarasinghe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.14947</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Compiler transformations for high-performance computing</title>
		<author>
			<persName><forename type="first">Susan</forename><forename type="middle">L</forename><surname>David F Bacon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><forename type="middle">J</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><surname>Sharp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="345" to="420" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A deep learning based cost model for automatic code optimization</title>
		<author>
			<persName><forename type="first">Riyadh</forename><surname>Baghdadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massinissa</forename><surname>Merouani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed-Hicham</forename><surname>Leghettas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamel</forename><surname>Abdous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taha</forename><surname>Arbaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karima</forename><surname>Benatchba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Machine Learning and Systems (MLSys)</title>
		<meeting>the 3rd Machine Learning and Systems (MLSys)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Tiramisu: A polyhedral compiler for expressing fast and portable code</title>
		<author>
			<persName><forename type="first">Riyadh</forename><surname>Baghdadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malek</forename><surname>Ben Romdhane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emanuele</forename><surname>Del Sozzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdurrahman</forename><surname>Akkas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patricia</forename><surname>Suriana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shoaib</forename><surname>Kamil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saman</forename><surname>Amarasinghe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE/ACM International Symposium on Code Generation and Optimization</title>
		<meeting>IEEE/ACM International Symposium on Code Generation and Optimization</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Loop transformations for restructuring compilers: the foundations</title>
		<author>
			<persName><forename type="first">Utpal</forename><surname>Banerjee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">nGraph-HE: a graph compiler for deep learning on homomorphically encrypted data</title>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Boemer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixing</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rosario</forename><surname>Cammarota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Casimir</forename><surname>Wierzynski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th ACM International Conference on Computing Frontiers (CF)</title>
		<meeting>the 16th ACM International Conference on Computing Frontiers (CF)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Xgboost: A scalable tree boosting system</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM International Conference on Knowledge Discovery and Data mining</title>
		<meeting>the 22nd ACM International Conference on Knowledge Discovery and Data mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">TVM: An automated end-to-end optimizing compiler for deep learning</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziheng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eddie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haichen</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meghan</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Ceze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI)</title>
		<meeting>eeding of the 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning to optimize tensor programs</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eddie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziheng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Krishnamurthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd Conference on Neural Information Processing Systems (NeurIPS)</title>
		<meeting>the 32nd Conference on Neural Information Processing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Chetlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cliff</forename><surname>Woolley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Vandermersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.0759</idno>
		<title level="m">Efficient primitives for deep learning</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cache-conscious structure layout</title>
		<author>
			<persName><forename type="first">M</forename><surname>Trishul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Chilimbi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">R</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><surname>Larus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)</title>
		<meeting>ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Compiler driven data layout optimization for regular/irregular array access patterns</title>
		<author>
			<persName><forename type="first">Doosan</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudeep</forename><surname>Pasricha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Issenin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikil</forename><surname>Dutt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunheung</forename><surname>Paek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunjun</forename><surname>Ko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGPLAN-SIGBED conference on Languages, compilers, and tools for embedded systems (LCTES)</title>
		<meeting>ACM SIGPLAN-SIGBED conference on Languages, compilers, and tools for embedded systems (LCTES)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Automatic generation of efficient sparse tensor format conversion routines</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fredrik</forename><surname>Kjolstad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saman</forename><surname>Amarasinghe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)</title>
		<meeting>the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automatic memory layout transformations to optimize spatial locality in parameterized loop nests</title>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Clauss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beno?t</forename><surname>Meister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="19" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An exploration of ARM system-level cache and GPU side channels</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Cronin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haining</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chase</forename><surname>Cotton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Computer Security Applications Conference (ACSAC)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Intel nGraph: An intermediate representation, compiler, and executor for deep learning</title>
		<author>
			<persName><forename type="first">Scott</forename><surname>Cyphers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Arjun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anahita</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jayaram</forename><surname>Bhiwandiwalla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Bobba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avijit</forename><surname>Brookhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Constable</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leona</forename><surname>Convey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omar</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><surname>Kanawi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.08058</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pretraining of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Tiny</surname></persName>
		</author>
		<author>
			<persName><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.10351</idno>
		<title level="m">Distilling BERT for natural language understanding</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">IOS: Inter-operator scheduler for CNN acceleration</title>
		<author>
			<persName><forename type="first">Yaoyao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gennady</forename><surname>Pekhimenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems (MLSys)</title>
		<meeting>Machine Learning and Systems (MLSys)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Building an on-chip deep learning memory hierarchy brick by brick</title>
		<author>
			<persName><forename type="first">Edo</forename><surname>Isak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sayeh</forename><surname>Vivancos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Sharify</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ameer</forename><surname>Ly-Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ciaran</forename><surname>Abdelhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milos</forename><surname>Bannon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Nikolic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><forename type="middle">Delmas</forename><surname>Mahmoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gennady</forename><surname>Lascorz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><forename type="middle">Moshovos</forename><surname>Pekhimenko</surname></persName>
		</author>
		<author>
			<persName><surname>Boveda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems (MLSys)</title>
		<meeting>Machine Learning and Systems (MLSys)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cortex: A compiler for recursive deep learning models</title>
		<author>
			<persName><forename type="first">Pratik</forename><surname>Fegade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Mowry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Machine Learning and Systems (MLSys)</title>
		<meeting>the 3rd Machine Learning and Systems (MLSys)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The CoRa tensor compiler: Compilation for ragged tensors with minimal padding</title>
		<author>
			<persName><forename type="first">Pratik</forename><surname>Fegade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Mowry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems (MLSys)</title>
		<meeting>Machine Learning and Systems (MLSys)</meeting>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bohan</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wuwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junru</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruihang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cody</forename><forename type="middle">Hao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.04296</idno>
		<title level="m">An abstraction for automatic tensorized program optimization</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An empirical study of the effect of source-level loop transformations on compiler stability</title>
		<author>
			<persName><forename type="first">Zhangxiaowen</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Szaday</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zehra</forename><surname>Sura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neftali</forename><surname>Watkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saeed</forename><surname>Maleki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Padua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Veidenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandru</forename><surname>Nicolau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of ACM on Programming Languages</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="29" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">XNNPACK: Highly optimized library of floating-point neural network inference operators for ARM, WebAssembly, and x86 platforms</title>
		<author>
			<persName><surname>Google</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Loop transformation recipes for code generation and autotuning</title>
		<author>
			<persName><forename type="first">Mary</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacqueline</forename><surname>Chame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewook</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabe</forename><surname>Rudy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malik</forename><forename type="middle">Murtaza</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Languages and Compilers for Parallel Computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal features with 3d residual networks for action recognition</title>
		<author>
			<persName><forename type="first">Kensho</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hirokatsu</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutaka</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision Workshops (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision Workshops (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A survey and critique of multiagent deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Hernandez-Leal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bilal</forename><surname>Kartal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autonomous Agents and Multi-Agent Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="750" to="797" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Gritsenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.03458</idno>
		<title level="m">Video diffusion models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">MKL-DNN</title>
		<author>
			<persName><surname>Intel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-06">2017. June-2022</date>
			<biblScope unit="volume">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName><forename type="middle">Openvino</forename><surname>Intel</surname></persName>
		</author>
		<author>
			<persName><surname>Toolkit</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019-06">2019. June-2022</date>
			<biblScope unit="volume">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">TASO: Optimizing deep learning computation with automatic generation of graph substitutions</title>
		<author>
			<persName><forename type="first">Zhihao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oded</forename><surname>Padon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Warszawski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Aiken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM Symposium on Operating Systems Principles (SOSP)</title>
		<meeting>the 27th ACM Symposium on Operating Systems Principles (SOSP)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Reduction of cache coherence overhead by compiler data layout and loop transformation</title>
		<author>
			<persName><forename type="first">Y-J</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dietz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Languages and Compilers for Parallel Computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Enhancing spatial locality via data layout optimizations</title>
		<author>
			<persName><forename type="first">Mahmut</forename><surname>Kandemir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alok</forename><surname>Choudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jagannathan</forename><surname>Ramanujam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nagaraj</forename><surname>Shenoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prithviraj</forename><surname>Banerjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Parallel Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">MLIR: Scaling compiler infrastructure for domain specific computation</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Lattner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uday</forename><surname>Bondhugula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacques</forename><surname>Pienaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">River</forename><surname>Riddle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatiana</forename><surname>Shpeisman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Vasilache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Zinenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/ACM International Symposium on Code Generation and Optimization</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">XLA: Tensorflow, compiled</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TensorFlow Dev Summit</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Automatic horizontal fusion for GPU kernels</title>
		<author>
			<persName><forename type="first">Ao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bojian</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gennady</forename><surname>Pekhimenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/ACM International Symposium on Code Generation and Optimization</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Optimizing memory efficiency for deep convolutional neural networks on GPUs</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srimat</forename><surname>Chakradhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huiyang</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference for High Performance Computing, Networking, Storage and Analysis</title>
		<meeting>the 16th International Conference for High Performance Computing, Networking, Storage and Analysis</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Analytical characterization and design space exploration for optimization of CNNs</title>
		<author>
			<persName><forename type="first">Rui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aravind</forename><surname>Sukumaran-Rajam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atanas</forename><surname>Rountev</surname></persName>
		</author>
		<author>
			<persName><surname>Sadayappan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</title>
		<meeting>the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">CoCoPIE: Making mobile ai sweet as pie-compression-compilation co-design goes a long way</title>
		<author>
			<persName><forename type="first">Shaoshan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanzhi</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.06700</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Optimizing CNN model inference on CPUs</title>
		<author>
			<persName><forename type="first">Yizhi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruofei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vin</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yida</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of USENIX Annual Technical Conference (ATC)</title>
		<meeting>eeding of USENIX Annual Technical Conference (ATC)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Data layout transformation for enhancing data locality on NUCA chip multiprocessors</title>
		<author>
			<persName><forename type="first">Qingda</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christophe</forename><surname>Alias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uday</forename><surname>Bondhugula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Henretty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sriram</forename><surname>Krishnamoorthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jagannathan</forename><surname>Ramanujam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atanas</forename><surname>Rountev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ponnuswamy</forename><surname>Sadayappan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongjian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibo</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">18th International Conference on Parallel Architectures and Compilation Techniques</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Rammer: Enabling holistic deep learning compiler optimizations with rtasks</title>
		<author>
			<persName><forename type="first">Lingxiao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jilong</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youshan</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenxiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lintao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidong</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th USENIX Symposium on Operating Systems Design and Implementation (OSDI)</title>
		<meeting>the 14th USENIX Symposium on Operating Systems Design and Implementation (OSDI)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Data-driven spatial locality</title>
		<author>
			<persName><forename type="first">Svetozar</forename><surname>Miucin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Fedorova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Memory Systems (MEMSYS)</title>
		<meeting>the International Symposium on Memory Systems (MEMSYS)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Understanding the impact of memory access patterns in intel processors</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Alaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haque</forename><surname>Monil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seyong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">S</forename><surname>Vetter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allen</forename><forename type="middle">D</forename><surname>Malony</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Workshop on Memory Centric High Performance Computing</title>
		<imprint>
			<biblScope unit="issue">MCHPC</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Advanced compiler design implementation</title>
		<author>
			<persName><forename type="first">Steven</forename><surname>Muchnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>Morgan kaufmann</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">DNNFusion: accelerating deep neural networks execution with advanced operator fusion</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiexiong</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gagan</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation (PLDI)</title>
		<meeting>the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation (PLDI)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title/>
		<author>
			<persName><surname>Nvidia</surname></persName>
		</author>
		<author>
			<persName><surname>Cutlass</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-06">2017. June-2022</date>
			<biblScope unit="volume">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title/>
		<author>
			<persName><surname>Nvidia</surname></persName>
		</author>
		<author>
			<persName><surname>Tensorrt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-06">2017. June-2022</date>
			<biblScope unit="volume">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd Conference on Neural Information Processing Systems (NeurIPS)</title>
		<meeting>the 33rd Conference on Neural Information Processing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A flexible approach to autotuning multi-pass machine learning compilers</title>
		<author>
			<persName><forename type="first">Phitchaya</forename><surname>Mangpo Phothilimthana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Sabne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Sarda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Srinivasa Murthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christof</forename><surname>Angermueller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Burrows</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudip</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ketan</forename><surname>Mandke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rezsa</forename><surname>Farahani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">30th International Conference on Parallel Architectures and Compilation Techniques (PACT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Halide: a language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ragan-Kelley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connelly</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fr?do</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saman</forename><surname>Amarasinghe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm Sigplan Notices</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="519" to="530" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Structure layout optimization for multithreaded programs</title>
		<author>
			<persName><forename type="first">Easwaran</forename><surname>Raman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Hundt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandya</forename><surname>Mannarswamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Code Generation and Optimization</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Relay: A new IR for machine learning frameworks</title>
		<author>
			<persName><forename type="first">Jared</forename><surname>Roesch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Lyubomirsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Logan</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Pollock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marisa</forename><surname>Kirisame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Tatlock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd ACM International Workshop on Machine Learning and Programming Languages (MAPL)</title>
		<meeting>the 2nd ACM International Workshop on Machine Learning and Programming Languages (MAPL)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">Nadav</forename><surname>Rotem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Fix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saleem</forename><surname>Abdulrasool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Garret</forename><surname>Catron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Summer</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Dzhabarov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hegeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meghan</forename><surname>Lele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Levenstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.00907</idno>
		<title level="m">Graph lowering compiler techniques for neural networks</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filip</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleg</forename><surname>Klimov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06347</idno>
		<title level="m">Proximal policy optimization algorithms</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Data layout optimization for portable performance</title>
		<author>
			<persName><forename type="first">Kamal</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Karlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Keasler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">R</forename><surname>Mcgraw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Parallel Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Nimble: Efficiently compiling dynamic neural networks for model inference</title>
		<author>
			<persName><forename type="first">Haichen</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Roesch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vin</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Tatlock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yida</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Machine Learning and Systems (MLSys)</title>
		<meeting>the 3rd Machine Learning and Systems (MLSys)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Integrating data layout transformations with the polyhedral model</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Shirako</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Workshop on Polyhedral Compilation Techniques (IMPACT)</title>
		<meeting>International Workshop on Polyhedral Compilation Techniques (IMPACT)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">An affine scheduling framework for integrating data layout and loop transformations</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Shirako</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Languages and Compilers for Parallel Computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Value learning for throughput optimization of deep learning workloads</title>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horace</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugh</forename><surname>Leather</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Machine Learning and Systems (MLSys)</title>
		<meeting>the 3rd Machine Learning and Systems (MLSys)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Value learning for throughput optimization of deep learning workloads</title>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horace</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugh</forename><surname>Leather</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems (MLSys)</title>
		<meeting>Machine Learning and Systems (MLSys)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">CODE: Compiler-based neuron-aware ensemble training</title>
		<author>
			<persName><forename type="first">Thanapon</forename><surname>Ettore Mg Trainiti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Noraset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Demeter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><surname>Campanoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Machine Learning and Systems (MLSys)</title>
		<meeting>the 3rd Machine Learning and Systems (MLSys)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Unity: Accelerating DNN training through joint optimization of algebraic transformations and parallelization</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Unger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sina</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandeep</forename><surname>Baines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Efrain</forename><forename type="middle">Quintero</forename><surname>Narvaez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinay</forename><surname>Ramakrishnaiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nirmal</forename><surname>Prajapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pat</forename><surname>Mccormick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamaludin</forename><surname>Mohd-Yusof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th USENIX Symposium on Operating Systems Design and Implementation</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Joint scheduling and layout optimization to enable multi-level vectorization</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Vasilache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Meister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muthu</forename><surname>Baskaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Lethin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IMPACT</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<pubPlace>Paris, France</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Vasilache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Zinenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theodoros</forename><surname>Theodoridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">S</forename><surname>Moses</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sven</forename><surname>Verdoolaege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04730</idno>
		<title level="m">Tensor comprehensions: Framework-agnostic high-performance machine learning abstractions</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">PET: Optimizing tensor programs with partially equivalent transformations and automated corrections</title>
		<author>
			<persName><forename type="first">Haojie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jidong</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zixuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shizhi</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liyan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiyuan</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanyong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihao</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th USENIX Symposium on Operating Systems Design and Implementation (OSDI)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Unifying data, model and hybrid parallelism in deep learning via tensor tiling</title>
		<author>
			<persName><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chien-Chin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.04170</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Tuna: A static analysis approach to optimizing deep neural networks</title>
		<author>
			<persName><forename type="first">Yao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vin</forename><surname>Sharma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.14641</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">UNIT: Unifying tensorized instruction compilation</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Animesh</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Nowatzki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/ACM International Symposium on Code Generation and Optimization</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Bolt: Bridging the gap between auto-tuners and hardware-native performance</title>
		<author>
			<persName><forename type="first">Jiarong</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yibo</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems (MLSys)</title>
		<meeting>Machine Learning and Systems (MLSys)</meeting>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Equality saturation for tensor graph superoptimization</title>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phitchaya</forename><surname>Mangpo Phothilimtha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remy</forename><surname>Yisu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudip</forename><surname>Willsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacques</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><surname>Pienaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Machine Learning and Systems (MLSys)</title>
		<meeting>the 3rd Machine Learning and Systems (MLSys)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruihang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junru</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Ceze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.04606</idno>
		<title level="m">SparseTIR: Composable abstractions for sparse compilation in deep learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Lorien: Efficient deep learning workloads delivery</title>
		<author>
			<persName><forename type="first">Cody</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Xingjian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haichen</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yida</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM Symposium on Cloud Computing (SoCC)</title>
		<meeting>ACM Symposium on Cloud Computing (SoCC)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Optimizing FPGA-based accelerator design for deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangyu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijin</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingjun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Cong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM/SIGDA International Symposium on Field-Programmable Gate Arrays (FPGA)</title>
		<meeting>ACM/SIGDA International Symposium on Field-Programmable Gate Arrays (FPGA)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Automatic partition-based operator fusion through layer by layer optimization</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruijie</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaochuang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deshi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuefeng</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems (MLSys)</title>
		<meeting>Machine Learning and Systems (MLSys)<address><addrLine>Apollo</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">AKG: automatic kernel generation for neural processing units using polyhedral transformations</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bojie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation (PLDI)</title>
		<meeting>the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation (PLDI)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">DietCode: Automatic optimization for dynamic tensor programs</title>
		<author>
			<persName><forename type="first">Bojian</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziheng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cody</forename><surname>Hao Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haichen</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Fromm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gennady</forename><surname>Pekhimenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems (MLSys)</title>
		<meeting>Machine Learning and Systems (MLSys)</meeting>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Ansor: generating high-performance tensor programs for deep learning</title>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengfan</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minmin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cody</forename><surname>Hao Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ameer</forename><surname>Haj-Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danyang</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koushik</forename><surname>Sen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th</title>
		<meeting>the 14th</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<author>
			<persName><forename type="first">Zhiying</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiafan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongding</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoliang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haipeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Wang</surname></persName>
		</author>
		<title level="m">Guihai Chen USENIX Symposium on Operating Systems Design and Implementation (OSDI)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Alpa: Automating inter-and Intra-Operator parallelism for distributed deep learning</title>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghao</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danyang</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th USENIX Symposium on Operating Systems Design and Implementation (OSDI)</title>
		<meeting><address><addrLine>Carlsbad, CA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2022-07">July 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Tenset: A large-scale program performance dataset for learned tensor compilers</title>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruochen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junru</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ameer</forename><surname>Haj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (NeuIPS)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">OLLIE: Derivation-based tensor program optimizer</title>
		<author>
			<persName><forename type="first">Liyan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haojie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jidong</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muyan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zixuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tuowei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shizhi</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kezhao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihao</forename><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.02025</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Deep-learning model sparsity via tensor-with-sparsity-attribute</title>
		<author>
			<persName><forename type="first">Ningxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanlu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingxiao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><surname>Sparta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th USENIX Symposium on Operating Systems Design and Implementation (OSDI)</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">AMOS: enabling automatic mapping for tensor computations on spatial accelerators with hardware abstraction</title>
		<author>
			<persName><forename type="first">Size</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renze</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anjiang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yicheng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qin</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingyang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual International Symposium on Computer Architecture</title>
		<meeting>the 49th Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="874" to="887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">FlexTensor: An automatic schedule exploration and optimization framework for tensor computation on heterogeneous system</title>
		<author>
			<persName><forename type="first">Size</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renze</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiwen</forename><surname>Sheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</title>
		<meeting>the 25th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">AStitch: enabling a new multi-dimensional optimization space for memory-intensive ml training and inference on modern simt architectures</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanda</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengzhan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoping</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feiwen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jidong</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</title>
		<meeting>the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</meeting>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">ROLLER: Fast and efficient tensor compilation for deep learning</title>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruofan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijia</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanbin</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jilong</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingxiao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqing</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th USENIX Symposium on Operating Systems Design and Implementation (OSDI)</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
