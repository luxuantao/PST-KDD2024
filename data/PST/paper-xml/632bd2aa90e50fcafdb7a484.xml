<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Seeking Diverse Reasoning Logic: Controlled Equation Expression Generation for Solving Math Word Problems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yibin</forename><surname>Shen</surname></persName>
							<email>shenyibin@meituan.com</email>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Informatics</orgName>
								<orgName type="institution">Kyoto University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qianying</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Informatics</orgName>
								<orgName type="institution">Kyoto University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhuoyuan</forename><surname>Mao</surname></persName>
							<email>zhuoyuanmao@nlp.ist.i.kyoto-u.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Informatics</orgName>
								<orgName type="institution">Kyoto University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhen</forename><surname>Wan</surname></persName>
							<email>zhenwan@nlp.ist.i.kyoto-u.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Informatics</orgName>
								<orgName type="institution">Kyoto University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fei</forename><surname>Cheng</surname></persName>
							<email>feicheng@i.kyoto-u.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Informatics</orgName>
								<orgName type="institution">Kyoto University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Informatics</orgName>
								<orgName type="institution">Kyoto University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><surname>Meituan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Informatics</orgName>
								<orgName type="institution">Kyoto University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Seeking Diverse Reasoning Logic: Controlled Equation Expression Generation for Solving Math Word Problems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>To solve Math Word Problems, human students leverage diverse reasoning logic that reaches different possible equation solutions. However, the mainstream sequence-tosequence approach of automatic solvers aims to decode a fixed solution equation supervised by human annotation. In this paper, we propose a controlled equation generation solver by leveraging a set of control codes to guide the model to consider certain reasoning logic and decode the corresponding equations expressions transformed from the human reference. The empirical results suggest that our method universally improves the performance on single-unknown (Math23K) and multipleunknown (DRAW1K, HMWP) benchmarks, with substantial improvements up to 13.2% accuracy on the challenging multiple-unknown datasets. * This denotes equal contribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Diverse Reasoning Logic Expression Bias during Decoding</head><p>Problem: 1 pan pizza and 2 cheeseburgers provide 2860 calories. 2 pan pizzas and 1 cheeseburger provide 2990 calories. Find the caloric content of each item.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Solving Math Word Problems (MWPs) is the task of obtaining mathematical solutions from natural language text descriptions. Recent studies leverage sequence-to-sequence (seq2seq) neural networks (NNs) for solving MWPs, which take in the text as the input and decode the corresponding humanannotated equation reference, which can further calculate the answer value <ref type="bibr" target="#b19">(Wang et al., 2017)</ref>. While promising results have been reported for singleunknown variable problems by designing task specialized encoder and decoder architectures <ref type="bibr">(Wang et al., 2018b</ref><ref type="bibr" target="#b18">(Wang et al., , 2019;;</ref><ref type="bibr" target="#b20">Xie and Sun, 2019;</ref><ref type="bibr" target="#b7">Liu et al., 2019;</ref><ref type="bibr" target="#b1">Guan et al., 2019;</ref><ref type="bibr">Zhang et al., 2020b,a;</ref><ref type="bibr" target="#b12">Shen and Jin, 2020)</ref>, using pre-trained models <ref type="bibr" target="#b14">(Tan et al., 2021;</ref><ref type="bibr" target="#b5">Liang et al., 2021)</ref> and leveraging auxiliary tasks <ref type="bibr" target="#b6">(Liu et al., 2020;</ref><ref type="bibr" target="#b11">Shen et al., 2021;</ref><ref type="bibr" target="#b4">Li et al., 2022)</ref>, various studies for a more challenging setting, MWPs with multiple-unknowns have recently been developed <ref type="bibr" target="#b15">(Upadhyay and Chang, 2017;</ref><ref type="bibr" target="#b10">Qin et al., 2020;</ref><ref type="bibr" target="#b0">Cao et al., 2021;</ref><ref type="bibr" target="#b9">Qin et al., 2021)</ref>. For verse mathematical reasoning, which is especially common for human students in multiple-unknown problems and complex single-unknown problems. Meanwhile, directly introducing diverse equation expressions to the seq2seq framework in a data augmentation manner could further aggravate the issue of expression bias, which refers to the discrepancy between the annotated equation expression and the model's correct prediction expression. As shown in the middle of Figure <ref type="figure">1</ref>, even if the model makes the correct prediction of the problem, the training loss accumulated by diverse expressions could be enormous. <ref type="bibr">Wang et al. (2018a)</ref> propose an equation normalization that reorders the variables in the equations as close as possible to their order in the input text. While their method could reduce the expression bias issue, they ignore the inherent diverse mathematical reasoning and limits to considering single-unknown problems.</p><p>Enlightened by recent methods in controlled text generation, which uses a control code to influence the style and topic of subsequent generated text <ref type="bibr" target="#b2">(Keskar et al., 2019;</ref><ref type="bibr" target="#b13">Shin et al., 2020)</ref>, we propose a new training paradigm, where a control code guides the decoding process to consider one type of mathematical reasoning logic and decode the corresponding equation expression. As shown in the bottom Figure <ref type="figure">1</ref>, the &lt;sol&gt; control code guides the model to consider the direct solution of each individual unknown x 1 and x 2 . Not only can it reduce the expression bias problem since the control code can provide guidance for the reasoning logic, but also training on the diverse equation expressions guided by the control codes can lead to better interpretation of the MWPs by considering diverse reasoning logic. We design various control codes for both single-unknown and multiple-unknown settings to allow the model to understand different reasoning orders. We conduct experiments on a single-unknown benchmark Math23K and two multiple-unknown benchmarks DRAW1K and HMWP. Experimental results show that our method improves the performance of both settings, with a more significant improvement in the challenging multiple-unknown setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>For each math word problem holding an original equation set (e 1 , e 2 , ...), we generate new equation expressions based on five types of diverse mathematical reasoning logic considering the ordering logic of given variables {n i } and unknown variables {x j }. i and j denote the ordered indices that the variables appear in the text. We then assign a corresponding control code to the equation expressions. The MWP solving model takes in the text and control code, and then is trained to predict the corresponding equation expression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Control Codes</head><p>We consider the diverse mathematical reasoning logic in two aspects. The first aspect considers diverse reasoning orders of given variables, which reflects in the diverse expressions of the commutative law and solution form. For example, n 1 * x 1 = n 2 could be transformed to the solution form x 1 = n 2 /n 1 which does not effect the mathematical equivalency. This approach is valid for both multi-unknown and single-unknown problems. The second aspect considers diverse reasoning orders of unknown variables, which reflects in the diverse expressions of equivalent equation sets. For example, swapping the equation order in the equation set does not affect the mathematical equivalency. This approach is valid for multi-unknown problems.</p><p>We preprocess the equation annotations with Sympy <ref type="bibr" target="#b8">(Meurer et al., 2017)</ref> so that they follow a predefined order similar to <ref type="bibr">Wang et al. (2018a)</ref>. Then we generate different types of equation expressions based on these preprocessed equations.</p><p>For the first aspect, we consider three types of diverse equation expressions.</p><p>? Commutative Law of Addition &lt;add&gt; We traverse the equation in prefix order, and swap the left and right subtrees of the addition operators. For example, x 1 = n 1 + n 2 + n 3 would be swapped two times. We first swap the two subtrees n 1 and n 2 of the first addition operator to x 1 = n 2 + n 1 + n 3 , and then swap the two subtrees n 2 + n 1 and n 3 of the second operator to x 1 = n 3 + n 2 + n 1 .</p><p>? Commutative Law of Multiplication &lt;mul&gt; Similarly, we traverse the equation in prefix order, and swap the left and right subtrees of the multiplication operators. For example, from</p><formula xml:id="formula_0">x 1 = n 1 * n 2 * n 3 to x 1 = n 3 * n 2 * n 1 .</formula><p>? Solution Form &lt;sol&gt; We consider a mathematical reasoning method that directly consid- ers the solution of each unknown variable. For example, from</p><formula xml:id="formula_1">n 1 /x 1 = n 2 to x 1 = n 1 /n 2 .</formula><p>For the second aspect, we consider two types of diverse equation expressions.</p><p>? Equation Swapping &lt;equ&gt; We swap the multiple-unknown equations in sequential order, which means given a list of equations (e 1 , e 2 , ...e n ), we swap them to the order (e n , e 1 , e 2 , ...e n-1 ).</p><p>? Unknown Variable Swapping &lt;var&gt; Similarly, we swap the multiple unknown variables in sequential order, which means given a list of unknown variables in the equation (x 1 , x 2 , ...x n ), we change the correspondence between them and the unknown variables in the original question, that the unknown variables in the new equation (x s 1 , x s 2 , ...x s n ) follows x s 1 denotes x n and x s i denotes x i-1 for other indices. For example, from</p><formula xml:id="formula_2">n 1 * x 1 + n 2 * x 2 = 0 to n 1 * x 2 + n 2 * x 1 = 0.</formula><p>To incorporate the control codes for guiding the equation expression decoding, we follow studies in controlled text generation <ref type="bibr" target="#b2">(Keskar et al., 2019)</ref> and append a special token to the encoder input. We use an independent special token for each expression category as the control code, such as &lt;add&gt;, including &lt;orig&gt; for the example of the original equation expression. We use the prediction of the original equation expression control code &lt;orig&gt; for test inference since it has the most training examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">MWP solving model</head><p>Solving multiple-unknown problems usually requires equation sets, which are challenging to generate. To tackle this problem, we follow the decoding target paradigm of <ref type="bibr" target="#b10">Qin et al. (2020)</ref>, which introduces a Universal Expression Tree (UET) to represent multiple-unknown equation sets uniformly as an expression tree by using a dummy node as the head of the equation set. UET can also handle single-unknown problems in a unified manner.</p><p>For the solver model, we use two strong baseline models for experiments. For the first model, we leverage a seq2seq pre-trained language model BART <ref type="bibr" target="#b3">(Lewis et al., 2020;</ref><ref type="bibr" target="#b11">Shen et al., 2021)</ref> as the solver model, which has reported promising results for text generation tasks. The encoder takes in the textual input and generates high-quality representations of the problem text. The decoder generates the UET based on these representations.</p><p>For the second model, we follow <ref type="bibr" target="#b4">Li et al. (2022)</ref> and use BERT-GTS as MWP solving model. We leverage the contextual pre-trained language model BERT as the encoder, and use a Goal-driven treestructured MWP solver (GTS) <ref type="bibr" target="#b20">(Xie and Sun, 2019)</ref> based on Long-Short-Term-Memory networks (LSTM) as the decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>We evaluate our proposed method on one single-unknown Chinese dataset Math23K <ref type="bibr" target="#b19">(Wang et al., 2017)</ref> and two multiple-unknown datasets, DRAW1K (Upadhyay and Chang, 2017) in English and HMWP <ref type="bibr" target="#b10">(Qin et al., 2020)</ref> in Chinese. We show the statistics of overall data size, single and multiple unknown problem size, and the usage of control codes of the datasets in Figure <ref type="figure" target="#fig_0">2</ref>. The five control code methods are enumerated for each example to generate new equation expressions. While &lt;sol&gt; is applicable for both single-unknown and multiple-unknown problems, the annotation schema in Math23K uses the Solution Form, which corresponds to &lt;orig&gt;, that no more further equation expressions are generated for &lt;sol&gt;. We use from 1.87 to 6.15 times of original data examples size for training on the three datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Math23K DRAW HMWP GTS <ref type="bibr" target="#b20">(Xie and Sun, 2019)</ref> 75.6 39.9 44.6 G2T <ref type="bibr">(Zhang et al., 2020b)</ref> 77.4 41.0 45.1 SAU-Solver <ref type="bibr" target="#b10">(Qin et al., 2020</ref> Table <ref type="table">2</ref>: Ablation Study on MWP datasets. + &lt;control code&gt; denotes using only one control code. All denotes using all control codes. -code denotes using the examples as data augmentation without control codes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>We show our experimental results on the three datasets in Table <ref type="table" target="#tab_1">1</ref>. We compare our results with three models: GTS uses an LSTM encoder and decoder, which considers tree structure information during decoding; G2T uses a Graph Neural Network that considers quantity information as the encoder and similar tree decoder; SAU-Solver introduces a semantically-alignment to the target vocabulary of the equations to improve the GTS decoder. As we can see, our method outperforms the baseline for both models on all datasets. The accuracy of different models gains improvement from 1.8% to 1.9% for single-unknown problems and from 4.8% to 13.2% for multiple-unknown problems. The results demonstrate the effectiveness of our method, especially for multiple-unknown problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ablation Study</head><p>We conduct further analysis on the more effective model BERT-GTS. In Table <ref type="table">2</ref>, we show the ablation study using different control codes. As shown in the  ticularly effective for all datasets since it has an extensive example for each dataset. Using all control codes together further boosts the model performance by providing diverse mathematical reasoning logic as guidance.</p><p>We also show the results of removing the control codes and solely using the diverse equation expressions in a data augmentation manner in Table 2. Solely introducing diverse mathematical reasoning logic can also improve the model performance compared to the baseline model. However, the expression bias problem limits the performance since training loss could accumulate for diverse equation expressions. By incorporating control codes to guide the decoding process, our method can consider diverse reasoning logic and reduce the expression bias problem in the meantime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Study on Variable Size</head><p>We show the performance on different given variable sizes of the BERT-GTS baseline model and our controlled equation generation method on Math23K in Figure <ref type="figure" target="#fig_1">3</ref>. As the variable size grows, the problem becomes more complex, and the performance gap between our method and the baseline becomes more significant. Our method can incorporate diverse equation expressions to help the model learn mathematical reasoning logic.</p><p>In this paper, we introduce diverse mathematical reasoning logic to the seq2seq MWP solver framework using five control codes to guide the solver to predict the corresponding equation expression in a controlled equation generation manner. The approach allows the solver to benefit from diverse reasoning logic beyond the human-annotated fixed solution equation. Meanwhile, the controlled equation generation training paradigm reduces the expression bias problem caused by diverse equation expressions. Experimental results show the effectiveness of our method, outperforming strong baselines on single-unknown (Math23K) and multipleunknown (DRAW1K, HMWP) datasets.</p><p>There exists other controlled equation generation strategies such as such as adding brackets to merge subtraction terms (e.g. from n 1 -n 2 -n 3 to n 1 -(n 2 +n 3 )) or combining current control codes to form a new type of equation expression, which potentially could lead to more than 10 controlled equation generation strategies. In addition, considering the prediction of multiple control codes in addition to &lt;orig&gt; could further improve the performance results, for example, applying ensemble learning methods such as major voting, or designing rankers to choose a optimal prediction among the prediction of multiple control codes. These problems could be considered as future work of this study.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Statistics of datasets and the usage of control codes.</figDesc><graphic url="image-1.png" coords="3,116.22,70.87,362.81,90.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Performance on different given variable sizes.</figDesc><graphic url="image-2.png" coords="4,338.88,222.72,152.79,123.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Results on MWP datasets. ? denotes our implementation results.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>)</cell><cell>-</cell><cell>39.2</cell><cell>44.8</cell></row><row><cell></cell><cell cols="3">BART  ? (Shen et al., 2021)</cell><cell>80.4</cell><cell>32.1</cell><cell>41.5</cell></row><row><cell></cell><cell cols="3">BERT-GTS  ? (Li et al., 2022)</cell><cell>82.6</cell><cell>42.2</cell><cell>48.3</cell></row><row><cell></cell><cell cols="2">Controlled BART</cell><cell></cell><cell>82.3</cell><cell>45.3</cell><cell>47.9</cell></row><row><cell></cell><cell cols="3">Controlled BERT-GTS</cell><cell>84.4</cell><cell>50.2</cell><cell>53.1</cell></row><row><cell>Model</cell><cell cols="3">Math23K DRAW HMWP</cell></row><row><cell>BERT-GTS</cell><cell>82.6</cell><cell>42.2</cell><cell>48.3</cell></row><row><cell>+ &lt;add&gt;</cell><cell>83.0</cell><cell>46.8</cell><cell>50.8</cell></row><row><cell>+ &lt;mul&gt;</cell><cell>83.3</cell><cell>47.6</cell><cell>51.9</cell></row><row><cell>+ &lt;sol&gt;</cell><cell>-</cell><cell>46.3</cell><cell>50.5</cell></row><row><cell>+ &lt;equ&gt;</cell><cell>-</cell><cell>48.3</cell><cell>50.1</cell></row><row><cell>+ &lt;var&gt;</cell><cell>-</cell><cell>47.4</cell><cell>50.1</cell></row><row><cell>All</cell><cell>84.4</cell><cell>50.2</cell><cell>53.1</cell></row><row><cell>-code</cell><cell>83.3</cell><cell>49.6</cell><cell>49.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Table, using each control code individually can improve the model's prediction. &lt;mul&gt; is par-</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://huggingface.co/yechen/bert-base-chinese</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Experimental Details</head><p>We evaluate Math23K on the standard train test setting. DRAW1K and HMWP are evaluated by 5-cross validation.</p><p>For Math23K and DRAW1K, we use the bertbase pre-trained encoder. For HMWP, we use the pre-trained encoder that could be found here 1 .</p><p>For Math23K, the max text length is 256, the max equation decoding length is 45, the batch size is 16 and the epochs number is 50. We use AdamW with a learning rate of 5e-5.</p><p>For DRAW1K, the max text length is 256, the max equation decoding length is 32, the batch size is 16 and the epochs number is 50. We use AdamW with a learning rate of 5e-5.</p><p>For HMWP, the max text length is 1024, the max equation decoding length is 100, the batch size is 8 and the epochs number is 50. We use AdamW with a learning rate of 5e-5.</p><p>Experiments are conducted on NVIDIA 3090 and A100(80G). The runtime for the experiments is around 6 hours.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A bottom-up dag structure extraction model for math word problems</title>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="39" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An improved coarse-to-fine method for solving generation tasks</title>
		<author>
			<persName><forename type="first">Wenyv</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangzhi</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the The 17th Annual Workshop of the Australasian Language Technology Association</title>
		<meeting>the The 17th Annual Workshop of the Australasian Language Technology Association<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Australasian Language Technology Association</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="178" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Varshney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.05858</idno>
		<title level="m">Ctrl: A conditional transformer language model for controllable generation</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.703</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">Marjan Ghazvininejad,. 2020</date>
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Seeking patterns, not just memorizing procedures: Contrastive learning for solving math word problems</title>
		<author>
			<persName><forename type="first">Zhongli</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenxuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunbo</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL 2022</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2486" to="2496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Zhenwen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangliang</forename><surname>Zhang</surname></persName>
		</author>
		<title level="m">Mwp-bert: A strong baseline for math word problems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Reverse operation based data augmentation for solving math word problems</title>
		<author>
			<persName><forename type="first">Qianying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyu</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daisuke</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.01556</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Tree-structured decoding for solving math word problems</title>
		<author>
			<persName><forename type="first">Qianying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyv</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daisuke</forename><surname>Kawahara</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1241</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2370" to="2379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sympy: symbolic computing in python</title>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Meurer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">P</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Paprocki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ond?ej</forename><surname>?ert?k</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sergey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Kirpichev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Rocklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergiu</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">K</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sartaj</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thilina</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Rathnayake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">E</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">P</forename><surname>Granger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harsh</forename><surname>Bonazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivam</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fredrik</forename><surname>Vats</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">J</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><forename type="middle">R</forename><surname>Curry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?t?p?n</forename><surname>Terrel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashutosh</forename><surname>Rou?ka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isuru</forename><surname>Saboo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumith</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><surname>Kulal</surname></persName>
		</author>
		<idno type="DOI">10.7717/peerj-cs.103</idno>
	</analytic>
	<monogr>
		<title level="j">PeerJ Computer Science</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Robert Cimrman, and Anthony Scopatz</publisher>
		</imprint>
	</monogr>
	<note>3:e103</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural-symbolic solver for math word problems with auxiliary tasks</title>
		<author>
			<persName><forename type="first">Jinghui</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yining</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.456</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5870" to="5881" />
		</imprint>
	</monogr>
	<note>Long Papers). Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semantically-aligned universal tree-structured solver for math word problems</title>
		<author>
			<persName><forename type="first">Jinghui</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihui</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rumin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.309</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3780" to="3789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generate &amp; rank: A multi-task framework for math word problems</title>
		<author>
			<persName><forename type="first">Jianhao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2021</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2269" to="2279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Solving math word problems with multi-encoders and multi-decoders</title>
		<author>
			<persName><forename type="first">Yibin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheqing</forename><surname>Jin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.coling-main.262</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain (Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2924" to="2934" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">AutoPrompt: Eliciting knowledge from language models with automatically generated prompts</title>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasaman</forename><surname>Razeghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Investigating math word problems using pretrained multilingual language models</title>
		<author>
			<persName><forename type="first">Minghuan</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingxiao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Annotating derivations: A new evaluation strategy and dataset for algebra word problems</title>
		<author>
			<persName><forename type="first">Shyam</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="494" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Translating a math word problem to a expression tree</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojiang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1132</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1064" to="1069" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mathdqn: Solving arithmetic word problems via deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianli</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingkuan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng Tao</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Template-based math word problem solvers with recursive neural networks</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianli</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><forename type="middle">Tian</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng Tao</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7144" to="7151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep neural solver for math word problems</title>
		<author>
			<persName><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1088</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="845" to="854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A goal-driven tree-structured neural model for math word problems</title>
		<author>
			<persName><forename type="first">Zhipeng</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shichao</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5299" to="5305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Jie Shao, and Qianru Sun. 2020a. Teacher-student networks with multiple decoders for solving math word problem</title>
		<author>
			<persName><forename type="first">Jipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Ka-Wei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ee-Peng</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2020/555</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Ninth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="4011" to="4017" />
		</imprint>
	</monogr>
	<note>International Joint Conferences on Artificial Intelligence Organization. Main track</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Graphto-tree learning for solving math word problems</title>
		<author>
			<persName><forename type="first">Jipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Ka-Wei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Bin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ee-Peng</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3928" to="3937" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
