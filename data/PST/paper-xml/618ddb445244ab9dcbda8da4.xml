<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Robust Knowledge Graph Embedding via Multi-task Reinforcement Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-11-11">11 Nov 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhao</forename><surname>Zhang</surname></persName>
							<email>zhangzhao2021@ict.ac.cn</email>
						</author>
						<author>
							<persName><forename type="first">Fuzhen</forename><surname>Zhuang</surname></persName>
							<email>zhuangfuzhen@buaa.edu.cn</email>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Hengshu</forename><surname>Zhu</surname></persName>
							<email>zhuhengshu@baidu.com</email>
						</author>
						<author>
							<persName><forename type="first">Chao</forename><surname>Li</surname></persName>
							<email>lichao@zhejianglab.com</email>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Hui</forename><surname>Xiong</surname></persName>
							<email>xionghui@ust.hk</email>
						</author>
						<author>
							<persName><forename type="first">Qing</forename><surname>He</surname></persName>
							<email>heqing@ict.ac.cn</email>
						</author>
						<author>
							<persName><forename type="first">Yongjun</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Qing He and Yongjun Xu are with Institute of Computing Technology</orgName>
								<orgName type="laboratory">Zhao Zhang is also with Zhejiang Lab</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Zhao Zhang, Beijing, Hangzhou</settlement>
									<country>China., China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Fuzhen Zhuang is with Institute of Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">SKLSDE</orgName>
								<orgName type="department" key="dep3">School of Computer Science</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Hengshu Zhu is with Baidu Talent Intelligence Center</orgName>
								<address>
									<postCode>100085</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Institute of Com-puting Technology</orgName>
								<orgName type="laboratory">Chao Li is with Zhejiang Lab</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Hangzhou, Beijing</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="laboratory">Hui Xiong is with Artificial Intelligence Thrust</orgName>
								<orgName type="institution">The Hong Kong Univer-sity of Science and Technology</orgName>
								<address>
									<postCode>511458</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="department">Fuzhen Zhuang</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Towards Robust Knowledge Graph Embedding via Multi-task Reinforcement Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-11-11">11 Nov 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2111.06103v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Knowledge graph</term>
					<term>Knowldge discovery</term>
					<term>Big data applications</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Nowadays, Knowledge graphs (KGs) have been playing a pivotal role in AI-related applications. Despite the large sizes, existing KGs are far from complete and comprehensive. In order to continuously enrich KGs, automatic knowledge construction and update mechanisms are usually utilized, which inevitably bring in plenty of noise. However, most existing knowledge graph embedding (KGE) methods assume that all the triple facts in KGs are correct, and project both entities and relations into a low-dimensional space without considering noise and knowledge conflicts. This will lead to low-quality and unreliable representations of KGs. To this end, in this paper, we propose a general multi-task reinforcement learning framework, which can greatly alleviate the noisy data problem. In our framework, we exploit reinforcement learning for choosing high-quality knowledge triples while filtering out the noisy ones. Also, in order to take full advantage of the correlations among semantically similar relations, the triple selection processes of similar relations are trained in a collective way with multi-task learning. Moreover, we extend popular KGE models TransE, DistMult, ConvE and RotatE with the proposed framework. Finally, the experimental validation shows that our approach is able to enhance existing KGE models and can provide more robust representations of KGs in noisy scenarios.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>K NOWLEDGE graphs (KGs) are multi-relational directed graphs composed of entities as nodes and relations as different types of edges. They represent information about real-world facts in the form of knowledge triples, which are denoted as (h, r, t), where h and t correspond to the head and tail entities and r denotes the relation between them, e.g., (Donald Trump, nationality, USA).</p><p>Due to their effectiveness for representing structured data, knowledge graphs have been playing a pivotal role in various AI-related applications, including information retrieval <ref type="bibr" target="#b10">[1]</ref>, question answering <ref type="bibr" target="#b11">[2]</ref>, and information extraction <ref type="bibr" target="#b12">[3]</ref>. However, the underlying symbolic nature of knowledge triples often makes KGs hard to manipulate. Therefore, the recent attention has been drawn on knowledge graph embedding (KGE), which aims to project both entities and relations into a continuous low-dimensional space, so as to simplify the manipulation while preserving the inherent structure of the KGs. Such embeddings encode rich information of KGs, and are widely utilized by downstream applications <ref type="bibr" target="#b13">[4]</ref>, <ref type="bibr" target="#b14">[5]</ref>, <ref type="bibr" target="#b15">[6]</ref>, <ref type="bibr" target="#b16">[7]</ref>.</p><p>Even though the sizes of modern KGs are quickly growing, existing KGs, such as Freebase <ref type="bibr" target="#b17">[8]</ref>, Wordnet <ref type="bibr" target="#b18">[9]</ref>, Yago <ref type="bibr" target="#b19">[10]</ref>, NELL <ref type="bibr" target="#b20">[11]</ref>, Google's KG 1 and other domainspecific KGs such as Gene Ontology <ref type="bibr" target="#b21">[12]</ref>, are far from complete and comprehensive. In order to continuously enrich KGs with the innumerable world knowledge, automatic mechanisms are utilized, which inevitably bring in plenty of noise and conflicts. Indeed, existing relation extraction models are not perfect <ref type="bibr" target="#b22">[13]</ref>. <ref type="bibr" target="#b23">[14]</ref> shows that state-of-theart relation extraction models only achieve around 60% precision when the recall is 20%. Figure <ref type="figure" target="#fig_0">1</ref> shows an example of the relation extraction results. When aiming to obtain the positive triple (Brussels, capitalOf, Belgium), the negative triple (Amsterdam, capitalOf, Belgium) is extracted, which indicates that the automatic mechanisms may bring in noise when enriching KGs. Moreover, a novel task named Wikidata vandalism <ref type="bibr" target="#b24">[15]</ref>, which revealed the deliberate destructions that exist in KGs, has attracted wide attention.</p><p>Indeed, the noisy data problem would degrade the performance of KGE models, thus lead to dissatisfactory results in downstream applications. Figure <ref type="figure" target="#fig_2">2</ref>   resent existing triples, while the dashed lines denote the predicted ones. The left part of Figure <ref type="figure" target="#fig_2">2</ref> contains a noise triple (New York, contained by, Canada) (denoted in a red line), and results in an incorrect prediction. The right part shows the correct prediction. Figure <ref type="figure" target="#fig_2">2</ref> shows that noisy triples in KGs greatly influence the performance of KGE models. However, this noisy data problem has largely been ignored by most of the existing models.</p><p>Two recent studies focused on computing a confidence score for each triple <ref type="bibr" target="#b23">[14]</ref> [16], and triples with higher confidence scores play a more important role during the training procedure. Particularly, CKRL <ref type="bibr" target="#b23">[14]</ref> is the first work that aims to detect possible noise in KGs while learning knowledge representations with confidence simultaneously, which learns the confidence of a triple with both triple information and path information in KGs. Results show that by assigning different confidence scores to triples, KGE models obtain better performance. NoiGAN <ref type="bibr" target="#b25">[16]</ref> is another state-of-the-art noise-aware KGE model that unifies the task of noise detection and knowledge representation with a generative adversarial networks (GAN) framework. NoiGAN finds that assigning hard confidence scores, i.e., 0 or 1, to triples instead of soft ones is beneficial to achieve better results. In this paper, we also agree with NoiGAN that the correctness of triples should be treated with a hard decision, i.e., true or false, instead of being dealt with a soft confidence score. Since from an optimal view, positive triples should be fully leveraged and negative triples should be completely removed.</p><p>To this end, in this paper, we propose a novel multitask reinforcement learning framework for robust KGE. Specifically, we first design reinforcement learning agents to select golden triples from the noisy training set while removing the false positives. Then a KGE model is trained based on the cleansed training set, and provides a delayed reward based on the quality of the selected triples for the reinforcement learning agents. Furthermore, it has been shown that semantically similar relations exist in large-scale KGs <ref type="bibr" target="#b26">[17]</ref>. In order to take full advantage of the correlations among similar relations, we treat the triple selection process of each relation as a single task, and train the selection processes of semantically similar relations in a collective way with multi-task learning. The reinforcement learning agents and the KGE model are trained in a joint way instead of independently, which avoids the accumulation of errors during the training process.</p><p>Our general framework can be easily utilized to extend a number of KGE models. Particularly, in this paper, we extend popular KGE models TransE <ref type="bibr" target="#b14">[5]</ref>, DistMult <ref type="bibr" target="#b27">[18]</ref>, ConvE <ref type="bibr" target="#b28">[19]</ref> and RotatE <ref type="bibr" target="#b29">[20]</ref> with our framework. Extensive experiments on popular benchmarks demonstrate the effectiveness of our framework.</p><p>In a nutshell, we highlight our key contributions as follows, 1) We propose a general multi-task reinforcement learning framework for robust KGE. 2) Our proposed framework is able to extend a number of state-of-the-art KGE models without additional information like text or logical rules. Particularly, we extend popular KGE models TransE, DistMult, ConvE and RotatE in this paper. 3) We evaluate our models on noisy datasets, and experimental results show that our extended models substantially outperform the base models as well as other baseline competitors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Knowledge Graph Embedding</head><p>Recent years have witnessed the increasing interest in KGE, which aims to represent entities and relations in KGs as low-dimensional vectors. Prior work roughly falls into three categories.</p><p>• Translation or rotation based models, which view relations as translations or rotations from a head entity to a tail entity <ref type="bibr" target="#b14">[5]</ref>, <ref type="bibr" target="#b30">[21]</ref>. TransE <ref type="bibr" target="#b14">[5]</ref> is one of the most widely used KGE model, which assumes h + r ≈ t when (h, r, t) holds. TransH <ref type="bibr" target="#b30">[21]</ref> is an extension of TransE, and introduces a mechanism of projecting entities into relation-specific hyperplanes that enables different roles of an entity in different relations. TransR <ref type="bibr" target="#b15">[6]</ref> introduces relation-specific projection matrices, and enables each entity to play different roles when involved in triples with different relations. STransE <ref type="bibr" target="#b31">[22]</ref> further extends TransR and uses two projection matrices for each relation, which distinguish the role of each entity when acting as the head or tail of a triple. RotatE <ref type="bibr" target="#b29">[20]</ref> projects entities and relations into a complex space, and view relations as rotations from head entities to tail entities. Rotate3D <ref type="bibr" target="#b32">[23]</ref> models the non-commutative composition pattern in three-dimensional space with quaternion representation • Tensor factorization based models, which assume the score of a (h, r, t) can be factorized into several tensors <ref type="bibr" target="#b27">[18]</ref>, <ref type="bibr" target="#b33">[24]</ref>. RESCAL <ref type="bibr" target="#b34">[25]</ref> is a representative model in the category, which represents each relation as a square matrix. The score function of RESCAL is defined as f (h, r, t) = h M r t, where f (h, r, t) denotes the score of triple (h, r, t), M r is a relation-specific matrix. Along this line, DistMult <ref type="bibr" target="#b27">[18]</ref> simplifies RESCAL by restricting M r as a diagonal matrix. ComplEx <ref type="bibr" target="#b33">[24]</ref> further extends DistMult, and projects both entities and relations into complex vectors instead of real-valued ones. • Neural network based models, which leverage the power of deep neural networks or graph neural networks in representation learning to embed KGs. ConvE <ref type="bibr" target="#b28">[19]</ref> for the first time utilizes convolutional neural network (CNN) to capture the interactions between entities and relations. ConvKB <ref type="bibr" target="#b35">[26]</ref> models the relationships among the same dimensional entries of the embeddings. InteractE <ref type="bibr" target="#b36">[27]</ref> extends ConvE by adding more interactions between entities and relations. R-GCN <ref type="bibr" target="#b37">[28]</ref> and KBAT <ref type="bibr" target="#b38">[29]</ref> are two state-of-the-art models that adopt graph neural network to model relational data, where R-GCN uses graph convolutional network, and KBAT utilizes graph attention network. In addition, besides the triple information in KGs, some studies also use external information like text <ref type="bibr" target="#b39">[30]</ref>, <ref type="bibr" target="#b40">[31]</ref> or logical rules <ref type="bibr" target="#b41">[32]</ref>, <ref type="bibr" target="#b42">[33]</ref> to conduct the KGE task. It is worth noting that the four models, TransE, DistMult, ConvE and RotatE, which we extend in this paper with our framework, have covered the above three categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Knowledge Graph Noise Detection</head><p>Noise data inevitably exists in existing KGs. Most KG noise detection works rely on a large amount of human supervision. DBpedia <ref type="bibr" target="#b43">[34]</ref> employs a worldwide crowd-sourcing effort to map knowledge triples to Wikipedia 2 info boxes. YAGO2 <ref type="bibr" target="#b44">[35]</ref> and Wikidata <ref type="bibr" target="#b45">[36]</ref> rely on human supervision to approve or reject a statement. These methods require a great deal of human supervision, which is extremely laborintensive, time-consuming and, most importantly, usually unavailable in real-world scenarios. UKGE <ref type="bibr" target="#b46">[37]</ref> aims to predict the confidence score for each triple. However, the setting of UKGE is different from our work. Specifically, in UKGE, each triple has a ground truth confidence score, while our work do not have such information. Recently, CKRL <ref type="bibr" target="#b23">[14]</ref> and NoiGAN <ref type="bibr" target="#b25">[16]</ref> are proposed to detect noise in KGs with only internal information, which focused on computing a confidence score for each triple, and triples with higher confidence scores play a more important role during the training process of KGE models. CKRL <ref type="bibr" target="#b23">[14]</ref> is the first noise-aware KGE model, which learns the confidence of a triple with both triple information and path information in KGs. NoiGAN <ref type="bibr" target="#b25">[16]</ref> proposes a GAN-based framework that unifies the task of KG noise detection and knowledge representation learning. In this paper, we filter out the false positives with a hard decision, i.e., true or false, via multitask reinforcement learning, and train the KGE models with the cleansed training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Multi-task Learning and Reinforcement Learning in KG-related Applications</head><p>Previous studies have shown the benefits of applying multitask learning to KG-related tasks. Zhang et al. <ref type="bibr">[38] [17]</ref> found semantically similar entities and relations in KGs, and trained the embeddings of similar entities and relations in a collective way with multi-task learning. Wang et al. <ref type="bibr" target="#b48">[39]</ref> 2. https://www.wikipedia.org/ proposed to use multi-task learning to jointly learn the entity representations in KGs and item representations in recommender systems. Luan et al. <ref type="bibr" target="#b49">[40]</ref> introduced a multitask setup of identifying and classifying entities, relations, and coreference clusters in scientific articles. Thus, this paper tries to train the triple selection processes of semantically similar relations in a collective way.</p><p>There are also some works that utilize reinforcement learning in KG-related tasks. Xiong et al. <ref type="bibr" target="#b50">[41]</ref> used reinforcement learning in the path-based KG reasoning task. Along this way, Lin et al. <ref type="bibr" target="#b51">[42]</ref> further proposed to use the reward shaping technique to improve the results. Reinforcement learning has also been shown to beneficial in the relation extraction task, Feng et al. <ref type="bibr" target="#b52">[43]</ref> and Qin <ref type="bibr" target="#b53">[44]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head><p>In this section, we introduce the technical details of our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>The proposed framework is shown in Figure <ref type="figure" target="#fig_3">3</ref>, where r 1 and r 2 are semantically similar relations. We define policy-based agents for r 1 and r 2 respectively. Since the two relations are semantically related, the two agents share some common information, which is denoted in dark green color in Figure <ref type="figure" target="#fig_3">3</ref>, making it possible for the two agents to be trained in a collective way. The reinforcement learning agents select golden triples for each relation, and provide a cleansed training set for the KGE model. At each state, the agent decides whether to select the current triple based on a stochastic policy. Then the KGE model is trained based on the cleansed training dataset, and provides a reward based on the model performance to update the policy-based agents. In our framework, the KGE model can be substituted by a number of existing KGE models, which guarantees the flexibility and the extendibility of our framework. The policy-based agents and the KGE model are interleaved together and are trained in a joint way during the training process. In the following, we introduce the details of the policy-based agents and the KGE model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Policy-based Agents</head><p>We present the state, action, reward and the optimization details of the policy-based agents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">State</head><p>The state vector should encode the following information: (i) the relation corresponding to the current triple sequence; (ii) the triple when making decision on; (iii) the already selected triples. We represent the state at time step t as a continuous real-valued vector s t ∈ R 5d , which is calculated as</p><formula xml:id="formula_0">s t = r ⊕ h ⊕ t ⊕ h ⊕ t,<label>(1)</label></formula><p>where d is the embedding size of entities and relations, ⊕ is the concatenation operation for vectors. r is the embedding of the current relation. h and t are the embeddings of the head and tail entities from the current triple when making decision on. h and t are the average of embedding vectors for the head and tail entities from the already selected triples. All the parameters above are obtained from the KGE model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Action</head><p>The action space is {0, 1}, where 1 indicates selecting the current triple, and 0 otherwise. Actions are decided based on a stochastic policy. We build an agent for each relation. Note that we also tried the setting that builds only one agent for all the relation types, but experiments show that we cannot get satisfactory performance under this setting.</p><p>In this paper, we adopt the logistic function as the policy function:</p><formula xml:id="formula_1">π Θ (a t = 1|s t ) = σ(w r • s t ),<label>(2) π</label></formula><formula xml:id="formula_2">Θ (a t = 0|s t ) = 1 − π Θ (a t = 1|s t ),<label>(3)</label></formula><p>where a t denotes the action at time step t, σ(•) is the sigmoid function, w r ∈ R 5d is the policy parameter for relation r, Θ represents the parameters to be learned. It is worth noting that we also tried different network structures as our policy network including CNN and MLP, but find this simple setting achieves the best performance.</p><p>To take advantage of the correlations among semantically similar relations, inspired by the regularized multi-task learning algorithm <ref type="bibr" target="#b54">[45]</ref>, we decompose w r into two parts, which is shown as</p><formula xml:id="formula_3">w r = u c + v r ,<label>(4)</label></formula><p>where u c ∈ R 5d is the common model parameter for all the relations that belong to the same relation cluster, which facilitates the knowledge sharing among semantically similar relations. Meanwhile, v r ∈ R 5d is the specific parameter for each individual relation, which represents the different characteristics of each relation. In particular, we do not focus on how to obtain similar relations in this paper, thus we simply obtain relation clusters based on the results of TransE using the k-means algorithm, which is a simple relation clustering method introduced by <ref type="bibr" target="#b26">[17]</ref>. The parameters of the policy function are denoted as </p><formula xml:id="formula_4">Θ = u 1 , u 2 , ..., u |C| ∪ v 1 , v 2 , ..., v |R| ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Reward</head><p>We assume when the triple selection process for a relation is finished, the agent comes to a terminal state s |Tr|+1 , where T r is the triple set of relation r in the training set. We get a delayed reward at the terminal state, while in other states, the reward is zero. We use the average score of the selected triples to compute the reward, which is</p><formula xml:id="formula_5">R = 1 | Tr | (h,r,t)∈ Tr f (h, r, t) + α | Tr | |T r | , (<label>5</label></formula><formula xml:id="formula_6">)</formula><p>where Tr is the set of selected triples for relation r. f (h, r, t) is the utilized score function, which can be substituted by the score function of TransE, DistMult, ConvE or RotatE. α | Tr| |Tr| is a heuristic term to encourage the model to select more triples. Experiments show that if we don't add this term into the reward function, our model tends to select only a few high-score positive triples, while most positive triples are filtered out, making our framework unable to provide sufficient training data for the KGE model. Specifically, when Tr = ∅, the reward is set as the average score of all the training triples. This reward setting enables our model to effectively select golden triples from T r and filter out T r that only contains noisy triples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Optimization</head><p>Our reinforcement learning model aims to maximize the expected reward for the triple selection process of each relation. More formally, for the triple selection process of relation r, the objective function is defined as</p><formula xml:id="formula_7">J(Θ) = E a∼πΘ(a|s) [R] + Ω.<label>(6)</label></formula><p>The first term is the expected reward, and the second term is a regularized term inspired by the regularized multi-task learning algorithm <ref type="bibr" target="#b54">[45]</ref>:</p><formula xml:id="formula_8">Ω = λ 1 u c 2 2 + λ 2 v r 2 2 .<label>(7)</label></formula><p>λ 1 and λ 2 are trade-off hyper-parameters. Large value of λ 1 will result in the separate training of each relation, while large value of λ 2 will lead to all relations in the same relation cluster sharing the same weight vector. Under this multitask setting, the triple selection processes of relations that belong to the same relation cluster are trained in a collective way through sharing knowledge via u c . According to the REINFORCE algorithm <ref type="bibr" target="#b55">[46]</ref>, we update the policy in the following way. For each relation, we sample a trajectory and get the corresponding reward R, and update the policy with the following gradients:</p><formula xml:id="formula_9">∇ Θ J(Θ) = |Tr| t=1 R∇ Θ log π Θ (a t |s t ) + ∇ Θ Ω,<label>(8)</label></formula><p>where ∇ represents the derivation operation.</p><p>In this paper, the extended KGE model using the proposed Multi-Task Reinforcement Learning framework is denoted as X-MTRL (X means TransE or DistMult or ConvE or RotatE in this paper). Besides, we also provide a variant of the proposed framework by setting u c in Equation ( <ref type="formula" target="#formula_3">4</ref>) and Equation ( <ref type="formula" target="#formula_8">7</ref>) as 0, i.e., all the relations are trained separately and don't share any common information, which is a Single-Task Reinforcement Learning framework. The extended single-task model using the variant framework is denoted as X-STRL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">KGE model</head><p>Our framework is capable of extending a number of stateof-the-art KGE models, and in this paper, we apply our framework to four popular KGE models TransE, DistMult, ConvE, and RotatE.</p><p>TransE <ref type="bibr" target="#b14">[5]</ref> is one of the most widely used KGE models, which views a relation as a translation from a head entity to a tail entity on the same low-dimensional hyperplane, i.e., h + r ≈ t when (h, r, t) holds. The score function of TransE is defined as</p><formula xml:id="formula_10">f (h, r, t) = − h + r − t Ln .<label>(9)</label></formula><p>L n can be L 1 or L 2 norm, which is decided based on the model performance over the validation set. Positive triples are supposed to have higher scores than negative ones. TransE adopts a margin-based loss function, which is defined as,</p><formula xml:id="formula_11">L T ransE = (h,r,t)∈ T [f (h , r, t ) − f (h, r, t) + γ] + , (<label>10</label></formula><formula xml:id="formula_12">)</formula><p>where T is the selected triples from the training set, [•] + = max(0, •), and γ is the margin separating positive instances from negative ones. (h , r, t ) represents negative triples which are generated by replacing the head entity or the tail entity of a positive triple with a random entity in the KG. Formally, (h , r, t ) ∈ {(h , r, t)|h ∈ E} ∪ {(h, r, t )|t ∈ E}, where E is the entity set.</p><p>DistMult <ref type="bibr" target="#b27">[18]</ref> is a representative model in the tensor factorization category, which adopts a bilinear function to compute the scores of knowledge triples. The score function is defined as</p><formula xml:id="formula_13">f (h, r, t) = h M r t,<label>(11)</label></formula><p>where M r is a relation-specific diagonal matrix, which represents the characteristics of a relation. Like triples in TransE, positive triples should have higher scores than negative ones in DistMult.</p><p>For DistMult and its extended models, we adopt the softplus loss function, which is shown as</p><formula xml:id="formula_14">L DistM ult = (h ,r,t ) log(1 + exp(−y (h ,r,t ) • f (h , r, t ))). (12)</formula><p>In Equation ( <ref type="formula">12</ref>), (h , r, t ) ∈ {(h, r, t)} ∪ N eg(h, r, t), where (h, r, t) is a selected triple in the training set, N eg(h, r, t) ⊂ {(h , r, t)|h ∈ E} ∪ {(h, r, t )|t ∈ E} is a set of corrupted triples. y (h ,r,t ) is the label of the triple (h , r, t ).</p><p>ConvE <ref type="bibr" target="#b28">[19]</ref> models the interactions between input entities and relations by convolutional and fully-connected layers. Given (h, r, t) triples, ConvE first reshapes the embedding of h and r into 2D tensors, then computes the scores of knowledge triples based on the reshaped tensors. The score function of ConvE is defined as</p><formula xml:id="formula_15">f (h, r, t) = g vec(g([ ĥ; r] * ω))W t,<label>(13)</label></formula><p>where ĥ and r are 2D reshapings of h and r: if h, r ∈ R d , then ĥ, r ∈ R d1×d2 , where d = d 1 d 2 . ω denotes a set of filters and * denotes the convolution operator. vec(•) is a vectorization function, g denotes the ReLU function, and W is the weight matrix. ConvE also assumes that positive triples have higher scores than negative ones. For ConvE and its extended models, we adopt the binary loss function, which is defined as</p><formula xml:id="formula_16">L ConvE = (h,r,t)∈ T − 1 N N i=1 (y (h,r,ti) • log(σ(f (h, r, t i ))) + (1 − y (h,r,ti) ) log(1 − σ(f (h, r, t i )))), (<label>14</label></formula><formula xml:id="formula_17">)</formula><p>where T is the set of all the selected triples from the training set, N denotes the number of candidates for the tail entity, and σ is the sigmoid function. RotatE <ref type="bibr" target="#b29">[20]</ref> is a recent model that maps entities and relations to the complex vector space and defines each relation as a rotation from the head entity to the tail entity. The score function of RotatE is defined as</p><formula xml:id="formula_18">f (h, r, t) = − h • r − t L1 ,<label>(15)</label></formula><p>where h, r, t ∈ C d are complex vectors, the modulus |r i | = 1, and • denotes the Hadamard (element-wise) product. Also, in RotatE, positive triples are supposed to have higher scores than negative ones.</p><p>The loss function of RotatE is defined as</p><formula xml:id="formula_19">L RotatE = − (h,r,t)∈ T [log σ(f (h, r, t) − η) − Σ (h ,r,t ) 1 k log σ(η − f (h , r, t ))],<label>(16)</label></formula><p>where σ(•) is the sigmoid function, k is the number of negative samples for each golden triple, and η is a hyperparameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Model Training</head><p>In  <ref type="bibr" target="#b14">[5]</ref>, where d is the dimension of the embedding space. The learning process of the above models is carried out using the Adam optimizer <ref type="bibr" target="#b56">[47]</ref>. Specifically, for DistMult and its extended models, L 2 regularizer is applied to all the entity and relation embeddings during the training procedure.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we evaluate the proposed framework on the noise detection, the link prediction and the triple classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>In the experiments, we use the datasets released by CKRL <ref type="bibr" target="#b23">[14]</ref>. Specifically, in the paper of CKRL, three datasets are constructed with noisy triples to be 10%, 20% and 40% of positive triples based on a popular benchmark FB15k <ref type="bibr" target="#b14">[5]</ref>.</p><p>All the three noisy datasets, which are denoted as FB15k-N1, FB15k-N2 and FB15k-N3 respectively, share the same entities, relations, validation and test sets with FB15k, with all generated negative triples fused into the original training set of FB15k. Since most noise and conflicts in real-world KGs derive from the misunderstanding between similar entities, e.g., the noise (Donald Trump, nationality, Canada) is more likely to occur in real-world KGs than (Donald Trump, nationality, Basketball), the noise triples are generated in the following way. Given a positive entity (h, r, t), the head or tail entity is replaced to form a negative triple (h , r, t) or (h, r, t ). The generation of negative triples is constrained that h (or t ) should have appeared in the head (or tail) position with the same relation r in the dataset. Under this setting, the tail entity of the relation nationality should be a country. This setting is able to generate harder and more confusing negative triples.</p><p>It is worth noting that recent studies have shown that FB15k suffers from the information leakage problem <ref type="bibr" target="#b28">[19]</ref>, <ref type="bibr" target="#b57">[48]</ref>. For example, the test set main contains triples (A, contains, B) when the training set contains triple (B, contained by, A). And one can attain the state-of-the-art results even using simple rules. In this case, we construct noisy datasets based on popular benchmark datasets FB15k-237 <ref type="bibr" target="#b57">[48]</ref> and WN18RR <ref type="bibr" target="#b28">[19]</ref> using the same method introduced in CKRL <ref type="bibr" target="#b23">[14]</ref>. The corresponding datasets are denoted as FB15k-237-N1, FB15k-237-N2 and FB15k-237-N3, WN18RR-N1, WN18RR-N2 and WN18RR-N3, respectively. The statistics of the datasets are summarized in Table <ref type="table" target="#tab_2">1</ref>, Table <ref type="table" target="#tab_4">2 and Table 3</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines</head><p>In this paper, we compare the proposed framework with the following baselines.</p><p>• TransE <ref type="bibr" target="#b14">[5]</ref>: one of the most popular and widely used translation-based models.</p><p>• DistMult <ref type="bibr" target="#b27">[18]</ref>: a popular tensor factorization based model.</p><p>• ConvE <ref type="bibr" target="#b28">[19]</ref>: a state-of-the-art neural network based model, which utilizes convolutional neural networks to compute the scores of given triples.</p><p>• RotatE <ref type="bibr" target="#b29">[20]</ref>: a recent KGE model which views each relation as a rotation from the head entity to the tail entity in the complex vector space.</p><p>• CKRL <ref type="bibr" target="#b23">[14]</ref>: a state-of-the-art KGE model which computes a confidence score for each triple during the training procedure. • X-Score: X-Score is a simple baseline that filters out triples with lower scores. Specifically, we first pre-train the KGE model with the noisy training set, then filter out the triples with lower scores. The proportion of triples that are removed is a hyperparameter δ. Finally, we re-train the KGE model with the retained triples. • X-NoiGAN: X-NoiGAN denotes the models extended with a recent confidence-aware method NoiGAN <ref type="bibr" target="#b25">[16]</ref>.</p><p>We use the hard version of NoiGAN since it achieves the best results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Knowledge Graph Noise Detection</head><p>The KG noise detection task aims to test the capacity of the reinforcement learning agents in selecting golden triples from the noisy training set. In the training stage, all the hyper-parameters are decided based on the model performance over the validation set via grid search. Specifically, in X-Score, a proportion of low-score triples are filtered out, and the proportion is denoted as a hyper-parameter δ. Particularly, we use 5% as the step size to set the value of δ from 0 to 1, and report the best performance. The hyper-parameter ranges are shown in Table <ref type="table" target="#tab_5">4</ref>. We find the best hyper-parameters as follows. For the extended models of TransE, ConvE, and RotatE, α is set as 0.05, 0.03 and 0.02 for FB15k based and FB15k-237 based models, and 0.05, 0.04 and 0.02 for WN18RR based models. For the extended models of DistMult, α is set as 0.3, 0.2 and 0.1 for Y-N1 to Y-N3 respectively, where Y represents FB15k, FB15k-237 or WN18RR in this paper. For all the extended models, λ 1 and λ 2 are set as 0.001 and 0.01 respectively, and the episode number M is set as 15. We set the batch size as 1024 for all the models. The learning rates are set as 0.001 for TransE, DistMult, ConvE and RotatE, and 0.0005 for their extended models. Specifically, for TransE and its extended models, γ is set as 1 and the L 1 norm is adopted by the score function. For RotatE and its extended models, η is set as 5. For X-Score, we set δ as 10% on Y-N1, and 15% on Y-N2. Specifically, for TransE-Score and RotatE-Score, δ is set as 25% on Y-N3. And for DistMult-Score and ConvE-Score, In the test stage, we adopt the F1 score to show the ability of our models in detecting noise. Specifically, for our models, since we make hard decisions over the noisy training set, the unselected triples are regarded as noise. For CKRL and X-Score, inspired by the evaluation method from the CKRL paper, the triples in the training set with lower f (h, r, t) = − h + r − t scores are detected as noisy ones, we compute the F1 scores with the recall changing from 0 to 1, and report the maximum F1 value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Experimental Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Experimental Results</head><p>The results are shown in Figure <ref type="figure" target="#fig_5">4</ref>. It can be clearly figured out that (1) on all the FB15k, FB15k-237, and WN18RR based datasets, TransE-STRL and TransE-MTRL clearly outperform CKRL, TransE-Score, and TransE-NoiGAN, which indicates that our framework can effectively detect and filter out noisy triples; (2) on FB15k based and FB15k-237 based data sets, TransE-MTRL outperforms TransE-STRL, which indicates that the information learned from similar relations is of great value, and validates the effectiveness of the collective training process for semantically similar relations. However, we find that on WN18RR based datasets, TransE-STRL achieves better results than TransE-MTRL. We conjecture the reason lies in that the semantic correlations among relations in FB15k and FB15k-237 are much stronger than that of WN18RR. Although the information learned from semantically similar relations are useful, the information learned from unrelated relations may damage the results <ref type="bibr" target="#b26">[17]</ref>. The results are in line with the findings in <ref type="bibr" target="#b26">[17]</ref>, which shows that the MTRL model is more useful for KGs which have dense semantic distributions over relations, while the STRL model is more suitable for KGs in which the semantic correlations among relations are weak.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Link Prediction</head><p>Link prediction, a.k.a. knowledge graph completion, aims to fill the missing values into incomplete knowledge triples. More formally, the goal of link prediction is to predict either the head entity in a given query (?, r, t) or the tail entity in a given query (h, r, ?).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Experimental Settings</head><p>In the training phase, we use the same hyper-parameter settings as the KG noise detection task. In the test phase, we replace the head and tail entities with all entities in KG in turn for each triple in the test set. Then we compute a score for each corrupted triple, and rank all the candidate entities according to the scores. Specifically, positive candidates are supposed to precede negative ones. Finally, the rank of the correct entity is stored. We compare our models with baselines using the following metrics: (i) Mean Reciprocal Rank (MRR, the mean of all the reciprocals of predicted ranks); (ii) Hits@n (H@n, the proportion of ranks not larger than n). All the results are reported in the "filtered" setting <ref type="bibr" target="#b14">[5]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Experimental Results</head><p>Evaluation results are shown in Table <ref type="table" target="#tab_6">5</ref>, Table <ref type="table" target="#tab_7">6</ref> and Table <ref type="table" target="#tab_9">7</ref>.</p><p>We divide all the results into 5 groups. The second, third, fourth and fifth group are the results of TransE, DistMult, ConvE, RotatE and their extended models, while the first group is the state-of-the-art baseline CKRL. Results in bold font are the best results in the group, and the underlined results denote the best results in the column. Numbers marked with * indicate that the improvement is statistically significant compared with the best baseline in the group (t-test with p-value &lt;0.05). From these tables, we have the following findings. (1) Our extended models clearly outperform the base models and other competitors, which clearly validates the effectiveness of the proposed framework, and shows that the policy-based agents can well filter out the noisy triples and retain the positive ones. (2) X-MTRL achieves the best results on FB15k and FB15k-237 based datasets, while X-STRL outperforms other baselines on WN18RR based data sets. The results confirm that MTRL models are more useful for KGs in which the semantic correlations among relations are strong, while the STRL models are more suitable for KGs which have sparse semantic distributions over relations. <ref type="bibr" target="#b12">(3)</ref> The margin between the base models and the best performed extended models become more significant as the noise rate in KGs goes higher.</p><p>Taking the metric of MRR as an example, comparing with RotatE, RotatE-MTRL gets the improvements of 0.017, 0.019 and 0.029 on FB15k-237-N1 to FB15k-237-N3 respectively. It indicates the proposed framework can well handle KGs with different noise rates. We also provide some case studies. Table <ref type="table" target="#tab_10">8</ref> shows some case studies of the triple selection process on FB15k-237-N1. In Table <ref type="table" target="#tab_10">8</ref>, four triple instances are divided into two groups. The first group contains two positive triples, and the second group is comprised of two negative ones. In Table <ref type="table" target="#tab_10">8</ref>, we normalize the confidence scores of CKRL to [0, 1] by s i = s i /s max , where s i is the confidence score of the i-th triple and s max is the maximal confidence score. It clearly shows that our model TransE-MTRL can assign proper hard weights to triples. In addition, Table <ref type="table">9</ref> gives some examples of relation clusters in FB15k-237-N1. Relations in Cluster 1 are sports-related relations, in Cluster 2 are film-related relations, while in Cluster 3 are language-related relations. From Table <ref type="table">9</ref> we can see that semantically related relations are clustered into the same group, which is helpful to facilitate knowledge sharing among these relations.</p><p>To further evaluate the generalization performance of the proposed framework, we extend popular KGE models R-GCN <ref type="bibr" target="#b37">[28]</ref>, CompGCN <ref type="bibr" target="#b58">[49]</ref> and ComplEx-N3 <ref type="bibr" target="#b59">[50]</ref> with our framework. Due to the space limitation, please refer to the Appendix for more details. Experimental results validate the generalization ability of the proposed framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Further Comparison between X-MTRL/STRL and X-Score</head><p>To further evaluate the effectiveness of the reinforcement learning method in selecting golden triples, we compare X-MTRL/STRL and X-Score. Specifically, we first run X-MTRL or X-STRL, and get the number of triples that are kept. Then, we run X-Score by keeping the same number of samples as X-MTRL or X-STRL. Finally, we compare the results of the above models. Particularly, since X-MTRL achieves the best results on FB15k and FB15k-237 based datasets, and X-STRL outperforms other models on WN18RR-based datasets, on FB15k and FB15k-237 based datasets, we compare X-Score with X-MTRL, while on WN18RR-based datasets, we compare X-Score with X-STRL. The results are shown in Table <ref type="table" target="#tab_11">10</ref>, <ref type="bibr">Table 11,</ref><ref type="bibr">and</ref>   We observe that X-MTRL or X-STRL significantly outperforms X-Score with a large margin, which indicates that the reinforcement learning setting is capable of selecting better samples than the score-based method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.4">Link Prediction Results on Sparse KGs</head><p>To evaluate the model performance on sparse KGs, we manually remove triples from the original training set of FB15k-237-N1, and construct four sparser datasets, which are shown in Table <ref type="table" target="#tab_4">13</ref>. The construction process is as follows, we first sample 80%, 60%, 40% and 20% of positive triples from the original training set of FB15k-237, then add noise to the sampled datasets with the same noise injection method as FB15k-237-N1. The four datasets are named as FB15k-237-N1-80%, FB15k-237-N1-60%, FB15k-237-N1-40% and FB15k-237-N1-20% respectively with different degrees of sparsity.</p><p>The results are shown in Table <ref type="table" target="#tab_16">14</ref>. Results in bold font are the best results in the group, and the underlined results denote the best results in the column. Numbers marked with * indicate that the improvement is statistically significant compared with the best baseline in the group (t-test with pvalue &lt;0.05). From Table <ref type="table" target="#tab_16">14</ref>, we have the following findings.</p><p>(1) Our extended models outperform the base models and other baselines. And MTRL models achieve better results than STRL ones. The results confirm the effectiveness of our MTRL framework. (2) We find our extended models outperform the base models on KGs with different degrees of sparsity, which clearly validates that the proposed framework is able to achieve better results on both dense and sparse KGs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Triple Classification</head><p>The triple classification task aims to predict the label (True or False) of a given triple (h, r, t), which is a benchmark test that evaluates the discriminative capability of KGE models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Experimental Settings</head><p>All the hyper-parameters are set the same as those introduced in the KG noise detection task. Since there are no explicit negative triples in existing KGs, we construct negative triples in validation and test set following the same protocol as described in Section 4.1. In the test phase, we follow the same decision process as introduced in <ref type="bibr" target="#b60">[51]</ref>: for TransE, DistMult, ConvE, RotatE and their extended models, a triple is regarded as a positive one if f (h, r, t) is above a threshold; otherwise negative. The thresholds are determined on the validation set. We adopt accuracy as our evaluation metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Experimental Results</head><p>The experimental results are shown in Table <ref type="table" target="#tab_6">15</ref>, which are also divided into 5 groups in the same way as Table <ref type="table" target="#tab_6">5</ref>, Table <ref type="table" target="#tab_7">6</ref> and Table <ref type="table" target="#tab_9">7</ref>. Numbers marked with * indicate that the improvement is statistically significant compared with the best baseline in the group (t-test with p-value &lt;0.05). Table <ref type="table" target="#tab_6">15</ref> leads to the following conclusions: (1) Our extended models achieve better results than baselines, which shows the discriminative capability of the extended models, and again validates the effectiveness and extendibility of our framework. <ref type="bibr" target="#b11">(2)</ref> We also find that MTRL models achieve the best results on FB15k and FB15k-237 based datasets, and STRL models perform the best in WN18RR based datasets, which again validates the MTRL models and SRTL models are more suitable for KGs with dense and sparse semantic distributions over relations, respectively. (3) The results confirm the quality of the learned knowledge representations, since they not only outperform baselines in the link prediction task, but also achieve better results in the triple classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we proposed a general multi-task reinforcement learning framework for robust KGE. Specifically, we exploited reinforcement learning to select positive triples for each relation and utilized multi-task learning to facilitate knowledge sharing among semantically similar relations. Moreover, we extended four popular KGE models with the proposed framework. Finally, we evaluated our framework on the KG noise detection, the link prediction and the triple classification tasks. The results showed that our approach could enhance existing KGE models and provide more robust representations of KGs. We are hopeful that our framework can provide a new perspective for other noiseaware tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. An example of relation extraction results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>shows a KG completion example, where KG completion aims to obtain new knowledge triples based on the existing ones in KGs with entity and relation embeddings. The solid lines rep-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. A knowledge graph completion case.</figDesc><graphic url="image-2.png" coords="2,107.58,43.70,396.83,79.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Reinforcement Learning Framework. r 1 and r 2 are semantically similar relations. l 1 and l 2 are the number of triples of r 1 and r 2 in the training set respectively.</figDesc><graphic url="image-3.png" coords="4,150.10,43.70,311.80,212.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>where C and R are the relation cluster set and relation set respectively. | • | denotes the cardinality of a set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. KG noise detection results.</figDesc><graphic url="image-4.png" coords="7,112.01,48.69,127.55,170.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>do 2 for r in R do 3</head><label></label><figDesc>our framework, before jointly training the policy-based agents and the KGE model, we pre-train the two modules respectively. First, we pre-train the KGE model based on the noisy training set. It is worth noting that in order to prevent the KGE model overfitting on the noisy triples, the training epochs for the pre-training process of the KGE model is limited as 100. Then the policy-based agents are pre-trained based on the embedding vectors from the KGE model. We find such a pre-training strategy is crucial for our framework, which is also widely used by previous reinforcement learning studies<ref type="bibr" target="#b52">[43]</ref>,<ref type="bibr" target="#b53">[44]</ref>. After the pre-training process, the joint training procedure is conducted, which is described in Algorithm 1. It is worth noting that we conducted experiments to explore the effect of triple order in the selection process on the final results. Experiments show that with the episode number M (in Algorithm 1) getting larger, the effect of triple order is getting smaller. And in our setting, the order of triples has only a small effect on the final result (only up to 0.002 Mean Reciprocal Rank score). Thus we use a random order to conduct the experiments. Entity embeddings {e1, e2, ..., e |E| } and relation embeddings {r1, r2, ..., r |R| } ; 1 for episode m = 1 to M Sample actions over Tr with Θ: A = {a1, ..., a |Tr | }, at ∼ πΘ(at|st); Get a cleansed triple set Tr for r with A; Replace the triples of relation r in the training data set with Tr;</figDesc><table><row><cell>4</cell><cell></cell></row><row><cell>6</cell><cell>Update Φ in the KGE model with the cleansed</cell></row><row><cell></cell><cell>training data set ;</cell></row><row><cell>7</cell><cell>Compute a reward R with Equation (5);</cell></row><row><cell>8</cell><cell>Update Θ with Equation (8);</cell></row><row><cell>9</cell><cell>end</cell></row><row><cell cols="2">10 end</cell></row><row><cell cols="2">The entity and relation embeddings of the KGE models are initialized with a uniform distribution U [−6/ √ d, 6/ √ d]</cell></row><row><cell cols="2">following TransE</cell></row></table><note>Algorithm 1: Joint Training Procedure Input: Episode number M . A policy function and a KGE model parameterized by Θ and Φ respectively. A noisy training data set of the KGE model T = {Tr|r ∈ R}; Output: 5</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 1</head><label>1</label><figDesc>Statistics of FB15k-based datasets.</figDesc><table><row><cell cols="2">Dataset #Rel</cell><cell>#Ent</cell><cell>#Train</cell><cell cols="2">#Valid</cell><cell>#Test</cell></row><row><cell>FB15k</cell><cell cols="5">1,345 14,951 483,142 50,000 59,071</cell></row><row><cell></cell><cell></cell><cell>FB15k-N1</cell><cell cols="2">FB15k-N2</cell><cell>FB15k-N3</cell></row><row><cell cols="2">#Neg triple</cell><cell>46,408</cell><cell>93,782</cell><cell></cell><cell>187,925</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 2</head><label>2</label><figDesc>Statistics of FB15k-237 based datasets.</figDesc><table><row><cell>Dataset</cell><cell>#Rel</cell><cell>#Ent</cell><cell>#Train</cell><cell cols="2">#Valid</cell><cell>#Test</cell></row><row><cell>FB15k-237</cell><cell>237</cell><cell cols="5">14,541 272,115 17,535 20,466</cell></row><row><cell></cell><cell cols="2">FB15k-237-N1</cell><cell cols="2">FB15k-237-N2</cell><cell cols="2">FB15k-237-N3</cell></row><row><cell>#Neg triple</cell><cell cols="2">27,211</cell><cell>54,423</cell><cell></cell><cell></cell><cell>108,846</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 3</head><label>3</label><figDesc>Statistics of WN18RR based datasets.</figDesc><table><row><cell>Dataset</cell><cell>#Rel</cell><cell>#Ent</cell><cell cols="3">#Train #Valid #Test</cell></row><row><cell>WN18RR</cell><cell>11</cell><cell cols="2">40,943 86,835</cell><cell cols="2">3,034</cell><cell>3,134</cell></row><row><cell></cell><cell cols="2">WN18RR-N1</cell><cell cols="2">WN18RR-N2</cell><cell>WN18RR-N3</cell></row><row><cell>#Neg triple</cell><cell>8,683</cell><cell></cell><cell>17,367</cell><cell></cell><cell>34,734</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 4</head><label>4</label><figDesc>Hyper-parameter Ranges.</figDesc><table><row><cell>Hyper-parameter</cell><cell>range</cell></row><row><cell>α in Equation (5)</cell><cell>{0.02, 0.03, 0.04, 0.05}</cell></row><row><cell>λ 1 and λ 2 in Equation (7)</cell><cell>{0.1, 0.01, 0.001}</cell></row><row><cell>the number of episodes M</cell><cell>{5, 10, 15}</cell></row><row><cell>batch size</cell><cell>{256, 512, 1024}</cell></row><row><cell>learning rate</cell><cell>{0.1, 0.01, 0.001}</cell></row><row><cell>γ in Equation (10)</cell><cell>{1, 5, 10}</cell></row><row><cell>η in Equation (16)</cell><cell>{1, 5, 10}</cell></row></table><note>δ in X-Score {0, 5%, 10%, ..., 1}</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 5</head><label>5</label><figDesc>Link prediction results on FB15k-N1 to FB15k-N3.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">FB15k-N1</cell><cell></cell><cell></cell><cell cols="2">FB15k-N2</cell><cell></cell><cell></cell><cell cols="2">FB15k-N3</cell><cell></cell></row><row><cell></cell><cell>MRR</cell><cell>H@10</cell><cell>H@3</cell><cell>H@1</cell><cell>MRR</cell><cell>H@10</cell><cell>H@3</cell><cell>H@1</cell><cell>MRR</cell><cell>H@10</cell><cell>H@3</cell><cell>H@1</cell></row><row><cell>CKRL</cell><cell>0.371</cell><cell>0.642</cell><cell>0.440</cell><cell>0.229</cell><cell>0.349</cell><cell>0.611</cell><cell>0.421</cell><cell>0.211</cell><cell>0.317</cell><cell>0.566</cell><cell>0.378</cell><cell>0.190</cell></row><row><cell>TransE</cell><cell>0.365</cell><cell>0.636</cell><cell>0.442</cell><cell>0.222</cell><cell>0.346</cell><cell>0.605</cell><cell>0.415</cell><cell>0.210</cell><cell>0.314</cell><cell>0.557</cell><cell>0.372</cell><cell>0.188</cell></row><row><cell>TransE-Score</cell><cell>0.368</cell><cell>0.638</cell><cell>0.432</cell><cell>0.224</cell><cell>0.347</cell><cell>0.607</cell><cell>0.417</cell><cell>0.209</cell><cell>0.317</cell><cell>0.557</cell><cell>0.378</cell><cell>0.187</cell></row><row><cell>TransE-NoiGAN</cell><cell>0.372</cell><cell>0.639</cell><cell>0.443</cell><cell>0.231</cell><cell>0.349</cell><cell>0.612</cell><cell>0.424</cell><cell>0.214</cell><cell>0.322</cell><cell>0.568</cell><cell>0.387</cell><cell>0.213</cell></row><row><cell>TransE-STRL (Ours)</cell><cell>0.373</cell><cell>0.642</cell><cell>0.445</cell><cell>0.226</cell><cell>0.359</cell><cell>0.621</cell><cell>0.429</cell><cell>0.218</cell><cell>0.328</cell><cell>0.577</cell><cell>0.391</cell><cell>0.215</cell></row><row><cell>TransE-MTRL (Ours)</cell><cell cols="11">0.378* 0.644* 0.450* 0.235* 0.367* 0.628* 0.437* 0.228* 0.336* 0.582* 0.397*</cell><cell>0.214</cell></row><row><cell>DistMult</cell><cell>0.435</cell><cell>0.700</cell><cell>0.517</cell><cell>0.298</cell><cell>0.361</cell><cell>0.653</cell><cell>0.428</cell><cell>0.222</cell><cell>0.251</cell><cell>0.528</cell><cell>0.281</cell><cell>0.128</cell></row><row><cell>DistMult-Score</cell><cell>0.439</cell><cell>0.702</cell><cell>0.519</cell><cell>0.297</cell><cell>0.364</cell><cell>0.658</cell><cell>0.430</cell><cell>0.222</cell><cell>0.255</cell><cell>0.531</cell><cell>0.288</cell><cell>0.133</cell></row><row><cell>DistMult-NoiGAN</cell><cell>0.441</cell><cell>0.708</cell><cell>0.527</cell><cell>0.299</cell><cell>0.366</cell><cell>0.661</cell><cell>0.435</cell><cell>0.226</cell><cell>0.259</cell><cell>0.537</cell><cell>0.295</cell><cell>0.141</cell></row><row><cell>DistMult-STRL (Ours)</cell><cell>0.445</cell><cell>0.714</cell><cell>0.533</cell><cell>0.301</cell><cell>0.371</cell><cell>0.667</cell><cell>0.441</cell><cell>0.229</cell><cell>0.268</cell><cell>0.549</cell><cell>0.308</cell><cell>0.147</cell></row><row><cell cols="2">DistMult-MTRL (Ours) 0.451*</cell><cell>0.713</cell><cell cols="10">0.539* 0.309* 0.379* 0.670* 0.443* 0.234* 0.283* 0.561* 0.313* 0.153*</cell></row><row><cell>ConvE</cell><cell>0.513</cell><cell>0.711</cell><cell>0.578</cell><cell>0.402</cell><cell>0.499</cell><cell>0.699</cell><cell>0.562</cell><cell>0.387</cell><cell>0.470</cell><cell>0.672</cell><cell>0.533</cell><cell>0.359</cell></row><row><cell>ConvE-Score</cell><cell>0.515</cell><cell>0.712</cell><cell>0.581</cell><cell>0.404</cell><cell>0.501</cell><cell>0.698</cell><cell>0.563</cell><cell>0.394</cell><cell>0.474</cell><cell>0.677</cell><cell>0.536</cell><cell>0.362</cell></row><row><cell>ConvE-NoiGAN</cell><cell>0.518</cell><cell>0.714</cell><cell>0.579</cell><cell>0.407</cell><cell>0.503</cell><cell>0.702</cell><cell>0.565</cell><cell>0.396</cell><cell>0.483</cell><cell>0.684</cell><cell>0.543</cell><cell>0.372</cell></row><row><cell>ConvE-STRL (Ours)</cell><cell>0.523</cell><cell>0.716</cell><cell>0.585</cell><cell>0.411</cell><cell>0.509</cell><cell>0.708</cell><cell>0.569</cell><cell>0.401</cell><cell>0.495</cell><cell>0.689</cell><cell>0.555</cell><cell>0.378</cell></row><row><cell>ConvE-MTRL (Ours)</cell><cell cols="12">0.526* 0.723* 0.588* 0.415* 0.515* 0.715* 0.579* 0.405* 0.502* 0.699* 0.561* 0.389*</cell></row><row><cell>RotatE</cell><cell>0.771</cell><cell>0.836</cell><cell>0.801</cell><cell>0.719</cell><cell>0.749</cell><cell>0.812</cell><cell>0.781</cell><cell>0.689</cell><cell>0.726</cell><cell>0.783</cell><cell>0.759</cell><cell>0.662</cell></row><row><cell>RotatE-Score</cell><cell>0.772</cell><cell>0.838</cell><cell>0.804</cell><cell>0.726</cell><cell>0.751</cell><cell>0.819</cell><cell>0.784</cell><cell>0.689</cell><cell>0.733</cell><cell>0.789</cell><cell>0.764</cell><cell>0.669</cell></row><row><cell>RotatE-NoiGAN</cell><cell>0.774</cell><cell>0.841</cell><cell>0.807</cell><cell>0.728</cell><cell>0.755</cell><cell>0.825</cell><cell>0.786</cell><cell>0.691</cell><cell>0.744</cell><cell>0.798</cell><cell>0.775</cell><cell>0.682</cell></row><row><cell>RotatE-STRL (Ours)</cell><cell>0.779</cell><cell>0.841</cell><cell>0.816</cell><cell>0.733</cell><cell>0.756</cell><cell>0.826</cell><cell>0.795</cell><cell>0.692</cell><cell>0.748</cell><cell>0.804</cell><cell>0.785</cell><cell>0.679</cell></row><row><cell>RotatE-MTRL (Ours)</cell><cell cols="2">0.783* 0.849*</cell><cell>0.813</cell><cell cols="7">0.736* 0.764* 0.831* 0.798* 0.706* 0.755* 0.815*</cell><cell>0.781</cell><cell>0.689*</cell></row></table><note>δ is set as 30% on Y-N3. The number of relation clusters are set as 300, 120 and 10 for FB15k, FB15k-237 and WN18RR based datasets, respectively. On FB15k and FB15k-237 based datasets, as the number of relation clusters increases, the result first goes up and then falls down. On WN18RR based datasets, the model performance continues going up as the number of relation clusters increases. For the impact of the number of relation clusters on the final results, please refer to the Appendix for details. We also tried different random seeds, but find the seed does not have a big effect on the final results (up to 0.001 MRR score). The embedding size is set as 100 for all the models for a fair comparison. For relations that have too many triples in the training data set (e.g., the relation /people/person/profession has 11972 triples in the training set of FB15k-237-N1), we sample a subset of 5000 triples to train the agents.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 6</head><label>6</label><figDesc>Link prediction results on FB15k-237-N1 to FB15k-237-N3.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">FB15k-237-N1</cell><cell></cell><cell></cell><cell cols="2">FB15k-237-N2</cell><cell></cell><cell></cell><cell cols="2">FB15k-237-N3</cell><cell></cell></row><row><cell></cell><cell>MRR</cell><cell>H@10</cell><cell>H@3</cell><cell>H@1</cell><cell>MRR</cell><cell>H@10</cell><cell>H@3</cell><cell>H@1</cell><cell>MRR</cell><cell>H@10</cell><cell>H@3</cell><cell>H@1</cell></row><row><cell>CKRL</cell><cell>0.227</cell><cell>0.387</cell><cell>0.249</cell><cell>0.144</cell><cell>0.209</cell><cell>0.371</cell><cell>0.234</cell><cell>0.133</cell><cell>0.195</cell><cell>0.359</cell><cell>0.222</cell><cell>0.129</cell></row><row><cell>TransE</cell><cell>0.221</cell><cell>0.383</cell><cell>0.242</cell><cell>0.143</cell><cell>0.207</cell><cell>0.365</cell><cell>0.229</cell><cell>0.128</cell><cell>0.192</cell><cell>0.347</cell><cell>0.213</cell><cell>0.114</cell></row><row><cell>TransE-Score</cell><cell>0.224</cell><cell>0.385</cell><cell>0.244</cell><cell>0.147</cell><cell>0.209</cell><cell>0.364</cell><cell>0.229</cell><cell>0.135</cell><cell>0.195</cell><cell>0.349</cell><cell>0.221</cell><cell>0.125</cell></row><row><cell>TransE-NoiGAN</cell><cell>0.228</cell><cell>0.388</cell><cell>0.249</cell><cell>0.150</cell><cell>0.212</cell><cell>0.373</cell><cell>0.237</cell><cell>0.138</cell><cell>0.199</cell><cell>0.363</cell><cell>0.229</cell><cell>0.136</cell></row><row><cell>TransE-STRL (Ours)</cell><cell>0.229</cell><cell>0.388</cell><cell>0.251</cell><cell>0.152</cell><cell>0.218</cell><cell>0.379</cell><cell>0.244</cell><cell>0.140</cell><cell>0.206</cell><cell>0.371</cell><cell>0.231</cell><cell>0.142</cell></row><row><cell>TransE-MTRL (Ours)</cell><cell cols="3">0.231* 0.391* 0.255*</cell><cell>0.151</cell><cell cols="8">0.230* 0.388* 0.247* 0.147* 0.226* 0.382* 0.245* 0.144*</cell></row><row><cell>DistMult</cell><cell>0.202</cell><cell>0.360</cell><cell>0.229</cell><cell>0.125</cell><cell>0.189</cell><cell>0.347</cell><cell>0.213</cell><cell>0.112</cell><cell>0.175</cell><cell>0.331</cell><cell>0.198</cell><cell>0.099</cell></row><row><cell>DistMult-Score</cell><cell>0.202</cell><cell>0.365</cell><cell>0.226</cell><cell>0.126</cell><cell>0.191</cell><cell>0.349</cell><cell>0.219</cell><cell>0.117</cell><cell>0.181</cell><cell>0.334</cell><cell>0.206</cell><cell>0.102</cell></row><row><cell>DistMult-NoiGAN</cell><cell>0.206</cell><cell>0.366</cell><cell>0.235</cell><cell>0.129</cell><cell>0.194</cell><cell>0.355</cell><cell>0.221</cell><cell>0.119</cell><cell>0.189</cell><cell>0.339</cell><cell>0.219</cell><cell>0.116</cell></row><row><cell>DistMult-STRL (Ours)</cell><cell>0.209</cell><cell>0.371</cell><cell>0.234</cell><cell>0.133</cell><cell>0.199</cell><cell>0.365</cell><cell>0.223</cell><cell>0.120</cell><cell>0.192</cell><cell>0.351</cell><cell>0.223</cell><cell>0.119</cell></row><row><cell cols="4">DistMult-MTRL (Ours) 0.214* 0.376* 0.239*</cell><cell>0.131</cell><cell>0.207*</cell><cell>0.363</cell><cell cols="6">0.233* 0.122* 0.201* 0.355* 0.230* 0.121*</cell></row><row><cell>ConvE</cell><cell>0.242</cell><cell>0.391</cell><cell>0.261</cell><cell>0.159</cell><cell>0.229</cell><cell>0.381</cell><cell>0.247</cell><cell>0.142</cell><cell>0.212</cell><cell>0.364</cell><cell>0.228</cell><cell>0.131</cell></row><row><cell>ConvE-Score</cell><cell>0.244</cell><cell>0.392</cell><cell>0.259</cell><cell>0.162</cell><cell>0.233</cell><cell>0.388</cell><cell>0.251</cell><cell>0.144</cell><cell>0.219</cell><cell>0.366</cell><cell>0.234</cell><cell>0.137</cell></row><row><cell>ConvE-NoiGAN</cell><cell>0.249</cell><cell>0.399</cell><cell>0.267</cell><cell>0.168</cell><cell>0.238</cell><cell>0.392</cell><cell>0.259</cell><cell>0.149</cell><cell>0.229</cell><cell>0.375</cell><cell>0.258</cell><cell>0.147</cell></row><row><cell>ConvE-STRL (Ours)</cell><cell>0.252</cell><cell>0.402</cell><cell>0.271</cell><cell>0.171</cell><cell>0.242</cell><cell>0.404</cell><cell>0.263</cell><cell>0.161</cell><cell>0.231</cell><cell>0.381</cell><cell>0.255</cell><cell>0.159</cell></row><row><cell>ConvE-MTRL (Ours)</cell><cell cols="5">0.258* 0.405* 0.274* 0.173* 0.252*</cell><cell>0.401</cell><cell cols="6">0.268* 0.166* 0.244* 0.392* 0.259* 0.162*</cell></row><row><cell>RotatE</cell><cell>0.301</cell><cell>0.489</cell><cell>0.336</cell><cell>0.192</cell><cell>0.292</cell><cell>0.471</cell><cell>0.319</cell><cell>0.177</cell><cell>0.273</cell><cell>0.461</cell><cell>0.311</cell><cell>0.164</cell></row><row><cell>RotatE-Score</cell><cell>0.303</cell><cell>0.488</cell><cell>0.335</cell><cell>0.197</cell><cell>0.295</cell><cell>0.477</cell><cell>0.324</cell><cell>0.179</cell><cell>0.278</cell><cell>0.464</cell><cell>0.319</cell><cell>0.169</cell></row><row><cell>RotatE-NoiGAN</cell><cell>0.308</cell><cell>0.492</cell><cell>0.342</cell><cell>0.199</cell><cell>0.299</cell><cell>0.482</cell><cell>0.331</cell><cell>0.186</cell><cell>0.289</cell><cell>0.471</cell><cell>0.328</cell><cell>0.173</cell></row><row><cell>RotatE-STRL (Ours)</cell><cell>0.311</cell><cell>0.497</cell><cell>0.342</cell><cell>0.202</cell><cell>0.299</cell><cell>0.489</cell><cell>0.329</cell><cell>0.191</cell><cell>0.289</cell><cell>0.471</cell><cell>0.326</cell><cell>0.185</cell></row><row><cell>RotatE-MTRL (Ours)</cell><cell cols="12">0.318* 0.501* 0.349* 0.205* 0.311* 0.492* 0.338* 0.201* 0.302* 0.491* 0.331* 0.199*</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 12 .</head><label>12</label><figDesc>We divide the results into 4 groups, which are the extended models of TransE, DistMult, ConvE, and RotatE, respectively. Results in bold font are the best</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 7</head><label>7</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="7">Link prediction results on WN18RR-N1 to WN18RR-N3.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">WN18RR-N1</cell><cell></cell><cell></cell><cell cols="2">WN18RR-N2</cell><cell></cell><cell></cell><cell cols="2">WN18RR-N3</cell><cell></cell></row><row><cell></cell><cell>MRR</cell><cell>H@10</cell><cell>H@3</cell><cell>H@1</cell><cell>MRR</cell><cell>H@10</cell><cell>H@3</cell><cell>H@1</cell><cell>MRR</cell><cell>H@10</cell><cell>H@3</cell><cell>H@1</cell></row><row><cell>CKRL</cell><cell>0.221</cell><cell>0.496</cell><cell>0.443</cell><cell>0.141</cell><cell>0.213</cell><cell>0.479</cell><cell>0.422</cell><cell>0.114</cell><cell>0.189</cell><cell>0.458</cell><cell>0.392</cell><cell>0.101</cell></row><row><cell>TransE</cell><cell>0.219</cell><cell>0.489</cell><cell>0.431</cell><cell>0.137</cell><cell>0.201</cell><cell>0.471</cell><cell>0.414</cell><cell>0.112</cell><cell>0.179</cell><cell>0.446</cell><cell>0.389</cell><cell>0.094</cell></row><row><cell>TransE-Score</cell><cell>0.221</cell><cell>0.493</cell><cell>0.435</cell><cell>0.141</cell><cell>0.207</cell><cell>0.473</cell><cell>0.419</cell><cell>0.114</cell><cell>0.183</cell><cell>0.445</cell><cell>0.392</cell><cell>0.098</cell></row><row><cell>TransE-NoiGAN</cell><cell>0.225</cell><cell>0.499</cell><cell>0.447</cell><cell>0.143</cell><cell>0.219</cell><cell>0.477</cell><cell>0.425</cell><cell>0.117</cell><cell>0.194</cell><cell>0.461</cell><cell>0.399</cell><cell>0.106</cell></row><row><cell>TransE-STRL (Ours)</cell><cell cols="2">0.231* 0.503*</cell><cell>0.446</cell><cell cols="9">0.146* 0.221* 0.486* 0.431* 0.124* 0.199* 0.473* 0.413* 0.122*</cell></row><row><cell>TransE-MTRL (Ours)</cell><cell>0.226</cell><cell>0.499</cell><cell>0.445</cell><cell>0.145</cell><cell>0.219</cell><cell>0.479</cell><cell>0.428</cell><cell>0.116</cell><cell>0.196</cell><cell>0.468</cell><cell>0.399</cell><cell>0.113</cell></row><row><cell>DistMult</cell><cell>0.411</cell><cell>0.473</cell><cell>0.425</cell><cell>0.376</cell><cell>0.394</cell><cell>0.455</cell><cell>0.413</cell><cell>0.352</cell><cell>0.371</cell><cell>0.437</cell><cell>0.392</cell><cell>0.329</cell></row><row><cell>DistMult-Score</cell><cell>0.414</cell><cell>0.477</cell><cell>0.424</cell><cell>0.379</cell><cell>0.397</cell><cell>0.458</cell><cell>0.419</cell><cell>0.355</cell><cell>0.379</cell><cell>0.441</cell><cell>0.391</cell><cell>0.334</cell></row><row><cell>DistMult-NoiGAN</cell><cell>0.419</cell><cell>0.481</cell><cell>0.428</cell><cell>0.379</cell><cell>0.405</cell><cell>0.463</cell><cell>0.424</cell><cell>0.366</cell><cell>0.392</cell><cell>0.455</cell><cell>0.412</cell><cell>0.336</cell></row><row><cell>DistMult-STRL (Ours)</cell><cell cols="12">0.422* 0.485* 0.433* 0.385* 0.411* 0.472* 0.432* 0.371* 0.404* 0.461* 0.419* 0.353*</cell></row><row><cell>DistMult-MTRL (Ours)</cell><cell>0.421</cell><cell>0.483</cell><cell>0.426</cell><cell>0.381</cell><cell>0.408</cell><cell>0.466</cell><cell>0.429</cell><cell>0.365</cell><cell>0.395</cell><cell>0.454</cell><cell>0.417</cell><cell>0.336</cell></row><row><cell>ConvE</cell><cell>0.419</cell><cell>0.488</cell><cell>0.431</cell><cell>0.384</cell><cell>0.394</cell><cell>0.462</cell><cell>0.415</cell><cell>0.366</cell><cell>0.375</cell><cell>0.441</cell><cell>0.399</cell><cell>0.338</cell></row><row><cell>ConvE-Score</cell><cell>0.421</cell><cell>0.491</cell><cell>0.433</cell><cell>0.383</cell><cell>0.396</cell><cell>0.465</cell><cell>0.414</cell><cell>0.369</cell><cell>0.383</cell><cell>0.451</cell><cell>0.402</cell><cell>0.341</cell></row><row><cell>ConvE-NoiGAN</cell><cell>0.426</cell><cell>0.493</cell><cell>0.439</cell><cell>0.389</cell><cell>0.399</cell><cell>0.472</cell><cell>0.419</cell><cell>0.375</cell><cell>0.389</cell><cell>0.465</cell><cell>0.408</cell><cell>0.353</cell></row><row><cell>ConvE-STRL (Ours)</cell><cell cols="2">0.431* 0.499*</cell><cell>0.436</cell><cell cols="9">0.396* 0.412* 0.479* 0.433* 0.383* 0.401* 0.473* 0.419* 0.361*</cell></row><row><cell>ConvE-MTRL (Ours)</cell><cell>0.421</cell><cell>0.489</cell><cell>0.429</cell><cell>0.383</cell><cell>0.395</cell><cell>0.466</cell><cell>0.412</cell><cell>0.367</cell><cell>0.395</cell><cell>0.467</cell><cell>0.412</cell><cell>0.358</cell></row><row><cell>RotatE</cell><cell>0.431</cell><cell>0.534</cell><cell>0.459</cell><cell>0.398</cell><cell>0.414</cell><cell>0.516</cell><cell>0.442</cell><cell>0.381</cell><cell>0.395</cell><cell>0.498</cell><cell>0.427</cell><cell>0.357</cell></row><row><cell>RotatE-Score</cell><cell>0.431</cell><cell>0.533</cell><cell>0.461</cell><cell>0.403</cell><cell>0.417</cell><cell>0.519</cell><cell>0.448</cell><cell>0.385</cell><cell>0.401</cell><cell>0.503</cell><cell>0.431</cell><cell>0.364</cell></row><row><cell>RotatE-NoiGAN</cell><cell>0.433</cell><cell>0.539</cell><cell>0.463</cell><cell>0.411</cell><cell>0.429</cell><cell>0.538</cell><cell>0.453</cell><cell>0.389</cell><cell>0.411</cell><cell>0.518</cell><cell>0.441</cell><cell>0.372</cell></row><row><cell>RotatE-STRL (Ours)</cell><cell cols="3">0.439* 0.547* 0.471*</cell><cell>0.409</cell><cell>0.433*</cell><cell>0.537</cell><cell cols="6">0.461* 0.396* 0.426* 0.522* 0.452* 0.385*</cell></row><row><cell>RotatE-MTRL (Ours)</cell><cell>0.433</cell><cell>0.541</cell><cell>0.467</cell><cell>0.405</cell><cell>0.432</cell><cell>0.536</cell><cell>0.458</cell><cell>0.393</cell><cell>0.417</cell><cell>0.518</cell><cell>0.447</cell><cell>0.381</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 8</head><label>8</label><figDesc>Triple selection examples by different models. The value indicates the weight of the triple in the training procedure.</figDesc><table><row><cell>Triple Instance</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 10</head><label>10</label><figDesc>Comparison between X-MTRL and X-Score on FB15k-N1 to FB15k-N3.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">FB15k-N1</cell><cell></cell><cell></cell><cell cols="2">FB15k-N2</cell><cell></cell><cell></cell><cell cols="2">FB15k-N3</cell><cell></cell></row><row><cell></cell><cell>MRR</cell><cell>H@10</cell><cell>H@3</cell><cell>H@1</cell><cell>MRR</cell><cell>H@10</cell><cell>H@3</cell><cell>H@1</cell><cell>MRR</cell><cell>H@10</cell><cell>H@3</cell><cell>H@1</cell></row><row><cell>TransE-Score</cell><cell>0.362</cell><cell>0.631</cell><cell>0.424</cell><cell>0.221</cell><cell>0.341</cell><cell>0.598</cell><cell>0.414</cell><cell>0.203</cell><cell>0.313</cell><cell>0.548</cell><cell>0.372</cell><cell>0.179</cell></row><row><cell>TransE-MTRL (Ours)</cell><cell cols="12">0.378* 0.644* 0.450* 0.235* 0.367* 0.628* 0.437* 0.228* 0.336* 0.582* 0.397* 0.214*</cell></row><row><cell>DistMult-Score</cell><cell>0.433</cell><cell>0.695</cell><cell>0.515</cell><cell>0.293</cell><cell>0.355</cell><cell>0.651</cell><cell>0.422</cell><cell>0.221</cell><cell>0.251</cell><cell>0.527</cell><cell>0.285</cell><cell>0.128</cell></row><row><cell cols="13">DistMult-MTRL (Ours) 0.451* 0.713* 0.539* 0.309* 0.379* 0.670* 0.443* 0.234* 0.283* 0.561* 0.313* 0.153*</cell></row><row><cell>ConvE-Score</cell><cell>0.514</cell><cell>0.713</cell><cell>0.577</cell><cell>0.401</cell><cell>0.499</cell><cell>0.695</cell><cell>0.561</cell><cell>0.389</cell><cell>0.472</cell><cell>0.673</cell><cell>0.535</cell><cell>0.363</cell></row><row><cell>ConvE-MTRL (Ours)</cell><cell cols="12">0.526* 0.723* 0.588* 0.415* 0.515* 0.715* 0.579* 0.405* 0.502* 0.699* 0.561* 0.389*</cell></row><row><cell>RotatE-Score</cell><cell>0.768</cell><cell>0.834</cell><cell>0.802</cell><cell>0.723</cell><cell>0.744</cell><cell>0.818</cell><cell>0.785</cell><cell>0.686</cell><cell>0.731</cell><cell>0.788</cell><cell>0.758</cell><cell>0.664</cell></row><row><cell>RotatE-MTRL (Ours)</cell><cell cols="12">0.783* 0.849* 0.813* 0.736* 0.764* 0.831* 0.798* 0.706* 0.755* 0.815* 0.781* 0.689*</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>TABLE 11</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="9">Comparison between X-MTRL and X-Score on FB15k-237-N1 to FB15k-237-N3.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">FB15k-237-N1</cell><cell></cell><cell></cell><cell cols="2">FB15k-237-N2</cell><cell></cell><cell></cell><cell cols="2">FB15k-237-N3</cell><cell></cell></row><row><cell></cell><cell>MRR</cell><cell>H@10</cell><cell>H@3</cell><cell>H@1</cell><cell>MRR</cell><cell>H@10</cell><cell>H@3</cell><cell>H@1</cell><cell>MRR</cell><cell>H@10</cell><cell>H@3</cell><cell>H@1</cell></row><row><cell>TransE-Score</cell><cell>0.221</cell><cell>0.383</cell><cell>0.244</cell><cell>0.143</cell><cell>0.205</cell><cell>0.359</cell><cell>0.221</cell><cell>0.134</cell><cell>0.192</cell><cell>0.345</cell><cell>0.217</cell><cell>0.120</cell></row><row><cell>TransE-MTRL (Ours)</cell><cell cols="12">0.231* 0.391* 0.255* 0.151* 0.230* 0.388* 0.247* 0.147* 0.226* 0.382* 0.245* 0.144*</cell></row><row><cell>DistMult-Score</cell><cell>0.199</cell><cell>0.361</cell><cell>0.222</cell><cell>0.125</cell><cell>0.188</cell><cell>0.344</cell><cell>0.216</cell><cell>0.113</cell><cell>0.177</cell><cell>0.331</cell><cell>0.203</cell><cell>0.101</cell></row><row><cell cols="2">DistMult-MTRL (Ours) 0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>.214* 0.376* 0.239* 0.131* 0.207* 0.363* 0.233* 0.122* 0.201* 0.355* 0.230* 0.121*</head><label></label><figDesc></figDesc><table><row><cell>ConvE-Score</cell><cell>0.241</cell><cell>0.391</cell><cell>0.255</cell><cell>0.158</cell><cell>0.230</cell><cell>0.384</cell><cell>0.244</cell><cell>0.141</cell><cell>0.218</cell><cell>0.364</cell><cell>0.231</cell><cell>0.133</cell></row><row><cell>ConvE-MTRL (Ours)</cell><cell cols="12">0.258* 0.405* 0.274* 0.173* 0.252* 0.401* 0.268* 0.166* 0.244* 0.392* 0.259* 0.162*</cell></row><row><cell>RotatE-Score</cell><cell>0.301</cell><cell>0.488</cell><cell>0.332</cell><cell>0.196</cell><cell>0.294</cell><cell>0.479</cell><cell>0.320</cell><cell>0.175</cell><cell>0.275</cell><cell>0.461</cell><cell>0.317</cell><cell>0.164</cell></row><row><cell>RotatE-MTRL (Ours)</cell><cell cols="12">0.318* 0.501* 0.349* 0.205* 0.311* 0.492* 0.338* 0.201* 0.302* 0.491* 0.331* 0.199*</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>TABLE 12</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="9">Comparison between X-STRL and X-Score on WN18RR-N1 to WN18RR-N3.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">WN18RR-N1</cell><cell></cell><cell></cell><cell cols="2">WN18RR-N2</cell><cell></cell><cell></cell><cell cols="2">WN18RR-N3</cell><cell></cell></row><row><cell></cell><cell>MRR</cell><cell>H@10</cell><cell>H@3</cell><cell>H@1</cell><cell>MRR</cell><cell>H@10</cell><cell>H@3</cell><cell>H@1</cell><cell>MRR</cell><cell>H@10</cell><cell>H@3</cell><cell>H@1</cell></row><row><cell>TransE-Score</cell><cell>0.219</cell><cell>0.491</cell><cell>0.434</cell><cell>0.138</cell><cell>0.203</cell><cell>0.471</cell><cell>0.415</cell><cell>0.109</cell><cell>0.181</cell><cell>0.443</cell><cell>0.388</cell><cell>0.095</cell></row><row><cell>TransE-STRL (Ours)</cell><cell cols="12">0.231* 0.503* 0.446* 0.146* 0.221* 0.486* 0.431* 0.124* 0.199* 0.473* 0.413* 0.122*</cell></row><row><cell>DistMult-Score</cell><cell>0.411</cell><cell>0.475</cell><cell>0.423</cell><cell>0.375</cell><cell>0.395</cell><cell>0.453</cell><cell>0.417</cell><cell>0.351</cell><cell>0.375</cell><cell>0.437</cell><cell>0.388</cell><cell>0.331</cell></row><row><cell cols="2">DistMult-STRL (Ours) 0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>.422* 0.485* 0.433* 0.385* 0.411* 0.472* 0.432* 0.371* 0.404* 0.461* 0.419* 0.353*</head><label></label><figDesc></figDesc><table><row><cell>ConvE-Score</cell><cell>0.418</cell><cell>0.488</cell><cell>0.430</cell><cell>0.381</cell><cell>0.397</cell><cell>0.461</cell><cell>0.409</cell><cell>0.365</cell><cell>0.383</cell><cell>0.450</cell><cell>0.402</cell><cell>0.339</cell></row><row><cell>ConvE-STRL (Ours)</cell><cell>0.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>431* 0.499* 0.436* 0.396* 0.412* 0.479* 0.433* 0.383* 0.401* 0.473* 0.419* 0.361*</head><label></label><figDesc></figDesc><table><row><cell>RotatE-Score</cell><cell>0.429</cell><cell>0.532</cell><cell>0.458</cell><cell>0.401</cell><cell>0.418</cell><cell>0.517</cell><cell>0.445</cell><cell>0.381</cell><cell>0.399</cell><cell>0.505</cell><cell>0.428</cell><cell>0.361</cell></row><row><cell>RotatE-STRL (Ours)</cell><cell>0.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>439* 0.547* 0.471* 0.409* 0.433* 0.537* 0.461* 0.396* 0.426* 0.522* 0.452* 0.385*</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>TABLE 13</cell></row><row><cell cols="2">Statistics of Datasets.</cell></row><row><cell>Datasets</cell><cell>#training triples</cell></row><row><cell>FB15k-237-N1</cell><cell>299,326</cell></row><row><cell cols="2">FB15k-237-N1-80% 239,460</cell></row><row><cell cols="2">FB15k-237-N1-60% 179,595</cell></row><row><cell cols="2">FB15k-237-N1-40% 119,730</cell></row><row><cell cols="2">FB15k-237-N1-20% 59,865</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>TABLE 14</head><label>14</label><figDesc>Link prediction results on sparse KGs.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">FB15k-237-N1</cell><cell></cell><cell></cell><cell cols="2">FB15k-237-N1-80%</cell><cell></cell><cell cols="3">FB15k-237-N1-60%</cell><cell></cell><cell></cell><cell cols="2">FB15k-237-N1-40%</cell><cell></cell><cell></cell><cell cols="2">FB15k-237-N1-20%</cell><cell></cell></row><row><cell></cell><cell>MRR</cell><cell>H@10</cell><cell>H@3</cell><cell>H@1</cell><cell>MRR</cell><cell>H@10</cell><cell>H@3</cell><cell>H@1</cell><cell>MRR</cell><cell>H@10</cell><cell>H@3</cell><cell>H@1</cell><cell>MRR</cell><cell>H@10</cell><cell>H@3</cell><cell>H@1</cell><cell>MRR</cell><cell>H@10</cell><cell>H@3</cell><cell>H@1</cell></row><row><cell>CKRL</cell><cell>0.227</cell><cell>0.387</cell><cell>0.249</cell><cell>0.144</cell><cell>0.193</cell><cell>0.324</cell><cell>0.218</cell><cell>0.126</cell><cell>0.144</cell><cell>0.246</cell><cell>0.152</cell><cell>0.107</cell><cell>0.115</cell><cell>0.179</cell><cell>0.103</cell><cell>0.078</cell><cell>0.047</cell><cell>0.083</cell><cell>0.052</cell><cell>0.026</cell></row><row><cell>TransE</cell><cell>0.221</cell><cell>0.383</cell><cell>0.242</cell><cell>0.143</cell><cell>0.191</cell><cell>0.322</cell><cell>0.214</cell><cell>0.124</cell><cell>0.147</cell><cell>0.251</cell><cell>0.155</cell><cell>0.115</cell><cell>0.122</cell><cell>0.181</cell><cell>0.105</cell><cell>0.089</cell><cell>0.059</cell><cell>0.089</cell><cell>0.056</cell><cell>0.031</cell></row><row><cell>TransE-NoiGAN</cell><cell>0.228</cell><cell>0.388</cell><cell>0.249</cell><cell>0.150</cell><cell>0.199</cell><cell>0.329</cell><cell>0.221</cell><cell>0.131</cell><cell>0.149</cell><cell>0.262</cell><cell>0.162</cell><cell>0.119</cell><cell>0.126</cell><cell>0.183</cell><cell>0.105</cell><cell>0.086</cell><cell>0.063</cell><cell>0.088</cell><cell>0.059</cell><cell>0.033</cell></row><row><cell>TransE-STRL (Ours)</cell><cell>0.229</cell><cell>0.388</cell><cell>0.251</cell><cell>0.152</cell><cell>0.207</cell><cell>0.335</cell><cell>0.224</cell><cell>0.136</cell><cell>0.154</cell><cell>0.271</cell><cell>0.168</cell><cell>0.114</cell><cell>0.132</cell><cell>0.188</cell><cell>0.109</cell><cell>0.086</cell><cell>0.066</cell><cell>0.087</cell><cell>0.064</cell><cell>0.032</cell></row><row><cell>TransE-MTRL (Ours)</cell><cell cols="3">0.231* 0.391* 0.255*</cell><cell>0.151</cell><cell cols="7">0.211* 0.342* 0.231* 0.144* 0.158* 0.277* 0.181*</cell><cell>0.117</cell><cell cols="8">0.137* 0.191* 0.114* 0.098* 0.073* 0.097* 0.071* 0.039*</cell></row><row><cell>DistMult</cell><cell>0.202</cell><cell>0.360</cell><cell>0.229</cell><cell>0.125</cell><cell>0.173</cell><cell>0.305</cell><cell>0.196</cell><cell>0.101</cell><cell>0.143</cell><cell>0.256</cell><cell>0.162</cell><cell>0.089</cell><cell>0.103</cell><cell></cell><cell>0.089</cell><cell>0.067</cell><cell>0.033</cell><cell>0.075</cell><cell>0.047</cell><cell>0.009</cell></row><row><cell>DistMult-NoiGAN</cell><cell>0.206</cell><cell>0.366</cell><cell>0.235</cell><cell>0.129</cell><cell>0.173</cell><cell>0.311</cell><cell>0.199</cell><cell>0.106</cell><cell>0.146</cell><cell>0.251</cell><cell>0.166</cell><cell>0.091</cell><cell>0.109</cell><cell>0.166</cell><cell>0.091</cell><cell>0.069</cell><cell>0.036</cell><cell>0.087</cell><cell>0.049</cell><cell>0.014</cell></row><row><cell>DistMult-STRL (Ours)</cell><cell>0.209</cell><cell>0.371</cell><cell>0.234</cell><cell>0.133</cell><cell>0.177</cell><cell>0.311</cell><cell>0.206</cell><cell>0.104</cell><cell>0.151</cell><cell>0.259</cell><cell>0.167</cell><cell>0.099</cell><cell>0.112</cell><cell>0.172</cell><cell>0.092</cell><cell>0.072</cell><cell>0.034</cell><cell>0.079</cell><cell>0.045</cell><cell>0.018</cell></row><row><cell cols="4">DistMult-MTRL (Ours) 0.214* 0.376* 0.239*</cell><cell>0.131</cell><cell cols="9">0.185* 0.319* 0.215* 0.113* 0.154* 0.263* 0.171* 0.104* 0.118*</cell><cell>0.169</cell><cell cols="2">0.099* 0.076*</cell><cell>0.034</cell><cell>0.083</cell><cell cols="2">0.053* 0.022*</cell></row><row><cell>ConvE</cell><cell>0.242</cell><cell>0.391</cell><cell>0.261</cell><cell>0.159</cell><cell>0.219</cell><cell>0.341</cell><cell>0.219</cell><cell>0.142</cell><cell>0.181</cell><cell>0.279</cell><cell>0.198</cell><cell>0.128</cell><cell>0.143</cell><cell>0.224</cell><cell>0.147</cell><cell>0.098</cell><cell>0.077</cell><cell>0.103</cell><cell>0.072</cell><cell>0.056</cell></row><row><cell>ConvE-NoiGAN</cell><cell>0.249</cell><cell>0.399</cell><cell>0.267</cell><cell>0.168</cell><cell>0.222</cell><cell>0.344</cell><cell>0.229</cell><cell>0.144</cell><cell>0.183</cell><cell>0.283</cell><cell>0.201</cell><cell>0.131</cell><cell>0.144</cell><cell>0.231</cell><cell>0.154</cell><cell>0.102</cell><cell>0.078</cell><cell>0.104</cell><cell>0.074</cell><cell>0.059</cell></row><row><cell>ConvE-STRL (Ours)</cell><cell>0.252</cell><cell>0.402</cell><cell>0.271</cell><cell>0.171</cell><cell>0.229</cell><cell>0.349</cell><cell>0.238</cell><cell>0.154</cell><cell>0.189</cell><cell>0.288</cell><cell>0.203</cell><cell>0.133</cell><cell>0.154</cell><cell>0.239</cell><cell>0.161</cell><cell>0.105</cell><cell>0.082</cell><cell>0.112</cell><cell>0.079</cell><cell>0.063</cell></row><row><cell>ConvE-MTRL (Ours)</cell><cell cols="6">0.258* 0.405* 0.274* 0.173* 0.231* 0.351*</cell><cell>0.231</cell><cell cols="10">0.162* 0.192* 0.292* 0.209* 0.139* 0.161* 0.245* 0.169* 0.111* 0.084*</cell><cell>0.109</cell><cell cols="2">0.083* 0.068*</cell></row><row><cell>RotatE</cell><cell>0.301</cell><cell>0.489</cell><cell>0.336</cell><cell>0.192</cell><cell>0.251</cell><cell>0.402</cell><cell>0.279</cell><cell>0.171</cell><cell>0.225</cell><cell>0.335</cell><cell>0.242</cell><cell>0.154</cell><cell>0.174</cell><cell>0.243</cell><cell>0.163</cell><cell>0.114</cell><cell>0.087</cell><cell>0.119</cell><cell>0.083</cell><cell>0.071</cell></row><row><cell>RotatE-NoiGAN</cell><cell>0.308</cell><cell>0.492</cell><cell>0.342</cell><cell>0.199</cell><cell>0.258</cell><cell>0.409</cell><cell>0.283</cell><cell>0.174</cell><cell>0.228</cell><cell>0.343</cell><cell>0.245</cell><cell>0.155</cell><cell>0.176</cell><cell>0.246</cell><cell>0.169</cell><cell>0.121</cell><cell>0.088</cell><cell>0.124</cell><cell>0.091</cell><cell>0.067</cell></row><row><cell>RotatE-STRL (Ours)</cell><cell>0.311</cell><cell>0.497</cell><cell>0.342</cell><cell>0.202</cell><cell>0.264</cell><cell>0.405</cell><cell>0.291</cell><cell>0.179</cell><cell>0.231</cell><cell>0.339</cell><cell>0.248</cell><cell>0.157</cell><cell>0.176</cell><cell>0.249</cell><cell>0.168</cell><cell>0.132</cell><cell>0.088</cell><cell>0.123</cell><cell>0.092</cell><cell>0.066</cell></row><row><cell>RotatE-MTRL (Ours)</cell><cell cols="7">0.318* 0.501* 0.349* 0.205* 0.269* 0.417* 0.294*</cell><cell>0.177</cell><cell>0.233*</cell><cell>0.342</cell><cell cols="5">0.253* 0.161* 0.185* 0.255* 0.177*</cell><cell>0.128</cell><cell>0.094*</cell><cell>0.123</cell><cell cols="2">0.102* 0.074*</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE 15</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Triple Classification Results.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A THE IMPACT OF THE CLUSTERING ALGORITHM ON THE FINAL RESULTS</head><p>In this section, we analyze the impact of the clustering algorithm on the final results. In this paper, we use the kmeans algorithm to cluster relations into different groups. Figure <ref type="figure">5</ref>, Figure <ref type="figure">6</ref> and Figure <ref type="figure">7</ref> show the impact of the relation cluster number on the final results. We have the following findings.</p><p>(1) There exists an optimal value for the number of relation clusters on FB15k and FB15k-237 based datasets. The models keep achieving better results as the number of relations goes from 0 to the optimal value. Then, after the value exceeds the optimal point, the results start falling down to a stable value. The reason lies as: (i) a small value of relation clusters leads to large-sized relation clusters. In this case, some unrelated relations may join in the same cluster, and degrade the performance; (ii) a large value of relation clusters leads to small-sized relation clusters, thus each relation cannot take full advantage of the information from semantically related relations. In this case, the results are also unsatisfactory.</p><p>(2) On WN18RR-based datasets, the results keep going up as the number of relation clusters increases. The reason is that the 11 relations in WN18RR are semantically unrelated. The information from semantically related relations is able to benefit the results, and the information from unrelated relations may degrade the performance. Our findings again validate that the MTRL model is more useful for KGs which have dense semantic distributions over relations, while the STRL model is more suitable for KGs in which the semantic correlations among relations are weak. It is worth noting that we also tried different random seeds to run the clustering algorithm, but find the seed does not have a big effect on the final results (up to MRR score). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX B LINK PREDICTION RESULTS ON STATE-OF-THE-ART KGE MODELS</head><p>To further evaluate the generalization performance of the proposed framework, we extend popular knowledge graph  embedding (KGE) models R-GCN <ref type="bibr" target="#b37">[28]</ref>, CompGCN <ref type="bibr" target="#b58">[49]</ref> and ComplEx-N3 <ref type="bibr" target="#b59">[50]</ref> with our framework. The results are shown in Table <ref type="table">16</ref>   </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<idno>75.1 / 73.9 / 72.2 81.5 / 79.6 / 77.3 81.9 / 80.7 / 78.8</idno>
		<title level="m">TransE</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><surname>Transe-Noigan</surname></persName>
		</author>
		<idno>75.6 / 75.1 / 73.2 81.7 / 80.4 / 78.9 82.4 / 81.4 / 79.9</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Transe-Strl</forename></persName>
		</author>
		<ptr target="Ours)75.9/75.4/74.281.9/80.5/78.982.9*/82.5*/81.1*TransE-MTRL" />
		<imprint>
			<biblScope unit="page">76</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><surname>Distmult-Noigan</surname></persName>
		</author>
		<idno>74.4 / 74.4 / 72.3 84.8 / 84.4 / 81.8 83.6 / 82.9 / 81.9</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Distmult-Strl</forename></persName>
		</author>
		<ptr target="Ours)74.8/74.6/73.484.7/84.1/82.283.4/83.1*/82.7*DistMult-MTRL" />
		<imprint>
			<biblScope unit="page">74</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<idno>77.8 / 76.9 / 75.4 86.9 / 85.5 / 81.3 82.9 / 82.1 / 80.8</idno>
		<title level="m">ConvE</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<idno>77.9 / 77.2 / 75.9 87.2 / 85.9 / 82.1 82.8 / 82.3 / 81.4</idno>
		<title level="m">ConvE-Score</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName><surname>Conve-Noigan</surname></persName>
		</author>
		<idno>77.8 / 77.6 / 76.8 87.2 / 86.2 / 83.3 82.9 / 82.8 / 81.9</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Conve-Strl</forename></persName>
		</author>
		<ptr target="Ours)78.3/77.8/77.187.4/86.5/83.683.8*/83.6*/83.1*ConvE-MTRL" />
		<imprint>
			<biblScope unit="page">78</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">-</forename><surname>Rotate</surname></persName>
		</author>
		<author>
			<persName><surname>Strl</surname></persName>
		</author>
		<ptr target="Ours)79.6/78.8/77.987.9/87.1/84.184.7*/84.5*/83.9*RotatE-MTRL" />
		<imprint>
			<biblScope unit="page">79</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Entity query feature expansion using knowledge base links</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dalton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dietz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="365" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Building watson: An overview of the deepqa project</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ferrucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chu-Carroll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gondek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Kalyanpur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lally</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Murdock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Nyberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Prager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">AI magazine</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="59" to="79" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-IJCNLP</title>
				<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1003" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Connecting language and knowledge bases with embedding models for relation extraction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Yakhnenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1366" to="1371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multirelational data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning entity and relation embeddings for knowledge graph completion</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2181" to="2187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ripplenet: Propagating user preferences on the knowledge graph for recommender systems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Information and Knowledge Management</title>
				<meeting>the 27th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="417" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName><forename type="first">K</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Communications of the ACM</title>
				<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="39" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Yago: a core of semantic knowledge</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kasneci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th international conference on World Wide Web</title>
				<meeting>the 16th international conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="697" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Toward an architecture for never-ending language learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Betteridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kisiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Settles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Hruschka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Fourth AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Gene ontology: tool for the of biology</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ashburner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Botstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dolinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Dwight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Eppig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature genetics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="29" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Structured minimally supervised learning for neural relation extraction</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3057" to="3069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Does william shakespeare really write hamlet? knowledge representation learning with confidence</title>
		<author>
			<persName><forename type="first">R</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Vandalism detection in wikidata</title>
		<author>
			<persName><forename type="first">S</forename><surname>Heindorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Engels</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="327" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Noigan: Noise aware knowledge graph embedding with gan</title>
		<author>
			<persName><surname>Anonymous</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rkgTdkrtPH" />
	</analytic>
	<monogr>
		<title level="m">Submitted to International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding with hierarchical relation structure</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3198" to="3207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>-T. Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Convolutional 2d knowledge graph embeddings</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Rotate: Knowledge graph embedding by relational rotation in complex space</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding by translating on hyperplanes</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1112" to="1119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Stranse: a novel embedding model of entities and relationships in knowledge bases</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sirts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter</title>
				<meeting>the 2016 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="460" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Rotate3d: Representing relations as rotations in three-dimensional space for knowledge graph embedding</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management</title>
				<meeting>the 29th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="385" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName><forename type="first">T</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">É</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2071" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A three-way model for collective learning on multi-relational data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Icml</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="809" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A novel embedding model for knowledge base completion based on convolutional neural network</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Phung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="327" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Interacte: Improving convolution-based knowledge graph embeddings by increasing feature interactions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Vashishth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nitin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Talukdar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Semantic Web Conference</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning attention-based embeddings for relation prediction in knowledge graphs</title>
		<author>
			<persName><forename type="first">D</forename><surname>Nathani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chauhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kaul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Representation learning of knowledge graphs with entity descriptions</title>
		<author>
			<persName><forename type="first">R</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2659" to="2665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Knowledge graph representation with jointly structural and textual encoding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1318" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Jointly embedding knowledge graphs and logical rules</title>
		<author>
			<persName><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="192" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding with iterative guidance from soft rules</title>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Dbpedia: A nucleus for a web of open data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kobilarov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cyganiak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ives</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The semantic web</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="722" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Yago2: A spatially and temporally enhanced knowledge base from wikipedia</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Berberich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">194</biblScope>
			<biblScope unit="page" from="28" to="61" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Wikidata: a free collaborative knowledgebase</title>
		<author>
			<persName><forename type="first">D</forename><surname>Vrandečić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kr Ötzsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="78" to="85" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Embedding uncertain knowledge graphs</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zaniolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3363" to="3370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Multie: Multi-task embedding for knowledge base completion</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Information and Knowledge Management</title>
				<meeting>the 27th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1715" to="1718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Multitask feature learning for knowledge graph enhanced recommendation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2000" to="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3219" to="3232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deeppath: A reinforcement learning method for knowledge graph reasoning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="564" to="573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Multi-hop knowledge graph reasoning with reward shaping</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">V</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3243" to="3253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Reinforcement learning for relation classification from noisy data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Robust distant supervision relation extraction via deep reinforcement learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Regularized multi-task learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Evgeniou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Observed versus latent features for knowledge base and text inference</title>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality</title>
				<meeting>the 3rd Workshop on Continuous Vector Space Models and their Compositionality</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="57" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Compositionbased multi-relational graph convolutional networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Vashishth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nitin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Talukdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Canonical tensor decomposition for knowledge base completion</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Obozinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2863" to="2872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="926" to="934" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
