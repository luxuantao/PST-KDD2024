<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Mi</forename><surname>Wang</surname></persName>
							<email>wangmi@whu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory of Information Engi-neering in Surveying, Mapping, and Remote Sensing</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<postCode>430079</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/LGRS.2021.3049125</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic Segmentation for Remote Sensing Images</head><p>Based on Adaptive Feature Selection Network Shao Xiang , Quangqi Xie, and Mi Wang</p><p>Abstract-Semantic segmentation plays a vital role in the segmentation of remote sensing field for its wide range of applications. The major current method for segmentation of remotely sensed imagery is using multiple scales strategy to improve the performance of segmentation networks. However, the ground object with uncertain scale in high-resolution aerial imagery is difficult to be segmented with conventional models. To address this problem, an adaptive feature selection module is designed, in which attention module learns weight contributions of each feature blocks in different scales. We employ the pyramid scene parsing network (PSPNet), DeepLabV3, and U-Net with the proposed module to conduct experiments on two benchmarks (the Vaihingen set and the WHU Building data set). The experimental results and comprehensive analysis validate the efficiency and practicability of the proposed method in semantic segmentation of remote sensing images.</p><p>Index Terms-Adaptive feature selection (AFS), remote sensing images, semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Semantic segmentation has great significance in remote sensing field. Remote-sensing image analysis has become an active research topic, and it was widely used in many applications such as environmental monitoring, urban planning, and land-used management. Many conventional segmentation methods used hand-crafted features to obtain segmented results. In this way, it is hard to achieve a significant performance for the complex appearance variations of ground objects in very high resolution (VHR) aerial imagery.</p><p>In recent years, with the development of deep learning, many state-of-the-art networks have emerged that can be used as encoding backbone, such as VGG <ref type="bibr" target="#b0">[1]</ref>, ResNet <ref type="bibr" target="#b1">[2]</ref>, and Inception <ref type="bibr" target="#b2">[3]</ref>. As an important branch of computer vision, semantic segmentation plays an extremely important role in image understanding. A battery of high-performance convolution neural networks (CNNs)-based models has been designed to achieve outstanding performance in many vision tasks, such as fully convolutional networks (FCNs) <ref type="bibr" target="#b3">[4]</ref>, U-Net <ref type="bibr" target="#b4">[5]</ref>, DeepLab <ref type="bibr" target="#b5">[6]</ref>, pyramid scene parsing network (PSP-Net) <ref type="bibr" target="#b6">[7]</ref>, and so on. Over the last few years, numerous attempts have been made by using these existing deep learning models <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b9">[10]</ref> (such as, Hourglass, U-Net, DenseNet, and so on) in the field of high-resolution aerial images.</p><p>Recently, Lin et al. <ref type="bibr" target="#b10">[11]</ref> proposed a scale-aware module (SAM) by resampling strategy aimed to make pixels adjust their positions with different scales to capture the characteristics of inconsistent scale from high-resolution remote sensing images. Wang et al. <ref type="bibr" target="#b11">[12]</ref> proposed an integrated network, which combines the advantages of atrous spatial pyramid pooling (ASPP) network and encoder-decoder structures, the aims are not only to extract multiscale contextual information, but also to restore sharper object boundaries simultaneously. To solve the problem of the loss of detailed information due to the use of downsampling operations, Sun and Wang <ref type="bibr" target="#b12">[13]</ref> proposed a maximum fusion scheme, which aims to improve the performance of model by effectively combining semantic information from deep layers and detailed information from shallow layers. In order to use digital surface models (DSMs) information to further improve the semantic segmentation results, Cao et al. <ref type="bibr" target="#b13">[14]</ref> proposed a DSM fusion (DSMF) branch structure model.</p><p>Pyramid pooling layer (PPL) <ref type="bibr" target="#b14">[15]</ref> and DeepLab <ref type="bibr" target="#b15">[16]</ref> have shown remarkable learning capabilities of feature representations in remote sensing images. PPL can effectively extract multiscale semantic information and synthesize global contextual information by using different scales. DeepLab employs the ASPP with multiple atrous convolutions to enhance the receptive field. However, one common shortage for the both approaches is that researchers need to adjust the multiple scales or atrous convolutions with different rates based on experience in previous works. To this end, we propose an adaptive feature selection (AFS) module to deal with the shortage. Previously, Li et al. <ref type="bibr" target="#b16">[17]</ref> proposed a selective kernel (SK) unit by fusing multiple branches with different kernel sizes, and multiple branches yield different sizes of the effective receptive fields in the fusion layer by using softmax attention that is guided by the information in these branches. Inspired by SK unit, we introduce a dynamic selection mechanism in PPL and DeepLab that allows each neuron to adaptively select feature information based on multiple scales or multiple receptive field sizes from different feature map blocks.</p><p>In this letter, the proposed method applies U-Net, PSPNet, and DeepLabV3 to segment multiple ground objects in remote sensing images simultaneously with a significant result. The main contributions of this letter are listed as follows.  select important feature information by using softmax attention.</p><p>2) The proposed method can be easily applied in U-Net, PSPNet, and DeepLab to boost performance of models at the cost of fewer parameters. 3) Compared with the state-of-the-art networks, we achieved significant results on Vaihingen data set <ref type="bibr" target="#b17">[18]</ref> and WHU Building data set <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. METHODS</head><p>In this section, we will elaborate the proposed AFS module and the details of application of AFS in PSPNet, DeepLabV3, and U-Net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Adaptive Feature Selection</head><p>To enable the model to adaptively select feature information, an AFS module is designed, in which multiple feature map blocks with different receptive fields are fused using attention mechanism. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, for any given feature map blocks X = {X i ,i ∈ [1, s]}, X i ∈ c×w×h , where c is the number of input channel, h and w represent spatial size, and s is the number of multiple feature map blocks with different receptive fields. We first conduct element-wise summation operations</p><formula xml:id="formula_0">X = s i=1 X i .</formula><p>(</p><p>Then, the global feature information was captured using global average pooling to generate channel-wise statistics S, and</p><formula xml:id="formula_2">S ∈ c×1 S = G P( X ) (2)</formula><p>where GP indicates the global average pooling operation. By default, we use a fully connected (FC) layer followed by batch normalization (BN) B and rectified linear unit (ReLU) activation σ to enable the guidance for the automatic selections. We can get Ŝ = σ (B(FC(S))).</p><p>(</p><formula xml:id="formula_3">)<label>3</label></formula><p>For better efficiency, we use the strategy that reduces the dimensionality of output in first FC layer, then transforming the dimensionality to match the number of multiple feature map blocks by using second FC layer without BN and ReLU.</p><p>To study the impact of multiple feature maps with different receptive fields, we use a soft attention across channels to adaptively generate multiple weights by softmax attention</p><formula xml:id="formula_4">P i = S i s i S i (<label>4</label></formula><formula xml:id="formula_5">)</formula><p>where P i is the attention weight of i th feature map block. Note that, P i ∈ s×1×1 and s i=1 P i = 1. In the next step, we use the matrix multiplication transformation to obtain new feature map blocks X with the attention weights on various receptive fields. The calculation can be formulated as</p><formula xml:id="formula_6">X = {X i • P i , i ∈ [1, s]}. (<label>5</label></formula><formula xml:id="formula_7">)</formula><p>In this way, neurons can achieve the automatic feature selection by learning the attention weights. Finally, the feature selection is finished via an element-wise summation</p><formula xml:id="formula_8">Y = s i=1 Xi (<label>6</label></formula><formula xml:id="formula_9">)</formula><p>where Y denotes the output of AFS module, and Y ∈ c×w×h . Through feature selection of AFS module, the attention module learns the weight contributions of each feature block with different receptive fields. The final feature map Y integrates multiple feature maps information and effectively reducing the redundant features to achieve AFS. Squeeze-andexcitation network (SENet) <ref type="bibr" target="#b19">[20]</ref> utilizes the gating mechanism to adaptively recalibrate the channel-wise feature responses via channel-wise importance. In contrast, our method focuses on the weight contributions on different branches by using attention mechanisms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. PSPNet-AFS</head><p>To validate the effectiveness of AFS module, we use PSPNet <ref type="bibr" target="#b6">[7]</ref> and ResNet50 <ref type="bibr" target="#b1">[2]</ref> as the encoding networks. PSPNet is a scene segmentation model based on PPL, which enables to provide an effective global contextual prior for pixel-level scene parsing and extract levels information. Multiple pixel-levels play a vital role in boosting the performance of network. PSPNet uses four-level feature maps with bin sizes of 1 × 1, 2 × 2, 3 × 3, and 6 × 6, respectively. The superior results demonstrate the effectiveness of PPL on several common data sets. During the process of experiments, we find that the four-level-based PSPNet obtained poor performance compared with AFS-based PSPNet.</p><p>Fig. <ref type="figure" target="#fig_1">2</ref>(a) depicts the position of AFS module in PSPNet. We introduce a novel PSPNet-AFS model which consists of two parts, PSPNet and AFS module. In this work, we employ more than four levels of receptive fields to segment remote sensing images that aims to obtain more effective representation. In this letter, we use the feature map blocks with scales from 1 to 32. Deep supervision is an effective method to help optimize the learning process. Therefore, we use additional supervision as the auxiliary branch loss. Weight of 0.2 is used to balance the auxiliary loss in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. DeepLabV3-AFS</head><p>For evaluating the generalization of the proposed module, we used the DeepLabV3 as the baseline to further prove the proposed method. Fig. <ref type="figure" target="#fig_1">2</ref>(b) shows the AFS-based DeepLabV3 model. As a hot spot of high-resolution aerial images, the problem of multiple scales in segmenting objects can be handled through using convolution with multiple atrous rates. In this work, we use AFS module to achieve automatic selection for different atrous rates, which aims to effectively enlarge the receptive field of filters to incorporate multiscale context in high-resolution aerial images. To this end, we employ numerous atrous rates to obtain rich and effective receptive fields from feature maps by using the proposed AFS module. In this letter, we used the atrous convolution kernel with the maximum rates of 8, 16, and 32, strides with 1, 2, and 4, respectively. During the training process, auxiliary branch loss with weight of 0.2 is used in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. U-Net-AFS</head><p>Fig. <ref type="figure" target="#fig_1">2(c</ref>) shows the structure of U-Net-AFS. PSPNet and DeepLabV3 use multiscale feature maps (&gt;2) to learn semantic information. In contrast, U-Net employs skipconnection method to fuse the contextual information, and the high-resolution features from the shallow layers are combined with the upsampling layers to enhance the information content in feature maps. Therefore, we employed the proposed structure to adaptively select contextual information from downsampling and upsampling layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTS</head><p>A. General Setup 1) Data Set: Vaihingen data set <ref type="bibr" target="#b17">[18]</ref> contains 33 highresolution aerial images (of different sizes, average size of around 2500 × 2000). Each image has three bands, including infrared, red, and green bands (IRRGs). Among the data sets, 16 ground-truth images are available for training, and the rest for testing. The ground-truth contains five classes, including impervious surfaces, buildings, low vegetation, trees, and cars. Note that, we only use the IRRG images with multispectral information in our experiment without DSM information. In this work, we split training data into training set and validation set for conducting experiments, and all training data are used for training in the testing phase. Additionally, we test the method on WHU Building data set <ref type="bibr" target="#b18">[19]</ref> to further evaluate the performance of AFS-based models. Satellite data set I (global cities) is used in this letter, which contains 204 images (512 × 512 pixel with resolutions varying from 0.3 to 2.5 m).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Details of Implementation:</head><p>We crop the original images with size of 256 × 256 due to the limited memory of GPU, using the sliding window strategy with overlapped 50 pixels. During the training process, data augmentation is used to enlarge the diversity of data set. We flip images in horizontal and vertical reflections and rotate them with 90 • , 180 • , and 270 • . We evaluate the performance of our proposed method by using per-class accuracy, mean Intersection over Union (mIoU), mean F1-score (F1), and overall accuracy (OA). We implement all the experiments on an Intel core 6 i7-7820X CPU at 3.6 GHz with RTX 2080Ti GPU (11G). All the models require 70 epochs for training, the initial learning rate of 0.01 is dynamically decreased every 20 epochs by 1/10, momentum and weight decay are set at 0.9 and 5e − 4, respectively, and batch size is set to 16 during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Results on Vaihingen Data Set</head><p>In this section, we implement sets of experiments based on the proposed method compared with the popular segmentation networks to estimate the effectiveness of proposed method.</p><p>We report the results on the Vaihingen test set in Table <ref type="table" target="#tab_1">I</ref>. All the models are trained with the same strategy to guarantee fair comparison. Not surprisingly, the proposed AFS-based models perform better than baselines. Particularly, compared with the DeepLabV3, DeepLabV3-AFS(32, 4) gains more obvious mIoU improvement about 1.09% but fewer parameters. Additionally, it is interesting to observe that the small objects "Car" can achieve a great improvement about 4.3%, which demonstrates that AFS module can achieve adaptively and effectively select feature information from multiple receptive fields. Meanwhile, we also observe that PSPNet-AFS obtains effective improvement, while the mIoU, F1-score, and OA obtained by PSPNet-AFS(32, 4) increased by 0.44%, 0.27%, and 0.24%, respectively. The competitive results further prove that the effectiveness and generalization of our method. Additionally, PSPNet with eight scales is used to validate the effect of the number of feature maps compared with PSPNet-AFS <ref type="bibr" target="#b7">(8,</ref><ref type="bibr" target="#b0">1)</ref>. Not surprisingly, our method performs better than PSPNet in an 0.4% mIoU improvement; F1-socre and OA are improved by 0.37% and 0.28%, respectively, which further demonstrate the efficiency of AFS module in aerial images, rather than the use of more scales than before to improve network performance. In addition, we conducted experiments to further validate the generalization for AFS-based models. We reported the results by using U-Net-AFS on Vaihingen data set. We can note that AFS-based U-Net performs better than conventional U-Net. Our method increased the mIoU by 0.76%, and F1-score of 0.53%.</p><p>To further demonstrate the reasonable results, we list several existing results obtained by the state-of-the-art methods in Table <ref type="table" target="#tab_1">II</ref>. We can observe that the proposed method outperforms most state-of-the-art networks. Though Lin et al. <ref type="bibr" target="#b10">[11]</ref> utilized ResNet101 as the encoding backbone, our method increases the mIoU by about 4.6%, and F1-score of 3.4%, which is a significant improvement, and demonstrates the notable performance of AFS-based models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Results on WHU Building Data Set</head><p>In order to better demonstrate the performance of AFSbased models, we test the proposed method on Satellite data set I (global cities) data set. Note that all the models use pretrained ResNet18 as the encoding backbone. Table <ref type="table" target="#tab_1">III</ref> shows the performances of different scales used in segmentation models on WHU Building data set. We can note that the general PSPNet and DeepLabV3 achieved relatively poor results at the same number of scales, which further demonstrates the effectiveness of the proposed method. In addition, we can observe that the results obtained at different scales are influenced by the different data sets. In other words, segmentation models with AFS method perform better than general models, because our method can adaptively extract useful information based on the feature map with multiple scales in different target segmentation tasks. Table <ref type="table" target="#tab_3">IV</ref> depicts the comparison of different attention mechanisms used in models. It is worth noting that the PSPNet-AFS achieved better results by using squeeze-and-excitation (SE) module <ref type="bibr" target="#b19">[20]</ref> than PSPNet at the same scales. Although the proposed module does not increase the score significantly, the segmentation models can achieve better performance with fewer parameters and stronger generalization under the AFS strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Visualization Analysis</head><p>Above, extensive experiments are conducted to evaluate the performance of the proposed method, and our model outperforms the baselines and the existing networks with better performance yet fewer parameters. To better depict the effectiveness of our method, we utilize the predicted results obtained from DeepLabV3-AFS, PSPNet-AFS, U-Net-AFS, and their baselines to present the visual results on Vaihingen test set.</p><p>As shown in Fig. <ref type="figure" target="#fig_3">3</ref>, we can see that DeepLabV3-AFS, PSPNet-AFS, and U-Net-AFS significantly boost the predicted   results. In the first sample in Fig. <ref type="figure" target="#fig_3">3</ref>, we can note that the shadow area marked in red boxes cannot be segmented effectively by conventional PSPNet and DeepLabV3. Additionally, we also note that there are some mislabeled data in the test set, such as sample-2 in Fig. <ref type="figure" target="#fig_3">3</ref>. We observe that the proposed AFS module achieves better results than previous methods. During the process of experiments, it is difficult to classify "Low vegetation" and "Tree" from images due the problem of mislabeled data in training sets, whereas the proposed method is still relatively good to achieve segmentation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CONCLUSION</head><p>In this letter, we propose a novel method to achieve segmentation task for remote sensing images with significant performance. We employ the AFS method to effectively learn weight contributions between channels on various receptive fields. Multiple scales yield different sizes of the effective receptive fields between channels by using softmax attention that is guided by the information in these feature maps. We conduct extensive experiments to estimate the proposed method by using PSPNet, DeepLabV3, and U-Net as the baseline. The competitive results on Vaihingen and WHU Building data sets demonstrate that the method is effective, and the comparative studies also show that the method is robust. In the future, we will use more baselines with the proposed module to achieve higher even faster segmentation performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Structure of AFS.</figDesc><graphic url="image-1.png" coords="2,50.03,58.61,249.02,94.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Architectures of AFS-based semantic segmentation networks. (a) PSPNet-AFS. (b) DeepLabV3-AFS. (c) U-Net-AFS.</figDesc><graphic url="image-2.png" coords="3,92.51,58.85,426.98,187.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Authorized licensed use limited to: Tsinghua University. Downloaded on December 30,2022 at 12:40:50 UTC from IEEE Xplore. Restrictions apply.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Visual results of the Vaihingen test set. Impervious surfaces: white, buildings: dark blue, low vegetation: light blue, trees: green, and cars: yellow.</figDesc><graphic url="image-3.png" coords="5,93.47,55.01,424.22,122.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I RESULTS</head><label>I</label><figDesc>OF THE VAIHINGEN DATA SET IN FIVE CATEGORIES (%)</figDesc><table><row><cell>TABLE II</cell></row><row><cell>PERFORMANCE OF THE DIFFERENT MODELS (%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV COMPARISON</head><label>IV</label><figDesc>OF DIFFERENT ATTENTION MECHANISMS ON SATELLITE DATA SET I (GLOBAL CITIES) DATA SET (%)</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">Authorized licensed use limited to: Tsinghua University. Downloaded on December 30,2022 at 12:40:50 UTC from IEEE Xplore. Restrictions apply.</note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported by the National Natural Science Foundation of China under Project 61825103, Project 91838303, and Project 91738302.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<ptr target="http://arxiv.org/abs/1409.1556" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. (ICCV)</title>
				<meeting>IEEE Int. Conf. Comput. Vis. (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015-12">Dec. 2015</date>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015-06">Jun. 2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015-06">Jun. 2015</date>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Med</title>
				<meeting>Med</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs</title>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018-04">Apr. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017-07">Jul. 2017</date>
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hourglass-shapenetwork based semantic segmentation for high resolution aerial imagery</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Deligiannis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Munteanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sens</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">522</biblScope>
			<date type="published" when="2017-05">May 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dense U-Net-based semantic segmentation of small objects in urban remote sensing images</title>
		<author>
			<persName><forename type="first">R</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="65347" to="65356" />
			<date type="published" when="2019-05">May 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dense semantic labeling of subdecimeter resolution images with convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tuia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="881" to="893" />
			<date type="published" when="2017-02">Feb. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">SAN: Scale-aware network for semantic segmentation of high-resolution aerial images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.03089</idno>
		<ptr target="http://arxiv.org/abs/1907.03089" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dense semantic labeling with atrous spatial pyramid pooling and decoder for high-resolution remote sensing imagery</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sens</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2018-12">Dec. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation of very high resolution remotely sensed images combined with DSM</title>
		<author>
			<persName><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="474" to="478" />
			<date type="published" when="2018-03">Mar. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">End-to-end DSM fusion networks for semantic segmentation in high-resolution aerial images</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1766" to="1770" />
			<date type="published" when="2019-11">Nov. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Objects segmentation from high-resolution aerial images using U-Net with pyramid pooling layers</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="115" to="119" />
			<date type="published" when="2019-01">Jan. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">DeepLab-based spatial feature extraction for hyperspectral image classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="225" to="251" />
			<date type="published" when="2019-02">Feb. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Selective kernel networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
				<meeting>IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06">Jun. 2019</date>
			<biblScope unit="page" from="510" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The DGPF test on digital aerial camera evaluation-Overview and test design</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cramer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Photogrammetrie-Fernerkundung-Geoinf</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="73" to="82" />
			<date type="published" when="2010-04">Apr. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for multisource building extraction from an open aerial and satellite imagery data set</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="574" to="586" />
			<date type="published" when="2019-01">Jan. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE/CVF Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2018-06">Jun. 2018</date>
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
