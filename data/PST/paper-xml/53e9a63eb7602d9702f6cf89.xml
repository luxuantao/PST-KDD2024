<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-facet Rating of Product Reviews</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Stefano</forename><surname>Baccianella</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Istituto di Scienza e Tecnologia dell&apos;Informazione Consiglio Nazionale delle Ricerche</orgName>
								<address>
									<addrLine>Via Giuseppe Moruzzi 1</addrLine>
									<postCode>56124</postCode>
									<settlement>Pisa</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andrea</forename><surname>Esuli</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Istituto di Scienza e Tecnologia dell&apos;Informazione Consiglio Nazionale delle Ricerche</orgName>
								<address>
									<addrLine>Via Giuseppe Moruzzi 1</addrLine>
									<postCode>56124</postCode>
									<settlement>Pisa</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fabrizio</forename><surname>Sebastiani</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Istituto di Scienza e Tecnologia dell&apos;Informazione Consiglio Nazionale delle Ricerche</orgName>
								<address>
									<addrLine>Via Giuseppe Moruzzi 1</addrLine>
									<postCode>56124</postCode>
									<settlement>Pisa</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-facet Rating of Product Reviews</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E3072C09D4320BAE1AA2627B9341239D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Online product reviews are becoming increasingly available, and are being used more and more frequently by consumers in order to choose among competing products. Tools that rank competing products in terms of the satisfaction of consumers that have purchased the product before, are thus also becoming popular. We tackle the problem of rating (i.e., attributing a numerical score of satisfaction to) consumer reviews based on their textual content. We here focus on multi-facet review rating, i.e., on the case in which the review of a product (e.g., a hotel) must be rated several times, according to several aspects of the product (for a hotel: cleanliness, centrality of location, etc.). We explore several aspects of the problem, with special emphasis on how to generate vectorial representations of the text by means of POS tagging, sentiment analysis, and feature selection for ordinal regression learning. We present the results of experiments conducted on a dataset of more than 15,000 reviews that we have crawled from a popular hotel review site.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Online product reviews are becoming increasingly available across a variety of Web sites, and are being used more and more frequently by consumers in order to make purchase decisions from among competing products 1 . For example, according to a study <ref type="bibr" target="#b0">[1]</ref> performed on TripAdvisor 2 , one of the most popular online review sites for tourism-related activities, among the users that use the TripAdvisor online booking system 97.7% are influenced by other travelers' reviews, and among them 77.9% use the reviews as a help to choose the best place to stay.</p><p>Software tools that organize product reviews and make them easily accessible to prospective customers are thus going to be more and more popular. Among the issues that the designers of these tools need to address are (a) content aggregation, such as in pulling together reviews from sources as disparate as newsgroups, blogs, and community Web sites; (b) content validation, as in filtering out fake reviews authored by people with vested interests <ref type="bibr" target="#b1">[2]</ref>; and (c) content organization, as in automatically ranking competing products in terms of the satisfaction of consumers that have purchased the product before.</p><p>We address a problem related to issue (c), namely, rating (i.e., attributing a numerical score of satisfaction to) consumer reviews based on their textual content. This problem arises from the fact that, while some online product reviews consist of a textual evaluation of the product and a score expressed on some ordered scale of values, many other reviews contain a textual evaluation only. These latter reviews are difficult for an automated system to manage, especially when a qualitative comparison among them is needed in order to determine whether product x is better than product y, or to identify the best product in the lot. Tools capable of interpreting a text-only product review and scoring it according to how positive the review is, are thus of the utmost importance.</p><p>In particular, our work addresses the problem of rating a review when the value to be attached to it must range on an ordinal (i.e., discrete) scale. This scale may be in the form either of an ordered set of numerical values (e.g., one to five "stars"), or of an ordered set of non-numerical labels (e.g., Poor, Good, Very good, Excellent); the only difference between these two cases is that, while in the former case the distances between consecutive scores are known, this is not true in the latter case. We also focus on multi-facet rating of product reviews, i.e., on the case in which the review of a product (e.g., a hotel) must be rated several times, according to several orthogonal aspects of the product (for a hotel: cleanliness, centrality of location, etc.).</p><p>The system we have realized could work as a building block for other larger systems that implement more complex functionality. For instance, a Web site containing product reviews whose users only seldom rate their own reviews could use this system to learn from the rated reviews to rate the others; yet another Web site containing only unrated product reviews could learn, from the rated reviews of another site which contains rated reviews, to rate its own reviews.</p><p>This work mostly focuses, rather than on the learning device used for generating a review rater, on the generation of the vectorial representations of the reviews that must be given as input to the learning device. These representations cannot simply consist of the usual bag-of-words representations used in classifying texts by topic, since classifying texts by opinion (which is the key contents of reviews) requires much subtler means <ref type="bibr" target="#b2">[3]</ref>. Two expressions such as "A great hotel in a horrible town!" and "A horrible hotel in a great town!" would receive identical bag-of-words representations, while expressing opposite evaluations of the hotel. We have addressed three aspects of the generation of meaningful representations of product reviews: (a) extracting complex features based on patterns of parts of speech; (b) making the extracted features more robust through the use of a lexicon of opinion-laden words; and (c) selecting discriminating features through techniques explicitly devised for ordinal regression (an issue which had practically received no attention in the literature).</p><p>The rest of the paper is organized as follows. Section 2 describes the key part of our work, i.e., how we generate the vectorial representations of the reviews. Section 3 describes a hotel review dataset we have crawled from the Web and the results of the experiments we have run on it. Section 4 presents related work, while Section 5 concludes, discussing avenues for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Generating Vectorial Representations of Product Reviews</head><p>In machine learning the problem of rating data items with values ranging on an ordinal scale is called ordinal regression (OR). OR consists of estimating a target function Φ : X → Y which maps each object x j ∈ X into exactly one of an ordered sequence Y = y 1 ≺ . . . ≺ y n of labels (aka "scores", or "ranks"), by means of a function Φ called the classifier <ref type="foot" target="#foot_0">3</ref> . This problem lies in-between singlelabel classification, in which Y is instead an unordered set, and metric regression, in which Y is instead a continuous, totally ordered set (typically: the set R of the reals). Throughout this work, as a learning device for ordinal regression we use -support vector regression ( -SVR) <ref type="bibr" target="#b3">[4]</ref>, as implemented in the freely available LibSvm library <ref type="bibr" target="#b4">[5]</ref>, with its parameters set at their default values. As all supervised learning devices, -SVR requires all training and test examples to be represented as feature vectors. As a baseline representation we use bag-of words with cosine-normalized tf idf weighting. As mentioned in the introduction, this representation cannot account for the subtle ways in which opinions are represented. In the rest of this section we will thus discuss our efforts at devising better representations for the purpose of product review rating.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Pattern Extraction</head><p>Our first move away from the simplistic bag-of-words representation has consisted in spotting units of text larger than words that have the potential to be useful additional features. For instance, for distinguishing "A great hotel in a horrible town!" from "A horrible hotel in a great town!", it may be useful to use "great hotel" and "horrible hotel" as features in their own right. While most previous works on identifying indexing units larger than words have used frequency considerations alone (see e.g., <ref type="bibr" target="#b5">[6]</ref>), we have chosen to bring to bear syntax; for instance, both "great hotel" and "horrible hotel" follow the part-ofspeech (POS) pattern "JJ NN", where "JJ" stands for "adjective" and "NN" for "noun". We have thus defined three POS patterns (which we have creatively called A, B, C -see Table <ref type="table">1</ref> for a detailed grammar) which we deemed could identify meaningful larger-than-word units to be used as features. Note that we will use the expressions matching these patterns as features additional to the features extracted via bag-of-words; that is, if "horrible hotel" matches either A, B, or C, both "horrible", "hotel", and "horrible hotel" will be used as features.</p><p>Pattern A models (possibly complex) noun phrases, such as "nice room" or "very rude staff". Pattern B captures instead complex expressions that also contain a verb, such as "hotel was very nice" or "staff helped very much". Pattern C instead addresses expressions stating that a subject has or does not have some property, such as "has a nice restaurant" or "has a bar".</p><p>Different expressions we extract may state in different forms the same opinion about the same subject: for example, the type-B expression "the room was very Table <ref type="table">1</ref>. POS patterns used to extract larger-than-word units. The leftmost part of this table defines the three POS patterns, while the rightmost part lists the terminal symbols used in the leftmost part, as extracted by a standard POS tagger. Verb "to be" CC,CS Conjunction Hv</p><p>Verb "to have" JJ Adjective NN,NN$ Noun and noun followed by Saxon genitive QL Qualifier RB Adverb V Verb (other than "be", "have", and "do")</p><p>nice but small" and the type-A expression "very nice but small room" convey the same information, which is also the same information collectively conveyed by the two type-A expressions "very nice room" and "small room". We have thus defined two canonical forms in which the expressions matching our patterns are converted once extracted from text, with the double aim of (a) reducing the number of distinct but semantically equivalent features, and (b) increasing the statistical robustness of the remaining features by increasing their counts. The two canonical forms are "ADJ NN" (for A-and B-type expressions) and "HV ADJ NN" (for C-type expressions). The transformation of expressions into their corresponding canonical form is obtained by (i) removing articles ("the hotel was very nice and good located" → "hotel was very nice and good located")<ref type="foot" target="#foot_1">4</ref> ;</p><p>(ii) splitting conjunctions, creating a pattern for every adjectival form ("hotel was very nice and good located" → "hotel was very nice" + "hotel was good located"); (iii) removing auxiliary verbs ("hotel was very nice" → "hotel very nice") (Applied only on Pattern B); (iv) putting adjectives in front of nouns ("hotel very nice" → "very nice hotel").</p><p>POS tagging also provides information about the presence of negations. This allowed us to add an explicit negation in front of any expression for which the POS tagger detected the presence of a negation (e.g., "the staff was not nice" → "not nice staff"), so as to avoid collapsing negated and non-negated statements of the same fact into the same feature.</p><p>Figure <ref type="figure" target="#fig_1">1</ref> shows a sample review from the training set of the corpus described in Section 3.1, with the expressions matching our POS patterns in boldface.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Pattern Aggregation through Sentiment Analysis</head><p>In the expressions extracted so far, different opinion-bearing terms may be used to express sentiment of similar polarity (i.e., positive vs. negative) and strength. For example, both "horrible location" and "disgusting location" express a strongly negative feeling about the location of a hotel. We use a lexical resource of opinion-laden terms with the aim of mapping specific expressions conveying opinion (such as "disgusting location") into more "abstract" expressions "Great location"! We loved the location of this hotel the area was great for affordable restaurants, bakeries, small grocers and near several good restaurants. Do not overlook the lovely church next door quite a treat! The rooms were servicable and some seemed to have been more recently refurbished. Just stay away from room 54 for the money it was a suite the comfort was not worth the price, poor heater and horrible shower, not a single shelf in the bathroom to hold a bar of soap. But 38 also a suite was much nicer. The basic twin rooms were fine and small as to be expected. I recommend this hotel overall but do not expect much help from the front desk as all but one of the staff bordered on surly. That was the most disappointing aspect of this otherwise nice hotel, the breakfast was fine and the breakfast room was lovely.  (such as "[Negative] location"). We then use these abstract expressions (here called simple GI expressions) as additional features for our vectorial representation (i.e., we retain as features both "horrible location", "disgusting location", and "[Negative] location"). The lexical resource we have chosen for our experiments is the [Positive]/[Negative] subset of the General Inquirer (GI) <ref type="bibr" target="#b6">[7]</ref>, a set of 1,915 (resp., 2,291) English words marked as having a positive (resp., negative) polarity. Examples of positive terms are "advantage", "fidelity" and "worthy", while examples of negative terms are "badly", "cancer", and "stagnant". In order to generate simple GI expressions, we match all the words in each of the extracted expressions against the GI lexicon<ref type="foot" target="#foot_2">5</ref> and, if the word is present, its [Positive] or [Negative] tag is used to generate a new expression in which the tag replaces the word (see Table <ref type="table" target="#tab_0">2</ref> for examples).</p><p>In the GI, words are also marked according to an additional, finer-grained set of sentiment-related tags (see Table <ref type="table" target="#tab_1">3</ref>); some of them denote the magnitude of the sentiment associated to the word, while others denote specific emotions and feelings evoked by the word. This allows us to cover the sentiment-carrying expressions that occur in our reviews in a finer-grained way. We thus generate a further type of expressions, which we call enriched GI expressions, by adding to all simple GI expressions the appropriate finer-grained sentiment-related tags. Table <ref type="table" target="#tab_0">2</ref> reports the 10 most frequent expressions in the "Value" dataset (see Section 3.1) with the simple and enriched GI expressions that are generated from them. All enriched GI expressions are added to the feature set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Feature Selection for Ordinal Regression</head><p>The final feature set thus consists of all words, all expressions (as from the patterns of Section 2.1), all simple GI expressions, and all enriched GI expressions. This means that the dimensionality of the resulting vector space may be very large. It seems thus necessary to add a feature selection phase, with the twofold aim of improving the efficiency of the learning phase and removing nondiscriminating features. As in practically all text learning tasks we will follow a "filter" approach <ref type="bibr" target="#b7">[8]</ref>, according to which each candidate feature, irrespectively of its nature (word, expression, etc.), is scored by a function that measures its discriminative power; only the t highest-scoring features will be retained. There are many standard feature selection methods for text classification <ref type="bibr" target="#b8">[9]</ref> and for metric regression <ref type="bibr" target="#b9">[10]</ref>; on the other hand, research on feature selection for ordinal regression has been much scarcer, and to the best of our knowledge the only work which addresses this problem is <ref type="bibr" target="#b10">[11]</ref>. However, the method proposed therein is not applicable in our context, since it amounts to classifying the training instances using the feature alone, evaluating the performance in terms of the chosen evaluation measure, and then taking the result as the importance score of the feature; since this amounts to learning a classifier for each feature, this method is applicable only when the original set of features is very small. In this work we propose and compare two feature selection methods for ordinal regression that draw inspiration from work on text classification.</p><p>Our first method, that we call minimum variance (MV), is based on measuring the variance of the distribution of a feature across the labels of our ordered scale, and retaining only the t features that have the smallest variance. For the purpose of computing variance, the labels are mapped to the first n natural numbers, and the value of a term occurrence is the natural number associated to the label of the document in which the term occurs. The intuitive justification of MV is that a useful feature is one that is capable of discriminating a small portion of the ordered scale from the and that features with a small variance are those which satisfy this property.</p><p>Our second method is inspired by <ref type="bibr" target="#b11">[12]</ref>, and is based on the observation that MV might well select many features that discriminate well some of the labels, while selecting few or no features that discriminate well the other labels. If, by absurd, all texts with label y were in German and all the other texts were in English, MV would likely pick mostly or only German words, since their variance is 0, with the consequence that an accurate model would likely be learned for y but not for the other labels. A solution to this problem is based on (i) provisionally "assigning" each feature t k to the label closest to its average label value; (ii) ranking, for each label, the features assigned to it; (iii) enforcing a "round robin" (RR) policy in which the n labels take turns in picking their favourite features from the top-most elements of their label-specific rankings. This method is referred to as RRMV in Table <ref type="table" target="#tab_3">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Setting</head><p>The dataset we use in this work is a set of 15,763 hotel reviews we have obtained by crawling from the TripAdvisor Web site all the reviews related to hotels in the towns of Pisa and Rome 6 (approximately 26,000 such reviews were obtained), and then applying a language recognition system, that we have implemented along the lines of <ref type="bibr" target="#b12">[13]</ref>, in order to filter out all reviews not in English 7 . Each review has a score of one to five "stars", both globally and for each of seven facets: "BusinessService", "CheckIn/FrontDesk", "Cleanliness", "Location", "Rooms", "Service", "Value". Aside from the "global" dataset, we have also defined seven facet-specific datasets, which contain all and only the reviews for which a label has been attributed for the given facet (not all reviews contain scores for all of the facets); the largest facet-specific dataset is "Value", with 12,038 reviews, while the smallest is "BusinessService" dataset, with 4,148 reviews. The label distribution is highly skewed, since 45% of all the reviews have a global score of 5 stars, 34.5% a global score of 4 stars, 9.4% 3 stars, 7.2% 2 stars and only 3.9% 1 star (the skew is even higher in the facet-specific datasets). This tends to make the system's task for the least frequent scores difficult. We have independently and randomly split each of the 8 datasets into a training set, containing 75% of the reviews of the entire dataset, and a test set, consisting of the other 25% 8 .</p><p>6 Pisa and Rome reviews were crawled on May 12 and 14, 2008, respectively. 7 Our implementation of this language recognition system is freely available for download from http://patty.isti.cnr.it/~baccianella/ling/ 8 All the datasets discussed in this paper are available for download from http://patty.isti.cnr.it/~baccianella/reviewdata/ Conforming to standard practice, as an evaluation measure we use mean absolute error, defined in terms of average deviation between the predicted and the true label. We report results using both the standard micro-and a newly proposed macro-averaged version of M AE (respectively noted M AE µ and M AE M ), defined as</p><formula xml:id="formula_0">M µ ( Φ, T e) = 1 |T e| xj ∈T e | Φ(x j ) -Φ(x j )|<label>(1)</label></formula><formula xml:id="formula_1">M AE M ( Φ, T e) = 1 n n i=1 1 |T e i | xj ∈T ei | Φ(x j ) -Φ(x j )|<label>(2)</label></formula><p>where T e denotes the test set and T e i denotes the set of test documents whose true label is y i . In M AE µ all examples count the same (since M AE µ is computed by taking the deviation between predicted and true label for each document and then averaging across documents), while in M AE M all labels count the same (since M AE M independently computes average deviation for all test documents with a given label and then averages across labels). M AE M (which, to the best of our knowledge, is being proposed here for the first time) is more adequate for dealing with highly imbalanced datasets like ours since on these datasets, when using the standard M AE µ , the system that trivially assigns all documents to the majority label may be difficult to outperform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results and Discussion</head><p>For POS-tagging the reviews we have used the POS-tagging utility provided by the Natural Language Toolkit<ref type="foot" target="#foot_3">9</ref> (NLTK) package. After feature extraction and selection, each selected feature is weighted by cosine-normalized tf idf . We provide two baselines, a "trivial" one ("MajorityLabel") in which all test documents are assigned the label most frequent in the training set, and a less trivial one ("BoW") based on -SVR and a simple bag-of-word representation with no feature selection. Table <ref type="table" target="#tab_2">4</ref> reports M AE µ and M AE M values for the two baselines. An effectiveness value is provided for the global dataset in the left-hand side of the table; for the seven facet-specific datasets, an effectiveness value that averages across them (with each dataset counting the same) is provided in the right-hand side. Table <ref type="table" target="#tab_3">5</ref> shows M AE µ and M AE M values obtained for various combinations of text representation method and feature selection method. In all experiments, the 10% top-scoring features are selected via the indicated feature selection method.</p><p>Several observations can be made based on these tables. The first is that representations more sophisticated than bag-of-words always provide superior or much superior performance than BoW; BoW+Expr+sGI+eGI provides the best representation in 2 cases out of 4 (given by 2 evaluation measures × 2 feature selection methods), provides consistently good performance across the table, and provides very substantial improvements over pure bag-of words. For The second observation is that, as a feature selection method, MV generally outperforms RRMV on M AE µ , but the contrary often happens on M AE M . This can be explained by the fact that only RRMV places equal importance on all labels, by selecting some highly discriminating features for each label; as a consequence, RRMV tends to excel when the results are evaluated with a measure, such as M AE M , that places equal importance on each label. Conversely, it is likely that for frequent labels MV finds many discriminating features, while it finds few for less frequent labels; as a consequence, MV tends to excel when the results are evaluated with a measure, such as M AE µ , that in fact attributes more importance to more frequent labels. However, we should observe that retaining only 10% of the total amount of features has proven a suboptimal choice, as can be observed by the general deterioration in performance that resulted in moving from BoW with all features (2nd line of Table <ref type="table" target="#tab_2">4</ref>) to BoW with 10% of the features only (1st line of Table <ref type="table" target="#tab_3">5</ref>). In the future we plan to experiment with different, less aggressive levels of feature selection.</p><p>The third observation is that, when M AE µ is used, in the "Global" experiments the "trivial" baseline (MajorityLabel) is only marginally improved upon by the BoW baseline (a non-trivial baseline in which a sophisticated learning device such as -SVR is involved), and even outperforms it on the "Average" experiments! This can be explained by the fact that the distribution of labels in these datasets is highly skewed towards a majority label (as noted in Section 3.1, this is especially true in the facet-specific datasets), with the consequence that the trivial classifier that assigns all test objects to the majority label may be hard to beat by any non-trivial classifier. In the light of this, the improvements obtained over BoW thanks to our methods acquire even more value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>In this section we review related work on the analysis and rating of product reviews, focusing on the differences between these approaches and ours.</p><p>The work of Dave et al. <ref type="bibr" target="#b13">[14]</ref> is the first to address the problem of scoring product reviews based on an analysis of their textual content. Unlike us, they address binary classification, only distinguishing between Positive and Negative reviews. Based on a corpus of reviews that they crawled from the Web they design and test a number of methods for building product review binary classifiers.</p><p>Unlike <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b15">[16]</ref> addresses product review scoring with respect to an ordinal scale of more than 2 values. Unlike us, their work is focused on the learning approach to be used. They propose and compare a multi-class SVM classifier, -SVR, and a meta-algorithm based on a metric labeling formulation of the problem. A related work is <ref type="bibr" target="#b14">[15]</ref>, where a semisupervised algorithm is applied that learns to rate product reviews from both rated and unrated training reviews. Also devoted to testing learning algorithms for rating product reviews is <ref type="bibr" target="#b18">[19]</ref>, which addresses multi-facet review rating on a corpus of Japanese reviews.</p><p>In <ref type="bibr" target="#b17">[18]</ref> rating inference is addressed in a simplified way: while the reviews in the training set are labeled according to a five-point scale, the system described is only capable of assigning labels in the set {Positive, Neutral, Negative}, thus "compressing" the original rating scale to a coarser one. This is very different from what we do, since our system is capable of predicting labels on ordinal scales containing an arbitrary number of labels.</p><p>In <ref type="bibr" target="#b21">[21]</ref> a new task in product review analysis is identified, i.e., the prediction of the utility of product reviews, which is orthogonal to scoring by perceived quality. The authors formalize the problem in terms of linear regression and experiment with two types of regression algorithms, -SVR and simple linear regression (SLR) as implemented in WEKA.</p><p>In <ref type="bibr" target="#b16">[17]</ref> online hotel reviews are ranked in a way similar to ours. The authors manually build a lexicon of expressions conveying either positive or negative sentiment with respect to the domain of hotel reviews. However, their experimental evaluation is weak, since a very small test set of reviews (about 250) is used, and the evaluation simply consists in ranking pairs of reviews according to which is more positive than the other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We have presented a system for automatically rating product reviews that independently rates many distinct aspects ("facets") of the product, so that the same review could be given different ratings for different facets. We have investigated various methods for the generation of the vectorial representations of the reviews to be fed to the learning system, including methods for the generation of complex features based on the detection of part-of-speech patterns, methods for enhancing the statistical robustness of these patterns through the application of a lexicon of opinion-laden words, and feature selection methods for ordinal regression. These latter methods, in particular, had never been presented in the literature, and are original contributions of this work. We have shown that a combination of all these methods substantively outperforms a baseline consisting of a bag-of-words representation.</p><p>Rating product reviews is a fairly recent application, so a lot of research still needs to be done. In the future, we would like to work on several problems that this work has highlighted, the first of which has to do with creating a larger and more varied dataset that can be considered representative of the many types of reviews one encounters for a given type of product. We intend to crawl a much larger reviews dataset, representative of the many types of destination which hotels cater for. The current dataset only represents towns interesting for their works of art, but other types of destination should be represented such as, e.g., seaside resorts, mountain destinations, and the like. The reason why such variety may be desirable is that different language may be used to praise a hotel in a seaside location than a hotel in a business-oriented town.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>RB ADV | QL ADV | JJ | AP ADV | CONG :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. An example hotel review from the dataset of Section 3.1. The expressions matching our POS patterns are shown in boldface.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>The 10 most frequent expressions in the "Value" dataset (see Section 3.1), together with their corresponding simple and enriched GI expressions</figDesc><table><row><cell>Expression</cell><cell>Simple GI Expression</cell><cell>Enriched GI Expression</cell></row><row><cell>great location</cell><cell>[Positive] location</cell><cell>[Strong] [Positive] location</cell></row><row><cell>great hotel</cell><cell>[Positive] hotel</cell><cell>[Strong] [Positive] hotel</cell></row><row><cell>helpful staff</cell><cell>[Positive] staff</cell><cell>[Virtue] [Positive] staff</cell></row><row><cell>friendly staff</cell><cell>[Positive] staff</cell><cell>[Emot] [Virtue] [Positive] staff</cell></row><row><cell>good location</cell><cell>[Positive] location</cell><cell>[Virtue] [Positive] location</cell></row><row><cell>nice hotel</cell><cell>[Positive] hotel</cell><cell>[Virtue] [Positive] hotel</cell></row><row><cell>very helpful staff</cell><cell>[Positive] staff</cell><cell>very [Virtue] [Positive] staff</cell></row><row><cell>very friendly staff</cell><cell>[Positive] staff</cell><cell>very [Emot] [Virtue] [Positive] staff</cell></row><row><cell>excellent location</cell><cell>[Positive] location</cell><cell>[Virtue] [Positive] location</cell></row><row><cell>great place</cell><cell>[Positive] place</cell><cell>[Strong] [Positive] place</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Fine-grained set of GI sentiment-related tags and their textual definitions Arousal] words indicating excitation, aside from pleasures or pains, but including arousal of affiliation and hostility [Emot] words related to emotion that are used as a disambiguation category, also available for general use [Virtue] words indicating an assessment of moral approval or good fortune, especially from the perspective of middle-class society [Vice] words indicating an assessment of moral disapproval or misfortune [NegAff] words of negative affect "denoting negative feelings and emotional rejection" [PosAff] words of positive affect "denoting positive feelings, acceptance, appreciation and emotional support"</figDesc><table><row><cell>Tag</cell><cell>Description</cell></row><row><cell cols="2">[Strong] words implying strength</cell></row><row><cell cols="2">[Power] indicating a concern with power, control or authority</cell></row><row><cell cols="2">[Weak] words implying weakness</cell></row><row><cell cols="2">[Submit] connoting submission to authority or power, dependence on others, vulnerability to</cell></row><row><cell></cell><cell>others, or withdrawal</cell></row><row><cell cols="2">[Pleasur] words indicating the enjoyment of a feeling, including words indicating confidence, in-</cell></row><row><cell></cell><cell>terest and commitment</cell></row><row><cell cols="2">[Pain] words indicating suffering, lack of confidence, or commitment</cell></row><row><cell>[Feel]</cell><cell>words describing particular feelings, including gratitude, apathy, and optimism, not</cell></row><row><cell></cell><cell>those of pain or pleasure</cell></row><row><cell>[</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Baseline results. Lower values indicate better accuracy. "Global" stands for results on the global dataset; "Average" stands for average results across the seven facet-specific datasets. MAE M MAE µ MAE M</figDesc><table><row><cell></cell><cell>Global</cell><cell>Average</cell></row><row><cell cols="2">MAE µ MajorityLabel 0.657 1.896</cell><cell>0.773 1.600</cell></row><row><cell>BoW</cell><cell>0.621 0.799</cell><cell>0.803 1.160</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Results obtained for various combinations of features and feature selection methods, with only 10% of the total number of features retained. "BoW" stands for bag-of-words, "Expr" for the expressions of Section 2.1, "sGI" and "eGI" for simple and enriched GI expressions, respectively. The best performing combinations are shown in boldface.</figDesc><table><row><cell></cell><cell>Global</cell><cell></cell><cell cols="2">Average</cell></row><row><cell></cell><cell>MAE µ</cell><cell>MAE M</cell><cell>MAE µ</cell><cell>MAE M</cell></row><row><cell></cell><cell cols="4">MV RRMV MV RRMV MV RRMV MV RRMV</cell></row><row><cell>BoW</cell><cell cols="4">0.682 0.654 1.141 0.970 0.847 0.872 1.291 1.269</cell></row><row><cell>BoW+Expr</cell><cell cols="4">0.456 0.547 0.830 0.657 0.752 0.743 1.561 1.093</cell></row><row><cell>BoW+Expr+sGI</cell><cell cols="4">0.448 0.776 1.165 0.937 0.781 0.824 1.008 1.181</cell></row><row><cell cols="5">BoW+Expr+sGI+eGI 0.437 0.565 0.942 0.677 0.733 0.741 1.032 1.092</cell></row><row><cell cols="5">instance, in the "Global" experiments M AE µ improves from .682 to .437 (a</cell></row><row><cell cols="5">35.9% relative improvement) over BoW, while M AE M improves from .970 to</cell></row><row><cell cols="2">.677 (a 30.2% relative improvement).</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>Consistently with most mathematical literature we use the caret symbol (ˆ) to indicate estimation.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>Any ill-formed or clumsy English expression in the examples we use is genuine, i.e., it appears somewhere in our review dataset.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>For some words with multiple senses GI has more than one entry; we do not perform any word sense disambiguation, and thus simply choose the most frequent sense.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_3"><p>http://nltk.sourceforge.net</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Use and impact of online travel review</title>
		<author>
			<persName><forename type="first">U</forename><surname>Gretzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Y</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 International Conference on Information and Communication Technologies in Tourism</title>
		<meeting>the 2008 International Conference on Information and Communication Technologies in Tourism<address><addrLine>Innsbruck, AT</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="35" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Review spam detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Jindal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on the World Wide Web (WWW 2007)</title>
		<meeting>the 16th International Conference on the World Wide Web (WWW 2007)<address><addrLine>Banff, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1189" to="1190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Opinion mining and sentiment analysis</title>
		<author>
			<persName><forename type="first">B</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1/2</biblScope>
			<biblScope unit="page" from="1" to="135" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">New support vector algorithms</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1207" to="1245" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">LIBSVM: a library for support vector machines</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://www.csie.ntu.edu.tw/cjlin/libsvm" />
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A learner-independent evaluation of the usefulness of statistical phrases for automated text categorization</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Caropreso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Matwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Databases and Document Management: Theory and Practice</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Chin</surname></persName>
		</editor>
		<meeting><address><addrLine>Hershey</addrLine></address></meeting>
		<imprint>
			<publisher>Idea Group Publishing</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="78" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The General Inquirer: A Computer Approach to Content Analysis</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Dunphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Ogilvie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1966">1966</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Irrelevant features and the subset selection problem</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kohavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Pfleger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Machine Learning (ICML 1994)</title>
		<meeting>the 11th International Conference on Machine Learning (ICML 1994)<address><addrLine>New Brunswick, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="121" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A comparative study on feature selection in text categorization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">O</forename><surname>Pedersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Conference on Machine Learning (ICML 1997)</title>
		<meeting>the 14th International Conference on Machine Learning (ICML 1997)<address><addrLine>Nashville, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="412" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Subset selection in regression, 2nd edn</title>
		<author>
			<persName><forename type="first">A</forename><surname>Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Chapman and Hall</publisher>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Feature selection for ranking</title>
		<author>
			<persName><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM International Conference on Research and Development in Information Retrieval (SIGIR 2007)</title>
		<meeting>the 30th ACM International Conference on Research and Development in Information Retrieval (SIGIR 2007)<address><addrLine>Amsterdam, NL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="407" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A pitfall and solution in multi-class feature selection for text classification</title>
		<author>
			<persName><forename type="first">G</forename><surname>Forman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Machine Learning (ICML 2004)</title>
		<meeting>the 21st International Conference on Machine Learning (ICML 2004)<address><addrLine>Banff, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">N-gram-based text categorization</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Cavnar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Trenkle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Annual Symposium on Document Analysis and Information Retrieval (SDAIR 1994)</title>
		<meeting>the 3rd Annual Symposium on Document Analysis and Information Retrieval (SDAIR 1994)<address><addrLine>Las Vegas, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="161" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mining the peanut gallery: Opinion extraction and semantic classification of product reviews</title>
		<author>
			<persName><forename type="first">K</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Pennock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on the World Wide Web</title>
		<meeting>the 12th International Conference on the World Wide Web<address><addrLine>Budapest</addrLine></address></meeting>
		<imprint>
			<publisher>WWW</publisher>
			<date type="published" when="2003">2003. 2003</date>
			<biblScope unit="page" from="519" to="528" />
		</imprint>
		<respStmt>
			<orgName>HU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Seeing stars when there aren&apos;t many stars: Graphbased semi-supervised learning for sentiment categorization</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the HLT/NAACL Workshop on Graph-based Algorithms for Natural Language Processing</title>
		<meeting>the HLT/NAACL Workshop on Graph-based Algorithms for Natural Language Processing<address><addrLine>New York, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales</title>
		<author>
			<persName><forename type="first">B</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Meeting of the Association for Computational Linguistics (ACL 2005)</title>
		<meeting>the 43rd Meeting of the Association for Computational Linguistics (ACL 2005)<address><addrLine>Ann Arbor, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="115" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Discovery of subjective evaluations of product features in hotel reviews</title>
		<author>
			<persName><forename type="first">V</forename><surname>Pekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vacation Marketing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="145" to="156" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Extracting product features and opinions from reviews</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP 2005)</title>
		<meeting>the Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP 2005)<address><addrLine>Vancouver, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="339" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Seeing several stars: A rating inference task for a document containing several evaluation criteria</title>
		<author>
			<persName><forename type="first">K</forename><surname>Shimada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Endo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PAKDD 2008. LNCS (LNAI)</title>
		<editor>
			<persName><forename type="first">T</forename><surname>Washio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Suzuki</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Ting</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Inokuchi</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">5012</biblScope>
			<biblScope unit="page" from="1006" to="1014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<pubPlace>Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multiple aspect ranking using the good grief algorithm</title>
		<author>
			<persName><forename type="first">B</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the North American Chapter of the Association for Computational Linguistics and Human Language Technology Conference (NAACL/HLT 2007)</title>
		<meeting>the Joint Conference of the North American Chapter of the Association for Computational Linguistics and Human Language Technology Conference (NAACL/HLT 2007)<address><addrLine>Rochester, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="300" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Utility scoring of product reviews</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Varadarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM International Conference on Information and Knowledge Management (CIKM 2006)</title>
		<meeting>the 15th ACM International Conference on Information and Knowledge Management (CIKM 2006)<address><addrLine>Arlington, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="51" to="57" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
