<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MarkovGNN: Graph Neural Networks on Markov Diffusion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-02-05">5 Feb 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Md</forename><forename type="middle">Khaledur</forename><surname>Rahman</surname></persName>
							<email>morahma@iu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Indiana University Bloomington</orgName>
							</affiliation>
							<affiliation key="aff3">
								<address>
									<addrLine>Conference&apos;17</addrLine>
									<postCode>2017</postCode>
									<settlement>July, Washington</settlement>
									<region>DC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Abhigya</forename><surname>Agrawal</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Indiana University Bloomington</orgName>
							</affiliation>
							<affiliation key="aff3">
								<address>
									<addrLine>Conference&apos;17</addrLine>
									<postCode>2017</postCode>
									<settlement>July, Washington</settlement>
									<region>DC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ariful</forename><surname>Azad</surname></persName>
							<email>azad@iu.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Indiana University Bloomington</orgName>
							</affiliation>
							<affiliation key="aff3">
								<address>
									<addrLine>Conference&apos;17</addrLine>
									<postCode>2017</postCode>
									<settlement>July, Washington</settlement>
									<region>DC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MarkovGNN: Graph Neural Networks on Markov Diffusion</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-02-05">5 Feb 2022</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/nnnnnnn.nnnnnnn</idno>
					<idno type="arXiv">arXiv:2202.02470v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Graph Neural Network</term>
					<term>Markov Clustering</term>
					<term>Graph Clustering</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most real-world networks contain well-defined community structures where nodes are densely connected internally within communities. To learn from these networks, we develop MarkovGNN that captures the formation and evolution of communities directly in different convolutional layers. Unlike most Graph Neural Networks (GNNs) that consider a static graph at every layer, MarkovGNN generates different stochastic matrices using a Markov process and then uses these community-capturing matrices in different layers. MarkovGNN is a general approach that could be used with most existing GNNs. We experimentally show that MarkovGNN outperforms other GNNs for clustering, node classification, and visualization tasks. The source code of MarkovGNN is publicly available at https://github.com/HipGraph/MarkovGNN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph representation learning using graph neural networks (GNNs) is often formulated by a message passing model <ref type="bibr" target="#b0">[1]</ref>. In this model, a node ? receives messages from its direct neighbors and then updates ?'s embedding based on the received messages and its own features. While a GNN with ? layers indirectly exchanges messages among ?-hop neighbors, messages in each layer are still restricted to direct neighbors. Message passing via direct neighbors on the same static graph structure may limit the expressive power of GNNs. Can we address this limitation by generating a series of graphs (using a diffusion process) created from a given static structure and then using the generated graphs at different layers of GNN? In this paper, we answer this question affirmatively using a Markov diffusion process <ref type="bibr" target="#b1">[2]</ref> to generate a series of graphs. The goal is not to develop a new GNN method, but to use Markov diffusion to improve the performance of any existing GNN. When an existing GNN method uses different Markov matrices at different layers, we call this augmented GNN MarkovGNN.</p><p>Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Conference'17, July 2017, Washington, DC, USA ? 2022 Association for Computing Machinery. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00 https://doi.org/10.1145/nnnnnnn.nnnnnnn</p><formula xml:id="formula_0">! " ! # ! $ ! % ! &amp; ! ' ! ( ! " ! # ! $ ! % ! &amp; ! ' ! ( (b) MarkovGNN (a) GCN ) ( (") ) ( (")</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(a) GCN (b) MarkovGNN</head><p>Figure <ref type="figure" target="#fig_1">1</ref>: Updating the embedding ?</p><p>(2)</p><p>1 of ? 1 at the second layer of GNN. (a) For GCN, ?</p><p>1 is computed directly from the embeddings of ? 2 and ? 3 (shown by solid arrowheads), which indirectly depend on the embeddings of ? 1 's two-hop neighbors ? 4 and ? 5 (shown by dashed arrowheads). (b) The second layer of MarkovGNN uses a different graph structure with an edge {? 1 , ? 4 } inserted and an edge {? 3 , ? 5 } deleted possibly because ? 4 is in the same community of ? 1 whereas ? 5 is in a different community. This graph structure is updated based on a Markov process that captures the community pattern in the graph. Thus, in MarkovGNN, ?</p><p>1 is computed directly from the embeddings of ? 2 , ? 3 , and ? 3 (shown by solid arrowheads), which are neighbors of ? 1 in the current graph.</p><p>To understand the limitation of using the same static graph at every layer, Fig. <ref type="figure" target="#fig_1">1</ref>(a) considers the message passing at the second layer of a graph convolutional network (GCN) <ref type="bibr" target="#b2">[3]</ref>. At the second layer of GCN, node ? 1 receives information from 2-hop neighbors (? 4 and ? 5 ) indirectly via 1-hop neighbors (? 2 and ? 3 ). Such indirect messages dilute information received from high-order neighbors and may create the "over-smoothing" <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> problem resulting from hidden-layer representations being squeezed into fixed-length vectors. What if we eliminate indirect messaging by using a new structure created from the original graph? Fig. <ref type="figure" target="#fig_1">1</ref>(b) shows a possible example where we added a new edge {? 1 , ? 4 } and deleted an existing edge {? 3 , ? 5 } based on a diffusion model (here, to capture the community structure in the original graph). If we use the modified graph shown in Fig. <ref type="figure" target="#fig_1">1</ref>(b) at the second layer of a GNN, we promote direct messages (from ? 4 ) and suppress messages (from ? 5 ) that are fully dictated by the modified graph structure. We argue that using a series of modified graphs in the hidden layers for direct messaging instead of a static graph for indirect messaging benefits most existing GNN methods.</p><p>Among many possibilities of perturbing the graph structure, we use the Markov diffusion process to generate a series of stochastic adjacency matrices. The Markov process generates a sequence of graphs (called Markov matrices) with an aim to find communities or clusters in the original graph (a community represents a subset of nodes that are densely connected to each other and loosely connected to other communities). Thus, the sequence of Markov matrices promotes or demotes edges based on their relevance to the underlying clustering pattern. Consequently, using Markov matrices at different layers of a GNN helps learning from graphs that expose homophily <ref type="bibr" target="#b5">[6]</ref> by organizing nodes into communities.</p><p>For graphs containing clustering patterns (e.g., graphs with high clustering coefficients measured by counting the number of triangles in the graph), MarkovGNN offers the following benefits. (1) By simulating the community formation process, MarkovGNN directly captures community patterns into different layers of a GNN.</p><p>(2) Different Markov matrices increasingly put more weight on intra-cluster edges and less weight on inter-cluster edges. Thus, the Markov process (not the GNN) automatically controls information flows on paths based on their relevance to form communities. This community-driven flow eliminates the need to sample neighbors used in GraphSAGE <ref type="bibr" target="#b6">[7]</ref> or sample graphs used in GraphSAINT <ref type="bibr" target="#b7">[8]</ref>.</p><p>(3) As shown in Fig. <ref type="figure" target="#fig_1">1</ref> (b), MarkovGNN only exchanges messages among 1-hop neighbors on modified graphs. Hence, it can prevent the "over-smoothing" <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> problem at high-degree nodes. (4) By pruning low probable diffusion paths, the Markov process keeps the graph sparse at every layer of the GNN and prevents the "neighborexplosion" <ref type="bibr" target="#b6">[7]</ref> problem. <ref type="bibr" target="#b4">(5)</ref> MarkovGNN is more expressive than Cluster-GCN <ref type="bibr" target="#b8">[9]</ref> that precomputes clusters and uses the clustering at every layer of GCN. We will show that a special variant of MarkovGNN is equivalent to Cluster-GCN.</p><p>This paper makes the following key contributions.</p><p>(1) We present MarkovGNN that uses a series of communityaware sparse adjacency matrices at different layers of a GNN. MarkovGNN can augment most GNN methods, and it is more general than GDC <ref type="bibr" target="#b9">[10]</ref> and Cluster-GCN </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND AND MOTIVATION</head><p>Let ? (? , ?) denote a graph with ? = |? | vertices and |?| edges. The adjacency matrix is denoted by an ? ? ? sparse matrix A where A[?, ?] = 1 if {? ? , ? ? }??, otherwise A[?, ?] = 0. Additionally, X ? R ??? denotes the feature matrix where the ?th row stores ? -dimensional features for the ?th vertex. The ?th convolution layer of a GNN computes embeddings of every node based on the embeddings of neighbors as follows:</p><formula xml:id="formula_3">X (?+1) = ? (AX (?) W (?) ),<label>(1)</label></formula><p>where X (?) ?R ??? ? is ? ? -dimensional embeddings used as input to the ?th layer, X (?+1) ?R ??? ? +1 is ? ?+1 -dimensional embeddings generated as the output from the ?th layer and W (?) ?R ? ? ?? ? +1 is the weight matrix. In most instances of GNN, the adjacency matrix is normalized and ReLU is used as the activation function.</p><p>The limitation of GNNs that use a fixed graph at every layer. If a static graph is used at every layer of a GNN, it only passes messages between adjacent nodes where the adjacency remains fixed throughout the execution of the algorithm. While GCN and other message passing networks do leverage higher-order neighborhoods in deeper layers, all messages must pass through immediate neighbors which create bottlenecks when flowing messages from higher-order neighbors <ref type="bibr" target="#b3">[4]</ref>. To remove such bottlenecks, high-pass and low-pass filters were proposed in a recent work with an aim to control the information flow <ref type="bibr" target="#b10">[11]</ref>. However, in almost all GNNs, the graph structure remains static at different layers. We argue that distinct graph structures used in different layers of GNNs could benefit most existing GNN methods. In particular, we aim to capture the formation of communities in different layers of GNN to improve the expressivity of GNN models.</p><p>Previous work that uses diffusion in GNNs. The most notable approach is called graph diffusion convolution (GDC) <ref type="bibr" target="#b9">[10]</ref> where a diffusion matrix ? is constructed via a generalized graph diffusion process:</p><formula xml:id="formula_4">? = ? ?? ?=0 ? ? T ? ,<label>(2)</label></formula><p>where ? ? is the weighting coefficient and T is a column stochastic adjacency matrix computed by AD -1 and D is the diagonal matrix of node degrees. The idea used in GDC is interesting because of the well-known fact that graph diffusion can act as a denoising filter similar to Gaussian filters on images <ref type="bibr" target="#b11">[12]</ref>. However, GDC creates the diffusion matrix as a pre-processing step and uses the same diffusion matrix ? in every layer of a GNN. Hence, GDC still uses the same connectivity matrix (albeit different from the original graph) in every layer and thus misses the opportunity to incrementally learn from the diffusion process. Furthermore, the diffusion process typically results in a dense matrix, making the computation prohibitively expensive. Even though GDC sparsifies ? before using it in GNNs, the computation of T ? becomes the bottleneck.</p><p>Our goal is to capture the diffusion process (more specifically via a Markov process) directly in different layers of GNN. We show that our algorithm performs much better than GDC in identifying clustering patterns and classifying nodes. Related work. Graph representation learning is a well-studied research area with numerous unsupervised and semi-supervised methods proposed in the literature. Early unsupervised works focused on graph embedding based on neighborhood exploration such as random-walk based methods <ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref> and ?-hop neighborhood exploration-based methods <ref type="bibr" target="#b16">[17]</ref>. While unsupervised embedding methods can capture the structure of a graph, they often cannot utilize node features or domain-specific labels. Graph Convolution Networks (GCN) is one of the first semi-supervised methods which used graph convolutions for better representation learning <ref type="bibr" target="#b2">[3]</ref>. Later, many other GNN methods have been proposed to tackle evolving issues in GNNs and improve graph learning, e.g., Graph-SAGE to incorporate different types of aggregation strategies <ref type="bibr" target="#b6">[7]</ref>, Graph Attention Networks (GAT) to consider the asymmetric influence of neighbors <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>, FastGCN to improve the training process <ref type="bibr" target="#b19">[20]</ref>, Cluster-GCN to resolve neighborhood expansion problem and train deep networks <ref type="bibr" target="#b8">[9]</ref>. Some recent efforts pre-process the original graph and/or input features to create a modified graph feeding it to all convolutional layers of the GNN <ref type="bibr" target="#b9">[10]</ref>. We refer readers to a comprehensive survey for further details <ref type="bibr" target="#b20">[21]</ref>. Most GNN methods use a static graph (often preprocessed) in all GNN layers <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref> whereas some methods use a mixing contribution of neighbors <ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref>. GraphSAINT uses different sampled subgraphs in different minibatches but it still uses the same subgraph at every layer <ref type="bibr" target="#b7">[8]</ref>. Different from previous work, we put efforts to use different graph structures in different convolutional layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THE MARKOV DIFFUSION PROCESS</head><p>In this section, we review the Markov process <ref type="bibr" target="#b1">[2]</ref> that simulates flow within a graph such that the flow eventually gets trapped in communities at the convergence of the process. The flow is simulated by random walks with the assumption that higher-length paths between two nodes within a community should stay within the community. The Markov process controls random walks by computing transition probabilities between every pair of vertices. The process incrementally promotes the probability of intra-cluster walks, demotes the probability of inter-cluster walks, and removes paths with low probabilities to discover natural clusters in the graph. Markov process consists of four operations: (a) starting with a column (or row) stochastic transition matrix and maintaining it until convergence, (b) expanding random walks by the expansion operation, (c) strengthening intra-cluster walks and weakening inter-cluster walks by the inflation operation, and (d) removes paths with low probabilities by the prune operation. These operations are repeated until converges.</p><p>The transition matrix. Let D be a diagonal matrix with</p><formula xml:id="formula_5">D[?, ?] = ? ?=1 A[?, ?],</formula><p>where A is the (weighted) adjacency matrix of the graph. The Markov process starts with a column-stochastic matrix M = AD -1 . The ?th column of M sums to one, and M[?, ?] is interpreted as the probability of following the edge from vertex ? to vertex ? in the random walk.</p><p>The expansion operation. The Markov process expands random walks by squaring the Markov matrix M. Thus, the expansion operation in the ?th iteration computes M i-1 ?M i-1 where M i-1 is the output of the previous iteration. The expansion step is similar to the generalized graph diffusion <ref type="bibr" target="#b9">[10]</ref> and typically converges to an idempotent matrix according to the Perron-Frobenius theorem.</p><p>The inflation operation. The parameterized inflation operation with parameter ? performs Hadamard power of the matrix denoted by M ?? and makes the matrix column stochastic again as follows:</p><formula xml:id="formula_6">M ?? [?, ?] = M[?, ?] ? / ? ?? ?=1 M[?, ?] ?<label>(3)</label></formula><p>The goal of the non-linear inflation step is to strengthen higher probability paths (intra-cluster connections) and weaken lower probability paths (inter-cluster connections).</p><p>The prune operation. To keep the Markov matrices sparse, we prune small entries that are below a threshold ? . This step ensures that random-paths are contained within communities and that the matrix does not become dense.</p><p>Generating Markov matrices for GNNs. Algorithm 1 shows the steps used to generate Markov diffusion matrices. The algorithm starts with a column stochastic matrix M 1 where each column sums to 1 and maintains column stochastic matrices in each iteration. The ?th iteration performs the expansion, inflation, and pruning operations. The algorithm converges when the stochastic matrix does not change in successive iterations. We store all stochastic matrices M 1 , M 2 , ..., M ? from different iterations so that they can be used in different layers of a GNN. While there is a memory</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Generating Markov matrices</head><p>Input: A: adjacency matrix of the graph, ? : value of element-wise inflation, ? : pruning threshold Output: M 1 , M 2 , ..., M ? : column stochastic matrices from all steps of the Markov process. </p><formula xml:id="formula_7">M 1 ? ColStochastic(A) 3:</formula><p>for ? ? 2.. do 4:</p><formula xml:id="formula_8">M ? ? M ?-1 ? M ?-1</formula><p>? the expansion operation 5:</p><formula xml:id="formula_9">M ? ? ColStochastic(M ?? ? ) ? inflation 6:</formula><p>M ? ? ColStochastic(Prune(M ? , ? )) ? Sparsify the matrix by pruning small entries less than ?</p><formula xml:id="formula_10">7: if M ? ? M ?-1 then 8: return M 1 , M 2 , ..., M ?</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 MarkovGCN forward propagation</head><p>Input: A: adjacency matrix of an input graph, X: input feature, ? : number of layers, ? : inflation parameter, ? : pruning threshold Output: X (?+1) : node representation from the last layer.</p><formula xml:id="formula_11">1: procedure MarkovGCN(A, ?, ?, ? ) 2: {M 1 , M 2 , ..., M ? } ? MARKOV(A, ?, ? ) ? Alg. 1 3: X (1) ? X 4:</formula><p>W (1) , ...W (?) are weight matrices 5:</p><p>{A (1) , A (2) , ..., A (?) } ? select ? matrices from {M 1 , M 2 , ..., M ? } one for each layer ? assuming ?&lt;=? 6:</p><p>for ? = 1 to ? do 7:</p><formula xml:id="formula_12">X (?+1) = ?? (A (?) X (?) W (?) ) + (1 -?)? (X 1 | . . . |X ?-1 ) 8: return X (?+1)</formula><p>overhead to store ? Markov matrices, it makes the GNN training much faster as we will show in the next section.</p><p>Convergence. It has already been shown that the Markov process converges quadratically <ref type="bibr" target="#b26">[27]</ref>. In most practical graphs, the process converges in less than 30 iterations. Upon convergence, connected components are treated as clusters in the original graph.</p><p>Hyperparameters in the Markov process. The exponent ? in inflation and the pruning threshold ? are hyperparameters that could be tuned to obtained different matrices. A higher value of the parameter ? demotes flow along long paths, which creates finegrained communities. Thus, the number and granularity of final communities can be controlled by ? . When ? is set to 1 (i.e., no inflation), the Markov process is equivalent to a general diffusion process <ref type="bibr" target="#b9">[10]</ref>. ? controls the sparsity of Markov matrices. We empirically identify suitable values of these parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">THE MARKOVGNN ALGORITHM</head><p>The MarkovGNN algorithm exchanges the static adjacency matrix with a series of Markov matrices at different layers of a GNN. This approach could be used with most popular GNN methods such as GCN, GAT, and GraphSAINT. Figure <ref type="figure" target="#fig_0">2</ref> illustrates the forward propagation of a MarkovGNN. In this example, the GNN has three layers (shown in the top panel), but the Markov process converges in four iterations (in the bottom panel). Hence, layer 1, 2, and 3 of GNN use M 1 , M 3 , and M 5 , respectively.</p><formula xml:id="formula_13">"(A (1) X (1) W (1) ) = X (2) Convolutional Layer 1 #"(A (2) X (2) W (2) ) + (1-#)X (2) = X (3) Convolutional Layer 2 #"(A (3) X (3) W (3)) ) + (1-#)X (2) = X (4) Convolutional Layer 3 Iter 1</formula><p>Iter 2 Iter 3 Iter 4</p><formula xml:id="formula_14">A (1) M 1 Markov process M 2 M 4 M 3 M 5 A (3) A (2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GNN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Residual Connection</head><p>Output Layer Residual connections. A MarkovGNN needs more layers than a typical GNN to utilize various Markov matrices at different layers. To train such deep GNN models, we use residual connection <ref type="bibr" target="#b27">[28]</ref> which has also been used in other deep networks <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref>. With residual connections, the embedding in the ?-th layer of MarkovGNN is computed as follows:</p><formula xml:id="formula_15">X (?+1) = ?? (A (?) X (?) W (?) ) + (1 -?)? (X 1 | . . . |X ?-1 ) (4)</formula><p>where, ? is the activation function, ? is a selector and ? is a hyperparameter that controls the contribution of the residual connections. Note that the operator ? in ?-th layer selects only one representation matrix from the previous ? -1 layers.</p><p>The MarkovGNN algorithm. Algorithm 2 describes the forward propagation of the MarkovGCN algorithm. If a GNN has ? layers, we select ? matrices {A (1) , A (2) , ..., A (?) } from the collection of stochastic matrices returned by Algorithm 1. Then, in the ?th layer of the GNN, convolution is performed on A (?) instead of the original adjacency matrix (line 7 of Algorithm 2). Here, we assume that the number of iterations ? needed for the convergence of the Markov process is greater than ?. This is a reasonable assumption since ? is typically more than 20 for most graphs we considered, and most GNNs have less than 10 layers in practice. The selection of ? matrices from ? available matrices is an algorithmic choice for which we show various experimental results. However, we always present Markov matrices to GNN layers in the same order they are generated in the Markov process.</p><p>Training MarkovGNN. In Algorithm 2, ? represents an activation function in line 7 for which either ReLU or LeakyReLU is a common choice. We add a tunable dropout parameter to each convolutional layer. In addition, a softmax layer is added to the last layer of MarkovGNN. Similar to other GNNs, our goal is to learn weight matrices by optimizing a loss function as follows:</p><formula xml:id="formula_16">L = ?? ? ?Y ? ???? (? ? , X (?) ? ),<label>(5)</label></formula><p>where, Y ? represents all vertices that have ground truth labels, and X</p><p>(?) ? represents ?th row of X ? having ground truth label ? ? . In MarkovGNN, we use the standard negative log-likelihood loss function and the Adam optimizer <ref type="bibr" target="#b30">[31]</ref> to train models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1: GNN variants by combining Markov matrices.</head><p>A (1)  A (2) ...</p><formula xml:id="formula_17">A (?) Equivalent GNN models M 1 M 1 M 1 GCN [3] ? ?=1 M ? ? ?=1 M ? ? ?=1 M ? GDC [10] M ? M ? M ? Cluster-GCN [9] M 1 M 2 M ? General Markov-GNN</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Variants of MarkovGNN</head><p>MarkovGNN is a general technique that could be used with any GNN. For example, when Markov matrices are used with GCN, we call this combination MarkovGCN (similarly, MarkovGAT, Markov-GraphSAINT, etc.). Furthermore, the Markov matrices can be combined differently to obtain several interesting variants of GNNs as shown in Table <ref type="table">1</ref> and described below. uMarkovGNN: union of Markov matrices. In row 2 of Table 1 we use M= ? ?=1 M ? as a unified Markov matrix and use it in all layers of a GNN. For example, the forward propagation at layer ? of GCN could be computed by MX (?) W (?) where the graph structure M does not change in different layers. We call this approach uMarkovGNN which is equivalent to the diffusion matrix considered in Eq. 2. However, uMarkovGCN may perform better than GDC because the former preserves a unified view of communities.</p><p>MarkovClusterGNN. In row 3 of Table <ref type="table">1</ref> we use the converged Markov matrix M ? at every layer of GNN (that is, the computation at layer ? is M ? X (?) W (?) ). We call this approach MarkovClus-terGNN, which is equivalent to Cluster-GCN <ref type="bibr" target="#b8">[9]</ref>. Cluster-GCN also identifies clusters using a graph partitioner and then uses the clustered graph at every layer of GCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Properties of MarkovGNN</head><p>Computational complexity. Let M ? have ? ? nonzeros per column (that is, the average degree of the corresponding graph is ? ? ). Then, the expected cost of computing M ? in line 4 of Algorithm 1 is ?? 2 ? <ref type="bibr" target="#b31">[32]</ref>. Similarly, let A (?) have ? ? nonzeros per column on average. Then the computational cost of the forward propagation is An upper bound on the number of layers ?. In most GNN methods, the number of layers is selected in an ad hoc fashion. By contrast, MarkovGNN sets an upper bound on the number of layers to the number of Markov matrices (?) returned by Algorithm 1.</p><formula xml:id="formula_18">? ?? ?=1 (?? ? ? ? + ?? ? ? ?+1 ).<label>(6)</label></formula><p>A potential solution to the neighbor-explosion problem. As communities are formed, the Markov process removes intracluster edges via the inflation and pruning operations. These steps keep the graph sparse at every layer and could prevent the neighborexplosion problem observed in GNNs.</p><p>The role of sampling from {M 1 , M 2 , ..., M ? } in GNN layers. The selection of ? matrices from ? available Markov matrices is an algorithmic choice for which we can consider various selection strategies. This selection of matrices from a pool of available matrices makes it unnecessary to sample neighbors used in GraphSAGE <ref type="bibr" target="#b6">[7]</ref> or to sample minibatches used in GraphSAINT <ref type="bibr" target="#b7">[8]</ref>.</p><p>Limitations of MarkovGNN. Since the Markov process iteratively identifies the community structure in the graph, MarkovGNN is expected to excel when the graph has well-defined communities. Our experimental results show clear evidence in favor of this hypothesis. However, when a graph has no clear clustering patterns MarkovGNN may not perform better than other GNN methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS 5.1 Experimental Setup</head><p>Overview of experiments. We conduct an extensive set of experiments by copuling Markov matrices with GCN, GAT, and GraphSAINT. As MarkovGNN is expected to capture the clustering patterns in a graph, we compare the clustering effectiveness of MarkovGNN with other GNN and unsupervised algorithms. We also examine the effectiveness of MarkovGNN in node classification and graph visualization tasks.</p><p>Experimental platforms. We conduct all experiments on an Intel Skylake processor with 48 cores and 128GB memory. We have implemented MarkovGNN using PyTorch Geometric (PyG) <ref type="bibr" target="#b32">[33]</ref> version 1.6.1 built on top of PyTorch 1.7.0. We used GCN, GAT, Cluster-GCN, GDC, and GraphSAINT implementations from PyG. We run all experiments 10 times and report the average results.</p><p>Datasets. We experimented with eight graphs from different domains such as actor co-occurrence network in films (Actor), airport networks (USAir), community network (Email), biological network (PPI), wikipedia networks (Chameleon and Squirrel), and citation networks (Cora and Citeseer). All these graphs have been used previously in different research papers <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref>. Table <ref type="table" target="#tab_0">2</ref> provides a summary of the graphs. The clustering coefficient of a node in a network measures the propensity to form a cluster and the Average Clustering Coefficient (ACC) is a generalization that represents this score for the whole network <ref type="bibr" target="#b35">[36]</ref>. For networks without input features, we use a one-hot encoding representation of vertices and feed it to the GNNs as input features.</p><p>Experiment settings. Among the graphs having input features, Cora and Citeseer have high assortativity whereas Chameleon and Squirrel have low assortativity <ref type="bibr" target="#b10">[11]</ref>. Thus, for the classification task using Cora and Citeseer networks, we use 30 vertices per class for training, 500 vertices for validation, and 1000 vertices for testing. For all other networks, we use 70% vertices for training, 10% vertices for validation, and the remaining 20% vertices for testing.</p><p>In clustering experiments, we infer the predicted labels of all nodes using the trained models and compute clustering measures such as the Adjusted Rand Index. Similarly, for visualization, we infer 64-dimensional embeddings from the last convolutional layer of GNN models and generate 2D layouts using t-SNE <ref type="bibr" target="#b36">[37]</ref>. Table <ref type="table">5</ref> reports a list of parameters used to run GNN methods.</p><p>Baselines. We choose five semi-supervised methods (GCN, GDC, GAT, Cluster-GCN, and GraphSAINT) to compare the effectiveness of proposed method. GCN is the pioneering work using graph convolutions in GNN models <ref type="bibr" target="#b2">[3]</ref>. GAT uses an attention mechanism by considering the influence of neighbors of a vertex in a graph <ref type="bibr" target="#b17">[18]</ref>. GDC uses diffusion on a graph (as shown in Eq. 2) and then feeds the modified graph to GNNs <ref type="bibr" target="#b9">[10]</ref>. In our analysis, we consider GCN as an underlying method for GDC. Cluster-GCN partitions the graph using a clustering technique before feeding it to GNN <ref type="bibr" target="#b8">[9]</ref>. GraphSAINT is a recent method based on different types of sampling approaches such as random-walk sampling and edge sampling <ref type="bibr" target="#b7">[8]</ref>. Besides these semi-supervised methods, we also experimented with two unsupervised graph embedding methods such as DeepWalk <ref type="bibr" target="#b12">[13]</ref> and struc2vec <ref type="bibr" target="#b14">[15]</ref>. We set different hyperparameters of other GNN models in PyG framework. Tables <ref type="table">5</ref> and<ref type="table">6</ref> provide a summary of the parameters used in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Clustering Performance of MarkovGNN</head><p>Theoretically, we expect that MarkovGNN would detect the clustering pattern present in the original graphs. To validate this expectation, we use predicted vertex labels from MarkovGNN and other GNN methods to partition the graph into predicted clusters. We then use true class labels as ground truth clusters and measure the similarity between predicted and ground-truth clustering patterns.</p><p>Cluster similarity metrics. To measure the similarity between the predicted and given clusterings, we use the adjusted Rand index (ARI) <ref type="bibr" target="#b37">[38]</ref> and V-Measure <ref type="bibr" target="#b38">[39]</ref>. The original Rand index identifies (1) the number of vertex pairs that are in the same cluster in both given and predicted clusterings (true positives) and (2) the number of vertex pairs that are in different clusters in given and predicted clusterings (true negatives). The true positives and true negatives are added and normalized by the total number of vertex pairs to get the Rand index. ARI simply adjusts the Rand index for the chance Figure <ref type="figure" target="#fig_3">3</ref>: Comparing the quality of predicted clustering using the ARI index (higher is better). In each plot, bars are organized in three groups based on their base GNN models: GCN (first four bars), GAT (5th and 6th bars), and GraphSAINT (the last two bars). MarkovGNN represents the last bar in each group, and it generally improves the performance of the base GNN model. grouping of elements. The higher the ARI the more similar two clusterings are. The V-Measure <ref type="bibr" target="#b38">[39]</ref> is computed from the homogeneity (?) and completeness (?) scores. Homogeneity computes how different the predicted labels are for each true cluster and completeness computes how different the clusters are for each type of predicted label. The V-Measure computes the weighted harmonic mean of ? and ? i.e., (1+?)?? ??+? , where ?&gt;1 puts more weight on homogeneity and ?&lt;1 puts more weight on completeness (in our case, ?=1). A higher value of V-Measure represents more similar clusterings.</p><p>MarkovGNN improves the prediction of clusters. Fig. <ref type="figure" target="#fig_3">3</ref> compares the quality of clustering predictions (using ARI) from various GNN methods. We observe that the Markov process improves the clustering performance of GCN, GAT, and GraphSAINT for almost all graphs. For example, MarkovGCN performs better than GCN for all graphs, MarkovGAT performs better than GAT for all graphs except Cora and Citeseer, and MarkovGraphSAINT performs better than GraphSAINT for all graphs except Cora and Citeseer. Furthermore, MarkovGCN consistently outperforms GDC and Cluster-GCN even though the latter two methods use diffusion and clustering to preprocess the graph. Overall, for any graph, there is at least one variant of MarkovGNN that performs better than any other approach according to the ARI measure. For example, MarkovGCN performs the best for Cora even though Markov matrices do not improve the performance of GraphSAINT and GAT on this graph. MarkovGNN also exhibits better clustering performance according to V-Measure (expect for the PPI network where GraphSAINT performs the best) as shown in Table <ref type="table">3</ref>. We also experimented with unsupervised embedding methods DeepWalk, and struc2vec for community detection (see Fig. <ref type="figure" target="#fig_7">8</ref>). However, the unsupervised methods perform much worse than GNNs, which is not surprising because unsupervised methods do not use class labels when finding the embedding or communities. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>When does MarkovGNN excel in predicting clusters?</head><p>When a graph has a high clustering coefficient, MarkovGNN predicts the original clustering much better than its peers. This behavior can be observed in Fig. <ref type="figure" target="#fig_3">3</ref> and Table <ref type="table">3</ref>. For example, in Fig. <ref type="figure" target="#fig_3">3</ref>, MarkovGCN performs much better than GCN for the USAir, Chameleon, Squirrel, and Email graphs that have clustering coefficients greater than or equal to 0.4 (see Table <ref type="table" target="#tab_0">2</ref>). Cluster-GCN also performs well (slightly worse than MarkovGCN) for these graphs, indicating the usefulness of cluster-based GNN methods. However, for Cora, Citeseer, and PPI that have a clustering coefficient less than 0.25, MarkovGCN performs slightly better than GCN. Hence, the utility of MarkovGNN may diminish when the input graph lacks clustering patterns. MarkovGNN not only attains higher values for ARI and V-measure, it also achieves higher true positive rates for individual classes. See Table <ref type="table" target="#tab_3">7</ref> in the appendix for an example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Node Classification</head><p>Fig. <ref type="figure">4</ref> shows the test accuracy in predicting node labels from different GNN methods. We observe that at least one variant of MarkovGNN shows the best test accuracy for all graphs except for Chameleon and Squirrel where GraphSAINT and Cluster-GCN, respectively, perform better than any other methods. Generally, MarkovGNN improves the performance of its base GNN model in most graphs. In particular, MarkovGCN tends to classify nodes well for graphs where graph-structural properties significantly impact node classification such as USAir. Similarly, the Email network has known ground truth communities that MarkovGCN can exploit to outperform other methods significantly. For widely studied graphs such as Cora and Citeseer, MarkovGCN marginally outperforms GCN. However, we do not expect significant improvement for Cora and Citeseer because of their lower average clustering coefficient values as shown in Table <ref type="table" target="#tab_0">2</ref>. Note that GraphSAINT is a recent method that performs exceptionally well for Chameleon and Squirrel networks. However, GraphSAINT does not perform well (often worse than vanilla GCN) for graphs with known communities (e.g., the Email network) and structures (e.g., USAir). By contrast, these are the networks where MarkovGCN excels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Visualization of clusters</head><p>If an embedding of a graph captures the community structure, it should provide an aesthetically pleasing visualization by keeping nodes well clustered in the embedding space. To show the impact of MarkovGNN, we take the 64-dimensional embedding from the last layer of a GNN and use t-SNE <ref type="bibr" target="#b36">[37]</ref> to project 64-dimensional embeddings to 2D space. We show the visualizations for the Chameleon graph from GCN and MarkovGCN in Fig. <ref type="figure">5</ref>, where nodes from the same ground-truth class are shown with the same color. Fig. <ref type="figure">5</ref> shows that the visualization obtained from MarkovGCN tends to place vertices from the same class together while keeping different classes well separated. For example, we can easily identify three clusters from the MarkovGCN-based visualization in the right subfigure, but the visualization from GCN failed to separate clusters from one another.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Running Time</head><p>We measure the average training time of all methods for 10 runs (available in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Parameter Sensitivity</head><p>The impact of the pruning threshold (? ). The pruning threshold plays an important role in the convergence of the Markov process. Fig. <ref type="figure" target="#fig_5">6</ref> (a) reports the ARI of MarkovGCN using Citeseer for different values of ? . For the Citeseer network, MarkovGCN achieves the best ARI score when ? ? 0.15. We also observe in Fig. <ref type="figure" target="#fig_5">6</ref> (a) that an increasing threshold decreases the number of edges in the second layer of MarkovGCN. Thus, the computation can be made much faster by sacrificing the accuracy slightly. If ARI or other measures do not change for different values of ? , we prefer to use higher thresholds which typically make training and inference faster. Impact of the inflation parameter (? ). To find an optimal value of inflation in Algorithm 1, we vary ? from 1.1 to 3.1 and run the experiments using Chameleon and PPI networks. Fig. <ref type="figure" target="#fig_5">6(b)</ref> shows that MarkovGCN is less sensitive to the inflation parameter. We observe better clustering results when ? is close to 1.5 and keep this value as a default value of ? .</p><p>Column vs row stochastic matrices. Algorithm 1 maintains column stochastic matrices in every iteration. Fig. <ref type="figure" target="#fig_5">6(c)</ref> shows that row stochastic matrices also perform similarly for different graphs.</p><p>Residual connections with a hyper-parameter (?). In Eqn. 4, we aim to control the residual connections with a hyper-parameter ?. To select a suitable residual connection for ? in Eqn. 4, we conduct an experiment to predict the impact of different layers on the predictive performance of a four-layer GNN. Following the protocol used by Raghu et al. <ref type="bibr" target="#b39">[40]</ref>, we disabled gradient updates for all layers except one and plot the observed validation accuracy for different layers in Fig. <ref type="figure" target="#fig_6">7</ref>. For the Cora network, we observe that the input layer (layer 1) has the highest accuracy followed by layer 4. The contribution of layer 1 is expected to be significant for graphs such as Cora that include node features. Thus, when adding residual connections, we select layer 1 in Eqn. 4. We set ? = 0.15 for the experiments of MarkovGCN in Fig. <ref type="figure" target="#fig_6">7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS</head><p>Communities in a network can be discovered by probabilistic random walks with pruning that iteratively promote intra-cluster edges and demote inter-cluster edges. The proposed algorithm MarkovGNN captures snapshots of this community discovery process in different layers of graph neural networks. We conclude with experimental evidence that using Markov matrices in different layers improve the performance of clustering, node classification and visualization tasks. MarkovGNN excels on graphs having welldefined community structures (e.g., a high value of the clustering coefficient) because it can preserve the communities in the embedding space. However, if a graph lacks a community structure, MarkovGNN performs no worse than GCN. Overall, MarkovGNN brings in the philosophy of using different community-capturing graphs at different of a GNN and provides a flexible design space for graph representation learning.</p><p>Table <ref type="table">6</ref>: Parameters used to run different variants of MarkovGCN in PyTorch-Geometric framework (Fig. <ref type="figure" target="#fig_3">3</ref>, Fig. <ref type="figure">4</ref>, and Table <ref type="table">3</ref>). We set embedding dimension to 64 for all methods. For all variants, we use Negative Log-Likelihood Loss Function and Adam Optimizer. ????? -Learning rate, ???????? -dropout rate, ??????? -the number of convolutional layers.  If no information is provided, then we use the default parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>In Table <ref type="table">5</ref>, we report the parameters for other methods. We analyze the parameter sensitivity for different variants of MarkovGCN. We use a grid search approach to search optimal value as described in Sec. A. After an extensive set of experiments, we pick a suitable set of parameters for the MarkovGCN models. We report the values of different parameters in Table <ref type="table">6</ref> that produced better results in our experiments. These sets of parameters have been used to generate the results in Fig. <ref type="figure" target="#fig_3">3</ref>, Fig. <ref type="figure">4</ref>, and Table <ref type="table">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C ADDITIONAL RESULTS</head><p>Table <ref type="table" target="#tab_3">7</ref> shows the contingency table for the Chameleon network where the (?, ?)th entry denotes the number of vertices in common between the ground truth class ? ? and the predicted class ? ? . Diagonal entries in Table <ref type="table" target="#tab_3">7</ref> denote true positive pairs of vertices stay together in both ground truth and predicted clusterings. These results are consistent with our hypothesis that MarkovGNN predicts the clustering patterns if available in the input graph.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>[9]. ( 2 )</head><label>2</label><figDesc>By using residual connections, MarkovGNN supports deeper GNNs. MarkovGNN reduces the neighbor-explosion problem in deeper layers by keeping Markov matrices sparse. (3) We conduct an extensive set of experiments to evaluate the performance of MarkovGNN with respect to baseline GNN methods that use the same graph at every layer. If vertices in the original graph are well-clustered, MarkovGNN preserves the clustering pattern in the embedding space and improves the performance of clustering, node classification, and visualization tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 :</head><label>1</label><figDesc>procedure Markov(A, ?, ? ) 2:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The bottom panel shows column stochastic matrices generated by the Markov process for hypothetical graphs with bi-directional edges shown by lines. Here, M 1 is the input stochastic matrix representing the given graph, M 5 is the converged matrix with four discovered communities, and M 2 , M 3 , and M 4 are intermediate matrices capturing the formation of communities. As communities are discovered, edges are added and deleted in different iterations. The top panel shows a GNN that uses a subset of Markov matrices in three convolutional layers where ? is the contribution factor of residual connections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Table 3 :</head><label>3</label><figDesc>V-Measure (avg. of 10 runs) for different methods. A higher value of V-Measure means a better prediction of clusters. The best value for each dataset is highlighted in green. Clust.GCN -Cluster-GCN. Methods USAir Cham.Squi. Email Cora Cite. PPI Actor GCN 0.564 0.436 0.297 0.778 0.616 0.418 0.534 0.211 GAT 0.443 0.479 0.152 0.653 0.636 0.425 0.318 0.014 Clust.GCN 0.591 0.532 0.38 0.86 0.588 0.383 0.609 0.058 GDC 0.263 0.414 0.26 0.634 0.629 0.418 0.359 0.091 GraphSAINT 0.448 0.566 0.217 0.817 0.517 0.315 0.673 0.221 MarkovGCN 0.607 0.576 0.493 0.86 0.638 0.426 0.601 0.228</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 4: Comparing the performance in classifying nodes using different variants of MarkovGNN and other GNN methods. In each plot, bars are organized in three groups based on their base GNN models: GCN (first four bars), GAT (5th and 6th bars), and GraphSAINT (the last two bars). MarkovGNN represents the last bar in each group, and it often improves the performance of the base GNN model.</figDesc><graphic url="image-69.png" coords="7,151.09,258.38,277.91,193.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Parameter sensitivity of MarkovGCN showing (a) influence of pruning threshold on Citeseer network with corresponding ARI and the remaining edges for the 2nd convolutional layer, (b) influence of inflation parameter for Chameleon and PPI networks, and (c) effect of using row vs. column stochastic matrices in Algorithm 1.</figDesc><graphic url="image-81.png" coords="8,393.55,90.02,154.94,112.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Layer-wise validation accuracy in GCN model for the Cora network.</figDesc><graphic url="image-85.png" coords="8,356.66,253.49,155.11,112.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Sensitivity of ? parameter for a homophilic and a heterophilic networks.</figDesc><graphic url="image-89.png" coords="11,83.34,508.18,188.34,141.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>Summary of Datasets. '-' indicates the absence of input features. Graphs are sorted based on the descending order of the Average Clustering Coefficient (ACC).The role of sparsity. As communities are formed in the Markov process, the corresponding matrices become increasingly sparse. As a result, ? ?+1 is typically smaller than ? ? in deeper layers of MarkovGNN. Thus, MarkovGNN is expected to run faster than other other diffusion based methods such as GDC.</figDesc><table><row><cell>Graphs</cell><cell cols="5">Nodes Edges Classes Features ACC</cell></row><row><cell>USAir</cell><cell>1,190</cell><cell>13,599</cell><cell>4</cell><cell>-</cell><cell>0.50</cell></row><row><cell>Chameleon</cell><cell>2,277</cell><cell>36,101</cell><cell>3</cell><cell>3,132</cell><cell>0.48</cell></row><row><cell>Squirrel</cell><cell>5,201</cell><cell>217,073</cell><cell>3</cell><cell>31,48</cell><cell>0.42</cell></row><row><cell>Email</cell><cell>1,005</cell><cell>25,571</cell><cell>42</cell><cell>-</cell><cell>0.40</cell></row><row><cell>Cora</cell><cell>2,708</cell><cell>10,556</cell><cell>7</cell><cell>1433</cell><cell>0.24</cell></row><row><cell>Citeseer</cell><cell>3,327</cell><cell>9,104</cell><cell>6</cell><cell>3703</cell><cell>0.14</cell></row><row><cell>PPI</cell><cell>2,361</cell><cell>7,182</cell><cell>13</cell><cell>-</cell><cell>0.13</cell></row><row><cell>Actor</cell><cell>7,600</cell><cell>33,544</cell><cell>5</cell><cell>931</cell><cell>0.08</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 4 :</head><label>4</label><figDesc>Average runtimes of 10 runs in seconds are reported. For all GNNs, we only measure the training time of 200 epochs in the PyG framework.</figDesc><table><row><cell cols="6">Graphs GCN GAT GDC Gr.SAINT MarkovGCN</cell></row><row><cell>USAir</cell><cell>7.4</cell><cell>21.5</cell><cell>95.3</cell><cell>212.4</cell><cell>7.7</cell></row><row><cell>Email</cell><cell>14.3</cell><cell>31.8</cell><cell>98.2</cell><cell>251.9</cell><cell>11.3</cell></row><row><cell>PPI</cell><cell>13.7</cell><cell>32.6</cell><cell cols="2">228.6 167.2</cell><cell>17.3</cell></row><row><cell cols="2">Chamel. 26.8</cell><cell>60.3</cell><cell cols="2">222.3 441.1</cell><cell>30.6</cell></row><row><cell cols="5">Squirrel 229.9 327.3 678.9 2,110.9</cell><cell>188.8</cell></row><row><cell>Cora</cell><cell>10.9</cell><cell>31.2</cell><cell cols="2">245.8 160.8</cell><cell>16.4</cell></row><row><cell cols="2">Citeseer 24.1</cell><cell>67.9</cell><cell cols="2">317.8 220.3</cell><cell>41.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4</head><label>4</label><figDesc></figDesc><table /><note><p>). Despite needing additional time in generating Markov matrices (Algorithm 1), MarkovGCN and GCN have comparable running time. On the other hand, GDC runs a diffusion step that is computationally similar to Markov clustering. However, effective sparsification by inflation and pruning makes MarkovGCN an order of magnitude faster than GDC. Similarly, MarkovGCN runs 10? to 100? faster than GraphSAINT. The advantage of MarkovGCN is originated from the decreasing number of edges in deeper layers. By contrast, other GNN methods use the same graph in all layers.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 7 :</head><label>7</label><figDesc>The contingency table for the Chameleon network.</figDesc><table><row><cell></cell><cell cols="3">Parameters for Markov Clustering Steps</cell><cell cols="4">Parameters for Convolutional Layers</cell></row><row><cell>MarkovGCN Variants</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>--markov_agg</cell><cell cols="7">Threshold (? ) Inflation (?) Row-Stochastic. LeakyRelu ????? ???????? ???????</cell></row><row><cell>False</cell><cell>0.1</cell><cell>1.6</cell><cell>FALSE</cell><cell>FALSE</cell><cell>0.01</cell><cell>0.5</cell><cell>4</cell></row><row><cell>USAir</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>True</cell><cell>0.09</cell><cell>1.5</cell><cell>FALSE</cell><cell>FALSE</cell><cell>0.01</cell><cell>0.5</cell><cell>4</cell></row><row><cell>False</cell><cell>0.3</cell><cell>1.5</cell><cell>FALSE</cell><cell>FALSE</cell><cell>0.01</cell><cell>0.4</cell><cell>3</cell></row><row><cell>Email</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>True</cell><cell>0.26</cell><cell>1.5</cell><cell>FALSE</cell><cell>FALSE</cell><cell>0.01</cell><cell>0.3</cell><cell>3</cell></row><row><cell>False</cell><cell>0.75</cell><cell>1.7</cell><cell>TRUE</cell><cell>FALSE</cell><cell>0.01</cell><cell>0.1</cell><cell>3</cell></row><row><cell>PPI</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>True</cell><cell>0.35</cell><cell>1.8</cell><cell>TRUE</cell><cell>FALSE</cell><cell>0.05</cell><cell>0.5</cell><cell>2</cell></row><row><cell>False</cell><cell>0.06</cell><cell>1.8</cell><cell>FALSE</cell><cell>FALSE</cell><cell>0.01</cell><cell>0.7</cell><cell>2</cell></row><row><cell>Chameleon</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>True</cell><cell>0.2</cell><cell>1.5</cell><cell>TRUE</cell><cell>FALSE</cell><cell>0.01</cell><cell>0.5</cell><cell>3</cell></row><row><cell>False</cell><cell>0.2</cell><cell>1.5</cell><cell>FALSE</cell><cell>FALSE</cell><cell>0.005</cell><cell>0.2</cell><cell>2</cell></row><row><cell>Squirrel</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>True</cell><cell>0.05</cell><cell>1.5</cell><cell>FALSE</cell><cell>FALSE</cell><cell>0.01</cell><cell>0.25</cell><cell>6</cell></row><row><cell>False</cell><cell>0.28</cell><cell>1.5</cell><cell>FALSE</cell><cell>FALSE</cell><cell>0.01</cell><cell>0.9</cell><cell>2</cell></row><row><cell>Cora</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>True</cell><cell>0.03</cell><cell>1.3</cell><cell>FALSE</cell><cell>FALSE</cell><cell>0.01</cell><cell>0.8</cell><cell>2</cell></row><row><cell>False</cell><cell>0.11</cell><cell>1.6</cell><cell>FALSE</cell><cell>FALSE</cell><cell>0.01</cell><cell>0.5</cell><cell>2</cell></row><row><cell>Citeseer</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>True</cell><cell>0.11</cell><cell>1.5</cell><cell>FALSE</cell><cell>FALSE</cell><cell>0.01</cell><cell>0.6</cell><cell>2</cell></row><row><cell>False</cell><cell>0.35</cell><cell>1.5</cell><cell>FALSE</cell><cell>FALSE</cell><cell>0.01</cell><cell>0.5</cell><cell>3</cell></row><row><cell>Actor</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>True</cell><cell>0.3</cell><cell>1.5</cell><cell>FALSE</cell><cell>FALSE</cell><cell>0.01</cell><cell>0.5</cell><cell>3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 8 :</head><label>8</label><figDesc>Adjusted Rand Index (ARI) and V-Measure for Deep-Walk and struc2vec for different graphs.</figDesc><table><row><cell></cell><cell>ARI</cell><cell></cell><cell cols="2">V-Measure</cell></row><row><cell>Graphs</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">DeepWalk struc2vec DeepWalk struc2vec</cell></row><row><cell>USAir</cell><cell>0.094</cell><cell>0.179</cell><cell>0.150</cell><cell>0.243</cell></row><row><cell>Email</cell><cell>0.472</cell><cell>0.010</cell><cell>0.704</cell><cell>0.237</cell></row><row><cell>PPI</cell><cell>0.030</cell><cell>0.033</cell><cell>0.107</cell><cell>0.068</cell></row><row><cell>Chameleon</cell><cell>0.098</cell><cell>0.035</cell><cell>0.155</cell><cell>0.106</cell></row><row><cell>Squirrel</cell><cell>0.013</cell><cell>0.019</cell><cell>0.031</cell><cell>0.025</cell></row><row><cell>Cora</cell><cell>0.352</cell><cell>0.022</cell><cell>0.457</cell><cell>0.101</cell></row><row><cell>Citeseer</cell><cell>0.205</cell><cell>0.011</cell><cell>0.212</cell><cell>0.066</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A EXPERIMENTAL DETAILS Software Versions. We use Python programming language v3.7.3 to implement and perform all of our experiments. We use several python packages for our implementation which are outlined as follows: PyTorch: 1.7.0, PyTorch Geometric <ref type="bibr" target="#b32">[33]</ref> version 1.6.1 along with PyTorch Sparse 0.6.8, PyTorch Scatter 2.0.5, PyTorch Cluster 1.5.8, PyTorch Spline Conv 1.2.0, NetworkX: 2.2, scikit-learn: 0.23.2, Matplotlib: 3.0.3, python-louvain: 0. <ref type="bibr" target="#b12">13</ref> Implementation Details. The proposed MarkovGCN model has been implemented in PyG framework 1 . We employ both ?????.?? and ?????_??????.?????? functions to perform dense and sparse matrix multiplication operation, respectively, on line 4 of Algorithm 1. We observe very little advantage of using the sparse matrix. Once MCL steps are done, we select ? number of matrices from ? matrices based on the type of MarkovGNN variant. Each convolutional layer of MarkovGNN has a different graph structure which is different from GCN. Otherwise, similar types of operations are performed in both forward and backward propagation as GCN using ??????? class. In ?-th convolutional layer, we pass all edges of ? (?) in MarkovGCN whereas GCN uses edges of the original matrix ?. Similar to other GNN models, we employ the negative log-likelihood loss function and Adam optimizer of PyTorch package. Instead of the threshold-based pruning strategy in Algorithm 1, we have also implemented a sparse ?-selection technique where we keep top-? weighted entries in each row of the stochastic matrix after each iteration of MCL. However, we have found from empirical results that this method is not very effective and thus we do not report here. To conduct the experiment for layer-wise contribution of Fig. <ref type="figure">7</ref> (b), we set the weight.requires_grad and bias.requires_grad parameters of the corresponding layer to True while setting other model parameters to False.</p><p>More Details on Dataset. We have collected USAir network from struc2vec <ref type="bibr" target="#b14">[15]</ref> which is publicly available 2 . Email network is available at Stanford SNAP dataset 3 . Chameleon and Squirrel networks <ref type="bibr" target="#b33">[34]</ref> have been collected from FAGCN <ref type="bibr" target="#b10">[11]</ref> which are publicly available 4 . PPI network has been collected from SuiteSparse website 5 . Cora and Citeseer networks are available in PyG datasets 6 . The last accessed date for all datasets is Oct. 20, 2021.</p><p>Classification, Clustering and Visualization using GNNs. We train all GNN models for 200 epochs. After each epoch, we compute the validation and test accuracy with the so far trained model. We choose the best value of the test classification accuracy for all GNN models. The graph clustering task requires labels for all vertices to avoid any bias in the experiment. Thus, we infer the labels for all vertices using the trained model which are then compared with the ground truth labels to compute performance metrics. For the visualization task, we infer the 64-dimensional embedding from the last layer of the GNNs, then apply t-SNE to generate 2D layouts using the default parameters and plot them using ?????????? package.</p><p>1 https://github.com/rusty1s/pytorch_geometric 2 https://github.com/leoribeiro/struc2vec 3 https://snap.stanford.edu/data/email-Eu-core.html 4 https://github.com/bdy9527/FAGCN 5 https://sparse.tamu.edu/Pajek/yeast 6 https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html Table <ref type="table">5</ref>: Parameters used to run other methods. We set embedding dimension to 64 for all methods. We run all GNNs in PyTorch-Geometric framework using Adam Optimizer.</p><p>Methods:Parameters GCN <ref type="bibr" target="#b2">[3]</ref>: number of convolutional layers = 2, ReLU activation function, learning rate = 0.01 GAT <ref type="bibr" target="#b17">[18]</ref>: number of convolutional layers = 2, head features = 8, ELU activation function, dropout rate = 0.6, learning rate = 0.005 Cluster-GCN <ref type="bibr" target="#b8">[9]</ref>: number of convolutional layers = 2, number of partition = 32 and 64, ReLU activation function, dropout rate = 0.5, learning rate = 0.005 GDC <ref type="bibr" target="#b9">[10]</ref>: column-wise normalization, PPR diffusion with al-pha=0.05, Top-K sparsification with k = 128, number of convolutional layers = 2, ReLU activation function, learning rate = 0.01 GraphSAINT <ref type="bibr" target="#b7">[8]</ref>: edge sampling approach, batch size = 128, number of steps = 5, sample coverage = 15, number of convolutional layers = 3, linear layer = 1, ReLU activation function, dropout rate = 0.2, learning rate = 0.01 Hyper-parameters of MarkovGCN. We analyze the hyperparameters of MarkovGCN extensively using a grid search technique. For other MarkovGNNs, we only use different graphs in different convolutional layers where other parameters remain the same. In MarkovGCN, we analyze three parameters in the Markov process and four parameters in convolutional layers. For Markov process, we use the following strategies:</p><p>? Set threshold (? ) in the ranges {0.001 . . . 0.009}, {0.01 . . . 0.09}, and {0.1 . . . 0.9}, and then increase by step size 0.0005, 0.005, and 0.05, respectively. ? Set inflation (? ) in the range {1.1, . . . 3.3} and increase by step 0.2. We do not observe much variation in performance when ? &gt; 2.</p><p>? Consider either row-stochastic or column-stochastic matrices in the Markov process. By default, MarkovGCN uses column-stochastic matrices. If row-stochastic parameter is set to TRUE, then it uses row-stochastic matrices.</p><p>We explore the parameters for convolutional layers by setting activation function as either LeakyReLU or ReLU, learning rate of Adam optimizer in the ranges {0.01 . . . 0.08} and {0.001 . . . 0.009} increasing the step size by 0.01 and 0.001, respectively. We set dropout rate in the range {0.1 . . . 0.9} increasing the step size by 0.1 and the number of convolutional layers from 2 to 10. Section 5.6 discusses the impact for some of these parameters on MarkovGCN's performance. Sensitivity of ?. In Fig. <ref type="figure">8</ref>, we show the sensitivity of ? parameter of Eqn. 4 using the Citeseer and Chameleon networks. We observe that the homophilic Citeseer network tends to have a lower value of ? whereas, the heterophilic Chameleon network tends to have a higher value to produce better accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B HYPER PARAMETERS</head><p>We use PyG implementations for other methods. The I/O part is the same for all GNN methods. To run a model, we need to set some parameters. We collect those values of parameters from the papers.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Graph clustering by flow simulation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Van Dongen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
		<respStmt>
			<orgName>Utrecht University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On the bottleneck of graph neural networks and its practical implications</title>
		<author>
			<persName><forename type="first">U</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yahav</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Graph neural networks exponentially lose expressive power for node classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>Oono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Suzuki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Beyond homophily in graph neural networks: Current limitations and effective designs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Heimann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koutra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">GraphSAINT: Graph sampling based inductive learning method</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Prasanna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks</title>
		<author>
			<persName><forename type="first">W.-L</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="257" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Diffusion improves graph learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wei?enberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">366</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Beyond Low-frequency Information in Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Graph learning from filtered signals: Graph system and diffusion kernel identification</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">E</forename><surname>Egilmez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pavez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ortega</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Signal and Information Processing over Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="360" to="374" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">struc2vec: Learning node representations from structural identity</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Saverese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Figueiredo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the 23rd ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="385" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Force2vec: Parallel force-directed graph embedding</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Sujon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Azad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Data Mining (ICDM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="442" to="451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">LINE: Large-scale information network embedding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WWW</title>
		<meeting>WWW</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Watch your step: Learning node embeddings via graph attention</title>
		<author>
			<persName><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of NeurIPS</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">FastGCN: Fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Factorizable graph convolutional networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Breaking the limit of graph neural networks by improving the assortativity of graphs with local mixing patterns</title>
		<author>
			<persName><forename type="first">S</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Budde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Neville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD</title>
		<meeting>KDD</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Mixhop: Higher-order graph convolutional architectures via sparsified neighborhood mixing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Alipourfard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Steeg</surname></persName>
		</author>
		<author>
			<persName><surname>Galstyan</surname></persName>
		</author>
		<idno>PMLR</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Towards deeper graph neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="338" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Graph clustering by flow simulation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Van Dongen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
		<respStmt>
			<orgName>University of Utrecht</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Residual attention graph convolutional network for web services classification</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">440</biblScope>
			<biblScope unit="page" from="45" to="57" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Simple and deep graph convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1725" to="1735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Communication optimal parallel multiplication of sparse random matrices</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ballard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Buluc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Demmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Grigori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lipshitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Toledo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-fifth annual ACM Symposium on Parallelism in Algorithms and Architectures</title>
		<meeting>the Twenty-fifth annual ACM Symposium on Parallelism in Algorithms and Architectures</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="222" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Fast graph representation learning with PyTorch Geometric</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.02428</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multi-Scale Attributed Node Embedding</title>
		<author>
			<persName><forename type="first">B</forename><surname>Rozemberczki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Complex Networks</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Geom-GCN: Geometric graph convolutional networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Generalizations of the clustering coefficient to weighted complex networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Saram?ki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kivel?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-P</forename><surname>Onnela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kaski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kertesz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">27105</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Comparing partitions</title>
		<author>
			<persName><forename type="first">L</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Arabie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of classification</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="193" to="218" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">V-Measure: A conditional entropy-based external cluster evaluation measure</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hirschberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">On the expressive power of deep neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2847" to="2854" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
