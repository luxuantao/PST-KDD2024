<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FedPS: A Privacy Protection Enhanced Personalized Search Framework</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jing</forename><surname>Yao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhicheng</forename><surname>Dou</surname></persName>
							<email>dou@ruc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<address>
									<country>Renmin University of China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<address>
									<country>Renmin University of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
							<email>jirong.wen@gmail.com</email>
							<affiliation key="aff2">
								<orgName type="department">Beijing Key Laboratory of Big Data Management and Analysis Methods</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Key Laboratory of Data Engineering and Knowledge Engineering</orgName>
								<address>
									<region>MOE</region>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Beijing Key Laboratory of Big Data Management and Analysis Methods</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">FedPS: A Privacy Protection Enhanced Personalized Search Framework</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3442381.3449936</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>personalized search</term>
					<term>privacy protection</term>
					<term>federated learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Personalized search returns each user more accurate results by collecting the user's historical search behaviors to infer her interests and query intents. However, it brings the risk of user privacy leakage, and this may greatly limit the practical application of personalized search. In this paper, we focus on the problem of privacy protection in personalized search, and propose a privacy protection enhanced personalized search framework, denoted with FedPS. Under this framework, we keep each user's private data on her individual client, and train a shared personalized ranking model with all users' decentralized data by means of federated learning. We implement two models within the framework: the first one applies a personalization model with a personal module that fits the user's data distribution to alleviate the challenge of data heterogeneity in federated learning; the second model introduces trustworthy proxies and group servers to solve the problems of limited communication, performance bottleneck and privacy attack for FedPS. Experimental results verify that our proposed framework can enhance privacy protection without losing too much accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>• Information systems → Personalization.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Personalized search tailors document lists for each user based on the user's interests to satisfy her information need behind the query which might be ambiguous <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b39">40]</ref>. Many studies have been proposed, including traditional methods relying on features <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref> and learning based models that employ deep learning to mine user preferences <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b49">50]</ref>. Existing models exploit users' personal information, such as historical query sequences and click behaviors, to infer their interests and real intent under a query. This raises the risk of user privacy leakage <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b36">37]</ref>. At present, the problem of user privacy protection is receiving more and more attention. Many countries formulate some privacy laws <ref type="bibr" target="#b22">[23]</ref>. In this paper, we focus on the privacy protection issue in personalized search, and investigate the possibility of implementing a personalized search model without exposing user privacy.</p><p>Current solutions for privacy protection in search mainly consider the identifiability and linkability of privacy <ref type="bibr" target="#b0">[1]</ref>. Identifiability means who is the user. Linkability is the possibility of inferring the user's interests from the observed query behaviors. Some studies utilize anonymous user id or group id to mask user identities <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b51">52]</ref>. But some users may issue private queries (such as their names), so that the user's interests, gender and other information can still be deduced from the log. For example, it was shown that detailed user profiles can be constructed based on the published AOL anonymous dataset <ref type="bibr" target="#b4">[5]</ref>. To reduce the linkability, the query obfuscation solution is explored <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b48">49]</ref>. It aims to hide the user's real search intent among a set of noisy queries. Though great effects of privacy protection have been achieved by these methods, they still expose the user's search behaviors to the server and collect the logs to train a personalization model on the server. As web technology develops, there are more malicious attackers on the web, and the obtained personal data can be used in various ways. According to <ref type="bibr" target="#b22">[23]</ref>, most users are worried about their personal data being collected, exploited or released. Currently, users access search engines through their own Internet devices with certain computing, storage and communication capabilities, such as smartphones. Thus, we can use these devices to store the privacy-sensitive data and complete some computing tasks locally.</p><p>In personalized search, every user owns a personal query log that contains the user's detailed query behaviors. We first mine the user's preferences from the log as her interest profile. For example, SLTB model <ref type="bibr" target="#b8">[9]</ref> extracts topic-based and clicked-based features from the search history, and PEPS <ref type="bibr" target="#b46">[47]</ref> trains personal word embeddings with the user's individual log. Then, a personalized ranking model is used to calculate scores for candidate documents based on the user profile to generate personalized results. This process mainly involves three materialized components: the query log, the created user profile, and the trained personalized ranking model. With regard to the contained privacy, the most sensitive part is the original query log of each user, which might contain much personal information; the second is the user profile built on the search history, which explicitly or implicitly reflects user interests.</p><p>The personalized ranking model has no direct access to the original log hence it contains much less privacy. To prevent user privacy leakage, we claim that we are not supposed to record user search logs and construct user profiles on a remote server -we can only store these data on the corresponding user's client devices. Thus, on each client, we only have a user's personal data whose amount is very limited. It is infeasible to solely use these data to train a reliable personalized model for the user. To jointly train a shared high-quality personalization model with query logs distributed on all users' devices, we adapt federated learning to personalized search and design a privacy protection enhanced framework, referred to as FedPS. With this framework, we are able to train a reliable personalized search model with all users' knowledge without exposing their original logs and profiles, which enhances privacy protection and saves the bandwidth of exchanging query logs.</p><p>In FedPS, the client submits the user issued query to the search engine along with cover queries to obscure the real intent, then the personalized ranking model deployed locally re-ranks the returned documents of the real query and shows the result to the user. The query logs, as well as the built user profile, are stored on the client. All clients and a server cooperate to train a shared personalized ranking model. In each step, the server samples several clients and sends the current model to them; these clients update the model with local data and upload the parameter updates; the server aggregates all updates to get a new model. We implement two models within the general framework. In the first one, namely FedPSFlat, we adapt the state-of-the-art personalized search model PEPS <ref type="bibr" target="#b46">[47]</ref> to make it privacy compatible. We select PEPS because it uses personal word embeddings that could be stored and updated locally and is free of data from other users, and it mitigates the data heterogeneity problem in federated learning. But there is only one central server in FedPSFlat and contacting numerous clients could cause a performance bottleneck for the server. Besides, some clients may have limited communication or computation during training. Thus, in the second model FedPSProxy, we introduce group servers and trustworthy proxies to improve the flatten FedPS to a hierarchical structure. The servers do not connect to the clients but proxies, and those poorly connected clients could transfer their model computing tasks to the proxy. This model solves performance pressure, limited communication and privacy attack for FedPS. Experiments on two log datasets prove that FedPS protects user privacy without losing too much search accuracy.</p><p>Our main contribution is summarized as: (1) We explore a crucial issue in personalized search -privacy protection, which is one of the bottlenecks in the practical application of personalized search. This is the first time the privacy issue is considered in a deep personalized search model. <ref type="bibr" target="#b1">(2)</ref> We give a detailed analysis on the data used and generated in personalized search, and evaluate their privacy sensitivity. Based on the analysis, we design a privacy protection enhanced framework FedPS by adapting federated learning to state-of-the-art personalized search and implement two specific models. Experiments confirm that our framework can protect privacy without affecting the ranking quality too much. (3) The first model FedPSFlat alleviates the data heterogeneity challenge of federated learning. The FedPSProxy introduces trustworthy proxies and group servers to relieve performance pressure for the server and promote privacy protection.</p><p>In the rest of the paper, related works are reviewed in Section 2. The FedPS framework is presented in Section 3. In Section 4 and 5, we introduce the experiments. The paper is concluded in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Personalized Search</head><p>Personalized search has been widely studied due to its ability to return users more satisfying search results. The basic idea is inferring user interests from the search history and re-ranking the general search results based on the interests. Most early models were derived from heuristic methods or used features to analyze user preferences. For example, Dou et al. <ref type="bibr" target="#b18">[19]</ref> proposed P-Click to re-rank documents by how many times the user has clicked them in the search history. Some models applied a topic model to obtain topic-based features from the clicked documents which are used to express user interests <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b42">[43]</ref><ref type="bibr" target="#b43">[44]</ref>. SLTB <ref type="bibr" target="#b8">[9]</ref> combined both the click-based and topic-based features extracted from the search history with the learning-to-rank (LTR) algorithm Lamb-daRank <ref type="bibr" target="#b11">[12]</ref>. Moreover, some works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b15">16]</ref> demonstrated that the user's location, reading level and other features are also helpful for personalization.</p><p>Deep learning has been widely applied in personalized search due to its representation learning ability for mining potential user preferences. Song et al. <ref type="bibr" target="#b38">[39]</ref> used the individual data to adapt the global model. Ge et al. <ref type="bibr" target="#b20">[21]</ref> designed a hierarchical RNN with queryaware attention to capture sequential information hidden in the history and dynamically build user profile according to the current query. Lu et al. <ref type="bibr" target="#b25">[26]</ref> and Yao et al. <ref type="bibr" target="#b47">[48]</ref> respectively leveraged the generative adversarial network and reinforcement learning to help construct better user profiles. Besides, some works <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b49">50]</ref> attempted to disambiguate the current query with personal word embeddings for each user, search context or knowledge graph. All these models make improvements in personalization, but they ignore the privacy protection issue. In this paper, we design a privacy protection enhanced personalization framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Privacy Protection in Personalization</head><p>The inherent tension between personalization and user privacy originates from the essence of personalization techniques, which track the user's search process to infer her query intents and interests. As people become more concerned about their privacy <ref type="bibr" target="#b22">[23]</ref>, privacy protection in personalization receives widespread attentions.</p><p>Shen et al. <ref type="bibr" target="#b36">[37]</ref> defined four levels of privacy protection in personalized search: pseudo identity, group identity, no identity and no personal information. Various approaches are proposed to achieve different levels of protection. They mainly focus on the identifiability and linkability of user privacy. Anonymous user id <ref type="bibr" target="#b16">[17]</ref>, group user id which is shared by a group of users <ref type="bibr" target="#b51">[52]</ref> and peer-to-peer query schemes <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b17">18]</ref> where each user submits queries issued by other users were exploited to mask the user identity. Considering the raw text may contain user privacy information, Li et al. <ref type="bibr" target="#b23">[24]</ref> and Bendersky et al. <ref type="bibr" target="#b5">[6]</ref> converted the original texts into anonymous n-grams or generalized attribute values. To prevent the disclosure of specific user interests from the collected user query log, many studies added noise to the recorded data <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b32">33]</ref>. Additional fake queries are generated along with the real query to obscure the user's query intents <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b35">36]</ref>. These approaches play a role in privacy protection, but all users' original query logs are still centrally collected by them to train the model.</p><p>Federated learning <ref type="bibr" target="#b27">[28]</ref> is a great technique to train a shared model with all users' data distributed on their individual devices, without the need to centrally store these data. It maintains a global model on the central server; each client trains the global model with the local data and sends the model update to the server; then all these updates are used to improve the current global model. This method guarantees the security of each participant's original data compared to traditional machine learning. However, it has also been demonstrated that the model updates might leak the user privacy <ref type="bibr" target="#b29">[30]</ref>. Consequently, several defense methods were proposed, such as Multi-Party Computation (MPC) <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b19">20]</ref>, Homomorphic Encryption (HE) <ref type="bibr" target="#b31">[32]</ref> and Differential Privacy (DP) <ref type="bibr" target="#b28">[29]</ref>. Secure MPC <ref type="bibr" target="#b19">[20]</ref> is a class of cryptography technique for many participants to jointly train a model in a peer-to-peer topology. DP <ref type="bibr" target="#b28">[29]</ref> protects privacy by adding random noise to the uploaded data, which has an impact on model accuracy. Our privacy protection enhanced framework is designed based on federated learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OUR PROPOSED APPROACH 3.1 Problem Formulation</head><p>In personalized search, we first analyze the user's search history to build a user interest profile, then the personalized ranking model customizes a document list for the user based on her user profile. This process mainly involves the user's original search log, user profile, a personalized ranking model and some shared assisted data such as term frequencies, word embeddings, etc. We carefully analyze their contents and list the involved user privacy below. • User's original search log, including all queries issued by the user, browsed document lists and click behaviors. Search log is the most privacy-sensitive data in personalized search, and studies <ref type="bibr" target="#b4">[5]</ref> have shown that information (such as name, residence, and hobbies) of some users can be identified by analyzing their issued queries.</p><p>• User profile constructed from the search log. Most personalized search models build user profiles to represent user interests. Different formats of user profiles are used in existing works. Typical profiles include term, topic, click distributions <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b42">43]</ref>, sequential search behavior representation vectors <ref type="bibr" target="#b20">[21]</ref>, and personal word embeddings <ref type="bibr" target="#b46">[47]</ref>. User profiles are usually aggregated vectorized representations of user behaviors, so they contain less private information than original query logs, but are still privacy-sensitive.</p><p>• A personalized ranking model that calculates the personalized score for candidate documents based on the query and user profile. Parameters of the model mainly reflect the personalized ranking strategy with extracted features or representation vectors as the input. Thus, the model contains little user privacy.</p><p>• Other auxiliary data used to help ranking, such as the shared word embeddings <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b49">50]</ref>. The association between user privacy and these data is determined by the specific model.</p><p>According to the analysis above, the user's original query log contains the most private information, followed by the user profile. Stated in Section 1, to protect privacy, we are not supposed to collect search logs and build user profiles on a remote server. Currently, users access the search engine through their own client devices with certain computing, storage and communication capabilities, denoted as 𝐶 1 , 𝐶 2 , . . . , 𝐶 𝑁 . Thus, we store the user's search history 𝐻 , build the user profile 𝑃 and personalize the general search results with a personalized ranking model on the client. To train the personalized ranking model, the safest method is to train a personal model on each user's client with local data. However, training a reliable neural model usually depends on a lot of samples, and the data of a single user is not enough. Therefore, we apply federated learning to personalized search to train a shared high-quality personalization model with rich data distributed on all users' clients.</p><p>In the following, we will describe our proposed framework FedPS and two different implementations within this framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">FedPS --The Framework</head><p>The FedPS framework is shown in Figure <ref type="figure" target="#fig_0">1</ref>, and the process is: the user enters a query on her own client; the client sends the issued query accompanied with several cover queries to the search engine to obtain the relevant documents; then the personalized ranking model deployed on the client adjusts the document list of the real query and presents the personalized result to the user. After the user gives feedback on the result, the issued query, general document list, personalized document list and the user's clicks are recorded in the local query log. During the whole process, the user's complete query log is only stored on the client.</p><p>We adopt the query obfuscation method <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b48">49]</ref> to hide the user's genuine query intent among a set of generated unrelated cover queries on the server side. The number of cover queries should not be too large which will otherwise affect the server's response speed. We first train an LDA (Latent Dirichlet Allocation) <ref type="bibr" target="#b9">[10]</ref> topic model on the whole document set to infer the topic proportion of the issued query, which represents the user's real search intent. Then, cover queries are generated on different topics with similar entropy on account of plausibility. The specific method is not the key problem of this paper, and more details can be found in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b48">49]</ref>. Different from recording cover queries into the search log on the server, which will distort the user interest profile and have negative impacts on personalization, we just add cover queries when submitting the query to the search engine. After all the search results are returned to the client, we filter those fake queries and only record the real query into the local log. Therefore, the user profile in our framework will not be influenced by cover queries.</p><p>All clients cooperate to train the personalized ranking model, and a central server is responsible for controlling the whole collaboration process. We optimize the personalized ranking model referring to FedAvg <ref type="bibr" target="#b27">[28]</ref>. FedAvg is one of the widely used distributed optimization algorithms in federated learning. Suppose that a certain amount of log data has been stored on each client, and there is a randomly initialized personalized ranking model on the server. The model here can be any learning based personalization model. At this point, all the 𝑁 clients begin to communicate with the server to train the model for a total of 𝑅 rounds. In each round, we ensure that every client updates the model once, and complete the model training on all clients in 𝑁 𝐾 steps. The operations in each step 𝑡 are: First, the server samples 𝐾 clients and sends the latest personalized ranking model 𝑀 𝑡 to them. Second, each of the selected clients receives the latest model 𝑀 𝑡 from the server, and updates the model for 𝐸 epochs with the training samples 𝐷 and user profile 𝑃 constructed on the local log data 𝐻 . Mini-batch stochastic gradient descent (SGD) optimization algorithm is applied. Then, all the selected clients send the parameter updates of the personalized ranking model back to the server, with the history 𝐻 , training samples 𝐷 and user profile 𝑃 kept locally. Third, the server aggregates the parameter updates from all the selected clients, and applies the aggregation to update the current model 𝑀 𝑡 to a new one 𝑀 𝑡 +1 . During the above joint model training process, some clients may have limited and unreliable communication because mobile devices are frequently offline or in poorly connected environments. Furthermore, the available computing resources on some clients may be not enough to complete the model training task. FedPS trains the shared personalization model in a synchronous way. To prevent the server from waiting too long for the clients with a poor connection or computation, we set maximum response time 𝑇 𝑚𝑎𝑥 , and ignore the model updates of those clients without response during this period. The whole federated training process is described in Algorithm 1.</p><p>Note that if the parameter 𝐾 = 1, the federated training algorithm degrades to a stream method. All available clients are accessed one by one to update the model trained by the last client. Due to the model is updated by only one user at each time, the parameter updates may expose the user's data features and privacy to subsequent users. Therefore, we don't consider the case of 𝐾 = 1, and select more than one client in each step. If 𝐾 = 𝑁 , the algorithm is similar to full-batch gradient descent, where we are required to consider all clients and spend a lot of time for calculation and communication in every step. To balance effects and efficiency, we usually set 𝐾 as an appropriate value between 1 and 𝑁 . We also conduct experiments to evaluate different values of 𝐾 in Section 5.2.</p><p>After the 𝑅 rounds of model training with the existing log data on all clients are completed, the server broadcasts the trained personalization model to clients for subsequent use. Users will continuously perform search on their own devices and generate new query logs. Thus, we can further update the personalized ranking model with new data. Both the general document lists returned by the search engine and locally personalized results are recorded on the client. In order to remove the impact of the previous personalized results We consider two ways to update the model. The first is an online training method that the client sends an update application to the server if the amount of newly generated data is enough. Then, the client updates the current global model with new data and uploads the parameter updates to the server. After the server receives parameter updates from 𝐾 clients, it aggregates all these updates to generate a new model and distributes the new model to all clients. However, there are numerous users; if each client communicates with the server to update the current model once some new data is generated, a relatively high communication cost will be required and the model performance may also be unstable. In addition, the frequency of searching and the amount of newly generated data are very unbalanced among users. Some users frequently issue new queries and their corresponding clients incrementally update the personalized ranking model will make the model biased towards these active users and the overall performance will be worse. On account of these problems, we propose a more feasible model updating method that reduces the communication cost and makes the model perform more stable. We set a fixed time interval to update the model (such as three days), which is determined by comprehensively considering the communication, computing resources and users' search frequency. During the time interval, all clients employ the model trained in the previous stage without updates. After a time interval, the server starts a task to jointly train a new personalized ranking model from scratch with the current log data on all clients for 𝑅 rounds, as stated in Algorithm 1. With the training completed, the server broadcasts the new model to all clients for the use of the next stage. Compared with the online incremental update, this method updates the model with data from all clients in every stage, which makes the model better adapt to the global data distribution and achieve better overall performance. To speed up the updating process and save computing resources, we can also choose to update the model of the previous stage with only the newly generated data in this stage.</p><p>So far we have described the FedPS framework. In the next sections, we implement two models to cope with some challenges, such as data heterogeneity and communication efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">FedPSFlat: the Flatten FedPS</head><p>In this implementation, we specially employ a personalized search model with a personal module to tackle the challenge of data heterogeneity in federated learning, introduced in the next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Personalized Search Model.</head><p>During search process, different users tend to click different documents under the same query. Therefore, the whole logs of all users naturally suffer from non-IID distribution. This is one of the critical challenges for standard federated learning which trains a single global model for all clients <ref type="bibr" target="#b24">[25]</ref>. Multi-task learning is used to model this statistical heterogeneity by treating the model training on every client as a separate task. Focusing on state-of-the-art personalized search model PEPS <ref type="bibr" target="#b46">[47]</ref>, it sets up a module which contains personal word embeddings for each user trained from the user's own search data to disambiguate the query keywords. In this case, the personal word embedding module can adapt to the user's personal data distribution, and it is promising to alleviate the problem of data heterogeneity like multi-task learning. We adapt PEPS as the personalized search model in this implementation, shown in Figure <ref type="figure" target="#fig_1">2</ref>. The main modules are briefly introduced as follows, and more details can be referred to <ref type="bibr" target="#b46">[47]</ref>.</p><p>Word embedding layer. There is a global word embedding matrix and a personal word embedding matrix in this layer. The global word embeddings are shared and updated with all users' data. The personal word embeddings are different for different users, which are trained with merely the corresponding user's data to contain the user interested word meanings and can be used as the interest profile. Without centrally collected query logs, we initialize the global word embeddings with the word2vec <ref type="bibr" target="#b30">[31]</ref> model trained on the document collection or the Wikipedia corpus. As for the personal word embeddings, we use the global word2vec model or that trained with the user's individual search log for initialization.</p><p>Matching &amp; Ranking. Through the word embedding layer, we are able to convert the query 𝑞 and document 𝑑 into vectors. Five kinds of text representations of different aspects are considered, including the personal and global word vectors 𝑃𝑊 𝑞 , 𝑃𝑊 𝑑 , 𝐺𝑊 𝑞 , 𝐺𝑊 𝑑 , personal and global contextual representations 𝑃𝐶𝑊 𝑞 , 𝑃𝐶𝑊 𝑑 , 𝐺𝐶𝑊 𝑞 , 𝐺𝐶𝑊 𝑑 to clarify the keywords by modeling the interactions between contexts with multi-head self-attention <ref type="bibr" target="#b40">[41]</ref>, and personalized query representation 𝑃𝑄 𝑞 .</p><p>With these text representations, we calculate the personalized scores and re-rank the candidate documents. As for the four types of word representations, we use the neural matching component KNRM <ref type="bibr" target="#b45">[46]</ref> to compute the interactive matching scores, i.e. 𝐹 𝑃𝑊 , 𝐹 𝑃𝐶𝑊 , 𝐹 𝐺𝑊 , 𝐹 𝐺𝐶𝑊 . For the query representation 𝑃𝑄 𝑞 , the cosine similarity with the document is calculated as 𝐹 𝑃𝑄 . In addition, a series of click and topic features are fed into an MLP to get a relevance score 𝐹 𝑟 . Finally, all the scores are combined through an MLP layer to compute the personalized score of the document as: 𝐹 (𝑞, 𝑑) = MLP(𝐹 𝑃𝑊 , 𝐹 𝑃𝐶𝑊 , 𝐹 𝐺𝑊 , 𝐹 𝐺𝐶𝑊 , 𝐹 𝑃𝑄 , 𝐹 𝑟 ).</p><p>(</p><formula xml:id="formula_0">)<label>1</label></formula><p>We use the pairwise learning-to-rank algorithm LambdaRank <ref type="bibr" target="#b11">[12]</ref> to train the model. For each client, document pairs are created on the local query log.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Model</head><p>Training. We train the personalized search model with Algorithm 1. Considering user privacy, we have to make a detailed discussion about the parameters to be exchanged. Generally, all the parameters of the above model can be divided into personal word embeddings, global word embeddings and parameters of the ranking module. The personal word embeddings are updated with the user's individual data and used as the user profile which contain a wealth of user privacy. Thus, we should keep this part of parameters on the client. Parameters of the personalized ranking model contain the least privacy. We can upload these parameters to the server for aggregation to obtain a more reliable personalized ranking model. In addition, the global word embeddings are knowledge shared by all users, but the updates of this module could also reflect the word distribution of the user's query log. Therefore, whether and how to upload the global word embeddings depends on the requirement of privacy protection. We consider the following three cases. Case1: Ignore the small amount of user privacy that may be contained in the updates of global word embeddings and upload the complete parameter updates.</p><p>Case2: Taking into account that some words with low frequency or specific information may leak personal privacy or interests, we sample some words that appear frequently in the document collection, and only upload the updates of these words.</p><p>Case3: In order to strictly avoid user privacy leakage, we do not upload parameter updates of global word embeddings, and keep the pre-trained global word embeddings fixed on the server side.</p><p>Although the whole global word embeddings have a large number of parameters, we only upload the embedding updates, which usually focus on a few terms and will not cause too much communication pressure and bandwidth expense.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">FedPSProxy: Hierarchical FedPS with Proxy</head><p>In FedPSFlat, all clients communicate with the central server to jointly train the personalized ranking model. This setting mitigates the privacy risks associated with centrally collecting data from each device, but there are still some problems. (1) The only central server may become the performance bottleneck in the training process. In actual application, there is a large number of clients. Communicating and exchanging data with all clients will take the server a lot of time and computing resources. A failure of the central server will interrupt all communications in the current training step.</p><p>(2) There exist potential risks of privacy attacks. After a client updates the model with local data, the parameter updates can reflect some information about the user's data. Directly uploading the updates to the server will provide the malicious server with an opportunity for privacy attack. Thus, it is necessary to mask the contained private information or break the link between the user and the corresponding parameter updates. Currently, there are several defense techniques to cope with privacy leakage in federated learning, including Multi-Party Computation (MPC) <ref type="bibr" target="#b19">[20]</ref>, Homomorphic Encryption (HE) <ref type="bibr" target="#b31">[32]</ref> and Differential Privacy (DP) <ref type="bibr" target="#b28">[29]</ref>. But encryption methods increase communication and computation cost, which grows quadratically with the number of clients especially costs for the central server. And DP leads to a loss of the model accuracy.(3) Some client devices with limited communication, computing or storage abilities may slow down the entire training process, because FedPS trains the global model in a synchronous way. In Section 3.2, we claim to set maximum response time and discard the model updates of those stragglers, but this loses training data and may reduce search accuracy.</p><p>We attempt to address the above issues by introducing proxies and group servers to improve the flatten FedPS where all clients communicate with the central server into a hierarchical structure. Figure <ref type="figure" target="#fig_2">3</ref> presents FedPSProxy, a four-layer implementation of FedPS. We can build more layers based on the number of users and privacy requirements. The central server sets a series of group servers to relieve the communication pressure. Each group server is responsible for communicating with a part of clients. And this structure can dynamically adapt to large-scale users scenario by increasing group servers. In addition, the clients can not directly access the server but through a proxy for privacy protection. We are supposed to ensure that the proxies are trustworthy and reliable. Referring to edge computation <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b50">51]</ref>, we set proxies as the edge gateway at home (the safest), edge server of a work organization and so on. Under this layered model, there are some changes in the operations of the server and clients. The specific changes and the benefits they can bring to eliminate the above problems are as follows:</p><p>Communicating with server/clients. In FedPSProxy, the central server cannot directly connect to the clients. When a client wants to submit an application of training or updating the model to the server, it first sends the message to the corresponding proxy. Then, the proxy aggregates all applications from clients in its scope and sends a message to the group server, which indicates how many clients made a request. The central server receives information about how many devices are connected from group servers, and samples a batch of available clients. Each group server broadcasts the latest global model to the selected proxies connected to it. Then, each proxy sends the model to the selected clients within its scope. Compared with client devices, the number of proxies is much smaller. Furthermore, there are a series of group servers responsible for communication, instead of a single central server. Thus, the FedPSProxy model solves the communication bottleneck the central server and improves efficiency since multiple group servers and proxies can work in parallel.</p><p>Uploading the model updates. When a client uploads the parameter updates to the server, it first sends the updates to the proxy, then the proxy aggregates the updates from all selected clients within its scope and uploads the aggregation to the group server. Finally, the central server combines parameter updates on all group servers. This method breaks the link between the model updates and the user on the server side, avoiding user privacy leakage. For example, a single user's updates of the global word embeddings can be used to infer the words that frequently appear in the user's query log, but the aggregated word embedding updates of multiple users within a proxy are not easy to expose the data information of a single user. In addition, if users require stronger privacy protection and do not expect to expose parameter updates to the proxy, it is more feasible and efficient to apply encryption algorithms such as MPC among small-scale users under the proxy, which can save a lot of communication and computation costs.</p><p>Uploading issued queries. Stated in Section 3.2, when the client submits a issued query to the server to obtain non-personalized search results, we apply the query obfuscation method to mask the user's real query intents. In FedPSProxy, the clients under a proxy upload their issued queries to the server through the same proxy, which hides each single user in a group of users and protects the user privacy. In such case, there is no necessity to upload additional cover queries, promoting the response speed and reducing bandwidth expense.</p><p>Offloading computing task. In our FedPSProxy model, the proxies are generally trustworthy and reliable in communication (e.g., edge server of the company). Thus, the client devices with weak communication, storage or computing abilities can offload their tasks composed of the local query log, user profile and personalized ranking model to the proxies. The proxy with stronger computing power will help train the model and generate personalized search results. If the user is still worried about the security of proxies, they can also choose to keep their original log data and the input layers on the client, and offload the remaining model layers and calculation to the proxy. In general, this method solves For the clients with limited capabilities and resources, this method also relieves their burden of communication and model calculation.</p><p>To conclude, FedPSProxy addresses the following problems: the performance bottleneck of the central server in large-scale user scenario, the limited connection and computation of clients, and the privacy attack of the flatten model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL SETTINGS 4.1 Dataset and Evaluation Metrics</head><p>Lacking the condition to conduct experiments with real client devices, we employ two widely used non-personalized query logs for simulation. Statistics of the two datasets are shown in Table <ref type="table" target="#tab_1">1</ref>.</p><p>AOL Dataset is a public three-month log <ref type="bibr" target="#b33">[34]</ref>. Each record is identified by an anonymous user id based on which we divide all the data into query logs of different users. We follow <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> to process the original AOL log. First, the query sequences are split into sessions with borders decided by the difference between two consecutive queries. Then, we cut the whole log into the background set and experimental set. The background set contains the first five weeks log, used to build interest profiles. The last eight weeks experimental data is separated into the training, validation and testing set with the proportion 6:1:1. In the AOL set, only the URLs of clicked documents are recorded, based on we crawled the title of the corresponding documents. Then, we use the same method as <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> to rank all documents with BM25 <ref type="bibr" target="#b34">[35]</ref> and sample the top documents as the candidates. 50 documents are selected for each test query, while 5 candidates per training/validation query to speed up training. To ensure the validity of personalization, we filter users without enough background set or training set.</p><p>Commercial dataset includes query logs in Jan. and Feb. 2013. Each query corresponds to a complete document list and the information of click behaviors. Following <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b25">26]</ref>, we view the documents with more than 30 seconds of dwelling time as satisfied clicked documents. With 30 minutes of inactivity as the interval, we separate the query sequences into sessions. Then, the log of the first six weeks is used as historical data to mine user interests; the remaining two weeks data is split into training, validation and testing set with 4:1:1 ratio.</p><p>Evaluation Metrics With the clicked documents as relevant and other candidate documents as irrelevant <ref type="bibr" target="#b47">[48]</ref>, we choose three common metrics to evaluate the ranking results: MAP, MRR and P@1. A more reliable metric for personalized search P-Improve is also employed. Following <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b49">50]</ref>, we only use P-Improve on the commercial dataset whose recorded document lists were really presented to users. In addition to ranking quality, we also define a metric to measure the privacy protection capability of models, which is the Kullback-Leibler divergence between the real user profile and the observed profile on the server <ref type="bibr" target="#b3">[4]</ref>. The larger the KL divergence, the less privacy is disclosed. Since user interests are mainly reflected in the issued queries and clicked documents, we use the distribution of all terms in the issued queries and clicked documents as the user profile.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines and Our Models</head><p>The original rankings of the AOL set are generated by BM25 <ref type="bibr" target="#b34">[35]</ref> and that of the commercial set is returned by the search engine. Besides, we select other baselines, including neural ranking models, personalized search models and privacy enhanced models. KNRM <ref type="bibr" target="#b45">[46]</ref>: It is a neural model using kernels to extract features from interactions between the query and document for ranking.</p><p>HRNN <ref type="bibr" target="#b20">[21]</ref>: This work employed a hierarchical RNN with queryaware attention to build the short-term and long-term user profiles.</p><p>HTPS <ref type="bibr" target="#b49">[50]</ref>: Inspired by BERT, Zhou et al. <ref type="bibr" target="#b49">[50]</ref> proposed to encode historical queries as the context with the transformer <ref type="bibr" target="#b40">[41]</ref> to enhance the representation of the current query for disambiguation.</p><p>PEPS <ref type="bibr" target="#b46">[47]</ref>: It trained personal word embeddings for each user to clarify the personalized meaning of some keywords. We turn off its query reformulation module in our implementation.</p><p>GroupUser <ref type="bibr" target="#b51">[52]</ref>: A group of users share a single identity and their search logs are recorded together to build user profile at the group level, hiding the privacy information of each single user.</p><p>CoverQuery <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4]</ref>: This approach first applies a topic model to infer the user's query intent, then generates several noisy queries from unrelated topics to conceal the user's real intentions.</p><p>The two privacy protection baselines are both applied to PEPS. FedPSFlat and FedPSProxy are our proposed models.</p><p>As for the personalized search model, we train a 100-dim word2vec model on the document set to initialize the global and personal word embeddings. Other hyper-parameters are set the same as PEPS <ref type="bibr" target="#b46">[47]</ref>. We adopt Adam optimizer to train the personalization model on clients. In each update step, we sample 10 clients to update the model for 1 epoch. The models are trained on the whole training set and evaluated on the test set. We assume 10% of the clients are in poor communication during training. We define 100 users as a group in the GroupUser approach. And we separate 100 users under each proxy, 1,000 proxies under each group server in FedPSProxy. One query is randomly generated as the cover query. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTAL RESULTS AND ANALYSIS 5.1 Overall Performance</head><p>The overall results, including the privacy protection effect and ranking quality, are shown in Table <ref type="table" target="#tab_2">2</ref> and Table <ref type="table" target="#tab_3">3</ref>. We can observe:</p><p>(1) The FedPS framework not only protects the user privacy best, but also achieves better personalization results than other privacy enhanced personalized models. In Table <ref type="table" target="#tab_2">2</ref>, FedPS models get the best result of KL-Divergence which is used to measure privacy protection capability. The GroupUser and Cov-erQuery use some noise to hide the user's true identity and query intentions but still collect user search logs on the server side. However, under our framework, the user's original log data is guaranteed not to be exposed at all so user privacy is better protected. Besides, FedPSProxy outperforms both the GroupUser and Cover-Query methods on all ranking metrics on the two datasets. Due to noise is added to the user's search log in the two baselines but FedPS exploits the real query log on the client side to analyze the user's interests, FedPS performs more accurate personalization. We find that the CoverQuery method also has no significant loss of the ranking results though some fake queries are added to the log. We deduce it may because that the fake queries increase training data of the model, thereby reducing the impacts of vague user profiles.</p><p>(2) The FedPS framework is adaptive to various learningbased personalized search models and attains comparable results to the original models trained from the centrally stored data. We apply the FedPS framework to several state-of-the-art personalization models, including HRNN, HTPS and PEPS, and compare them with the original models. As shown in Table <ref type="table" target="#tab_3">3</ref>, there is no much difference between the results of them and the original models on all ranking metrics, confirmed by paired t-test at 𝑝 &lt; 0.05 level. This proves that our privacy protection framework will not cause much loss to ranking quality.</p><p>(3) FedPSProxy outperforms FedPSFlat. In this experiment, we set 10% of the clients in poor connection or offline during the training process. For FedPSProxy, these stragglers can transfer their computing tasks to the corresponding proxy which helps complete model training. But FedPSFlat ignores these clients as well as the To summarize, our FedPS framework indeed has the ability to protect user privacy without impacting the model accuracy too much, and it greatly adapts to various learning based personalized search models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Study of Different Parameters</head><p>In Algorithm 1, there are two main parameters to be determined, i.e. the number of sampled clients and the epoch the model is trained locally in each step. We experiment with different parameters and illustrate the results in Table <ref type="table" target="#tab_4">4</ref>   This study shows different hyper-parameters have various effects on the model accuracy, time and computation resources. We should take these factors into account to select appropriate parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Effect of Proxy and Group Server</head><p>In order to test whether FedPSProxy has the ability to reduce the performance burden of the central server and promote the federated training efficiency, we compare the required communication time to jointly train the personalization model for a round with FedPSFlat and FedPSProxy. We focus on the time spent for broadcasting the current global model and uploading the parameter updates, both of which are assumed equal to 𝑇 . We consider 118,065 users in the AOL log. FedPSFlat samples 𝐾 clients in each training step, while FedPSProxy first samples 𝐾/10 proxies, then samples 10 clients in the scope of each proxy. Moreover, we discuss that 10% or 20% of the devices have a poor connection and suffer a delay 𝑑 in each step. In FedPSProxy, such stragglers offload their computing tasks to the proxies so we ignore the time cost of data exchange between these clients and the corresponding proxy. The comparison of communication time is shown in Table <ref type="table" target="#tab_7">6</ref>  From Table <ref type="table" target="#tab_7">6</ref>, we see that the training process of FedPSFlat can be easily slowed down by the devices with limited connection, costing much additional delay time. In contrast, FedPSProxy is almost not affected by these errors, showing robust performance and higher efficiency. The value before 𝑇 is used to measure the communication time of data exchange among the server, group servers, proxies and clients. FedPSProxy greatly reduces the communication time of the central server compared to FedPSFlat, and the reduction is greater as the number of sampled clients 𝐾 gets larger, significantly relieving the communication pressure of the server.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Simulation of Continuous Model Updating</head><p>In the previous experiments, the personalization model is trained on the whole training set, without subsequent updates. In this section, we use the data of AOL from 3 𝑟𝑑 Apr to 31 𝑠𝑡 May to simulate the continuous model updating process described in our FedPS framework. We cut all log into 8 stages with one week as a stage. First, the personalized search model is initialized with the first week data. Then, after each week, we apply the currently available logs to retrain the model from scratch, and evaluate the trained model on the data of the next stage. We consider 10% of the clients are offline or in poor connection. The improvement on MAP of PEPS and our two models over the original ranking are displayed in Figure <ref type="figure" target="#fig_4">4</ref>.</p><p>Generally, the results of all models improve as the data increases with stages. FedPSProxy performs inferior to PEPS trained with the centrally collected data in early stages. It may due to the training data on each client is very limited in early stages, and using the limited local data to update the model could cause the model to overfit to the individual data. In later stages with enough local data, FedPSProxy almost achieves similar results as PEPS. FedPSFlat performs a little worse than FedPSProxy, because it ignores the offline clients during training while computing tasks of these stragglers are transferred to the proxies in FedPSProxy. As the data on each client increases, the effect differences between the two FedPS models caused by discarding training data of the offline clients get larger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this study, we focus on the privacy protection issue in personalized search, and propose a privacy protection enhanced framework FedPS for learning based personalized search models. It stores the user's privacy-sensitive query log and interest profile on the client, and employs federated learning to jointly train a shared personalized ranking model with all clients and their decentralized data. Within this framework, we design two implementations, FedPS-Flat and FedPSProxy. FedPSFlat eliminates the challenge of data heterogeneity. The second model improves FedPSFlat by introducing proxies and group servers to promote privacy protection and relieve the performance pressure caused by large-scale clients. Experimental results confirm that our framework protects user privacy without affecting the model accuracy too much. In the future, we will explore techniques for stronger user privacy protection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The FedPS framework. The blue lines illustrate the data stream of the use process, and the black lines show the data stream of the training process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Structure of the personalized search model employed in FedPSFlat.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Architecture of the four-layer FedPSProxy model. The blue lines show the data stream of the use process, and the black lines show the data stream of the training process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Results of continuous model updating simulation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 1 Federated Training of Personalization Model 𝐵 is the local batch size, 𝐸 is the local model update epoch, and 𝜂 is the learning rate. Randomly initialize the personalized ranking model 𝑀 1 for each round 𝑟 = 1, 2, . . . , 𝑅 do for each step in 𝑡 = 1, 2, . . . , 𝑁 𝐾 do Server samples 𝐾 different clients Wait updates from each of the 𝐾 selected clients for 𝑇 𝑚𝑎𝑥 for each client 𝑗 ∈ {1, 2, . . . , 𝐾 } in parallel do Receive 𝑀 𝑡 from the central server Construct training samples 𝐷 and user profile 𝑃 on 𝐻 𝑀 𝑗 𝑡 +1 ← 𝑆𝐺𝐷 (𝑀 𝑡 , 𝐷, 𝑃, 𝐵, 𝐸, 𝜂), 𝑛 𝑗 = ∥𝐷 ∥ Send parameter updates Δ𝑀 Broadcast the trained model 𝑀 to all clients on the subsequent updating of the personalized ranking model, we always utilize the non-personalized data.</figDesc><table><row><cell></cell><cell>𝑗 𝑡 +1 to the central server</cell></row><row><cell>end for</cell><cell></cell></row><row><cell>𝑀 𝑡 +1 = 𝑀 𝑡 + 𝐾 𝑗=1</cell><cell>𝑛 𝑗 𝑛 (Δ𝑀 𝑡 +1 ), 𝑛 = 𝐾 𝑗 𝑗=1 𝑛 𝑗 (Δ𝑀 𝑡 +1 of 𝑗</cell></row><row><cell cols="2">clients exceeding 𝑇 𝑚𝑎𝑥 are NULL.)</cell></row><row><cell>end for</cell><cell></cell></row><row><cell>end for</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the experimental datasets.</figDesc><table><row><cell>Type</cell><cell>AOL</cell><cell>Commercial</cell></row><row><cell>Time Span</cell><cell cols="2">Mar−May 2006 Jan−Feb 2013</cell></row><row><cell>User Num</cell><cell>118,067</cell><cell>5,998</cell></row><row><cell>Query Num</cell><cell>3,461,637</cell><cell>738,731</cell></row><row><cell>Session Num</cell><cell>391,893</cell><cell>275,910</cell></row><row><cell>Avg/Max Query Per User</cell><cell>29.32 (1,449)</cell><cell>138.93 (3,207)</cell></row><row><cell cols="2">Avg/Max Query Per User-Day 2.043 (358)</cell><cell>5.713 (153)</cell></row><row><cell cols="3">the delay and data loss in the federated training process caused by</cell></row><row><cell cols="3">stragglers, making the FedPS framework more feasible and effective.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>KL-Divergence of different privacy protection techniques. The best results are shown in bold.</figDesc><table><row><cell>Model</cell><cell cols="2">AOL Dataset KL-Divergence</cell><cell cols="2">Commercial Dataset KL-Divergence</cell></row><row><cell>GroupUser</cell><cell>2.715</cell><cell>-61.72%</cell><cell>2.854</cell><cell>-65.84%</cell></row><row><cell cols="2">CoverQuery 5.071</cell><cell>-28.50%</cell><cell>1.563</cell><cell>-81.29%</cell></row><row><cell>FedPSFlat</cell><cell>7.092</cell><cell>-</cell><cell>8.356</cell><cell>-</cell></row><row><cell>FedPSProxy</cell><cell>5.403</cell><cell>-23.82%</cell><cell>9.073</cell><cell>8.58%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Overall performance of models. The percentages are computed based on the corresponding original model. † means no significant differences from the original model with paired t-test at p&lt;0.05 level. Results of the best privacy enhanced model are shown in bold. -0.20% .7232 † -0.19% .6252 † -0.22% .8201 † -0.06% .8299 † -0.07% .7151 † -0.07% .2528 † -0.20% FedPSFlat .7074 † -0.58% .7205 † -0.57% .6229 † -0.59% .8195 † -0.13% .8294 † -0.13% .7247 † -0.12% .2525 † -0.32% FedPSProxy .7112 † -0.04% .7242 † -0.06% .6262 † -0.06% .8204 † -0.02% .8304 † -0.01% .7254 † -0.03% .2532 † -0.04%</figDesc><table><row><cell>Model</cell><cell>MAP</cell><cell>AOL Dataset MRR</cell><cell>P@1</cell><cell>MAP</cell><cell cols="2">Commercial Dataset MRR P@1</cell><cell></cell><cell>P-Imp.</cell></row><row><cell>Original Rank</cell><cell cols="4">.2504 -64.8% .2596 -64.2% .1534 -75.5% .7399 -9.8%</cell><cell>.7506 -9.6%</cell><cell cols="2">.6162 -15.1% -</cell><cell>-</cell></row><row><cell>KNRM</cell><cell cols="8">.4291 -39.7% .4391 -39.4% .2704 -56.9% .4916 -40.1% .5001 -39.8% .2849 -60.7% .0655 -74.1%</cell></row><row><cell>HRNN</cell><cell>.5423 -</cell><cell>.5545 -</cell><cell>.4854 -</cell><cell>.8065 -</cell><cell>.8191 -</cell><cell>.7127 -</cell><cell cols="2">.2404 -</cell></row><row><cell cols="9">FedPS &amp; HRNN .5419  † -0.07% .5541  † -0.07% .4852  † -0.04% .8061  † -0.05% .8186  † -0.06% .7125  † -0.03% .2402  † -0.08%</cell></row><row><cell>HTPS</cell><cell>.7091 -</cell><cell>.7231 -</cell><cell>.6272 -</cell><cell>.8219 -</cell><cell>.8315 -</cell><cell>.7287 -</cell><cell cols="2">.2548 -</cell></row><row><cell>FedPS &amp; HTPS</cell><cell cols="8">.7085  † -0.08% .7226  † -0.07% .6268  † -0.06% .8213  † -0.07% .8309  † -0.07% .7282  † -0.07% .2545  † -0.12%</cell></row><row><cell>PEPS</cell><cell>.7115 -</cell><cell>.7246 -</cell><cell>.6266 -</cell><cell>.8206 -</cell><cell>.8305 -</cell><cell>.7256 -</cell><cell cols="2">.2533 -</cell></row><row><cell>GroupUser</cell><cell cols="8">.6917 -2.78% .7037 -2.88% .6106 -2.55% .8145 -0.74% .8248 -0.69% .7204 -0.72% .2411 -4.8%</cell></row><row><cell>CoverQuery</cell><cell>.7101  †</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>The performance of FedPSProxy with different 𝐾. 𝐾 is the number of clients sampled in each step. 'Rounds' is the total rounds of model training until convergence. 'Steps' is the total number of model update steps.</figDesc><table><row><cell>𝐾</cell><cell></cell><cell>Rounds (𝑅)</cell><cell cols="2">Steps ( 𝑁 𝐾  *  𝑅)</cell><cell>Best MAP</cell></row><row><cell>5</cell><cell>3</cell><cell>1x</cell><cell cols="2">15762 1x</cell><cell>0.7112</cell></row><row><cell>10</cell><cell>3</cell><cell>1x</cell><cell>7881</cell><cell>0.5x</cell><cell>0.7112</cell></row><row><cell>20</cell><cell>4</cell><cell>1.3x</cell><cell>5254</cell><cell>0.33x</cell><cell>0.7112</cell></row><row><cell>30</cell><cell>6</cell><cell>2x</cell><cell>5254</cell><cell>0.33x</cell><cell>0.7079</cell></row><row><cell>100</cell><cell>8</cell><cell>2.7x</cell><cell>2101</cell><cell>0.13x</cell><cell>0.6997</cell></row><row><cell cols="6">training data on them, thus affecting the model performance. In</cell></row><row><cell cols="6">actual application, there are indeed a certain percent of clients</cell></row><row><cell cols="6">with poor communication ability, so FedPSProxy with some error</cell></row><row><cell cols="6">tolerance shows better applicability and feasibility.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>and Table5. 'Rounds' is the total rounds of model training until convergence, used to measure the communication and computation cost. 'Steps' represents the total number of model update steps. In each step, the selected clients work in parallel, so 'Steps' can be a measure of wall-clock time cost.Number of sampled clients 𝐾. In each joint training step, we sample more than one clients to update the current model with their own data locally, then merge their updates to generate a new global model, which avoids exposing the parameter updates of a single user to other users. Presented in Table4, we set 𝐾 as 5,10,20,30 and 100. As 𝐾 increases, we observe that the total number of training</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>The performance of FedPSProxy with different 𝐸. 𝐸 represents the epoch of local model training in each step.</figDesc><table><row><cell>𝐸</cell><cell></cell><cell>Rounds (𝑅)</cell><cell cols="2">Steps ( 𝑁 𝐾  *  𝑅)</cell><cell>Best MAP</cell></row><row><cell>1</cell><cell>4</cell><cell>1x</cell><cell cols="2">10508 1x</cell><cell>0.7112</cell></row><row><cell>2</cell><cell>3</cell><cell>0.75x</cell><cell>7881</cell><cell>0.75x</cell><cell>0.7091</cell></row><row><cell>3</cell><cell>3</cell><cell>0.75x</cell><cell>7881</cell><cell>0.75x</cell><cell>0.7088</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Communication time for joint model training in a round by different FedPS models. 𝑇 represents the time to exchange model parameters, and 𝑑 is the delay time due to poor communication. 𝐾 makes the model converge slowly and requires more rounds of communication and model training. Moreover, the model with smaller 𝐾 tends to achieve greater results. The possible reason is that the gradients generated by various clients with non-i.i.d data might be quite different from each other and it would be less effective to update the model by simply aggregating these gradients, causing the model turn into worse performance.Local epoch 𝐸. For each selected client, it can update the model with local data for 𝐸 epochs. We conduct experiments with 𝐸 = 1, 2, 3 respectively. 𝐾 is set as 10. Observing the results in Table5, we find developing local update on each client for more than one epoch could reduce the number of training rounds and steps, saving communication and wall-clock time costs. But there are some losses on the model's effect. We infer it may due to that more local training epochs in each step cause the model to overfit to the limited local data, thus decreasing the global performance of the model.</figDesc><table><row><cell>Settings</cell><cell cols="4">delay ratio=10% 𝐾 = 100 𝐾 = 1000 𝐾 = 100 delay ratio=20% 𝐾 = 1000</cell></row><row><cell>FedPSFlat</cell><cell cols="2">236134𝑇 + 11551𝑑</cell><cell cols="2">236134𝑇 + 23557𝑑</cell></row><row><cell cols="2">FedPSProxy 47240𝑇</cell><cell>6902𝑇</cell><cell>44878𝑇</cell><cell>6664𝑇</cell></row><row><cell cols="5">steps decreases, saving wall-clock time. However, a larger value</cell></row><row><cell>of</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>Zhicheng Dou is the corresponding author. This work was supported by National Natural Science Foundation of China No. 61872370 and No. 61832017, Beijing Outstanding Young Scientist Program NO. BJJWZYJH012019100020098, and Shandong Provincial Natural Science Foundation under Grant ZR2019ZD06.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Intent-aware Query Obfuscation for Privacy Protection in Personalized Web Search</title>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Wasi Uddin Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongning</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR 2018</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="285" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Multi-Task Learning for Document Ranking and Query Suggestion</title>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Wasi Uddin Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongning</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Context Attentive Document Ranking and Query Suggestion</title>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Wasi Uddin Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongning</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR 2019</title>
				<meeting>SIGIR 2019</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Topic Model based Privacy Protection in Personalized Web Search</title>
		<author>
			<persName><forename type="first">Md</forename><forename type="middle">Masudur</forename><surname>Wasi Uddin Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongning</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR 2016</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A Face is exposed for AOL searcher no. 4417749</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Barbaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Zeller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
			<pubPlace>New York Times</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning from User Interactions in Personal Search via Attribute Parameterization</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bendersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Najork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM 2017</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="791" to="799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Inferring and using location metadata to personalize web search</title>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">N</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filip</forename><surname>Radlinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryen</forename><forename type="middle">W</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emine</forename><surname>Yilmaz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Classificationenhanced ranking</title>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">N</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krysta</forename><forename type="middle">Marie</forename><surname>Svore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings WWW 2010</title>
				<meeting>WWW 2010</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="111" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Modeling the impact of short-and long-term behavior on search personalization</title>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">N</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryen</forename><forename type="middle">W</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fedor</forename><surname>Borisyuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyuan</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR &apos;12</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="185" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Latent Dirichlet Allocation</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="page" from="601" to="608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Practical Secure Aggregation for Privacy-Preserving Machine Learning</title>
		<author>
			<persName><forename type="first">Keith</forename><surname>Bonawitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Kreuter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Marcedone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Brendan Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarvar</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Segal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karn</forename><surname>Seth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CCS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Learning to rank using gradient descent</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erin</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Renshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Lazier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicole</forename><surname>Deeds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><forename type="middle">N</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><surname>Hullender</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Towards query log based personalization using topic models</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>James Carman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Crestani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Baillie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM 2010</title>
				<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Preserving user&apos;s privacy in web search engines</title>
		<author>
			<persName><forename type="first">Jordi</forename><surname>Castellà-Roca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Viejo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordi</forename><surname>Herrera-Joancomartí</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Commun</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="13" to="14" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">UPS: efficient privacy protection in personalized web search</title>
		<author>
			<persName><forename type="first">Gang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">He</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidan</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunjun</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR 2011</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="615" to="624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Personalizing web search results by reading level</title>
		<author>
			<persName><forename type="first">Kevyn</forename><surname>Collins-Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">N</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryen</forename><forename type="middle">W</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>De La Chica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM 2011</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="403" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Tor and Circumvention: Lessons Learned -(Abstract to Go with Invited Talk)</title>
		<author>
			<persName><forename type="first">Roger</forename><surname>Dingledine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CRYPTO 2011</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">6841</biblScope>
			<biblScope unit="page" from="485" to="486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">User-private information retrieval based on a peer-to-peer community</title>
		<author>
			<persName><forename type="first">Josep</forename><surname>Domingo-Ferrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Bras-Amorós</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianhong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesús</forename><forename type="middle">A</forename><surname>Manjón</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Knowl. Eng</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="1237" to="1252" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">A large-scale evaluation and analysis of personalized search strategies</title>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruihua</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A Pragmatic Introduction to Secure Multi-Party Computation</title>
		<author>
			<persName><forename type="first">David</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Rosulek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Found. Trends Priv. Secur</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="70" to="246" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Personalizing Search Results Using Hierarchical RNN with Query-aware Attention</title>
		<author>
			<persName><forename type="first">Songwei</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Building user profiles from topic models for personalised search</title>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Crestani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carman</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM&apos;13</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2309" to="2314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Privacy-Enhanced Web Personalization</title>
		<author>
			<persName><forename type="first">Alfred</forename><surname>Kobsa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Adaptive Web, Methods and Strategies of Web Personalization</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">4321</biblScope>
			<biblScope unit="page" from="628" to="670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multi-view Embedding-based Synonyms for Email Search</title>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bendersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongbo</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Najork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR 2019</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="575" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Federated Learning: Challenges, Methods, and Future Directions</title>
		<author>
			<persName><forename type="first">Tian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anit</forename><surname>Kumar Sahu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Virginia</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.1109/MSP.2020.2975749</idno>
		<ptr target="https://doi.org/10.1109/MSP.2020.2975749" />
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="50" to="60" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">PSGAN: A Minimax Game for Personalized Search with Limited and Noisy Click Data</title>
		<author>
			<persName><forename type="first">Shuqi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR 2019</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="555" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Knowledge Enhanced Personalized Search</title>
		<author>
			<persName><forename type="first">Shuqi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR 2020</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="709" to="718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Communication-Efficient Learning of Deep Networks from Decentralized Data</title>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eider</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seth</forename><surname>Hampson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blaise</forename><surname>Agüera Y Arcas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS 2017 (Proceedings of Machine Learning Research</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="1273" to="1282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning Differentially Private Recurrent Language Models</title>
		<author>
			<persName><forename type="first">H</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunal</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Exploiting Unintended Feature Leakage in Collaborative Learning</title>
		<author>
			<persName><forename type="first">Luca</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Congzheng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emiliano</forename><surname>De Cristofaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitaly</forename><surname>Shmatikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Symposium on Security and Privacy</title>
				<meeting><address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-05-19">2019. May 19-23, 2019</date>
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="page" from="691" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Efficient Estimation of Word Representations in Vector Space</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2013, Proceedings</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Privacy-Preserving Deep Learning via Additively Homomorphic Encryption</title>
		<author>
			<persName><forename type="first">Shiho</forename><surname>Moriai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="0198">2019. 2019. 198</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Embellishing Text Search Queries To Protect User Privacy</title>
		<author>
			<persName><forename type="first">Hweehwa</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuhua</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaokui</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB Endow</title>
				<meeting>VLDB Endow</meeting>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="598" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">A picture of search</title>
		<author>
			<persName><forename type="first">Greg</forename><surname>Pass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdur</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cayley</forename><surname>Torgeson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The Probabilistic Relevance Framework: BM25 and Beyond</title>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Zaragoza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="333" to="389" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Knowledge-based scheme to create privacy-preserving but semantically-related queries for web search engines</title>
		<author>
			<persName><forename type="first">David</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordi</forename><surname>Castellà-Roca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Viejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">218</biblScope>
			<biblScope unit="page" from="17" to="30" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Privacy protection in personalized search</title>
		<author>
			<persName><forename type="first">Xuehua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGIR Forum</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="4" to="17" />
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Web search personalization with ontological user profiles</title>
		<author>
			<persName><forename type="first">Ahu</forename><surname>Sieg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bamshad</forename><surname>Mobasher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><forename type="middle">D</forename><surname>Burke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Adapting deep RankNet for personalized search</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
				<imprint>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="83" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Understanding and predicting personal navigation</title>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Teevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">J</forename><surname>Liebling</surname></persName>
		</author>
		<author>
			<persName><surname>Gayathri Ravichandran</surname></persName>
		</author>
		<author>
			<persName><surname>Geetha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM 2011</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="85" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Attention is All you Need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Search Personalization with Embeddings</title>
		<author>
			<persName><forename type="first">Thanh</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dat</forename><surname>Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alistair</forename><surname>Willis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Temporal Latent Topic User Profiles for Search Personalisation</title>
		<author>
			<persName><forename type="first">Thanh</forename><forename type="middle">Tien</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alistair</forename><surname>Willis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Son</forename><surname>Ngoc Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Song</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="605" to="616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Enhancing personalized search by mining and modeling task behavior</title>
		<author>
			<persName><forename type="first">Ryen</forename><forename type="middle">W</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Hassan Awadallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongning</forename><surname>Wang</surname></persName>
		</author>
		<idno>WWW &apos;13. 1411-1420</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Personalized Federated Learning for Intelligent IoT Applications: A Cloud</title>
		<author>
			<persName><forename type="first">Qiong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiwen</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2020. 2002. 2020</date>
			<biblScope unit="page">10671</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">End-to-End Neural Ad-hoc Ranking with Kernel Pooling</title>
		<author>
			<persName><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russell</forename><surname>Power</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR 2017</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Employing Personal Word Embeddings for Personalized Search</title>
		<author>
			<persName><forename type="first">Jing</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR 2020</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1359" to="1368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">RLPer: A Reinforcement Learning Model for Personalized Search</title>
		<author>
			<persName><forename type="first">Jing</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<idno>WWW &apos;20. ACM / IW3C2</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2298" to="2308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Hide-n-Seek: An Intent-aware Privacy Protection Plugin for Personalized Web Search</title>
		<author>
			<persName><forename type="first">Puxuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wasi</forename><surname>Uddin Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongning</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR 2018</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1333" to="1336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Encoding History with Context-aware Representation Learning for Personalized Search</title>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Edge Intelligence: Paving the Last Mile of Artificial Intelligence With Edge Computing</title>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">En</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liekang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junshan</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
				<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="1738" to="1762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Anonymizing user profiles for personalized web search</title>
		<author>
			<persName><forename type="first">Yun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Verdery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW 2010</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1225" to="1226" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
