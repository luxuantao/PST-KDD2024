<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Facial Image Hallucination through Coupled-Layer Neighbor Embedding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Junjun</forename><surname>Jiang</surname></persName>
							<email>junjun0595@163.com</email>
							<affiliation key="aff0">
								<orgName type="institution">China University of Geosciences</orgName>
								<address>
									<postCode>430074</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">National Engineer-ing Research Center for Multime-dia Software</orgName>
								<orgName type="department" key="dep2">School of Computer</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<postCode>430072</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Ruimin</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">China University of Geosciences</orgName>
								<address>
									<postCode>430074</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Zhongyuan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">China University of Geosciences</orgName>
								<address>
									<postCode>430074</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhen</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">China University of Geosciences</orgName>
								<address>
									<postCode>430074</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiayi</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">China University of Geosciences</orgName>
								<address>
									<postCode>430074</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Electronic Information School</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<postCode>430072</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Facial Image Hallucination through Coupled-Layer Neighbor Embedding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">721F74C3B2FD44C6D10F38E66E27F44F</idno>
					<idno type="DOI">10.1109/TCSVT.2015.2433538</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2015.2433538, IEEE Transactions on Circuits and Systems for Video Technology This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2015.2433538, IEEE Transactions on Circuits and Systems for Video Technology</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T06:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Intelligence surveillance video</term>
					<term>image processing</term>
					<term>manifold learning</term>
					<term>super-resolution</term>
					<term>facial image hallucination</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As the facial image captured by a low-cost camera is typically very Low-Resolution (LR), blurring and noisy, traditional neighbor embedding based facial image hallucination methods from one single manifold (i.e., the LR image manifold) fail to reliably estimate the intention geometrical structure, consequently leading to a bias to the image reconstruction result. In this paper, we introduce the notion of neighbor embedding from the LR and the High-Resolution (HR) image manifolds simultaneously and propose a novel neighbor embedding model, termed the Coupled-Layer Neighbor Embedding (CLNE), for facial image hallucination. CLNE differs substantially from other neighbor embedding models in that it has two layers: the LR and the HR layers. The LR layer in this model is the local geometrical structure of the LR patch manifold, which is characterized by the reconstruction weights of the LR patches; the HR layer is the intrinsic geometry that can geometrically constrain the reconstruction weights. With this coupled constraint paradigm between the adaptation of the LR layer and the HR one, CLNE can achieve a more robust neighbor embedding through iteratively updating the LR patch reconstruction weights and the estimated HR patch. The experimental results in simulation and real conditions confirm that the proposed method outperforms the related state-of-the-art methods in both quantitative and visual comparisons.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Facial image hallucination (or face super-resolution) refers to the process by which a higher resolution enhanced face is reconstructed from one or multiple Low-Resolution (LR) ones. It benefits a number of real-world applications from image compression to face identification. Especially in the intelligence surveillance video system, an High-Resolution (HR) face image with detailed features is obviously significant to improve the system's performance. Practical facial image Fig. <ref type="figure">1</ref>. (Best viewed in colors and magnification) The neighborhood preservation rates between the degradation manifold and the original HR image manifold according to different neighbor number K. We also show the neighborhood preservation rates between the reconstructed manifold by our proposed CLNE and the original HR image manifold (please see Section IV-C for more details). Note that the number in the parentheses denotes the downsampling factor of the image degradation process. hallucination methods may make use of a single still image or a sequence of consecutive video frames with sub-pixel translation for synthesizing a higher-resolution image. In this paper, we focus on the problem of generating an HR image from one single LR frontal image of human face and inferring some details missing from the original image that cannot be achieved by simply sharpening.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Prior Work</head><p>In 2000, Baker and Kanade <ref type="bibr" target="#b0">[1]</ref> proposed a facial image hallucination method to infer the HR face from an input LR one under a Bayesian formulation. This is the pioneering work on hallucinating face image. Liu et al. <ref type="bibr" target="#b1">[2]</ref> proposed to integrate a global parametric model and a local nonparametric model for face super-resolution. In the first step, the relationship between the HR face images and their smoothed, down-sampled LR ones is learned via Principal Component Analysis (PCA) based global parametric model. In the second step, the residual between the original HR image and the reconstructed one is recompensed through a local nonparametric based on Markov Random Field (MRF).</p><p>Inspired by above works, in recent years, there has been a good deal of research into learning-based approaches for facial image hallucination. These learning-based methods all use a training set of HR and LR image pairs to build a co-occurrence model. With the learned model, one can predict the missing details of the observed input LR image through "borrowing" information from some similar examples (patches) in the training set globally <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> or locally <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. A comprehensive over-view of current advances in facial image hallucination is given in <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. In the following, we only give some representative literatures related to our work.</p><p>Global face based hallucination methods treat the input face image as a whole, and hallucinate the input LR face image with some classical data models, such as PCA <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, Kernel PCA (KPCA) <ref type="bibr" target="#b4">[5]</ref>, Locality Preserving Projections (LPP) <ref type="bibr" target="#b5">[6]</ref>, Non-negative Matrix Factorization (NMF) <ref type="bibr" target="#b6">[7]</ref> and Canonical Correlation Analysis (CCA) <ref type="bibr" target="#b7">[8]</ref>. These approaches can well capture the global face structure variations. However, they depend heavily on the training sets and fail to render the fine individual details of the input face effectively, especially when the input LR face image is very different from the training images or the training set size is small. To alleviate the above problem, decomposing a complete face image into smaller face patches has been introduced recently <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>.</p><p>Local patch based facial image hallucination methods. Inspired by Locally Linear Embedding (LLE) <ref type="bibr" target="#b20">[21]</ref> manifold learning, these approaches assume that the LR and HR patch manifolds share the same local geometrical structure. Specifically, they all directly apply the reconstructed weights obtained in the LR patch space to the corresponding HR patches. For example, Chang et al. <ref type="bibr" target="#b9">[10]</ref> proposed a Neighbor Embedding (NE) image super-resolution method to estimate an HR patch by linearly combining K candidate HR patches with the weights calculated in the LR patch space. Human face is a highly structured object, and position information plays an important role in face image reconstruction and analysis <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>. Ma et al. <ref type="bibr" target="#b10">[11]</ref> proposed a positionpatch based facial image hallucination method through solving a Least Squares Regression (LSR) problem. However, if the number of training samples is larger than the dimensions of the observed patch, the reconstruction weights are not unique. To obtain a much more stable and accurate solution, Jung et al. <ref type="bibr" target="#b11">[12]</ref> proposed to apply the sparsity prior, which has been proved to be highly effective in image restoration <ref type="bibr" target="#b23">[24]</ref>, to the LR patch representation and present a Spare Representation (SR) based facial image hallucination method. Wang et al. <ref type="bibr" target="#b24">[25]</ref> further proposed a Weighted Adaptive Sparse Regularization (WASR) method to super-resolve face image. Most recently, our previous work <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref> further improved these position-patch based facial image hallucination methods by incorporating the local geometrical constraint of manifold instead of incorporating the sparsity constraint into the patch reconstruction objective function as in <ref type="bibr" target="#b11">[12]</ref>, reaching sparsity and locality simultaneously <ref type="bibr" target="#b25">[26]</ref>. As far as we know, this Locality-constraint Representation (LcR) based facial image hallucination method <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref> obtained the best performance reported in the literatures. In short, the objective of these local patch based methods is to obtain the best patch representation in the LR patch space, and transform it to the corresponding HR patches to construct the target HR patch. They can induce good results when the LR and HR image manifolds meet the similar structure assumption.</p><p>In the recent years, some facial image hallucination approaches focused on the face recognition task have been proposed <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>. For example, Li et al. <ref type="bibr" target="#b28">[29]</ref> proposed coupled locality preserving mappings to project the LR and HR face images onto an unified feature space. In order to simultaneously recognize and super-resolve the LR faces, Hennings Yeomans et al. <ref type="bibr" target="#b29">[30]</ref> expressed the constraints between LR and HR images in a regularization formulation. Recently, Biswas et al. <ref type="bibr" target="#b30">[31]</ref> proposed to use the multidimensional scaling to match LR face images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Motivation and Contributions</head><p>However, above-mentioned facial image hallucination methods based on manifold learning can provide good results only for simple degradation process (e.g., down-sampling by a factor of 2 or 4). In many real-world applications, a variety of factors (such as under-exposure, optical blurring, defocusing, noise and so on) may lead to significant quality reduction. This will give rise to the manifold consistency assumption fails. As shown in Fig. <ref type="figure">1</ref>, the neighborhood preservation rates<ref type="foot" target="#foot_0">1</ref>  <ref type="bibr" target="#b31">[32]</ref> decrease with the increase of down-sampling factor or Poisson noise added. Therefore, the structure of the LR image manifold cannot reflect the truth especially when the degeneration process is very complex (with large down-sampling factor or heavy noise). For example, when K = 20, the neighborhood preservation rate falls sharply from 80% of "down-sample (2)" to 32% of "down-sample <ref type="bibr" target="#b7">(8)</ref>+Poisson".</p><p>Given this point, Li et al. <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b28">[29]</ref> proposed to project the original LR and HR coupled manifolds onto the common manifold (from the perspective of manifold alignment) by learning two explicit mappings. Then they performed the super-resolution reconstruction based on NE learning between the new couple of the common manifold and the original HR image manifold. To achieve an optimal alignment in the sense of statistical correlation, Huang et al. <ref type="bibr" target="#b7">[8]</ref> applied CCA to maximize the correlation between the local neighbor relationships of LR and HR images. They super-resolved the input LR image in the coherent subspace.</p><p>In order to overcome the structure mismatch between LR and HR image manifolds, in this paper we propose a Coupled-Layer Neighbor Embedding (CLNE) model to combine the geometrical structure of LR and HR image manifolds simultaneously. In CLNE, we try to preserve the geometrical structure of the LR patch manifold as well as the HR one for the reconstructed HR patch manifold. For the structure of the HR image manifold is unaffected by the degeneration process <ref type="bibr" target="#b32">[33]</ref>, we use the unaffected structure to regularize the NE based super-resolution reconstruction, leading to much more robust results. As shown in Fig. <ref type="figure" target="#fig_0">2</ref>, our model can be updated by re-estimating the weights and the K-Nearest Neighbor (K-NN) from the current image estimation. For the incorporation of the original HR patch geometry, the proposed method can achieve better hallucination results compared with state-of-theart approaches.</p><p>The first key insight of our work is that embedding the neighbor information of the data space, since locality prior is very important for image modeling. This observation has previously been made in many NE based super-resolution methods (see e.g., <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref>). These works, however, define the geometry in the LR space, whereas we calculate geometry in the LR and HR space (in which can truly reflect the neighborhood relationship) simultaneously.</p><p>The second key insight of our work is that updating the reconstruction weights and the estimated HR image, since the interaction between the LR space and HR space can further improve the effectiveness of NE learning. Different from Li et al. <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b28">[29]</ref> and Huang et al. <ref type="bibr" target="#b7">[8]</ref>'s methods, which all try to learn the relationship between the LR and HR training sets in an uniform space, we hypothesize the existence of a virtual path which smoothly connects the LR image and the corresponding HR one. Therefore, we can expect to gradually optimize the target HR image with the estimated HR face image.</p><p>Built upon our preliminary work reported in <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, this paper is an improved version with the following contributions:</p><p>• The introduction section is rewritten to provide an extensive review of relevant work and to make our contributions clear. • An CLNE ensemble with post-processing to improve the performance of our original CLNE model. Specially, to further improve the super-resolution reconstruction quality, we manually add the high frequency of the reconstructed HR face to the target HR face image. • The geometrical structure of HR image manifold is very important for the NE based super-resolution methods, so we give the analysis on the different iteration number of our proposed approach in this extended version. • In addition to the experiments on FEI face database <ref type="bibr" target="#b36">[37]</ref>,</p><p>we give extensive experimental evaluations and in-depth analysis on its performance on some real-world images from CMU+MIT face database <ref type="bibr" target="#b37">[38]</ref> and our collected faces by a surveillance camera.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Organization of this paper</head><p>The rest of the paper is organized as follows. We first review the traditional NE based super-resolution method in Section II. The details of the proposed CLNE approach are presented in Section III. Comparative results are reported in Section IV. Finally, we give concluding remarks and future directions in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. SUPER-RESOLUTION THROUGH NEIGHBOR EMBEDDING</head><p>We denote Y m L and Y m H be the LR and HR training face images,m = 1, ..., M , where M is the sample number. Each face image is divided into N small overlapping LR patches</p><formula xml:id="formula_0">{y m L (i, j)|1 ≤ i ≤ U, 1 ≤ j ≤ V } and corresponding HR patches {y m H (i, j)|1 ≤ i ≤ U, 1 ≤ j ≤ V }, N = U V</formula><p>, in which U represents the patch number in every column, V represents the patch number in every row, and the term (i, j) indicates the position information. Given one LR input image X L denoted in patches as {x L (i, j)|1 ≤ i ≤ U, 1 ≤ j ≤ V }, our goal is to hallucinate each LR patch X L (i, j) to obtain the HR one X H (i, j), and then integrate all the hallucinated HR patches according to their position (i, j) to generate the target HR face image X H . LLE <ref type="bibr" target="#b20">[21]</ref> is a promising Nonlinear Dimensionality Reduction (NDR) method which is to map high dimensional data onto a low dimensional space, by preserving the neighborhood relationship in the high dimensional data space. The manifold learning theory is extended in some image super-resolution works <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b38">[39]</ref> with a similar assumption. In NE based super-resolution methods, they assume that the LR and HR patch pairs share the same local geometrical structure, which is characterized by how an LR patch can be reconstructed by its neighbors in the LR patch space. For each patch x L (we drop the position subscript (i, j) for convenience from then on) in the LR image, the NE based method firstly computes the reconstruction weights with its K-NN by minimizing the local reconstruction error. Then the HR embedding (patch) can be estimated with the corresponding HR patch neighbors and the same reconstruction weights. This algorithm can be summarized as the following two steps.</p><p>Firstly, for each LR patch x L in the input LR image, the optimal reconstruction weights are obtained by minimizing the local reconstruction error:</p><formula xml:id="formula_1">J NE (w) = ||x L - k∈C(x L ) w k y k L || 2 2 s.t. k w k = 1,<label>(1)</label></formula><p>where</p><formula xml:id="formula_2">C(x L ) is the index set of the K-NN of x L in the LR training patches {y m L } M m=1 . Minimizing Eq. (1) is a constrained least squares problem. Denote G L = (x L 1 T -X K ) T (x L 1 T -X K )</formula><p>, where 1 is a column vector of ones and X K is a matrix with its columns being the K-NN of x L in the LR space. Thus, Eq. ( <ref type="formula" target="#formula_1">1</ref>) can be optimized by solving a linear system equation G L w = 1, subject to k∈C(x L ) w k = 1. Secondly, the target HR image patch can be generated by applying the same reconstruction weights to corresponding neighbor HR patches in the HR space:</p><formula xml:id="formula_3">x H = k∈C(x L ) w k y k H .<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. A COUPLED-LAYER NEIGHBOR EMBEDDING MODEL</head><p>A. The LR layer</p><p>In the LR layer, we try to preserve the geometrical structure of the LR patch manifold for the reconstructed HR patch manifold.</p><p>Following Chang et al.'s work <ref type="bibr" target="#b9">[10]</ref>, which states that the HR image patches and LR ones form manifolds of the same geometrical structure, we propose in this paper that the LR layer is characterized by a weight vector reconstructed from K LR patches,</p><formula xml:id="formula_4">J(w, x H ) = x L - k∈C(x H ) w k y k L 2 2 s.t. k∈C(x H ) w k = 1,<label>(3)</label></formula><p>where C(x H ) is the index set of the K-NN of the estimated HR patch x H in the HR training patches {y m H } M m=1 . Different from the traditional manifold learning based super-resolution methods <ref type="bibr" target="#b9">[10]</ref>, we search the K-NN in the HR patch manifold by the estimated HR patch.</p><p>In addition, we have noted that the definition of the weight vector considers only one manifold (the LR patch manifold) and neglects the original HR patch manifold, which is much more credible and discriminant <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b38">[39]</ref>. Therefore, we additionally impose the HR patch reconstruction term to the objective function:</p><formula xml:id="formula_5">J(w, x H )= x L - k w k y k L 2 2 +α x H - k w k y k H 2 2 s.t. k w k =1, (<label>4</label></formula><formula xml:id="formula_6">)</formula><p>where α is a parameter balancing the contribution of these two terms and the index k is in the set C(x H ) unless otherwise stated. Note that x H is the target HR patch that we do not know in advance. Therefore, the objection function (4) with respect to w and x H can be solved respectively and iteratively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The HR layer</head><p>In the HR layer, we try to apply the geometrical structure of the original HR patch manifold to regularize the reconstructed HR patch manifold. In particular, from a view of graphbased learning, for any two HR patches in the K-NN set y k H k∈C(x H ) , the smaller their local distance, the more likely they would have the same reconstruction weights. Now we will show how to explore the intrinsic geometry (similarity) in the data matrix y k H k∈C(x H ) with the size of K × K. Similar with the work of <ref type="bibr" target="#b22">[23]</ref>, the penalty weighting matrix S with size of K × K is defined on the neighborhoods of the data points as follows:</p><formula xml:id="formula_7">s ij = exp -y i H -y j H 2 2 σ 2 i, j = 1, ..., K. (5)</formula><p>Here, σ is the parameter specifying the width of Gaussian function, which is set to τ i,j y i H -y j H 2 2 K 2 . τ is a scale parameter. The objective function with our choice of symmetric weights S incurs a heavy penalty if the weights of neighboring points y i H and y j H are very different. Therefore, minimizing it is an attempt to ensure that, if y i H and y j H are "close", w i and w j are close as well. Following some simple algebraic steps, tr (AB) = tr (BA) and tr (A) = tr A T , we see that</p><formula xml:id="formula_8">1 2 K i,j=1 (w i -w j ) 2 S i,j = w(D -S)w T = wLw T (<label>6</label></formula><formula xml:id="formula_9">)</formula><p>where D is a diagonal matrix, whose entries are column sums of S, D ii = j s ij (or row since S is symmetric), L = D -S is the Laplacian matrix <ref type="bibr" target="#b39">[40]</ref>. Here, the superscript "T" means transpose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Objective function and optimization</head><p>In the objective function, We would like to engage both properties: (i) LR patch manifold neighbor embedding; (ii) HR patch manifold preservation on the reconstructed weights. Therefore, the objective function of our proposed CLNE model is defined as:</p><formula xml:id="formula_10">J CLNE (w, x H ) = x L - k w k y k L 2 2 + α x H - k w k y k H 2 2 +βwLw T s.t. k=1 w k = 1, (7)</formula><p>where β is a parameter controls the tradeoff between matching the LR input patch and finding an HR patch that is compatible with its neighbors. The constrained optimization (7) can be reformulated as</p><formula xml:id="formula_11">J CLNE (w, z x ) = z x - k w k z k y 2 2 +βwLw T s.t. k=1 w k = 1, (8) where z x = x L √ αx H and z k y = y k L √ αy k H .</formula><p>The minimization of J CLN E (w, z x ) with respect to w and z x can be solved respectively and iteratively. Eq. ( <ref type="formula">8</ref>) is convex to w and z x , thus it can be converge. For the p-th block, firstly set x H to the Bicubic interpolation version of the input LR patch. Then update the weights w with x H by minimizing J CLNE (w, z x ) as a constrained least squares problem <ref type="bibr" target="#b9">[10]</ref> (additionally, if we firstly also set w to a random vector and then update x H with the weights w, the experimental results show that we can obtain the same results). Note that the optimal weight vector has a closed-form solution given by:</p><formula xml:id="formula_12">w = (G + βL)\1, (<label>9</label></formula><formula xml:id="formula_13">)</formula><p>where G is the local covariance matrix for z x as:  Obtain the optimal reconstruction weights according to Eq. ( <ref type="formula" target="#formula_12">9</ref>), Eq. ( <ref type="formula" target="#formula_14">10</ref>) and Eq. ( <ref type="formula" target="#formula_18">11</ref>).</p><formula xml:id="formula_14">G = CC T . (<label>10</label></formula><formula xml:id="formula_15">)</formula><formula xml:id="formula_16">(i, j)|1 ≤ i ≤ U, 1 ≤ j ≤ V } M m=1 , {y m H (i, j)|1 ≤ i ≤ U, 1 ≤ j ≤ V } M m=1 , and {x L (i, j)|1 ≤ i ≤ U, 1 ≤ j ≤ V }, respectively. 2: Initialize: p = 0 ,X H (0) = Bicubic(X L ). 3: for each LR patch x L (i, j) of X L do 4:</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9:</head><p>Construct the target HR patch:</p><formula xml:id="formula_17">x H = k∈C(x H )</formula><p>w k z k y .</p><p>10:</p><p>until p &gt; maxIter -1 11: end for 12: Integrate all the obtained HR patches above according to the position. The estimated HR image XH can be generated by averaging pixel values in the overlapping regions. 13: Refine the estimated HR image XH according to Eq. <ref type="bibr" target="#b11">(12)</ref> to obtain the final HR image X H .</p><p>Define C as:</p><formula xml:id="formula_18">C = (z x • ones(1, K) -Z K y ),<label>(11)</label></formula><p>where Z K y is a matrix with its columns being z k y , ones(1, K) is a 1 × K row vector of ones. The final optimal weight vector is obtained by rescaling it so that w k =1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Facial image hallucination via CLNE</head><p>According to the position of face, the patches are processed in raster-scan order in the input LR image, from left to right and top to bottom. Following <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, we enforce compatibility between adjacent patches (the values in the overlapped regions are simply averaged). To further improve the super-resolution quality, we manually add the high frequency of the reconstructed HR face to the target HR face image:</p><formula xml:id="formula_19">X H = f * XH + XH , (<label>12</label></formula><formula xml:id="formula_20">)</formula><p>where f is a Gaussian high-pass filter.</p><p>The entire facial image hallucination process is summarized as Algorithm 1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL VALIDATION A. Datasets and parameters settings</head><p>The experiments are conducted on FEI face dataset<ref type="foot" target="#foot_2">2</ref>  <ref type="bibr" target="#b36">[37]</ref>, which contains 400 images from 200 subjects and each subject has two frontal images, one with neutral expression and the other with smiling expression. All the images are cropped to 144×120 pixels (as shown in Fig. <ref type="figure" target="#fig_2">3</ref>), and we randomly choose 360 images (180 subjects) as the training set, leaving the rest 40 images (20 subjects) for testing. Therefore, all the test images are absent completely in the training set. Note that the face can be aligned by some recently proposed automatic alignment methods <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref> and feature points matching methods <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>. The LR images are formed by smoothing (an averaging filter of size 8×8), down-sampling (by a factor of eight, thus the size of LR images are 18×15 pixels), adding Poisson noise (which can better models real noise than Gaussian noise <ref type="bibr" target="#b44">[45]</ref>) to the corresponding HR images. Following <ref type="bibr" target="#b19">[20]</ref>, we firstly upscale the LR image (by factor of two) to 36×30 pixels using Bicubic interpolation, and then extract the image pixels according to top-to-bottom and left-to-right scanning to form the feature vector for each patch.</p><p>For these local patch based facial image hallucination methods, the size of a patch is important for getting reliable results. If the patch size is too small, it would give little information and cannot capture the geometrical features of human face. On the other hand, as the patch size becomes larger, the hallucinated face may be smooth and lose some visual details. In our experiments, we recommend to set the size as 20×20 pixels for HR patch and the overlap between neighbor patches as 16 pixels, while corresponding LR patch size is set to 5×5 pixels with a overlap of four pixels. As for the proposed method, the parameters of the CLNE model are tuned carefully: α = 0.01, β = 0.2 , K = 75, and the max iteration maxIter is set to four (more details about the influence of the iteration number will be investigated in the following subsection).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Hallucinated results with different iteration number</head><p>For effective neighbor embedding learning, we propose to introduce the geometrical structure of HR image manifold  (which is much more confident than that of the LR one <ref type="bibr" target="#b32">[33]</ref>) to regularize the reconstruction weights. For we cannot obtain the HR image in advance, in Eq. ( <ref type="formula">7</ref>) we design an iterative strategy to update the reconstruction weights and the estimated HR patch. For each iteration, the estimated HR image will asymptotically towards the ground truth. In order to certify the effectiveness of the iterative strategy of the proposed method, we evaluate the performance of CLNE with different iteration number. Fig. <ref type="figure" target="#fig_3">4</ref> shows the average PSNR and Structural SIMilarity (SSIM) results of all the 40 test face images with different iteration number. As can be seen, with the increase of iteration number, the performance becomes better and better. Note that the proposed method will reach a stable performance when the iteration number reaches four, and this also shows the quick convergence of the proposed approach.</p><p>C. Changes of the neighborhood preservation rates after super-resolution From Fig. <ref type="figure">1</ref>, we learn that the neighborhood preservation rates sharply drop when the sampling factor increases or Poisson noise is added. In this subsection, we test the changes of the neighborhood preservation rates after super-resolution reconstruction. In particular, to obtain the super-resolved HR version of each LR training face image, we utilize the proposed CLNE method to update all the LR training faces by a so-called leave-one-out strategy. Then, we recalculate the neighborhood preservation rates between the reconstructed HR image manifold and the original HR image manifold (see the black line with circle shown in Fig. <ref type="figure">1</ref>). As can be seen from Fig. <ref type="figure">1</ref>, the neighborhood preservation rates are improved when the proposed approach applies.  <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref> are taken for comparison with the proposed approach. The parameters of all comparison methods are set to the best performance. We compare above facial image hallucination approaches quantitatively in terms of the PSNR and SSIM indexes <ref type="bibr" target="#b45">[46]</ref> in Fig. <ref type="figure" target="#fig_4">5</ref>. We can see that our approach gives the most faithful hallucination results to the original images. Specifically, the average PSNR and SSIM improvements of CLNE method over the second best method (i.e., Jiang et al.'s LcR based method <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>) are 0.64 dB and 0.0465, respectively. Note that there are three parts in our objective function <ref type="bibr" target="#b6">(7)</ref>, and the last two parts are the HR patch manifold regularization terms. If we drop the HR patch manifold regularization terms, the objective function <ref type="bibr" target="#b6">(7)</ref> reduce to traditional NE approach without considering the HR patch manifold (as shown in Eq. ( <ref type="formula" target="#formula_1">1</ref>)). From Fig. <ref type="figure" target="#fig_4">5</ref>, we learn that the superresolution performance with HR image manifold loss (Chang et al's NE method) and without the HR image manifold loss (our proposed CLNE method). We can learn that gain of CLNE over NE is 1.48 dB in term of PSNR and 0.0431 in term of SSIM respectively. Some visual results are shown in Fig. <ref type="figure" target="#fig_6">6</ref>. It can be observed that the hallucinated results of Bicubic interpolation are with blurring and jagging artifacts along the face counters. Ma et al.'s LSR method <ref type="bibr" target="#b10">[11]</ref> will add the noise rather than remove it. This result is also consistent with the objective evaluation from Fig. <ref type="figure" target="#fig_4">5</ref> (LSR has significantly lower PSNR and SSIM values than other comparison methods). The main reason is the unstable solution of solving the least squares problem. NE <ref type="bibr" target="#b9">[10]</ref> and SR <ref type="bibr" target="#b11">[12]</ref> can remove noise to some extent. However, the hallucinated faces of NE <ref type="bibr" target="#b9">[10]</ref> are blurring and lack of facial details, while SR <ref type="bibr" target="#b11">[12]</ref> are dirty. LcR method <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref> and CLPM method <ref type="bibr" target="#b15">[16]</ref> remove most of the noise of the input LR face images and well maintain the global face contours. However, the hallucinated faces by LcR <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref> are blurred and untrusted (please see the mouths of the third from rows three through eight and the eyes of the last three rows). CLPM  <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, and our proposed CLNE method. The last column are the original HR faces. <ref type="bibr" target="#b15">[16]</ref> is better than NE method <ref type="bibr" target="#b9">[10]</ref> and is robust to noise to some extent. The eighth column is the hallucinated HR face of our proposed CLNE method, we can see that CLNE outperforms all the comparison methods and generates the reasonable results with more facial details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparison results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Several</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Experiments with real images</head><p>In order to extensively verify the effectiveness of the proposed method, we not only conduct experiments on artificially generated LR faces, but also conduct experiments on some real images, which can better represent the true spatial feature relationship between the HR image and the degraded LR one, such as samples from the CMU+MIT face database (as shown in Fig. <ref type="figure" target="#fig_7">7 (a)</ref>) or captured by the surveillance camera at a distance and in low-light conditions (as shown in Fig. <ref type="figure" target="#fig_9">8 (a)</ref>).</p><p>In order to simulate the actual surveillance conditions in the night, we establish a test room (as shown in Fig. <ref type="figure" target="#fig_8">9</ref>) with some adjustable light sources. The system is consisted by the image capture equipment (one surveillance camera), the transformation equipment, the PC machine and the digital video control and management software. We collected the images at different distances for some subjects. In our experiments, we use the distance of 3.0 m. The light intensity is adjusted to a pre-set value. In our experiments, the three input LR images are captured under the light intensity of 1.8 lux.</p><p>Note that we manually align the extracted faces to the mean face of FEI face database through the positions of the two eye centers. Again, we can see that the proposed method gives the best performance especially when the input LR face image is contaminated by strong noise (the fourth row of Fig. <ref type="figure" target="#fig_7">7</ref>) or degraded by uncontrollable real conditions as shown in Fig. <ref type="figure" target="#fig_9">8</ref>, while the comparison approaches will result in practice in noticeable reconstruction artifacts.</p><p>From Fig. <ref type="figure" target="#fig_7">7</ref>, we can learn that Ma et al.'s LSR <ref type="bibr" target="#b10">[11]</ref> is very sensitive to noise, and this is mainly because of that LSR tries to use the least squares representation to construct the noise patch. In other words, it aims at synthesizing and "explaining" the noise but replacing the noise. Jung et al.'s SR <ref type="bibr" target="#b11">[12]</ref> can remove most of the small noise; when the noise is very strong, the SR method will cause severe distortion. This is consistent with the sparse representation theory which states that sparse recovery is robust against small magnitude noise in the observation. Chang et al.'s NE <ref type="bibr" target="#b9">[10]</ref> and our proposed CLNE are very robust to noise, and we attribute this to the incorporated locality prior of these two methods. By incorporating the locality prior, it is quite sensible to replace the noisy patches with similar "clean" ones.</p><p>However, we should realize that these hallucinated faces appear noise or with superfluous details, and are relatively poor compared with the results from the simulated experiments (Section IV-D), in which training images and test ones are from FEI face database. This shows a weakness of the learning-based super-resolution techniques which require a certain similarity between the training and LR input images. It may help to explain why the results of the real surveillance face images (Fig. <ref type="figure" target="#fig_9">8</ref>) are worse than that of the CMU+MIT face database (Fig. <ref type="figure" target="#fig_7">7</ref>). The test images of these three persons (Fig. <ref type="figure" target="#fig_7">7</ref>) and training images are from Europe and America, while the persons of Fig. <ref type="figure" target="#fig_9">8</ref> are Chinese. Thus the input LR faces (Fig. <ref type="figure" target="#fig_7">7</ref>) are much more similar to the training samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. MAIN FINDINGS AND FUTURE DIRECTIONS</head><p>The Coupled-Layer Neighbor Embedding (CLNE) has been proposed as a novel neighbor embedding model for facial image hallucination. In CLNE, we firstly use the Bicubic interpolator to estimate the initial HR face image to obtain the initial HR patch, then search the K-NN in the HR patch space and calculate the similarity graph in the K-NN for them, and then obtain the reconstruction weights by minimizing the reconstruction error and preserving the graph for the constructed HR patch manifold. After several iteration steps, we can get the target HR patch image. Concatenating and integrating all the hallucinated HR patches, we generate the target HR face image. Experimental results on FEI face database and some real images are presented to demonstrate the effectiveness of the proposed approach.</p><p>In this paper, we directly use the whole training image patches as the dictionary, and to learn a compact and rep- resentative dictionary will be our further work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Illustration of the proposed CLNE model. The LR layer tries to preserve the geometrical structure of the LR patch manifold for the reconstructed HR patch manifold, while the HR layer tries to apply the geometrical structure of the original HR patch manifold to regularize the reconstructed HR patch manifold.</figDesc><graphic coords="2,46.93,56.72,255.12,151.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>repeat 5 : 6 :</head><label>56</label><figDesc>Find the K-NN index set C(x H ) and the corresponding K-NN HR patch set of x H .Calculate the similarity matrix S for the K-NN of x H in the HR patch set.7:Calculate the Laplacian matrix: L = D -S, where D ii = j s ij .8:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Some training faces in FEI Face Database. Each subject has two frontal images, one with a neutral expression and the other with a smiling facial expression.</figDesc><graphic coords="5,341.12,56.72,192.76,114.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Facial image hallucination results in terms of PSNR and SSIM with different iteration number.</figDesc><graphic coords="6,46.93,56.73,255.13,195.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. PSNR (left) and SSIM (right) competitions of seven different methods. The average PSNR and SSIM of different methods: Bicubic (PSNR = 24.28, SSIM = 0.6939); Chang et al.'s NE[10] (PSNR = 24.92, SSIM = 0.7356); Ma et al.'s LSR [11] (PSNR = 24.46, SSIM = 0.6767); Li et al.'s CLPM [29] (PSNR = 25.34, SSIM = 0.7329); Jung et al.'s SR [12] (PSNR = 25.26, SSIM = 0.7304); Jiang et al.'s LcR [19], [20] (PSNR = 25.76, SSIM = 0.7322) and our proposed CLNE method (PSNR = 26.40, SSIM = 0.7787).</figDesc><graphic coords="6,39.84,294.21,269.29,154.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>typical facial image hallucination techniques: Bicubic interpolation, Chang et al.'s NE method [10], Li et al.'s CLPM method [29], Ma et al.'s LSR method [11], Jung et al.'s SR method [12] and Jiang et al.'s LcR method</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Comparison of results based on different methods. From left to right: the input LR faces, results of Bicubic, Chang et al.'s NE [10], Ma et al.'s LSR [11], Li et al.'s CLPM [29], Jung et al.'s SR [12], Jiang et al.'s SR [19], [20], and our proposed CLNE method. The last column are the original HR faces.</figDesc><graphic coords="7,79.22,56.72,453.55,486.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Comparison of results based on the extracted faces from (a): (b) Input LR faces; (c) Bicubic; (d) Chang et al.' NE [10]; (e) Li et al.'s CLPM [16]; (f) Ma et al.'s LSR [11]; (g) Jung et al.'s SR [12]; (h) Jiang et al.'s LcR [19], [20]; (i) Our proposed CLNE method.</figDesc><graphic coords="8,100.48,56.72,411.03,280.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. A test room used to simulate the actual surveillance conditions in low-light.</figDesc><graphic coords="8,312.78,379.29,249.44,69.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Comparison of results based on the extracted faces from (a): (b) Input LR faces; (c) Bicubic; (d) Chang et al.'NE [10]; (e) Li et al.'s CLPM [16]; (f) Ma et al.'s LSR [11]; (g) Jung et al.'s SR [12]; (h) Jiang et al.'s LcR [19], [20]; (i) Our proposed CLNE method.</figDesc><graphic coords="9,93.40,56.72,425.21,338.15" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The higher neighborhood preservation rate value is, the more consistent between coupled manifolds are.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. XXX, NO. XXX, FEBRUARY 2015</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>http://fei.edu.br/ ∼ cet/facedatabase.html</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The research was supported by the project was supported by the Fundamental Research Funds for the Central Universities, China University of Geosciences (Wuhan), the Open Foundation of Hubei Provincial Key Laboratory of Intelligent Robot (HBIR201404), the National Natural Science Foundation of China (61231015, 61172173, 61303114), and the National Key Technologies R&amp;D Program (2013AA014602).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Hallucinating faces</title>
		<author>
			<persName><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Automatic Face and Gesture (FG)</title>
		<meeting>IEEE Conf. on Automatic Face and Gesture (FG)</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="83" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A two-step approach to hallucinating faces: global parametric model and local nonparametric model</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="192" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Hallucinating face by eigentransformation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst. Man Cybern. Part C-Appl. Rev</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="425" to="434" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An example-based face hallucination method for single-frame, low-resolution facial images</title>
		<author>
			<persName><forename type="first">J.-S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1806" to="1816" />
			<date type="published" when="2008-10">Oct 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Super-resolution of face images using kernel pca-based prior</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rajagopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Multimedia</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="888" to="892" />
			<date type="published" when="2007-06">Jun 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An adaptive learning method for face hallucination using locality preserving projections</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">FG</title>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Image super-resolution via sparse representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2861" to="2873" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Super-resolution of human face image using canonical correlation analysis</title>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2532" to="2543" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning low-level vision</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Pasztor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">T</forename><surname>Carmichael</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="47" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Super-resolution through neighbor embedding</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="275" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hallucinating face by position-patch</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2224" to="2236" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Position-patch based face hallucination using convex optimization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing Letters, IEEE</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="367" to="370" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Face hallucination via sparse coding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1264" to="1267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A comprehensive survey to face hallucination</title>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Super-resolution: A comprehensive survey</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nasrollahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Moeslund</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Machine Vision &amp; Applications</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Aligning coupled manifolds for face hallucination</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing Letters, IEEE</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="957" to="960" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">From local pixel structure to global image super-resolution: A new face hallucination framework</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-M</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="433" to="445" />
			<date type="published" when="2011-02">Feb 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hallucinating face in the dct domain</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-K</forename><surname>Cham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2769" to="2779" />
			<date type="published" when="2011-10">Oct 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Position-patch based face hallucination via locality-constrained representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="212" to="217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Noise robust face hallucination via locality-constrained representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Multimedia</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Nonlinear dimensionality reduction by locally linear embedding</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page" from="2323" to="2326" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A global geometric framework for nonlinear dimensionality reduction</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">D</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page" from="2319" to="2323" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Face recognition using laplacianfaces</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="328" to="340" />
			<date type="published" when="2005-03">march 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Close the loop: Joint blind image restoration and recognition with sparse representation prior</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Nasrabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="770" to="777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Face hallucination via weighted adaptive sparse regularization</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="802" to="813" />
			<date type="published" when="2014-05">May 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Nonlinear learning using local coordinate coding</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="2223" to="2231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Low-resolution recognition: a review</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Visual Computer</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="359" to="386" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Very low resolution face recognition problem</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="327" to="340" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Low-resolution face recognition via coupled locality preserving mappings</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing Letters, IEEE</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20" to="23" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Simultaneous super-resolution and feature extraction for recognition of low-resolution faces</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Hennings-Yeomans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multidimensional scaling for matching low-resolution face images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Flynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2019" to="2030" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Neighborhood issue in single-frame image super-resolution</title>
		<author>
			<persName><forename type="first">K</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Efficient single image superresolution via graph-constrained least squares regression</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools Appl</title>
		<imprint>
			<biblScope unit="page" from="1C" to="24" />
			<date type="published" when="2013-06">Jun. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Coupled-layer neighbor embedding for surveillance face hallucination</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<biblScope unit="page" from="2802" to="2806" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Localityconstraint iterative neighbor embedding for face hallucination</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Face super-resolution via multilayer locality-constrained iterative neighbor embedding and intermediate dictionary learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4220" to="4231" />
			<date type="published" when="2014-10">Oct 2014</date>
		</imprint>
	</monogr>
	<note>Image Processing</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A new ranking method for principal components analysis and its application to face image analysis</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Thomaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Giraldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="902" to="913" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Neural network-based face detection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Rowley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baluja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="23" to="38" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Dual-geometric neighbor embedding for image super resolution with sparse tensor</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2793" to="2803" />
			<date type="published" when="2014-07">July 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Spectral Graph Theory</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R K</forename><surname>Chung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>American Mathematical Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Non-rigid visible and infrared face registration via regularized gaussian fields criterion</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="772" to="784" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Robust l 2 e estimation of transformation for non-rigid registration</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1115" to="1129" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Robust point matching via vector field consensus</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1706" to="1721" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Regularized vector field learning with sparse approximation for mismatch removal</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3519" to="3532" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Image denoising in mixed poissongaussian noise</title>
		<author>
			<persName><forename type="first">F</forename><surname>Luisier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Blu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Unser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="696" to="708" />
			<date type="published" when="2011-03">March 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">He is currently an Associate Professor with the School of Computer Science, China University of Geosciences. His research interests include applications of image processing and pattern recognition in video surveillance, image super-resolution, image interpolation, and face recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">His research interests include video compression, image processing, and multimedia communications, etc. Zhen Han received the B.S. degree in computer science and technology and Ph.D. degree in computer application technology from Wuhan University</title>
		<title level="s">Junjun Jiang received the B.S. degree in Information and Computing Science from School of Mathematical Sciences</title>
		<meeting><address><addrLine>Quanzhou, China; Wuhan, China; Nanjing, China; Wuhan, China; Wuhan, China; Wuhan, China; Wuhan, China; Wuhan, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">2004. 2009. 2014. 1994. 2008. 2002. 2009. 2008. 2014</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="600" to="612" />
		</imprint>
		<respStmt>
			<orgName>Huaqiao University ; Wuhan University ; National Engineering Research Center for Multimedia Software, Wuhan University and Key Laboratory of Multimedia Network Communication Engineering in Hubei province ; School of Computer, Wuhan University ; Huazhong University of Science and Technology</orgName>
		</respStmt>
	</monogr>
	<note>Now he is a lecturer in school of computer, Wuhan University. His research interests include image/video compressing and processing, computer vision and artificial intelligence. Jiayi Ma received the B.S. degree from the Department of Mathematics, and the Ph.D. Degree from the School of Automation. respectively. From 2012 to 2013, he was with the Department of Statistics, University of California at Los Angeles. He is now a Post-Doctoral with the Electronic Information School, Wuhan University. His current research interests include in the areas of computer vision, machine learning, and pattern recognition</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
