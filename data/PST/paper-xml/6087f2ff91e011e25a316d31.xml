<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
				<funder>
					<orgName type="full">School of Electronics Engineering and Computer Science at Peking University</orgName>
				</funder>
				<funder>
					<orgName type="full">Peng Cheng Laboratory</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-04-26">26 Apr 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wei</forename><surname>Zeng</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaozhe</forename><surname>Ren</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Teng</forename><surname>Su</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hui</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chao</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shaojie</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mingyue</forename><surname>Guo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shanzhi</forename><surname>Gu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Gaojun</forename><surname>Fan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yaowei</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xuefeng</forename><surname>Jin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yonghong</forename><surname>Tian</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Yi</orgName>
								<address>
									<addrLine>Liao Zhiwei Wang Xin Jiang Zhenzhang Yang Kaisheng Wang Xiaoda Zhang Chen Li Ziyan Gong Yifan Yao Xinjing Huang Jun Wang Jianfeng Yu Qi Guo Yue Yu Yan Zhang Jin Wang Hengtao Tao Dasen Yan Zexuan Yi Fang Peng Fangqing Jiang Han Zhang Lingfeng Deng Yehong Zhang Zhe Lin</addrLine>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-04-26">26 Apr 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2104.12369v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Pre-trained Language Models</term>
					<term>Large-scale Deep Models</term>
					<term>Distributed Training</term>
					<term>Chinese Language Understanding and Generation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large-scale Pretrained Language Models (PLMs) have become the new paradigm for Natural Language Processing (NLP). PLMs with hundreds of billions parameters such as  have demonstrated strong performances on natural language understanding and generation with few-shot in-context learning. In this work, we present our practice on training large-scale autoregressive language models named PanGu-?, with up to 200 billion parameters. PanGu-? is developed under the MindSpore 2 and trained on a cluster of 2048 Ascend 910 AI processors 3 . The training parallelism strategy is implemented based on MindSpore Auto-parallel, which composes five parallelism dimensions to scale the training task to 2048 processors efficiently, including data parallelism, op-level model parallelism, pipeline model parallelism, optimizer model parallelism and rematerialization. To enhance the generalization ability of PanGu-?, we collect 1.1TB high-quality Chinese data from a wide range of domains to pretrain the model. We empirically test the generation ability of PanGu-? in various scenarios including text summarization, question answering, dialogue generation, etc. Moreover, we investigate the effect of model scales on the few-shot performances across a broad range of Chinese NLP tasks. The experimental results demonstrate the superior capabilities of PanGu-? in performing various tasks under few-shot or zero-shot settings.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Pre-trained Language Models (PLMs) <ref type="bibr">[1, 2, 3, 4, 5, 6, 7, 8, 9, etc.]</ref> have gained great success in the Natural Language Processing (NLP). By learning contextual representation of text from large-scale corpora in a self-supervised manner, PLMs can achieve state-of-the-art performances on a wide range of Natural Language Understanding (NLU) and Natural Language Generation (NLG) tasks.</p><p>Radford et. al. <ref type="bibr" target="#b10">[10]</ref> demonstrates a significant gains on a variety of NLP tasks via Generative Pre-trained Transformer (GPT), which is an autoregressive language model first pretrained on unsupervised text data and then finetuned for each supervised task. Devlin et.al. <ref type="bibr" target="#b2">[2]</ref> proposes BERT, a bidirectional Transformer with the masked language model (MLM) pretraining objective, which obtains new state-of-the-art performances on the GLUE benchmark of NLU tasks. After them, there have been an increasing number of research work on developing the pretraining techniques and continuously improving the performance of downstream NLP tasks. Among all the techniques, researchers find that the performance of PLMs can be steadily improved simply by enlarging the amount of the training data as well as the capacity of the model. For instance, RoBERTa <ref type="bibr" target="#b5">[5]</ref> shows that BERT can be substantially improved by training the model longer with more data. GPT-2 <ref type="bibr" target="#b11">[11]</ref> as the successor of GPT, which shares the same architecture but contains 1.5 billion parameters and is trained with 40GB text, can perform reasonably well on multiple tasks in the zero-shot setting. The T5 model <ref type="bibr" target="#b6">[6]</ref> with 11 billion parameters trained on the 745GB C4 data, keeps pushing the performance of both NLU and NLG tasks.</p><p>Recently, the OpenAI team announced its lasted version of the GPT-series models: GPT-3 <ref type="bibr" target="#b1">[1]</ref>. The largest GPT-3 model contains 175 billion parameters and is trained using 570GB of text data. Besides its strong capability in generating high-quality text, GPT-3 is especially effective in solving a wide range of tasks without task-specific finetuning in the few-shot, or even zero-shot settings. Moreover, on many of the tasks the performance improves steadily as the size of the GPT model grows, and sometimes even reaches the level of the prior state-of-the-art finetuning approaches. From applications perspective, GPT-3 is revolutionary, as it relieves the need for labelling many examples and retraining model for every new task, which hinders the applicability of NLP models in real-world applications. However, GPT-3 is now only available for limited access via OpenAI API, and it is primarily trained with English data. To promote the public research of Chinese PLMs, we propose training a very large-scale Chinese PLM named PanGu-? with number of parameters up to 200 billion. To the best of our knowledge, this is the largest Chinese PLM up to the publication of this technical report.</p><p>The difficulty in training a PLM rises as the scale of the model grows beyond the level of 10 billion. The main challenges lie in three aspects:</p><p>? Model Design. There have been a couple of architectures of PLMs besides GPT and BERT. However, not all the PLMs can be smoothly scaled to hundreds of billions of parameters. For examples, some models may have problem of slow convergence or even divergence during training as the model size increases. Inspired by GPT-3 and our preliminary experiments, we choose the Transformer-based autoregressive language model as the base architecture. Besides, we develop an additional query layer on top of the Transformer layers to induce the expected output of the model during pretraining. Our experiments demonstrate that the structure of PanGu-? can scale up to 200 billion parameters. ? Training Corpora. Training data is essential in building a strong and generalisable pretrained model. On one hand, the amount of the data should be sufficient to feed a large PLM. On the other hand, the data should be of high quality and diversity to ensure the generality of the PLM. To build Chinese corpus with comprehensive coverage, we collect a large amount of data from a wide range of resources, including Common Crawl, e-Books, encyclopedias, news, and so on. Based on them, we conduct multiple processes of data filtering and cleaning to make sure the processed data are of high quality and reliability. ? Distributed Training. The memory requirement of training PanGu-? with 200 billion parameters is much beyond the memory capacities of modern AI processors. It is difficult to acquire large end-to-end throughput while keeping high resource utilization on a cluster of processors. The problem becomes more challenging when considering the topology of hardware. We combine five-dimensional parallel functionalities with a carefully designed parallelization strategy and apply them to the largest PanGu-?, which is efficiently trained on a cluster of 2048 Ascend 910 AI processors <ref type="bibr" target="#b12">[12]</ref> and powered by CANN <ref type="foot" target="#foot_0">4</ref> .</p><p>We train three PanGu-? models on a high-quality 1.1TB Chinese text corpus with increasing magnitude of parameter sizes, which are PanGu-? 2.6B, PanGu-? 13B, and PanGu-? 200B, respectively. We first evaluate the models on language modeling tasks, showing that the perplexity can be decreased with the increase of model capacity and the amount of data and computation. Then we investigate the text generation ability of PanGu-? in various scenarios such as dialogue generation, summarization, question answering, etc. We demonstrate a few generated samples for different applications in the experiment section. Furthermore, we evaluate the task-agnostic few-shot performances of PanGu-? 2.6B and 13B on a wide range of NLP tasks, including cloze tasks, reading comprehension, closed-book QA, Winograd style tasks, commonsense reasoning, natural language inference, and text classification. The experimental results demonstrate that with the growing model capacity, the performance on various tasks can generally improve.</p><p>We are currently seeking a proper way to let both non-profit research institutes and commercial companies to get access to our pretrained PanGu-? models, either by releasing the code and model or via APIs. We are also assessing the possibility of releasing all or part of our pretraining data, within the constraints of the law and legality.</p><p>To facilitate the community to pretrain a large-scale language model by their own, the parallel computing functionalities are open-sourced in the Auto-parallel module of MindSpore The reminder of this technical report is organized as follow. Section 2 describe the architecture of our PanGu-? models.</p><p>In section 3, we detail our methods to construct a 1.1TB high-quality training corpus from 80TB raw data collected from various sources. Section 4 addresses the parallelization paradigm of model training and scheduling strategy on a cluster of Ascend processors. Section 5 presents the experimental results of PanGu-? models on various tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overview</head><p>PanGu-? is a large-scale autoregressive language model (ALM) pretrained on a large corpus of text, mostly in Chinese language. It models the generative process of all the tokens in the corpus, where the generation of a token depends on its previous tokens in a sequence. Assuming that a sequence X = {x 1 , x 2 , ..., x N } is composed of N tokens, the training objective can be formulated as maximization of the log-likelihood:</p><formula xml:id="formula_0">L = N n=1 log p(x n |x 1 , ..., x n-1 ; ?),<label>(1)</label></formula><p>where p(x n |x 1 , ..., x n-1 ; ?) is the probability of observing the n-th token x n given the previous context x 1:n-1 , and ? denotes the model parameters.</p><p>The architecture of PanGu-? is based on Transformer <ref type="bibr" target="#b13">[13]</ref>, which has been extensively used as the backbone of a variety of pretrained language models such as BERT <ref type="bibr" target="#b2">[2]</ref> and GPT <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b1">1]</ref>. Different from them, we develop an additional query layer on top of Transformer layers to predict the next token. The diagram of the model is shown in Figure <ref type="figure">1</ref>. We elaborate each part as follow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Model Structure</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Transformer Layers</head><p>A standard transformer layer includes two sub-layers: multi-head attention (MHA) and fully connected feed-forward network (FFN).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-head Attention:</head><p>A self-attention network in the l-th Transformer layer is parameterized by four projection matrices:</p><formula xml:id="formula_1">W k h , W q h , W v h , W m h ? R d?d/N h ,</formula><p>where d is the hidden dimension, h is the index of head, and N h is the number of heads. Given the output H l-1 ? R N ?d from the precedent layer, three major components, i.e., query</p><formula xml:id="formula_2">Q h = H l-1 W q h , key K h = H l-1 W k h , and value V h = H l-1 W v h are produced.</formula><p>The attention function is computed as: With multiple attention heads, the output becomes:</p><formula xml:id="formula_3">A h = Q h K h = H l-1 W q h W k h H l-1 , Attention h (H l-1 ) = Softmax( A h ? d )V h = Softmax( A h ? d )H l-1 W v h .<label>(2)</label></formula><formula xml:id="formula_4">MHA(H l-1 ) = N h h=1 Attention h (H l-1 )W m h , H MHA l = H l-1 + MHA(LayerNorm(H l-1 )).<label>(3)</label></formula><p>Feed-forward Network: The FFN layer is composed of two linear layers, parameterized by</p><formula xml:id="formula_5">W 1 ? R d?d f f , b 1 ? R d f f , W 2 ? R d f f ?d , b 2 ? R d</formula><p>, where d f f is the dimension of the inner-layer. Fed with the output of MHA layer as input, the output of FFN layer is then computed as:</p><formula xml:id="formula_6">FFN(H MHA l ) = GeLU(H MHA l W 1 + b 1 )W 2 + b 2 , H l = H MHA l + FFN(LayerNorm(H MHA l )).<label>(4)</label></formula><p>For both MHA and FFN, we take the pre-layer normalization scheme, which can make the training of Transformer model easier and faster <ref type="bibr" target="#b14">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Query Layer</head><p>We design the query layer on top of the stacked Transformer layers, which aims to explicitly induce the expected output.</p><p>In the pretraining stage of the autoregressive model, it comes to the prediction of the next token. The structure of the query layer resembles the transformer layer, except that an additional embedding p n ? R d indicating the next position is used as the query vector in the attention mechanism. Specifically, assuming H L is the output of the uppermost transformer layer, the attention vector in the query layer is computed as:</p><formula xml:id="formula_7">a h = p n W q h W k h H L .</formula><p>(5) The subsequent computation of MHA and FFN remains the same as the original Transformer. We denote the final output as o n . The negative log-likelihood of next token becomes:</p><formula xml:id="formula_8">CrossEntropy(x n , Softmax(o n W o + b o )),<label>(6)</label></formula><p>where x n denotes the true token and W o , b o is the additional task-dependent parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Model Configurations</head><p>To evaluate the scaling ability of the PanGu-? model, we train three models with increasing magnitude of parameter sizes, that is, PanGu-? 2.6B, PanGu-? 13B, and PanGu-? 200B. Table <ref type="table">1</ref> shows the detailed configurations of the three models, including the number of total parameters, the hidden dimension for the tokens, the inner dimension of the feed-forward layer, and the number of attention heads. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dataset</head><p>A large-scale Chinese text corpus of high quality is crucial for the pretraining of our PanGu-? models, especially the one with 200B parameters. Existing large-scale text corpora for pretraining super large language models are mainly English. For example, the GPT-3 <ref type="bibr" target="#b1">[1]</ref> is trained using a dataset which contains 570GB filtered texts from Common Crawl with 92.6% of the words are English. The Colossal Clean Crawled Corpus (C4) for training T5 consists of about 750GB clean English texts scraped from the web <ref type="bibr" target="#b6">[6]</ref>. To the best of our knowledge, there are three Chinese text corpora that are above 100GB: (a) CLUECorpus2020 (100GB), which is retrieved from the Common Crawl dataset <ref type="bibr" target="#b15">[15]</ref>; (b) the Chinese multi-modal pretraining data, released by <ref type="bibr" target="#b16">[16]</ref> which contains 300GB texts; and (c) WuDaoCorpus <ref type="foot" target="#foot_2">6</ref> , which opens about 300GB text data to only specific partners so far. However, all the above datasets are still not enough to train the super large-scale models up to 200B parameters compared to the data size used in existing English pretrained models.</p><p>Even though the raw web datasets such as SogouT <ref type="foot" target="#foot_3">7</ref> and Common Crawl<ref type="foot" target="#foot_4">8</ref> contain massive amount of Chinese texts, the construction of our desired dataset is still challenging due to the highly varying quality of the raw web data, the huge amount of storage and computation to preprocess the data, and the lack of well-defined metrics to evaluate the quality of the data.</p><p>To tackle the aforementioned issues, we construct a 1.1TB high-quality Chinese text corpus by cleaning and filtering enormous raw data from multiple sources. A big data management platform is built to accelerate the massive data analysis and processing. Both manual and model-based evaluation measures are used to guide the data preprocessing and training data selection, as detailed in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset Construction</head><p>To construct a large-scale high-quality Chinese corpus, we collect nearly 80TB raw data from the public datasets (e.g., BaiDuQA, CAIL2018, Sogou-CA, etc.), web pages data from Common Crawl, encyclopedia, news and e-books. As shown in Figure <ref type="figure">2</ref>  <ref type="foot" target="#foot_5">9</ref> . With the distributed processing capability and the tools of our platform, the efficiency of the data analysis and processing is significantly improved (see Table <ref type="table" target="#tab_2">3</ref>.1 for the processing time). Next, we introduce the details of each step in the dataset construction process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Cleaning and Filtering</head><p>Among the five data sources as shown in Fig <ref type="figure">2</ref>, the Common Crawl data contributes the most amount to our corpus but unfortunately contains a significant amount of low-quality web pages. To improve the data quality, we first adopt the following rule-based text cleaning strategies over the raw web pages from Common Crawl:</p><p>? Remove the document which contains less than 60% Chinese characters, or less than 150 characters, or only the title of a webpage;</p><p>? Remove the special symbols and duplicated paragraphs in each document;</p><p>? Identify advertisements based on keywords and remove documents which contain advertisements;</p><p>? Convert all traditional Chinese text to simplified Chinese;</p><p>? Identify the navigation bar of the web page and remove it.</p><p>Then, three filters are applied to the preprocessed documents to further remove the harmful, advertising and low-quality documents.</p><p>? Sensitive word filtering: The original documents of Common Crawl include a lot of harmful or sensitive website contents which would mislead our generative model. Thus, we manually collect 724 sensitive words and remove documents containing more than three of the sensitive words.</p><p>? Model-based spam filtering: To further remove the advertisements and spams, we train a spam classification model using fastText<ref type="foot" target="#foot_6">10</ref> on a manually labeled dataset. The negative training examples are 10K junk documents manually selected from the Common Crawl dataset, and the positive examples are sampled from the highquality Chinese text corpus. We remove the documents that are classified as spams.</p><p>? Low-quality document filtering: Following the practice in GPT-3, we train a classifier to score the quality of each document and eliminate the documents with scores below a threshold (see Appendix A of <ref type="bibr" target="#b1">[1]</ref> for details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Text Deduplication</head><p>Although we have removed duplicated paragraphs in each document in the previous step, there are still documents with highly overlapped content across different data sources. Therefore, we carry out fuzzy data deduplication over the documents across all our data sources.</p><p>Due to the super large scale of the whole dataset, the conventional MinHashLSH algorithm in Spark incurs more than 8 hours to duplicate less than 200MB data, which is too slow to meet our efficiency requirement. To accelerate the deduplication process, we design a distributed large-scale text data duplication detection and deduplication algorithm by exploiting the computing framework of our big data management platform. The proposed algorithm takes only 3.5 hours to complete the deduplication process for 500GB documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Data Quality Evaluation</head><p>Give above preprocessing steps, one key question is how the cleaning rules and the filtering thresholds are decided. In this work, we evaluate the data quality after each round of preprocessing and update the cleaning rules and the filtering models according to the evaluation results. Both manual and model-based evaluations are considered. The manual evaluation is conducted over randomly sampled texts from the perspectives of sentence smoothness and the amount of low-quality contents (e.g., advertisements, repeated short sentences, spams, etc.). However, the manual evaluation can only cover a very small proportion of the whole dataset. To improve the accuracy of the data evaluation, we train the PanGu-? 350M model using 30GB data sampled from the preprocessed dataset and evaluate the data quality using the PPL on a high-quality development dataset. The preprocessed dataset that achieves lower PPL is considered to have higher quality and its corresponding cleaning rules and filtering models are considered to be better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training Data Selection</head><p>Using the construction process in Figure <ref type="figure">2</ref>, a Chinese text corpus with 1.1TB data is built from the five types of data sources. The composition of our corpus and the processing steps adopted to each data source is shown in Table <ref type="table" target="#tab_2">3</ref>.2.</p><p>Based on the new corpus, we construct two training datasets with 100GB and 1TB text data for our medium (2.6B and 13B) and large (200B) models, respectively. As shown in Table <ref type="table" target="#tab_2">3</ref>.2, each data source is sampled during training with different proportions according to the quality of the processed dataset evaluated using the method in Section 3.1.3. The distribution of the number of token in each training dataset is shown in Figure <ref type="figure" target="#fig_2">3</ref>. The averaged document lengths of the 100GB and 1TB dataset are 239 and 405 tokens, respectively. The 1TB dataset has a larger averaged document length due to the large proportion of Common Crawl dataset. Note that the length of the text will affect the generation performance of the model. When the averaged number of token for the training samples is small, the model will be biased to generate short texts and be good at processing downstream tasks requiring short texts, and vice versa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">System</head><p>Training PanGu-? 200B and using it for inference are difficult. The memory requirement for just storing PanGu-? 200B is around 750 GB. Training such a huge model consumes several times more memory than just storing the parameters, since the gradients and optimizer states are also essential for updating the parameters. As a contrast, the memory of modern AI processors (e.g., GPU, Ascend 910 AI processor <ref type="bibr" target="#b12">[12]</ref>) is still around 30-40 GB. Thus, it is inevitable to partition the model to a collection of devices (processors). The problem is challenging in two perspectives. First, multiple basic parallel functionalities should be combined to acquire the end-to-end high performance. Finding the best     Each parallelism dimension trades computation (or communication) overheads for memory (or throughput) benefits. To acquire maximum end-to-end throughput, a balanced composition point should be found along these dimensions. The problem becomes more challenging when considering the heterogeneous bandwidths in a cluster of devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gradients Optimizer states</head><p>Figure <ref type="figure" target="#fig_5">5</ref>(b) demonstrates a typical organization of a cluster. Each server includes multiple devices, and the servers in a rack are connected by a ToR (top of rack) switch. Racks are then connected by the Spine switch. The bandwidth between devices in a server is greater than that across servers in a rack, and the latter one is greater than that across racks. Therefore, the model is partitioned across servers in a rack using the pipeline parallelism regime, resulting in that each server holds a stage of the model layers. Then, the stage is split using the op-level parallelism across the devices in each server, in order to utilize the high bandwidths. Each rack owns the whole model, and different racks are data parallel. Deploying data parallelism and optimizer parallelism across racks is due to that the induced communication operators are not on the critical path of the training iteration, which could be fused and overlapped with backward propagation to improve the performance.</p><p>Figure <ref type="figure" target="#fig_7">6</ref> shows how a combined parallelization is applied to the PanGu-? 200B model. First, 64 layers of the model are partitioned into 16 stages, each stage containing 4 layers. For each layer, involved parameters and tensors are partitioned for each operator. Specifically, the parameters involved in query (Q), key (K) and value (V ) operators are partitioned into 8 slices. The input tensor of these three operators is partitioned into 16 slices, and the number of optimizer model parallelism is determined accordingly. <ref type="foot" target="#foot_8">12</ref> Parallelization strategies for other operators in the layer are configured likewise. Rematerialization is configured to perform within each layer, which limits the extra computation overheads. Totally, 2048 Ascend 910 AI processors are used to train the full PanGu-? 200B model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation</head><p>The parallel-related functionalities are implemented in the Auto-parallel module of MindSpore. The Auto-parallel decouples machine learning models from complicated underlying parallel implementations, and let researchers focus on  Figure <ref type="figure">7</ref> shows how to specify the combined parallelization strategy to PanGu-?. Figure <ref type="figure">7</ref>(a) and Figure <ref type="figure">7</ref>(b) shows the pseudocode of configuring Attention and FeedForward to conduct op-level parallelism, respectively. qkv_mm's sharding strategy is ((2, 1), (1, 2)), indicating that x is partitioned along the row (batch or data) dimension into 2 slices, while q_w, k_w and v_w are partitioned along the column dimension. Since the device number is 4 here, each device holds a distinct pair of a x's slice and a q_w's (k_w's and v_w's) slice. matmul's sharding strategy is ((2, 2), (2, 1)), where the contracting dimension is partitioned, thus an AllReduce is needed here to perform the operation. Likewise, another AllReduce is needed in Figure <ref type="figure">7</ref>(b)'s matmul2. Auto-parallel can find such needed operators. Furthermore, the tensor redistribution is designed to automatically find the transformation (a list of operators) between any two inconsistent distributed tensor layouts with minimum communication cost, and then the operators are inserted into the data flow graph. The sharding strategy of batch_mm in Figure <ref type="figure">7</ref>(a) corresponds to splitting the batch and head dimension.</p><p>Figure <ref type="figure">7</ref>(d) shows the pseudocode of conducting pipeline parallelism in MindSpore. The number of stages is configured as 2, and the number of devices is 8. Thus, 4 devices together perform each stage. The layer1 is configured to be the stage 0, thus replicated on 4 devices. Likewise, layer2 is replicated on the other 4 devices. If combined with Figure <ref type="figure">7</ref>(a) and Figure <ref type="figure">7</ref>(b), the desired parallelization strategy is obtained to PanGu-?. <ref type="foot" target="#foot_9">13</ref> Send and Receive are inferred to communicate the activation output from stage 0 to stage 1, and then are automatically inserted into the data flow graphs on two stages, respectively.</p><p>In the future, we will: a) develop a cost model and a parallelization strategy searching algorithm for all parallelism dimensions in order to completely liberate developers from the underlying parallel-related works; b) support the heterogeneous-parallelism to offload a part of tensors and the corresponding computations to the host CPU to accelerate the training; c) use Sparse Attention to speedup the computation.</p><p>All training and inference jobs are run on the ModelArts<ref type="foot" target="#foot_10">14</ref> platform, which manages the end-to-end workflows and provides the functionality of cluster scheduling for a job to acquire a hierarchical cluster.   The curves of training loss for the PanGu-? models are shown in Figure <ref type="figure" target="#fig_10">8</ref>. We adopt the number of training tokens as the x-axis since the batch size for the 200B model is not comparable to that of the 13B and 2.6B models. The loss of 200B model converges to around 2.49, while the losses of 13B and 2.6B models converge to 2.58 and 2.64 respectively. From the training curves, we can observed that the losses are still decreasing by the end of training, which indicates that our PanGu-? model are still under-trained, and may have great potential to improve. We also evaluate the perplexity of our PanGu-? models on the validation set, which is randomly sampled from the Common Crawl dataset. The results in Table <ref type="table" target="#tab_6">6</ref> show that PanGu-? models with larger parameters sizes achieve smaller perplexity values, indicating that larger PanGu-? models are better language models.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Task Description</head><p>In this section, we evaluate our models on a broad spectrum of natural language processing tasks. Similar to the GPT-3 <ref type="bibr" target="#b1">[1]</ref>, the experiments are conducted under three learning settings, i.e., zero-shot, one-shot, and few-shot, without any finetuning. For each task, we evaluate the models with the test sets when publicly available. Otherwise, we use the development sets instead. For some tasks with a very large test set or development set, we randomly sample a subset from the dataset in the experiments to reduce the computational cost. The evaluation datasets are classified into 7 categories by the task similarities, and we describe each category as follows.</p><p>Cloze and completion tasks, including WPLC, CHID <ref type="bibr" target="#b30">[30]</ref>, PD&amp;CFT <ref type="bibr" target="#b31">[31]</ref>, CMRC2017 <ref type="bibr" target="#b32">[32]</ref>, and CMRC2019 <ref type="bibr" target="#b33">[33]</ref>. Chinese WPLC (Word Prediciton with Long Context) is a dataset created to test the ability to model long-range dependencies, similar to the LAMBADA dataset <ref type="bibr" target="#b34">[34]</ref> for English. The CHID (Chinese IDiom dataset) requires the model to identify the ground-truth idiom from 10 candidate idioms. The PD&amp;CFT task requires the model to predict the mask words in sentences derived from People's Daily (PD) news dataset and Children's Fairy Tale (CFT) dataset. The CMRC2017 (Chinese Machine Reading Comprehension) task contains two different sub-task: cloze-style task and user query reading comprehension task, among which we only evaluate our models on the cloze-style task. While the aforementioned tasks are word-level tasks, the CMRC2019 is a sentence cloze-style dataset that involves filling the right sentence from several candidate sentences into the passage. For the CMRC2019 and the CHID, a list of candidate choices are provided, making them classification tasks, while for WPLC, CMRC2017 and PD&amp;CFT, the models need to generate the answer as no candidate choices are given. Accuracy metric is employed for evaluating the cloze-style tasks.  Reading comprehension tasks, including CMRC2018 <ref type="bibr" target="#b35">[35]</ref>, DRCD <ref type="bibr" target="#b36">[36]</ref>, and DuReader <ref type="bibr" target="#b37">[37]</ref>. These are all spanextraction tasks originally. That is, given a passage as context and a question, the models need to extract a text span from the passage which contains the correct answer to the question. The evaluation metrics, including F1 and exact match (EM), measure the similarity between the predicted span and the ground-truth text span. Instead of span-extraction, we formulate these task as generation tasks where the models generate the texts directly. The similarity between the generated text span and the ground-truth text span is evaluated. Note that for the DuReader task, we select the Zhidao subset for evaluation in our experiment.</p><p>Closed-book question answering (QA) tasks, including WebQA <ref type="bibr" target="#b38">[38]</ref>. We follow the same closed-book setting in GPT-3 <ref type="bibr" target="#b1">[1]</ref>, where the models are not allowed to access any external knowledge when answering open-domain factoid questions about broad factual knowledge.</p><p>Winograd-Style tasks, including CLUEWSC2020 <ref type="bibr" target="#b39">[39]</ref>. CLUEWSC2020 is a Chinese Winograd Schema Challenge dataset, which is an anaphora/coreference resolution task. In practice, we convert the task into a multiple-choice classification problem.</p><p>Common sense reasoning tasks, including C 3 <ref type="bibr" target="#b39">[39]</ref>. C 3 is a free-form multiple-choice reading comprehension dataset which can benefit from common sense reasoning. Different from the extraction-based reading comprehension tasks, the answers to of C 3 questions cannot be directly found in the given context. Therefore, we use it to evaluate the common sense reasoning ability of the models.</p><p>Natural language inference (NLI) tasks, including Chinese Multi-Genre NLI (CMNLI) and Original Chinese Natural Language Inference (OCNLI) <ref type="bibr" target="#b39">[39]</ref>. The NLI tasks require the model to identify the relation between two sentences, either entailment, neutral or contradiction. We formulate these tasks as three-class classification problems.</p><p>Text classification tasks, including TouTiao Text Classification for News Titles (TNEWS), IFLYTEK app description classification (IFLYTEK), Ant Financial Question Matching Corpus (AFQMC), and Chinese Scientific Literature (CSL) <ref type="bibr" target="#b39">[39]</ref>. These text classification tasks covers broad domains of text, including news, applications, financial text, scientific text. For the TNEWS and IFLYTEK tasks, there are 15 and 119 categories originally. However, we randomly sample three candidates as negative labels for each instance and perform 4-class classification. The reason is that the computational cost of our perplexity-based classification method increases linearly to the total number of candidate categories, which will be described in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Evaluation Details</head><p>The tasks can be generally classified into two-categories: classification tasks and generation tasks. For the classification tasks, we resolve the task as perplexity comparison tasks. For some tasks, the samples needs to be filled into a tailor-designed template as the input to the models. The templates for each task are described in Table <ref type="table" target="#tab_7">7</ref>, where "/" means the task does not involve a template. The decoding strategies for these text generation tasks are described in Table <ref type="table" target="#tab_8">8</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Generation method</head><p>The generation tasks include word-level generation tasks and sentence-level generation tasks. Since our PanGu-? models are autoregressive language models capable of text generation, the generation tasks can be solved naturally by simply generating the answers. For the cloze tasks such as WPLC, PD&amp;CFT, and CMRC2017, the prompts are the context before the positions to be predicted. For the reading comprehension tasks and closed book QA tasks, templates are designed if necessary. For example, in the reading comprehension tasks, the sample is filled into a template Reading document : $Document Question: $Question Answer:, which serves as the prompt for the model to generate the answer.</p><p>As in GPT-3, the few-shot task is designed as in-context learning, where K prompts are concatenated one by one. The first K -1 prompts contain the ground truth answer while the last prompt is the sample we want to predict. An example for CMRC2018 task is shown in Figure <ref type="figure">9</ref> Figure <ref type="figure">9</ref>: A prompt for generation task of CMRC2018</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Perplexity-based method</head><p>The perplexity-based method solves the classifications tasks. For each pair of &lt;text, label&gt;, an input will be generated automatically according to a pre-designed criteria, as shown in Table <ref type="table" target="#tab_7">7</ref>. The sequence generated by the template will be fed into the model and a perplexity value will be computed. The label associated with the smallest perplexity value will be considered as the predicted label for this passage.</p><p>We also employ the in-context learning strategy for solving few-shot tasks. An example for few-shot OCNLI task is shown in Figure <ref type="figure" target="#fig_12">10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Results</head><p>Table <ref type="table" target="#tab_9">9</ref> compares PanGu-? 2.6B with CPM [3]<ref type="foot" target="#foot_11">15</ref> , a recently released generative Chinese PLM with 2.6B parameters, on 16 downstream tasks in Chinese. PanGu-? 2.6B achieves higher performance compared to CPM 2.6B on more than 11 tasks in zero-shot setting, 12 tasks on the one-shot setting, and 14 tasks on the few-shot setting. In general, the experimental results indicate that PanGu-? 2.6B achieves higher in-context learning ability over CPM 2.6B, especially for few-shot learning and generation-tasks. Regarding generation-tasks, PanGu-? 2.6B outperforms CPM 2.6B with an improvement of 6 points on average. To be more specific, PanGu-? 2.6B surpasses CPM 2.6B with 5 points in  scores for both reading comprehension and closed-book QA tasks, 7 points in scores for cloze (without choices) tasks respectively. Regarding perplexity-tasks, PanGu-?is comparable to CPM 2.6B on natural language inference with CMNLI and OCNLI datasets, while it is slightly worse than CPM on classification tasks with TNEWS and IFLYTEK datasets. We suppose that the main factor that contributes to the different performance of CPM 2.6B and PanGu-? 2.6B is the training data. We collect massive and diverse data from a wide range of sources, which allows our PanGu-? model to handle more diverse tasks.  </p><formula xml:id="formula_9">????????? Generation ????????? Prompt ?????????????????????? Generation ???????????????????????????????????????? Prompt ???????? Generation ????????????????????????4?(?)???????? ???????????????????????????</formula><p>Table <ref type="table" target="#tab_10">10</ref> compares PanGu-? 13B with PanGu-? 2.6B. PanGu-? 13B outperforms PanGu-? 2.6B on all generation-tasks and most of the perplexity-tasks. Regarding CMRC2018, DRCD and WebQA tasks of PanGu-? 13B, the few-shot performance surpasses zero-shot by more than 10 points, demonstrating that PanGu-? 13B has superior in-context learning ability. PanGu-? 13B outperforms PanGu-? 2.6B with an improvement of 3 points on average. To be more specific, PanGu-? 13B surpasses PanGu-?2.6B with 4 points for both reading comprehension and closed-book QA tasks, 2 points for cloze (without choices) tasks respectively. Regarding the NLI tasks, the 13B model performs worse than the 2.6B model, which is consistent with the observations in GPT-3. Overall, the comparison results between PanGu-? 13B with PanGu-? 2.6B demostrate that a larger scale of pretrained model generally improves the performance on few-shot learning tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Natural Language Generation Examples</head><p>We evaluate the generation capabilities of PanGu-? 200B on various text generation scenarios. We show some of the examples in this section. We do not conduct any post-editing to the generated text, except that we truncate the generated text when the model does not stop generation at a reasonable point. Among the scenarios we have tested, we find that our PanGu-? model is particularly good at poetry&amp;duilian generation, text summarization, dialog generation, and fiction generation, where roughly 90% of the generated examples are acceptable to human. We believe there are certainly more applications for PanGu-? models to explore in the future. </p><formula xml:id="formula_10">????????????????????????????????????? ????????????????????????????????????? ????????????????????????????????????? ????????????????????????????????????? ????????????????????????????????????? ????????????????????????????????????? ????????????????????????????????????? ????????????????????????????????????? ????????????????????????????????????? ????????????????????????????????????? ????? Prompt ??????????? Generation ???????????????????????????????????? ???????????????????????????????????? ???????????????????????????????????? ????????????????????????????????????? ????????????????????????????????????? ????????????????????????????????????? ????????????????????????????????????? ????????????????????????????????????? ????????????????????????????????????? ???????????????????????????????"?"???? ??"?"????????????????????????????????? ???</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have pretrained large-scale Chinese autoregressive language models named PanGu-?, with up to 200 billion parameters. PanGu-? has been developed under the MindSpore framework and trained on a cluster of 2048 Ascend AI processors. We believe there are many open problems in the field of large-scale PLMs:</p><p>? Large-scale language models have demonstrated its promising few-shot capabilities in NLP tasks. However, the behaviors of such models are not systematically studied yet. How to make proper use of large PLMs and how to develop efficient few-shot algorithms remain open questions. ? Though effective, the computational cost for the inference of super large language models is still expensive.</p><p>Thus it is worthwhile studying how to save the cost for the inference of large PLMs without sacrificing much of their performance. Model compression and acceleration of large PLMs could be an interesting topic. ? Training a even larger PLM with trillions of parameters will certainly bring more challenges to the both software and hardware sides. In addition, more efficient model structures such MoE <ref type="bibr" target="#b40">[40]</ref> or Switch Transformers <ref type="bibr" target="#b41">[41]</ref> are also expected for relieving the computational cost of model training and inference. ? Pretrained multi-modal models integrating language, vision and speech data have attracted much attention recently <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b16">16]</ref>. Similar to the scaling law of language models, the performance of pertrained multi-modal models may also improve when the model sizes increase and more training data are collected. This is definitely a promising direction to explore. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>6 Figure 1 :</head><label>61</label><figDesc>Figure 1: The architecture of PanGu-?. The model is based on a uni-directional Transformer decoder. A query layer is stacked on top of Transformer layers with the position embedding as the query in the attention mechanism to generate the token at the next position.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Table 1 :Figure 2 :</head><label>12</label><figDesc>Figure 2: The data sources and the process of constructing pretraining data for PanGu-?.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The distribution of tokens in (a) 1TB dataset and (b) 100GB dataset. The total number of tokens represents the (number of tokens in each document) ? (number of documents with this token number).</figDesc><graphic url="image-6.png" coords="8,75.14,72.00,226.10,113.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(d) Optimizer states and gradients are partitioned along data parallelism in optimizer model parallelism (e) Some activation memories are abandoned in the forward phase to reduce the peak memory consumption</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Five parallel functionalities, and how each works to optimize memory and throughput.</figDesc><graphic url="image-49.png" coords="8,76.68,435.24,82.74,107.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: A combined parallelization of the model, and how it is scheduled to the cluster.</figDesc><graphic url="image-122.png" coords="9,207.88,98.52,86.62,62.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: A simplified PanGu-?'s parallelization strategy. The ellipsoids stand for the operators, blue rectangles represent tensors, and green rectangles represent trainable parameters. Parameters are partitioned along the row and column dimension respectively, and the input tensor is partitioned along the row dimension. And, two layers are assigned to different pipeline stages.</figDesc><graphic url="image-300.png" coords="10,85.07,185.61,443.48,102.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>( b )Figure 7 :</head><label>b7</label><figDesc>Figure 7: The pseudocode of configuring op-level and pipeline parallelism in MindSpore. The red bold fonts are keywords to specify parallelization strategies.</figDesc><graphic url="image-310.png" coords="11,166.94,283.08,219.82,74.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Our PanGu-? models are developed under the Mindspore framework and are trained on a cluster of 2048 Ascend 910 AI processors. The detailed settings are shown in Table5. For the training of the 200B model, we use 2048 Ascend processors at the first phase and then switch to 1024 Ascends processors in the middle, in order to conduct other experiments using the rest of resources. The Byte Pair Encoding (BPE) is used as the tokenizer, and the vocabulary size are 40,000. The sequence length for the training data is set to 1024 for all the models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Training curves of three PanGu-? models with different model sizes. The x-axis denotes the number of training tokens, which is measured as training_steps * batch_size * sequence_length. The y-axis denotes the training loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>$Question\n??(Read document: $Document\nQuestion?$Question\nAnswer: ) ?????$Document\n??$Question\n??(Read document: $Document\nQuestion?$Question\nAnswer: ) ?????$Document\n??$Question\n??(Read document: $Document\nQuestion?$Question\nAnswer: Closed book QA WebQA ??$Question\n??(Question?$Question\nAnswer: ) Winograd-Style CLUEWSC2020 / Common sense reasoning C 3 ?: $Question\n?:$Choice\n???????: $Passage (Question: $Question\nAnswer:$Choice\nAnswer from dialogue: $Passage) NLI CMNLI OCNLI $S1??/??/??$S2 ($S1?Yes/Maybe/No, $S2) $S1??/??/??$S2 ($S1?Yes/Maybe/No, $S2) $passage (This passage is about$label: $passage) ????$label??????$passage (This application is about: $passage) ??????????/???$S1?$S2 (The following two sentences have the same/different semantics: $S1. $S2) ???$passage?????$keyword?/??????? (Abstract: $passage, keywords: $keyword True/False keywords)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Prompt for perplexity-based tasks of OCNLI</figDesc><graphic url="image-312.png" coords="15,136.62,72.00,338.76,99.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>5 , a deep learning training/inference framework that could be used for mobile, edge and cloud scenarios. Besides the basic parallel functionalities, Auto-parallel is easy enough to use by freeing developers from parallel model training with minimal (or zero) code modifications from the standalone version, as if the model is trained on a single device.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Processing time for each step in the dataset construction.</figDesc><table><row><cell>, our data construction process includes three steps: rule-based data cleaning, model-based</cell></row><row><cell>data filtering and text deduplication. To improve the quality of the training dataset, the first two steps (i.e., cleaning</cell></row><row><cell>and filtering) are iteratively enhanced via manual and model-based data quality evaluations. The data construction</cell></row><row><cell>process is done on a big data management platform built based on the open source Spark/Hadoop framework using</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Data composition of the 1.1TB Chinese text corpus.</figDesc><table><row><cell></cell><cell>Size (GB)</cell><cell>Data source</cell><cell>Processing steps</cell></row><row><cell>Public datasets</cell><cell>27.9</cell><cell>15 public datasets including DuReader, BaiDuQA, CAIL2018, Sogou-CA, etc.</cell><cell>Format conversion 11 and text deduplication</cell></row><row><cell>Encyclopedia</cell><cell>22</cell><cell>Baidu Baike, Sogou Baike, etc.</cell><cell>Text deduplication</cell></row><row><cell>e-Books</cell><cell>299</cell><cell>e-Books on various topics (e,g., novels, his-tory, poetry, ancient prose, etc.).</cell><cell>Sensitive word and model-based spam filtering</cell></row><row><cell>Common Crawl</cell><cell>714.9</cell><cell>Web data from January 2018 to December 2020 from Common Crawl.</cell><cell>All steps</cell></row><row><cell>News</cell><cell>35.5</cell><cell>News data from 1992 to 2011.</cell><cell>Text deduplication</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Sampling strategy of the corpora in training PanGu-? models.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">PanGu-? 200B</cell><cell cols="2">PanGu-? 2.6B&amp;13B</cell></row><row><cell></cell><cell>Quantity</cell><cell>Weight in</cell><cell>Epochs elapsed</cell><cell>Quantity</cell><cell>Weight in</cell></row><row><cell></cell><cell>(tokens)</cell><cell>training mix</cell><cell>when training</cell><cell>(tokens)</cell><cell>training mix</cell></row><row><cell>Public datasets</cell><cell>25.8B</cell><cell>10.23%</cell><cell>3.65</cell><cell>7B</cell><cell>27.99%</cell></row><row><cell>e-Books</cell><cell>30.9B</cell><cell>12.23%</cell><cell>0.41</cell><cell>5.6B</cell><cell>18%</cell></row><row><cell>Common Crawl</cell><cell>176.2B</cell><cell>62.81%</cell><cell>0.85</cell><cell>2.5B</cell><cell>10%</cell></row><row><cell>News</cell><cell>19.8B</cell><cell>7.83%</cell><cell>2.2</cell><cell>5.6B</cell><cell>22%</cell></row><row><cell>Encyclopedia data</cell><cell>5.8B</cell><cell>6.9%</cell><cell>3</cell><cell>5.8B</cell><cell>23%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>The detailed settings for training PanGu-? models.</figDesc><table><row><cell>Models</cell><cell>#Training Steps</cell><cell cols="2">#Ascend processors Adam Betas</cell><cell cols="2">Learning Rate Weight Decay</cell></row><row><cell cols="2">PanGu-? 2.6B 0?70,000</cell><cell>512</cell><cell cols="2">? 1 =0.9 ,? 2 =0.999 1e-4</cell><cell>0.01</cell></row><row><cell>PanGu-? 13B</cell><cell>0?84,000</cell><cell>1024</cell><cell>? 1 =0.9 ,? 2 =0.98</cell><cell>5e-5</cell><cell>0.01</cell></row><row><cell>PanGu-? 200B</cell><cell cols="2">0?130,000 130,000?260,000 1024 2048</cell><cell>? 1 =0.9 ,? 2 =0.95</cell><cell>2e-5</cell><cell>0.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>The validation perplexity of the PanGu-? models.</figDesc><table><row><cell>Models</cell><cell>Validation PPL</cell></row><row><cell>PanGu-? 2.6B</cell><cell>19.33</cell></row><row><cell>PanGu-? 13B</cell><cell>17.69</cell></row><row><cell>PanGu-? 200B</cell><cell>15.59</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>The input&amp;prompt template for each task.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>The decoding strategies for text generation tasks.</figDesc><table><row><cell>Task</cell><cell>Dataset</cell><cell>Decoding strategies</cell></row><row><cell></cell><cell>WPLC</cell><cell>top-k, k=1</cell></row><row><cell>Cloze and completion</cell><cell>PD&amp;CFT</cell><cell>top-k, k=1,temperature=0.9</cell></row><row><cell></cell><cell>CMRC2017</cell><cell>top-p, p=0.9, temperature=1</cell></row><row><cell></cell><cell>CMRC2018</cell><cell>top-p, p=0.8, temperature=0.8</cell></row><row><cell>Reading comprehension</cell><cell>DRCD</cell><cell>top-p, p=0.8, temperature=0.8</cell></row><row><cell></cell><cell>DuReader</cell><cell>top-p, p=0.9, temperature=0.7</cell></row><row><cell>Closed book QA</cell><cell>WebQA</cell><cell>top-k, k=5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Performance comparison of CPM 2.6B v.s. PanGu-? 2.6B on few-shot NLP tasks.</figDesc><table><row><cell>Zero-Shot</cell><cell>One-Shot</cell><cell>Few-Shot</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Performance comparison of PanGu-? 2.6B v.s. PanGu-? 13B on few-shot NLP tasks.</figDesc><table><row><cell>Zero-Shot</cell><cell>One-Shot</cell><cell>Few-Shot</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 :</head><label>11</label><figDesc>An example of the reading comprehension by PanGu-? model.</figDesc><table><row><cell></cell><cell>Reading Comprehension</cell></row><row><cell></cell><cell>???????????????(??)?????????????????</cell></row><row><cell></cell><cell>?????????????????????????????????????</cell></row><row><cell></cell><cell>?????????????????????????????????????</cell></row><row><cell></cell><cell>?236????412001????????????????????????????</cell></row><row><cell></cell><cell>?????????????????????????????????????</cell></row><row><cell>Prompt</cell><cell>????????????????????????????????????? ?????????????????????????????????????</cell></row><row><cell></cell><cell>????????????????????????????????????? ?????????????????????????????????????</cell></row><row><cell></cell><cell>?????????????????????????????????????</cell></row><row><cell></cell><cell>???????????????????? ?????????????????</cell></row><row><cell></cell><cell>??</cell></row><row><cell cols="2">Generation ????</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 12 :</head><label>12</label><figDesc>An example of open domain QA by PanGu-? model. The second example implies that PanGu-? model does not simply act like a retrieval model to answer open domain questions, but can understand and decompose the questions and generated answers accordingly.</figDesc><table><row><cell>Open Domain Question Answering</cell></row><row><cell>Prompt</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 13 :</head><label>13</label><figDesc>PanGu-? model can generate Gushi and Duilian without finetuning on the domain-specific data.</figDesc><table><row><cell>Gushi &amp; Duilian Generation</cell></row><row><cell>Prompt Generation ???????????????????????????????? ??????????????</cell></row><row><cell>Prompt Generation ?????????? ??????????</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 15 :</head><label>15</label><figDesc>Examples of QA with long answers by PanGu-?. These questions are from Zhihu.com. Overall, the answers seems plausible, but still contain irrelevant or repetitive or illogical sentences.</figDesc><table><row><cell></cell><cell>Article Generation</cell></row><row><cell>Prompt</cell><cell>???????????</cell></row><row><cell>Generation</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 16 :</head><label>16</label><figDesc>Our PanGu-? model is good at dialog generation. Given a shot conversation as prompt, the model generates several rounds of conversations automatically. This implies that PanGu-? model has potentials to assist humans in automatic reply in customer service, or design dialogues for NPC (Non-Player Character) in games. Source of Prompt: https://www.zhihu.com/question/313316339/answer/623572659. Author???????.</figDesc><table><row><cell></cell><cell>Game Dialog Generation</cell></row><row><cell></cell><cell>???????????????????</cell></row><row><cell></cell><cell>???(???????????????????)</cell></row><row><cell></cell><cell>????????????????</cell></row><row><cell></cell><cell>????????????????????????????????????</cell></row><row><cell>Prompt</cell><cell>??????????????????????? ?????????????????????????????????????</cell></row><row><cell></cell><cell>?????????????????????????????????????</cell></row><row><cell></cell><cell>???</cell></row><row><cell></cell><cell>?????????????????????????????????????</cell></row><row><cell></cell><cell>?????????????? ?????????????????????????????????????</cell></row><row><cell></cell><cell>??????</cell></row><row><cell></cell><cell>?????????????????????????????????????</cell></row><row><cell></cell><cell>?????????????????</cell></row><row><cell></cell><cell>????????????????????????????????????? ???????????????????????????????</cell></row><row><cell></cell><cell>?????????????????????????????????????</cell></row><row><cell></cell><cell>????????????????????????????????????</cell></row><row><cell></cell><cell>??????????????????????????</cell></row><row><cell></cell><cell>?????????????????????????????????????</cell></row><row><cell></cell><cell>?????;???????????????????????????????</cell></row><row><cell>Generation1</cell><cell>???????????????</cell></row><row><cell></cell><cell>?????????????????????????????</cell></row><row><cell></cell><cell>?????????????????????????????????????</cell></row><row><cell></cell><cell>?????????????????????????????????????</cell></row><row><cell></cell><cell>?????????????????????????????????????</cell></row><row><cell></cell><cell>??????????????????????????</cell></row><row><cell></cell><cell>?????????????????????????????????????</cell></row><row><cell></cell><cell>?????????</cell></row><row><cell></cell><cell>?????????????????????????????????????</cell></row><row><cell></cell><cell>???????????????????????????</cell></row><row><cell></cell><cell>????????????????????????????</cell></row><row><cell></cell><cell>??????????</cell></row><row><cell></cell><cell>????????????????????????????????????</cell></row><row><cell></cell><cell>?????????????????????????????????????</cell></row><row><cell></cell><cell>??????????????????;??????????????????</cell></row><row><cell></cell><cell>???????????????????????</cell></row><row><cell></cell><cell>?????????????????????????????????????</cell></row><row><cell></cell><cell>??????????????</cell></row><row><cell></cell><cell>????...?????????????</cell></row><row><cell></cell><cell>?????????</cell></row><row><cell></cell><cell>??????????</cell></row><row><cell></cell><cell>???????????????????</cell></row><row><cell></cell><cell>?????????????</cell></row><row><cell></cell><cell>?????</cell></row><row><cell>Generation2</cell><cell>?????????????????????????????????????</cell></row><row><cell></cell><cell>?????????????????</cell></row><row><cell></cell><cell>?????????????</cell></row><row><cell></cell><cell>????????????????????????????????????</cell></row><row><cell></cell><cell>????????????????????????????????????? ?????????????????????????</cell></row><row><cell></cell><cell>?????????????????????????????</cell></row><row><cell></cell><cell>?????????????????????????????????????</cell></row><row><cell></cell><cell>????????????????????????????????????? ????????????</cell></row><row><cell></cell><cell>?????</cell></row><row><cell></cell><cell>???????</cell></row><row><cell></cell><cell>??????????????</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 17 :</head><label>17</label><figDesc>We find that our PanGu-? model is very good at writing fiction in different styles. This could be due to the fact that our train data contains lots of e-books. Source: ??????and??????, respectively.</figDesc><table><row><cell></cell><cell>Fiction Generation</cell></row><row><cell>Prompt</cell><cell>????????????????????????????????????? ?????????????????????????????????????</cell></row><row><cell>Generation</cell><cell>???????????????????????????? ????????????????????????????????????? ????????????????????????????????????? ????????????????????????????????????? ??????????????????"??"???????????????? ?????????????????????????????????????</cell></row><row><cell></cell><cell>??????????????????????</cell></row><row><cell></cell><cell>"???,?????????????????,??????????????,??</cell></row><row><cell></cell><cell>????????????"</cell></row><row><cell></cell><cell>??????????????,????????</cell></row><row><cell></cell><cell>??????????,???????,??????????</cell></row><row><cell></cell><cell>????,?????????????:"???,????!"</cell></row><row><cell></cell><cell>???????,????????????????????</cell></row><row><cell></cell><cell>????,???????,????,??????????</cell></row><row><cell></cell><cell>??????,???????????,?????????,??????????</cell></row><row><cell></cell><cell>?????????</cell></row><row><cell></cell><cell>"??...????????,??????!"</cell></row><row><cell></cell><cell>????????,"????,????,?????????????,?????? ?,???????"</cell></row><row><cell></cell><cell>"?????!"??????????,??????,????????????</cell></row><row><cell></cell><cell>???????????????????,?????????????????? ?????????,????????</cell></row><row><cell></cell><cell>???????????,?????????,???????</cell></row><row><cell></cell><cell>???????????,?????????:"??!?????????????</cell></row><row><cell></cell><cell>?,????????????"</cell></row><row><cell></cell><cell>????????:"??????</cell></row></table><note><p>Prompt "??...??????????????????????????" ?????? ?????????????? ??????????"???????????? ???????????" Generation "??...?????"????????????,"????????,???????? ????,?????????????!" "??????" ?????????,????????:"??????,?????"</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0"><p>https://www.hiascend.com/en/software/cann</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1"><p>https://gitee.com/mindspore/mindspore</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2"><p>https://data.baai.ac.cn/data-set-details/0c8dc71dd06ae75a10ca422fb49b0751</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_3"><p>https://www.sogou.com/labs/resource/t.php</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_4"><p>https://commoncrawl.org/the-data/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_5"><p>4 computing nodes with 28TB storage + 2 CUPs (24 cores) + 1.5TB Memory and 4 computing nodes with 7.3TB storage + 2 CPUs (64 cores) + 1TB memory.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_6"><p>https://fasttext.cc/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_7"><p>We remove the labels in all the labeled datasets such that the model is trained for few-shot learning instead of multi-task learning.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_8"><p>The '8' is called model parallel number and '16' is called data (and optimizer) parallel number in our system. In the example of Figure6, the model parallel number and data parallel number are both 2.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_9"><p>The stategy of optimizer parallelism is hidden in how batch dimension is split in the configuration. We omit the configuration for rematerialization here.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_10"><p>https://www.huaweicloud.com/product/modelarts.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15" xml:id="foot_11"><p>https://github.com/TsinghuaAI/CPM-Generate</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7">Acknowledgements</head><p>We thank <rs type="person">Hanyang Wan</rs>, <rs type="person">Qian Zhao</rs>, <rs type="person">Yong Li</rs>, <rs type="person">Zhou Cao</rs>, <rs type="person">Yongqiang Lai</rs>, <rs type="person">Zhijian Guo</rs>, <rs type="person">Yue Wang</rs>, <rs type="person">Zherui Chang</rs>, <rs type="person">Junqiu Wei</rs>, <rs type="person">Pingyi Zhou</rs>, <rs type="person">Yulong Ao</rs>, <rs type="person">Wenzhi Liu</rs> for their great support to this work. Also thanks for the support by the <rs type="funder">School of Electronics Engineering and Computer Science at Peking University</rs>, <rs type="institution">Central Software Institute and Noah's Ark Lab at Huawei Technologies</rs>, and <rs type="funder">Peng Cheng Laboratory</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Our PanGu-? model is especially good at text summarization. We tried several latest news articles which are not in our training data. For more than 90% of the input article, our PanGu-? model can generate satisfactory results</title>
		<idno>??????????????3GWifi???DA6810?????????? ????????????DA3100???????????&quot;ME909</idno>
		<ptr target="https://new.qq.com/omn/20210421/20210421A079J800.html" />
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>Text Summarization Prompt &quot;??????????????????2012????????????????? ?2012?????????????????????</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">B</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><surname>Amodei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pei</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxian</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusheng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haozhe</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fanchao</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaozhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoyang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huanqi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daixuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenbo</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><surname>Cpm</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.00413</idno>
		<title level="m">A large-scale generative Chinese pre-trained language model</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">XLNet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5753" to="5763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized BERT pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shikun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danxiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Hao Tian</surname></persName>
		</author>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09223</idno>
		<title level="m">ERNIE: Enhanced representation through knowledge integration</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ERNIE: Enhanced language representation with informative entities</title>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1441" to="1451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Junqiu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaozhe</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoguang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Neural contextualized representation for chinese language understanding</publisher>
			<pubPlace>Nezha</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">DaVinci: A scalable architecture for neural network computing</title>
		<author>
			<persName><forename type="first">Heng</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajin</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiping</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Hot Chips 31 Symposium (HCS)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On layer normalization in the transformer architecture</title>
		<author>
			<persName><forename type="first">Ruibin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunchang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tieyan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10524" to="10533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Liang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianqian</forename><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.01355</idno>
		<title level="m">CLUECorpus2020: A large-scale Chinese corpus for pre-training language model</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName><forename type="first">An</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianyan</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhikang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinbao</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huiling</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Hongxia Yang. M6: A chinese multimodal pretrainer</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mesh-TensorFlow: Deep learning for supercomputers</title>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youlong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Penporn</forename><surname>Koanantakool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingsheng</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cliff</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blake</forename><surname>Hechtman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Exploring hidden dimensions in accelerating convolutional neural networks</title>
		<author>
			<persName><forename type="first">Zhihao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sina</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Aiken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Supporting very large models using automatic dataflow graph partitioning</title>
		<author>
			<persName><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chien-Chin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EuroSys</title>
		<meeting>EuroSys</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">GShard: Scaling giant models with conditional computation and automatic sharding</title>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.16668</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08053</idno>
		<title level="m">Megatron-LM: Training multi-billion parameter language models using model parallelism</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Accpar: Tensor partitioning for heterogeneous deep learning accelerators</title>
		<author>
			<persName><forename type="first">Linghao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youwei</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuehai</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiran</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. HPCA</title>
		<meeting>HPCA</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Beyond data and model parallelism for deep neural networks</title>
		<author>
			<persName><forename type="first">Zhihao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Aiken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. MLSys</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Talwalkar</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Smith</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</editor>
		<meeting>MLSys</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">GPipe: Efficient training of giant neural networks using pipeline parallelism</title>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youlong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">PipeDream: Generalized pipeline parallelism for DNN training</title>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Harlap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amar</forename><surname>Phanishayee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Seshadri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><forename type="middle">R</forename><surname>Devanur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><forename type="middle">R</forename><surname>Ganger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><forename type="middle">B</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SOSP</title>
		<meeting>SOSP</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">DAPPLE: A pipelined data parallel approach for training large models</title>
		<author>
			<persName><forename type="first">Shiqing</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongyan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoping</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lixue</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lansong</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. PPoPP</title>
		<meeting>PPoPP</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Efficient algorithms for device placement of dnn graph operators</title>
		<author>
			<persName><forename type="first">Amar</forename><surname>Jakub M Tarnawski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Phanishayee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Divya</forename><surname>Devanur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fanny</forename><forename type="middle">Nina</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName><surname>Paravecino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Hetpipe: Enabling large DNN training on (whimpy) heterogeneous GPU clusters through integration of pipelined model parallelism and data parallelism</title>
		<author>
			<persName><forename type="first">Jay</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gyeongchan</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><forename type="middle">M</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungmin</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaesik</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><forename type="middle">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Young Ri</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. USENIX ATC</title>
		<meeting>USENIX ATC</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Zero: Memory optimizations toward training trillion parameter models</title>
		<author>
			<persName><forename type="first">Samyam</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Rasley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olatunji</forename><surname>Ruwase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SC</title>
		<meeting>SC</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">ChID: A large-scale Chinese IDiom dataset for cloze test</title>
		<author>
			<persName><forename type="first">Chujie</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aixin</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="778" to="787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Consensus attention-based neural networks for Chinese reading comprehension</title>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhipeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoping</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1777" to="1786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dataset for the first evaluation on Chinese machine reading comprehension</title>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhipeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoping</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Language Resources and Evaluation (LREC)</title>
		<meeting>International Conference on Language Resources and Evaluation (LREC)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A sentence cloze dataset for Chinese machine reading comprehension</title>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhipeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoping</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6717" to="6723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The LAMBADA dataset: Word prediction requiring a broad discourse context</title>
		<author>
			<persName><forename type="first">Denis</forename><surname>Paperno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Germ?n</forename><surname>Kruszewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ngoc</forename><forename type="middle">Quan</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandro</forename><surname>Pezzelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gemma</forename><surname>Boleda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Fern?ndez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1525" to="1534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A span-extraction dataset for Chinese machine reading comprehension</title>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhipeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoping</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP-IJCNLP</title>
		<meeting>EMNLP-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5886" to="5891" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">DRCD: A Chinese machine reading comprehension dataset</title>
		<author>
			<persName><forename type="first">Chieh</forename><surname>Chih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trois</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiying</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName><surname>Tsai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.00920</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yajuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiqi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaoqiao</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05073</idno>
		<title level="m">Chinese machine reading comprehension dataset from real-world applications</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Dataset and neural recurrent sequence labeling model for open-domain factoid question answering</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06275</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">CLUE: A Chinese language understanding evaluation benchmark</title>
		<author>
			<persName><forename type="first">Liang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yudong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yechen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianqian</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weitang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rongzhao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijian</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanting</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yina</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zuoyu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">He</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoweihua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qipeng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cong</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinrui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengliang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05986</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Outrageously large neural networks: The sparsely-gated mixture-of-experts layer</title>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Azalia</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krzysztof</forename><surname>Maziarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity</title>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
