<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Nonparametric Scene Parsing: Label Transfer via Dense Scene Alignment</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ce</forename><surname>Liu</surname></persName>
							<email>celiu@csail.mit.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Artificial Intelligence Laboratory</orgName>
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jenny</forename><surname>Yuen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Artificial Intelligence Laboratory</orgName>
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
							<email>torralba@csail.mit.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Artificial Intelligence Laboratory</orgName>
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Nonparametric Scene Parsing: Label Transfer via Dense Scene Alignment</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CAA37FE832DC530B54D88B7FE5FDE94C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T04:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we propose a novel nonparametric approach for object recognition and scene parsing using dense scene alignment. Given an input image, we retrieve its best matches from a large database with annotated images using our modified, coarse-to-fine SIFT flow algorithm that aligns the structures within two images. Based on the dense scene correspondence obtained from the SIFT flow, our system warps the existing annotations, and integrates multiple cues in a Markov random field framework to segment and recognize the query image. Promising experimental results have been achieved by our nonparametric scene parsing system on a challenging database. Compared to existing object recognition approaches that require training for each object category, our system is easy to implement, has few parameters, and embeds contextual information naturally in the retrieval/alignment procedure.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Scene parsing, or recognizing and segmenting objects in an image, is one of the core problems of computer vision. Traditional approaches to object recognition begin by specifying an object model, such as template matching <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b4">5]</ref>, constellations <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b6">7]</ref>, bags of features <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b24">25]</ref>, or shape models <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b5">6]</ref>, etc. These approaches typically work with a fixed-number of object categories and require training generative or discriminative models for each category given training data. In the parsing stage, these systems try to align the learned models to the input image and associate object category labels with pixels, windows, edges or other image representations. Recently, context information has also been carefully modeled to capture the relationship between objects at the semantic level <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13]</ref>. Encouraging progress has been made by these models on a variety of object recognition and scene parsing tasks.</p><p>However, these learning-based methods do not, in general, scale well with the number of object categories. For example, to expand an existing system to include more object categories, we need to train new models for these categories and, typically adjust system parameters. Training can be a tedious job if we want to include thousands of object categories for a scene parsing system. In addition, the complexity of contextual relationships amongst objects also increases rapidly as the quantity of object categories expands.</p><p>Recently, the emergence of large databases of images has opened the door to a new family of methods in computer vision. Large database-driven approaches have shown the potential for nonparametric methods in several applications. Instead of training sophisticated parametric models, these methods try to reduce the inference problem for an unknown image to that of matching to an existing set of annotated images. In <ref type="bibr" target="#b20">[21]</ref>, the authors estimate the pose of a human relying on 0.5 million training examples. In <ref type="bibr" target="#b11">[12]</ref>, the proposed algorithm can fill holes on an input image by introducing elements that are likely to be semantically correct through searching a large image database. In <ref type="bibr" target="#b18">[19]</ref>, a system is designed to infer the possible object categories that may appear in an image by retrieving similar images in a large database <ref type="bibr" target="#b19">[20]</ref>. Moreover, the authors in <ref type="bibr" target="#b26">[27]</ref> showed that with a database of 80 million images, even simple SSD match can give semantically meaningful parsing for 32 × 32 images.</p><p>Motivated by the recent advances in large databasedriven approaches, we designed a nonparametric scene parsing system to transfer the labels from existing samples to annotate an image through dense scene alignment, as illustrated in Figure <ref type="figure" target="#fig_0">1</ref>. For a query image (a), our system first retrieves the top matches in the LabelMe database <ref type="bibr" target="#b19">[20]</ref> using a combination of GIST matching <ref type="bibr" target="#b17">[18]</ref> and SIFT flow [16]. Since these top matches are labeled, we transfer the annotation (c) of the top matches to the query image and obtain the scene parsing result in (d). For comparison, the ground-truth user annotation of the query is displayed in (e).</p><p>Our system is able to generate promising scene parsing results if images from the same scene category are retrieved in the annotated database.</p><p>However, it is nontrivial to build an efficient and reliable scene parsing system using dense scene alignment. The SIFT flow algorithm proposed in <ref type="bibr" target="#b15">[16]</ref> does not scale well with image dimensions. Therefore, we propose a flexible, coarse-to-fine matching scheme to find dense correspondences between two images. To account for the multiple annotation suggestions from the top matches, a Markov random field model is used to merge multiple cues (e.g. likelihood, prior and spatial smoothness) into reliable annotation. Promising experimental results are achieved on images from the LabelMe database <ref type="bibr" target="#b19">[20]</ref>.</p><p>Our goal is to explore the performance of scene parsing through the transfer of labels from existing annotated images, rather than building a comprehensive object recognition system. We show, however, that the performance of our system outperforms existing approaches <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b22">23]</ref> on our dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">SIFT Flow for Dense Scene Alignment</head><p>As our goal is to transfer the labels of existing samples to parse an input image, it is essential to find the dense correspondence for images across scenes. Liu et al. <ref type="bibr" target="#b15">[16]</ref> have demonstrated that SIFT flow is able to establish semantically meaningful correspondences between two images through matching local SIFT structures. In this section we extend the SIFT flow algorithm <ref type="bibr" target="#b15">[16]</ref> to be more robust to matching outliers by modifying the objective function for matching, and more efficient for aligning large-scale images using a coarse-to-fine approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Modified matching objective</head><p>Let p = (x, y) contain the spatial coordinate of a pixel, and w(p) = (u(p), v(p)) be the flow vector at p. Denote s 1 and s 2 as the per-pixel SIFT feature <ref type="bibr" target="#b16">[17]</ref> for two images <ref type="foot" target="#foot_1">1</ref> , and ε contains all the spatial neighborhood (a four-neighbor system is used). Our modified energy function is defined as:</p><formula xml:id="formula_0">E(w) = p min s 1 (p) -s 2 (p + w(p)) 1 , t + p η |u(p)| + |v(p)| + (p,q)∈ε min α|u(p) -u(q)|, d + (p,q)∈ε min α|v(p) -v(q)|, d .<label>(1)</label></formula><p>In this objective function, truncated L1 norms are used in both the data and the smoothness terms to account for matching outliers and flow discontinuities, with t and d as the threshold, respectively. An L1 norm is also imposed on the magnitude of the flow vector as a bias towards smaller displacement when no other information is available. Notice that in <ref type="bibr" target="#b15">[16]</ref> only an L1 norm is used for the data term and the small displacement biased is formulated as an L2 norm. This energy function is optimized by running sequential Belief Propagation (BP-S) <ref type="bibr" target="#b25">[26]</ref> on a dual plane setup <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Coarse-to-fine matching scheme</head><p>While SIFT flow has demonstrated the potential for aligning images across scenes <ref type="bibr" target="#b15">[16]</ref>, its performance scales poorly with respect to the image size. In SIFT flow, a pixel in one image can literally match to any other pixel in another image. Suppose the image has h 2 pixels, then the time and space complexity of the BP algorithm to estimate the SIFT flow is O(h 4 ). As reported in <ref type="bibr" target="#b15">[16]</ref>, the computation time for 145 × 105 images with an 80 × 80 searching neighborhood is 50 seconds. The original implementation of SIFT flow would require more than two hours to process a pair of 256 × 256 images in our database with a memory usage of 16GB to store the data term.</p><p>To address the performance drawback, we designed a coarse-to-fine SIFT flow matching scheme that significantly improves the performance. The procedure is illustrated in Figure <ref type="figure" target="#fig_1">2</ref>. For simplicity, we use s to represent both s 1 and s 2 . A SIFT pyramid {s (k) } is established, where s (1) = s and s (k+1) is smoothed and downsampled from s (k) . At each pyramid level k, let p k be the coordinate of the pixel to match, c k be the offset or centroid of the searching window, and w(p k ) be the best match from BP. At the top pyramid level s (3) , the searching window is centered at p 3 (c 3 = p 3 ) with size m×m, where m is the width (height) of s (3) . The complexity of BP at this level is O(m 4 ). After BP</p><formula xml:id="formula_1">0 1 2 -1 -2 5 -2 (a) (b) (c)</formula><p>Figure <ref type="figure">3</ref>. We generalized distance transform function for truncated L1 norm <ref type="bibr" target="#b7">[8]</ref> to pass message between neighboring nodes that have different offsets (centroids) of the searching window. converges, the system propagates the optimized flow vector w(p 3 ) to the next (finer) level to be c 2 where the searching window of p 2 is centered. The size of this searching window is fixed to be n × n with n = 11. This procedure iterates from s (3) to s (1) until the flow vector w(p 1 ) is estimated. Since n is fixed at all levels except for the top, the complexity of this coarse-to-fine algorithm is O(h 2 log h), a significant speed up compared to O(h 4 ).</p><p>When the matching is propagated from an coarser level to the finer level, the searching windows for two neighboring pixels may have different offsets (centroids). We modify the the distance transform function developed for truncated L1 norm <ref type="bibr" target="#b7">[8]</ref> to cope with this situation, with the idea illustrated in Figure <ref type="figure">3</ref>. To compute the message passing from pixel p to its neighbor q, we first gather all other messages and data term, and apply the routine in <ref type="bibr" target="#b7">[8]</ref> to compute the message from p to q assuming that q and p have the same offset and range. The function is then extended to be outside the range by increasing α per step, as shown in Figure <ref type="figure">3 (a)</ref>. We take the function in the range that q is relative to p as the message. For example, if the offset of the searching window for p is 0, and the offset for q is 5, then the message from p to q is plotted in Figure <ref type="figure">3 (c</ref>). If the offset of the searching window for q is -2 otherwise, the message is shown in Figure <ref type="figure">3 (b)</ref>.</p><p>Using the proposed coarse-to-fine matching scheme and modified distance transform function, the matching between two 256 × 256 images takes 31 seconds on a workstation with two quad-core 2.67 GHz Intel Xeon CPUs and 32 GB memory, in a C++ implementation. Further speedup (up to 50x) can be achieved through GPU implementation <ref type="bibr" target="#b3">[4]</ref> of the BP-S algorithm since this algorithm can be parallelized. We leave this as future work.</p><p>A natural question is whether the coarse-to-fine matching scheme can achieve the same minimum energy as the ordinary matching scheme (only one level without coarseto-fine) <ref type="bibr" target="#b15">[16]</ref>. An experiment is conducted to compare these two algorithms (refer to Section 4.1 for more details). In general, we found that the coarse-to-fine matching outperforms the ordinary matching in terms of obtaining lower energy. This is consistent with what has been discovered in  . For a query image, we first find a K-nearest neighbor set in the database using GIST matching <ref type="bibr" target="#b17">[18]</ref>. The nearest neighbors are re-ranked using SIFT flow matching scores, and form a top M -voting candidate set. The annotations are transferred from the voting candidates to the query image.</p><p>the optical flow community: coarse-to-fine search not only speeds up computation but also leads to lower energy. This can be caused by the inherent self-similarity nature of SIFT features across scales: the correspondence at a coarser level is a good prediction for the correspondence at a finer level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Scene Parsing through Label Transfer</head><p>Now that we have a large database of annotated images and a technique of establishing dense correspondences across scenes, we can transfer the existing annotations to a query image through dense scene alignment. For a given query image, we retrieve a set of K-nearest neighbors in our database using GIST matching <ref type="bibr" target="#b17">[18]</ref>. We then compute the SIFT flow from the query to each nearest neighbor, and use the achieved minimum energy (defined in Eqn. 1) to rerank the K-nearest neighbors. We further select the top M re-ranked retrievals to create our voting candidate set. This voting set will be used to transfer its contained annotations into the query image. This procedure is illustrated in Figure <ref type="figure" target="#fig_3">4</ref>.</p><p>Under this setup, scene parsing can be formulated as the following label transfer problem. For a query image I with its corresponding SIFT image s, we have a set of voting candidates {s i , c i , w i } i=1:M , where s i , c i and w i are the SIFT image, annotation, and SIFT flow field (from s to s i ) of the ith voting candidate. c i is an integer image where c i (p) ∈ {1, • • • , L} is the index of object category for pixel p. We want to obtain the annotation c for the query image by transferring c i to the query image according to the dense correspondence w i .</p><p>We build a probabilistic Markov random field model to integrate multiple labels, prior information of object category, and spatial smoothness of the annotation to parse image I. Similar to that of <ref type="bibr" target="#b22">[23]</ref>, the posterior probability is defined as:</p><formula xml:id="formula_2">-log P c|I, s, {s i , c i , w i } = p ψ c(p); s, {s ′ i } + α p λ c(p) +β {p,q}∈ε φ c(p), c(q); I +log Z, (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>where Z is the normalization constant of the probability. This posterior contains three components, i.e. likelihood, prior and spatial smoothness.</p><p>The likelihood term is defined as</p><formula xml:id="formula_4">ψ c(p) = l = min i∈Ω p,l s(p)-s i (p+w(p)) , Ω p,l = ∅ τ, Ω p,l = ∅ (3)</formula><p>where Ω p,l = {i; c i (p + w(p)) = l} is the index set of the voting candidates whose label is l after being warped to pixel p. τ is set to be the value of the maximum difference of SIFT feature: τ = max s1,s2,p s 1 (p)s 2 (p) .</p><p>The prior term is λ(c(p) = l) indicates the prior probability that object category l appears at pixel p. This is obtained from counting the occurrence of each object category at each location in the training set.</p><formula xml:id="formula_5">λ c(p) = l = -log hist l (p)<label>(4)</label></formula><p>where hist l (p) is the spatial histogram of object category l.</p><p>The smoothness term is defined to bias the neighboring pixels into having the same label if no other information is available, and the probability depends on the edge of the image: the stronger luminance edge, the more likely that the neighboring pixels may have different labels.</p><formula xml:id="formula_6">φ c(p), c(q) = δ[c(p) = c(q)]</formula><p>ǫ+e -γ I(p)-I(q) 2 ǫ+1</p><p>(5) where γ = (2 &lt; I(p) -I(q) 2 &gt;) -1 <ref type="bibr" target="#b22">[23]</ref>.</p><p>Notice that the energy function is controlled by four parameters, K and M that decide the mode of the model, and α and β that control the influence of spatial prior and smoothness. Once the parameters are fixed, we again use BP-S algorithm to minimize the energy. The algorithm converges in two seconds on a workstation with two quad-core 2.67 GHz Intel Xeon CPUs.</p><p>A significant difference between our model and that in <ref type="bibr" target="#b22">[23]</ref> is that we have fewer parameters because of the nonparametric nature of our approach, whereas classifiers where trained in <ref type="bibr" target="#b22">[23]</ref>. In addition, color information is not included in our model at the present as the color distribution for each object category is diverse in our database.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We used a subset of the LabelMe database <ref type="bibr" target="#b19">[20]</ref> to test our system. This dataset contains 2688 fully annotated images, most of which are outdoor scenes including street, beach, mountains, fields and buildings. From these images we randomly selected 2488 for training and 200 for testing. We chose the top 33 object categories with the most labeled pixels. The pixels that are not labeled, or labeled as other object categories, are treated as the 34th category: "unlabeled". The per pixel frequency count of these object categories in the training set is shown at the top of Figure <ref type="figure" target="#fig_4">5</ref>. The color of each bar is the average RGB value of the corresponding object category from the training data with  saturation and brightness boosted for visualization. The top 10 object categories are sky, building, mountain, tree, unlabeled, road, sea, field, grass, and river. The spatial priors of these object categories are displayed at the bottom of Figure <ref type="figure" target="#fig_4">5</ref>. White means zero probability and saturated color means the highest probability. We observe that sky occupies the upper part of image grid and field occupies the lower part.</p><p>Notice that there are only limited numbers of samples for the objects such as sun, cow, bird, and moon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Evaluation of the dense scene alignment</head><p>We first evaluate our coarse-to-fine SIFT flow matching for dense scene alignment. We randomly selected 10 images from the test set as the query, and check the minimum energy obtained between the query and the best SIFT flow match using coarse-to-fine scheme and ordinary scheme (non coarse-to-fine), respectively. For these 256 × 256 images, the average running time coarse-to-fine SIFT flow is 31 seconds, whereas it takes 127 minutes in average for the ordinary matching. The coarse-to-fine scheme not only runs   . Coarse-to-fine SIFT flow not only runs significantly faster, but also achieves lower energies most of the time. In this experiment, we randomly selected 10 samples in the test set and computed the lowest energy of the best match with the nearest neighbors. We tested both the coarse-to-fine algorithm proposed in this paper and the ordinary matching scheme in <ref type="bibr" target="#b15">[16]</ref>. Except for sample #8, coarse-to-fine matching achieves lower energy than the ordinary matching algorithm.</p><p>significantly faster, but also achieves lower energies most of the time compared to the ordinary matching algorithm <ref type="bibr" target="#b15">[16]</ref> as shown in Figure <ref type="figure" target="#fig_7">7</ref>.</p><p>Before evaluating the performance our system on object recognition, we want to evaluate how well SIFT flow performs in matching structures across different images and how it compares with human selected matches. Traditional optical flow is a well-defined problem and it is straightforward for humans to annotate motion for evaluation <ref type="bibr" target="#b14">[15]</ref>. In the case of SIFT flow, however, there may not be obvious or unique best pixel-to-pixel matching as the two images may contain different objects, or the same object categories with very different instances.</p><p>To evaluate the matching obtained by SIFT flow, we performed a user study where we showed 11 users image pairs with preselected sparse points in the first image and asked the users to select the corresponding points in the second image. As shown on the right of Figure <ref type="figure" target="#fig_8">8</ref>, user annotation can be ambiguous. Therefore, we use the following metric to evaluate SIFT flow: for a pixel p, we have several human annotations z i as its flow vector, and w(p) as the estimated SIFT flow vector. We compute Pr ∃z i , z iw(p) ≤ r|r , namely the probability of one human annotated flow is within distance r to SIFT flow w(p). This function (or r is plotted on the left of Figure <ref type="figure" target="#fig_8">8</ref> (red curve). For comparison, we plot the same probability function (blue curve) for minimum L1-norm SIFT matching, i.e. SIFT flow matching without spatial terms. Clearly SIFT flow matches better to human annotation than minimum L1-norm SIFT matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results of scene parsing</head><p>Our scene parsing system is illustrated in Figure <ref type="figure" target="#fig_5">6</ref>. The system retrieves a K-nearest neighbor set for the query image (a), and further selects M voting candidates with the minimum SIFT matching score. For the purpose of illustration we set M = 3 here. The RGB image, SIFT image, and annotation of the voting candidates are shown in (c) to (e), respectively. The SIFT flow field is visualized in (f) using   the same visualization scheme as in <ref type="bibr" target="#b15">[16]</ref>. After we warp the voting candidates into the query with respect to the flow field, the warped RGB (g) and SIFT image (h) are very close to the query (a) and (b). Combining the warped annotations in (i), the system outputs the parsing of the query in (j), which is close to the ground-truth annotation in (k).</p><p>Some label transferring results are displayed in Figure <ref type="figure" target="#fig_12">12</ref>. The input image from the test set is displayed in column (a). We show the best match, its corresponding annotation, and the warped best match in (b), (c) and (d), respectively, to hint the annotation for the query, even though our system takes the top M matches as voting candidates. Again, the warped image (d) looks similar to the input, indicating that SIFT flow successfully matches image structures. The scene parsing results output from our system are listed in column (e) with parameter setting K = 50, M = 5, α = 0.1, β = 70. The ground-truth user annotation is listed in (f). Notice that the gray pixels in (f) are "unlabeled", but our system does not generate "unlabeled" output. For sample 1, 5, 6, 8 and 9, our system generates reasonable predictions for the pixels annotated as "unlabeled". The pixelwise recognition rate of our system is 74.75% by excluding the "unlabeled" class <ref type="bibr" target="#b22">[23]</ref>. A failure example for our system is shown in Figure <ref type="figure" target="#fig_13">13</ref> when the system fails to retrieve images with similar object categories to the query.</p><p>For comparison, we downloaded and ran the code from <ref type="bibr" target="#b22">[23]</ref> using the same training and test data with the conditional random field turned off. The overall pixel-wise recognition rate of their system on our data set is 51.67%, and the per-class rates are displayed in Figure <ref type="figure" target="#fig_10">9 (c)</ref>. For fairness we also turned off the Markov random field model in our framework by setting α = β = 0, and plotted the corresponding results in Figure <ref type="figure" target="#fig_10">9</ref> (b). Clearly, our system outperforms <ref type="bibr" target="#b22">[23]</ref> in terms of both overall and per-class recognition rate. Overall, our system is able to predict the right object categories in the input image with a segmentation fit to image boundary, even though the best match may look different from the input, e.g. 2, 11, 12 and 17. If we divide the object categories into stuff (e.g. sky, mountains, tree, sea and field) and things (e.g. cars, sign, boat and bus) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13]</ref>, our system generates much better results for stuff than for things. The recognition rate for the top 7 object categories (all are "stuff") is 82.72%. This is because in our current system we only allow one labeling for each pixel, and smaller objects tend to be overwhelmed by the labeling of larger objects. We plan to build a recursive system in our future work to further retrieve things based on the inferred stuff. We investigate the performance of our system by varying the parameters K, M , α and β. First, we fix α = 0.1, β = 70 and plot the recognition rate as a function of K in Figure <ref type="figure" target="#fig_11">10</ref> (a) with different M . Overall, the recognition rate increases as more nearest neighbors are retrieved (K ↑) and more voting candidates are used (M ↑) since, obviously, multiple candidates are needed to transfer labels to the query. However, the recognition drops as K and M continue to increase as more candidates may include noise to label transfer. The maximum performance is obtained when K = 50 and M = 5. Second, we fix M = 5, and plot the recognition rate as a function of K by turning on prior and spatial terms (α = 0.1, β = 70) and turning them off (α = β = 0) in Figure <ref type="figure" target="#fig_11">10 (b)</ref>. Prior and spatial smoothness increase the performance of our system by about 7 percentage.</p><p>Lastly, we compared the performance of our system with a classifier-based system <ref type="bibr" target="#b4">[5]</ref>. We downloaded their code and trained a classifier for each object category using the same training data. We converted our system into a binary object detector for each class by only using the per-class likelihood term. The per-class ROC curves of our system Figure <ref type="figure" target="#fig_0">11</ref>. The ROC curve of each individual pixel-wise binary classifier. Red curve: our system after being converted to binary classifiers; blue curve: the system in <ref type="bibr" target="#b4">[5]</ref>. We used convex hull to make the ROC curves strictly concave. The number (n, m) underneath the name of each plot is the quantity of the object instances in the test and training set, respectively. For example, (170, 2124) under "sky" means that there are 170 test images containing sky, and 2124 training images containing sky. Our system obtains reasonable performance for objects with sufficient samples in both training and test sets, e.g. sky, building, mountain and tree. We observe truncation in the ROC curves where there are not enough test samples, e.g. field, sea, river, grass, plant, car and sand. The performance is poor for objects without enough training samples, e.g. crosswalk, sign, boat, pole, sun and bird. The ROC does not exist for objects without any test samples, e.g. desert, cow and moon. In comparison, our system outperforms or equals <ref type="bibr" target="#b4">[5]</ref> for all object categories except for grass, plant, boat, person and bus. The performance of <ref type="bibr" target="#b4">[5]</ref> on our database is low because the objects have drastically different poses and appearances. The dark gray pixels in (f) are "unlabeled". Notice how our system generates a reasonable parsing even for these "unlabeled" pixels. (red) and theirs (blue) are plotted in Figure <ref type="figure" target="#fig_0">11</ref>. Except for five object categories, grass, plant, boat, person and bus, our system outperforms or equals theirs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We presented a novel, nonparametric scene parsing system to transfer the annotations from a large database to an input image using dense scene alignment. A coarse-to-fine SIFT flow matching scheme is proposed to reliably and efficiently establish dense correspondences between images across scenes. Using the dense scene correspondences, we warp the pixel labels of the existing samples to the query. Furthermore, we integrate multiple cues to segment and recognize the query image into the object categories in the database. Promising results have been achieved by our scene alignment and parsing system on a challenging database. Compared to existing approaches that require training for each object category, our nonparametric scene parsing system is easy to implement, has only a few parameters, and embeds contextual information naturally in the retrieval/alignment procedure.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. For a query image (a), our system finds the top matches (b) (three are shown here) using a modified, corse-to-fine SIFT flow matching algorithm. The annotations of the top matches (c) are transferred and integrated to parse the input image as shown in (d). For comparison, the ground-truth user annotation of (a) is shown in (e).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. An illustration of our coarse-to-fine pyramid SIFT flow matching. The green square is the searching window for p k at each pyramid level k. For simplicity only one image is shown here, where p k is on image s1, and c k and w(p k ) are on image s2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4</head><label>4</label><figDesc>Figure 4. For a query image, we first find a K-nearest neighbor set in the database using GIST matching<ref type="bibr" target="#b17">[18]</ref>. The nearest neighbors are re-ranked using SIFT flow matching scores, and form a top M -voting candidate set. The annotations are transferred from the voting candidates to the query image.</figDesc><graphic coords="3,388.79,75.50,129.62,51.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Above: the per-pixel frequency counts of the object categories in our dataset (sorted in descending order). The color of each bar is the average RGB value of each object category from the training data with saturation and brightness boosted for visualization. Bottom: the spatial priors of the object categories in the database. White means zero and the saturated color means high probability.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. System overview. Our algorithm computes the SIFT image (b) of an query image (a), and uses GIST [18] to find its K nearest neighbors in our database. We apply coarse-to-fine SIFT flow to align the query image to the nearest neighbors, and obtain top M as voting candidates (M = 3 here). (c) to (e): the RGB image, SIFT image and user annotation of the voting candidates. (f): the inferred SIFT flow. From (g) to (i) are the warped version of (c) to (e) with respect to the SIFT flow in (f). Notice the similarity between (a) and (g), (b) and (h). Our system combines the voting from multiple candidates and generates scene parsing in (j) by optimizing the posterior. (k): the ground-truth annotation of (a).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7</head><label>7</label><figDesc>Figure 7. Coarse-to-fine SIFT flow not only runs significantly faster, but also achieves lower energies most of the time. In this experiment, we randomly selected 10 samples in the test set and computed the lowest energy of the best match with the nearest neighbors. We tested both the coarse-to-fine algorithm proposed in this paper and the ordinary matching scheme in<ref type="bibr" target="#b15">[16]</ref>. Except for sample #8, coarse-to-fine matching achieves lower energy than the ordinary matching algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. The evaluation of SIFT flow using human annotation. Left: the probability of one human annotated flow lies within r distance to the SIFT flow as a function of r (red curve). For comparison, we plot the same probability for direct minimum L1-norm matching (blue curve). Clearly SIFT flow matches human perception better. Right: the histogram of the standard deviation of human annotation. Human perception of scene correspondence varies from subject to subject.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 .</head><label>9</label><figDesc>Figure9. The per-class recognition rate of our system and the one in<ref type="bibr" target="#b22">[23]</ref>. (a) Our system with the the parameters optimized for pixel-wise recognition rate. (b) Our system with α = β = 0, namely, with the Markov random field model turned off. (c) The performance of the system in<ref type="bibr" target="#b22">[23]</ref> also with the conditional random field turned off, trained and tested on the same data sets as (a) and (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. (a): Recognition rate as a function of the number of nearest neighbors K and the number of voting candidates M . (b): recognition rate as a function of the number of nearest neighbors K. Clearly, prior and spatial smoothness help improve the recognition rate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 12 .</head><label>12</label><figDesc>Figure 12. Some scene parsing results output from our system. (a): query image; (b): the best match from nearest neighbors; (c): the annotation of the best match; (d): the warped version of (b) according to the SIFT flow field; (e): the inferred per-pixel parsing after combining multiple voting candidates; (f): the ground truth annotation of (a).The dark gray pixels in (f) are "unlabeled". Notice how our system generates a reasonable parsing even for these "unlabeled" pixels.</figDesc><graphic coords="7,303.56,554.63,232.80,115.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 13 .</head><label>13</label><figDesc>Figure 13. Our system fails when no good matches can be retrieved in the database. Since the best matches do not contain river, the input image is mistakenly parsed as a scene of grass, tree and mountain in (e). The ground-truth annotation is in (f).</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>978-1-4244-3991-1/09/$25.00 ©2009 IEEE</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>SIFT descriptors are computed at each pixel using a 16 × 16 window. The window is divided into 4 × 4 cells, and image gradients within each cell are quantized into a 8-bin histogram. Therefore, the pixel-wise SIFT feature is a 128-D vector.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgements</head><p>Funding for this research was provided by the Royal Dutch/Shell Group, NGA NEGI-1582-04-0004, MURI Grant N00014-06-1-0734, NSF Career award (IIS 0747120), and a National Defense Science and Engineering Graduate Fellowship.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On seeing stuff: the perception of materials by humans and machines</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPIE, Human Vision and Electronic Imaging VI</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Shape context: A new descriptor for shape matching and object recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Puzicha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Shape matching and object recognition using low distortion correspondence</title>
		<author>
			<persName><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Real-time connectivity constrained depth map computation using programmable graphics hardware</title>
		<author>
			<persName><forename type="first">N</forename><surname>Cornelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1099" to="1104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pictorial structures for object recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A discriminatively trained, multiscale, deformable part model</title>
		<author>
			<persName><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Efficient belief propagation for early vision</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="54" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Object class recognition by unsupervised scale-invariant learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Pyramid match kernels: Discriminative classification with sets of image features</title>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Beyond nouns: Exploiting prepositions and comparative adjectives for learning visual classifiers</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Scene completion using millions of photographs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGGRAPH</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning spatial context: Using stuff to find things</title>
		<author>
			<persName><forename type="first">G</forename><surname>Heitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">II</biblScope>
			<biblScope unit="page" from="2169" to="2178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Humanassisted motion annotation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">SIFT flow: dense correspondence across different scenes</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Object recognition from local scale-invariant features</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<meeting><address><addrLine>Kerkyra, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="1150" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Modeling the shape of the scene: a holistic representation of the spatial envelope</title>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="145" to="175" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Freeman. Object recognition by scene alignment</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">LabelMe: a database and web-based tool for image annotation</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="157" to="173" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fast pose estimation with parameter sensitive hashing</title>
		<author>
			<persName><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Efficient MRF deformation model for non-rigid image matching</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shekhovtsov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kovtun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Hlavac</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Textonboost: Joint appearance, shape and context modeling for multi-class object recognition and segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Video Google: a text retrieval approach to object matching in videos</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Describing visual scenes using transformed dirichlet processes</title>
		<author>
			<persName><forename type="first">E</forename><surname>Sudderth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Willsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zabih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Veksler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tappen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A comparative study of energy minimization methods for markov random fields</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1068" to="1080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">80 million tiny images: a large dataset for non-parametric object and scene recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rapid object detection using a boosted cascade of simple features</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
