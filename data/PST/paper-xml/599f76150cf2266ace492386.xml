<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Networks for Pattern-Based Short-Term Load Forecasting: A Comparative Study</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">grzegorz</forename><surname>Dudek</surname></persName>
							<email>dudek@el.pcz.czest.plwww.gdudek.el.pcz.pl</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Czestochowa University of Technology</orgName>
								<address>
									<addrLine>Al. Armii Krajowej 17</addrLine>
									<postCode>42-200</postCode>
									<settlement>Czestochowa</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Networks for Pattern-Based Short-Term Load Forecasting: A Comparative Study</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">189F3ECD0FFE68062EB4AB7FFA04808B</idno>
					<idno type="DOI">10.1016/j.neucom.2016.04.021</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T04:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>neural networks</term>
					<term>patterns of seasonal cycles</term>
					<term>short-term load forecasting</term>
					<term>time series</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work several univariate approaches for short-term load forecasting based on neural networks are proposed and compared. They include: multilayer perceptron, radial basis function neural network, generalized regression neural network, fuzzy counterpropagation neural networks, and self-organizing maps. A common feature of these methods is learning from patterns of the seasonal cycles of load time series. Patterns used as input and output variables simplify the forecasting problem by filtering out a trend and seasonal variations of periods longer than a daily one. Nonstationarity in mean and variance is also eliminated. In the simulation studies using real power system data the neural network forecasting methods were tested and compared with other popular forecasting methods such as ARIMA and exponential smoothing. The best results were achieved for generalized regression neural network and one-neuron perceptron learned locally.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Short-term load forecasting (STLF) is essential for power system control and scheduling. Load forecasts of short horizon (from minutes to days) are necessary for energy companies to make decisions related to planning of electricity production and transmission, such as unit commitment, generation dispatch, hydro scheduling, hydrothermal coordination, spinning reserve allocation, interchange and low flow evaluation. Electricity markets also require the precise load forecasts because the load is the basic driver of electricity prices. The forecast accuracy translates to financial performance of energy companies (suppliers, system operators) and other market participants and financial institutions operating in energy markets.</p><p>Neural networks are widely used in STLF due to their flexibility which can reflect process variability in uncertain dynamic environment and complex relationships between variables. The main drivers of the power system load are: weather conditions (temperature, wind speed, cloud cover, humidity, precipitation), time, demography, economy, electricity prices, and other factors such as geographical conditions, consumer types and their habits. The relationships between explanatory variables and power system load are often unclear and unstable in time. In this work we focus on univariate forecasting methods, in which only historical load time series is used as input to predict the future values of power system load.</p><p>The load time series contains a trend, multiple seasonal variations and a stochastic irregular component. From Fig. <ref type="figure" target="#fig_0">1</ref>, where the hourly electrical load of the Polish power system is shown (these data can be downloaded from the website http://gdudek.el.pcz.pl/varia/stlf-data), it can be observed annual, weekly and daily cycles. A daily profile, on which we focus in STLF, depends on the day of the week and season of the year. Moreover, it may change over the years. The noise level in a load time series depends on the system size and the customer structure. A trend, amplitude of the annual cycle, weekly and daily profiles and noise intensity may change considerably from dataset to dataset.</p><p>Due to the importance of STLF and the problem complexity many various STLF methods has been developed.</p><p>They can be roughly classified as conventional and unconventional ones. Conventional STLF approaches use regression methods, smoothing techniques and statistical analysis. The most commonly employed conventional models are: the Holt-Winters exponential smoothing (ES) and the autoregressive integrated moving average  (ARIMA) models <ref type="bibr" target="#b0">[1]</ref>. In ES the time series is decomposed into a trend component and seasonal components which can be combined additively or multiplicatively. ES can be used for nonlinear modelling of heteroscedastic time series, but the exogenous variables cannot be introduced into the model. The most important disadvantages of ES are overparameterization and a large number of starting values to estimate. Recently developed exponentially weighted methods in application to STLF are presented in <ref type="bibr" target="#b1">[2]</ref>.</p><p>ARIMA processes are well suited to express the stochastic nature of the load time series. Modelling of multiple seasonal cycles as well as introducing exogenous variables is not a problem in ARIMA. The disadvantage of ARIMA models is that they are able to represent only linear relationships between variables. The difficulty in using ARIMA is the problem of order selection which is considered to be subjective. To simplify the forecasting problem the time series is often decomposed into a trend, seasonal components and an irregular component. These components, showing less complexity than the original series, are modeled independently (see <ref type="bibr" target="#b2">[3]</ref>). Another decomposition way based on wavelet transform is described in <ref type="bibr" target="#b3">[4]</ref>.</p><p>Unconventional STLF approaches employ new computational methods such as artificial intelligence and machine learning ones. These approaches are reviewed in <ref type="bibr" target="#b4">[5]</ref> and <ref type="bibr" target="#b5">[6]</ref>. They include: neural networks (NNs), fuzzy inference systems, neuro-fuzzy systems, support vector machines and ensembles of models. Among them the most popular are NNs. They have many attractive features, such as: universal approximation property, learning capabilities, massive parallelism, robustness in the presence of noise, and fault tolerance. But there are also some drawbacks of using NNs: disruptive and unstable training, difficulty in matching the network structure to the problem complexity, weak extrapolation ability and many parameters to estimate (hundreds of weights). These issues with NNs as well as problems with load time series complexity are addressed in STLF literature in various ways. For example in <ref type="bibr" target="#b6">[7]</ref> the Bayesian approach is used to control the multilayer perceptron (MLP) complexity and to select input variables. As inputs are used: lagged load values (unprocessed), weather variables, and dummies to represent the days of the week and calendar variables. In <ref type="bibr" target="#b7">[8]</ref> the load time series is decomposed using wavelet transform into lower resolution components (approximation and details). Each component is predicted by MLP using gradient-based algorithm.</p><p>After learning the MLP weights are adjusted using evolutionary algorithm to get result nearer to the optimal one. Similar decomposition of the load time series using the wavelet transform for extraction relevant information from the load curve was used in <ref type="bibr" target="#b8">[9]</ref>. A particle swarm optimization algorithm was employed to adjust the MLP weights. In <ref type="bibr" target="#b9">[10]</ref> a generic framework combining similar day selection, wavelet decomposition, and MLP is presented. The MLP is trained on the similar days which are first decomposed using Daubechies wavelets. The similar day is a day which has the same weekday index, day-of-a-year index and similar weather to that of tomorrow (forecasted).</p><p>Another popular NN architecture used for STLF is radial basis function NN (RBFNN). It approximates the relationship between explanatory variables and load by a linear combination of radial basis functions (usually Gaussian), which nonlinearly transform the input data. Comparing to MLP the learning algorithm for RBFNN is simpler (there is no need for laborious error backpropagation). In <ref type="bibr" target="#b10">[11]</ref> a hybrid system for STLF is described, where RBFNN forecasts the daily load curve based on historical loads. The RBFNN weights are optimized using genetic algorithm. Then the Mamdani-type fuzzy inference system corrects the forecast depending on the errors and maximal daily temperature. RBFNN is employed in <ref type="bibr" target="#b11">[12]</ref> to forecast loads based on historical loads and historical and forecasted temperatures. Then the neuro-fuzzy system corrects the forecast depending on changes in electricity prices. In the approach described in <ref type="bibr" target="#b12">[13]</ref> the load time series is decomposed into five components using wavelet transform. Low-frequency component is predicted using RBFNN, while the high-frequency components (details) are predicted by averaging the details of few days of the same type as the day of forecast.</p><p>A self-organizing map (SOM) is another popular NN used in STLF. This network is trained using unsupervised competitive learning to produce a low-dimensional representation of the input space. Input vectors are grouped and represented by neurons. The hierarchical forecasting method composed of two SOMs is presented in <ref type="bibr" target="#b13">[14]</ref>. One SOM maps the input vector composed of the past loads and indicators of forecasted hour into the vector of distances between neurons and the winning neuron. This vector is an input for the second SOM, which maps it to the forecasted load at hour t. The forecasts of loads for the next hours are obtained recursively. In <ref type="bibr" target="#b14">[15]</ref> SOM is used for grouping the variable vector carrying information about the short-term dynamics of the load time series. These variables include load characteristic points, such as peaks, selected weather factors, and indicators of a day of the week and a holiday. Then, for each group the regression model was built using support vector regression with selected historical loads and temperatures as inputs. SOM is applied in <ref type="bibr" target="#b15">[16]</ref> to split the historical data dynamics into clusters. A flexible smooth transition autoregressive model is used to determine the forecast based on the linear combination of autoregressive models identified for each cluster. The weights in this combination are determined dynamically using MLP. SOM with a planar layer of neurons is used in <ref type="bibr" target="#b16">[17]</ref> for grouping the standardized daily load curves. Observing the responses of neurons for the successive training samples, the probabilities of transitions between neurons are determined. The forecast of the standardized load curve is calculated from the weights of these neurons for which the probability of transition from the winner neuron is non-zero. To get the forecast of the real loads, the forecasts of the daily load curve mean and standard deviation are needed. For this purpose, the authors use RBFNN. In <ref type="bibr" target="#b17">[18]</ref> the vector of predictors is expanded by the additional component: the forecasted variable. Such vectors are grouped using SOM. After learning, vector of predictors is presented (without additional component) and recognized by the winning neuron. The weight of the winner corresponding to the additional component of the input training vector is the forecast. The predictors are historical loads projected into lower dimensional space using curvilinear component analysis.</p><p>Many other types of NNs have been used for STLF including: recurrent NNs, generalized regression NNs, probabilistic NNs, adaptive resonance theory NNs, functional link NNs and counterpropagation NNs. The survey of NN applications to STLF can be found in <ref type="bibr" target="#b18">[19]</ref> and <ref type="bibr" target="#b19">[20]</ref>.</p><p>In this work a specific way of data preprocessing is used to simplify the STLF problem. We create patterns which contain information about the shapes of daily curves. Input and output patterns are defined which represent two types of daily periods. The input pattern represents the daily period preceding the forecasted one and the output pattern represents the forecasted daily period. Due to representing multiple seasonal time series using patterns we eliminate nonstationarity in input data, and we remove the trend and seasonal cycles of periods longer than the daily one. To model the relationship between input and output patterns we use NNs of different architectures. Each NN is optimized on a set of patterns which are similar to the query pattern (i.e. the input pattern for which we want to get the forecast). Thus the NN is optimized locally in the neighborhood of the query pattern (NN is locally competent).</p><p>This approach maximizes the model quality for particular query pattern. For another query pattern the NN is learned and optimized again.</p><p>The rest of this paper is organized as follows. In Section 2 we define patterns of daily cycles of the load time series. Section 3 presents neural models for STLF. The learning and optimization procedures for each model are given. In Section 4 we test the neural models on real load data. We compare results of the proposed methods to other STLF methods. Finally, Section 5 concludes the paper. At the end of the article the list of symbols is presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">DATA PREPROCESSING AND PATTERNS</head><p>A time series of electrical load expresses a trend, which is usually non-linear, and three seasonal cycles: annual, weekly and daily ones (see Fig. <ref type="figure" target="#fig_0">1</ref>). It is nonstationary in mean and variance. Data preprocessing by defining patterns simplifies the forecasting problem of the time series with multiple seasonal cycles. Two types of patterns of seasonal cycles are introduced: input ones x and output ones y. They are vectors:</p><formula xml:id="formula_0">x = [x 1 x 2 … x n ] T  X = ℝ n and y = [y 1 y 2 …</formula><p>y n ] T  Y = ℝ n , with components that are functions of the actual time series points. The x-pattern represents the vector of loads (L) in successive timepoints of the daily period:</p><formula xml:id="formula_1">L = [L 1 L 2 … L n ] T .</formula><p>By transforming a time series into a sequence of x-patterns we eliminate a trend and seasonal variations of periods longer than the basic period of length n (daily period in our case; n = 24 for hourly load time series, n = 48 for half-hourly load time series, and n = 96 for quarter-hourly load time series). Moreover, a time series mapped into x-patterns can have some desirable features like stationarity. The goal of y-patterns is to encode and unify the daily cycles to be forecasted using time series characteristics (coding variables) determined from the history. This allows us to determine the forecast of the daily curve having the forecasted y-pattern.</p><p>Functions mapping time series points L into x-patterns are dependent on the time series nature (trend, seasonal variations), the forecast period and horizon. They should maximize the model quality. In this work we use the input patterns x i (representing the i-th daily period) which components are defined as follows:</p><formula xml:id="formula_2">i i t i t i D L L x   , , ,<label>(1)</label></formula><p>where: i = 1, 2, …, Nthe daily period number, t = 1, 2, …, nthe time series element number in the period i, L i,tthe t-th time series point (load) in the period i, i Lthe mean load in the period i,</p><formula xml:id="formula_3">    n l i l i i L L D 1 2 , ) (</formula><p>the dispersion of the time series elements in the period i.</p><p>Definition (1) expresses normalization of the load vectors L i representing the i-th daily period. After normalization we get vectors x i having unity length, zero mean and the same variance. Note that the time series which is nonstationary in mean and variance is represented now by x-patterns having the same mean and variance. The trend and additional seasonal variations, i.e. the weekly and annual ones, are filtered. X-patterns carry information only about the shapes of the daily curves.</p><p>Whilst x-patterns represent input variables, i.e. loads for the day i, y-patterns represent output variables, i.e.</p><p>forecasted loads for the day i+, where  is a forecast horizon in days. The output pattern y i , representing the load</p><formula xml:id="formula_4">vector L i+ = [L i+,1 L i+,2 … L i+,n</formula><p>] T , has components defined as follows:</p><formula xml:id="formula_5">i i t i t i D L L y    , ,  .<label>(2)</label></formula><p>where</p><formula xml:id="formula_6">: i = 1, 2, …, N, t = 1, 2, …, n.</formula><p>This transformation is similar to (1) but instead of using coding variables</p><formula xml:id="formula_7">  i L and        n l i l i i L L D 1 2 , ) (   </formula><p>determined for the day i+, in (2) we use coding variables i L and D i determined for the day i. This is because the coding values for the day i+ are not known in the moment of forecasting. Using the known coding values for the day i enables us to calculate the forecast of vector L i+ when the forecast of pattern y i is generated by the model. To do this we use transformed equation (2):</p><formula xml:id="formula_8">i i t i t i L D y L    , ,    ,<label>(3)</label></formula><p>where t i y ,  is the forecasted t-th component of the pattern y i .</p><p>Using transformation (1) we unify the level and dispersion of x-patterns by subtracting i L (level of the i-th daily period) and dividing the result by D i (dispersion of the i-th daily period). This is shown in Fig. <ref type="figure">2</ref>  Because the level and dispersion of y-pattern depends on the day type , we construct the forecasting model for the particular day of the week using training set containing patterns from the history corresponding to this day type:  forecast the load time series using the pattern-based approach, first we filter out the information about dispersion and the position of the days i and i+ in the weekly and annual cycles ((1) and ( <ref type="formula" target="#formula_5">2</ref>)). Then we construct the model on patterns and we generate the forecast of the y-pattern. Finally we introduce the information about the position of the forecasted day in the weekly and annual cycles and current dispersion using <ref type="bibr" target="#b2">(3)</ref>.</p><formula xml:id="formula_9"> = {x i , y i  : i  },</formula><p>More functions defining patterns can be found in <ref type="bibr" target="#b20">[21]</ref>. A fragment of time series represented by x-patterns does not have to coincide with the daily period. It can include several cycles or a part of one cycle. Moreover, it does not have to include contiguous sequence of timepoints. We can select points to the input pattern. We can also use the feature extraction methods to create new pattern components from the original time series (see for example <ref type="bibr">[21 Dud15</ref>] were principal component analysis and partial least-squares regression are used for defining x-patterns).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">NEURAL MODELS FOR STLF</head><p>In this section we present several approaches to STLF based on NNs: multilayer perceptron (MLP), radial basis function NN (RBFNN), generalized regression NN (GRNN), counterpropagation NN (CPNN), and self organizing map (SOM). A common feature of these approaches is that they work on patterns defined in Section 2. GRNN, CPNN and SOM generate the forecast of y vector as output, whilst MLP and RBFNN generate the forecast of one component of y as output. The NNs are optimized and learned separately for each query pattern x*.</p><p>The proposed NN models for pattern-based STLF can be summarized in the following steps:</p><p>1. Map the original time series elements to patterns x and y using (1) and (2).</p><p>2. Select the l nearest neighbors of the query pattern x* in  and create the set  = {x i , y i  : i  }, where  is the set of numbers of the l nearest neighbors of x*.</p><p>3. Optimize the NN model in the local leave-one-out procedure.</p><p>4. Learn the NN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>Calculate the forecast of y or y t for x* using NN.</p><p>6. Decode the forecast of y or y t to get the forecast of load using (3).</p><p>In step 2 the set of l nearest neighbors is selected using Euclidean distance. The dot product can be used instead as a distance measure because vectors x are normalized. The sets  and  contain only historical patterns representing the same day types  as x* and corresponding y which is forecasted.</p><p>In step 3 we search for optimal values of hyper-parameters such as number of nearest neighbors k or number of neurons. Because the number of hyper-parameters in the proposed models is small, one or two, the simple grid search is applied to deal with this issue. Optimization is performed in the local leave-one-out cross validation (LLOO), where the validation samples are chosen one by one from the set  of nearest neighbors of the query pattern. So we optimize the NN model to get the best performance for the neighborhood of the current query pattern.</p><p>The learning algorithms applied in step 4 are dependent on the NN type and are described in the next sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Multilayer Perceptron</head><p>The MLP learns using the local learning procedure <ref type="bibr" target="#b22">[23]</ref>. In this case the training samples are selected from the neighborhood of the current query point x*. By the neighborhood of the query pattern we mean the set of its k nearest neighbors in the training set . The model is optimized and learned to fit accurately to the target function in the neighborhood of x* and is competent only for this query pattern. Note that the local complexity of the target function is lower than the global one. So we can use a simple MLP model with not many hidden neurons that learns quickly. The research reported in <ref type="bibr" target="#b22">[23]</ref> showed that using only one neuron with sigmoid activation function brings not worse results than networks with several neurons in the hidden layer. So in this work we use one sigmoid neuron as an optimal MLP architecture.</p><p>The number of nearest neighbors k (learning points) is the only hyper-parameter tuned in LLOO procedure. The MLP learns using Levenberg-Marquardt algorithm with Bayesian regularization <ref type="bibr" target="#b23">[24]</ref>, which minimizes a combination of squared errors and net weights. This prevent overfitting. The MLP optimization is summarized in Algorithm 1.</p><p>Algorithm 1: MLP forecasting model optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">RBF Network</head><p>RBFNN learns on the training set . The number of RBF neurons is selected in the learning procedure by adding neurons one by one until the specified error goal is met. Initially the RBF layer has no neurons. A new neuron is added with weights equal to the input pattern x with the greatest error. (This is implemented in newrb function from Neural Networks Toolbox, Matlab 2012b.) The spread of RBFs,  RBF , determines the smoothness of the fitted function. The larger the spread is, the smoother the function will be. It is assumed that the spread is the same for all neurons and is equal to the mean distance between each x   and its k-th nearest neighbor in . The number k is selected in LLOO procedure. The weights of the linear neurons in the output layer are calculated using simple linear algebra.</p><p>Although the RBFNN builds the global model for , it is optimized locally in LLOO. So we can expect good results only for the current query pattern x*. The RBFNN optimization procedure is shown in Algorithm 2.</p><p>Algorithm 2: RBFNN forecasting model optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Generalized Regression Neural Network</head><p>GRNN is a type of RBFNN with a one pass learning and highly parallel structure <ref type="bibr" target="#b24">[25]</ref>. This is a memory-based network, where each learning pattern x i is represented by one neuron with RBF: G i (x) with the center</p><formula xml:id="formula_10">C i = x i , i  .</formula><p>The algorithm provides smooth approximation of a target function even with sparse data in a multidimensional space. The advantages of GRNN are fast learning and easy tuning.</p><p>The layer of RBF neurons maps n-dimensional x-pattern space into N  -dimensional space of similarities between input pattern and the training x-patterns:</p><formula xml:id="formula_11">X  [G 1 (X) G 2 (X) ... G N (X)],</formula><p>where N  = || is the number of training samples. The output of GRNN is:</p><formula xml:id="formula_12">       N i i N i i i G G 1 1 ) ( ) ( x y x y  . (<label>4</label></formula><formula xml:id="formula_13">)</formula><p>The only parameter is the RBF spread,  RBF . As in RBFNN it determines the smoothness of the fitted function. We assume the same spread for all neurons calculated as  RBF = a(the mean distance between each x   and its 5-th nearest neighbor in ). The value of a is adjusted in LLOO procedure. In <ref type="bibr" target="#b25">[26]</ref> differential evolution was applied for searching for the best values of the N  spreads, i.e. spreads were adjusted individually for each neuron. But this did not bring an expected reduction in the test error.</p><p>The GRNN optimization is presented in Algorithm 3.</p><p>Algorithm 3: GRNN forecasting model optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Fuzzy Counterpropagation Neural Network</head><p>CPNN in a forward-only version <ref type="bibr" target="#b26">[27]</ref>  </p><p>where: j = 1, 2, ..., c, m  1 is the weighting exponent and (x i , w j ) is the degree of membership of x i in the j-th cluster.</p><p>The membership function (x i , w j ) is defined as <ref type="bibr" target="#b27">[28]</ref>:</p><formula xml:id="formula_15">1 1 1 2 ) , (                            c k m k i j i j i w x w x w x  , (<label>6</label></formula><formula xml:id="formula_16">)</formula><p>where ||x -w|| is the distance between pattern x and a fuzzy center w.</p><p>Clusters are represented by hidden neurons and the centers w j are the weight vectors of neurons (instar weight vectors). The n-dimensional outstar weight vector v j coming out of the j-th neuron is calculated as the mean of ypatterns weighted by the degree of membership of the x-patterns paired with them in the j-th cluster:</p><formula xml:id="formula_17">       N i m j i N i i m j i j 1 1 ) , ( ) , ( w x y w x v   . (<label>7</label></formula><formula xml:id="formula_18">)</formula><p>The forecast for x* is calculated as follows:</p><formula xml:id="formula_19">     c j m j c j j m j 1 1 ) *, ( ) *, ( w x v w x y    . (<label>8</label></formula><formula xml:id="formula_20">)</formula><p>Thus the forecast is the mean of the weight vectors v j weighted by the degrees of membership of the query pattern</p><p>x* in the clusters j = 1, 2, .., c.</p><p>In the second approach (FCPNN2) the clusters are created in the space Y by grouping y-patterns using fuzzy cmeans method. The cluster centers v j , which are the n-dimensional outstar weight vectors coming out of the hidden neurons, are:</p><formula xml:id="formula_21">       N i m j i N i i m j i j 1 1 ) , ( ) , ( v y y v y v   , (<label>9</label></formula><formula xml:id="formula_22">)</formula><p>where the membership function (y i , v j ) is defined similarly to <ref type="bibr" target="#b5">(6)</ref>.</p><p>Every of c instar weight vectors w j is calculated as the mean of x-patterns weighted by the degree of membership of the y-patterns paired with them in the j-th y-cluster:</p><formula xml:id="formula_23">       N i m j i N i i m j i j 1 1 ) , ( ) , ( v y x v y w   . (<label>10</label></formula><formula xml:id="formula_24">)</formula><p>The forecast y  is determined in the same way as in the first approach, according to <ref type="bibr" target="#b7">(8)</ref>. The only difference between FCPNN1 and FCPNN2 is the way of weight vectors w j and v j learning. In FCPNN1 the instar weight vectors w j represent centers of the x-pattern clusters. And the outstar weight vectors v j represent mean of y-patterns weighted by the membership of the x-patterns paired with them in the j-th cluster. Note that the y-patterns represented by v j can be dispersed in the Y-space and do not form a compact clusters (i.e. the clusters overlap). In The two versions of FCPNN described above have the same architecture shown in Fig. <ref type="figure" target="#fig_8">3</ref>. The optimization procedures for FCPNN1 and FCPNN2 in which we search for the optimal number of neurons using LLOO in Algorithms 4 and 5 are shown.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Self Organizing Map</head><p>Three approaches are proposed for STLF based on SOM. In the first one (SOM1) the paired x-and y-patterns are concatenated and form 2n-dimensional vectors</p><formula xml:id="formula_25">z i = [x i T y i T</formula><p>] T , i  . SOM learns on vectors z i using "winner takes most" algorithm. After learning each instar weight vector of the j-th neuron (w j ) represents the cluster center. The weights vectors have two n-dimensional parts corresponding to x-and y-patterns: w j = [w x,j T w y,j T ] T , j = 1, 2, …, c. In the forecasting phase the query pattern x* is presented and the winning neuron is detected as that one having the nearest w x to x*:</p><formula xml:id="formula_26">j x c j j , ,..., 2 , 1 * min arg * w x    . (<label>11</label></formula><formula xml:id="formula_27">)</formula><p>The y-part of weight vector of the winner is the forecast for x*:</p><formula xml:id="formula_28">* , j y w y   . (<label>12</label></formula><formula xml:id="formula_29">)</formula><p>Thus the forecasted y-pattern is the mean of y-patterns forming the nearest cluster represented by j*-th neuron. The</p><formula xml:id="formula_30">], * , * [ ] * , * [ ] 1 * , * [ L A n i L A n i L A i L A i i L i i                (14)</formula><p>where L is the number of days defining the period length, e.g. 30, A is the annual period (365 or 366 days), and n = 1, 2, ... is dependent on the length of the time series.</p><p>2. the day type is the same as for the forecasted y-pattern:  = *.</p><p>The forecasted y-pattern is calculated as the mean of the weight vectors w j weighted by the numbers of the corresponding label entries satisfying the above conditions: </p><formula xml:id="formula_31"> . (<label>15</label></formula><formula xml:id="formula_32">)</formula><p>The number of neurons c is selected in LLOO procedure as shown in Algorithm 8.</p><p>Algorithm 6: SOM1 forecasting model optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">SIMULATION STUDY</head><p>In this section the proposed neural models are tested on STLF problems (one day ahead forecasts,  = 1) using four The neural models were compared with ARIMA and exponential smoothing (ES). In ARIMA and ES we decompose the time series into n separate series: one series for each t = 1, 2, ..., n. This eliminates the daily seasonality and simplifies the forecasting problem. The ARIMA and ES parameters were estimated for each forecasting task (i.e. the forecast of system load at time t of the day i+) using 12-week time series fragments immediately preceding the forecasted day. Atypical days in these fragments were replaced with the days from the previous weeks. Due to using short time series fragments for parameter estimation (much shorter than the annual period) and due to time series decomposition into n series we do not have to take into account the annual and daily seasonalities in the models. In such case the number of parameters is much smaller and they are easier to estimate compared to models with triple seasonality.</p><p>For each forecasting task we create seasonal ARIMA(p, d, q)(P, D, Q) v model (where the period of the seasonal pattern appearing v = 7, i.e. one week period) as well as the ES state space model. ES models are classified into 30 types <ref type="bibr" target="#b29">[30]</ref> depending on how seasonal, trend and error components are taken into account (they can be expressed additively or multiplicatively, and the trend can be damped or not). To estimate parameters of ARIMA and ES the stepwise procedures for traversing the model spaces implemented in the forecast package for the R environment for statistical computing <ref type="bibr" target="#b30">[31]</ref> were used. These automatic procedures return the optimal models with the lowest Akaike information criterion value.</p><p>The forecasting errors for the test samples (mean absolute percentage errors which are traditionally used in STLF, MAPE = 100mean(|(forecastactual value)/actual value|))) and their interquartile ranges as a measure of error dispersion in Table <ref type="table">I</ref> are shown. In this table the errors of the naïve method of the form: the forecasted daily curve is the same as seven days ago, are also presented for comparison. The errors generated by the models were compared in pairs and the Wilcoxon rank sum test with 5% significance level was used to confirm statistically significant difference between them. The statistically best results are marked in Table <ref type="table">I</ref> with an asterisk and the second best results are marked with a double asterisk.</p><p>As we can see from this table GRNN takes the first place among tested models for all data. MLP was not much worse. In Fig. <ref type="figure" target="#fig_7">4</ref> the ranking of the models are presented based on the mean error on the test sets for all time series.</p><p>The worse among neural models turned out to be those based on SOM. The conventional forecasting models:</p><p>ARIMA and ES work significantly worse than the best NN models, but ES was better than ARIMA in all cases. Note that the ARIMA and ES are optimized on the time series fragments (12-week fragment in our case) directly preceding the forecasted fragment. In the proposed approach the neural models are optimized on the selected patterns  which are the most similar to the query pattern. These patterns represent fragments of the load time series from different periods.</p><p>More detailed results, the probability density functions (PDF) of the percentage errors (PE = 100(forecastactual value)/actual value) estimated using kernel density estimation, are shown in Fig. <ref type="figure" target="#fig_12">5</ref>. Note that only small differences can be observed in the PDF for MLP and GRNN.</p><p>The learning and optimization procedure of the winning neural STLF model, GRNN, is the simplest among other models. Note that in GRNN only one parameter is estimated, the spread. The dimensions of input and output patterns do not affect the number of parameters to estimate unlike in other tested models. This should be considered as a valuable property. The second winner, one-neuron perceptron, has n + 1 weights to learn and the number of nearest neighbors k to select for each timepoint of the forecasted period t. FCPNN and SOM models require data clustering and more complex learning. It is worth noting that the GRNN model is very similar to the STLF model based on Nadaraya-Watson estimator proposed in <ref type="bibr" target="#b31">[32]</ref>. The only difference is that we use the product kernels as RBFs in the Nadaraya-Watson model. The spreads (or bandwidths) in the case of product kernels are determined for each dimension. So instead of one parameter as in the case of GRNN we estimate n parameters. Errors achieved by both models are similar (see <ref type="bibr" target="#b25">[26]</ref>).</p><p>Patterns of the time series sequences enable us to simplify the problem of forecasting non-stationary time series with multiple seasonal cycles and trend. Time series representation by patterns can be used in various forecasting models. In <ref type="bibr" target="#b21">[22]</ref> local linear regression models based on patterns were proposed. The relationship between input and output patterns is modeled locally in the neighborhood of query pattern using linear regression. Results generated by the best linear models, i.e. principal components regression and partial least-squares regression are comparable with results generated by the best NN models (see Table <ref type="table" target="#tab_1">2</ref>). In these both linear models the components of input patterns are constructed by linear combination of original components.</p><p>Another group of STLF models using patterns: models based on the similarity between patterns of seasonal cycles are presented in <ref type="bibr" target="#b32">[33]</ref>. They include: Nadaraya-Watson estimator, nearest neighbor estimation-based models and pattern clustering-based models such as classical clustering methods and new artificial immune systems. These models construct the regression curve aggregating the forecast patterns from the history with weights dependent on the similarity between input patterns paired with the forecast patterns. Simulation studies reported in <ref type="bibr" target="#b32">[33]</ref> were performed on the same datasets as in this work so you can compare results (see Table <ref type="table">XI</ref> in <ref type="bibr" target="#b32">[33]</ref>). Table <ref type="table" target="#tab_1">2</ref> summarizes results for the above mentioned models.  <ref type="bibr" target="#b21">[22]</ref> PLSR -partial least-squares regression <ref type="bibr" target="#b21">[22]</ref> N-WE -Nadaraya-Watson estimator <ref type="bibr" target="#b32">[33]</ref> FNM -fuzzy neighborhood model <ref type="bibr" target="#b32">[33]</ref> FP1+k-means -model based on k-means clustering of concatenated x-and y-patterns (similarly to SOM1) <ref type="bibr" target="#b32">[33]</ref> FP2+k-means -model based on k-means clustering of x-and y-patterns independently (similarly to SOM2) [33] AIS1 -artificial immune system working on concatenated x-and y-patterns (similarly to SOM1) <ref type="bibr" target="#b32">[33]</ref> AIS 2 -artificial immune system working on separate populations of patterns: type x and y (similarly to SOM2) <ref type="bibr" target="#b32">[33]</ref>  The forecasting accuracy of the neural models was increased due to the local approach to the model optimization.</p><p>The model hyper-parameters were tuned in the local version of leave-one-out to get the best performance in the neighborhood of the query pattern.</p><p>In general, the proposed approaches can be applied for different power systems. The future improvement may include taking into account additional input variables such as weather conditions. This can be done by building correction models that learn relationships between errors of the forecasts generated by the basic model and weather factors. Another improvement relates to construction specialized forecasting models for atypical days. In the future work we are going to apply the proposed models to other time series representing processes and phenomena from different areas: economy, meteorology, industry etc. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The hourly electricity demand in Poland in three-year (a) and oneweek (b) intervals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) and (b). It can be seen from these figures that the daily cycles of different days of the week   {Monday, …, Sunday} and from different seasons of the year are represented by x-patterns having the same mean and variance.Using transformation (2) we unify the daily curves in y-patterns for each day type  separately. This is because in (2) we use the coding variables determined for the i-th day to encode L i+ . The y-patterns of Mondays for  = 1 are located higher than y-patterns of other days because we use i L of Sundays in (2), which are usually lower than   i L of Mondays. For analogous reasons y-patterns of Saturdays and Sundays are located at a lower level than y-patterns of weekdays. This is shown in Fig.2 (c). The dispersions of the y-patterns are also dependent on the day type  because in (2) we use coding variables D i instead of D i+ .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Fig. 2. Two fragments of the load time series (a) and their x-patterns (b) and y-patterns (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>approximates a continuous function by a piecewise constant function (step function). All input points belonging to the same cluster represented by the j-th neuron from the competitive layer are mapped to the same output point stored in the weights coming out of that neuron. To make the fitting function continuous we introduce fuzzy membership the input point x in clusters represented by neurons. Two models for STLF based on fuzzy counterpropagation NN (FCPNN) are proposed. In the first one (FCPNN1) weights of the competitive layer neurons are learned using fuzzy c-means clustering [28]. The n-dimensional input space X is partitioned into c fuzzy clusters with centers (or prototypes):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>FCPNN2 the clusters are</head><label></label><figDesc>detected in Y-space and represented by the outstar weight vectors v j . So they represent neighboring y-patterns. But now the instar weight vectors w j can represent dispersed x-patterns which do not form compact clusters in X-space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Algorithm 4 :</head><label>4</label><figDesc>FCPNN1 forecasting model optimization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The FCPNN architecture.</figDesc><graphic coords="17,162.20,83.65,248.39,175.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>real load time series:  PL: time series of the hourly load of the Polish power system from the period of 2002-2004. The test sample includes data from 2004 with the exception of 13 atypical days (e.g. public holidays),  FR: time series of the half-hourly load of the French power system from the period of 2007-2009. The test sample includes data from 2009 except for 21 atypical days,  GB: time series of the half-hourly load of the British power system from the period of 2007-2009. The test sample includes data from 2009 except for 18 atypical days,  VC: time series of the half-hourly load of the power system of Victoria, Australia, from the period of 2006-2008. The test sample includes data from 2008 except for 12 atypical days. The number of nearest neighbors in LLOO procedures in all cases was set to l = 12. Other settings are:  for MLP (see Algorithm 1): k min = 2, k max = 30,  for RBFNN (see Algorithm 2): k min = 2, k max = 20,  for GRNN (see Algorithm 3): a min = 0.2, a max = 1.6, s = 0.2,  for FCPNN1 (see Algorithm 4): c min = 2, c max = 50,  for FCPNN2 (see Algorithm 5): c min = 2, c max = 50,  for SOM1 (see Algorithm 6): c min = 2, c max = 50,  for SOM2 (see Algorithm 7): c x,min = 2, c x,max = 50, c y,min = 2, c y,max = 50,  for SOM3 (see Algorithm 8): L = 30, c min = 50, c max = 150.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Ranking of the STLF models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Probability distribution functions of the percentage errors (PEs).</figDesc><graphic coords="25,57.00,524.45,245.50,155.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>of this work is to propose and compare new neural STLF models which learn using patterns representing daily load curves. Due to patterns the problem of forecasting multiple seasonal nonstationary time series simplifies. The seasonal variations of periods longer than the daily one as well as a trend observed in load time series are filtered. Thus the forecasting model does not have to capture the complex nature of the process expressed in the time series.The forecasting results demonstrate that the neural models learned using patterns perform remarkably well. The model based on GRNN turned out to be the most accurate in STLF compared to other neural models: MLP, RBFNN, FCPNNs and SOMs, as well as the classical statistical models: ARIMA and ES. GRNN is the simplest among tested methods. It has only one parameter to estimate, the spread of RBFs. Such a model is easy to optimize and has good generalization properties. Its learning and optimization procedures are extremely fast. Another valuable feature of GRNN is its ability to predict all components of y-pattern at once. The dimension of y-pattern as well as the dimension of x-pattern do not affect the number of parameters to estimate. The second best model, MLP, predict only one component of y-pattern at once. Due to the local approach to MLP learning only one-neuron model is sufficient to approximate the target function in the neighborhood of the query pattern.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>LIST OF SYMBOLS x = [x 1 x 2 … x n ] T  X = ℝ nthe input pattern y = [y 1 y 2 … y n ] T  Y = ℝ nthe output pattern x*the query pattern y  -the forecast of y-pattern w = [w 1 w 2 … w n ] T  X = ℝ nthe instar weight vector of the neuron from the competitive layer (in SOMs and FCPNNs) v = [v 1 v 2 … v n ] T  Y = ℝ nthe outstar weight vector of the neuron from the competitive layer in FCPNNs cthe number of neurons representing clusters in SOMs and FCPNNs   {Monday, …, Sunday}day of the week the set of numbers of y-patterns from the history representing the same day type  as the forecasted y-pattern N  = ||the number of training samples in  Nthe number of training samples in  nlength of the daily period and the number of components in vectors x and y the set of numbers of the l nearest neighbors of x* in   = {x i , y i  : i  }the training set containing pairs of patterns from the history, such that y i represent the same day of the week  as the forecasted y-pattern  = {x i , y i  : i  }the set containing pairs of patterns, such that x i is one of the l nearest neighbors of x* in   = {y i : i = 1, 2, …, N}the training set for SOM3 containing y-patterns from the history  Neural network models for pattern-based short-term load forecasting are proposed  Pattern-based approach simplifies forecasting time series with multiple seasonal cycles  MLP, RBFNN, GRNN, fuzzy counterpropagation NN and SOM were compared  GRNN and one-neuron perceptron learned locally gave best results in STLF</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="2,95.00,3.34,406.22,713.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="4,95.00,3.34,406.22,713.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="5,95.00,3.34,406.22,713.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="6,95.00,3.34,406.22,713.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="7,95.00,3.34,406.22,713.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="10,95.00,3.34,406.22,713.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="11,95.00,3.34,406.22,713.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="15,95.00,3.34,406.22,713.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="19,95.00,3.34,406.22,713.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="23,95.00,3.34,406.22,713.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="26,95.00,3.34,406.22,713.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="28,95.00,3.34,406.22,713.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="29,95.00,3.34,406.22,713.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="30,95.00,3.34,406.22,713.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Forecast errors and their interquartile ranges for state-of-the-art pattern based STLF models</figDesc><table><row><cell>Model</cell><cell>MAPE</cell><cell>PL</cell><cell>IQR</cell><cell>MAPE</cell><cell>FR</cell><cell>IQR</cell><cell>MAPE</cell><cell>GB</cell><cell>IQR</cell><cell>MAPE</cell><cell>VC</cell><cell>IQR</cell></row><row><cell>PCR</cell><cell>1.35</cell><cell></cell><cell>1.33</cell><cell>1.71</cell><cell></cell><cell>1.78</cell><cell>1.60</cell><cell></cell><cell>1.68</cell><cell>3.00</cell><cell></cell><cell>2.70</cell></row><row><cell>PLSR</cell><cell>1.34</cell><cell></cell><cell>1.32</cell><cell>1.57</cell><cell></cell><cell>1.61</cell><cell>1.54</cell><cell></cell><cell>1.61</cell><cell>2.83</cell><cell></cell><cell>2.60</cell></row><row><cell>N-WE</cell><cell>1.30</cell><cell></cell><cell>1.30</cell><cell>1.66</cell><cell></cell><cell>1.67</cell><cell>1.55</cell><cell></cell><cell>1.63</cell><cell>2.82</cell><cell></cell><cell>2.56</cell></row><row><cell>FNM</cell><cell>1.38</cell><cell></cell><cell>1.38</cell><cell>1.67</cell><cell></cell><cell>1.71</cell><cell>1.60</cell><cell></cell><cell>1.66</cell><cell>2.91</cell><cell></cell><cell>2.67</cell></row><row><cell>FP1+k-means</cell><cell>1.69</cell><cell></cell><cell>1.64</cell><cell>2.05</cell><cell></cell><cell>2.17</cell><cell>1.84</cell><cell></cell><cell>1.88</cell><cell>3.34</cell><cell></cell><cell>3.01</cell></row><row><cell>FP2+k-means</cell><cell>1.59</cell><cell></cell><cell>1.51</cell><cell>1.94</cell><cell></cell><cell>2.05</cell><cell>1.76</cell><cell></cell><cell>1.84</cell><cell>3.13</cell><cell></cell><cell>2.94</cell></row><row><cell>AIS1</cell><cell>1.50</cell><cell></cell><cell>1.50</cell><cell>1.93</cell><cell></cell><cell>1.95</cell><cell>1.77</cell><cell></cell><cell>1.84</cell><cell>3.04</cell><cell></cell><cell>2.75</cell></row><row><cell>AIS2</cell><cell>1.50</cell><cell></cell><cell>1.51</cell><cell>1.93</cell><cell></cell><cell>1.96</cell><cell>1.78</cell><cell></cell><cell>1.87</cell><cell>3.33</cell><cell></cell><cell>2.93</cell></row><row><cell>where:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">PCR -principal components regression</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Algorithm 7: SOM2 forecasting model optimization.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>Algorithm 8: SOM3 forecasting model optimization.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ACKNOWLEDGMENT I am very grateful to James W. Taylor with the Saïd Business School, University of Oxford for supplying British and French data, and Shu Fan and Rob J. Hyndman with the Business and Economic Forecasting Unit, Monash University for supplying Australian data. This work was supported in part by the Polish Ministry of Science and Higher Education under Grant N N516 415338.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>number of neurons c is selected in LLOO procedure (see <ref type="bibr">Algorithm 6)</ref>. Increasing c we increase the model variance and decrease its bias (more accurate fitting to training data).</p><p>In the second approach (SOM2) inspired by <ref type="bibr" target="#b28">[29]</ref> patterns x and y from  are clustered independently by two SOMs. After learning the weight vectors of the first SOM (w x,j , j = 1, 2, …, c x ) represent centers of the x-pattern clusters, while the weight vectors of the second SOM (w y,k , k = 1, 2, …, c y ) represent centers of the y-pattern clusters.</p><p>On the basis of the training set the conditional probabilities P(n y,k |n x,j ) that the pattern y i belongs to the cluster represented by the k-th neuron (n y,k ) of the second SOM (SOMy), when the corresponding pattern x i belongs to the cluster represented by the j-th neuron (n x,j ) of the first SOM (SOMx) are estimated. (This is done by presenting the successive training pairs x i , y i  to both SOMs and counting the number of wins of each pair of neurons n y,k and n x,j .)</p><p>In the forecasting phase the query pattern x* is presented to SOMx and the j*-th neuron is selected as the winner.</p><p>The forecasted y-pattern is calculated as the mean of the weight vectors of SOMy w y,k weighted by the conditional probabilities P(n y,k |n x,j* ):</p><p>The weight vectors of these SOMy neurons, which probability of winning is the highest after having observed the winning of the j*-th SOMx neuron, have the largest share in mean <ref type="bibr" target="#b12">(13)</ref>. The numbers of neurons c x and c y were selected using grid search in LLOO procedure according to Algorithm 7.</p><p>In the third approach (SOM3) the SOM learns using only y-patterns. The training set contains all y-patterns from the history:  = {y i : i = 1, 2, …, N}. After learning n-dimensional weight vectors w j , j = 1, 2, ..., c represent centers of the y-pattern clusters. The neurons are labeled. The label contains information about days represented by ypatterns belonging to the cluster: the day numbers i and the day types . For example the label of the neuron representing five y-patterns can be in the form: 398-Tue, 399-Wed, 400-Thu, 764-Thu, 765-Wed, i.e. five entries: day number-day type.</p><p>To forecast the y-pattern having the number i* and representing the day type * all labels are searched. For each label we count the number of entries e j which satisfy two conditions:</p><p>1. the day is from the same period of the year as the forecasted y-pattern:</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Weron</surname></persName>
		</author>
		<title level="m">Modeling and Forecasting Electricity Loads and Prices</title>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Short-term load forecasting with exponentially weighted methods</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Power Systems</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="458" to="464" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Modeling electricity loads in California: ARMA models with hyperbolic noise</title>
		<author>
			<persName><forename type="first">J</forename><surname>Nowicka-Zagrajek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page" from="1903" to="1915" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Short-term load forecasting using lifting scheme and ARIMA models</title>
		<author>
			<persName><forename type="first">C.-M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-N</forename><surname>Ko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="5902" to="5911" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Computational intelligence techniques for short-term electric load forecasting</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tzafestas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tzafestas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Intelligent and Robotic Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="7" to="68" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Artificial intelligence in short term electric load forecasting: A state-of-the-art survey for the researcher</title>
		<author>
			<persName><forename type="first">K</forename><surname>Metaxiotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kagiannas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Askounis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Psarras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Energy Conversion and Management</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="1525" to="1534" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An evaluation of Bayesian techniques for controlling model complexity and selecting inputs in a neural network for short-term load forecasting</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Hippert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="386" to="395" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Short-term load forecasting of power systems by combination of wavelet transform and neuro-evolutionary algorithm</title>
		<author>
			<persName><forename type="first">N</forename><surname>Amjady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Keynia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Energy</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="46" to="57" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Applying wavelets to short-term load forecasting using PSO-based neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">A</forename><surname>Bashir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>El-Hawary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Power Systems</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20" to="27" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Short-term load forecasting: similar daybased wavelet neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Luh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Coolbeth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Rourke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Power Systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="322" to="330" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Short-term load forecasting with a new nonsymmetric penalty function</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kebriaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Araabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rahimi-Kian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Power Systems</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1817" to="1825" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">RBF neural network and ANFIS-based short-term load forecasting approach in real-time price environment</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Caixin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shaolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yuming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Power Systems</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="853" to="858" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Wavelet transform and neural networks for short-term electrical load forecasting</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Y</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Energy Conversion &amp; Management</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1975" to="1988" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A hierarchical neural model in short-term load forecasting</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">A S</forename><surname>Carpinteiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J R</forename><surname>Reis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Alves Da Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="405" to="412" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Short-term load forecasting based on an adaptive hybrid method</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Power Systems</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="392" to="401" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A SOM-based hybrid linear-neural model for short-term load forecasting</title>
		<author>
			<persName><forename type="first">V</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="2874" to="2885" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Prediction of electric load using Kohonen maps -application to the Polish electricity consumption</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lendasse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cottrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Wertz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Verleysen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. American Control Conference ACC 2002</title>
		<meeting>American Control Conference ACC 2002</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="3684" to="3689" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Forecasting electricity consumption using nonlinear projection and self-organizing maps</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lendasse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Wertz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Verleysen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="299" to="311" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neural networks for short-term load forecasting: a review and evaluation</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Hippert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Pedreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Souza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Power Systems</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="44" to="55" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Soft computing based techniques for short-term load forecasting</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Kodogiannis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Anagnostakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fuzzy Sets and Systems</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="page" from="413" to="426" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pattern similarity-based methods for short-term load forecasting -Part 1: principles</title>
		<author>
			<persName><forename type="first">G</forename><surname>Dudek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="277" to="287" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pattern-based local linear regression models for short-term load forecasting</title>
		<author>
			<persName><forename type="first">G</forename><surname>Dudek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Electric Power System Research</title>
		<imprint>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="page" from="139" to="147" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Forecasting time series with multiple seasonal cycles using neural networks with local learning</title>
		<author>
			<persName><forename type="first">G</forename><surname>Dudek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Soft Computing</title>
		<title level="s">LNCS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">7894</biblScope>
			<biblScope unit="page" from="52" to="63" />
		</imprint>
	</monogr>
	<note>ICAISC 2013</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Gauss-Newton approximation to Bayesian regularization</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">D</forename><surname>Foresee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Hagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc.Conf. Neural Networks</title>
		<meeting>.Conf. Neural Networks</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="1930" to="1935" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A general regression neural network</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Specht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="568" to="576" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Generalized regression neural network for forecasting time series with multiple seasonal cycles</title>
		<author>
			<persName><forename type="first">G</forename><surname>Dudek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Intelligent Systems and Computing 323, Intelligent Systems</title>
		<imprint>
			<date type="published" when="2014">2014. 2015</date>
			<biblScope unit="page" from="839" to="846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">R</forename><surname>Hecht-Nielsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>Addison-Wesley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Bezdek</surname></persName>
		</author>
		<title level="m">Pattern Recogniition with Fuzzy Objective Function Algorithms</title>
		<imprint>
			<publisher>Plenum Press</publisher>
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Forecasting time-series by Kohonen classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lendasse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Verleysen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>De Bodt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cottrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gregoire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Symposium on Artificial Neural Networks</title>
		<meeting>European Symposium on Artificial Neural Networks</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="221" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Forecasting with Exponential Smoothing: The State Space Approach</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Hyndman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Koehler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Ord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Snyder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Automatic time series forecasting: the forecast package for R</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Hyndman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Khandakar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Short-term load forecasting based on kernel conditional density estimation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Dudek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Przegląd Elektrotechniczny (Electrical Review)</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="164" to="167" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pattern similarity-based methods for short-term load forecasting -Part 2: models</title>
		<author>
			<persName><forename type="first">G</forename><surname>Dudek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="422" to="441" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Currently, he is an associate professor at the Department of Electrical Engineering, Czestochowa University of Technology. He is the author of two books concerning load forecasting and power system operation and over 70 scientific papers. His research interests include pattern recognition, machine learning, artificial intelligence, and their application to classification, regression, forecasting and optimization problems especially in power engineering</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
		<respStmt>
			<orgName>Czestochowa University of Technology, Poland ; from Lodz University of Technology, Poland</orgName>
		</respStmt>
	</monogr>
	<note>2003 and habilitation degree in computer science</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
