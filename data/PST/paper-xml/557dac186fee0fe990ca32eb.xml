<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">854F0BBF7D47EFDBC0AC8D85E37D9ECC</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T05:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multidimensional Indexing for</head><p>Recognizing Visual Shapes Andrea Califano, Senior Member, IEEE, and Rakesh Mohan, Member, IEEE Abstract-This paper introduces an analytical framework for studying some properties of model acquisition and recognition techniques based on indexing. The goal is to demonstrate that several problems previously associated with the approach can be attributed to the low dimensionality of invariants used. These include limited index selectivity, excessive accumulation of votes in the look-up table buckets, and excessive sensitivity to quantization parameters. Theoretical results demonstrate that using high-dimensional, highly descriptive global invariants produces better results in terms of accuracy, false positive suppression, and computation time.</p><p>A practical example of high-dimensional global invariants is introduced and used to implement a 2-D shape acquisitionhecognition system. The acquisitiodrecognition system is based on a two-step table look-up mechanism. First, local curve descriptors are obtained by correlating image contour information at short range. Then, seven-dimensional global invariants are computed by correlating triplets of local curve descriptors at longer range.</p><p>This experimental system is meant to illustrate the behavior of a high-dimensional indexing scheme. Indeed, its performance shows good agreement with the analytical model with respect to database size, fault tolerance, and recognition speed. Model acquisition time is linear to cubic in the number of object features. Object recognition time is constant to linear in the number of models in the database and linear to cubic in the number of features in the image. The system has been tested extensively, with more than 250 arbitrary shapes in the database. Unsupervised shape and subpart acquisition is demonstrated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>BJECT recognition is a central problem in computer 0 vision. It involves a set of object models that must be recognized in images. General surveys on model-based object recognition systems can be found in [SI and 1201. The basic paradigm contains three components: matching, pose computation, and verijication <ref type="bibr" target="#b34">[36]</ref>. During matching, features extracted from an image are compared and associated with features of the object models. As all feature combinations may have to be explored, brute-force matching is computationally equivalent to an exponential search. One distinguishing contribution of various recognition systems has been solutions for cutting down the complexity in this area. However, even when techniques aimed at optimizing this process are used (e.g., alignment <ref type="bibr" target="#b33">[35]</ref>), each model in the database must be separately considered. In applications with large databases, this computation burden has to be borne every time an image is processed.</p><p>Recently, an alternate paradigm called indexing or hashing has been proposed <ref type="bibr">[40]</ref>, <ref type="bibr" target="#b19">[21]</ref>. In indexing, the feature correspondence and search of model database are replaced by a table look-up mechanism. The latter approach is described in detail in Section I1 and in the rest of this paper. Indexing schemes share a uniform underlying structure. They compute invariants from an image that are then used as indexes to look-up a table containing references to the object models. The table look-up returns a list of candidate models with associated weighs indicating their likelihood. Proposed indexing schemes mainly differ in the choice of invariants employed as indexes.</p><p>The benefits of indexing over traditional search-based matching schemes should be specifically evident in applications involving large model databases. Such applications could include image databases for multimedia applications, information services, libraries, and archives where the number of objects of interest ranges in the thousands. Indexing does not require considering each model separately and is thus less dependent on the database size. Additionally, indexing offers a straightforward approach to automatic or semiautomatic model acquisition, a useful feature in most applications.</p><p>Although there is an abundance of proposed indexing schemes <ref type="bibr">[38]</ref>, <ref type="bibr">[40]</ref>, 1211, there have been few computational principles guiding their design. Some Straightforward error analysis is provided in <ref type="bibr">[43]</ref>, while an estimate of the upper bounds on the database size for an optimally designed similarity geometric hashing techiques is given in <ref type="bibr" target="#b48">[50]</ref>.</p><p>Given the lack of a computational framework and the practical difficulty of rigorously testing such experimental systems on large libraries under varying conditions, it is difficult to evaluate the performance of indexing schemes. This is especially true in the context of their natural applications, namely large model databases. This problem has recently been highlighted by Grimson and <ref type="bibr">Huttenlocher [30]</ref> in their critique of geometric hashing. More recently, some papers have been aimed at answering these remarks, and to extend the accuracy and capacity of the original geometric hashing with the introduction of a Bayesian scheme <ref type="bibr" target="#b48">[50]</ref>.</p><p>This paper presents a computational framework in which acquisitionhecognition techniques based on indexicg are analyzed. The criteria include computational efficiency, noise tolerance, and database as a function of the amount of information contained in the invariants used as indexes. A result of our analysis is that for indexing schemes to be practical, they must take advantage of look-up tables containing a large 0162-8828/94$04.00 0 1994 IEEE number of coarsely quantized entries. We propose that this can be best accomplished using very descriptive invariants and indexes, typically of a high-dimensional nature. In the context of this paper, we will therefore use the term high dimensional to indicate invariants that have a large number of coarsely quantized distinct values.</p><p>The following analysis compares high-dimensional indexing techniques to the more conventional low-dimensional schemes, such as geometric hashing. A first result is that the use of high-dimensional indexes drastically increases the signal-tonoise ratio of the approach. That is, the number of votes for randomly generated incorrect hypothesis becomes smaller as the dimensionality of the indexes increases, while the average number of votes for correct hypotheses is kept constant. A second, equally important, result is that the amount of computation required by the technique is significantly reduced. Section 111 presents this analysis in detail.</p><p>We will show that the use of high-dimensional indexes is crucial to overcoming some drawbacks of table look-up based techniques such as geometric hashing 1381, <ref type="bibr">[42]</ref> when large object model databases are considered. These include poor index selectivity, excessive accumulation of votes in each bucket, sensitivity to noise and quantization parameters, and probability of false positives. The latter would require further processing to be successfully eliminated.</p><p>Crimson [30], for instance, suggests that geometric hashing performs well on simple scenes with little sensor noise, but that its performance degrades significantly even with limited amounts of clutter or perturbation. This paper shows that these problems, far from being inherent limitations of the underlying approach, are mainly a result of the low dimensionality of the indexes of choice. In Grimson's error analysis, buckets are characterized by 2-D invariant vectors of limited descriptive power. With these assumptions and a reasonable choice of quantization parameters, a practical table would have on the order of 10' useful buckets. Even under ideal conditions (e.g., uniform index distribution in the table), as the number of object models stored in the table increases, the number of entries per bucket becomes inevitably large. This will be discussed in detail in Section 111. When quantization effects, nonuniform distribution of the indexes, and projective distortion of the objects are taken into account, the complexity of the analyzed scenes, the maximum size of the database, and the computation time become issues, as pointed out in <ref type="bibr">[30]</ref>.</p><p>On the other hand, our analysis shows that indexing techniques based on higher dimensional look-up tables are faster, can reliably handle very large model databases. and can reduce the occurrence of false positives. As a practical example, in Section IV we will introduce an experimental 2-D acquisition/recognition system based on a seven-dimensional table look-up mechanism. This test system uses global shape descriptors that are invariant to 2-D similarity transforms. The corresponding look-up table can hold about 106 buckets. Assuming a perfectly uniform index distribution, and on the order of 10" indexes per model, the resulting shape table could accommodate on the order of IO3 objects before saturation (i.e., before its average number of entries per buckets becomes equal to I ). Under similar assumptions, standard geometric hashing techniques would be limited to a table size of about 10' buckets. This could hold a scarce 10 objects before saturation, a two orders of magnitude difference. Nonuniform index distributions would degrade performance similarly both in the low-and high-dimensional approaches. Since rehashing methods <ref type="bibr" target="#b48">[50]</ref> can alleviate the problem of nonuniformity, we do not consider it an issue. Additionally, in our example, coarser quantization is selected along each parameter axis (i.e., fewer buckets per axis) with respect to that assumed for standard geometric hashing. This offers the benefit of an increased noise resilience. The relationships between quantization and index dimensionality is studied in detail in Subsection 111-D.</p><p>The novelty of the proposed approach is the use of correlated complex local shape parameters (i.e., four-dimensional local shape descriptors) to produce viewpoint invariant, highdimensional global descriptors. These descriptors are used as indexes in a look-up table to identify the 2-D shapes from a database and recover their position, scale, and orientation. Local shape descriptors are also extracted within a look-up table paradigm by correlating over short distances information such as local edge orientation and position.</p><p>The seven-dimensional invariant used in the test 2-D acquisition/recognition system is intended to be only one practical example of a high-dimensional global descriptor. Further andysis will undoubtedly lead to more robust and/or higher dimensional global invariants. In fact, such invariants based on the long-distance correlation of lower dimensional local shape descriptors can be easily obtained in a number of different ways. These include using nongeometric parameters, e.g., intensity ratios at different image locations, color, range information, and other visual clues. This makes high-dimensional indexing a viable candidate for analyzing a variety of inputs modalities, not limited to 2-D contour shapes.</p><p>The design of the test system [I81 has allowed us to focus on various design issues (beyond that of index dimensionality) involved in the making of a practical working recognition system. These include local versus global descriptors, featuresubpart hierarchy, and heuristics for reducing recognition time. Finally, the proposed approach tries to address what we consider important limitations of conventional table look-up techniques. For instance, they often fail to offer a straightforward mechanism to support important notions in shape representation, such as subparts identification and hierarchical database organizations. The system is described in Sections IV through X, and some experimental results arepresented in Section XI.</p><p>Again, we wish to emphasize that we are presenting a specific implementation of the high-dimensional indexing paradigm. The test system is an example intended mainly to illustrate some advantages of the approach. Many other alternative embodiments are possible, and we make no claims regarding the specific robustness of the global invariant of choice. The computational analysis of Section 111, however, is of a general nature and is valid for any other indexing schemes regardless of the specific choice of invariants. Therefore it can be used to guide the design and predict the performance of any such system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">INDEXING</head><p>In general, index-based recognition systems compute invariants from an image and are then used to index in a look-up table During model acquisition, the locations (buckets) of the table so indexed are filled with entries containing references to object models and some additional parameters. The latter, for instance, can be used for pose recovery. During recognition, the models listed in the indexed entries are collected into a list of candidate models. Next, the most likely of the candidate models are selected on the basis of the number of votes they have received, i.e., the number of times they were indexed. Recognition may include pose computation and verification. The look-up tables used in the process are usually sparse in that, on average, only a few of their entries are used. The lookup tables are, therefore, often implemented as a hash tables to save storage.</p><p>One well-known index-based approach is geometric hashing [42], [40], <ref type="bibr" target="#b19">[21]</ref>. In its basic form, geometric hashing handles 2-D shapes under similarity (affine) transformations. In the image, two (three) feature points are chosen as a reference frame, also called a basis, and the orthogonal (affine) coordinates ( a , @ ) of each other model point in that basis are used as indexes. Hence, the dimensionality of the index is 2. Each indexed entry is updated with the basis vectors and with a reference to the corresponding object model. This process is repeated using each couple (triplet) of model points as a basis. In its original conception, geometric hashing is therefore a single-step, bottom-up scheme. That is, object models are recognized directly from sampled image points without grouping or correlating any intermediate geometric features or subparts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Indexing Versus Matching</head><p>The benefits of indexing over traditional matching-based schemes should be specifically evident in applications involving large collections of object models. Typical examples would be image databases for multimedia applications, information services, libraries, archives, and department stores, where the number of objects that need to be recognized number in the thousands. Indexing does not involve a search over the image database and is thus less sensitive to the size of the model database. For this reason it is very important to study how well these techniques scale with respect to the number of object models in the database.</p><p>In these applications, model acquisition is performed only once, usually off-line, while recognition is performed repeatedly, often under timing constraints. The acquisition step can thus afford to be computationally expensive, while recognition must be as computationally efficient as possible. This is precisely what happens in the case of indexing techniques, where most of the computational load is shifted from recognition to acquisition time. This is accomplished by precomputing a large number of model representation invariants and storing them in fast access look-up tables.</p><p>Another benefit of indexing over matching for large model databases is automated model acquisition. Automated model acquisition is important if a system has to deal with a large number of objects. Yet, many recognition paradigms lack the means for automatic model building <ref type="bibr">[241, [281, [331, [371, [381, [471, 1531, [ W .</ref> The object models, rather than being acquired from raw data, are either hand crafted or generated assuming that precise geometric information is available. Model database organization and indexing schemes are critical whenever a scalable recognition behavior is to be accomplished. On the other hand, in indexing the scheme for acquiring models is similar to that for recognition. Models are acquired simply from characteristic images of the objects, making precise 3-D models or CAD/CAMtype models superfluous. Model acquisition is incremental. A system does not have to be recomputed from the start if a model must be added (or deleted).</p><formula xml:id="formula_0">[2], [6], [7], [lo], [12],</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Local Versus Global Features</head><p>Footprints [38] and structural indexing [55] use highdimensional indexes. This is due to the specific choice of invariants and has not been motivated by any underlying computational scheme. Both techniques use local shape descriptors directly as indexes.</p><p>Local shape, however, tends to be a less discriminating visual clue in large databases where each different model may share a substantial portion of the local shape structure with many others. A more discriminating factor is the geometrical conjiguration of local shapes. In general, indexes that convey global configurational characteristics are more selective than their local counterparts. This can also be understood by considering that, by definition, the amount of visual information associated with a local shape descriptor is lower than that associated with a combination of several such descriptors and their relative geometric configuration on the object model. As a consequence, locally derived indexes must be quantized at a finer grain to obtain an equivalent range of quantized values. This has the undesired effect of making them more susceptible to noise.</p><p>A similar situation, where local versus global descriptors are compared, can be found in the discussion of single-versus multi-window Hough transforms [ 151.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Segmentation</head><p>Segmentation is not required in indexing, but even partial segmentation will aid any indexing scheme. Various bottomup techniques, such as region segmentation and perceptual organization <ref type="bibr" target="#b44">[45]</ref>, will help. Indexing schemes in general have ignored the issue of segmentation and have been implemented without it. <ref type="bibr" target="#b19">[21]</ref> relies on simple perceptual organization to aid indexing.</p><p>We have experimented with simple heuristics similar to the local focus feature approach of [lo]. This has allowed us to reduce the recognition complexity from cubic to linear in the number of image features. The scheme is described in detail in Section IX-B. There are, however, shortcomings to using such heuristics, such as scale dependency and the need for feature saliency assignment. Another viable scheme is random sampling with a threshold on the number of votes. This latter heuristic is similar to various search cutoff schemes <ref type="bibr" target="#b27">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="111.">MULTIDIMENSIONAL INDEXING</head><p>The following analysis compares high-dimensional indexing techniques [ 181 to the more conventional low-dimensional schemes, such as geometric hashing. Not unexpectedly, a result is that the use of high-dimensional indexes drastically increases the signal-to-noise ratio of the approach. That is, the number of votes for randomly generated incorrect hypothesis becomes smaller as the dimensionality of the indexes increases, while the average number of votes for correct hypotheses is kept constant. Another important result is that the amount of computation required by the technique is significantly reduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. A Framework for Indexing Techniques</head><p>To justify this claim let us introduce a formal framework for indexing techniques. Define {Si}:&gt;: as a set of model shapes.</p><p>For each model shape S;, a number Ns? of independent ddimensional vector indexes aj = (01,. t r z j , . . . % a d J ) ; j = 1. . . . ~ *Wst is generated. These are usually invariant with respect to a given coordinate transformation (similarity, affine, etc.). Later we will use Ns as the average number of index generated from a model shape.</p><p>In the case of geometric hashing with similarity transforms, for instance, the invariant indexes are 2-D bases ( a , @ ) that represent the Cartesian projections of a model shape point on the Cartesian frame defined by other two shape points (see </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Storage:</head><p>For each index a,, a tuple (Sz! G) is then appended to the entry of a hash table Ho indexed by the indexes a,, where Si is the reference to the ith model shape and G is a vector of geometric parameters that can be used to recover the complete (partial) pose of the model shape in an image. For example, G could contain the coordinates of the center of mass of. the shape in the Cartesian or affine base determined by the index aj.</p><p>After the process is repeated for each model shape, entries in the table Ho will contain lists of ( S ; G ) tuples. We will use and ( N E ) for the number of tuples ( S ; G ) that each bucket of the table Ho will hold on average. Recognition: NI indexes ak: are generated in a similar way from an image. The value of N I usually depends on the number of objects in the image. The indexes are used to retrieve entries in Ho, that is, lists of (Si, G) tuples. Each reference Si corresponds to a model shape that, with the proper pose in the image, could have generated the index.</p><p>The complete (or a partial) pose P = ( ( 2 0 , yo),?y, s) of the shape is recovered, using the coordinate base associated with the index ak and the vector G . Each tuple (S;,P) thus generated from the list in HO is treated as evidence for the corresponding model shape hypotheses at the corresponding pose in the image. This evidence is accumulated in a second hash table H I , where entries are indexed by the tuples (Si; P)</p><p>and have a value proportional to the number of times that the corresponding tuple has been generated by the image indexes. possible shape hypothesis. If Np is the number of different quantized poses that a model shape can have in the image, the table H1 will have a virtual number of entries given by N H ~ = NltINp, where N,tr is the number of model shapes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H I is therefore a histogram of the votes received by each</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Outline of the Analysis</head><p>The goal of the analysis is first to identify the role of the different parameters N n f . G (the average number of index generated from a model shape), etc., on the behavior of indexing techniques. The second objective is to understand the mechanism responsible for false positives with large databases of model shapes. Finally, we will show how using more descriptive higher dimensional indexes decreases the probability of false positives and reduces the amount of computation required. This will be accomplished by studying the behavior of N G ( ~) (the average number of votes for a correct hypothesis) and P B ( ~. d) (the probability of getting k votes for an incorrect hypothesis because of random cooccurrences) as a function of d (the index dimensionality). Specifically, we will keep N G ( ~) constant as d grows, by simultaneously increasing the number of indexes per model or using coarser quantization of the indexes. We will then show that PB ( k , d) effectively decreases as d increases for values of k around G ( d ) .</p><p>In the analysis, we will assume uniform distribution of the indexes in Ho for simplicity. A nonuniform distribution produces identical effects on the low-and high-dimensional indexing. Therefore. the comparison between the two still --holds. Also, a number of techniques can be used to generate a more uniform distribution (see for instance <ref type="bibr" target="#b47">[49]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Index Formation Techniques</head><p>Typically, indexes are formed by first generating tuples of interest features (e.g., interest points) and then by using their global configuration and associated information to produce the indexes. If I is the size of the tuple, and the model contains 71 points, we can then generate up to (7) 1-tuples. The total number can be bounded by means of a variety of techniques that will be mentioned in the following.</p><p>We will restrict our analysis of high-versus low-dimensional techniques to the cases where the number of local interest features in a tuple, I , is constant. The increase in dimensionality will therefore come from the use of extra local information, such as 1st-and 2nd-order local shape properties, color, texture, or other visual clues that can be used with the 1-tuple geometric structure to generate invariants. Under this assumption, the number of considered 1-tuples, both at storage and at recogntion time, is constant and independent of the dimensionality of the approach.</p><p>Then, the effects of occlusion can be simply modeled as follows. Let us assume that a minumum ratio of unoccluded local interest features p , is available. That is, pu is the ratio of unoccluded local interest features with respect to the total number of local interest features in an image. Then, if 1 is the tuple size and NT is the number of l-tuples, then at least N ~p e are unoccluded. We can simplify this by defining a minimum bound pu = p i on the probability that an 1-tuple is unoccluded. This probability is constant and independent of the dimensionality d of the invariants that have been selected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Problems with Conventional Approaches</head><p>The limit of indexing techniques lies primarily in the finite size of the first hash table Ho. As the number of models NRI stored in Ho grows larger, the lists of tuples (&amp;, G ) stored in the table will also increase. This leads to two important consequences:</p><p>The time required to process an image TE( N,,*) increases since all the indexed tuples must be processed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The probability P ~( l c , d ) of having many votes (large values of IC)</head><p>for incorrect hypotheses generated by random cooccurrences increases. This may ultimately lead to false positives, that is, incorrect hypotheses that have more support than correct ones. Correct Votes: To demonstrate these claims, let us first determine the average support that a model shape in the image will generate for the corresponding hypothesis in H I .</p><p>Given a certain level of noise, a noise model, and a quantization step Aai for each axis of the index vector, there will be a probability that the quantized value of index computed from a noisy scene will match the correct index generated at storage time by a given model shape present in the image. Here the p g , are the probabilities that there is equivalence on the ith axis of a d- dimensional index vector. If the values of the pgT are assumed, for simplicity, to be all equal to pg we will have</p><formula xml:id="formula_1">pG((1) = pt ( 2 )</formula><p>Then, with the average number of indexes generated per model shape, the probability of getting k matching indexes from a model shape in the image can be obtained from the binomial distribution</p><p>The average number of matching indexes is then</p><formula xml:id="formula_2">(3)<label>(4)</label></formula><p>From the previous section, p~ is the worst case probability that any such index is not destroyed by occlusion. The average effect of occlusion could then be roughly modeled by introducing pu in the estimate of z ( d ) by replacing p: by p ~~p : . As stated before, pc. is independent of d and is rather a funtion of the number of independent local interest points used to generate the indexes.</p><p>A similar approach can be used to model image clutter. The main effect of clutter, i.e., multiple objects in the scene, on indexing techniques is to reduce the probability of generating correct indexes while increasing that of incorrect ones. Both average effects can be modeled by smaller values of pg. For r n unoccluded object models in a scene, for instance, the probability of generating a correct index for a specific object model is p i / 7 n l , where 1 is again the number of independent local interest points used to compute the &amp;dimensional index. Heuristics such as the radius of coherence (Section IX-B) or the local focus feature approach [lo] can be used to bring this value close to p i / r r ~. For the sake of simplicity, in the following discussion we will use only the term p,, by factoring in it both the occlusion and clutter contributions. In any case, the negative contribution of clutter will affect both techniques in the same way since the number of local features 1 used to generate the indexes is independent on the dimensionality of the technique. This is consistent with the comparative nature of this paper.</p><p>Since for each correct index the corresponding list in H0</p><p>will contain at least one correct reference to the corresponding shape, the average number of votes that correct hypotheses will receive in H I is at least equal to G</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>( d ) . T G ( c t ) 2 z(it).</head><p>The inequality sign reflects random votes that may accumulate in the correct hypothesis bucket. The probability of getting k votes for a correct shape-pose pair is then P G ( ~, d ) or better.</p><p>It is obvious that unless ps z 1, these values will strongly depend on the dimensionality of the index. Lower dimensionality will in general correspond to a higher percentage of correct votes. On the other hand, incorrect hypotheses with many votes will also be more probable. To study the use of higher dimensional indexes objectively, we will normalize %( d ) so that it is kept constant with respect to (1, that is, </p><formula xml:id="formula_3">- N s ( d ) = N s ( l ) / p f -l . pG(d* A%) = p ; ( A a d ) (6) is constant with respect to d. That is p ~( d , And) = p G ( d -1, Aad-1) = ' . . = pc(1, A N I ) .</formula><p>By a combination of the above techniques. To avoid a larger number of indexes to process as d grows, we will rely on coarser quantization.</p><p>Using Coarser Quantization: Let us assume a Gaussian model with variance 0 . For simplicity's sake, we will assume the same model over all the index axes. If the quantization step Aa is of the same order as 0, then doubling or redoubling Acu should make the probability ps z 1. Therefore, from (6), reducing the number of quantization intervals nb by a small factor (i.e., 2 , s . . .) satisfies the requirements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>p G ( d , A t r d ) = p G ( d -1 , A a d -l ) = _ ' ' = p G ( l , h a l ) . (7)</head><p>A more interesting case is when the original AN is small compared to 0 . In this case, a much larger reduction in the number of good votes results when high-dimensional indexes are used. We will analyze this as a worst-case scenario. Since Aa is smaller than the width of the error distribution the probability of an index generated during recognition to match the corresponding one stored at acquisition will be the sum over all ith quantization steps of the combined probability that both measurements fall within the same ith quantization step of width Aa. Here, xo is the expected value for the measurement and A" the start of the useful range of values on the index axis (assumed to be the same on all axes for simplicity). The probability of a match is then where n b ( d ) the number of quantization steps on each index axis as a function of d (also assumed to be the same on all axes for simplicity). If A is the range of admissible values for the index along an axis, 7 1 b ( d ) is computed as In any case, the probability of a correct index match will be larger than </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Probability distribution of one index value with Gaussian noise</head><p>which is the approximate value of the sum in (9) over the quantization step closest to the midpoint in the Gaussian (see gray area in Fig. <ref type="figure">2</ref>). We will use this value as a lower bound.</p><p>It is then possible to compute a pessimistic estimate of how much more coarsely we should quantize the index axes in order to insure a constant average number of correct votes as d increases. The requirements of (7) are satisfied by choosing fewer quantization steps as d increases, as expressed by where p g l = p,(l) and 7261 = n b ( 1 ) , for simplicity. This corresponds to a larger quantization step, Incorrect Votes: The mechanism that allows incorrect votes to accumulate in a given entry of H1 (model-pose hypothesis) is slightly more complex. First, we must distinguish between correct indexes and incorrect ones. Let us consider a correct index generated from the image, that is, one that matches exactly the one produced by the corresponding model shape at storage time. Then, at least one of the (Si, G ) tuples in the corresponding entry in the table HO will be correct. Thus, if NE is the average length of the list stored in Ho, on average the other NE -1 tuples will vote for random model shapes that happen to share the common index.</p><p>Since the model shapes that share an index are typically completely uncorrelated, it is reasonable to assume that these incorrect votes will spread uniformly over H I .</p><p>If the index is incorrect, all of the entry tuples in the corresponding HO entry will support incorrect hypotheses. Therefore, on average, N E incorrect votes will be generated.</p><p>Since in general NE &gt;&gt; 1, we can ignore the correct contribution in the first case and assume that any image index will in general generate X E incorrect votes that will spread uniformly in the second table H I .</p><p>Under this assumptions, the probability that an entry in H1 randomly accumulates k votes is given by the binomial  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P . ( k d )</head><p>Since X G ( d ) is kept constant, we can determine the behavior of the technique by looking at the ratio P B ( ~, d ) / P ~( k ,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1).</head><p>If this value is reduced as d increases, then the use of highdimensional indexes is effective in reducing the chances of false positives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">k ( d -)</head><p>--2;::;; -(ai</p><formula xml:id="formula_4">N I iv 'V &lt; 4--[ ("*&amp;-l 1 (19) 1 (1-m)</formula><p>The most important factor in this equation is the value of nb&amp;.</p><p>If this is greater than one, the probability that an hypothesis will receive a large number ( k large) of votes is reduced exponentially with both Ic and d. If the value is smaller than one, the opposite effect will be observed. Also, the second term in (19) will be almost indeDendent of d. n h . K is larger than 1 unless the noise produces complete indetermination on the value of the index.</p><p>If Aa is the size of the quantization on the parameter a , and A = Al -A0 is the allowable range of values, then n b = A/&amp;. In this case, we have n b f i = &amp;,, which is greater than 1 unless the Gaussian width is larger than the allowable range for the parameter.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>changes.</head><p>Recognition time versus database size as dimensionality of index</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Time Requirements</head><p>As d increases and G ( d ) is kept constant, F E ( $ ) becomes Since nbyg is always larger than 1, the value of K(d) will then decrease exponentially with d. In the best case, when both look-up tables are implemented as arrays, the average time required to process one image index is given by where to is the time required to compute the index and to access the corresponding entry in Ho and tl is the time required for generating the index and updating the corresponding entry in HI. Therefore, the total time required to process the NI image indexes will be Fig. <ref type="figure">5</ref> shows the time to process 1000 image indexes as the model in the database grows from 1 to 1000 and for values of d = 2 (geometric hashing) to d = 7 (multidimensional indexing). Assuming that the hash table is stored on disk (for large databases), we choose t o = 0.1 s and tl = 0.01 s. The other fixed parameters have the same value chosen in the previous subsection. As we can see, although the behavior is linear in both cases, the growth rate is significantly different, leading to much faster recognition times for the higher dimensional case.</p><p>When the size of the table is approximately lo6, the linear growth constant is so small that recognition time on a shape table with 100 objects is only 10% larger than the one with a single object. By comparison, with a table 10 times smaller, the increase is of 123%. The memory required in the two cases, however, is the same since the number of entries stored in the table is not a function of its total size and sparse data can be represented in a compact form by means of a hashing paradigm.</p><p>Index-based techniques can perform recognition when the average number of entry per bucket is much above saturation, i.e., larger than 1. Nonetheless, as this value keeps increasing, so do the recognition time and the chances of detecting false positives. Besides, having fewer entries per bucket corresponds to a higher discriminating power of the table. Fewer shape instance hypotheses are produced, so the clustering process in parameter space or the checking of the candidate object model become much easier tasks. Since the performance of the table is based on its size and sparseness (see <ref type="bibr" target="#b20">(22)</ref>), shape tables are ideally implemented as hash tables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. APPROACH OUTLINE</head><p>To substantiate in practice the results of Section 111, we have implemented a 2-D recognition system based on highdimensional indexing associated with global descriptors. This section and the following one should therefore be viewed as the description and study of one out of many possible implementations of a high-dimensional indexing scheme. We make no claims for the robustness and stability of the invariants used as indexes other than they are high dimensional (e.g., very descriptive) and, to the extent of our purposes, invariant to similarity transformations. Better descriptors can be devised, more refined techniques for detecting local shape can be used, and better ways for correlating local shape over long distances can be identified. The point here is only to provide a simplified test system to study the practical behavior of a recognition system based on the above-discussed approaches.</p><p>A two-step process is proposed (see Fig. <ref type="figure" target="#fig_9">6</ref>). In the first stage local features are detected from a 2-D contour image. Edge-curves are segmented and labeled on the basis of their local shape. An indexing-based recognition scheme is used to recognize curve shapes. This is only to show that a homogeneous approach to recognition where only indexing is used is viable. Other fundamentally different schemes could be equally well used to detect the local features.</p><p>In the second stage, the local curve features are used to compute high-dimensional invariants. These invariants are used to index into a shape table containing references to object models. Thus the scheme performs indexing twice: First, shortrange autocorrelation operators on edges are used to index into a small set of simple localized shape descriptors, and next global autocorrelation operators on local curve shapes are used to index into a global look-up table that contains the shape model representations.</p><p>During model acquisition, one or more characteristic views of the model are presented. Local curve-shape features are detected, and high-dimensional indexes are computed from combinations of the curve features. At each corresponding indexed location in the shape table (bucket), an entry is appended with a reference to the model and specific parameters to recover pose. During recognition, high-dimensional indexes are computed with an identical approach. These indexes are subsequently used to index into the shape table and to recover all the models stored in the respective table entries. Also, from the entry and the configuration of the local shapes used to compute the index, the pose-i.e., the location, orientation, and scale of the identified shape-is computed. The hypothesized shape instances (i.e., models and their pose) are then ordered on the basis on the number of times they were indexed. Only those shape instances with the best match to the input data are selected, thus effectively segmenting the image into distinct recognized objects.</p><p>A performance 'analysis with respect to fault tolerance and recognition behavior with a large number of shapes (= 290) is given in Section X. Both acquisition and recognition are efficient; specifically, recognition is not exponential in the problem size as it is the case for many systems <ref type="bibr" target="#b27">[29]</ref> and grows very slowly with the number of models in the database.</p><p>It grows approximately as K + cQ, where K is a constant time, Q is the number of objects in the database, and c is of the order of 0.001. In Sections V through IX, the approach is discussed in detail and compared with existing techniques. Section X deals with subpart detection. Finally, Section XI provides some examples to support the theoretical analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. SHAPE AUTOCORRELATION</head><p>The computation of indexes, both for the local curve features and for the global high-dimensional invariants, is based on the concept of shape autocorrelation <ref type="bibr">[ 171.</ref> We introduce shape autocorrelation operators and describe their application in a generalized parameter transform framework. Some notions are recalled from previous work [ 151, <ref type="bibr">[ 161.</ref> The main result here is that for computing shape descriptors, in our case invariant indexes, it is better to use information spatially distributed over the object shape rather than at localized portions of the shape (as is done in footprints <ref type="bibr">[38]</ref> or structural hashing [ S I ) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Extending the Generalized Neighborhood Framework</head><p>The generalized Hough transform, proposed by Ballard in <ref type="bibr" target="#b43">[4]</ref>, has shown how stochastic evidence integration techniques can be applied to the recognition of arbitrary shapes in 2- space. However, due to the locality of the parameter estimation technique typical of the Hough paradigm, the complexity of the model search space becomes large once arbitrary rotation and scale transformation are allowed in the input.</p><p>Recent work [15], [I61 has shown how it is possible to further generalize the notion of parameter transform to include a mechanism for fusing evidence embedded in distant portions of the image. This is achieved by extending the neighborhood concept to include compact nonconnected data set (generalized neighborhoods) and by devising transform operators for this new sparse data structure.</p><p>In the usual parameter transform formulation, a local mapping operator f ( P , 5 : y) is devised to estimate the likelihood of the presence of a specific feature P = (PI. p2 . . . ~ p s ) , in a small local neighborhood A ( z . y) in the image space, centered around the point (x, v). Here, the parameters p 1 , p 2 , . . . , p s uniquely identify the feature of interest. Evidence integration is performed by integrating the estimator over the entire image:</p><formula xml:id="formula_5">P = J f ( P , 3 ' , y ) d z d y . (<label>23</label></formula><formula xml:id="formula_6">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head><p>The resulting value is a confidence measure on the presence of the corresponding feature in the input. By repeating the operation for each possible feature, a density function over the parameter space of the features is generated. Peaks in the density function indicate high values of confidence with respect to the corresponding feature. Selection techniques are used to isolate the peaks from the background noise.</p><p>The mapping operator, in the case of generalized neighborhoods, is structured as a shape autocorrelation function of the form E P , : t . . v , ) = .I,,,, ...J where k determines the order of the correlation function. A( { ( x , y)}) is a neighborhood for partial evidence integration which can depend on the values of the different (z: y). Such neighborhoods, usually, can be the entire image or a portion of the image centered around (x,y).</p><p>Parameter transforms based on generalized neighborhoods achieve two or more orders of magnitude better accuracy in the parameter estimation of simple parametric features. They have been used to reliably extract up to eight-dimensional parametric features, such as conic sections in 3-space from range data [ 161. Specific advantages of autocorrelation mapping operators are discussed in <ref type="bibr">(161.</ref> In conventional nonparametric feature extraction paradigms, such as the generalized Hough transform, analytical operators are replaced by a look-up table. Identically, in our framework, shape autocorrelation operators are replaced by a multidimensional table look-up mechanism where the indexes are function of various parameters extracted at different location over the image. For instance, given three points (XI, yl). (ZZ? PZ), and</p><p>( 5 3 , y3), one could have an index ( 9 , %), where</p><formula xml:id="formula_7">sz = J ( 2 1 -.3)2 + (y1 -y3)2 s = J(.l -.2)2 + (y1 -y2)2 + J(. z -5 3 1 2 + (Yz -y3I2 + J(T3 -.1)2 + (Y3 -Y1)2. (25)</formula><p>Here, the index ( 2 . 2 ) is invariant with respect to similarity transformations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. INDEX COMPUTATION</head><p>We present the general scheme for computing invariant indexes using spatial autocorrelation operators. Since the basic principles for computing indexes is the same for both the local curve-feature detection stage and the following object detection (or acquisition) stage, they are treated here together. Later, we will describe in detail individually the procedure for detecting local features and the procedure for object recognition/acquisition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Shape Autocorrelation Operutors</head><p>In its most general formulation, we define a shape autocorrelation operator of order k to take a combination of k different feature points as an input and to return an ntuple, called an index. The index is computed on the basis of the geometrical relationship between the k points and local properties at those points. These local measures can include the 0th-and 1st-order properties (location and tangent) of the edge points or higher order local information such as local curve shape. The combinations of k points considered can be constrained in various ways. For example, to extract local curve shape, the combinations can be limited to points lying in small local neighborhoods on the same connected curve piece, while for complete object recognition, the points may lie in distant portions of the image and on different curves.</p><p>The operator of (25) that generates the 2-D vector ($ a) from three points, for instance, is one of the simplest possible Yd-order shape autocorrelation operators.</p><p>In a kth order shape autocorrelation transform, first a set of k-tuples of points is generated from an image. Successively, shape autocorrelation operators are applied to the k-tuples, and the result is used to index in a look-up table. Finally, the output from the table is used to produce a density function on a parameter space where vectors correspond to specific feature instances.</p><p>' . S</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Index</head><p>In the domain of similarity transforms on 2-D shapes, 3rdorder spatial autocorrelation operators are more than adequate, since rotation, scale, and translation between model and model instance can be uniquely determined from two corresponding points. Each feature point in the image can be associated with all possible combinations of two other points, thus forming a set of triangles (see Fig. <ref type="figure" target="#fig_11">7</ref>). For each triangle, for instance, we can generate a four-dimensional index a l , cyz) consisting of two ratios 9 and 2 (where S = s1 + s2 + sg).</p><p>which determine the geometry of the triangle, and the two angles a1 and az, which describe lSt-order properties of the curve with respect to the given 3-point set. Here, (11 is the angle between the tangent at p 2 and line p2p3: cyz is a similar angle for point p 3 (see Fig. <ref type="figure" target="#fig_11">7</ref>). While the tangent at the first point can also be used as a part of the index, we have chosen not to do so. Then, if the tangent measurement at pl is in error, at least all the indexes generated here will not be in error. Different quantization criteria can be applied to each of the index parameters to limit them to a finite number of discrete values (in our experiments, this number ranges from 8 to 32). The generated index is invariant to similarity transformations. In fact, if other global properties of the tuples of points are also invariant, they can be used to form indexes of higher dimensions, as we will show later. In general, all ratios between visual parameters (e.g., intensity, curvature, etc.) are good candidates for invariance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. SHAPE ACQUISITION AND RECOGNITION</head><p>We present the general scheme for acquiring and recognizing shapes. The basic procedure is the same for both the local curve-feature detection stage and the following object detection (or acquisition) stage. For local curve features, the models are a small set of curve shapes (4 in our present implementation), and the feature detection process consist of recognizing these curve shapes. For object shapes, the local curve shapes serve as features. The object models are acquired from characteristic views of the object, and recognition is performed on images following feature detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Acquisition</head><p>During the acquisition phase, we consider a single, unoccluded 2-D shape in the image so that the position of its center of mass (ZO; yo), its scale p (default value is l.O), its orientation vector d (Fig. <ref type="figure" target="#fig_11">7</ref>), and a symbolic label L are known. In the case of 3-D shapes, different views corresponding to different aspects of the object [39], [ 191 can  be acquired. Alternatively, given the high computational cost of finding different aspects of an object <ref type="bibr" target="#b25">[27]</ref> and the ability of our system to recognize shapes from partial instances and with some amount of projective invariance (Section XI), the Gaussian viewing sphere can be sampled at regular intervals. Finally, as shown in <ref type="bibr">[40]</ref> and <ref type="bibr" target="#b19">[21]</ref>, 3-D pose can be recovered directly from 2-D contours using tuples of points.</p><p>For each triplet of feature points, we compute the index and another n-tuple, called an entry, containing four elements:</p><p>1) The label L of the object 2) The position (ZT, y ~) of the center of mass in the new right-hand coordinate system defined by the normalized vector t 2 3 and its corresponding orthonormal one</p><p>3) The ratio p = : .</p><p>4) The angle CYT between the two vectors t 2 3 and d.</p><p>These parameters depend on scale, orientation, and location and are used to recover the position, of an object shape from the corresponding 3-point sets. The generated discrete index addresses a bucket in a look-up table to which we add the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>entry ( L , ( z T . Y T ) , P , ~T ) .</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Recognition</head><p>During recognition, in a similar fashion, all possible combinations of three edge points from an image are used to generate a set of indexes. From each entry contained in the table-bucket addressed by the index generated by a triplet of points, we obtain a shape label L and compute the location, orientation, and scale of this shape L.This triplet votes for an instance of shape L in the recovered pose. A shape instance is one particular instance of a shape and consists of a shape label and specific values for location, rotation, and scale. The process is repeated for all triplets of points, and votes are accumulated for the different shape instances computed from the entries. Triplets located on the same shape in general provide correct estimates for the parameters, thus accumulating votes for the correct shape instance. Other triplets produce pseudorandom results that are scattered over numerous shape instances and result in negligible accumulation.</p><p>Let us define discrimination = 2, where V, are the votes for the correct shape instance and V, the maximum votes received for an incorrect shape instance, as a measure of the capability of this technique to distinguish between different shape instances. If a shape instance A shares a fraction p of points and their local properties with another shape instance B, then the discrimination between the two shapes is approximately P -~.</p><p>For each shape hypothesis, we record the features used to generate its value, and record the number of times it was indexed, as votes for that hypothesis. We assume that distinct objects in the image do not share features. Thus, different shape hypotheses generated by overlapping feature subsets compete, and only the one with highest number of votes is selected. Since these selected shapes do not share image data, they inherently constitute a segmentation of the input image.</p><p>By modifying the number of points, properties, and relationships considered, the same framework of evidence integration can be extended to other domains. In 3-space, for instance, one could use the relative position of combinations of three points from a range data image. The correspondence between three model and three range points fully determines 3-D object position. The 1 st-order properties (surface normals) can be represented, for instance, by the angle between the tangent planes and the plane containing the three 3-D points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. LOCAL SHAPE FEATURES</head><p>The scheme outlined above for computing indexes could be used directly for recognizing object shapes with edge-points serving as features. However, the dimensionality of the index computed from just edge points is limited to at most five (of which we used only four above for noise rejection). Also, the O(n3) computational complexity with respect to the number of points n considered makes the approach unappealing when large data sets are explored.</p><p>To overcome this difficulty, we caii consider only a small specific subset of the edge points. In this case, the stability and robustness of the transform would depend on reliably selecting the same subset of points when acquiring and recalling the shapes, that is, selecting the same points for the same objects in different images. Our solution is to encode local 2-D shape of edge-curves into a small set of localized and symmetric shape descriptors. Next, we use this intermediate representation as the features for the spatial transform that computes indexes for object shapes.</p><p>Curve shapes have been labeled based on curvatures measures <ref type="bibr" target="#b33">[35]</ref> and by fitting parametric functions [%]. However, derivatives and retrieving the curve parameters from an image is a hard task requiring optimal segmentation of the input. Also, the existence of a parametric representation does not guarantee its stability. In other words, small variations in some of the parameters can produce arbitrary large variations in the shape of the associated 2-D curve, and vice versa. Thus, matching the extracted representations to the stored ones becomes almost impossible. In these cases, parametric representations are not viable candidates for either the acquisition or the recognition process. Curvature requires computation of high-order derivatives, and is scale dependent.</p><p>We have chosen to use the same framework we have for object recognition to also serve as a feature detection. Thus, we have replaced feature detection by model-based recognition. A number of curve shapes (four in the current implementation) are chosen as models. These models are acquired by presenting digitized images of the curve-shape under various similarity transformations. The best curve shape at each edge point is recognized using indexing. Then, among the overlapping curve shapes, the best ones are chosen, giving a segmentation of the edge-curves into nonoverlapping labeled sections. The location of the curve-shape is given by the pose computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Detecting Local Shapes</head><p>The framework. for the acquisition and extraction of local shape descriptors is analogous to the one described in Section VII-A. While global (object) shapes are not associated with any specific point on the shape and are positioned through their centers of mass, local shapes are associated with a specific point on the shape. For instance, elliptical arcs are associated with points where curvature reaches local extrema. Four local shape descriptors used by us are lines, circular arcs, and elliptic arcs (minima and maxima of curvature). Larger sets of local shape descriptors can also be considered [l], <ref type="bibr">191.</ref> Given the image of an object, edges are extracted using standard techniques such as a Canny edge detector <ref type="bibr">[14]</ref>. Edges are then linked into curves using simple eight neighbors connectivity. We assume that the shape of the object is completely captured by these 2-D edge-curves. However, no assumption is made on the connectivity of the contours other than piecewise continuity. Hence, noisy or broken contours are acceptable.</p><p>Given a point (so.:yo) on a edge-curve, we symmetrically sample around that position along the curve (see Fig. <ref type="figure">8</ref>), generating a set { ( x ~, pi). -N 5 i 5 N } . The sampling step is proportional to the length of the longest symmetrical interval around the point for which the tangent behavior is monotonic on a coarse scale, and the total tangent variation is less than F. It is also inversely proportional to the total variation in tangent angle on the symmetric interval for tangent angle less than F. We assume that faster variations in tangent correspond to smaller features, which accordingly require finer sampling for detection. As the sampling is based on angles and not on distances, it is scale independent. Next, we form all possible combinations of the point ( x ~. o , y ~) with two others from the set {(xi, yi)}. The index ( 2 , 2 , o1 0 2 ) described in Section 11-B, is computed from each triplet and it is used to address a bucket in the local-shape look-up table. The point to note here is that all possible triplets of edges on the curve are not considered. One of the points is always at the center of the symmetric sampling; the second point always comes from one side of this point and the third from the other side.  During acquisition, images of the curve shapes are presented in various poses. The computation for indexes is performed with (zo, yo) fixed at the center of symmetry for the model curve shape. Since during acquisition one of the points in the triplets is always at the center of symmetry of the curve, the strongest response during recognition will also be when the edge point (that we are sampling symmetrically around) is actually at the center of symmetry for its local curve shape. For each triplet, the entry (A, [j, S ) is inserted in the table.</p><p>Here, X is the symbolic label for the local curve shape. /3 is the angle between the vector t and the normal to the contour n, required to recover the shape orientation from the triplet. S = s1 + s2 + s3 is used for scale normalization.</p><p>During recognition, given (z0,yo) and the sample set {(zt,yt)}, triplets are formed and indexes computed in an analogous fashion. For each entry in the table addressed by these indexes, a feature instance is computed that uniquely identifies the local shape and its orientation and scale. After all the triplets have been considered, the feature instance with the highest data support (i.e., votes) describes the shape around (-CO, :yo). By considering the subset of {(zGz. yz)} that successfully voted for a given feature, it is also possible to recover the section of the contour associated with it. The process is repeated for each edge point. Since sampling rates are recomputed for each point, sample density varies dynamically along the curve.</p><p>After the feature detection phase, local descriptors supported by overlapping portions of a curve are compared. From each overlapping set, the feature with the highest number of votes is chosen. In this way, curves are naturally segmented into a small set of nonoverlapping localized shapes (see Figs. 9, 10, and 11). These local descriptors are positioned at coarsely sampled contour locations. To increase stability, a finer localization is obtained by extracting new descriptors on a pixel-by-pixel basis, in a small neighborhood of the original local descriptors, and selecting again from among the results the one with highest support. In our experimental testing, this approach has shown stability with respect to scale, rotation, translation, and limited projective transformations of the input data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IX. OBJECT SHAPES</head><p>Object shape acquisition and recognition is performed on images after the local curve shape detection stage. The overall scheme follows that outlined in Section VII. One main difference is that the index is now seven-dimensional, with the additional information provided by the feature labels. Also, a number of heuristics to improve speed are introduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Model Acquisition</head><p>The shape database is similar to associative memories [32], which usually employ distributed representations, i.e., each global shape (object) description is not localized to a particular location but is distributed over the memory. Such a holographic representation engenders fault tolerance to loss of parts of the memory. The distributed representation of visual shapes is contained in the shape table and selection mechanisms described in Section 11. Shapes are represented as a collection of entries distributed over the table. Tables, because of their extreme sparseness (see Section III), are implemented as hash tables.</p><p>New shapes are acquired by performing a shape autocorrelation transform on presented instances from images. To enhance recognition in the presence of noise and small projective transforms of the input, two heuristics are employed. Different instances of the same shape in different poses are acquired. For each object, the acquisition is repeated on different views until the indexes no longer hit empty buckets in the shape table with a significant rate. Secondly, a stochastic index perturbation mechanism during acquisition is performed. That is, a small randomness (order of the index parameter quantization) is added to a index along each of its dimensions. This, while not modeling the noise and its effect on the index, increases the fault tolerance of recognition.</p><p>One potential problem is that an incorrect measurement may be caused by incorrect noise and segmentation and, yet it will still be recorded in the library. Insuring that a minimum number of votes be measured from an index before it is accepted as a valid invariant [48] will help alleviate this problem.</p><p>The local shape of a curve around a point provides additional dimensions for the indexes used for object shapes.</p><p>The index becomes (2. ~, a 1 , n 2 , X 1 , A 2 , X j ) , where A, is the symbolic label for the local curve shape at point (x,,yJ. These indexes are of a higher dimensionality, endowing the transform with even more selectivity. The scale (or size) of the local curve descriptors could be used to generate additional parameters for the index, but these are not used in this paper. The entries used and the acquisition process is the same as described in Section VILA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Object Shape Recognition</head><p>To recognize objects in an image, the first stage is feature detection. Edges are detected in the image and local curve features then detected using the scheme detailed in Section VIII. Triplets of local-curve shapes are used to generate the seven-dimensional indexed as described in the previous section. The object recognition scheme proceeds as described in Section VII-B.</p><p>A direct implication of the use of 3rd-order autocorrelation function is O(71:) time complexity, where T I S is the number of local shapes. It is further possible to reduce the time requirement to 0 ( 7 1 , ) , as shown below.</p><p>Local shape descriptors are assigned a weight wT, proportional to their visual relevance. This measure uses the length of their support normalized with respect to the size of the model (this favors large features) and/or the tangent angle variation per unit length that they describe (this favors corners). Larger features have a higher chance of being correctly identified from the input. Rapid variations in tangent correspond to curvature maxima that are well localized on contours. For each model, we isolate a small constant number c of such highly relevant descriptors D, = { D t : i 5 c} from the others D = { D J : j 5 n,},D, C D. We then generate all possible triplets with the first element from the set D and the other two from the set D,. The total number of triplets formed, n, (i), is a linear function of n,. Thus, the time required to acquire a shape model, or one view for a 3-D object, is O(71,). This representation is still highly redundant since each feature is present in at least (5) triplets.</p><p>Furthermore, since real-world objects have in general a compact structure, it is possible to exploit this property to reduce the number of triplets considered. For each identified descriptor we generate a circle of coherence centered at its location. The radius of this circle is proportional to the size of the local shape. Other descriptors from the same shape are more likely to be found within such a circle. This technique has been demonstrated to reduce the number of triplets from cubic to a linear with respect to the image size <ref type="bibr">[16]</ref>.</p><p>To make recognition robust with respect to noise in cluttered environments, we extend the radius of coherence until a required constant number of descriptors with desired visual significance U),, normalized with respect to that of the considered descriptor, are found, and we create triplets only with these. This search can be made efficient using multiresolution analysis techniques [ 131, or by maintaining the features in a heap structure. In either case, maximum search time is within The heuristics of saliency and locality used here to reduce recognition time are similar to the local focus feature approach of Bolles and CaiIi [lo] and the salient feature methods of Tumey et al. [ V I . These heuristics are only approximate, and they can fail. Under our scheme, it is not possible to assign saliency in a totally scale-independent manner. Similarly, locality is only a very crude approximation to object level segmentation. Sophisticated bottom-up segmentation techniques such as perceptual organization <ref type="bibr" target="#b44">[45]</ref> will obviously aid this system but have not yet been incorporated into it. Therefore, while saliency and locality can 'often reduce the recognition time to linear in the number of features, for images where these approximations fail features other than the salient ones are copsidered and the search is expanded over the full image.</p><p>O ( n log n ) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>X. SUBPART IDENTIFICATION</head><p>We introduce a method for using indexing to automatically detect subparts in the acquired objects. This, coupled with the flexible structure of the global shape indexes, allows for the formulation of a hierarchical organization of the shape database. Such a layered structure of the shape table, however, has not yet been implemented in the current system.</p><p>Complex object shapes can be broken into smaller and simpler components that are termed subparts. These subparts, if considered as separate shapes, may recursively be broken into their component subparts. Subparts allow description of complex shapes in terms of simpler ones rather than directly in terms of primitive features, in this case the local shape descriptors. Thus subparts allow a hierarchical representation of shape, with the sttape hierarchy starting from local features, through subparts and finally object shapes. This way of structuring the information allows for more compact representation, more efficient extraction and higher data abstraction capabilities. The hierarchical shape representation is useful for recognition, shape representation <ref type="bibr" target="#b21">[23]</ref>, and possibly in reasoning about the objects. In indexing systems, the number of levels can be increased by using subparts. This will add to the dimensionality of indexes used for object recognition.</p><p>The different varieties of subparts can be categorized by whether they can overlap or are strictly nonoverlapping, and by the technique used to categorize a part of a shape as a subpart. For example, in <ref type="bibr" target="#b21">[23]</ref> subparts are nonoverlapping and are formed by segmenting the shapes at comers; in <ref type="bibr" target="#b44">[45]</ref> subparts can be overlapping and are segmented on the basis of perceptual organization criteria.</p><p>We define subparts as locally connected parts of one shape that are shared (rotated and scaled) among a significant number of other shapes. The subparts can be overlapping. There is a lower bound c m the number of shape descriptors required to form a subpart, as very small sections of a shape (such as a portion of a curve or straight line) can be shared by numerous objects and lack descriptive power. Our definition of subparts does not depend on segmentation, i.e., on the reliable decomposition of different instances of a shape in  an identical way. However, we can take advantage of any subpart segmentation method to speed up our computation by constraining the generalized neighborhoods to the segmented sections.</p><p>In our framework, the concept of subpart finds natural expression in terms of evidence integration across several models. Given a table containing a large number of shapes, we use the indexes generated during the acquisition process to extract information from the image, based on previously acquired shape information. If a significant match is found between the new shape and other shapes from the database, the set of matching elements (i.e., single local shape descriptors that produced matching triplets) is analyzed as a possible candidate for a subpart. If it satisfies some criteria, such as being compact and connected, it is classified as such and inserted in the subpart database.</p><p>The experimental result of unsupervised subpart detection from the set of objects shown in Figs. 12, 13, 14, and 15 is shown in Fig. <ref type="figure" target="#fig_18">16</ref>.</p><p>Subparts assume semantic roles identical to local shape descriptors. That is, they are located at their center of mass and assigned a label, an orientation, and scale. As such, they participate in the recognition of the shapes they belong to, by or relative position and orientation, or due to spurious votes that do not lead to a valid support. From this set, we select features that have participated in more than 2 votes, where n is the number of used shape descriptors, i.e., we select those whose votes were not produced by stochastic or correlated noise [ 161. Nt is the total number of triplets formed by the descriptors. 4) Next, we check if the selected features form any subpart that A and B share. To belong to a subpart, the features must exhibit spatial coherence, i.e., they must be related by spatial proximity and continuity along edgecontours. Continuity is checked by looking for overlap or adjacency in the edge support for each feature. If an edge-contour has more than 10% of its features missing, then it is not a viable candidate for forming a subpart, and the subpart itself is not considered. Therefore, for a set of selected local shape descriptors in B to be a subpart of A, the features must belong to a spatially coherent segment of B and only to that segment of B. A minimum number of spatially coherent features of B must be selected, insuring that the entity defined as a subpart is more than simply a local shape or a small part of curve. M~~~ formally, suppose there are two distinct objects A and B and we wish to know the subparts shared by them. The process of obtaining the subparts is: Detect the local shape descriptors for object and perform recognition on the global shape table that already Subpart B is not limited to sharing subparts with a single object. If more than one set of features has significant vote, each of those feature sets has to be checked for being a subpart (steps The process described above is for finding common subparts between object models during their acquisition. We have not yet and 4)subparts for recognition contains a representation of A.</p><p>If there are no significant votes accumulated for the object A, then B and A do not share any subpart. If significant votes are obtained for object A, then B and A are likely to share a subpart. For each shape instance of A with significant votes the local shape descriptors in B that voted for that instance are known. Note that more than one shape instance of A can be created if A and B share more than one subpart at different scales</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>XI. EXPERIMENTS</head><p>We report some experiments testing the performance of the multidimensional indexing scheme presented before. These experiments provide some empirical test of one multidimensional indexing system. It is difficult to test and quantify how well a system will perform under real-world conditions. There is no way of trying out all real-world situations with varying numerous parameters like object shapes, viewing angles, distances,  scales, noise, occlusion, and scene complexity. That is exactly why we provided a computational framework for analyzing indexing. The system implementation and experimentation are to be treated more as a proof of concept.</p><p>The system was implemented in Lisp on a Symbolics 3650. The size of the shape table was 220 with the seven index dimensions (see Section IX-A) quantized to 24% 24, 2J, 23, 2J. 22. 2 2 . arid 2' levels, respectively. For the object hypotheses table H I , the number quantization levels for pose parameters was 24 for ( L T , yt), and 8 for p and a : ~. Note that the quantization levels for the axes is much lower than the lo2 value typically used in geometric hashing, and the shape table size is larger by a factor z 10'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Model Database Size</head><p>To test the recognition performance as a function of the number of object models in the shape library, we sequentially added 290 models to the shape memory and tested the recognition of the first five objects introduced into the shape table. Some of the shape models used are shown in Fig. <ref type="figure" target="#fig_20">17</ref>. Discrimination ratio (Section VII-B) was used as the parameter to evaluate recognition performance. The shapes were generated by a random process and contain a similar number of local features (5 to IO) and are of similar size (perimeter). Thus, the recognition task was hard in the sense that there was no large variation in shape.</p><p>Fig. <ref type="figure" target="#fig_21">18</ref> shows the plot of discrimination versus number of objects in the database. The plotted discrimination value is the average discrimination for the five test shapes. It is interesting to note how performance degraded abruptly once the first few shapes were added to the database but the asymptotic behavior shows a discrimination power larger than 12, i.e., correct instances received at least 12 times more votes than the number of votes for the next best shape.</p><p>The experiment indicates that the indexing scheme adopted is good in the sense that the table does not saturate quickly. This experiment is limited in that the recognition test was The average recognition time was 6 s per object. On the average, there were 5 local shape descriptors per object, resulting in 125 indexes per object for acquisition and recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Recognition</head><p>We report some experiments as examples of invariance of recognition capabilities to geometric transformations of the model in the image.</p><p>We selected the domain of leaf shapes to demonstrate the acquisition and recognition of complex nonparametric shapes. The images used for the experiments are obtained from a database of drawings. The drawings were treated as images and the sampled drawings treated as edge maps. Thinning, contour linking, etc., were run on these edge maps just like on conventional edge maps obtained from intensity images. The images are cleaner than real images, but they provided more control over the experiments. Two of the leaves from this set are shown in Figs. 9 and 19. In this and following figures we show the contours obtained by linking the edges. The contours have been smoothed with a Gaussian filter. The local shape descriptors detected along these two leaves are shown in Figs. <ref type="figure" target="#fig_15">11</ref> and<ref type="figure">20</ref>. Rotation and translation in rhe image plane: An image of leaf #1 that has undergone rotation and translation in the image plane is considered (Figs. <ref type="figure" target="#fig_28">23</ref> and<ref type="figure" target="#fig_6">24</ref>). The system correctly identifies the shape and the parameters for rotation, translation, and scale. The original leaf #1 model, translated and rotated with the recovered amounts, is overlaid (Fig. <ref type="figure">25</ref>). The difference in registration is due to the coarsely quantized computation of the geometrical transform parameters, e.g., the position of the center of mass. If needed, a more precise registration of the model to the image can be obtained by reducing the size of the quantization, rematching only the selected local shapes, and accumulating evidence only for the matched model. Scaling: An image of the leaf is taken at a shorter distance, leading to projected shape of the leaf being 1.8 times larger than the model (Figs. <ref type="figure" target="#fig_9">26</ref> and<ref type="figure" target="#fig_24">27</ref>). The recognition phase returns the correct label for the leaf and the correct scale factor. Fig. <ref type="figure" target="#fig_26">28</ref> shows the recognized model, scaled by the computed scale factor, and projected onto the image. Skew: Slant and/or tilt of the object corresponds to viewing the object from different viewpoints. This introduces a skew in the projected shape of the object-for example, a rectangle appears as a parallelogram. A leaf is viewed with the image plane slanted and titled about from a plane parallel to the leaf (i.e., projective transform, Figs. <ref type="figure" target="#fig_27">29</ref> and<ref type="figure" target="#fig_25">30</ref>). There is also some magnification due to differences in the viewing distances. Even given the relatively large change in viewing direction, the correct model is recognized. This model, with the scale and translation accounted for, is overlaid on the image in Fig. <ref type="figure" target="#fig_28">31</ref>. Currently, recovery of viewing directions is not incorporated since objects are 2-D shapes. Views, generalization, and separability: The current system handles 2-D shapes. One way of handling 3-D  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">)</head><p>shapes are categorized as the most similar objects in the shape memory (generalization), but after learning of the shape, the system is able to distinguish between that similar shape and the newly learned one (separability). Clutter and Occlusion: Finally, to show that the system works correctly for complex scenes, we analyze an image, shown in Fig. <ref type="figure" target="#fig_28">32</ref>, containing multiple instances of the same object at different locations, and in the presence of other objects not in the database. All the instances of leaf #1 are correctly recognized. Due to the presence of multiple objects, other models in the memory also receive some votes. These models are suppressed by the constraint satisfaction mechanism. Again, the difference in the registration of the object model with the image (Fig. <ref type="figure" target="#fig_28">33</ref>) reflects the quantized values used for computing the transforms. The average recognition time was 25 s per object for the similarity transform and 5 min for the cluttered scene. On the average, there were 30 local shape descriptors per object, resulting in 1500 indexes per object. These views can sample the Gaussian viewing sphere or be obtained from an aspect graph. Generalization is the ability of a recognition that on being presented with a shape that is not in its database, it categorizes it as the shape it is most similar to. Separability means if the unlearned shape, which is categorized as a shape similar to it, is subsequently learned, new presentations of it get correctly categorized to the newly learned shape. To show that viewpoint direction parameters could be recovered by learning the projected representations of the model at different viewing angles, we acquired the skewed leaf shape as a separate leaf model. In subsequent recognition of the skewed shape, the correct instance is recognized. This not only indicates that the system can be incremented to handle 3-D shapes but also the factors of generalization and separability, i.e.,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>XII. CONCLUSION AND FUTURE DIRECTIONS</head><p>We have provided a computational framework for the analysis of indexing-based model acquisition and recognition systems. This analysis shows that high-dimensional indexing strategies can solve a number of problems that have hampered conventional look-up table techniques. The benefits include improvements in recognition time, discriminating power, and maximum size of the database. As a proof of concept, a highdimensional indexing system for the automatic acquisition and recognition of complex visual shapes has been presented. The associative memory structure used for storing and recalling shapes instances exhibits the properties of robustness, generalization, and recall from partial descriptions. The mechanism is also capable of unsupervised detection of subparts shared by newly acquired and stored models.</p><p>Such a memory scheme is efficiently implemented both in terms of time and space requirements. Model acquisition is accomplished in time complexity of O ( n 3 ) , where the model is described by n local shape descriptors. The requirement for memory space is O(mn) to O ( m n 3 ) , with m the number of distinct object shapes (or distinct object shapes times their different aspects for 3-D shapes) and 71 the average number of local shapes per model. Recognition time grows very slowly with the number of models in the database and is given by NTT(lc + cQ), where N T ~ is the number of feature triplets in the image, k and c are constants, and Q is the number of models in the database. c in our case is as low as 0.001.</p><p>Current work is focusing on the extension of the multidimensional indexing paradigm to 3-D shapes and applications beyond object recognition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Manuscript received July 29, 1991; revised April 27, 1993. Recommended The authors are with the IBM Thomas I. Watson Research Center, Ex-IEEE Log Number 9214453. for acceptance by Associate Editor D. Huttenlocher. ploratory Computer Vision Group, Yorktown Heights, NY 10598.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Fig. I(a)). In the affine case, (m.jj) are the affine projections of a shape model point on the affine coordinate frame defined by other three model shape points (see Fig. l(b)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Fig. I . Similarity (a) and affine (b) index generation in geometric hashing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Fig. 2. assumption.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>NI-&gt;&gt; IC, this can be rewritten asThe only dependency on d is in the term f l ~. This will strongly depend on the size of the table Ho and therefore on the dimensionality of the indexes. N E can be computed as the number of references stored in Ho divided by the size of Ho Here, n b ( d ) i s the number of separate quantized values on each index axis, assumed to be the same along each axis for simplicity. Substituting (1 6) in (1 5), we obtain The final result is obtained by substituting the normalized value of n b ( d ) as a function of d from (13), where we use the notation n b = n b ( 0 ) for simplicity. Note that N H ~ has also been replaced by Nhf N p , as discussed in Section 111-A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Ratio of bad votes in multidimensional and 2-D indexing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Probability of false positive with k votes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 3</head><label>3</label><figDesc>Fig.3plots the value of (19) as a function of both k and d for N s ( 0 ) = 100: NAI = 1000. N I = 1000,nb = 64, and p g = 0.1. Note that even for such a low probability of getting correct 1 -D index values, the probability ratio drops exponentially as d is increased from 2 to 7. Analogous results can be obtained for all practical values of the fixed parameters. Fig.4plots the expected value for the probability of obtaining incorrect hypotheses with k votes as d increases (different curves) for the same set of parameters. Notice how the probability of bad hypotheses tends to accumulate toward the lower votes range as d increases. Therefore, the performance of low-dimensional indexing techniques can become exponentially better if more information can be used reliably to build higher dimensional, more descriptive indexes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>-Fig</head><label></label><figDesc>Fig. 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. The system architecture flowchart</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>~k . -l .Yk ~ 1 ) I ) [f (P. Z: y. 3'1, ?/l, ' ' ' , :Ck , yk)12dzi dyl . . . d z k d ? j k , (24)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 7</head><label>7</label><figDesc>Fig. 7 Computing index from feature triplets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>Fig. 8. Sampling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Shape of leaf # 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. IO .</head><label>IO</label><figDesc>Fig. IO. Local shape descriptors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 1 1 .</head><label>1</label><figDesc>Fig. 1 1. Selected descriptors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Lamp # 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Lamp # 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Fig. 16 .</head><label>16</label><figDesc>Fig. 16. Defected subpan.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. Lamp # 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Fig. 17 .</head><label>17</label><figDesc>Fig. 17. Some of the objects used for testing database size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Fig. 18 .</head><label>18</label><figDesc>Fig. 18. Average discriminability vs. size of database.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Fig. 19 .</head><label>19</label><figDesc>Fig. 19. Shape of leaf ## 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Fig. 21 .FigFig. 22 .Fig. 26 .Fig. 23 .</head><label>21222623</label><figDesc>Fig. 21. Rotated leaf.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Fig. 27 .</head><label>27</label><figDesc>Fig. 27. Skewed leaf.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Fig. 30 .</head><label>30</label><figDesc>Fig. 30. Complex scene.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>Fig. 28 .</head><label>28</label><figDesc>Fig. 28. Local descriptors</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head>Fig. 29 .</head><label>29</label><figDesc>Fig. 29. Rec :ognition/location</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head>Fig. 3</head><label>3</label><figDesc>Fig. 3 I. Recognitiodlocation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>This can be accomplished in three ways:By increasing the average number of indexes generated by each model shape as d increases. In this case, By using coarser quantization step An = Au(d),</figDesc><table><row><cell>And for</cell></row><row><cell>short, along each index axis as d increases. An appropriate</cell></row><row><cell>choice should yield larger values of p , (And) (in this case</cell></row><row><cell>a function of Acr) as d increases such that the global</cell></row><row><cell>probability of getting a good d-dimensional index</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Andrea Califano (SM'90) was born in Napoli, Italy. He received the Laurea in physics from the University of Florence, Florence, Italy, in 1985. He continued his thesis research on the chaotic behavior of high-dimensional dynamical systems as a research associate at the Instituto Nazionale di Ottica in Florence, Italy.</p><p>In 1986 he spent six months as a Visiting Scientist at the Information Mechanic Group at the Massachusetts Institute of Technology, Cambridge, MA, where he was involved in research on cellular automata. From 1986 to 1991, he was a Research Staff Member in the Exploratory Computer Vision Group at the IBM T. J. Watson Research Center, Yorktown Heights, NY, where he is currently the Manager of the Computational Biology and Pattern Matching Group. His current research interests are in the areas of computer vision, pattern matching, and their applications' to molecular biology and genetics. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rakesh Mohan</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The curvature primal sketch</title>
		<author>
			<persName><forename type="first">H</forename><surname>Asada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brady</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">I</biblScope>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A geometric matcher for recognizing and positioning 3-D rigid objects</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ayache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Faugeras</surname></persName>
		</author>
		<author>
			<persName><surname>Faverjon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf fntelligent Robots and Computer Vision, Proc. SPIE</title>
		<imprint>
			<date type="published" when="1984-10">Oct. 1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Parameter nets: A theory of low level vision</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Ballard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 7rh fnt. Joint Conf ArriJcial Inreffigence</title>
		<meeting>7rh fnt. Joint Conf ArriJcial Inreffigence</meeting>
		<imprint>
			<date type="published" when="1981-08">Aug. 1981</date>
			<biblScope unit="page" from="1068" to="1078" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generalizing the Hough transform to detect arbitrary shapes</title>
		<author>
			<persName><surname>__</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="11" to="122" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Computer vision research</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Bolle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Califano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kjeldsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. DARPA h u g e Understanding Workshop</title>
		<meeting>DARPA h u g e Understanding Workshop<address><addrLine>Palo Alto, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1989-05">May 1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Visual recognition using concurrent and layered parameter networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Bolle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Califano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kjeldsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Con$ Computer Vision and Puttern Recognition</title>
		<meeting>IEEE Con$ Computer Vision and Puttern Recognition<address><addrLine>San Diego</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1984-03">June 1989. Mar. 1984</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="137" to="155" />
		</imprint>
	</monogr>
	<note>Shape matching of two-dimensional objects</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Three-dimensional object recognition</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Besl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. S u n q s</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">I</biblScope>
			<biblScope unit="page" from="75" to="145" />
			<date type="published" when="1985-03">Mar. 1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A shape representation based on geometric topology: Bumps, Gaussian curvature, and the topological zodiac</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Blicher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. loth Int. Join1 Conf: Artificial Intelligence</title>
		<meeting>loth Int. Join1 Conf: Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1987-08">Aug. 1987</date>
			<biblScope unit="page" from="767" to="770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Recognizing and locating partially visible objects: The local feature focus approach</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Bolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Cain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Robotics Reseurch</title>
		<imprint>
			<biblScope unit="volume">I</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="57" to="82" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">3-DPO: A threedimensional part orientation system</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Bolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Horaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Hannah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 8th Int. Joint Con5 on Artificial fnrelligence</title>
		<meeting>8th Int. Joint Con5 on Artificial fnrelligence</meeting>
		<imprint>
			<date type="published" when="1983-08">Aug. 1983</date>
			<biblScope unit="page" from="1" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Model-based three-dimensional interpretations of twodimensional images</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Brooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattent Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="140" to="150" />
			<date type="published" when="1983-03">Mar. 1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Smart sensing within a pyramid vision machine</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Burt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1988-08">Aug. 1988</date>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="1006" to="O1015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A computational approach to edge detection</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Canny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="679" to="698" />
			<date type="published" when="1986-11">Nov. 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Feature recognition using correlated information contained in multiple neighborhoods</title>
		<author>
			<persName><forename type="first">A</forename><surname>Califano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 7th Not. Conf Artificiol Intelligence</title>
		<meeting>7th Not. Conf Artificiol Intelligence</meeting>
		<imprint>
			<date type="published" when="1988-07">July 1988</date>
			<biblScope unit="page" from="83" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generalized neighborhoods: A new approach to feature extraction</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Bolle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc IEEE Conf Computer Vision and Puttern Recognition</title>
		<meeting>IEEE Conf Computer Vision and Puttern Recognition</meeting>
		<imprint>
			<date type="published" when="1989-06">June 1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generalized shape autocorrelation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Califano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mohan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAf-90</title>
		<meeting>AAAf-90</meeting>
		<imprint>
			<date type="published" when="1990-07">July 1990</date>
			<biblScope unit="page" from="1067" to="1073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-dimensional indexing for recognizing visual shapes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Califano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mohan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Con$ Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Con$ Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1991-06">June 1991</date>
			<biblScope unit="page" from="28" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Characteristic view as a basis for threedimensional object recognition</title>
		<author>
			<persName><forename type="first">I</forename><surname>Chakravarty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPfE Conf Robot Vision</title>
		<meeting>SPfE Conf Robot Vision</meeting>
		<imprint>
			<date type="published" when="1982">1982</date>
			<biblScope unit="page">1201</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Model-based recognition in robot vision</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surveys</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">I</biblScope>
			<biblScope unit="page" from="66" to="108" />
			<date type="published" when="1986-03">Mar. 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Model group indexing for recognition</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">T</forename><surname>Clemens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Cmf: Computer Vision and Prrttern Recognition</title>
		<meeting>IEEE Cmf: Computer Vision and Prrttern Recognition</meeting>
		<imprint>
			<date type="published" when="1991-06">June 1991</date>
			<biblScope unit="page" from="4" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Use of the Hough transform to detect lines and curves in images</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">0</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Hart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">I</biblScope>
			<biblScope unit="page" from="11" to="15" />
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Large hierarchical object recognition using libraries of parameterized model sub-parts</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Ettinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pmc. IEEE Conf Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="1988-06">June 1988</date>
			<biblScope unit="page" from="3" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A 3-D recognition and positioning algorithm using geometric matching between primitive surfaces</title>
		<author>
			<persName><forename type="first">D</forename><surname>Faugeras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 8th Int. Joint Con$ Artificial Intelligence</title>
		<meeting>8th Int. Joint Con$ Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1983-08">Aug. 1983</date>
			<biblScope unit="page" from="99" to="1002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Connectionist models and their properties</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Ballard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Sci</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="205" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Model-based recognition and localization from sparse range data or tactile data</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E L</forename><surname>Crimson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lozano-Perez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. . I . Roborics Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="3" to="34" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Computing the aspect graph for line drawings of polyhedral objects</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Gigus</surname></persName>
		</author>
		<author>
			<persName><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">fEEE Trans. Purrern Anal. and Machine Intell.</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1990-02">Feb. 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Special purpose automatic programming for 3-D model-based vision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Goad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. DARPA Image Understanding Workshop</title>
		<meeting>DARPA Image Understanding Workshop</meeting>
		<imprint>
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">The combinatorics of heuristic search termination for object recognition in cluttered environments</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E L</forename><surname>Crimson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989-05">May 1989</date>
		</imprint>
	</monogr>
	<note>Massachusetts Institute of Technology, Cambridge, MIT AI Memo 1 I I I</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">On the sensitivity of geometric hashing</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E L</forename><surname>Crimson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd Int. Conf Computer Vision</title>
		<meeting>3rd Int. Conf Computer Vision<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">CAGD-based computer vision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Workshop on Computer Vision</title>
		<meeting>Workshop on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1987-12">Nov.-Dec. 1987</date>
			<biblScope unit="page" from="100" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Andreson</surname></persName>
		</author>
		<title level="m">Purrrllel Models of Associnriwe Memory</title>
		<imprint>
			<publisher>Lawrence Erlbaum</publisher>
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">3D PO&apos;s strategy for matching threedimensional objects in range data</title>
		<author>
			<persName><forename type="first">P</forename><surname>Horaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. fEEE 1984 fnt. Conf: Robotics</title>
		<meeting>fEEE 1984 fnt. Conf: Robotics</meeting>
		<imprint>
			<date type="published" when="1984-03">Mar. 1984</date>
			<biblScope unit="page" from="78" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Methods and means for recognizing complex patterns</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V C</forename><surname>Hough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">U. S. Patent</title>
		<imprint>
			<biblScope unit="volume">3069654</biblScope>
			<date type="published" when="1341">1341. 1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Object recognition using alignment</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1st. fnt. Conf Computer Vision</title>
		<meeting>1st. fnt. Conf Computer Vision<address><addrLine>I I</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="page" from="102" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Three-dimensional recognition of solid objects from a two-dimensional image</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Massachusetts Institute of Technology</title>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1045">1045. 1988</date>
		</imprint>
		<respStmt>
			<orgName>MIT AI-Lab. Tech</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Automatic generation of object recognition programs</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ikeuchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Proc</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1016" to="1035" />
			<date type="published" when="1988-08">Aug. 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Twodimensional, model-based, boundary matching using footprints</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kalvin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J T</forename><surname>Schonberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><surname>Sharir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">fnt. J. Robotics Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1986">1986</date>
			<pubPlace>Winter</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The internal representation of solid shape with respect to vision</title>
		<author>
			<persName><forename type="first">J</forename><surname>Koenderink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Doorn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biologiccrl CJhem</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="21" to="21" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">On recognition of 3D object from 2D images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lamdan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Wolfson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pmc. fEEE Conf: Robotic.\. cmd Automution</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Object recognition by affine invariant matching</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lamdan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Wolfson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">P roc. lEEE Conf: Computer Vision and Pattern Recognition</title>
		<imprint>
			<biblScope unit="page" from="335" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Geometric hashing: A general and efficient model-based recognition scheme</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lamdan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Wolfson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2nd Int. COR[ Computer Vision</title>
		<meeting>2nd Int. COR[ Computer Vision</meeting>
		<imprint>
			<date type="published" when="1988-12">Dec. 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">On the error analysis of geometric hashing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lamdan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Wolfson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989-10">Oct. 1989</date>
			<biblScope unit="page" from="1407" to="1413" />
		</imprint>
		<respStmt>
			<orgName>Robotics Lab. New York University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep. 213</note>
	<note>PQ. 3 7 4 5 . 94-104</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Ieee Transactions On</forename><surname>Pa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ITERN ANALYSIS AND MACHINE INTELLIGENCE</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The viewpoint consistency constraint</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">I</biblScope>
			<biblScope unit="page" from="57" to="72" />
			<date type="published" when="1987">APRIL 1994 1441. 1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Perceptual organization for scene segmentation and description</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. and Machine Intell</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="616" to="635" />
			<date type="published" when="1992-06">June 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The evolution and testing of a modelbased object recognition system</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Mundy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Heller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd. Int. Conf Computer Vision</title>
		<meeting>3rd. Int. Conf Computer Vision</meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="268" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Description and recognition of curved objects</title>
		<author>
			<persName><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">0</forename><surname>Binford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intell</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">I</biblScope>
			<biblScope unit="page" from="77" to="98" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">On a scalable parallel implementation of geometric hashing on the connection machine</title>
		<author>
			<persName><forename type="first">I</forename><surname>Rigoutsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hummel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Courant Inst. of Math. Science, New York Univ., Tech. Rep</title>
		<imprint>
			<biblScope unit="volume">554</biblScope>
			<date type="published" when="1991-04">Apr. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Robust similarity invariant matching in the presence of noise</title>
		<author>
			<persName><forename type="first">I</forename><surname>Rigoutsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hummel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 8th. Israeli Con$ Artificial Intelligence and Computer Vision</title>
		<meeting>8th. Israeli Con$ Artificial Intelligence and Computer Vision</meeting>
		<imprint>
			<date type="published" when="1991-12">Dec., 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Kak</surname></persName>
		</author>
		<title level="m">Digital Picture Processing</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Academic</publisher>
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m">Parallel Distributed Processing: Exploratioms in the Microsructures of Computing</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</editor>
		<editor>
			<persName><surname>Mcclelland</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Computing with connections in visual recognition of origami objects</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sabbah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Sci</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">I</biblScope>
			<biblScope unit="page" from="25" to="50" />
			<date type="published" when="1985-03">Jan.-Mar. 1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Reconstruction of curved-surface bodies from a set of imperfect projections</title>
		<author>
			<persName><forename type="first">R</forename><surname>Shapka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th Inr</title>
		<meeting>5th Inr</meeting>
		<imprint>
			<date type="published" when="1977-08">Aug. 1977</date>
			<biblScope unit="page" from="22" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Efficient two dimensional object recognition</title>
		<author>
			<persName><forename type="first">F</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf Pattern Recognition, Alantic City</title>
		<meeting>Int. Conf Pattern Recognition, Alantic City</meeting>
		<imprint>
			<date type="published" when="1990-06">June 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Nonplanar curve and surface estimation in 3-space</title>
		<author>
			<persName><forename type="first">G</forename><surname>Taubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Con$ on Robotics and Automation</title>
		<meeting>IEEE Con$ on Robotics and Automation</meeting>
		<imprint>
			<date type="published" when="1988-04">Apr. 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Recognizing partially occluded parts</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Tumey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Mudge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Volz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1985-07">July 1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Projective invariants of shapes</title>
		<author>
			<persName><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. DARPA Image Understanding Workshop</title>
		<meeting>DARPA Image Understanding Workshop</meeting>
		<imprint>
			<date type="published" when="1988-04">Apr. 1988</date>
			<biblScope unit="page" from="1125" to="1126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Scale space filtering</title>
		<author>
			<persName><forename type="first">A</forename><surname>Witkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 8th Int. Joint Conf Artificial Intelligence</title>
		<meeting>8th Int. Joint Conf Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1983-08">Aug. 1983</date>
			<biblScope unit="page" from="1019" to="1021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
