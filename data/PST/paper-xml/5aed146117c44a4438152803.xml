<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MAERI: Enabling Flexible Dataflow Mapping over DNN Accelerators via Reconfigurable Interconnects</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hyoukjun</forename><surname>Kwon</surname></persName>
							<email>hyoukjun@gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology Atlanta</orgName>
								<address>
									<country key="GE">Georgia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ananda</forename><surname>Samajdar</surname></persName>
							<email>anandsamajdar@gatech.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Georgia Institute of Technology Atlanta</orgName>
								<address>
									<country key="GE">Georgia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tushar</forename><surname>Krishna</surname></persName>
							<email>tushar@ece.gatech.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">Georgia Institute of Technology Atlanta</orgName>
								<address>
									<country key="GE">Georgia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MAERI: Enabling Flexible Dataflow Mapping over DNN Accelerators via Reconfigurable Interconnects</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3173162.3173176</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep neural networks (DNN) have demonstrated highly promising results across computer vision and speech recognition, and are becoming foundational for ubiquitous AI. The computational complexity of these algorithms and a need for high energy-efficiency has led to a surge in research on hardware accelerators. To reduce the latency and energy costs of accessing DRAM, most DNN accelerators are spatial in nature, with hundreds of processing elements (PE) operating in parallel and communicating with each other directly.</p><p>DNNs are evolving at a rapid rate, and it is common to have convolution, recurrent, pooling, and fully-connected layers with varying input and filter sizes in the most recent topologies. They may be dense or sparse. They can also be partitioned in myriad ways (within and across layers) to exploit data reuse (weights and intermediate outputs). All of the above can lead to different dataflow patterns within the accelerator substrate.</p><p>Unfortunately, most DNN accelerators support only fixed dataflow patterns internally as they perform a careful codesign of the PEs and the network-on-chip (NoC). In fact, the majority of them are only optimized for traffic within a convolutional layer. This makes it challenging to map arbitrary dataflows on the fabric efficiently, and can lead to underutilization of the available compute resources.</p><p>DNN accelerators need to be programmable to enable mass deployment. For them to be programmable, they need to be configurable internally to support the various dataflow patterns that could be mapped over them. To address this need, we present Maeri, which is a DNN accelerator built with a set of modular and configurable building blocks that can easily support myriad DNN partitions and mappings by appropriately configuring tiny switches. Maeri provides</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The recent resurgence of the AI revolution has transpired because of synergistic advancements across big data sets, algorithms, and hardware. As a result, deep neural networks (DNNs) are being deployed at an increasing scale -across the cloud and IoT platforms -to solve complex regression and classification problems in image <ref type="bibr" target="#b0">[1]</ref> and speech recognition <ref type="bibr" target="#b1">[2]</ref> with accuracies surpassing those of humans.</p><p>The microarchitecture of DNN inference engines is currently an area of active research in the computer architecture community. GPUs are extremely efficient for training due to the mass parallelism they offer, multi-core CPUs continue to provide platforms for algorithmic exploration, and FP-GAs provide power-efficient and configurable platforms for algorithmic exploration and acceleration; but for mass deployment across various domains (smartphones, cars, etc), specialized DNN accelerators are needed to maximize performance/watt. This observation has led to a flurry of ASIC proposals for DNN accelerators over the recent years <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref>.</p><p>This work is motivated by a key practical and open challenge for DNN accelerator ASIC design going forward: programmability. DNNs have been evolving at a massive rate. this short list itself, we can observe heavy variability in terms of layer types (convolution, recurrent, pooling, and fully-connected) and input/filter sizes. Some DNNs such as GoogLeNet <ref type="bibr" target="#b9">[10]</ref> have additional "reduction" layers to reduce image dimensions. An efficient accelerator is one that can be programmed to run all these DNNs efficiently. We can consider programming an accelerator as a dataflow graph (DFG) partitioning and mapping problem to target accelerator substrates (analogous to the role of a compiler in programmable CPUs). The classic way of partitioning DFGs is layer by layer, and then slicing the layer into blocks that can be mapped over the underlying hardware substrate. This is the approach most of the early DNN accelerators took <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref>. However, recently new partitioning approaches that explore additional data reuse opportunities by changing the partition have emerged -across layers <ref type="bibr" target="#b15">[16]</ref>, kernels <ref type="bibr" target="#b16">[17]</ref> and hybrid (kernel, channel, output) <ref type="bibr" target="#b17">[18]</ref>. Among these, for e.g., Fused CNN <ref type="bibr" target="#b15">[16]</ref> retains intermediate outputs of a layer and calculate the next layer output using the retained outputs, not iterating over within a layer. This reduces data transfer between memory and processing elements (PEs), thereby increasing power efficiency. In addition to partitioning, the DFG can also be transformed by removing some of the edges whose weights are zero or close to zero. This is the idea of sparsity <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref> and has been popular recently to reduce power consumption inside DNN accelerators. Each of these promising approaches lead to a unique dataflow between neurons in the DFG, which in turn translates to irregular data movement patterns on-chip between the processing and memory elements. Unfortunately, most ASIC DNN accelerators proposed to date are fairly rigid in terms of their internal implementations, as the PEs and network-on-chip (NoC) are tightly coupled, as we discuss in Section 2, making it infeasible for them to support all these dataflows within the same substrate. As a result, each new optimization has resulted in a new accelerator proposal optimized for the given optimization goal <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b19">20]</ref>. This makes the hardening of DNN accelerators into an IP or a discrete chip impractical.</p><p>How do we design a single accelerator substrate that can handle the growing number of dataflows resulting from multiple kinds of layers, dense and sparse connections, and various partitioning approaches? Our proposed design philosophy is to make the interconnects within the accelerator reconfigurable. The DNN DFG is fundamentally a multidimensional multiply-accumulate calculation. Each dataflow is essentially some kind of transformation of this multidimensional loop <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>. We propose to design DNN accelerators as a collection of multiply and adder engines, each augmented with tiny configurable switches that can be configured to support different kinds of dataflows. Our design is called Maeri (Multiply-Accumulate Engine with Reconfigurable Interconnect) <ref type="foot" target="#foot_0">1</ref> . Maeri can be viewed as a design methodology rather than a fixed design by itself, that makes a case for building accelerators using a suite of plug-and-play building blocks rather than as a monolithic tightly-coupled entity. These building blocks can be tuned at runtime using our novel tree-based configurable interconnection fabrics to enable efficient mapping of myriad dataflows.</p><p>We demonstrate Maeri with multiple case studies, demonstrating how it handles convolutions, recurrent layers, irregular filter sizes, and sparsity, providing 8-459% better utilization compared to a tightly-coupled rigid accelerator while adding 6.5% power overhead and reducing 36.8% area overhead over a state-of-the-art baseline like Eyeriss <ref type="bibr" target="#b5">[6]</ref>, and adding 47.0% area and increase throughput by 49.0% over a systolic array <ref type="bibr" target="#b22">[23]</ref>.</p><p>The rest of the paper is organized as follows. Section 2 provides background on DNNs and internal dataflows. Section 3 presents the Maeri design. Section 4 demonstrates various dataflow mappings over Maeri. Section 5 presents implementation results. Section 6 evaluates Maeri with myriad case studies. Section 7 presents related work, and Section 8 concludes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Motivation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Deep Neural Networks</head><p>Neural networks are a rich class of algorithms that can be trained to model the behavior of complex mathematical functions. Neural networks models human brain with a large collection of "neurons" connected with "synapses". Each neuron is connected with many other neurons and its output enhances or inhibits the actions of the connected neurons. The connection is based on weights associated with synapses. Deep neural networks (DNNs) have multiple internal (called hidden) neuron layers before the final output layer that performs the classification.</p><p>At present, there are three popular flavors of DNNs <ref type="bibr" target="#b22">[23]</ref>: Convolution Neural Networks (CNN), Recurrent Neural Networks (RNN), and Multi-Layer Perceptrons (MLP). CNNs are feed-forward DNNs that have demonstrated a remarkable degree of accuracy in recognition and classification of images, exceeding human capabilities <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b23">24]</ref>. Each convolution layer receives input in form of a raw image or input activations (the output of a previous convolution layer) and produces output activations by sliding a set of filters over input images <ref type="bibr" target="#b24">[25]</ref>. Convolutions account for almost 90% of the computations in a CNN <ref type="bibr" target="#b25">[26]</ref>. RNNs add feed-back connections inside the neural network to reflect past context during the feed-forward computation and are used for tasks involving signals with temporal correlation, such as speech recognition, transliteration and so on. The most popular RNN is long short-term memory (LSTM) <ref type="bibr" target="#b26">[27]</ref>, where the output at a certain time depends on the current input value, three gate values (forget, input, and output), and one state value. RNNs account for 29% of Google's inference traffic <ref type="bibr" target="#b22">[23]</ref>.</p><p>All DNNs typically use one or more fully-connected (FC) layers at the end to perform the actual data classification. MLPs are built using multiple fully-connected (FC) layers and are the most general forms of DNNs. DNNs also employ other layers such as pooling (POOL) to reduce the outputs of multiple neurons into one. Table <ref type="table" target="#tab_0">1</ref> lists some of the most popular DNNs in use today.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Dataflows in a DNN</head><p>In this work, we define "dataflow" as the data communication pattern within a DNN accelerators between the compute and memory elements. Dataflow affects the data reuse pattern, which is critical to throughput and energy efficiency of the accelerator, and has thus been the subject of active research recently. We identify three factors that affect the dataflow.  Dataflows due to DNN Topology. DNN topologies have been evolving at timescales of a few months due to the proliferation of deep learning frameworks <ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref> and the availability of massive datasets. The DNNs listed in Table <ref type="table" target="#tab_0">1</ref> can beat humans at vision/speech level tasks. The most accurate DNNs employ a multitude of layers, and input/filter sizes. Naturally, these lead to different dataflow patterns based on the type of layer, and the size, as Section 2.1 alluded to.</p><p>Dataflows due to DNN Topology Mapping. The DNN DFG can be partitioned in multitude ways in computation order, partitioning size, and hardware mapping of each partition, as Figure <ref type="figure" target="#fig_0">2</ref> shows. For example, Fused-layer <ref type="bibr" target="#b15">[16]</ref> modified the computation order (loop order) in CNNs to maximize data reuse that minimizes energy-consuming global communication between the PE array and the prefetch buffer. Ma et al. <ref type="bibr" target="#b31">[32]</ref> leverage loop blocking, unrolling, ordering to minimize storage in DRAM. All these loop optimizations change the dataflow within an accelerator fabric. For instance, a loop ordering optimization in a CNN like AlexNet <ref type="bibr" target="#b8">[9]</ref> across the first three layers would lead to a 11x11 filter, 5x5 filter, and 3x3 filter being simultaneously mapped over an accelerator array which is quite difficult to achieve in most accelerator implementations, as Section 2.3 discusses.</p><p>Dataflows due to DFG optimizations (e.g., pruning zero edges; sparsity) The DNN DFG can also be changed by removing redundant parts, especially zero weights or inputs, because a multiplication with zero is always zero, and does not affect the result of the accumulation. EIE <ref type="bibr" target="#b6">[7]</ref> reported that the percentage of zeros in weights from practical benchmarks ranges from 4% to 25%, which implies that we can reduce the number of computation from 4% to 25%. This in turn has led to accelerator implementations optimized for handling the unique dataflows stemming from sparsity <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b19">20]</ref>.</p><p>In summary, various DNN topology and DFG transformations/pruning lead to different dataflow patterns. In particular, to exploit the benefits of DFS optimization and pruning, hardware support is essential. Therefore, we need DNN accelerator designs that can efficiently handle myriad dataflows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Challenges with supporting flexible dataflows</head><p>The big challenge with DNN accelerator ASICs today is that the design process tightly couples PEs and the NoC. A generic Distribute Sending input activations and filter weights for each neuron to the various PEs from the PB. Distributes can be unicasts, multicasts or broadcasts, depending on the dataflow mapping. Compute The multiplication of input activations with the weight to generate a partial sum (psum) in each PE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reduce</head><p>Adding the partial sums generated by PEs to generate the output activation of each neuron.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Collect</head><p>Transferring the final output activations from multiple PEs to the PB, to be used as inputs to the next layer. Collect can also be used to store psums in case the entire neuron does not fit over the PE array. all-to-all NoC like a crossbar or a mesh is extremely area and power inefficient <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b32">33]</ref> for an array of 100s of tiny PEs, and as a result almost every DNN accelerator has used a hierarchy of buses <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b33">34]</ref> and/or trees <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref>. For instance, Eyeriss <ref type="bibr" target="#b5">[6]</ref> uses buses to connect 12 PEs together in a row, and 14 rows are connected together by another bus. SCNN <ref type="bibr" target="#b7">[8]</ref> creates clusters 4x4 PEs with adder trees internally, and an external bus connecting to on-chip SRAM. The size of each cluster is often determined by the nominal size of the filters (3?3 or 4?4), and the buses/trees optimized for data distribution and collection from these. This rigid structure restricts arbitrary filter sizes or cross-layer dataflows from being supported in these designs. Moreover, when optimizations such as sparsity are introduced, the filter sizes can vary dramatically while the size of the PE clusters is fixed, leading to inefficient utilization as we demonstrate in this work.</p><p>FPGAs have been popular substrates to evaluate various dataflows <ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref> as they provide the flexibility of running different dataflows based on their reconfigurable substrate. A recent work explored FPGA design optimization for DNN <ref type="bibr" target="#b34">[35]</ref> demonstrates that the nominal tiling factor for CNN computation can vary layer by layer and generates optimized RTL for every layer. Fused CNN <ref type="bibr" target="#b35">[36]</ref> augments this approach to support cross-layer dataflows on FPGAs. However, while FPGAs vs. ASICs for DNNs is an ongoing debate, with the flexibility being touted as a reason to prefer the former, the area, timing, and power-efficiency of ASICs is still the strong case for ASICs.</p><p>The goal of this work is to provide the flexibility afforded by FPGAs today in terms of flexible dataflow support to ASIC accelerators. Our key idea is to use a homogeneous, rather than hierarchical, design and provide flexibility within the NoC topology to create virtualized clusters of arbitrary sizes at runtime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Taxonomy of communication flows</head><p>Without loss of generality, we assume a DNN accelerator implementation comprising of an on-chip memory buffer (that we call a prefetch buffer (PB)) to prefetch inputs/weights/intermediate values from DRAM), and a multitude of PEs. Any dataflow pattern mapped over these PEs can essentially be  The AS is an adder augmented with a tiny switch that enables us to map arbitrary adder trees over our nonblocking reduce network called ART. It also contains comparators for pooling operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Building Block Description</head><p>The The DN is based on a chubby-tree structure, which is a tree-based network with wider link bandwidth in higher levels of a tree. We exploit abandunt bandwidth at high level links to enable multicast functionality, which is one of the most common traffic patterns in DNN accelerators.</p><p>Data_In Data_Out_L Data_Out_R</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(c) Simple Switch (SS)</head><p>The SS provides 1:2 switching functionality in the distribute network's chubby tree nodes. abstracted to perform one of four operations listed in Table <ref type="table" target="#tab_3">2</ref>. We use this taxonomy throughout this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Maeri Building Blocks</head><p>Figure <ref type="figure">1</ref> provides an overview of the Maeri microarchitecture. We use a homogeneous design with plug-and-play building blocks that are listed in Figure <ref type="figure" target="#fig_2">3</ref>. A prefetch buffer (PB) serves as a cache of DRAM, and stores input activations, weights, intermediate partial sums that could not be fully accumulated, and output activations. Lookup Tables (LTs) implementing activation functions are located between the root of the reduction-tree and the PB. The secret sauce that enables our flexibility is two-fold:</p><p>(i) We augment the multipliers and adders with configurable switches, calling them multiplier switch (MS) and adder switch (AS) respectively, enabling Maeri to optimize for the collective communication patterns.</p><p>(ii) We use two configurable interconnection networksa distribution network, built using tiny simple switches (SS) sends inputs to the MSes from the PB. A reduce+collect network, built using ASes, sends outputs back to the PB via activation units implemented with look-up tables.</p><p>The entire accelerator is controlled by a programmable controller which manages reconfiguration of all three sets of switches (MS, AS, and SS) for mapping the target dataflow.</p><p>We design two novel interconnect topologies specialized for the distribution and reduction+collection flows, which we describe next. These networks can be tuned to provide full non-blocking bandwidth to the compute blocks, but can be pruned to reduce the bandwidth if required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Distribution Network</head><p>The data distribution network of Maeri sends input activations and weights from the PB to the MSes. It ensures full non-blocking bandwidth to all the multipliers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Topology</head><p>We use a binary-tree as our base topology for distributions as it is multicast friendly, and augment it with (a) chubby links for supporting higher-bandwidth from the PB, and (b) forwarding links, to implementing store-and-forward multicasts for CNNs. We describe these next.</p><p>Chubby Links from Root. A fat-tree supports 2x bandwidth at every level from the leaves up to the root, and is a classic topology for providing non-blocking bandwidth, and is used extensively in datacenter networks <ref type="bibr" target="#b37">[38]</ref>. However, such a topology is infeasible to build on-chip since the bandwidth requirement at the root would require too many wires and ports at the PB, resulting in significant area and power overheads 2 . To address this design-challenge, we propose a Chubby tree, where the bandwidth at the upper levels is either 2x or the same as that at the lower levels. For example, in Figure <ref type="figure">4</ref>, the bandwidth of link level 0 is twice of that of link level 1, but the bandwidth of link level 2 and 3 is 1x.</p><p>We size the bandwidth at the root to equal that of the PB, and taper the width going down the tree, providing nonblocking flows till that level. Once bandwidth becomes 1x, the bandwidth for the rest of the levels are compensated for by adding local buffers at the MSes to hide the communication delay due to multiplexing of links at the 1x levels.</p><p>Forwarding Links at Leaves. Maeri provides local data forwarding links (FL) between the leaves (i.e., adjacent MSes) in the distribution network, as shown in Figure <ref type="figure">5</ref> to provide store-and-forward multicast for input activation values. We 2 For a fat-tree network, 256 MSes would require a 256 ported SRAM.  highlight their use for CNNs in Section 4. The FL's are unidirectional since Maeri maps data over the multiplier switches in order, so input activation values flow in one direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Microarchitecture, Routing and Flow-Control</head><p>The distribution tree nodes use a simple switch (SS) whose microarchitecture is shown in Figure <ref type="figure" target="#fig_2">3</ref>. SSes are bufferless demuxes with a select line that is set by the input data directly. SSes to chubby links are direct connections, since no bandwidth sharing is being done. Since the topology is binary-tree based, input data is source routed, with a bit to choose between the left and right paths at each switch. Since the SSes are bufferless, the flow-control is end to end between FIFOs at the MSes and the PB. Also, we provide single-cycle traversals from the PB to the leaves (MS) for every piece of data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Data Reduction and Collection Network: ART</head><p>Binary trees are well-suited for performing reductions and have been used in prior DNN implementations <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21]</ref> to implement adder trees within the PE clusters described in Section 2.3. However, they have a key inefficiency: the fixed topology of a tree is inherently inefficient whenever the number of partial sums to be accumulated is smaller than the width of the adder tree. Figure <ref type="figure" target="#fig_4">6</ref>(a) illustrates this. Suppose there are 16 multipliers, all connected via a 16-node binary reduction tree. Each node in the reduction tree is an adder. This tree is perfect for performing a reduction for a 16-input neuron. However, suppose we map three neurons over these multipliers, each generating five partial sums, as Figure <ref type="figure" target="#fig_4">6</ref>(a) shows. Each neuron requires four additions to generate an output, so the total additions required is 12. The reduction tree contains 16 adders, which should be sufficient to perform the additions for all neurons in parallel. However, the four links in red are shared by multiple neurons, limiting the tree from generating all three outputs simultaneously.</p><p>More formally, the challenge pertains to mapping arbitrary number of reduction trees over an underlying fixed topology. To provide flexibility to support any mapping, this needs to be addressed, otherwise there would be a drop in utilization. We solve this challenge by proposing a new tree topology called an Augmented Reduction Tree (ART).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Topology of ART</head><p>The ART is a binary-tree augmented with additional links to enable multiple arbitrary sized reductions to run in a nonblocking manner.</p><p>Forwarding Links at Intermediate Levels. Figure <ref type="figure" target="#fig_4">6</ref>(b) shows a binary tree with additional links for forwarding the adder outputs to other adders at the same level, instead of to the parent. This removes contention from three out of the four links, and only one link is shared by Neuron 2 and Neuron 3.</p><p>Chubby Links from Root. Physical bandwidth limitations at the root node can limit the number of parallel reductions (collects) as Figure <ref type="figure" target="#fig_4">6</ref>(b) showed. To address this, we augment the ART with chubby links, just like the one in Section 3.1. This eliminates contention completely, as shown in Figure <ref type="figure" target="#fig_4">6</ref>(c). Definition. Augmented Reduction Tree is an undirected graph that consists of a complete binary tree and additional links that connects adjacent tree nodes in the same level with different parents except between leaves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Formal Definition and Properties of ART</head><p>Property 1: Configurability. An ART with N leaves can map any adder tree onto its substructure when the adder tree accumulates values from k consecutive leaves and k &lt; N.</p><p>Property 2: Non-Blocking. An ART can map multiple of such adder trees mentioned in (1) without any sharing any link if the sets of leaves of each adder tree are all disjoint.</p><p>Property 1 guarantees any sized reduction operation can be mapped over an ART. Property 2 guarantees that multiple non-blocking reduction operations can be mapped over an ART. For example, if an ART has 32 leaves and we map We have formally proved these two properties. However, in the interest of space, we provide an intuition on why the ART supports multiple parallel reductions, and will provide the formal proof as supplementary material once Maeri is released. Intuitively, additional connectivity between adjacent adder switches on the same level, but with different parents, provides opportunity to accumulate more partial sums closer to the lower levels which reduces the number of required links in upper levels. Without the additional connectivity, partial sums from the adjacent nodes have to traverse upper levels to be accumulated, increasing the possibility of link congestion. We do not add forwarding links (FLs) between nodes sharing the same parent node because the parent node anyway needs to be traversed to reach the top.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Microarchitecture, Routing, and Flow-Control</head><p>The ART is built using adder switches (AS), whose microarchitecture is shown in Figure <ref type="figure" target="#fig_2">3</ref>. Each AS is statically configured to act as either 2:1 ADD, 3:1 ADD, 1:1 ADD plus 1:1 forward, or 2:2 forward. Section 4.1 describes the configuration algorithm. Each AS also houses a comparator for POOL layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Mapping Dataflows over Maeri</head><p>Our dataflow mapping is mapping neurons one by one over the MSes. We call this Virtual Neuron (VN) Construction. It essentially means configuring the ART, since that is the one that decides which multiplier outputs need to be reduced.</p><p>Once the ART is configured, each VN can operate in parallel. The dataflow flexibility in our design comes from allowing each VN, which is a MAC operation, to take arbitrary number of MSes (rather than relying on fixed clusters). We also support folding of a VN over itself and multiplex multiple multiply operations over fewer MSes (Section 4.8).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Virtual Neuron (VN) Construction over ART</head><p>We describe the algorithm for VN construction over the ART for the example in Figure <ref type="figure" target="#fig_6">7(a)</ref>. Each triangle represents a sub-tree, and each circle is an AS. We focus on the FL marked in the figure . Here, the VN spans from T2 to T6 (Figure <ref type="figure" target="#fig_6">7(a)</ref>).</p><p>Step 1: Compute the span of the neuron on the left and right side of the FL. Set the direction of the FL from the smaller to the larger span. We define span to be the number of sub-trees that the VN (generating the psums) crosses. In Figure <ref type="figure" target="#fig_6">7</ref> (a), the VN spans from T2 to T4 (i.e., three) on the left, and from T5 to T6 (i.e., two) on the right.</p><p>The direction of the FL is set from right to left. If the spans are same, then the direction can be set arbitrarily.</p><p>Step 2: Check if the sub-trees in direction of the smaller span need to use the parent to this FL on that direction. If not, activate this FL. The parent of FL on the right side (i.e., P R ) does not need to be activated for this neuron, since T7 and T8 are not part of the span. Thus the FL is activated by configuring AS A to forward the output from T5 to AS B , and AS B to act as a 3:1 adder.</p><p>Figure <ref type="figure" target="#fig_6">7</ref> (b) shows an alternate scenario. Here, Step 1 determines the direction to be left to right. However, on the left, the AS C has to be activated regardless of the FL, because T2's output goes to it, hence FL does not need to be activated. AS B is configured to forward its output to AS C , not AS A .</p><p>How is the span computed by the algorithm? We represent the leaves (MSes) spanning each neuron using a bitvector. The ART controller starts from FLs in the lowest level to activate FLs, before going up the levels. The number of bits that are 1 on the left and right of this FL in the bit-vector are used to compute the span. Whenever a FL is activated, the bits corresponding to smaller span (i.e., the leaves that will create the psums that will cross this FL) are cleared. This prevents activating multiple FLs in different levels of the ART for the same partial sums.</p><p>Next, we demonstrate how example DNN dataflows can be mapped over Maeri.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Mapping a CONV Layer</head><p>We demonstrate how a CONV layer can be mapped over Maeri with a walk-through example in Figure <ref type="figure" target="#fig_8">8</ref>. The weight filter is 2x2, and the input/output activations with one channel are 4x4. This example assumes that each MS stores one input activation and one filter weight value locally and the chubby ART, together with the PB, provides sufficient bandwidth to cover all simultaneous reduction flows.</p><p>Stage 1: VN Construction. Maeri first constructs a VN by configuring the ART based on the dimension of the target CNN layer, as Figure <ref type="figure" target="#fig_8">8-(1)</ref> shows. The controller then maps the filter weights to a set of consecutive multiplication switches in each VN, and configures the corresponding sub adder tree of the ART using the reconfiguration algorithm described earlier in Section 4.1. Each VN is responsible for generating one row of output activations, and two VNs can share one AS. In the example, VN 0 and 1 share the AS marked in purple. Although an AS (or a node of the ART) is shared, the overall structure still maintains the non-blocking feature since one of the inputs is sent up the tree, and the other is sent laterally using the forwarding link simultaneously, as Section 4.1 described. This configuration step happens before running each CNN layer and remains constant throughout the run. In case the layer does not fit, it can be folded and mapped multiple times as we explain later in this section. + W01 X Xi(j+1) + W10 X X(i+1)j + W11 X X(i+1)(j+1)   <ref type="formula">2</ref>) shows. Recall that in CNNs, the weight matrix slides over input images (Section 2.1); as a result the same weight value is required by multiple VNs, each of which is computing an output activation. We exploit the multicast functionality of the distribution tree, by sending one value from the PB and replicating it at the intermediate simple-switches. For example, weight W 00 , W 01 , W 10 , and W 11 are sent to the first, the second, the third, and the fourth multiplier switch in each VN, respectively. We can exploit the bandwidth of the chubby tree structure to deliver multiple unique weights simultaneously to different multiplier switches. Because each VN requires the same set of weight values, the PB distributes weights only once for every CONV layer, which remain stationary throughout the run of the layer.</p><p>Stage 2.2: Input Activation Distribution. The same input activations are used by multiple neurons, just like the weights. For example, both VN 0 and 1 require input activations X 10 and X 11 , and both VN 1 and 2 require X 20 and X 21 . These are multicasted from the PB. Unlike weight distribution, the distribution of new input activations from the PB needs to be performed whenever each VN has finished computing all psums for one output activation. This part is overlapped (pipelined) with the generation of output activations (Stage 3). For instance, in Figure <ref type="figure" target="#fig_8">8</ref>-(3a), we can see that new input activations X 02 and X 12 arrive at VN 0, X 12 and X 22 arrive at VN 1, and so on, while it is computing O 00 .</p><p>We model the sliding window behavior of the CNN filter weight matrix by using the local forwarding links between the multiplier switches. Each input activation is forwarded left up to the width of the filter row (which is two in this example) and then discarded. For instance, in Figure <ref type="figure" target="#fig_8">8</ref>-(3a), we can see that new input activations X 01 and X 11 are forwarded from the second and fourth MSes to the first and third respectively. Because of the data forwarding, each VN requires only two new input activation values (same as row size of the filter) from the PB every cycle, for generating a new output activation. This reduces the bandwidth requirement of the distribution tree, and the overall energy consumption by reducing the interconnect traversal length.</p><p>Stage 3: Output Generation. After the series of initialization steps (VN construction and weight/input activation distribution) finishes, Maeri starts to produce output activation values. Each VN generates output activation values for one row of the output matrix.For the CNN computation example in Figure <ref type="figure" target="#fig_8">8</ref>, Figure <ref type="figure" target="#fig_8">8-(</ref>3) shows VN 0 producing O 00 , followed by O 01 , and so on. Similarly, VN 1 generates O 10 , O 11 , O 12 , and O 13 . After finishing one row, input activations corresponding to another row are mapped on the VN, and so on till the end of the current convolution layer.</p><p>Optimizing for Spatial Reuse in CNNs. Maeri tries to optimize and get the best of the three kinds of dataflows described in the Eyeriss <ref type="bibr" target="#b24">[25]</ref> taxonomy. Each multiplier switch acts as a weight stationary node without requiring weights to be forwarded back and forth. Each row of the weight filter is mapped sequentially across the multipliers of a VN making it row stationary. And finally, the configurable ART within each VN acts like an output stationary node as it accumulates psums locally. Together, we get a design optimized for high-throughput and low-energy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Mapping a RNN/LSTM Layer</head><p>Figure <ref type="figure">9</ref> shows how Maeri runs a LSTM layer. A LSTM computation consists of four steps: calculating (1) gate values, (2) input transform, (3) next state value using the results from step 1 and 2, and (4) output activation using the results from step 1 to 3. Step 3 and 4 also calculates partial sums and accumulates them like all the other CNN/RNN computation steps. For state value computation (step 3), each VN receives the previous state value calculated the previous time epoch from the PB using the distribution tree, and the forget/input gate values and input transform calculated in step 1 and 2 in the current time epoch. It then generates the current state values based on the received values, as Figure <ref type="figure" target="#fig_2">9-(3)</ref> shows. For the output activation computation (step 4), each VN receives the output gate value and current state value, multiplies the two, and sends the result over to the activation units to produce the final output activation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Mapping a POOL Layer</head><p>Mapping a POOL layer over Maeri requires creating a VN with with the values to be pooled, and configuring the AS to act as a comparator, rather than an adder. The output of the ART is then the pooled value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Mapping a FC Layer</head><p>A FC neuron gets inputs from all neurons in the previous layer. Correspondingly, the VN for this can be mapped as before, except that it would span many more MSes. In the extreme case, the entire ART can be configured to compute the output for one neuron, as Figure <ref type="figure">10</ref> shows. In case one neuron does not fit over the free MSes (either because the number of inputs is greater than the total MSes, or some of the MSes are already configured into other VNs), the neuron can be mapped via folding, as Section 4.8 discusses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Mapping Cross-Layers</head><p>A cross-layer mapping <ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref> can easily be supported over Maeri since each VN can be independently configured. Thus each VN could correspond to neurons of the same layer (as Figure <ref type="figure" target="#fig_8">8</ref>) illustrated, or different layers (Figure <ref type="figure">10</ref>), without requiring any change in the algorithm. In the latter case, the intermediate output from the PB need not be sent to DRAM, but can be streamed to the next VN on-chip directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Mapping Sparse Networks</head><p>Mapping sparse CNNs is also quite trivial in Maeri, as Figure <ref type="figure">10</ref> shows. The size of each VN would be different size since the filter sizes vary depending on weight sparsity. Note that Maeri can support mapping of sparse CNNs but might still need additional support to identify sparsity and stored compressed data <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">Optimization: Folding over Rows</head><p>There can be multiple reasons to fold a physical neuron over fewer MSes than its inputs, such as: (i) there are insufficient MSes, or (ii) the bandwidth of the PB or the chubby distribution tree is insufficient to cover all the input activation values during output activation calculation, (iii) the VN is memory bound and waiting for data from DRAM. To support N-way folding, MSes need to have at least N local buffers. This increases their area and power, but lowers the bandwidth requirement from the network.</p><p>For example, in Figure <ref type="figure" target="#fig_8">8</ref>, we can allocate two multiplier switches rather than four within a virtual neuron. In such a mapping, the output of the VN through the ART is an intermediate psum (one row of the filter in this example). This is sent to the PB for temporary storage and then sent back to the corresponding VN to be accumulated into the final output activation that is sent to the activation units.   with the same number of compute units and area as Eyeriss (Table <ref type="table" target="#tab_11">3</ref>). The left and right column plots area (a, c) and power (b, d), respectively. (e) plots the post place-and-routed area of MAERI, systolic array and Eyeriss, normalized to the 16 PE systolic array.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Implementation</head><p>We implemented Maeri in BSV (Bluespec System Verilog) <ref type="bibr" target="#b38">[39]</ref> and synthesized it with TSMC 28nm standard cell and SRAM library at 200MHz. For comparison, we also synthesized and placed-and-routed Eyeriss <ref type="bibr" target="#b5">[6]</ref> 3 and a systolic array <ref type="bibr" target="#b22">[23]</ref> in our 28nm environment. We created two design points -one where all three accelerators have the same number of compute (i.e., multiply-accumulate or MAC) units and SRAM size, and one where all three have the same chip area, as Table <ref type="table" target="#tab_11">3</ref> shows. We find Maeri to be more area-efficient than Eyeriss for two reasons: (i) its fine-grained MS and AS together are much more area-efficient than a full PE, and (ii) Maeri does not require fully-addressable local register file like Eyeriss; it uses FIFOs instead and relies on delivery of the correct data in the correct order via the distribution tree or local forwarding links. For the same area, Maeri and systolic array can house 206 (2.23 ?) and 1024 (7.09 ?) more compute units than Eyeriss. Because of the larger number of compute units and Maeri's near 100% utilization based on its fully non-blocking trees, synthesis tools report higher power in Maeri than Eyeriss. Thinning the adder tree links can bring us to the same power point, while still offering full configurability but at a loss of 3 We thank the authors of Eyeriss for sharing their RTL with us. full utilization every cycle, which might be an acceptable trade-off as a lot of DNN layers are memory-bound <ref type="bibr" target="#b22">[23]</ref>. The prefetch buffer (SRAM) dominates in both area and power in the two designs. For the same number of PEs (compute units), the systolic array required the smallest area and power because of its simple structure (MACs connected in a grid). This is also illustrated in Figure <ref type="figure" target="#fig_11">11</ref>(e). However, systolic array suffers from low utilization <ref type="bibr" target="#b22">[23]</ref> and requires a large number of SRAM reads because of the lack of data reuse inside a PE array as we demonstrate later in Section 6.3.  When the workload is sparser, the bandwidth requirement for partial sum collection increases because the PE array (multiplier switches and ART in Maeri) can cover more number of neurons at the same time. This becomes a bottleneck for the baseline where the clusters are connected by a bus (which limits bandwidth) even though the total number of computations goes down in a sparse workload. In contrast, Maeri provides 73.8% utilization even at 50% sparsity, and correspondingly 6.9 ? speedup.</p><p>Cross-Layer Dataflow. We model five cross-layer dataflows by fusing a combination of AlexNet convolution layers. baseline accelerator with four 4x4 clusters (i.e., a 16:1 reduction trees in each). We observe 1.08 -1.5 ? speedup. Accelerators based on fixed clusters or fixed sized reduction trees are inefficient in utilizing all the PEs for certain mappings. For example, in Map C, we map three convolutional layers with three 3?3 filters. In the baseline, we can only use 9 PEs within each cluster; trying to utilize the remaining 7 PEs introduces non-uniform traffic that fixed interconnection cannot support. However, MAERI with its flexible interconnects can support such irregularity and thus maintains high utilization and provides maximum speedup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Maeri Deep Dive</head><p>ART vs Fat Tree vs Plain Tree. Figure <ref type="figure" target="#fig_14">15</ref> plots the utilization of the MSes across three kinds of reduction trees: ART, fat-tree, and four plain adder trees as the size of each virtual neuron (VN) mapped over the trees increases. Our aim is to quantify the benefits discussed earlier in Figure <ref type="figure" target="#fig_4">6</ref>. We can observe that the ART provides relatively uniform and high utilization while fat tree and plain adder trees involves significant fluctuation in utilization. Such fluctuations occur because the efficiency of plain adder trees and fat trees are highly sensitive to VN size (number of non-zero weights within a filter of a channel). The plain adder trees have low utilization because VNs less than size 16 end up instantiating an entire tree with idle multipliers, and thus provide 100% utilization only at a VN size of 16. If the VN size is a power of 2, the Fat Tree works identical to the ART since all operations fit within the binary tree structure and the ART's forwarding links are not required. When the VN size is not a power of two (which is the case in VGGnet and in sparse designs), the utilization of a fat-tree drops but ART continues to provide high utilization. ART also has fluctuations of utilization in cases where the total multipliers (64 in this example) is not a multiplier of the VN size as that leads to idle multiplier switches due to temporal folding (see <ref type="bibr">Figure 17 (b)</ref>). Support from an advanced compiler may enable utilizing such multipliers by temporally folding large VNs over few multipliers. Maeri trees vs. traditional NoCs. Compared to other traditional NoC designs, the NoC in Maeri is highly areaand power-efficient, as post-synthesis area and power overhead over NoC bandwidth plot in Figure <ref type="figure" target="#fig_5">16</ref> presents. The overhead of the NoC design in Maeri is minimal compared to mesh or crossbar while the NoC design provides sufficient bandwidth for the traffic in Maeri. This is because the NoC architecture in Maeri is optimized just for communication patterns within DNN accelerators, and it consists of extremely light-weight building blocks. Maeri vs. systolic arrays. Systolic array is also considered as a major approach <ref type="bibr" target="#b22">[23]</ref> to implement an accelerator because of the low PPA cost based on its relatively simple design and high parallelism it provides. We compare a systolic array-based design and Maeri using the example presented in Figure <ref type="figure" target="#fig_16">17</ref>, which is a convolutional layer with eight 3x3x3 weight values, 5x5x3 input activation values, and a stride of one. In the example in Figure <ref type="figure" target="#fig_16">17</ref>, both the systolic array (a) and Maeri (b) contain 64 PEs (i.e., MACs in Systolic array, multiplier and adder switches in Maeri). The array reads input activation and weights from the left and the top of the array, respectively. Because systolic array is based on store-and-forward dataflow style, a controller needs to carefully adjust the injection time of each data, which lets the PEs remove complicated hardware for control data I/O. For example, if weight value A1 in the first 3D filter (red) and input activation a1 are injected to PE 0 at cycle t, weight value A1 in the second 3D filter (blue) needs to be injected at cycle t+1 because input activation a1 arrives at the PE 1 at cycle t+1. To process one sliding window in the example convolution layer, the systolic array needs to read 216 weights and input activation (3?3?3?8). Because each sliding window is mapped on each row of the systolic array, the systolic array can process eight sliding windows within an iteration. Each iteration requires not 27 (the number of partial sums to be generated in a sliding window) cycles but 43 cycles (27 + 8 (injection delay)+ 8 (time to process the last weight/input activation set, which are weight I3 in the last filter (green) and input activation y3)) and generates eight output activations. Because the example requires sliding the window 25 times, the total number of iteration is four with an incomplete iteration that computes only one output activation at the last iteration. Therefore, the total cycles to process the example layer is 156 cycles <ref type="bibr">(43 ? 3 + 27)</ref>. Although systolic array provides high throughput, it cannot reuse data within the PE array so it requires to read different data every cycle to each row and column, which results in high energy consumption <ref type="bibr" target="#b24">[25]</ref>. Therefore, the systolic array need to read 1,323 times from the SRAM in the example.</p><p>In contrast, Maeri minimizes the number of SRAM reads by weight reuse in multiplier switches and multicasting input activations, which requires 516 reads (35% compared to the systolic array) for the same example. To the process the same convolutional layer in the example, Maeri first maps each channel in 3D filters (nine weights) as a VN across the multipliers, creating seven VNs in this example (with the last multiplier idle). We utilize temporary register in each adder switch to accumulate output activations from folded filters mapped in different iterations. In this manner, the number of iterations is four, and each iteration consumes 37 cycles (1 for configuration + 9 for weight distribution + 27 for multicasting input activations) with 8x chubby distribution tree. Therefore, Maeri requires 143 cycles to process the example convolutional layer, which reduces 9% of total latency compared to the example systolic array presented in Figure <ref type="figure" target="#fig_16">17 (a)</ref>.</p><p>In summary, Maeri provides 9% better throughput and 65% less SRAM reads in the above example. Extending the same analysis to 256x256 systolic array (TPU <ref type="bibr" target="#b22">[23]</ref> specifications) vs Maeri with 256x256 multipliers on VGG16, we observe MAERI issues 6.3x less memory reads. Furthermore, the improvements can be more significant in irregular dataflows such as sparse and inter-layer fusion. However, the better performance and energy efficiency from less SRAM reads of Maeri comes at an area cost, as Figure <ref type="figure" target="#fig_11">11</ref> shows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Works</head><p>In past five years, hardware support to accelerate DNN applications has been one of the most active research topics in computer architecture. The most related works to ours are related to supporting flexible dataflows and NoCs inside accelerators. In addition, we also discuss other related work across DNN accelerator design.</p><p>DNN accelerators for flexible dataflows: FlexFlow [34] and DNA <ref type="bibr" target="#b17">[18]</ref> are two recent DNN ASIC proposals with similar motivation as this work. FlexFlow demonstrates a design that provides feature-map, neuron, and synapse-level parallelism, by different mapping strategies across PE rows and columns. DNA leverages Weight, Input, and Output Reuse within the same fabric. However, both FlexFlow and DNA's flexible dataflows are restricted within a layer, not across layers. Moreover, both FlexFlow and DNA are restricted to CNNs and cannot run LSTMs but Maeri can.</p><p>NoCs for DNN accelerators: Most of NoC studies for DNNs <ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref><ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b42">[43]</ref> have proposed meshes due to their flexibility to support all-to-all communication. Diannao <ref type="bibr" target="#b2">[3]</ref> and Shidiannao <ref type="bibr" target="#b43">[44]</ref> relied on mesh-based interconnects for data transfer. However, meshes add extremely high area and power overheads as our work and others <ref type="bibr" target="#b32">[33]</ref> have shown. Dadiannao <ref type="bibr" target="#b3">[4]</ref> employed fat tree for scatter and gather and 3D mesh via hyperTransport 2.0 to distribute data among nodes. Eyeriss <ref type="bibr" target="#b5">[6]</ref> uses separate buses for its scatters and gathers. CNN accelerators: Convolution Engine <ref type="bibr" target="#b44">[45]</ref> explored the trade-off between flexibility and efficiency in accelerator domain with an example of convolution accelerator for image processing domain. Diannao <ref type="bibr" target="#b2">[3]</ref>, DaDiannao <ref type="bibr" target="#b14">[15]</ref>, ShiDiannao <ref type="bibr" target="#b43">[44]</ref> are early spatial DNN accelerators. A recent work in FPGA <ref type="bibr" target="#b20">[21]</ref> proposed constraint aware optimization tool for FPGA based accelerators. The design used simple adder trees within their computation engine, which can benefit from our ART. Eyeriss <ref type="bibr" target="#b24">[25]</ref> analyzed data flow patterns in existing CNN accelerators and proposed a new data flow pattern for CNN acceleration called row stationary, which performs better than other data flow patterns in terms of throughput and energy efficiency. Maeri's VN mapping, without folding, essentially implemented row stationary across the multipliers, and output stationary over the ART to get the best of both dataflows. These works are all complementary to Maeri, since our configurable distribution and reduction trees can enable them to support more dataflows.</p><p>Cross-layer CNN Accelerators: A lot of recent works have performed design-space exploration of DNN accelerators, such as finding a better way to map data on to hardware or the best configuration of DNN accelerators, using novel simulation infrastructures <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b45">[46]</ref><ref type="bibr" target="#b46">[47]</ref><ref type="bibr" target="#b47">[48]</ref>. Fused-layer CNN <ref type="bibr" target="#b15">[16]</ref> and others <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b48">49]</ref> have explored CNN architectures optimized for cross-layer optimizations over FPGAs. FPGAs provide immense flexibility in tuning the RTL for the right dataflow pattern and filter size(s) for mapping. Maeri aims to provide an ASIC substrate with similar flexibility.</p><p>Sparse CNN Accelerators: SCNN <ref type="bibr" target="#b7">[8]</ref> is a recent accelerator for sparse CNNs, that leverages sparsity in activations and weights. Cnvlutin <ref type="bibr" target="#b18">[19]</ref> compresses activation values based on the ReLU operator. Cambricon-X uses weight sparsity to keep only non-zero weights in its buffers. EIE <ref type="bibr" target="#b6">[7]</ref> uses a compressed representation of weights and activations, delivering only non-zero operands to multipliers, for FC layers.</p><p>Maeri's aim is not to identify sparsity or compress data, but rather to construct arbitrary sized VNs given sparse data.</p><p>RNN accelerators: Although the algorithm of RNNs has been discussed for more than 20 years <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b49">[50]</ref><ref type="bibr" target="#b50">[51]</ref><ref type="bibr" target="#b51">[52]</ref>, hardware design of RNNs was not as active as that of CNNs. In the last two years, hardware acceleration of LSTMs on FPGAs <ref type="bibr" target="#b52">[53]</ref><ref type="bibr" target="#b53">[54]</ref><ref type="bibr" target="#b54">[55]</ref><ref type="bibr" target="#b55">[56]</ref><ref type="bibr" target="#b56">[57]</ref> has been explored. Maeda et al. suggested new learning algorithm of Hopfield RNN <ref type="bibr" target="#b57">[58]</ref> which has been implemented on FPGA <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b59">60]</ref>. An ASIC implementation for accelerating the control of RNN networks was recently demonstrated <ref type="bibr" target="#b60">[61]</ref>. DNPU <ref type="bibr" target="#b61">[62]</ref> implements CNN and RNN-LSTM in a single chip like Maeri but requires independent compute units for CNN and RNN while Maeri computes both CNN and RNN with the same substrate. Google's TPU <ref type="bibr" target="#b22">[23]</ref> can also run CNNs and LSTMs together though it uses a Systolic Array which cannot provide the same level of configurability as our distribution and reduction networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>In this paper we describe and evaluate Maeri, a fabric for mapping arbitrary dataflows that arise in DNNs due to its topology or mapping. Our approach is to augment multipliers and adders with tiny switches, and interconnect them via a novel reconfigurable interconnect that supports arbitrary sized neurons. We demonstrate how Maeri is not only capable of running CONV, LSTM, POOL and FC layers, but also supports cross-layer mapping and sparsity. Maeri's NoCs add minimal overheads over NoCs in state-of-the-art CNN accelerators while providing tremendous flexibility. We believe that Maeri is quite robust to supporting new optimizations in DNNs as it can construct arbitrary sized MAC engines very efficiently. This work also opens up exciting opportunities in compiler designs that can take arbitrary DNNs and map them efficiently over a Maeri-like fabric.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Techniques to optimize dataflows in DNN accelerators</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. The microarchitecture of building blocks used in Maeri and description of them.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .Figure 5 .</head><label>45</label><figDesc>Figure 4. Example of chubby distribution tree. Leaves are multiplier switches. Other nodes are simple switches without compute units.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Motivation for Augmented Reduction Tree (ART). Three neurons are mapped over five multipliers each. Each neuron generates one output using the adder tree for reduction. Red links in (a) and (b) represent congested links, thick links in (c) and (d) represent links with double bandwidth. The forward links (FL) in the ART are bi-directional.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 (</head><label>6</label><figDesc>d) presents an example of an ART. We formally define it and present two key properties.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Two examples of forwarding link reconfiguration. multiple adder trees that accumulates five values, ART can accommodate four adder trees at the same time providing non-blocking reduction.We have formally proved these two properties. However, in the interest of space, we provide an intuition on why the ART supports multiple parallel reductions, and will provide the formal proof as supplementary material once Maeri is released. Intuitively, additional connectivity between adjacent adder switches on the same level, but with different parents, provides opportunity to accumulate more partial sums closer to the lower levels which reduces the number of required links in upper levels. Without the additional connectivity, partial sums from the adjacent nodes have to traverse upper levels to be accumulated, increasing the possibility of link congestion. We do not add forwarding links (FLs) between nodes sharing the same parent node because the parent node anyway needs to be traversed to reach the top.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. CONV computation in Maeri. W, X, and O represent weights, input activations, and output activation, respectively. The indices of W, X, and O represent the position of each value within each matrix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Stage 2 . 1 :</head><label>21</label><figDesc>Weight Distribution. Next, Maeri starts to distribute filter weights, as Figure8-(</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. Area and power breakdown of MAERI, systolic array and Eyeriss. Comp match (a,b) and area match (c,d) indicate design points</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 12 .Figure 13 .</head><label>1213</label><figDesc>Figure 12. Total latency and compute unit utilization of systolic array(SA), Eyeriss<ref type="bibr" target="#b24">[25]</ref> style row-stationary accelerator, and Maeri with 64 PEs (multiplier switches) for selected conv layers in Alexnet and VGG16. The latency is normalized to the first Alexnet convolutional layer delay in an ideal accelerator with 64 PEs, infinite bandwidth between all the PEs and the PB, and 1-cycle fixed point computational units.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 15 .</head><label>15</label><figDesc>Figure 15. PE utilization with ART, fat-tree and four 16-wide plain adder trees when 64 PEs are used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>2 Figure 16 .</head><label>216</label><figDesc>Figure 16. Area and power comparison between the NoC in Maeri and traditional NoCs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 17 .</head><label>17</label><figDesc>Figure 17. A mapping example of a convolution layer with eight 3x3x3 filters and 5x5x3 input activation over (a) a systolic array with 64 PEs and (b) MAERI with 64 multiplier switches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Parameters of recent DNNs. 8-459% better utilization across multiple dataflow mappings over baselines with rigid NoC fabrics.</figDesc><table><row><cell>DNN</cell><cell>Year</cell><cell>Num</cell><cell>Num</cell><cell>Num</cell><cell>Num</cell><cell>Filter</cell><cell>Input</cell></row><row><cell></cell><cell></cell><cell>CNV</cell><cell>RNN</cell><cell>POOL</cell><cell>FC</cell><cell>Sizes</cell><cell>Sizes</cell></row><row><cell>Alexnet [9]</cell><cell>2013</cell><cell>6</cell><cell>0</cell><cell>1</cell><cell>1</cell><cell>11x11,</cell><cell>224x224</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>5x5,</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>3x3</cell><cell></cell></row><row><cell>Googlenet [10]</cell><cell>2014</cell><cell>59</cell><cell>0</cell><cell>16</cell><cell>5</cell><cell>1x1,</cell><cell>224x224</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>3x3,</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>5x5</cell><cell></cell></row><row><cell>Resnet-50 [11]</cell><cell>2014</cell><cell>49</cell><cell>0</cell><cell>2</cell><cell>0</cell><cell>1x1,</cell><cell>224x224</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>3x3</cell><cell></cell></row><row><cell>VGGnet-16 [12]</cell><cell>2015</cell><cell>13</cell><cell>0</cell><cell>5</cell><cell>3</cell><cell>1x1,</cell><cell>224x224</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>3x3</cell><cell></cell></row><row><cell cols="2">DeepSpeech2 [2] 2016</cell><cell>2</cell><cell>7</cell><cell>0</cell><cell>1</cell><cell>41x11,</cell><cell>13x41x11</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(GRU)</cell><cell></cell><cell></cell><cell>21x11</cell><cell></cell></row><row><cell>Deep voice [13]</cell><cell>2017</cell><cell>0</cell><cell>40</cell><cell>0</cell><cell>3</cell><cell>-</cell><cell>28x29</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>lists some of the popular ones in use today. Within An overview of Maeri. Maeri is designed to efficiently handle CONV, LSTM, POOL and FC layers. It can also handle cross-layer and sparse mappings. We implement this flexibility using a novel configurable interconnection network topology within the accelerator.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Activation Control</cell><cell></cell><cell></cell><cell></cell><cell cols="3">Layer Topology</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Activation Units</cell><cell></cell><cell></cell></row><row><cell>Dense Dataflow</cell><cell>Map</cell><cell>Distribution/Reduction Network</cell><cell>Control</cell><cell>x</cell><cell>+ Accelerator Controller + + + + + x x x x x</cell><cell>+ x x</cell><cell>+</cell><cell>+ x x</cell><cell>+</cell><cell>+ x x x x + +</cell><cell>+</cell><cell>+ Augmented x x Reduction Tree (ART)</cell><cell>Prefetch Buffer</cell><cell>From/To DRAM + Adder Switch Multiplier Switch MAERI Blocks 1:2 Switch x</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Distribution Tree</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Local Buffer</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Weights / Input Activations</cell><cell></cell><cell>Local Forwarding</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Data link</cell></row></table><note><p><p>MAERI</p>Figure 1.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Taxonomy of On-Chip Communication Flows.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>(a) Adder Switch (AS) (d) Configurable Look-up Table (LT)</head><label></label><figDesc></figDesc><table><row><cell>Fwd_In_L Fwd_In_R</cell><cell></cell><cell>Output_Up</cell></row><row><cell>Input _L</cell><cell>+/&gt;</cell><cell>Fwd_Out_L</cell></row><row><cell>Input _R</cell><cell></cell><cell>Fwd_Out_R</cell></row><row><cell>Lookup</cell><cell>Table</cell><cell></cell></row><row><cell>Address</cell><cell>Value</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>, and C h represent weights for forget/intput/output gated and input transform multiplied with previous output activations. X and H represent input and output activations. The indices of F, I, O, and C indicate the ID of corresponding neuron and position within the weight vector (e.g., F 30 indicates the first forget gate filter weight value for neuron 4.) The index of F h , I h , O h , and C h means its corresponding neuron ID (e.g., C h3 represents the filter weight value to be multiplied with the previous output activation in neuron 4). The four steps presented generate an output activation for each VN. f k , i k , o k , and t k represent forget/input/output gate values and input transform at the current time epoch. B f , B i , B o , and B c are bias values for each gate value and input transform. s k and s pk are the state values for the current and previous epoch, respectively.</figDesc><table><row><cell cols="3">F00 F01 F02 F03 ? I00 I01 I02 I03 ? O00 O01 O02 O03 ?</cell><cell>? ? ?</cell><cell>X</cell><cell cols="4">X30 X31 X32 X33 X00 X01 X02 X03 X10 X11 X12 X13 X20 X21 X22 X23</cell><cell>+</cell><cell cols="2">? Fh0 ? Oh0</cell><cell cols="3">Filters X ? ? Ih0 ? ? Ch0</cell><cell>? ?</cell></row><row><cell cols="3">Filters C00 C01 C02 C03 ?</cell><cell>?</cell><cell></cell><cell cols="4">Input Activation</cell><cell></cell><cell cols="6">Output Activation Hp0 Hp1 Hp2 Hp3</cell></row><row><cell cols="4">H0 H1 H2 H3 Output Activation =</cell><cell></cell><cell></cell><cell cols="10">(Prev) RNN(LSTM) Computation</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="8">To/From Prefetch Buffer</cell></row><row><cell></cell><cell></cell><cell cols="2">VN 0</cell><cell></cell><cell></cell><cell></cell><cell>+</cell><cell></cell><cell cols="2">VN 1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">+</cell><cell></cell><cell>f0</cell><cell></cell><cell></cell><cell></cell><cell>f1</cell><cell></cell><cell></cell><cell cols="2">+</cell><cell></cell></row><row><cell>+</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">+</cell><cell></cell><cell></cell><cell>+</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>+</cell></row><row><cell>+</cell><cell cols="2">+</cell><cell></cell><cell cols="2">+</cell><cell></cell><cell>+</cell><cell>+</cell><cell></cell><cell cols="2">+</cell><cell></cell><cell>+</cell><cell></cell><cell>+</cell></row><row><cell cols="5">F00 F01 F02 F03</cell><cell>Hp0</cell><cell>Bf</cell><cell cols="4">F10 F11 F12 F13</cell><cell cols="2">Hp1</cell><cell>Bf</cell><cell cols="2">Mult. valA</cell></row><row><cell cols="6">X00 X10 X20 X30 1</cell><cell cols="6">1 X01 X11 X21 X31 1</cell><cell></cell><cell>1</cell><cell cols="2">Mult. valB</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="9">(1) Gate Value Computation</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="8">To/From Prefetch Buffer</cell></row><row><cell></cell><cell></cell><cell cols="2">VN 0</cell><cell></cell><cell></cell><cell></cell><cell>+</cell><cell></cell><cell cols="2">VN 1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">+</cell><cell></cell><cell>t0</cell><cell></cell><cell></cell><cell></cell><cell>t1</cell><cell></cell><cell></cell><cell cols="2">+</cell><cell></cell></row><row><cell>+</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">+</cell><cell></cell><cell></cell><cell>+</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>+</cell></row><row><cell>+</cell><cell cols="2">+</cell><cell></cell><cell cols="2">+</cell><cell></cell><cell>+</cell><cell>+</cell><cell></cell><cell cols="2">+</cell><cell></cell><cell>+</cell><cell></cell><cell>+</cell></row><row><cell cols="5">C00 C01 C02 C03</cell><cell>Hp0</cell><cell>Bc</cell><cell cols="4">C10 C11 C12 C13</cell><cell cols="2">Hp1</cell><cell>Bc</cell><cell cols="2">Mult. valA</cell></row><row><cell cols="6">X00 X10 X20 X30 1</cell><cell cols="6">1 X01 X11 X21 X31 1</cell><cell></cell><cell>1</cell><cell cols="2">Mult. valB</cell></row><row><cell></cell><cell></cell><cell cols="11">(2) Input Transform Computation</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="8">To/From Prefetch Buffer</cell></row><row><cell cols="2">VN 0</cell><cell cols="2">VN 1</cell><cell></cell><cell cols="2">VN 2</cell><cell cols="2">VN 3 +</cell><cell cols="2">VN 4</cell><cell cols="3">VN 5</cell><cell cols="2">VN 6</cell></row><row><cell></cell><cell>s0</cell><cell cols="2">+</cell><cell></cell><cell>s2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>s4</cell><cell></cell><cell cols="2">+</cell><cell>s6</cell></row><row><cell>+</cell><cell>s1</cell><cell></cell><cell></cell><cell></cell><cell cols="2">+</cell><cell>s3</cell><cell></cell><cell>+</cell><cell>s5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>+</cell></row><row><cell>+</cell><cell cols="2">+</cell><cell></cell><cell cols="2">+</cell><cell></cell><cell>+</cell><cell>+</cell><cell></cell><cell cols="2">+</cell><cell></cell><cell>+</cell><cell></cell><cell>+</cell></row><row><cell>f0</cell><cell>i0</cell><cell>f1</cell><cell>i1</cell><cell></cell><cell>f2</cell><cell>i2</cell><cell>f3</cell><cell>i3</cell><cell>f4</cell><cell>i4</cell><cell>f5</cell><cell></cell><cell>i5</cell><cell cols="2">f6</cell><cell>i6</cell><cell>A</cell></row><row><cell cols="2">sp0 t0</cell><cell cols="2">sp1 t1</cell><cell></cell><cell cols="2">sp2 t2</cell><cell cols="2">sp3 t3</cell><cell cols="2">sp4 t4</cell><cell cols="3">sp5 t5</cell><cell cols="2">sp6 t6</cell><cell>B</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="9">(3) State Value Computation</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="8">To/From Prefetch Buffer</cell></row><row><cell cols="2">VN 0</cell><cell cols="2">VN 1</cell><cell></cell><cell cols="2">VN 2</cell><cell>VN 3 +</cell><cell></cell><cell>VN 4</cell><cell></cell><cell cols="3">VN 5</cell><cell cols="2">VN 6</cell></row><row><cell cols="2">H0</cell><cell cols="2">+</cell><cell cols="2">H2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>H4</cell><cell></cell><cell>+</cell><cell cols="2">H6</cell></row><row><cell>+</cell><cell>H1</cell><cell></cell><cell></cell><cell></cell><cell>+</cell><cell cols="2">H3</cell><cell></cell><cell>+</cell><cell>H5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>+</cell></row><row><cell>+</cell><cell>+</cell><cell></cell><cell></cell><cell cols="2">+</cell><cell cols="2">+</cell><cell>+</cell><cell></cell><cell>+</cell><cell></cell><cell></cell><cell>+</cell><cell></cell><cell>+</cell></row><row><cell>o0</cell><cell cols="2">o1</cell><cell></cell><cell></cell><cell>o2</cell><cell></cell><cell>o3</cell><cell cols="2">o4</cell><cell></cell><cell>o5</cell><cell></cell><cell></cell><cell>o6</cell><cell>A</cell></row><row><cell>sa0</cell><cell cols="2">sa1</cell><cell></cell><cell></cell><cell>sa2</cell><cell></cell><cell>sa3</cell><cell cols="2">sa4</cell><cell cols="2">sa5</cell><cell></cell><cell></cell><cell cols="2">sa6</cell><cell>B</cell></row><row><cell></cell><cell cols="12">(4) Output Activation Computation</cell><cell></cell><cell></cell></row><row><cell>Figure 9.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>LSTM RNN computation in Maeri. F, I, O, and C indicate weights for forget/intput/output gated and input transform multiplied with input activations. F h , I h , O h</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Figure 10. Mapping POOL, FC, Cross-Layer and Sparse CNNs over Maeri.Step 1 and 2 require the same number of MSes within a neuron and share the same input activation data set; thus we merge them in Maeri. In other words, when each VN receives input activations, it reuses them to calculate all the gate values (step 1) and input transforms (step 2) for the received input activation, before the PB distributes the next round of input activation. The computed gate values are collected by the PB (over the ART) and stored.</figDesc><table><row><cell cols="4">VN 0 To/From Prefetch Buffer VN 1 &gt;</cell><cell>&gt;</cell><cell>VN 2</cell><cell>&gt;</cell><cell>VN</cell><cell>+</cell><cell>+</cell><cell>?</cell><cell></cell><cell>VN 0</cell><cell>+ VN 1</cell><cell>VN 2</cell><cell>+</cell><cell></cell><cell>VN 3</cell><cell>VN 0</cell><cell cols="3">VN 1 + To/From Prefetch Buffer VN 2 +</cell><cell></cell><cell>VN 3</cell><cell>+</cell></row><row><cell>&gt;</cell><cell></cell><cell>&gt;</cell><cell></cell><cell>&gt;</cell><cell></cell><cell></cell><cell>+</cell><cell>+</cell><cell></cell><cell>+</cell><cell></cell><cell>+</cell><cell></cell><cell>&gt;</cell><cell></cell><cell>+</cell><cell></cell><cell></cell><cell>+</cell><cell>+</cell><cell></cell><cell>+</cell><cell></cell></row><row><cell>&gt;</cell><cell>&gt;</cell><cell>&gt;</cell><cell>&gt;</cell><cell>&gt;</cell><cell>&gt;</cell><cell>+</cell><cell>+</cell><cell>+</cell><cell>+</cell><cell>+</cell><cell>+</cell><cell>+</cell><cell>+</cell><cell>&gt;</cell><cell>&gt;</cell><cell>&gt;</cell><cell>+</cell><cell>+</cell><cell>+</cell><cell>+</cell><cell>+</cell><cell>+</cell><cell>+</cell></row><row><cell>?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>?</cell></row><row><cell></cell><cell></cell><cell cols="3">(1) Pooling Layer</cell><cell></cell><cell></cell><cell cols="4">(2) Fully-Connected Layer</cell><cell></cell><cell></cell><cell cols="3">(3) Hybrid Mapping</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">(4) Sparse Convolution</cell><cell></cell></row><row><cell cols="12">Gate values and input transform calculation. For step</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="12">1 and 2, Maeri first constructs VNs. Maeri distributes input</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="12">activations and weights, and performs the same multiply-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="12">accumulate computation as the CONV example. However,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="12">unlike the CONV case, it iterates four weight filters (for-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="12">get/input/output gate and input transform) and reuses input</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="12">activation values for each gate value and input transform</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">computation.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p><p><p>State value and output activation calculation. After the completion of step 1 and 2 over target input activations (X), Maeri reconstructs VNs to calculate state and output activation values, as Figure</p>9</p>-(3) and (4) show. The reason for reconstructing VNs is because the calculations in step 3 and 4 requires fewer number of multiplier switches; Retaining the same VN configurations as step 1 and 2 would lead to underutilization of the available multipliers. However, sometimes reconstructing VNs between step 3 and 4 may not help if the VNs are too fine-grained (say each VN just has two multiplier switches), since the ART might not have enough bandwidth to compute and transmit all output activations to the root every cycle. Our optimization tool considers this aspect and generates appropriate parameters for the ART controller so Maeri prevents such a contention scenario.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 3 .</head><label>3</label><figDesc>Maeri implementation details and comparison with Eyeriss and Systolic Array. SysArray/MAERI (Comp) and SysArray/-MAERI (Area) are Systolic array/MAERI implementations that have the same number of compute units and area as Eyeriss respectively.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Maeri is a Korean word that means echo. We selected this name because a DNN accelerator's dataflow involves sending input data toward computation units and receiving a weighted-accumulation (echo) back.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Deep speech 2: End-to-end speech recognition in english and mandarin</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fougner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vaino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Johannes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Raiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Seetapun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.02595</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Diannao: A small-footprint high-throughput accelerator for ubiquitous machine-learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Temam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="269" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Dadiannao: A machine-learning supercomputer</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Temam</surname></persName>
		</author>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="609" to="622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Shidiannao: Shifting vision processing closer to the sensor</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fasthuber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ienne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Temam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Eyeriss: An energyefficient reconfigurable accelerator for deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Solid-State Circuits</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="127" to="138" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Eie: efficient inference engine on compressed deep neural network</title>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pedram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Scnn: An accelerator for compressed-sparse convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Parashar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mukkara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Puglielli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Venkatesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Khailany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Keckler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th Annual International Symposium on Computer Architecture</title>
		<meeting>the 44th Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="27" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">O</forename><surname>Arik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chrzanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gibiansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Raiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shoeybi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.07825</idno>
		<title level="m">Deep voice: Real-time neural text-to-speech</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Diannao: A small-footprint high-throughput accelerator for ubiquitous machine-learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Temam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Sigplan Notices</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="269" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dadiannao: A machine-learning supercomputer</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Temam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 47th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 47th Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="609" to="622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fused-layer CNN accelerators</title>
		<author>
			<persName><forename type="first">M</forename><surname>Alwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Milder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">49th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">C-brain: A deep learning accelerator that tames the diversity of cnns through adaptive data-level parallelization</title>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DAC</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep convolutional neural network architecture with reconfigurable computation patterns</title>
		<author>
			<persName><forename type="first">F</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Very Large Scale Integration</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>VLSI) Systems</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cnvlutin: Ineffectual-neuron-free deep neural network computing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Albericio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hetherington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aamodt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">E</forename><surname>Jerger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moshovos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Cambricon-x: An accelerator for sparse neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Optimizing fpga-based accelerator design for deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FPGA</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="161" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Flexflow: A flexible dataflow accelerator architecture for convolutional neural networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bajwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Boden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Borchers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Boyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Cantin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Coriell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Daley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gelb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">V</forename><surname>Ghaemmaghami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gottipati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gulland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hagmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hogberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hundt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hurt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaffey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaworski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Khaitan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Killebrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Laudon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lundin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mackean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Maggiore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mahony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Narayanaswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Norrie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Omernick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Penukonda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Phelps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Salek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Samadiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Severn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sizikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Snelham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Souter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Steinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Swing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Thorson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Toma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tuttle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wilcox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Yoon</surname></persName>
		</author>
		<title level="m">Proceedings of the 44th Annual International Symposium on Computer Architecture (ISCA)</title>
		<meeting>the 44th Annual International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>datacenter performance analysis of a tensor processing unit</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Eyeriss: A spatial architecture for energy-efficient dataflow for convolutional neural networks</title>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="367" to="379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Minimizing computation in convolutional neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="281" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Theano: A Python framework for fast computation of mathematical expressions</title>
		<author>
			<persName><forename type="first">Theano</forename><surname>Development</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Team</forename></persName>
		</author>
		<idno>abs/1605.02688</idno>
		<imprint>
			<date type="published" when="2016-05">May 2016</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>Software available from tensorflow.org</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Torch7: A matlab-like environment for machine learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BigLearn, NIPS Workshop</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Optimizing loop operation and dataflow in fpga acceleration of deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vrudhula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-S</forename><surname>Seo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays</title>
		<meeting>the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="45" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Rethinking nocs for spatial neural network accelerators</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Samajdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NOCS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Flexflow: A flexible dataflow accelerator architecture for convolutional neural networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">High Performance Computer Architecture (HPCA), 2017 IEEE International Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="553" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Optimizing fpga-based accelerator design for deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays</title>
		<meeting>the 2015 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="161" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fused-layer cnn accelerators</title>
		<author>
			<persName><forename type="first">M</forename><surname>Alwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Milder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Microarchitecture (MICRO), 2016 49th Annual IEEE/ACM International Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Dnnweaver: From high-level deep network models to fpga acceleration</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Amaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Thwaites</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kotha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Esmaeilzadeh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>in the Workshop on Cognitive Architectures</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Jupiter rising: A decade of clos topologies and centralized control in google&apos;s datacenter network</title>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Armistead</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bannon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Boving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Felderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Germano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kanagala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Provost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Simmons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wanderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Holzle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stuart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGCOMM Computer Communication Review</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="183" to="197" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Bluespec system verilog: efficient, correct rtl from high level specifications</title>
		<author>
			<persName><forename type="first">R</forename><surname>Nikhil</surname></persName>
		</author>
		<editor>MEMOCODE</editor>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="69" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Network-on-chip architectures for neural networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Vainbrand</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="135" to="144" />
		</imprint>
		<respStmt>
			<orgName>NOCS</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Reconfigurable platforms and the challenges for large-scale implementations of spiking neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Harkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Field Programmable Logic and Applications</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="page" from="483" to="486" />
		</imprint>
	</monogr>
	<note>FPL 2008. International Conference on</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A generic reconfigurable neural network architecture implemented as a network on chip</title>
		<author>
			<persName><forename type="first">T</forename><surname>Theocharides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SOC</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Connection-centric network for spiking neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Emery</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="144" to="152" />
		</imprint>
		<respStmt>
			<orgName>NOCS</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Shidiannao: Shifting vision processing closer to the sensor</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fasthuber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ienne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Temam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="92" to="104" />
			<date type="published" when="2015">2015</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Convolution engine: balancing efficiency &amp; flexibility in specialized computing</title>
		<author>
			<persName><forename type="first">W</forename><surname>Qadeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hameed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Shacham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Venkatesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Horowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="24" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Neutrams: Neural network transformation and co-design under neuromorphic hardware constraints</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Aladdin: A pre-rtl, power-performance accelerator simulator enabling large design space exploration of customized architectures</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Reagen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Brooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="97" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Cnnlab: a novel parallel framework for neural networks using gpu and fpga-a practical study with trade-off analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.06234</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Maximizing CNN accelerator efficiency through resource partitioning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Milder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">44th International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Finding structure in time</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="211" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Serial order: A parallel distributed processing approach</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in psychology</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="page" from="471" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning task-dependent distributed representations by backpropagation through structure</title>
		<author>
			<persName><forename type="first">C</forename><surname>Goller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kuchler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Neural Networks</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="347" to="352" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">X M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Martini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05552</idno>
		<title level="m">Recurrent neural networks hardware implementation on fpga</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Fpga-based accelerator for long short-term memory recurrent neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASP-DAC</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="629" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Fpga acceleration of recurrent neural network based language model</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FCCM</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="111" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Fpga-based low-power speech recognition with recurrent neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SiPS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="230" to="235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Ese: Efficient speech recognition engine with sparse lstm on fpga</title>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FPGA</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="75" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Neural networks and physical systems with emergent collective computational abilities</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Hopfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the national academy of sciences</title>
		<meeting>the national academy of sciences</meeting>
		<imprint>
			<date type="published" when="1982">1982</date>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="2554" to="2558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Simultaneous perturbation learning rule for recurrent neural networks and its fpga implementation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1664" to="1672" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Transforming the lstm training algorithm for efficient fpga-based adaptive control of nonlinear dynamic systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tavcar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dedic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bokal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zemva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Informacije Midem-Journal of Microelectronics Electronic Components and Materials</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="131" to="138" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Dynamic approximation with feedback control for energy-efficient recurrent neural network hardware</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mukhopadhyay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISLPED</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="168" to="173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">14.2 dnpu: An 8.1 tops/w reconfigurable cnn-rnn processor for general-purpose deep neural networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Yoo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="240" to="241" />
		</imprint>
		<respStmt>
			<orgName>ISSCC</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
