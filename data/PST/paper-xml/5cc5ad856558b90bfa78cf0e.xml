<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Spectrogram based multi-task audio classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yuni</forename><surname>Zeng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="laboratory">Machine Intelligence Laboratory</orgName>
								<orgName type="institution">Sichuan University</orgName>
								<address>
									<postCode>610065</postCode>
									<settlement>Chengdu</settlement>
									<country>People&apos;s Republic of China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Hua</forename><surname>Mao</surname></persName>
							<email>huamao@scu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="laboratory">Machine Intelligence Laboratory</orgName>
								<orgName type="institution">Sichuan University</orgName>
								<address>
									<postCode>610065</postCode>
									<settlement>Chengdu</settlement>
									<country>People&apos;s Republic of China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="laboratory">Machine Intelligence Laboratory</orgName>
								<orgName type="institution">Sichuan University</orgName>
								<address>
									<postCode>610065</postCode>
									<settlement>Chengdu</settlement>
									<country>People&apos;s Republic of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dezhong</forename><surname>Peng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="laboratory">Machine Intelligence Laboratory</orgName>
								<orgName type="institution">Sichuan University</orgName>
								<address>
									<postCode>610065</postCode>
									<settlement>Chengdu</settlement>
									<country>People&apos;s Republic of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhang</forename><surname>Yi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="laboratory">Machine Intelligence Laboratory</orgName>
								<orgName type="institution">Sichuan University</orgName>
								<address>
									<postCode>610065</postCode>
									<settlement>Chengdu</settlement>
									<country>People&apos;s Republic of China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Spectrogram based multi-task audio classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">17DB884A501063AD12E4702325351C89</idno>
					<idno type="DOI">10.1007/s11042-017-5539-3</idno>
					<note type="submission">Received: 22 September 2017 / Revised: 30 November 2017 / Accepted: 14 December 2017</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T04:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Multi-task learning</term>
					<term>Convolutional neural networks</term>
					<term>Deep residual networks</term>
					<term>Audio classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Audio classification is regarded as a great challenge in pattern recognition. Although audio classification tasks are always treated as independent tasks, tasks are essentially related to each other such as speakers' accent and speakers' identification. In this paper, we propose a Deep Neural Network (DNN)-based multi-task model that exploits such relationships and deals with multiple audio classification tasks simultaneously. We term our model as the gated Residual Networks (GResNets) model since it integrates Deep Residual Networks (ResNets) with a gate mechanism, which extract better representations between tasks compared with Convolutional Neural Networks (CNNs). Specifically, two multiplied convolutional layers are used to replace two feed-forward convolution layers in the ResNets. We tested our model on multiple audio classification tasks and found that our multi-task model achieves higher accuracy than task-specific models which train the models separately.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sound provides us with rich information about its producer and environment. As the human auditory system is able to segregate and identify complex sounds, we can imagine that a machine that can perform similar functions would be very useful in applications such as speech recognition in noisy backgrounds <ref type="bibr" target="#b27">[28]</ref>. Audio classification is an important aspect of pattern recognition and has been widely used in professional media applications and entertainment. Different audio classification tasks-such as speech-music discrimination, audio emotion recognition, accent recognition, and music retrieving information-have driven successful applications in recent years <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b27">28]</ref>.</p><p>Most methods of audio classification treat tasks separately such as accent classification <ref type="bibr" target="#b12">[13]</ref>, emotion recognition <ref type="bibr" target="#b4">[5]</ref>, speaker identification <ref type="bibr" target="#b21">[22]</ref> and so on. However, some tasks are closely related. For example, while accent recognition and speaker identification are always regarded as two individual classification tasks, in most situations when the speaker is confirmed, the accent is determined and unchangeable. In this case, accent recognition and speaker identification are related. Relational information of Alzheimer's disease is exploited for feature selection to improve classification accuracy in <ref type="bibr" target="#b34">[35]</ref>. Here, we sought to use relationship to simultaneously predict different tasks, and hypothesized that this would increase classification accuracy.</p><p>Multi-task Learning (MTL) is a subfield of machine learning in which multiple tasks are solved simultaneously. Thus, MTL can exploit the intrinsic relationships among related tasks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref>. The goal of MTL is to improve generalization performance by training all tasks at same time from a shared representation. Because of the training way of MTL, tasks can benefit from what are learned for other tasks <ref type="bibr" target="#b3">[4]</ref>. Borrowing the idea from MTL, we designed a multi-task model for audio classification tasks. Our model exploits the relationships found in audio and deals with multiple audio classification tasks jointly. We find out that our model help to improve the accuracy of tasks comparing with the state-of-the-art result obtained by task-specific models <ref type="bibr" target="#b28">[29]</ref>.</p><p>The inputs to our model are audio spectrograms, the visual representations of audio. Spectrograms are very detailed and accurate images of audio that have been widely used in audio classification tasks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b27">28]</ref>. Deep Neural Networks (DNNs) are very good at abstracting data, and have been used with great success in fields such as speech recognition <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12]</ref> and image recognition <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b29">30]</ref>. Specifically, we focus on Convolutional Neural Networks (CNNs) in this paper. CNNs can efficiently exploit invariance presented in spectrogram <ref type="bibr" target="#b26">[27]</ref> for their convolutional and pooling operations.</p><p>Thus, we propose a new CNNs-based model for learning a shared representation among all the spectrograms from different classification tasks. Called the Gated Residual Neural Networks (GResNets) model, it is a variation of a deep Residual Networks (ResNets) model with the addition of a gate mechanism. Because of the notorious gradient vanishing/exploding problem in learning DNNs, ResNets use the linear connection of Long Short-Term Memory (LSTM) <ref type="bibr" target="#b15">[16]</ref> to ease the training of very deep CNNs <ref type="bibr" target="#b14">[15]</ref>. Another way for the LSTM to solve this problem is to use the gate mechanism <ref type="bibr" target="#b15">[16]</ref>. In our GRes-Nets, we combine both mechanisms to get better representations. Because of the strong ability to learn invariance presented in spectrograms <ref type="bibr" target="#b26">[27]</ref>, CNNs are used as the component of our GResNets like ResNets. Finally, the abstracted features are used for audio classification in the last softmax layer. The experimental results demonstrated that multi-task models for related audio classification tasks outperform the task-specific models of each task.</p><p>The rest of this paper is organized as follows. We begin with related works about audio classification in Section 2. Next, we review the MTL, spectrogram, ResNets and gate mechanism in Section 3. Section 4 introduce the structure of our proposed model. Then, in Section 5 we use the proposed model for different multiple audio classification tasks. Finally, we conclude our work in Section 6.</p><p>Many methods in machine learning are adapted for the single audio classification task. Support Vector Machine (SVM) was used to classify two English accents <ref type="bibr" target="#b24">[25]</ref> and eight emotions with continuous wavelet-transform features <ref type="bibr" target="#b28">[29]</ref>, respectively. In another study, a Gaussian Mixture Model (GMM) was trained for an accent-classification task by fusing several acoustic and text-language subsystems <ref type="bibr" target="#b12">[13]</ref>. In <ref type="bibr" target="#b4">[5]</ref>, SVM, principal component analysis and artificial neural network were combined to classify six emotions from speech.</p><p>Deep learning methods have also been widely used for audio classification tasks. A CNNs-based neural network called convolutional deep belief network was developed for several individual audio classification tasks, such as speaker identification, speaker gender classification, and music genre classification <ref type="bibr" target="#b20">[21]</ref>. In <ref type="bibr" target="#b10">[11]</ref>, a CNNs based model was trained from fixed-length spectrogram features and single-label audio data for various predominant instrument recognition in polyphonic music. In <ref type="bibr" target="#b21">[22]</ref>, a CNNs and Gated Recurrent Unit (GRU)-based neural network was proposed for speaker identification and verification. Additionally, LSTM was used in a hybrid emotion inference model that was proposed for inferring user emotion in a real-world voice-dialogue application, and a recurrent autoencoder was proposed to pre-train the LSTM to improve accuracy <ref type="bibr" target="#b31">[32]</ref>. Further, GMM and DNNs were combined to identify distant accents in reverberant environments <ref type="bibr" target="#b25">[26]</ref>. The authors found that this combination of classifiers outperformed the individual GMM and DNNs classifiers.</p><p>All the methods above focus on the single audio classification task. However, few studies have focused on the multiple audio classification tasks through MTL method. Two reports concentrated on emotion recognition from spoken language and song at the same time <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b32">33]</ref>. Especially <ref type="bibr" target="#b32">[33]</ref> showed that MTL-SVM models have significantly better performance for audio emotion recognition than task-specific SVM models. Additionally, the authors have also used an MTL approach on SVM for cross-corpus acoustic recognition of emotion in speaking and singingk <ref type="bibr" target="#b33">[34]</ref>.</p><p>Many recent deep learning approaches also have used MTL. Deep Relationship Networks <ref type="bibr" target="#b23">[24]</ref> were proposed to learn the relationshio between tasks. The model shares convolutional layers, while learning task-specific fully-connected layers. <ref type="bibr" target="#b13">[14]</ref> introduced a joint many-task model to solve complex task in the field of nature language processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminaries</head><p>Our multi-task model abstract the shared features from spectrograms and GResNets is a variation of CNN-based ResNets. Thus, here we review basic information related to MTL, spectrograms, ResNets and gate mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Multi-task learning</head><p>MTL is a scope of machine learning where multiple learning tasks are solved at the same times. The goal of MTL is to improve the prediction accuracy of multiple classification tasks by learning them jointly. Models which combine neural network method with MTL use a shared hidden layers on all tasks <ref type="bibr" target="#b3">[4]</ref>.</p><p>Mathematically, given a data set</p><formula xml:id="formula_0">S = {(x 1 , Y 1 ), • • • , (x N , Y N )}</formula><p>where N is the number of sample, x i denotes the i th sample and</p><formula xml:id="formula_1">Y i = (y 1 i , y 2 i • • • , y T i )</formula><p>represents the set of labels. Let N T indicate the set of tasks and T be the number of tasks. For each task t ∈ N T , we select p samples where p &lt; N samples from audio set as the trainset {(x 1 , y t 1 ), • • • , (x p , y t p )}. The MTL aims to learn a fitted model F to get accurate labels for each input:</p><formula xml:id="formula_2">(y 1 i , y 2 i , ..., y T i ) = F (θ, x i ),<label>(1)</label></formula><p>where i ≤ p. θ denotes the parameters of the multi-task model. We assume that the cost function is J (•), and the parameters of the multi-task model are learned by minimizing the following formula:</p><formula xml:id="formula_3">T t J (x t , Y t , θ). (<label>2</label></formula><formula xml:id="formula_4">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Spectrogram</head><p>A spectrogram is regarded as a very detailed and accurate representation of audio information. A common spectrogram is an image where one axis represents time, the other axis is frequency and the color of each point indicates the amplitude of those points. Thus, a spectrogram shows amplitude changes for every frequency component in the signal. Figure <ref type="figure" target="#fig_1">1</ref> gives an example of audio spectrograms that contain different emotions. From the spectrograms, we can observe that the amplitude of happy and angry emotions diverge in the 5000H z to 15000H z frequency range.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Deep residual networks</head><p>Compared fully-connected neural networks, CNNs <ref type="bibr" target="#b19">[20]</ref> are also designed to recognize pattern directly from pixel images but with less parameters. CNNs have led to a series of significant achievements in audio classification. Using sparse connectivity and shared weights ease network optimization by reducing the number of parameters and the risk of overfitting <ref type="bibr" target="#b17">[18]</ref>. The basic layers of CNNs are convolutional layer and pooling layer. Figure <ref type="figure" target="#fig_2">2</ref> shows a typical architecture for CNNs. A convolutional layer is composed of several kernels and aims to get the feature maps <ref type="bibr" target="#b9">[10]</ref>. To generate output feature maps, convolutional layer input maps are convoluted with learnable kernels and the results are transformed by a nonlinear activation function.</p><p>Suppose l denotes the l th layer of CNNs and k l denotes the parameters of the kernel and b l is the bias parameters at the l th layer. The input and output of the l th layer are  defined by a l-1 and a l respectively. f (•) denotes an activation function. The convolutional operation <ref type="bibr" target="#b2">[3]</ref> can be formulated as</p><formula xml:id="formula_5">a l j = f ( i∈M j a l-1 i * k l ij + b l j ),<label>(3)</label></formula><p>where M j denotes a selection of input maps. The convolutional operation is not only widely used in CNNs but also in sparse coding <ref type="bibr" target="#b35">[36]</ref>.</p><p>A pooling layer is usually placed after the convolutional layer to achieve invariant representation and reduces the number of parameters <ref type="bibr" target="#b7">[8]</ref>. The number of feature maps for a pooling layer is the same as the number of feature maps in the previous convolutional layer. There are two typical pooling operations: average pooling and max pooling <ref type="bibr" target="#b1">[2]</ref>. The computation of the pooling layer is formulated as follows</p><formula xml:id="formula_6">a l j = f (β l j down(a l-1 j ) + b l j ), (<label>4</label></formula><formula xml:id="formula_7">)</formula><p>where down(•) represents a sub-sampling function. and β is the multiplicative bias that is given to one output map <ref type="bibr" target="#b2">[3]</ref>.</p><p>ResNets was the winning model of the ILSVRC 2015 ImageNet challenge and has been proposed to ease the training of very deep CNNs <ref type="bibr" target="#b14">[15]</ref>. Deep ResNets are composed of several stacked residual blocks. Figure <ref type="figure">3</ref> illustrates a residual block where weight layers denote convolutional layers and a shortcut connection is used for identifying mapping.</p><p>For a block of ResNets, let the input for the blocks be x,</p><formula xml:id="formula_8">y = F (x, W ) + x, (<label>5</label></formula><formula xml:id="formula_9">)</formula><p>where y is the output of the building block, W indicates the weights of the residual block and the function F (•) is the output of two convolutional layers. The dimensionality of x and F (x, W ) must be the same for (5) to be valid. To match dimensions, the general solution is to use a linear projection weight matrix W s as:</p><formula xml:id="formula_10">y = F (x, W ) + W s x. (<label>6</label></formula><formula xml:id="formula_11">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Gate mechanism</head><p>The limitation by using the deep neural networks is the gradient vanishing/exploding problem. The gate mechanism proposed in LSTM <ref type="bibr" target="#b15">[16]</ref> can be viewed as the milestone for solving the gradient problem. The input/output gates are used to control how much information should be kept in the cell. The forget gate is used to control the values kept in the Figure <ref type="figure">4</ref> illustrates the gate mechanism intuitively. The gate mechanism can be formulated as y = f 1 • f 2 where f 1 and f 2 denote the activation function and gate function, respectively. • is the element-wise multiplication. If f 2 = 0, the gate is fully closed and if f 2 = 1, the gate is fully opened.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The proposed model</head><p>We proposed a new CNN-based architecture to extract the shared feature of all tasks. The details are described in this section. We begin by describing the fundamental block of GResNets and then introduce our proposed multi-task model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The gated residual networks blocks</head><p>The gradient vanishing/exploding problem is a known limitation for deep learning. LSTM <ref type="bibr" target="#b15">[16]</ref> first proposed to solve this problem by combining a gate mechanism with linear Fig. <ref type="figure">4</ref> The illustration of a gate connections. ResNets were proposed to train very deep feedforward neural networks by appending linear connections to solve the gradient vanishing/exploding problem, but without a gate mechanism. <ref type="bibr" target="#b14">[15]</ref>.</p><p>Inspired by these two mechanisms, we propose GResNets, which is composed of the basic blocks shown in Fig. <ref type="figure" target="#fig_3">5</ref>. The weight layers F 1 (•), F 2 (•) indicate convolutional layers, each of which is followed by a normalization method <ref type="bibr" target="#b16">[17]</ref>. Let x be the input of a block. We define * as element-wise multiplication. Thus, the output y of one block can be formulated as</p><formula xml:id="formula_12">y = F 1 (x, W 1 ) * F 2 (x, W 2 ) + x, (<label>7</label></formula><formula xml:id="formula_13">)</formula><p>where W 1 and W 2 indicate the weights of the convolutional layers. Usually, the activation functions in convolutional layers are the sigmoid function, the tanh function, or the ReLu function. We compare different activation functions in Section 5.3. Like ResNets, we also use a linear projection weight M to match the dimensions of F 1 (•) * F 2 (•) and x:</p><formula xml:id="formula_14">y = F 1 (x, W 1 ) * F 2 (x, W 2 ) + Mx. (<label>8</label></formula><formula xml:id="formula_15">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The proposed model for audio classification</head><p>The basic idea of MTL is to share parameters between related tasks. Our multi-task model is a neural network with different number of the softmax classifiers. Let N T indicate the set of tasks and T be the number of tasks. The classification layer of the multi-task model includes T softmax classifiers. For task-specific model, T = 1. Figure <ref type="figure">6</ref> illustrates an example of the MTL model which is used for two audio classification tasks. The first convolution and pooling layers are used to reduce the dimension of the input spectrograms and the number of parameters. The next several GResNets blocks and a full connected layer are stacked to get the shared representation between the two tasks. Then, the extracted features are used in the softmax layer to generate predictions for each task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>To evaluate our GResNets and the MTL method could improve the accuracy of each tasks, we designed experiments on tasks with different relationship. In the first part of experiments, the GResNets model was used for two tasks of The Ryerson audio-visual database of Fig. <ref type="figure">6</ref> The structure of our proposed model emotional speech and song (RAVDESS) <ref type="bibr" target="#b22">[23]</ref>. One task is to identify eight kinds of emotions, another one is to discriminate song and speech. There is no direct connection between those two tasks. We regard the first task as the main task and to find whether what the second task learned would help the first task or not. We also compared the performance of the GResNets model with different numbers of blocks, activation functions, and other CNN structures.</p><p>The Voice Cloning Toolkit (VCTK) <ref type="foot" target="#foot_0">1</ref> data set was used in the second part of experiments. There also are two tasks, accent recognition and speaker identification. But those two tasks have a kind of relationship like inclusion relation. In this part of experiments, we focus on comparing the multi-task model and task-specific models and using an evaluation metric (the confusion matrix) to represent the improvement of MTL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Corpus</head><p>RAVDESS corpus RAVDESS consists of 24 actors (12 females and 12 males) speaking and singing with various emotions. The speaking set contains eight emotional expressions: neutral, calm, happy, sad, angry, fearful, disgust and surprised. The singing set includes six emotions: neutral, calm, happy, sad, angry, and fearful. All emotions (except neutral) are expressed at normal and strong levels of intensity. All audio files in this corpus are encoded as 16 bits, 48 kHz wav files. We use the MIRtoolbox <ref type="bibr" target="#b18">[19]</ref> to generate 257 × 399 pixel spectrograms for each audio file. The RAVDESS dataset originally contains speech and song as two separated categories, so we mixed all audio while the first experiment performs to classify them. Figure <ref type="figure">7</ref> shows the spectrograms from the audio set for the eight categories of emotion in speech.  English, Indian, Irish, NewZealand, Northern Irish, Scottish, South African and Welsh accents. All speech data in the VCTK was recorded at a sampling of 96 kHz and at 24 bits. All recordings were converted into 16 bits, down-sampled to 48 kHz based on STPK, and manually end-pointed. We generated 257 × 399 pixels spectrograms for each sentence. Figure <ref type="figure">8</ref>, spectrogram features of American, Canadian, English, Irish, Northern Irish, and Scottish accents. Figure <ref type="figure" target="#fig_6">9</ref> illustrates the spectrograms for six speakers who have the same accent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental settings</head><p>We extracted the spectrogram features with a 512-length hamming window with 50% overlap for each audio. The experiment was performed on an MXNet framework <ref type="bibr" target="#b5">[6]</ref> with NVIDIA Tesla K40m GPU. The parameters of our multi-task model are illustrated in Fig <ref type="figure" target="#fig_1">10</ref>.</p><p>The underline parameters are changeable in different experiments. l denotes the number of GResNets blocks. n, m, p, q, and num f ilters are the CNNs parameters. n × m, p × q are the kernel sizes. num f ilters represents the number of feature maps and t is the number of tasks. The learning rate is 0.001 during training and the cost function is cross-entropy loss.</p><p>To measure performance, we used two evaluation metrics. The first one was Unweighted Average Recall (UAR) <ref type="bibr" target="#b12">[13]</ref>. In the binary class case ('Y' and 'NY'), it is defined as:</p><formula xml:id="formula_16">UAR = Recall(Y ) + Recall(NY ) 2<label>(9)</label></formula><p>where Recall(•) is the recall result of the class. The second metric was accuracy (Acc). k-fold cross-validation was used for estimating model. In k-fold cross-validation, the data set is randomly divided into k equal folds. For the k subsets, a single subset is retained for use in validation and the other subsets are used as training data. The average value of the k results is the report accuracy in our paper. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experiment 1: emotion recognition in speaking and singing</head><p>We used the model for emotion recognition in speaking and singing and compared it with the results for the model in <ref type="bibr" target="#b28">[29]</ref>, which use the common sentence 'Dogs are sitting by the door' from RAVDESS corpus at the strong intensity level. There are two tasks (1) emotion recognition and (2) speech vs. song discrimination. The main task is to classify 8 categories of emotion. The parameters were: t = 2, 5-fold cross-validation, l = 7 and num f ilters was 32, 64, 64, 96, 96, 96 and 96 for each of the layers respectively. First, we compared how well task-specific and our multi-task model recognize emotions. Table <ref type="table" target="#tab_0">1</ref> shows the emotion classification results of the task-specific models and our multitask model. SVM and Continuous Wavelet Transform (CWT) features <ref type="bibr" target="#b28">[29]</ref> were used to classify eight categories of emotion and reported a maximum accuracy of 60.1%. Moreover, another baseline is provided based on a standard SVM classifier. In this experiment, n × m is 1 × 1, p × q is 3 × 3, f 1 is the tanh function and f 2 is the tanh function because of the functions used in <ref type="bibr" target="#b15">[16]</ref>. The classification accuracy for song and speech was 97.31% using the multi-task model and the UAR was 97.37.</p><p>Next, we compared the results of GResNets with different number layers l from our multi-task model when n × m was 1 × 1, p × q was 3 × 3, f 1 was the sigmoid function and  f 2 was the (4 × sigmoid -2) function. As shown in Fig. <ref type="figure" target="#fig_8">11</ref>, the best performance occurred when l = 7 in our multi-task model. Then, we compared the GResNets model with ResNets and CNNs models. To replace the GResNets blocks of our multi-task model, we used ResNets blocks and CNNs that contained the same convolutional layers. Thus, for ResNets blocks Fig. <ref type="figure">3</ref>, the first weight layer is a convolutional layer with a 1 × 1 kernel when the second layer is with a 3 × 3 kernel. Table <ref type="table" target="#tab_1">2</ref> shows the results for emotion recognition using different multi-task models.</p><p>Last, for a multi-task model based on GResNets, each GResNets block included two convolutional layers as explained in Section 4.1. We compared the different activation functions for two convolutional layers of a GResNet block. Tables <ref type="table" target="#tab_2">3</ref> and<ref type="table" target="#tab_3">4</ref> show the multi-task results in different activation functions with different kernels in convolutional layers.</p><p>From the results, following observations are obtained:</p><p>• Table <ref type="table" target="#tab_0">1</ref> shows that the multi-task model outperforms task-specific models when classifying emotion from speech and song, indicating that emotion recognition benefits from MTL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spoken and Sung Emotion Recognition</head><p>Emotion, Acc(%) Emotion, UAR(%) SS, Acc(%) SS,UAR(%)  • Table <ref type="table" target="#tab_1">2</ref> summarizes the results of the multi-task model with different CNNs structures.</p><p>The results indicate that under the same conditions, the model based on GResNets performs better than other CNN structures for emotion recognition. • Tables <ref type="table" target="#tab_2">3</ref> and<ref type="table" target="#tab_3">4</ref> show that the best recognition occurred for spoken and sung emotion when the activation function in GResNets blocks of our multi-task model was the tanh function.</p><p>We think the reason is that the value range of the tanh function is from minus to positive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Experiment 2: accent and speaker recognition</head><p>Although accent recognition and speaker identification are two related tasks, they are always regarded as two individual tasks. Here, we aimed to determine whether our multi-task model is better than using the task-specific for accent recognition and speaker identification. For this experiment, we included data from the VCTK because of huge imbalance for each accent. We trained three GResNets models: (1) a GResNets model to classify six categories of accents: American, Canadian, English, I rish, NorthernI rish, and Scottish accents;</p><p>(2) a GResNets model for identifying 37 speakers classification;</p><p>(3) a multi-task model with t = 2 and l = 4. In this experiment, n × m was 3 × 3, p × q was 3 × 3, f 1 was the sigmoid function and f 2 was the (4 × sigmoid -2) function. Their results are summarized in Table <ref type="table" target="#tab_4">5</ref>. For a clear comparison between multi-task and single-task learning, we report the confusion matrixes for accents using the task-specific and multi-task models (Tables <ref type="table" target="#tab_5">6</ref> and<ref type="table" target="#tab_6">7</ref>).</p><p>Accent and speaker recognition tasks have certain relationship that once the speaker is fixed, his or her accent is also fixed. Our model takes advantage of this relationship to learning accent and speaker recognition tasks simultaneously. The experimental results also prove this. From tables, it's obvious that the multi-task model improves accuracy from 83.05% to 88.52% in speaker identification and from 89.67% to 92.44% in accent recognition. Thus, speaker identification and accent recognition, as two individual but related audio classification tasks can benefit from MTL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper, we proposed a multi-task model for audio classification that is based on GRes-Nets and applied it to related audio classification tasks: (1) recognition emotion from speech and song; and (2) accents and speakers recognition. We evaluate our model among tasks with different relationship on multiple audio classification tasks. We found that related audio tasks recognition is in the scope of MTL and the experimental results show that recognition accuracy is better when using a multi-task model than multiple task-specific models. Thus, we perform efficient inference that different relationship audio classification tasks can benefit from MTL methods.</p><p>The model developed in this study has led to advancements in some audio classification tasks. While these algorithms have resulted in effective performance for recognizing eight emotions and six accents, we cannot make further claims regarding other related audio classification tasks. Thus, these findings should be confirmed in additional audio classification tasks in the future, such tasks with conflicts. Moreover, we are trying to give some mathematical justification about why our model works.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) angry emotion (b) happy emotion</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1</head><label>1</label><figDesc>Fig. 1 Spectrograms for happy and angry emotion. (Better viewed in color)</figDesc><graphic coords="4,45.81,475.60,345.88,107.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 Fig</head><label>2</label><figDesc>Fig. 2 One illustrative of one CNN architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 a</head><label>5</label><figDesc>Fig. 5 a block from our proposed Gated ResNets</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 Fig. 8</head><label>78</label><figDesc>Fig. 7 Spectrograms of emotion in speech VCTK corpus The Voice Cloning Toolkit (VCTK) 2 includes speech data uttered by 109 native English speakers in different accents. It includes American, Australian, Canadian,</figDesc><graphic coords="9,78.51,59.50,283.51,456.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9</head><label>9</label><figDesc>Fig. 9 Illustration spectrograms features of different speakers from American</figDesc><graphic coords="11,47.52,59.20,345.88,181.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig.</head><label></label><figDesc>Fig. Parameter settings of our multi-task model. n, m, p, q, num f ilters, t and f 1 , f 2 are changeable under different situations</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 11</head><label>11</label><figDesc>Fig. 11 Classification by using different number of feature abstract layer where 'SS' denotes speech-song discrimination</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="10,45.81,59.32,345.88,181.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Emotion recognition results of different models where '+S' denotes task-specific model and '+M' denotes that model uses for multi-task</figDesc><table><row><cell></cell><cell>Acc(%)</cell><cell>UAR(%)</cell></row><row><cell>SVM+S</cell><cell>48.01</cell><cell>48.66</cell></row><row><cell>CWT+SVM+S [29]</cell><cell>60.1</cell><cell>-</cell></row><row><cell>ResNet+S</cell><cell>53.30</cell><cell>50.33</cell></row><row><cell>GResNet+S</cell><cell>60.35</cell><cell>59.70</cell></row><row><cell>GResNet+M</cell><cell>64.48</cell><cell>64.52</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Results of emotion recognition in our multi-task model based on CNNs, ResNets and gated ResNets</figDesc><table><row><cell></cell><cell>Emotion</cell><cell></cell><cell>Speech and song</cell><cell></cell></row><row><cell></cell><cell>Acc(%)</cell><cell>UAR(%)</cell><cell>Acc(%)</cell><cell>UAR(%)</cell></row><row><cell>SVM+M</cell><cell>54.63</cell><cell>56.02</cell><cell>91.04</cell><cell>91.25</cell></row><row><cell>CNNs+M</cell><cell>53.73</cell><cell>54.8</cell><cell>92.24</cell><cell>92.08</cell></row><row><cell>ResNets+M</cell><cell>57.21</cell><cell>58.58</cell><cell>94.62</cell><cell>94.36</cell></row><row><cell>GResNets+M</cell><cell>64.48</cell><cell>64.52</cell><cell>97.31</cell><cell>97.37</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>Results for different activation functionsf 1 , f 2 (n × m is 1 × 1, p × q is 3 × 3)</figDesc><table><row><cell>Convolutional layers</cell><cell></cell><cell>Emotion</cell><cell></cell><cell>Speech and song</cell><cell></cell></row><row><cell>f 1</cell><cell>f 2</cell><cell>Acc(%)</cell><cell>UAR(%)</cell><cell>Acc(%)</cell><cell>UAR(%)</cell></row><row><cell>sigmoid</cell><cell>4*sigmoid-2</cell><cell>62.39</cell><cell>62.76</cell><cell>93.13</cell><cell>93.38</cell></row><row><cell>Sigmoid</cell><cell>tanh</cell><cell>60.60</cell><cell>60.66</cell><cell>94.03</cell><cell>94.09</cell></row><row><cell>Sigmoid</cell><cell>sigmoid</cell><cell>55.22</cell><cell>56.72</cell><cell>91.04</cell><cell>91.50</cell></row><row><cell>Sigmoid</cell><cell>relu</cell><cell>60.00</cell><cell>61.90</cell><cell>94.03</cell><cell>94.09</cell></row><row><cell>Relu</cell><cell>sigmoid</cell><cell>60.00</cell><cell>59.99</cell><cell>91.94</cell><cell>92.34</cell></row><row><cell>Relu</cell><cell>tanh</cell><cell>54.63</cell><cell>57.04</cell><cell>94.03</cell><cell>93.90</cell></row><row><cell>Tanh</cell><cell>tanh</cell><cell>64.48</cell><cell>64.52</cell><cell>97.31</cell><cell>97.37</cell></row><row><cell>Tanh</cell><cell>sigmoid</cell><cell>57.91</cell><cell>56.65</cell><cell>92.54</cell><cell>91.87</cell></row><row><cell>4*sigmoid-2</cell><cell>sigmoid</cell><cell>58.21</cell><cell>56.03</cell><cell>90.75</cell><cell>90.95</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc>Results for different activation functionsf 1 , f 2 (n × m is 3 × 3, p × q is 3 × 3)</figDesc><table><row><cell cols="2">Convolutional layers</cell><cell>Emotion</cell><cell></cell><cell>Song and speech</cell><cell></cell></row><row><cell>f 1</cell><cell>f 2</cell><cell>Acc(%)</cell><cell>UAR(%)</cell><cell>Acc(%)</cell><cell>UAR(%)</cell></row><row><cell>sigmoid</cell><cell>4*sigmoid-2</cell><cell>60.60</cell><cell>58.95</cell><cell>90.74</cell><cell>91.27</cell></row><row><cell>Sigmoid</cell><cell>tanh</cell><cell>60.59</cell><cell>58.57</cell><cell>93.73</cell><cell>94.20</cell></row><row><cell>Sigmoid</cell><cell>sigmoid</cell><cell>56.42</cell><cell>54.88</cell><cell>87.16</cell><cell>88.38</cell></row><row><cell>Relu</cell><cell>sigmoid</cell><cell>61.79</cell><cell>61.42</cell><cell>91.94</cell><cell>92.05</cell></row><row><cell>Tanh</cell><cell>tanh</cell><cell>65.97</cell><cell>66.90</cell><cell>94.33</cell><cell>94.37</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5</head><label>5</label><figDesc>Speaker and accent recognition for task-specific and multi-tasks models</figDesc><table><row><cell></cell><cell>GResNets+S</cell><cell></cell><cell>GResNets+M</cell><cell></cell></row><row><cell></cell><cell>Acc(%)</cell><cell>UAR(%)</cell><cell>Acc(%)</cell><cell>UAR(%)</cell></row><row><cell>Speaker identification</cell><cell>83.05</cell><cell>83.33</cell><cell>88.52</cell><cell>87.36</cell></row><row><cell>Accent recognition</cell><cell>89.67</cell><cell>89.82</cell><cell>92.44</cell><cell>92.57</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6</head><label>6</label><figDesc>Confusion matrix for accent recognition using a task-specific model</figDesc><table><row><cell>Accents</cell><cell>American</cell><cell>Canadian</cell><cell>English</cell><cell>Irish</cell><cell>Northern Irish</cell><cell>Scottish</cell></row><row><cell>American</cell><cell>217</cell><cell>7</cell><cell>4</cell><cell>10</cell><cell>9</cell><cell>0</cell></row><row><cell>Canadian</cell><cell>4</cell><cell>217</cell><cell>5</cell><cell>1</cell><cell>6</cell><cell>1</cell></row><row><cell>English</cell><cell>7</cell><cell>7</cell><cell>206</cell><cell>30</cell><cell>3</cell><cell>4</cell></row><row><cell>Irish</cell><cell>6</cell><cell>1</cell><cell>10</cell><cell>226</cell><cell>12</cell><cell>1</cell></row><row><cell>Northern Irish</cell><cell>2</cell><cell>7</cell><cell>3</cell><cell>2</cell><cell>241</cell><cell>3</cell></row><row><cell>Scottish</cell><cell>0</cell><cell>4</cell><cell>4</cell><cell>0</cell><cell>0</cell><cell>221</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7</head><label>7</label><figDesc>Confusion matrix for accent recognition using our multi-task model</figDesc><table><row><cell>Accents</cell><cell>American</cell><cell>Canadian</cell><cell>English</cell><cell>Irish</cell><cell>Northern Irish</cell><cell>Scottish</cell></row><row><cell>American</cell><cell>222</cell><cell>13</cell><cell>2</cell><cell>6</cell><cell>4</cell><cell>0</cell></row><row><cell>Canadian</cell><cell>1</cell><cell>224</cell><cell>1</cell><cell>0</cell><cell>7</cell><cell>1</cell></row><row><cell>English</cell><cell>4</cell><cell>5</cell><cell>228</cell><cell>10</cell><cell>3</cell><cell>7</cell></row><row><cell>Irish</cell><cell>4</cell><cell>3</cell><cell>16</cell><cell>230</cell><cell>2</cell><cell>1</cell></row><row><cell>Northern Irish</cell><cell>0</cell><cell>6</cell><cell>4</cell><cell>5</cell><cell>241</cell><cell>2</cell></row><row><cell>Scottish</cell><cell>1</cell><cell>3</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>224</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>http://homepages.inf.ed.ac.uk/jyamagis/page3/page58/page58.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>http://homepages.inf.ed.ac.uk/jyamagis/page3/page58/page58.html</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported by the National Natural Science Foundation of China [grant numbers 61402306, 61432012]</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep speech 2: End-to-end speech recognition in english and mandarin</title>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Anubhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chrzanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fougner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Raiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Seetapun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33nd international conference on machine learning</title>
		<meeting>the 33nd international conference on machine learning</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A theoretical analysis of feature pooling in visual recognition</title>
		<author>
			<persName><forename type="first">Y-L</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th international conference on machine learning</title>
		<meeting>the 27th international conference on machine learning</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="111" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Notes on convolutional neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bouvrie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Nets</title>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach Learn</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="75" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Speech emotion recognition: features and classification models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y-L</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digital Signal Process</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1154" to="1160" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Mxnet: a, flexible and efficient machine learning library for heterogeneous distributed systems</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<idno>arXiv:abs/1512.01274</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Robust multi-task feature learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The international conference on knowledge discovery and data mining</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="895" to="903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deep learning (Adaptive Computation and Machine Learning series</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Shift-invariant sparse coding for audio classification</title>
		<author>
			<persName><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kwong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Third conference on uncertainty in artificial intelligence</title>
		<meeting>the Twenty-Third conference on uncertainty in artificial intelligence</meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="149" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kuen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<idno>arXiv:abs/1512.07108</idno>
		<title level="m">Recent advances in convolutional neural networks</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for predominant instrument recognition in polyphonic music</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans Audio Speech Lang Process</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="208" to="221" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Deep speech: scaling up end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<idno>arXiv:abs/1412.5567</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised accent classification for deep data fusion of accent and language information</title>
		<author>
			<persName><forename type="first">Jhl</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Comm</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="19" to="33" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A joint many-task model: Growing a neural network for multiple NLP tasks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tsuruoka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<idno>arXiv:abs/1611.01587</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Batch normalization: accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd international conference on machine learning</title>
		<meeting>the 32nd international conference on machine learning</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Lab</surname></persName>
		</author>
		<ptr target="http://deeplearning.net/tutorial/contents.html" />
		<title level="m">Convolutional neural networks (lenet)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">MIR in matlab (II): a toolbox for musical feature extraction from audio</title>
		<author>
			<persName><forename type="first">O</forename><surname>Lartillot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Toiviainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Music Information Retrieval</title>
		<meeting>the 8th International Conference on Music Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="127" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning for audio classification using convolutional deep belief networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Largman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in annual conference on neural information processing systems</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1096" to="1104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<idno>arXiv:abs/1705.02304.2017</idno>
		<title level="m">Deep speaker: an end-to-end neural speaker embedding system</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ravdess: The ryerson audio-visual database of emotional speech and song</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Livingstone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Peck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Russo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual meeting of the canadian society for brain, behaviour and cognitive science</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning multiple tasks with deep relationship networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno>arXiv:abs/1506.02117</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Accent classification using support vector machines</title>
		<author>
			<persName><forename type="first">C</forename><surname>Pedersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Diederich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual IEEE/ACIS, international conference on computer and information science</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="444" to="449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Distant-talking accent recognition by combining GMM and DNN</title>
		<author>
			<persName><forename type="first">K</forename><surname>Phapatanaburi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sakagami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Iwahashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools Appl</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="5109" to="5124" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Timbre analysis of music audio signals with convolutional neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Slizovskaia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gómez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Serra</surname></persName>
		</author>
		<idno>arXiv:abs/1703.06697</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">In: Speech, audio, image and biomedical signal processing using neural networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="169" to="189" />
		</imprint>
	</monogr>
	<note>Audio signal processing</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Continuous wavelet transform based speech emotion recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Shegokar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sircar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on signal processing and communication systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>arXiv:abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Common cues to emotion in the dynamic facial expressions of speech and song</title>
		<author>
			<persName><forename type="first">R</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">F</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Wanderley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Q J Exp Psychol</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="952" to="970" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Inferring users&apos; emotions for human-mobile voice dialogue applications</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on multimedia and expo</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Recognizing emotion from singing and speaking using shared models</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Essl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Provost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on affective computing and intelligent interaction</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="139" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Cross-corpus acoustic emotion recognition from singing and speaking: a multi-task learning approach</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Provost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Essl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on acoustics, speech and signal processing</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5805" to="5809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A novel multi-relation regularization method for regression and classification in ad diagnosis</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H-I</forename><surname>Suk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="401" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Convolutional sparse coding for trajectory reconstruction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Pattern Anal Mach Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="529" to="540" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A novel relational regularization feature selection method for joint regression and classification in AD diagnosis</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H-I</forename><surname>Suk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="205" to="214" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Early diagnosis of alzheimers disease by joint feature selection and classification on temporally structured support vector machine</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on medical image computing and computer-assisted intervention</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="264" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">She received her B.S. degree in College of Computer Science, Sichuan University at 2016. Her current research interests include Neural Networks and Big Data</title>
	</analytic>
	<monogr>
		<title level="m">Hua Mao received the B.S. degree and M.S. degree in Computer Science from Electronic Science and Technology of China (UESTC) in 2006 and 2009, respectively. She received her Ph</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
		<respStmt>
			<orgName>Yuni Zeng is a Ph.D. student at Machine Intelligence Laboratory, College of Computer Science, Sichuan University ; D. degree in Computer Science and Engineering in Aalborg University</orgName>
		</respStmt>
	</monogr>
	<note>Her current research interests include Neural Networks and Big Data</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Dezhong</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sc</surname></persName>
		</author>
		<author>
			<persName><surname>Degree</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<pubPlace>Chengdu, China</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Electronic Science and Technology of China (UESTC)</orgName>
		</respStmt>
	</monogr>
	<note>in applied mathematics</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">in computer science and engineering</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Degree</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">UESTC</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
		<respStmt>
			<orgName>University of Electronic Science and Technology of China</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<author>
			<persName><forename type="first">Assistant</forename><surname>Lecturer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lecturer</title>
		<title level="s">the school of Applied Mathematics</title>
		<imprint>
			<date type="published" when="2001">2001. 2003</date>
		</imprint>
		<respStmt>
			<orgName>University of Electronic Science and Technology of China (UESTC</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">His current research interests include Neural Networks and Big Data. He is the founding director of Machine Intelligence Laboratory</title>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Computational Models and Applications (Springer, 2007), and Subspace Learning of Neural Networks</title>
		<meeting><address><addrLine>Chengdu, China; Beijing, China; Chengdu, China</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Chengdu Section</publisher>
			<date type="published" when="1994">2006. 2007.07-2009.09. 1994. 2004. 2010. 2015. 2009 2012. 2014</date>
		</imprint>
		<respStmt>
			<orgName>University of Electronic Science and Technology of China ; Deakin University, Melbourne, Australia; Currently, he is a Professor at Sichuan University ; Computer Science, Sichuan University</orgName>
		</respStmt>
	</monogr>
	<note>Zhang Yi received the Ph.D. degree in mathematics from the Institute of Mathematics, The Chinese Academy of Science. He is also the founder of IEEE Computational Intelligence Society, Chengdu Chapter and he is an IEEE fellow</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
