<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Framework for Recommending Accurate and Diverse Items Using Bayesian Graph Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jianing</forename><surname>Sun</surname></persName>
							<email>jianing.sun@huawei.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
								<orgName type="institution">Montreal Research Center</orgName>
								<address>
									<settlement>Montreal</settlement>
									<region>QC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Guo</surname></persName>
							<email>guowei67@huawei.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dengcheng</forename><surname>Zhang</surname></persName>
							<email>zhangdengcheng@huawei.com</email>
							<affiliation key="aff2">
								<orgName type="laboratory">Huawei Distributed and Parallel Software Lab</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yingxue</forename><surname>Zhang</surname></persName>
							<email>yingxue.zhang@huawei.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
								<orgName type="institution">Montreal Research Center</orgName>
								<address>
									<settlement>Montreal</settlement>
									<region>QC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Florence</forename><surname>Regol</surname></persName>
							<email>florence.robert-regol@mail.mcgill.ca</email>
							<affiliation key="aff3">
								<orgName type="department">Department of Electrical and Computer Eningeering</orgName>
								<orgName type="institution">McGill University</orgName>
								<address>
									<settlement>Montreal</settlement>
									<region>QC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yaochen</forename><surname>Hu</surname></persName>
							<email>yaochen.hu@huawei.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
								<orgName type="institution">Montreal Research Center</orgName>
								<address>
									<settlement>Montreal</settlement>
									<region>QC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huifeng</forename><surname>Guo</surname></persName>
							<email>huifeng.guo@huawei.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ruiming</forename><surname>Tang</surname></persName>
							<email>tangruiming@huawei.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Han</forename><surname>Yuan</surname></persName>
							<email>yuanhan3@huawei.com</email>
							<affiliation key="aff2">
								<orgName type="laboratory">Huawei Distributed and Parallel Software Lab</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiuqiang</forename><surname>He</surname></persName>
							<email>hexiuqiang1@huawei.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mark</forename><surname>Coates</surname></persName>
							<email>mark.coates@mcgill.ca</email>
							<affiliation key="aff3">
								<orgName type="department">Department of Electrical and Computer Eningeering</orgName>
								<orgName type="institution">McGill University</orgName>
								<address>
									<settlement>Montreal</settlement>
									<region>QC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
								<orgName type="institution">Montreal Research Center</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">KDD &apos;20</orgName>
								<address>
									<addrLine>August 23-27</addrLine>
									<postCode>2020</postCode>
									<region>Virtual Event</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Framework for Recommending Accurate and Diverse Items Using Bayesian Graph Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3394486.3403254</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>• Information systems → Recommender systems Graph Convolution Networks</term>
					<term>Recommendation</term>
					<term>Collaborative Filtering</term>
					<term>Bayesian Graph Neural Networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Personalized recommender systems are playing an increasingly important role for online consumption platforms. Because of the multitude of relationships existing in recommender systems, Graph Neural Networks (GNNs) based approaches have been proposed to better characterize the various relationships between a user and items while modeling a user's preferences. Previous graph-based recommendation approaches process the observed user-item interaction graph as a ground-truth depiction of the relationships between users and items. However, especially in the implicit recommendation setting, all the unobserved user-item interactions are usually assumed to be negative samples. There are missing links that represent a user's future actions. In addition, there may be spurious or misleading positive interactions. To alleviate the above issue, in this work, we take a first step to introduce a principled way to model the uncertainty in the user-item interaction graph using the Bayesian Graph Convolutional Neural Network framework. We discuss how inference can be performed under our framework and provide a concrete formulation using the Bayesian Probabilistic Ranking training loss. We demonstrate the effectiveness of our proposed framework on four benchmark recommendation datasets. The proposed method outperforms state-of-the-art graph-based recommendation models. Furthermore, we conducted an offline evaluation on one industrial large-scale dataset. It shows that our proposed method outperforms the baselines, with the potential gain being more significant for cold-start users. This illustrates the potential practical benefit in real-world recommender systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Online consumption (e.g., online shopping, watching videos, reading news, etc) has become more and more popular with the rapid development of the Internet and mobile devices. In such online applications, it is difficult to meet users' diverse and personalized needs. Under these circumstances, recommender systems, which guide users to find the items of interest in a gigantic and rapidly expanding pool of candidates, have emerged. Recommender systems tend to recommend the same item to users of similar interests, which is known as collaborative filtering (CF). As one of the most successful implementations of model-based CF methods, matrix factorization (MF) <ref type="bibr" target="#b16">[18]</ref> models achieved the best performance in Netflix contest. MF models (such as pLAS <ref type="bibr" target="#b13">[15]</ref>, MF <ref type="bibr" target="#b16">[18]</ref> and SVD++ <ref type="bibr" target="#b15">[17]</ref>) learn user and item embeddings by reconstructing the historical user-item interactions. The learned user and item embeddings are expected to characterize user preferences and item features. More recently, deep learning models ( <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">13]</ref>), which can learn more complex non-linear relationships between users and items, have been developed to enhance the performance of traditional MF models.</p><p>Despite the progress, most existing MF based methods struggle to address three major challenges: the sparsity issue, the uncertainty issue and the diversity issue. First, existing methods rely on the historical user-item interactions, so when the interactions are sparse, it is hard to learn high-quality representations, resulting in performance deterioration. Second, existing methods recognize the provided data as ground truth without uncertainty, but in many practical settings, the user-item interactions are collected from noisy environments. On one hand, some spurious user-item positive interactions are present due to noisy information; on the other hand, some potential user-item positive interactions are missing because the item is never presented to the user. This is falsely indicated as a negative interaction. Third, most existing methods for top-𝑁 recommendation focus on the relevance of each individual item independently and overlook the diversity of the top-𝑁 recommended items (i.e., the mutual influence between items). As observed in <ref type="bibr" target="#b20">[22]</ref>, ignoring diversity of the recommended list leads to sub-optimal performance.</p><p>Recently, graphs have been used to represent the relational information present in recommendation datasets. The user-item interaction can be naturally viewed as a bipartite graph. Furthermore, the similarities between users and the commonalities of items can be explicitly modelled as user-user and item-item graphs, respectively. These graphs allow algorithms to exploit the relationships between the entities. Graph Convolutional (Neural) Networks (GCNs) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b14">16]</ref> have proven to be among the best performing architectures for a variety of graph learning tasks. The key idea in GCNs is to learn how to iteratively aggregate feature information from local graph neighborhoods using neural networks. This aggregation step allows each node to learn a more general node representation from its local neighborhood. Incorporating the graph representation in recommendation systems has shown to be effective for alleviating the data sparsity and cold start problems and improves the recommendation relevance <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b36">38]</ref>. The proposed systems exploit user-item interaction graphs <ref type="bibr" target="#b29">[31,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b36">38]</ref>, user-user and (or) item-item co-occurrence graphs <ref type="bibr" target="#b17">[19]</ref> and heterogeneous graphs <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6]</ref> coming from heterogeneous interaction types (search, guide, click, etc.) or interaction motives.</p><p>Although there has been progress, existing graph-based recommendation models totally neglect the uncertainty issue in the bipartite user-item interaction graph. The edges provided in the observed bipartite graph are only based on historical interactions from the perspective of the data collector. They do not represent a complete picture of a user's interactions (for example, many other items may have been purchased from a different store and are therefore not present in the record). In addition, there may be spurious or misleading positive interactions, where a user has inadvertently clicked or purchased something on a temporary whim. On the other front, existing graph-based models also fail to address the lack of recommendation diversity. The neighborhood aggregation step in the graph-based recommendation approach generally leads to a learned user embedding that is considerably closer to the embeddings of items that he/she has interacted with previously. As a result, there is the potential that graph-based approaches can further dampen diversity by recommending very similar items to those involved in historical interactions. Expanding the aggregation neighbourhood can alleviate this to some extent, but this has the adverse effect of incorporating more spurious edges and noisy relationships, leading to performance degradation.</p><p>To address the limitations of the current GNN based recommendation approaches, we propose a new training framework based on Bayesian Graph Neural Networks (BGNNs). The proposed BGNN incorporates a random graph generative model based on nodecopying <ref type="bibr" target="#b22">[24]</ref>. The node-copying model can be used to produce sample graphs that are similar to the observed graph, but they contain sufficient diversity in terms of edges to promote better learning. Importantly, the model is very efficient and scalable, allowing it to be applied to very large graphs. The BGNN allows us to address the uncertainty in the observed user-item interaction records and at the same time bring diversity into the recommendation results. Bayesian GNNs have not previously been used for the task of recommendation, but it has been shown that they can produce significant performance improvements in semi-supervised node classification when there are very few training labels <ref type="bibr" target="#b21">[23,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b28">30,</ref><ref type="bibr" target="#b37">39]</ref>.</p><p>In this paper, we propose a novel Bayesian graph convolutional neural based recommender system framework, BGCF (Bayesian Graph Collaborative Filtering). There are two major contributions:</p><p>1) We introduce a principled way to address the uncertainty in the user-item bipartite graph in the recommendation training using Bayesian Graph Convolutional Neural Networks. We discuss how inference can be performed under our framework and provide a concrete formulation using the Bayesian Probabilistic Ranking training loss <ref type="bibr" target="#b24">[26]</ref>.</p><p>2) By performing thorough experiments on three commonly used recommendation datasets and one industrial large-scale dataset, we demonstrate that our proposed solution can achieve accurate and diverse recommendation results, and at the same time alleviate the data sparsity problem for users who do not have rich interaction history. The proposed method outperforms state-of-the-art graphbased recommender systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Model-based Collaborative Filtering Models</head><p>Collaborative filtering (CF) has been well studied for personalized recommender systems during the last decade. The basic assumption of CF is that users with similar preferences tend to like the same items, and items with similar audiences tend to have the same features. Model-based CF methods learn the similarities between items and users by fitting a model to the user-item interaction data. Latent factor models are common, such as probabilistic Latent Semantic Analysis (pLAS <ref type="bibr" target="#b13">[15]</ref>), Matrix Factorization (MF <ref type="bibr" target="#b16">[18]</ref>) and SVD++ <ref type="bibr" target="#b15">[17]</ref>. They learn user and item embeddings by reconstructing the historical user-item interactions. The learned user and item embeddings are expected to characterize user preferences and item features. Predicting an unknown rating of a user-item pair relies on the learned user and item embeddings (the predicted score is the inner product of the corresponding user and item embeddings).</p><p>Despite their success, model-based CF methods suffer from two limitations: (1) they are confined to the inner-product mechanism to measure the similarity between users and items and (2) as a result of merely relying on user-item interactions they suffer from data sparsity. Many approaches have been proposed to address these two problems. Recently, neural networks have been incorporated into collaborative filtering architectures <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">13]</ref>. These use a combination of fully-connected layers, convolution, inner-products and sub-nets to capture complex similarity relationships. An effective and common approach to alleviate the data sparsity problem is to leverage side information. For example, factorization machines <ref type="bibr" target="#b23">[25]</ref> can provide a mechanism for incorporating side information such as user demographics and item attributes. Another line of research to tackle the sparsity problem is to exploit structure proximity in the user-item bipartite graph (or other graph information), which will be elaborated upon in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph Based Recommendation</head><p>There has been a considerable research effort devoted to the use of graph models in recommendation systems. Early works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b35">37]</ref> use traditional random walks and label propagation to model the similarity scores for user-item pairs. With the success of graph (convolutional) neural networks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b14">16]</ref> on a wide range of applications, recent works have switched focus to applying GNNs in recommendation systems <ref type="bibr" target="#b29">[31,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b36">38]</ref>. Graph Convolutional Matrix Completion (GCMC) <ref type="bibr" target="#b29">[31]</ref> models the recommendation task as a matrix completion problem and uses a graph convolution autoencoder to learn user and item embeddings. Pinterest have proposed Pin-Sage <ref type="bibr" target="#b36">[38]</ref>, a large-scale GNN-based recommendation model to learn the embeddings from the pin-board bipartite graph. This has been reported to achieve significant performance gains for the Pinterest recommendation system. Neural Graph Collaborative Filtering (NGCF) <ref type="bibr" target="#b32">[34]</ref> designs a novel information propagation layer which enables explicit interactions between a user and its neighbor items to learn embeddings for users and items on the user-item interaction bipartite graph. GNNs have also been employed for the specific task of social recommendation in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b26">28,</ref><ref type="bibr" target="#b33">35,</ref><ref type="bibr" target="#b34">36]</ref> to better leverage the user's social relationships. It is worth mentioning another line of work which addresses the complex and heterogeneous interaction types between users and items in large-scale e-commerce networks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6]</ref>. This problem setting is not in the scope of this paper since we address only the setting where there is a single type of interaction between user and item. However, with an extended graph generative model, our proposed approach has the potential to generalize to the case of multiple interaction types (the heterogeneous graph setting).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Bayesian Graph Neural Networks</head><p>Almost all GNNs approaches process a graph as a ground-truth depiction of the relationship between nodes, but often the graphs employed in applications are derived from noisy data. Addressing the uncertainty on the underlying graph was first considered in <ref type="bibr" target="#b37">[39]</ref> for the problem of node classification. In this work, the authors target the inference of a generative graph model (using an MMSBM <ref type="bibr" target="#b18">[20]</ref> as the random graph model) to address the uncertainty in the underlying observed graph and formulate the structure uncertainty exploration using a Bayesian framework. However, the approach is not flexible and does not utilize the node attributes or labels. These limitations were addressed in the follow-up works <ref type="bibr" target="#b21">[23,</ref><ref type="bibr" target="#b22">24]</ref>, where <ref type="bibr" target="#b21">[23]</ref> uses a non-parametric model for the graph generative model and <ref type="bibr" target="#b22">[24]</ref> proposes a node copying model to achieve flexibility in the generative model and improve computational efficiency.</p><p>In the context of recommendation, <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b12">14,</ref><ref type="bibr" target="#b19">21]</ref> directly apply a Bayesian framework to model the users' preferences. Our approach is very different in that we use the Bayesian framework to model the uncertainty in the interaction graph within our graph neural network model. In the following sections, we address in depth how we can incorporate the Bayesian graph neural network framework into the recommendation system to better model the uncertainty in the user-item interaction graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRELIMINARIES</head><p>We base our model on a Bayesian formulation that incorporates graph uncertainty, the generative graph model of node copying, and the Bayesian Personalized Ranking loss. We briefly review these three components in the preliminary section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Bayesian Graph Convolutional Networks</head><p>In <ref type="bibr" target="#b37">[39]</ref>, to alleviate the effect of the potential noise in the observed graph, the authors view the graph as a random variable and consider a Bayesian approach. The framework they present can be generalized to target any prediction task and value of interest. To extend the model, we need to specify a probability function for the value of interest that depends on a graph G, the parameters for the graph generation model or a vector of other intermediate random parameters 𝝀, and any available node attributes D : 𝑝 (•|D, G, 𝝀). The posterior of interest 𝑝 (•|D, G 𝑜𝑏𝑠 ) is obtained by marginalizing over the random variables G and 𝝀:</p><formula xml:id="formula_0">𝑝 (•|D, G 𝑜𝑏𝑠 ) = ∫ 𝑝 (•|D, G, 𝝀)𝑝 (𝝀|D, G, G 𝑜𝑏𝑠 ) 𝑝 (G|D, G 𝑜𝑏𝑠 ) 𝑑 G 𝑑𝝀 .<label>(1)</label></formula><p>Zhang et al. presented this model in <ref type="bibr" target="#b37">[39]</ref> for the node classification task. In their case, the value of interest is the node labels. They used a stochastic block model for 𝑝 (G|G 𝑜𝑏𝑠 ), which is an appropriate choice for node classification. However the above Bayesian Graph Convolutional Network (BGCN) formulation has two limitations. First, it ignores the possible dependence of the graph G on D. Second, it requires an appropriate parametric random graph model (such as a Mixed-Membership Stochastic Block Model (MMSBM) model <ref type="bibr" target="#b18">[20]</ref>). Whether a model is appropriate depends very much on the encountered graph structure, which can vary greatly for different problem settings.</p><p>Since there is no inherent block structure in a recommender system bipartite graph, the MMSBM is not an applicable graph model. As an alternative, we use a more general generative model for graphs based on copying nodes, as introduced in <ref type="bibr" target="#b22">[24]</ref>. We demonstrate in the following sections that this model can be adapted naturally to the recommender system setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Node Copying</head><p>In <ref type="bibr" target="#b22">[24]</ref>, Pal et al. introduce the node copying model for 𝑝 (G). Samples from this model are generated by probabilistically rearranging (with replacement) the adjacency matrix rows of the observed graph 𝐴 𝑜𝑏𝑠 . The 𝑖 𝑡ℎ -row in the adjacency matrix 𝐴 𝑜𝑏𝑠 𝑖,: encodes the neighborhood of node 𝑖, so if we have a sampled graph with 𝐴 𝑖,: = 𝐴 𝑜𝑏𝑠 𝑗,: , the neighborhood of node 𝑗 was copied to node 𝑖.</p><p>The copying operation for the whole graph can be expressed using an auxiliary random vector 𝜻 = [𝜁 1 , 𝜁 2 , ...𝜁 𝑁 ] ⊤ ∈ {1, 2, ...𝑁 } 𝑁 , where each entry 𝜁 𝑖 denotes the row 𝐴 𝑜𝑏𝑠 𝜁 𝑖 ,: that will be placed in 𝐴 𝑖,: of the sampled G. The distribution of 𝑝 (𝜻 ) should be proportional to some node similarity that can be derived from the observed graph and data (G 𝑜𝑏𝑠 , D). A suitable node similarity is task dependent and should be specified accordingly.</p><p>The complete graph sampling process involves two phases. First we sample 𝜻 . The entries are assumed to be mutually independent so it can be factorized as follows:</p><formula xml:id="formula_1">𝑝 (𝜻 |G 𝑜𝑏𝑠 , D) = 𝑁 𝑖=1 𝑝 (𝜁 𝑖 |G 𝑜𝑏𝑠 , D) .<label>(2)</label></formula><p>Once a realization of 𝜻 is obtained, a second layer of randomness is added by performing the copying for each node with some probability 0 ≪ 𝜖 ≤ 1. The event of copying node 𝑗 to node 𝑞 is denoted by the indicator function 1 { G 𝑞 =G 𝑜𝑏𝑠,𝑗 } , so the generative model can be written as:</p><formula xml:id="formula_2">𝑝 (G|G 𝑜𝑏𝑠 , 𝜻 ) = 𝑁 𝑖=1 𝜖 1 {G 𝑖 =G 𝑜𝑏𝑠,𝜁 𝑖 } (1 − 𝜖) 1 {G 𝑖 =G 𝑜𝑏𝑠,𝑖 } .<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Bayesian Personalized Ranking loss for Implicit Recommendation</head><p>In <ref type="bibr" target="#b24">[26]</ref>, Rendle et al. introduce a ranking loss for recommendation systems based on a Bayesian model. We build on that model in this work, extending it to take into account the multiple graphs of the node-copying BGNN, so we now provide a brief review. Let us denote the set of items that are neighbours in the observed graph for user 𝑢 as 𝐼 + 𝑢 := {𝑖 ∈ 𝐼 : (𝑢, 𝑖) ∈ G 𝑜𝑏𝑠 }, where 𝐼 is the set of all items. The training set can then be written as:</p><formula xml:id="formula_3">𝐷 𝑆 := (𝑢, 𝑖, 𝑗)|𝑖 ∈ 𝐼 + 𝑢 ∧ 𝑗 ∈ 𝐼 \ 𝐼 + 𝑢 .<label>(4)</label></formula><p>In other words, the training set is all triples (𝑢, 𝑖, 𝑗) such that user 𝑢 interacted with 𝑖 but did not interact with 𝑗. The test set, denoted 𝐷 𝑆 , consists of all triples (𝑢, 𝑖, 𝑗) such that neither edge (𝑢, 𝑖) nor (𝑢, 𝑗) appears in G 𝑜𝑏𝑠 . The goal of the recommender system is to generate a total ranking &gt; 𝑢 of all items for each user 𝑢. The binary relation &gt; 𝑢 is required to be a total order on the set of items 𝐼 . The relation 𝑖 &gt; 𝑢 𝑗 specifies that user 𝑢 prefers item 𝑖 to item 𝑗.</p><p>In the Bayesian personalized ranking framework of <ref type="bibr" target="#b24">[26]</ref>, our task is to maximize:</p><formula xml:id="formula_4">𝑝 Θ|{&gt; 𝑢 } 𝐷 𝑆 ∝ 𝑝 {&gt; 𝑢 } 𝐷 𝑆 |Θ 𝑝 (Θ) .<label>(5)</label></formula><p>Here Θ are the parameters of the model, and {&gt; 𝑢 } 𝐷 𝑆 are the observed preferences in the training data. We aim to identify the parameters Θ that maximize this posterior over all users and all pairs of items. We assume that users act independently, so:</p><formula xml:id="formula_5">𝑝 {&gt; 𝑢 } 𝐷 𝑆 |Θ = (𝑢,𝑖,𝑗) ∈𝐷 𝑆 𝑝 (𝑖 &gt; 𝑢 𝑗 |Θ)<label>(6)</label></formula><p>We define the probability that a user prefers item 𝑖 over 𝑗 as</p><formula xml:id="formula_6">𝑝 (𝑖 &gt; 𝑢 𝑗 |Θ) := 𝜎 x𝑢𝑖 𝑗 (Θ) .<label>(7)</label></formula><p>Here x𝑢𝑖 𝑗 is a function of the model parameters Θ and the observed graph for each triple (𝑢, 𝑖, 𝑗). In our case, we use the difference between the dot products of the user and item embeddings, so x𝑢𝑖</p><formula xml:id="formula_7">𝑗 (Θ) = ℎ 𝑢 (Θ) • ℎ 𝑖 (Θ) − ℎ 𝑢 (Θ) • ℎ 𝑗 (Θ).</formula><p>If we adopt a normal distribution as the prior for 𝑝 (Θ) then we can formulate the optimization objective as:</p><formula xml:id="formula_8">BPR−OPT := ln 𝑝 Θ|{&gt; 𝑢 } 𝐷 𝑆 = (𝑢,𝑖,𝑗) ∈𝐷 𝑆 ln 𝜎 x𝑢𝑖 𝑗 − 𝜆 Θ ||Θ|| 2 (8)</formula><p>We can optimize this via stochastic gradient descent by repeatedly drawing triples (𝑢, 𝑖, 𝑗) randomly from the training set and updating the model parameters Θ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHODOLOGY 4.1 Bayesian Graph Neural Networks for Personalized Ranking</head><p>In this section we build on the Bayesian Personalized Ranking framework of <ref type="bibr" target="#b24">[26]</ref> to develop an approach that incorporates the strengths of Bayesian GNNs and thus takes into account the uncertainty in the graph observations. We develop two strategies; our experimental approach is based on the second.</p><p>4.1.1 Marginalizing over sampled graphs. In this approach, we focus on the predictive posterior:</p><formula xml:id="formula_9">𝑝 {&gt; 𝑢 } 𝐷 𝑆 |{&gt; 𝑢 } 𝐷 𝑆 ∝ ∫ G 𝑝 (G|G 𝑜𝑏𝑠 ) ∫ Θ 𝑝 {&gt; 𝑢 } 𝐷 𝑆 |Θ, G 𝑝 (Θ|{&gt; 𝑢 } 𝐷 𝑆 , G) 𝑑Θ 𝑑 G .<label>(9)</label></formula><p>Note that G 𝑜𝑏𝑠 encodes the same information as {&gt; 𝑢 } 𝐷 𝑆 . A triple (𝑢, 𝑖, 𝑗) ∈ 𝐷 𝑆 indicates an edge (𝑢, 𝑖) in G 𝑜𝑏𝑠 (the bipartite useritem interaction graph) and the absence of the edge (𝑢, 𝑗). We choose to write 𝑝 (G|G 𝑜𝑏𝑠 ) instead of 𝑝 (G|{&gt; 𝑢 } 𝐷 𝑆 ) to emphase the relationship between the sampled G and G 𝑜𝑏𝑠 . In this expression, we highlight that the probabilities are dependent on G because the sampled graph structure directly affects the models used to form the embeddings. When attempting to maximize this predictive posterior, we can choose to approximate the inner integral with respect to Θ by</p><formula xml:id="formula_10">𝑝 {&gt; 𝑢 } 𝐷 𝑆 | Θ G , G</formula><p>, where Θ G are the model parameters that maximize 𝑝 (Θ|{&gt; 𝑢 } 𝐷 𝑆 , G). This latter maximization is the same as BPR-OPT in <ref type="bibr" target="#b11">(13)</ref>, as outlined in Section 3.3, but the inclusion of G indicates that x𝑢𝑖 𝑗 (Θ, G, G 𝑜𝑏𝑠 ) now takes into account the embeddings derived using G (and G 𝑜𝑏𝑠 ), as expressed in <ref type="bibr" target="#b17">(19)</ref>.</p><p>Adopting a Monte-Carlo approximation to the integral, and drawing 𝑁 𝐺 graphs using the copying model from 𝑝 (G|G 𝑜𝑏𝑠 ), we have:</p><formula xml:id="formula_11">𝑝 ({&gt; 𝑢 } 𝐷 𝑆 |{&gt; 𝑢 } 𝐷 𝑆 ) ≈ 1 𝑁 𝐺 𝑁 𝐺 𝑙=1 𝑝 ({&gt; 𝑢 } 𝐷 𝑆 | Θ 𝑙 , G 𝑙 ) ,<label>(10)</label></formula><p>where Θ 𝑙 = arg max 𝑝 (Θ|{&gt; 𝑢 } 𝐷 𝑆 , G 𝑙 ). The maximization to identify Θ 𝑙 is performed using stochastic gradient descent for each graph G 𝑙 . The procedure thus involves sampling graphs and performing training of the weights for each graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.1.2</head><p>Redefining the probability mapping to address uncertainty. The procedure described above involves increased computation, because we must train multiple models, one for each sampled graph.</p><p>There are also more parameters in the collective models, so there is a potential for overfitting. We can alleviate these concerns by assuming a common model for all of the graphs. In this case, we return to the original BPR framework, but change our choice of x𝑢𝑖 𝑗 (Θ, G 𝑜𝑏𝑠 ). Let ℎ 𝑢,G denote the embedding in <ref type="bibr" target="#b17">(19)</ref> for user 𝑢 under sampled graph G, and let ℎ 𝑖,G be the corresponding embedding for item 𝑖. Then we define:</p><formula xml:id="formula_12">x𝑢𝑖 𝑗 (Θ, G 𝑜𝑏𝑠 ) := ∫ G ℎ 𝑢,G • ℎ 𝑖,G − ℎ 𝑢,G • ℎ 𝑗,G 𝑝 (G|G 𝑜𝑏𝑠 ) 𝑑 G ≈ 1 𝑁 𝐺 𝑁 𝐺 𝑙=1 ℎ 𝑢,G 𝑙 • ℎ 𝑖,G 𝑙 − ℎ 𝑢,G 𝑙 • ℎ 𝑗,G 𝑙 ,<label>(11)</label></formula><p>where the G 𝑙 are drawn from 𝑝 (G|G 𝑜𝑏𝑠 ). With this formulation, we can identify the model parameters using stochastic gradient descent as for BPR-OPT, but instead of drawing just a triple (𝑢, 𝑖, 𝑗), we first draw a graph G and then a triple (𝑢, 𝑖, 𝑗) ∈ 𝐷 𝑆 . The final estimates of rankings are derived from</p><formula xml:id="formula_13">𝑝 (𝑖 &gt; 𝑢 𝑗 |Θ, G 𝑜𝑏𝑠 ) := 𝜎 x𝑢𝑖 𝑗 ( Θ, G 𝑜𝑏𝑠 ) . (<label>12</label></formula><formula xml:id="formula_14">)</formula><p>where Θ is the solution to the maximization of:</p><formula xml:id="formula_15">BPR−OPT G 𝑜𝑏𝑠 := (𝑢,𝑖,𝑗) ∈𝐷 𝑆 ln 𝜎 x𝑢𝑖 𝑗 (Θ, G 𝑜𝑏𝑠 ) − 𝜆 Θ ||Θ|| 2 (13)</formula><p>Equation <ref type="bibr" target="#b10">(11)</ref>, in conjunction with <ref type="bibr" target="#b6">(7)</ref>, illustrates that our final prediction of a ranking involves averaging the scores x𝑢𝑖 𝑗 ( Θ, G 𝑜𝑏𝑠 , G) over all sampled graphs and then applying the sigmoid function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Neighborhood Copying Graph Generative Model</head><p>In order to sample graphs G with the node copying graph generative model, we first need to form a suitable copying distribution An intuitive way to measure similarity between users is to look at preferences over items they share in the historical interaction record. The Jaccard index is a statistic used for gauging the similarity between finite sample sets. It is defined as the size of the intersection divided by the size of the union of the sample sets. We specify the similarity between every pair of users 𝑢 𝑖 , 𝑢 𝑗 ∈ U as the Jaccard index for the interaction records of the two users:</p><formula xml:id="formula_16">𝑠𝑖𝑚(𝑢 𝑖 , 𝑢 𝑗 ) = N (𝑢 𝑖 ) ∩ N (𝑢 𝑗 ) N (𝑢 𝑖 ) ∪ N (𝑢 𝑗 ) . (<label>14</label></formula><formula xml:id="formula_17">)</formula><p>Here N (•) denotes the neighborhood of a node in the observed useritem interaction graph G 𝑜𝑏𝑠 . To define 𝑝 (𝜻 ), we simply normalize the metric over the pairs of users and set it to 0 otherwise. As the distribution is not dependent on any additional information D, it is only conditioned on G 𝑜𝑏𝑠 :</p><formula xml:id="formula_18">𝑝 (𝜁 𝑗 = 𝑚|G 𝑜𝑏𝑠 ) =        𝑠𝑖𝑚 ( 𝑗,𝑚) |U| 𝑖=1 𝑠𝑖𝑚 ( 𝑗,𝑖) if 𝑗, 𝑚 ∈ U 0 otherwise<label>(15)</label></formula><p>Having specified this distribution, we can then adopt the graph sampling strategy from the copying model 𝑝 (G|𝜁 ) according to equation <ref type="bibr" target="#b2">(3)</ref>. By sampling multiple graphs from our graph generative model, the observed graph, which is usually very sparse, can be directly augmented with likely neighbors discovered from the neighborhood of nodes with high similarities. In practice, we set the probability of copying nodes 𝜖 to be high to gain more additional edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Joint Training on Observed Graph and Sampled Graphs</head><p>From our proposed neighborhood copying graph generative model, we obtain 𝑙 graph samples G 1 , ..., G 𝑙 ∼ 𝑝 (G|G 𝑜𝑏𝑠 , 𝜻 ). The next step is how to learn useful information from these sampled graphs. In this section, we first present our representation learning model, specifying how we learn user and item embeddings from both the observed graph and each individual graph sample.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Learning on Sampled</head><p>Graphs G with Attention. Unlike the observed graph G 𝑜𝑏𝑠 whose edges are all true labels, the graph samples introduce some spurious edges while attempting to find potential links. The graph attention network (GAT) <ref type="bibr" target="#b31">[33]</ref> applies a self attention strategy operating on groups of spatially close neighbors to implicitly specify different weights for different nodes in a neighborhood. This follows the intuition that higher importance should be given to the neighbors that are more similar with the central node. GAT applied a single-layer feedforward neural network with multiple weights on sets of node feature vectors and on the transformation function to map from R 2𝑑 → R as the attention score.</p><p>To avoid the high complexity of GAT, we apply a simplified attention mechanism by simply taking the dot product between each neighbor with the central node as the attention coefficient. This reduces the influence of noisy edges in the neighborhood and emphasizes the discovered nodes with potentially positive effects. More specifically, the attention coefficient is calculated as follows:</p><formula xml:id="formula_19">𝛼 𝑗𝑘 = exp(𝒆 𝒋 • 𝒆 𝒌 ) 𝑖 ∈𝑁 ( 𝑗) exp(𝒆 𝒋 • 𝒆 𝒊 )<label>(16)</label></formula><p>𝛼 𝑗𝑘 represents the importance weight of each node 𝑘 ∈ N G ( 𝑗) on the target node 𝑗, where N G ( 𝑗) denotes the neighborhood of node 𝑗 in the sampled graph G. 𝒆 * are input node embeddings. Once obtained, the attention coefficients are used to compute a linear combination over sets of neighbor nodes, together with a mean aggregator, to serve as the learned representation from a sampled graph G: where 𝑛 𝑗 denotes the number of neighbors of node 𝑗 in a sampled graph G. 𝑾 1 G and 𝑾 2 G are shared weight matrices. denotes the concatenation operation.</p><formula xml:id="formula_20">hG 𝑗 = 𝑾 1 G 𝑘 ∈N G ( 𝑗) 𝛼 𝑗𝑘 𝒆 𝒌 𝑾 2 G • 1 𝑛 𝑗 𝑘 ∈N G ( 𝑗) 𝒆 𝑘 ,<label>(17)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Learning on</head><p>Observed Graph G 𝑜𝑏𝑠 . For a target node 𝑗, we learn its representation vector hG 𝑜𝑏𝑠 𝑗 on the observed graph by applying a mean aggregator over its neighbors. We select the mean aggregator because edges in the observed graph are all ground-truth and the mean aggregator is the simplest permutation-invariant operator. The process is as follows:</p><formula xml:id="formula_21">hG 𝑜𝑏𝑠 𝑗 = 𝜎 𝑾 G 𝑜𝑏𝑠 • 1 𝑛 𝑗 𝑘 ∈N G 𝑜𝑏𝑠 ( 𝑗) 𝒆 𝑘 ,<label>(18)</label></formula><p>where 𝑛 𝑗 denotes the number of neighbors of node 𝑗 in the observed graph G 𝑜𝑏𝑠 , and 𝑾 G 𝑜𝑏𝑠 is a shared weight matrix. </p><p>where 𝜎 (•) denotes the tanh non-linear transformation, and denotes the concatenation operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>Datasets. To evaluate the effectiveness of our method, we conduct extensive experiments on four popular benchmarks: Amazon-Movies, Amazon-Beauty, Amazon-CDs. These benchmark datasets are publicly accessible, real-world data with various domains, sizes, and sparsity levels. We filter out long-tailed users and items with fewer than 10 interactions for all 3 datasets.</p><p>For each user, we randomly select 20% of the rated items as ground truth for testing, The remaining 70% and 10% data constitutes the training and validation set. Table <ref type="table" target="#tab_0">1</ref> summarizes the statistics of all the datasets. Amazon-Movies, Amazon-Beauty and Amazon-CDs: Amazonreview * is a popular dataset for product recommendations <ref type="bibr">[12]</ref>. We select three subsets, Amazon-Movies, Beauty and CDs.</p><p>Evaluation Metrics. For all experiments, we evaluate the recommendation accuracy of our model and baselines in terms of Recall@k and NDCG@k. Because accuracy alone does not guarantee satisfactory recommendations, we also assess serendipity@k <ref type="bibr" target="#b20">[22]</ref>, which factors in how surprising and relevant a recommendation is. Surprise is measured as a weighted average of the differences between the probability that an item 𝑖 is recommended for a specific user and the probability that item 𝑖 is recommended for any user. It is computed <ref type="bibr" target="#b25">[27]</ref> as:</p><formula xml:id="formula_23">SRDP@k = 1 |U| 𝑢 ∈U 1 |𝐼 𝑘 (𝑢)| 𝑖 ∈𝐼 𝑘 (𝑢) max 𝑃 𝑖 (𝑢)−𝑃 𝑖 (U), 0 * 𝑟𝑒𝑙 𝑖 (𝑢) . (20) Here 𝑃 𝑖 (𝑢) = |𝐼 𝑘 (𝑢) |−𝑟𝑎𝑛𝑘 𝑖 |R 𝑢,𝑘 |−1</formula><p>represents the probability of recommending item 𝑖 to a specific user 𝑢, and 𝑃 𝑖 (U) = 𝐷 (𝑖)/ 𝑢 ∈U 𝐷 (𝑢) represents the approximate probability of recommending that item for any user. We use 𝐷 (𝑖) and 𝐷 (𝑢) to denote the degrees of item 𝑖 and user 𝑢 in the observed graph.</p><p>Baselines. To demonstrate the effectiveness, we compare our proposed model with the following methods: 1) Classical collaborative filtering methods: BPRMF <ref type="bibr" target="#b24">[26]</ref>, NeuMF <ref type="bibr" target="#b11">[13]</ref> 2) Graph neural network-based CF methods: GC-MC <ref type="bibr" target="#b29">[31]</ref>, PinSAGE <ref type="bibr" target="#b36">[38]</ref>, PinSAGE-LSTM, and NGCF <ref type="bibr" target="#b32">[34]</ref>.</p><p>Please refer to the supplementary material for the detailed descriptions of the above baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Our proposed methods:</head><p>• Base: To prove that our proposed BGCF framework can be applied to any graph learning based recommendation models, we modified a simplified GNN-based base model from <ref type="bibr" target="#b27">[29]</ref>. Instead of applying GCN on self graphs, we utilize self connections as an additional regularization term in the loss function. A graph convolutionn layer with mean aggregator is applied on each side of the user-item bipartite graph, to learn user &amp; item embeddings by aggregating from one-hop neighbors.</p><p>• BGCF: This is the main model that we propose; it incorporates a Bayesian graph neural network to account for uncertainty in the observed user-item interactions and employs node-copying as a graph-generative model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION AND ANALYSIS</head><p>In Table <ref type="table">2</ref>, we can see that the proposed model outperforms all the baselines on recall@20 and .serendipity@20 metrics. This trend can be observed across all considered datasets. Also, the base model is consistently being outperformed by BGCF on all metrics, which suggests that the Bayesian formulation is advantageous. In the following section, we further investigate the advantages coming from this model by considering alternative metrics focused on diversity rather than recall. We also provide some insight into which kind of user sees the most improvement and conduct a more thorough ablation study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Accuracy-Diversity Trade-off under Ranking-based Techniques</head><p>Diversity (i.e. novelty) and accuracy metrics have different objective that are at odds with each other, in that increasing one can result in sacrificing the other. This trade-off has been studied in both the natural language generation setting <ref type="bibr" target="#b1">[2]</ref>, where Caccia et al. evaluate diversity vs. quality and show the danger of focusing on one metric, and in recommendation <ref type="bibr" target="#b0">[1]</ref>, where the trade-off between diversityin-top-𝑁 and precision-in-top-𝑁 has been examined. In practice, having a recommender system with a better accuracy/diversity trade-off allows the platform to have more personalized recommendations while maintaining comparable levels of accuracy. In this section we evaluate the quality of accuracy-diversity trade-off between our proposed algorithm and the second best model which has the highest accuracy results than other baselines. To evaluate on the quality of trade-off, we need a method that can modify one metric. In <ref type="bibr" target="#b0">[1]</ref>, Adomavicius et al. proposed the application of another ranking function 𝑟𝑎𝑛𝑘 (𝑖,𝑇 𝑅 ) upon the top-𝑘 recommendation list. The parameter 𝑇 𝑅 is a ranking threshold; only items initially ranked above 𝑇 𝑅 are re-ranked.</p><p>In our case, we apply a popularity based ranking function, 𝑟𝑎𝑛𝑘 (𝑖,𝑇 𝑅 ) = 𝐷 (𝑖), where 𝐷 (𝑖) denotes the degree of an item. Given a ranked recommendation list with 𝑘 = 20, we re-rank the items above𝑇 𝑅 based on their popularity. Thus, items with high prediction score but high popularity are demoted. Choosing different 𝑇 𝑅 values allows the user to set the desired balance between accuracy and novelty, e.g., when 𝑇 𝑅 = 22, we re-rank the top 22 items based on popularity and produce the final top-20 results based on the re-ranked list.</p><p>We conduct the re-ranking process on two datasets, comparing the effect on our model and on NGCF. The novelty of recommendations is computed <ref type="bibr" target="#b30">[32]</ref> as Nov@k = 1  From Figure <ref type="figure">3</ref> we can see that for any desired diversity-level (measured by Nov@20), our model has a higher accuracy than NGCF.</p><formula xml:id="formula_24">| U | 𝑢 ∈U 𝑖 ∈𝐼 𝑘 (𝑢) − log 2 𝐷 (𝑖)/ | U | |𝐼 𝑘 (𝑢) |</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">User density analysis</head><p>We analyse our results to highlight which type of users benefits from this recommender model. In Figure <ref type="figure">4</ref>, we can view the relative improvement of our proposed model over the best preforming baselines (BPR, NGCF, PinSage-lstm) for different groups of users based on sparsity. The overall trend is that the algorithm offers the most relative improvement over the alternative methods for users with less items, hence for sparser users (user that have a history of less than 40 clicks).</p><p>Statistics of Sampled Graphs. In Table <ref type="table" target="#tab_1">3</ref>, we analyse the quality of the generated graphs from the copying model by computing the edge overlap between the observed graph and the realizations. As a reference point, we also report the same statistics obtained from a copying model with an uninformative copying distribution 𝜻 (uniform over all nodes) G ∼ 𝑝 (G|G 𝑜𝑏𝑠 , 𝜻 𝑈 𝑛𝑖 ) and from a random degree-corrected model 𝑝 𝑑𝑒𝑔𝑟𝑒𝑒 (G) which randomly assign edges to each user while preserving each user's degree. From this table we can see that the copying model is able to maintain some of the initial edges and can even discover some directly present in the test set, while the other models generate graphs that contain less than 2% of the initial links (training and test clicks combined). This Table <ref type="table">2</ref>: The overall performance comparison. Bold values denote scenarios where a Wilcoxon signed rank test indicates a statistically significant difference between the best and second-best (underline) algorithms Amazon-CDs R@20 N@20 SRDP@20 Amazon-Movies R@20 N@20 SRDP@20 Amazon-Beauty R@20 N@20 SRDP@20 BPRMF 0. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Ablation Studies</head><p>As our proposed model involves multiple novel components, we conduct an ablation study to assess their individual contributions to the performance. In particular, we want to evaluate the benefits coming from 1) using the sampled graphs from the node copying model (∼ G); 2) training jointly with the observed graph and the sampled graphs (∼ G + G 𝑜𝑏𝑠 ); and 3) using attention aggregation on the sampled graphs together with mean aggregation. We report results on partial architectures for one trial on the Amazon-CDs dataset in Table <ref type="table" target="#tab_2">4</ref>. The first observation is that jointly training with both graphs (mean + G 𝑜𝑏𝑠 + ∼ G) is better than only training with either the observed graphs or the sampled graphs. Secondly, using an attention-based aggregation also yields a performance improvement of the same order as the joint training. 1) Rich side information, including app features (e.g., app size, category and etc), user features (e.g., user's various behaviors in the App Store), and context features (e.g., device type, operation time and etc) are kept in the dataset. As side information is usually available in live recommender systems and is very valuable to alleviate the cold-start problem, it is necessary to verify the performance of our model under such a setting.</p><p>2) Benchmark datasets are usually split into training and test sets in such a way that the interaction records of each user are randomly distributed between the two sets. Every user in the test set is therefore also present in the training set. However, in live recommender systems, it is required to serve every user, including those that have had no interactions in the system before, who are called cold-start users. In order to better match this scenario, we split our industrial dataset according to the timestamp, i.e., the first 33 consecutive days of records are the training set, while the next 7 days of records are the test set.</p><p>7.1.2 Evaluation Protocol. In the industrial dataset, there are 943,177 users, 9,768 apps and 4,000,000 download records. Each download record in the dataset is considered as a positive instance, while the interactions of a user and her unobserved apps in the universal app set are all considered as negative. We evaluate the performance of difference models on such binary labeled instances by LogLoss (also known as cross entropy), a widely used metric in binary classification. A smaller LogLoss value indicates better performance. We use the same baseline models described in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Performance Comparison</head><p>We compare the performance of different models for three different settings: all the test users, warm-start test users (who appear in the training set) and cold-start test users (who do not appear in the training set). The performance comparison on the industrial dataset is presented in Table <ref type="table" target="#tab_3">5</ref>. As can be observed, our model has superior performancecompared to the best baseline (NGCF) by 2.17% in terms of LogLoss on all the test users. This result demonstrates the effectiveness of our model in recommendation scenarios with </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">SUPPLEMENTARY 9.1 Baselines</head><p>To demonstrate its effectiveness, we compare our proposed model with the following methods: Classical collaborative filtering methods:</p><p>• BPRMF <ref type="bibr" target="#b24">[26]</ref>: A general learning framework for personalized ranking recommendation using implicit feedback. • NeuMF <ref type="bibr" target="#b11">[13]</ref>: NeuMF replaces the inner product with an MLP to learn the user-item interaction function.</p><p>Graph neural network-based CF methods:</p><p>• GC-MC <ref type="bibr" target="#b29">[31]</ref>: This is a graph-based auto-encoder approach that treats recommendation as a matrix completion problem. To improve the efficiency of graph sampling, we incorporate three practical techniques, namely Layer-wise Uniqueness, Parallel Sampling and Asynchronous Sampling. 9.2.3 Layer-wise Uniqueness. To reduce the number of random walks, we sample neighbors while enforcing layer-wise uniqueness. This means that when we sample the neighbors for a set of 𝑆 nodes in the same layer, we first generate the set 𝑆 𝑢 of unique target nodes. The indices in the original array are recovered using the gather operation. The duplicate nodes share the same neighbors, therefore we only need to sample the neighbors of unique nodes.</p><p>By enforcing layer-wise uniqueness we can reduce the number of random walks dramatically and therefore improve the training efficiency. However, as it eliminates duplicate neighbors at each layer, it distorts the neighborhood distributions, which may degrade the accuracy of the algorithms. Therefore there is a trade-off between accuracy and efficiency. 9.2.4 Parallel sampling. When performing neighbor sampling for each layer, the random walks are independent. Therefore, we can perform parallel random walks with multi-processing and multithreading, to accelerate the sampling procedure for one batch. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Motivation figure for node copying in the context of recommender system setting.Bayesian Graph Neural Networks (BGNNs). The proposed BGNN incorporates a random graph generative model based on nodecopying<ref type="bibr" target="#b22">[24]</ref>. The node-copying model can be used to produce sample graphs that are similar to the observed graph, but they contain sufficient diversity in terms of edges to promote better learning. Importantly, the model is very efficient and scalable, allowing it to be applied to very large graphs. The BGNN allows us to address the uncertainty in the observed user-item interaction records and at the same time bring diversity into the recommendation results. Bayesian GNNs have not previously been used for the task of recommendation, but it has been shown that they can produce significant performance improvements in semi-supervised node classification when there are very few training labels<ref type="bibr" target="#b21">[23,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b28">30,</ref><ref type="bibr" target="#b37">39]</ref>.In this paper, we propose a novel Bayesian graph convolutional neural based recommender system framework, BGCF (Bayesian Graph Collaborative Filtering). There are two major contributions:1) We introduce a principled way to address the uncertainty in the user-item bipartite graph in the recommendation training using Bayesian Graph Convolutional Neural Networks. We discuss how inference can be performed under our framework and provide a concrete formulation using the Bayesian Probabilistic Ranking training loss<ref type="bibr" target="#b24">[26]</ref>.2) By performing thorough experiments on three commonly used recommendation datasets and one industrial large-scale dataset, we demonstrate that our proposed solution can achieve accurate and diverse recommendation results, and at the same time alleviate the data sparsity problem for users who do not have rich interaction history. The proposed method outperforms state-of-the-art graphbased recommender systems.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overall architecture for our proposed BGCF training process.</figDesc><graphic url="image-21.png" coords="6,79.02,83.68,453.97,118.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>4. 3 . 3</head><label>33</label><figDesc>Joint Embedding Vector as Node Representation. By learning on the observed graph G 𝑜𝑏𝑠 and the sampled graphs G, for a target node 𝑗, we obtain two representation vectors hG 𝑗 and hG 𝑜𝑏𝑠 𝑗 . These represent the potential preferences and observed preferences, respectively, of a user or an item. The final representation of a node is hence developed as the combination of hG 𝑗 and hG 𝑜𝑏𝑠 𝑗 : ĥ𝑗 = 𝜎 hG 𝑗 hG 𝑜𝑏𝑠 𝑗 ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>.</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure3: Re-ranking results on our proposed algorithm and second base model on Amazon-CDs and Amazon-Movies under thresholds 𝑇 𝑅 ∈ {22, 24, 26, 28, 30}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>9. 2 . 5</head><label>25</label><figDesc>Asynchronous Sampling. As Graph Sampling and Network Training are disjoint across different batches, they can be scheduled sequentially or in parallel. In sequential scheduling, Graph Sampling and Network Training are processed sequentially, i.e., Network Training starts when the neighbor nodes of the current batch are sampled, and the Graph Sampling for the next batch is not started until the Network Training of the current batch finishes. Obviously, this is inefficient. We create multiple threads to perform Graph Sampling in parallel to Network Training, so that the network keeps training without being interrupted by the sampling procedure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of public datasets.</figDesc><table><row><cell cols="5">Dataset # User # Item # Interaction Density</cell></row><row><cell cols="3">Movies 44,439 25,047</cell><cell>1,070,860</cell><cell>0.096%</cell></row><row><cell>Beauty</cell><cell>7,068</cell><cell>3,750</cell><cell>79,506</cell><cell>0.299%</cell></row><row><cell>CDs</cell><cell cols="2">43,169 35,648</cell><cell>777,426</cell><cell>0.051%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc>Overlap ratio of the generated samples with the observed graph (%) averaged over 10 graphs spurious edges / train edges/ test edges ratio (%).Dataset 𝑝 𝑑𝑒𝑔𝑟𝑒𝑒 (G) 𝑝 (G|G 𝑜𝑏𝑠 , 𝜻 𝑢𝑛𝑖 ) 𝑝 (G|G 𝑜𝑏𝑠 , 𝜻 )</figDesc><table><row><cell></cell><cell cols="2">0794 0.0501</cell><cell>0.0097</cell><cell>BPRMF</cell><cell>0.0667 0.0436</cell><cell>0.0103</cell><cell>BPRMF</cell><cell>0.1312 0.0778</cell><cell>0.0100</cell></row><row><cell>NMF</cell><cell cols="2">0.1008 0.0604</cell><cell>0.0109</cell><cell>NMF</cell><cell>0.0820 0.0511</cell><cell>0.0110</cell><cell>NMF</cell><cell>0.1152 0.0692</cell><cell>0.0081</cell></row><row><cell>GC-MC</cell><cell cols="2">0.0791 0.0473</cell><cell>0.0093</cell><cell>GC-MC</cell><cell>0.0638 0.0401</cell><cell>0.0084</cell><cell>GC-MC</cell><cell>0.1082 0.0659</cell><cell>0.0077</cell></row><row><cell>PinSage</cell><cell cols="2">0.1265 0.0799</cell><cell>0.0134</cell><cell>PinSage</cell><cell>0.0872 0.0559</cell><cell>0.0114</cell><cell>PinSage</cell><cell>0.1378 0.0821</cell><cell>0.0138</cell></row><row><cell cols="3">PinSage-lstm 0.1269 0.0805</cell><cell>0.0135</cell><cell>PinSage-lstm</cell><cell>0.0886 0.0567</cell><cell>0.0115</cell><cell>PinSage-lstm</cell><cell>0.1354 0.0786</cell><cell>0.0135</cell></row><row><cell>NGCF</cell><cell cols="2">0.1258 0.0792</cell><cell>0.0129</cell><cell>NGCF</cell><cell>0.0866 0.0555</cell><cell>0.0112</cell><cell>NGCF</cell><cell>0.1513 0.0917</cell><cell>0.0151</cell></row><row><cell>Base</cell><cell cols="2">0.1455 0.0907</cell><cell>0.0161</cell><cell>Base</cell><cell>0.1023 0.0663</cell><cell>0.0132</cell><cell>Base</cell><cell>0.1432 0.0869</cell><cell>0.0145</cell></row><row><cell>BGCF</cell><cell cols="2">0.1506 0.0948</cell><cell>0.0168</cell><cell>BGCF</cell><cell>0.1066 0.0693</cell><cell>0.0139</cell><cell>BGCF</cell><cell>0.1534 0.0912</cell><cell>0.0153</cell></row><row><cell>CDs</cell><cell>98.9 / 1.0 / 0.1</cell><cell cols="2">98.8 / 1.1 / 0.1</cell><cell>60.5 / 38.3 / 1.2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Movie</cell><cell>98.9 / 1.0 / 0.1</cell><cell cols="2">98.7 / 1.1 / 0.2</cell><cell>66.5 / 32.4 / 1.1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Beauty 98.5 / 1.4 / 0.1</cell><cell cols="2">98.4 / 1.5 / 0.1</cell><cell>80.8 / 18.3 / 0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">highlights the sparsity of the datasets and further motivates the</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">use of the copying model.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Ablation study for the proposed algorithm. We collect and sample 33 consecutive days of userapp download records from the game center of a mainstream App Store for training, and the next 7 days for testing. Compared with the four publicly accessible datasets used above, this industrial dataset has two unique characteristics:</figDesc><table><row><cell>Architecture</cell><cell cols="4">R@10 R@20 N@10 N@20</cell></row><row><cell cols="2">Base (mean + G 𝑜𝑏𝑠 ) 0.0971</cell><cell>0.1455</cell><cell>0.0761</cell><cell>0.0917</cell></row><row><cell>mean + ∼ G</cell><cell>0.0989</cell><cell>0.1459</cell><cell>0.0769</cell><cell>0.0921</cell></row><row><cell>mean + G 𝑜𝑏𝑠 + ∼ G</cell><cell>0.1004</cell><cell>0.1486</cell><cell>0.0784</cell><cell>0.0938</cell></row><row><cell>BGCF*</cell><cell cols="4">0.1016 0.1506 0.0791 0.0948</cell></row><row><cell cols="5">7 APPLICATION ON INDUSTRIAL DATASET</cell></row><row><cell cols="5">In addition to the benchmark comparison, we also validate the</cell></row><row><cell cols="5">superiority of our proposed model on an industrial dataset.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Performance Comparison (LogLoss) in Industrial Dataset. The lower the better.In this work, we propose a novel recommendation model based on Bayesian Graph Neural Networks and the node copying graph generative model to naturally incorporate the uncertainty in the underlying user-item interaction graph. From our extensive experiments, our proposed model shows consistent recommendation accuracy improvement over state-of-the-art methods for four public benchmark datasets and one large-scale industrial dataset. Via thorough analysis of case studies, we highlight that our proposed Bayesian Graph Collaborative Filtering framework can bring more recommendation diversity and alleviate the data sparsity problem.</figDesc><table><row><cell></cell><cell>All users</cell><cell>Warm-start users</cell><cell>Cold-start users</cell></row><row><cell>BPR</cell><cell>0.37618</cell><cell>0.33590</cell><cell>0.41103</cell></row><row><cell>NMF</cell><cell>0.36028</cell><cell>0.31783</cell><cell>0.39588</cell></row><row><cell>GC-MC</cell><cell>0.36050</cell><cell>0.31189</cell><cell>0.40083</cell></row><row><cell>PinSage</cell><cell>0.36075</cell><cell>0.31032</cell><cell>0.40288</cell></row><row><cell>PinSage-lstm</cell><cell>0.35974</cell><cell>0.31090</cell><cell>0.40056</cell></row><row><cell>NGCF</cell><cell>0.35806</cell><cell>0.31158</cell><cell>0.39713</cell></row><row><cell>BGCF</cell><cell>0.35030</cell><cell>0.30624</cell><cell>0.38729</cell></row><row><cell cols="4">rich side information. More specifically, our model outperforms</cell></row><row><cell cols="4">the best baseline (NGCF) by 1.71% and 2.48% on warm-start users</cell></row><row><cell cols="4">and cold-start users, respectively. This improvement verifies the</cell></row><row><cell cols="4">superiority of our model in alleviating the cold-start issue.</cell></row><row><cell cols="2">8 CONCLUSION</cell><cell></cell><cell></cell></row><row><cell cols="4">Besides, using Mindspore, the all-scenario deep learning framework</cell></row><row><cell cols="4">developed by Huawei, the computation in the proposed approach</cell></row><row><cell cols="4">can be easily and automatically parallelized to execute on multiple</cell></row><row><cell cols="3">GPUs, rendering training extremely efficient.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>• PinSAGE<ref type="bibr" target="#b36">[38]</ref>: PinSAGE is a recent industry application of graph representation learning for recommendation. It deploys Graph-Sage<ref type="bibr" target="#b10">[11]</ref> on an item-item graph with both image and text information as the input node features with mean aggregator. A hit rate improvement of more than 20% is reported in<ref type="bibr" target="#b36">[38]</ref>.• PinSAGE-LSTM: Same overall architecture as PinSAGE but we replaced the mean aggregator with LSTM aggregation proposed in<ref type="bibr" target="#b10">[11]</ref>. Although LSTM is an undesirable aggregator because it is not permutation invariant with respect to the ordering of neighborhood, it has better expressive capability and can model more complicated neighborhood relationships.• NGCF<ref type="bibr" target="#b32">[34]</ref>: NGCF is the state-of-the-art graph-based CF method.It explicitly integrates a bipartite graph structure into the embedding learning process to model the high-order connectivity in the user-item graph. In the Graph Construction component, the bipartite graph is constructed based on the original user-item interaction records as well as the generated neighbors by the proposed node copying method. The nodes in the constructed bipartite graph contain user/item features. Graph Sampling: In the Graph Sampling component, neighbor nodes are sampled (instead of considering all the neighbor nodes) to aggregate neighborhood information for a central node, in order to maintain training efficiency. A key difference between graph neural networks and traditional neural networks is that graph sampling is the efficiency bottleneck and also a critical factor affecting performance. Therefore we will elaborate on the details of the graph sampling methodology in Section 9.2.2. Network Training: Each edge in the constructed bipartite graph is treated as a positive instance. For each such positive instance, several negative instances (each of which represents a user and an unobserved item) are sampled to conduct pair-wise training. Then the Network Training component performs the forward computing and backward updating. 9.2.2 Graph Sampling. As discussed in Section 2, random walk and its variants are widely used to retrieve the neighbor nodes of a given central node in GNN based algorithms. When we utilize Random Walk as the neighbor sampling method, two key factors should be considered. The first factor is the length of each random walk. If 2-hop neighbors of a central node are needed, then its 1-hop neighbors are sampled and afterwards 1-hop neighbors of such sampled neighbors of the target node are also sampled. That is to say, the length of a random walk determines the number of neighbours sampled in each random walk. The second factor is the number of random walks starting at a central node.</figDesc><table><row><cell cols="2">9.2 Industrial Application</cell></row><row><cell cols="2">9.2.1 System Panorama. In our implementation, the whole training</cell></row><row><cell cols="2">procedure is composed of 3 components, namely Graph Construc-</cell></row><row><cell cols="2">tion, Graph Sampling and Network Training as shown in Algo-</cell></row><row><cell cols="2">rithm 1. In Algorithm 1, N indicates the number of batches used</cell></row><row><cell cols="2">to learn the representation of nodes in the constructed bipartite</cell></row><row><cell>graph.</cell><cell></cell></row><row><cell cols="2">Algorithm 1: Overall training procedure</cell></row><row><cell>6</cell><cell>end</cell></row><row><cell cols="2">7 end</cell></row><row><cell cols="2">Graph Construction:</cell></row></table><note>1 while not converge do 2 Graph Construction with node copying; 3 for 𝑖 ∈ [0, N ] do 4 Graph Sampling for current batch 𝑖; 5 Network Training for current batch 𝑖;</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Improving Aggregate Recommendation Diversity Using Ranking-Based Techniques</title>
		<author>
			<persName><forename type="first">Gediminas</forename><surname>Adomavicius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kwon</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Language GANs Falling Short</title>
		<author>
			<persName><forename type="first">Massimo</forename><surname>Caccia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Caccia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learning Representations (ICLR)</title>
				<meeting>Int. Conf. Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Representation Learning for Attributed Multiplex Heterogeneous Network</title>
		<author>
			<persName><forename type="first">Yukuo</forename><surname>Cen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGKDD Int. Conf. Knowledge Discovery and Data Mining</title>
				<meeting>ACM SIGKDD Int. Conf. Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bayesian mixed-E ects models for recommender systems</title>
		<author>
			<persName><forename type="first">Michelle</forename><surname>Keim Condli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Madigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Posse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. ACM SIGIR Conf. on Research and Development in Information Retrieval</title>
				<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering</title>
		<author>
			<persName><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. Neural Information Processing Systems</title>
				<meeting>Adv. Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Metapath-guided Heterogeneous Graph Neural Network for Intent Recommendation</title>
		<author>
			<persName><forename type="first">Junxiong</forename><surname>Shaohua Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaotian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linmei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Biyu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongliang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf. Artificial Intelligence</title>
				<meeting>Int. Joint Conf. Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Graph Neural Networks for Social Recommendation</title>
		<author>
			<persName><forename type="first">Wenqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. World Wide Web</title>
				<meeting>Int. Conf. World Wide Web</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep social collaborative filtering</title>
		<author>
			<persName><forename type="first">Wenqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Conf. Recommender Systems</title>
				<meeting>ACM Conf. Recommender Systems</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ItemRank: A Random-Walk Based Scoring Algorithm for Recommender Engines</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Augusto</forename><surname>Pucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf. Artificial Intell</title>
				<meeting>Int. Joint Conf. Artificial Intell</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">DeepFM: A Factorization-Machine based Neural Network for CTR Prediction</title>
		<author>
			<persName><forename type="first">Huifeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuqiang</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf. Artificial Intelligence</title>
				<meeting>Int. Joint Conf. Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. Neural Inf. Proc. Systems</title>
				<meeting>Adv. Neural Inf. . Systems</meeting>
		<imprint>
			<date type="published" when="2016">2017. 2016</date>
		</imprint>
	</monogr>
	<note>Proc. Int. Conf. World Wide Web</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. World Wide Web</title>
				<meeting>Int. Conf. World Wide Web</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A non negative matrix factorization for collaborative filtering recommender systems based on a Bayesian probabilistic model</title>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Hernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesús</forename><surname>Bobadilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Ortega</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<publisher>Knowledge-Based Systems</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Latent semantic models for collaborative filtering</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Information System</title>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learning Representations</title>
				<meeting>Int. Conf. Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Factorization meets the neighborhood: a multifaceted collaborative filtering model</title>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGKDD Int. Conf. Knowledge Discovery and Data Mining</title>
				<meeting>ACM SIGKDD Int. Conf. Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Matrix Factorization Techniques for Recommender Systems</title>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">M</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Graph Intention Network for Click-through Rate Prediction in Sponsored Search</title>
		<author>
			<persName><forename type="first">Feng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenrui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conf. Research and Development in Information Retrieval</title>
				<meeting>ACM Int. Conf. Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Scalable MCMC for mixed membership stochastic blockmodels</title>
		<author>
			<persName><forename type="first">Wenzhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungjin</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Artificial Intelligence and Statistics</title>
				<meeting>Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Efficient Bayesian methods for graph-based recommendation</title>
		<author>
			<persName><forename type="first">Ramon</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renato</forename><surname>Assunção</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodrygo Lt</forename><surname>Santos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Conf. Recommender Systems</title>
				<meeting>ACM Conf. Recommender Systems</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Being accurate is not enough: how accuracy metrics have hurt recommender systems</title>
		<author>
			<persName><forename type="first">John</forename><surname>Sean M Mcnee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">A</forename><surname>Riedl</surname></persName>
		</author>
		<author>
			<persName><surname>Konstan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI&apos;06 extended abstracts on Human factors in computing systems</title>
				<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Non-Parametric Graph Learning for Bayesian Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Soumyasundar</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saber</forename><surname>Malekmohammadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florence</forename><surname>Regol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingxue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yishi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Coates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Uncertainty in Artificial Intelligence</title>
				<meeting>Conf. Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bayesian Graph Convolutional Neural Networks using Node Copying</title>
		<author>
			<persName><forename type="first">Soumyasundar</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florence</forename><surname>Regol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Coates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Learning and Reasoning with Graph-Structured Representations Workshop (ICML</title>
				<meeting>Learning and Reasoning with Graph-Structured Representations Workshop (ICML</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Factorization Machines</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Data Mining</title>
				<meeting>IEEE Int. Conf. Data Mining</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">BPR: Bayesian Personalized Ranking from Implicit Feedback</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeno</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Uncertainty in Artificial Intelligence</title>
				<meeting>Conf. Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">How good your recommender system is? A survey on evaluations in recommendation</title>
		<author>
			<persName><forename type="first">Thiago</forename><surname>Silveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoping</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Journal of Machine Learning and Cybernetics</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Session-based Social Recommendation via Dynamic Graph Attention Networks</title>
		<author>
			<persName><forename type="first">Weiping</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiping</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conf. Web Search and Data Mining</title>
				<meeting>ACM Int. Conf. Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-Graph Convolution Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Jianing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingxue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huifeng</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Data Mining</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Ruiming Tang, and Xiuqiang He</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Louis</forename><forename type="middle">C</forename><surname>Tiao</surname></persName>
			<affiliation>
				<orgName type="collaboration">Variational Spectral Graph Convolutional Networks.</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Pantelis</forename><surname>Elinas</surname></persName>
			<affiliation>
				<orgName type="collaboration">Variational Spectral Graph Convolutional Networks.</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Harrison</forename><surname>Nguyen</surname></persName>
			<affiliation>
				<orgName type="collaboration">Variational Spectral Graph Convolutional Networks.</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Edwin</forename><forename type="middle">V</forename><surname>Bonilla</surname></persName>
			<affiliation>
				<orgName type="collaboration">Variational Spectral Graph Convolutional Networks.</orgName>
			</affiliation>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Graph Convolutional Matrix Completion</title>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGKDD Int. Conf. Knowledge Discovery and Data Mining</title>
				<meeting>ACM SIGKDD Int. Conf. Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Deep Learning Day</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rank and relevance in novelty and diversity metrics for recommender systems</title>
		<author>
			<persName><forename type="first">Saúl</forename><surname>Vargas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Castells</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Conf. Recommender Systems</title>
				<meeting>ACM Conf. Recommender Systems</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Graph Attention Networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learning Representations</title>
				<meeting>Int. Conf. Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Neural Graph Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conf. Research and Development in Information Retrieval</title>
				<meeting>ACM Int. Conf. Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A neural influence diffusion model for social recommendation</title>
		<author>
			<persName><forename type="first">Le</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peijie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanjie</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiting</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. ACM SIGIR Conf. on Research and Development in Information Retrieval</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dual Graph Attention Networks for Deep Latent Representation of Multifaceted Social Effects in Recommender Systems</title>
		<author>
			<persName><forename type="first">Qitian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengrui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guihai</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. World Wide Web</title>
				<meeting>Int. Conf. World Wide Web</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">HOP-rec: high-order proximity for implicit recommendation</title>
		<author>
			<persName><forename type="first">Jheng-Hong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chih-Ming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan-Ju</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Feng</forename><surname>Tsai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc ACM Conf. Recommender Systems</title>
				<meeting>ACM Conf. Recommender Systems</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Graph Convolutional Neural Networks for Web-Scale Recommender Systems</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGKDD Int. Conf. Knowledge Discovery and Data Mining</title>
				<meeting>ACM SIGKDD Int. Conf. Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Bayesian graph convolutional neural networks for semi-supervised classification</title>
		<author>
			<persName><forename type="first">Yingxue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumyasundar</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deniz</forename><surname>Üstebay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Int. Conf. Artificial Intelligence</title>
				<meeting>AAAI Int. Conf. Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
