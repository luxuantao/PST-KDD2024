<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Yuping</forename><surname>Lai</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">are with the Pattern Recognition and Intelligent System Laboratory</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<postCode>100876</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Information Security</orgName>
								<orgName type="institution">University of Technology</orgName>
								<address>
									<postCode>100144</postCode>
									<settlement>Beijing</settlement>
									<country>North China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">W</forename><forename type="middle">Bastiaan</forename><surname>Kleijn</surname></persName>
							<idno type="ORCID">0000-0002-1973-3920</idno>
							<affiliation key="aff0">
								<orgName type="laboratory">are with the Pattern Recognition and Intelligent System Laboratory</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<postCode>100876</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Victoria University of Wellington</orgName>
								<address>
									<postCode>6140</postCode>
									<settlement>Wellington</settlement>
									<country key="NZ">New Zealand</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">are with the Pattern Recognition and Intelligent System Laboratory</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<postCode>100876</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Liang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">are with the Pattern Recognition and Intelligent System Laboratory</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<postCode>100876</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">School of Electronic Engineering and Computer Science</orgName>
								<orgName type="laboratory">SketchX Lab</orgName>
								<orgName type="institution">Queen Mary University of London</orgName>
								<address>
									<postCode>E1 4NS</postCode>
									<settlement>London</settlement>
									<country key="GB">U.K</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jun</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">are with the Pattern Recognition and Intelligent System Laboratory</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<postCode>100876</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Information Security</orgName>
								<orgName type="institution">University of Technology</orgName>
								<address>
									<postCode>100144</postCode>
									<settlement>Beijing</settlement>
									<country>North China, China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Zhanyu</forename><forename type="middle">Ma</forename><surname>Ma</surname></persName>
							<email>mazhanyu@bupt.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">are with the Pattern Recognition and Intelligent System Laboratory</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<postCode>100876</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8D4088BA1E1DA5366F4A7F7AEBF1048F</idno>
					<idno type="DOI">10.1109/TNNLS.2018.2844399</idno>
					<note type="submission">received January 21, 2018; revised March 18, 2018; accepted May 31, 2018.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Bayesian estimation</term>
					<term>computer vision</term>
					<term>Dirichlet process (DP) mixture</term>
					<term>inverted Dirichlet distribution</term>
					<term>variational learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we develop a novel variational Bayesian learning method for the Dirichlet process (DP) mixture of the inverted Dirichlet distributions, which has been shown to be very flexible for modeling vectors with positive elements. The recently proposed extended variational inference (EVI) framework is adopted to derive an analytically tractable solution. The convergency of the proposed algorithm is theoretically guaranteed by introducing single lower bound approximation to the original objective function in the EVI framework. In principle, the proposed model can be viewed as an infinite inverted Dirichlet mixture model that allows the automatic determination of the number of mixture components from data. Therefore, the problem of predetermining the optimal number of mixing components has been overcome. Moreover, the problems of overfitting and underfitting are avoided by the Bayesian estimation approach. Compared with several recently proposed DP-related methods and conventional applied methods, the good performance and effectiveness of the proposed method have been demonstrated with both synthesized data and real data evaluations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>widely applied to many areas, such as pattern recognition, machine learning, data mining, and computer vision <ref type="bibr" target="#b2">[3]</ref>- <ref type="bibr" target="#b11">[12]</ref>. Among all finite mixture models, the finite Gaussian mixture model (GMM) has been the most popular method for modeling continuous data. Much of its popularity is due to the fact that any continuous distribution can be arbitrarily well approximated by a GMM with unlimited number of mixture components. Moreover, the parameters in a GMM can be estimated efficiently via maximum likelihood (ML) estimation with the expectation maximum (EM) algorithm <ref type="bibr" target="#b12">[13]</ref>. By assigning prior distributions to the parameters in a GMM, the Bayesian estimation of GMM can be carried out with conjugate prior-posterior pair matching <ref type="bibr" target="#b13">[14]</ref>. Both the ML and the Bayesian estimation algorithms can be represented in analytically tractable form <ref type="bibr" target="#b13">[14]</ref>.</p><p>Recent studies have shown that non-Gaussian statistical models, e.g., the beta mixture model (BMM) <ref type="bibr" target="#b5">[6]</ref>, the Dirichlet mixture model (DMM) <ref type="bibr" target="#b6">[7]</ref>, the Gamma mixture model (GaMM) <ref type="bibr" target="#b14">[15]</ref>, and the von Mises-Fisher mixture model (vMM) <ref type="bibr" target="#b15">[16]</ref>, can model the non-Gaussian distributed data more efficiently, compared with the conventional GMM. For example, the BMM has been widely applied in modeling gray image pixel values <ref type="bibr" target="#b5">[6]</ref> and DNA methylation data <ref type="bibr" target="#b16">[17]</ref>. In order to efficiently model proportional data <ref type="bibr" target="#b6">[7]</ref>, i.e., vector contains elements in [0, 1] and the summation of them equals 1, DMM can be utilized to describe the underlying distribution. In generalized-K (K G ) fading channels, GaMM has been used to analyze the capacity and error probability <ref type="bibr" target="#b14">[15]</ref>, which are scalar nonnegative variables. The vMM has been widely used in modeling directional data, such as yeast gene expression <ref type="bibr" target="#b15">[16]</ref> and topic detection <ref type="bibr" target="#b17">[18]</ref>. The finite inverted DMM (IDMM), among others, has been demonstrated to be an efficient tool for modeling data vector with positive elements <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>. Moreover, the inverted Dirichlet distribution also has connections with nonnegative matrix factorization (NMF). In sparse NMF <ref type="bibr" target="#b20">[21]</ref>, the l 1 -norm constraint is usually applied to favor the sparseness. As the definition of the inverted Dirichlet distribution is similar to the nonnegative properties of the columns in the original matrix and the basis matrix, selecting proper prior distribution to describe the underlying distribution of the aforementioned columns can favor the sparse NMF. The work in <ref type="bibr" target="#b21">[22]</ref> proposed a pragmatic approach with a group of novel document-level features to Chinese word segmentation on patents. The performance of the Chinese patent word segmentation is significantly improved. A real-data-driven framework for mobile big data has been proposed such that it can uncover its huge potential to understand human mobility, which has a significant impact on future wireless networks <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>. Zhao et al. <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref> proposed to utilize the patch importance and label correlation jointly in action units classification. These works validate the effectiveness of the joint learning with support vector machine (SVM)-based learning and CNN-based learning for multilabel classification tasks.</p><p>An essential problem in finite mixture modeling is how to automatically decide the appropriate number of mixture components based on the data. The component number has a strong effect on the modeling accuracy <ref type="bibr" target="#b26">[27]</ref>. If the number of mixture components is not properly chosen, the mixture model may overfit or underfit the observed data. To deal with this problem, many methods have been proposed. These can be categorized into two groups: deterministic approaches <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref> and Bayesian methods <ref type="bibr" target="#b29">[30]</ref>. Deterministic approaches are generally implemented by ML estimation under an EM-base and require the integration of entropy measures or some information theoretic criteria, such as the minimum message length <ref type="bibr" target="#b28">[29]</ref>, the Bayesian information criterion <ref type="bibr" target="#b30">[31]</ref>, and the Akaike information criterion <ref type="bibr" target="#b31">[32]</ref>, to determine the number of components in the mixture model. It is worth noting that, in general, the EM algorithm converges to a local maximum or a saddle point and its solution is highly dependent on its initialization. On the other hand, the Bayesian methods, which are not sensitive to initialization by introducing proper prior distributions to the parameters in the model, have been widely used to find a suitable number of components in a finite mixture model. In this case, the parameters of a finite mixture model (including the parameters in a component and the weighting coefficients) are treated as random variables under the Bayesian framework. The posterior distributions of the parameters, rather than simple point estimates, are computed <ref type="bibr" target="#b1">[2]</ref>. The model truncation in the Bayesian estimation of finite mixture model is carried out by setting the corresponding weights of the unimportant mixture components to zero (or a small value close to zero) <ref type="bibr" target="#b1">[2]</ref>. However, the number of mixture components should be properly initialized, as it can only decrease during the training process.</p><p>The increasing interest in mixture modeling has led to the development of the model selection method <ref type="foot" target="#foot_0">1</ref> . Recent work has shown that the nonparametric Bayesian approach <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref> can provide an elegant solution for automatically determining the complexity of model. The basic idea behind this approach is that it provides methods to adaptively select the optimal number of mixing components, while also allows the number of mixture components to remain unbounded. In other words, this approach allows the number of components to increase as new data arrives, which is the key difference from finite mixture modeling. The most widely used Bayesian nonparametric <ref type="bibr" target="#b34">[35]</ref> model selection method is based on the Dirichlet process (DP) mixture model <ref type="bibr" target="#b35">[36]</ref>- <ref type="bibr" target="#b37">[38]</ref>. The DP mixture model extends distributions over measures, which has the appealing property that it does not need to set a prior on the number of components. In essence, the DP mixture model can also be viewed as an infinite mixture model with its complexity increasing as the size of data set grows. Recently, the DP mixture model has been applied in many important applications. For instance, the DP mixture model has been adopted to a mixture of different types of non-Gaussian distributions, such as the DP mixture of beta-Liouville distributions <ref type="bibr" target="#b38">[39]</ref>, the DP mixture of student's-t distributions <ref type="bibr" target="#b39">[40]</ref>, the DP mixture of generalized Dirichlet distributions <ref type="bibr" target="#b40">[41]</ref>, the DP mixture of student's-t factors <ref type="bibr" target="#b41">[42]</ref>, and the DP mixture of hidden Markov random field models <ref type="bibr" target="#b42">[43]</ref>.</p><p>Generally speaking, most parameter estimation algorithms for both the deterministic and the Bayesian methods are timeconsuming, because they have to numerically evaluate a given model selection criterion <ref type="bibr" target="#b28">[29]</ref>. This is especially true for the fully Bayesian Markov chain Monte Carlo (MCMC) <ref type="bibr" target="#b43">[44]</ref>, which is one of the widely applied Bayesian approaches with numerical simulations. The MCMC approach has its own limitations, when high-dimensional data are involved in the training stage <ref type="bibr" target="#b44">[45]</ref>. This is due to the fact that its sampling-based characteristics yield a heavy computational burden, and it is difficult to monitor the convergence in the high-dimensional space. To overcome the aforementioned problems, variational inference (VI), which can provide an analytically tractable solution and good generalization performance, has been proposed as an efficient alternative to the MCMC approach <ref type="bibr" target="#b45">[46]</ref>. With an analytically tractable solution, the numerical sampling during each iteration in the optimization stage can be avoided. Hence, the VI-based solutions can lead to more efficient estimation which has been successfully applied in a variety of applications <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b46">[47]</ref>- <ref type="bibr" target="#b48">[49]</ref>.</p><p>Motivated by the ability of the Bayesian nonparametric approaches to solve the model selection problem and the good performance recently obtained by the VI framework, we focus on the variational learning of the DP mixture of inverted Dirichlet distributions [also known as the infinite IDMM (InIDMM)]. Since InIDMM is a typical non-Gaussian statistical model, it is not feasible to apply the standard VI framework to obtain an analytically tractable solution for the Bayesian estimation. As a variate of VI, stochastic VI (SVI) <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref> has been proposed as an alternative solution to approximate the posterior distributions. The algorithm under the SVI framework is scalable and suitable for massive data. However, when dealing with non-Gaussian distributions, the expectations in the update iterations <ref type="bibr" target="#b49">[50,</ref><ref type="bibr">Fig. 4</ref>] cannot be calculated explicitly, and some sampling methods are also required to approximate the expectations. In order to derive an analytically tractable solution for the variational learning of InIDMM, the recently proposed extended VI (EVI) <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, which is particularly suitable for non-Gaussian statistical models, has been adopted to provide an appropriate single lower bound (SLB) approximation to the original object function. With the auxiliary function, an analytically tractable solution for the Bayesian estimation of InIDMM is derived. The key contributions of this paper are threefold.</p><p>1) The finite IDMM has been extended to the InIDMM under the stick-breaking process framework <ref type="bibr" target="#b51">[52]</ref>. Thus, the difficulty in automatically determining the number of mixture components can be overcome. 2) An analytical solution is derived with the EVI framework for InIDMM. Moreover, compared with the recently proposed algorithm for InIDMM <ref type="bibr" target="#b52">[53]</ref>, which is based on multiple lower bound (MLB) approximation, our algorithm can not only theoretically guarantee convergence but also provide better approximations.</p><p>3) The proposed method has been applied in several important applications in computer vision, such as image categorization and object detection. The good performance has been illustrated with both synthesized and real data evaluations. The remaining part of this paper is organized as follows.</p><p>Section II provides a brief overview of the finite inverted Dirichlet mixture and the DP mixture. The InIDMM is also proposed. In Section III, a Bayesian learning algorithm with EVI is derived. The proposed algorithm has an analytically tractable form. The experimental results with both synthesized and real data evaluations are reported in Section IV. Finally, we draw conclusions and future research directions in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. STATISTICAL MODEL</head><p>In this section, we first present a brief overview of the finite IDMM. Then, the DP mixture model with stick-breaking representation is introduced. Finally, we extend the IDMM to InIDMM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Finite Inverted Dirichlet Mixture Model</head><p>Given a D-dimensional vector x = {x 1 , . . . , x D } generated from an IDMM with M components, the probability density function (pdf) of</p><p>x is denoted as <ref type="bibr" target="#b18">[19]</ref> IDMM</p><formula xml:id="formula_0">( x| π, ) = M m=1 π m iDir( x| α m )<label>(1)</label></formula><p>where = { α m } M m=1 and π = {π m } M m=1 is the mixing coefficient vector subject to the constraints 0 ≤ π m ≤ 1 and M m=1 π m = 1. Moreover, iDir( x| α) is an inverted Dirichlet distribution with its (D + 1)-dimensional positive parameter vector α = {α 1 , . . . , α D+1 } defined as</p><formula xml:id="formula_1">iDir( x| α) = D+1 d=1 α d D+1 d=1 (α d ) D d=1 x α d -1 d 1 + D d=1 x d - D+1 d=1 α d (2)</formula><p>where x d &gt; 0 for d = 1, . . . , D and (•) is the Gamma function defined as (a) = ∞ 0 t a-1 e -t dt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Dirichlet Process With Stick-Breaking</head><p>The DP <ref type="bibr" target="#b37">[38]</ref> is a stochastic process used for the Bayesian nonparametric data analysis, particularly in a DP mixture model (infinite mixture model). It is a distribution over distributions rather than parameters, i.e., each draw from a DP is a probability distribution itself, rather than a parameter vector <ref type="bibr" target="#b53">[54]</ref>. We adopt the DP to extend the IDMM to the infinite case, such that the difficulty of the automatic determination of the model complexity (i.e., the number of mixture components) can be overcome. To this end, the DP is constructed by the following stick-breaking formulation <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b55">[56]</ref>, which is an intuitive and simple constructive definition of the DP.</p><p>Assume that H is a random distribution and ϕ is a positive real scalar. We consider two countably infinite collections of independently generated stochastic variables m ∼ H and λ m ∼ Beta(λ m ; 1, ϕ)<ref type="foot" target="#foot_1">2</ref> for m = {1, . . . , ∞}, where Beta(x; a, b) is the beta distribution defined as Beta(</p><formula xml:id="formula_2">x; a, b) = ((a + b)/(a)(b))x a-1 (1 -x) b-1 .</formula><p>A distribution G is said to be DP distributed with a concentration parameter ϕ and a base measure or base distribution H [denoted as G ∼ DP(ϕ, H )], if the following conditions are satisfied:</p><formula xml:id="formula_3">G = ∞ m=1 π m δ m , π m = λ m m-1 l=1 (1 -λ l ) (3)</formula><p>where {π m } is a set of stick-breaking weights with constraints ∞ m=1 π m = 1, and δ m is a delta function whose value is 1 at location m and 0 otherwise. The generation of the mixing coefficients {π m } can be considered as the process of breaking a unit length stick into an infinite number of pieces. The length of each piece, λ m , which is proportional to the rest of the "stick" before the current breaking, is considered as an independent random variable generated from Beta(λ m ; 1, ϕ). Because of its simplicity and natural generalization ability, the stick-breaking construction has been a widely applied scheme for the inference of DPs <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b56">[57]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Infinite Inverted Dirichlet Mixture Model</head><p>Now, we consider the problem of modeling</p><p>x by an InIDMM, which is actually an extended IDMM with an infinite number of components. Therefore, (1) can be reformulated as</p><formula xml:id="formula_4">InIDMM( x| π, ) = ∞ m=1 π m iDir( x| α m )<label>(4)</label></formula><p>where</p><formula xml:id="formula_5">π = {π m } ∞ m=1 and = { α m } ∞ m=1 .</formula><p>Then, the likelihood function of the InIDMM given the observed data set</p><formula xml:id="formula_6">X = { x n } N n=1 is given by InIDMM(X | π, ) = N n=1 ∞ m=1 π m iDir( x n | α m ) . (<label>5</label></formula><formula xml:id="formula_7">)</formula><p>In order to clearly illustrate the generation process of each observation</p><p>x n in the mixture model, we introduce a latent indication vector variable z n = {z n1 , z n2 , . . .}. z has only one element equal to 1 and the other elements in z are 0. For example, z nm = 1 indicates that the sample</p><p>x n comes from the mixture component m. Therefore, the conditional distribution of X given the parameters and the latent variables</p><formula xml:id="formula_8">Z = {z nm } are InIDMM(X |Z, ) = N n=1 ∞ m=1 iDir( x n | α m ) z nm . (6)</formula><p>Moreover, to exploit the advantages of the Bayesian framework, conjugate prior distributions are introduced for all the unknown parameters according to their distribution properties.</p><p>In this paper, we place the conjugate priors over the unknown stochastic variables Z, , and λ = (λ 1 , λ 2 , . . .) such that a full Bayesian estimation model can be obtained. It is worth to note that, the InIDMM models were proposed in <ref type="bibr" target="#b57">[58]</ref>. The key contribution of this paper is to propose new estimation procedure which is more efficient.</p><p>In the aforementioned full Bayesian model, the prior distribution of Z given π is given by</p><formula xml:id="formula_9">p(Z| π) = N n=1 ∞ m=1 π z nm m . (<label>7</label></formula><formula xml:id="formula_10">)</formula><p>As π is a function of λ according to the stick-breaking construction of the DP as shown in (3), we rewrite <ref type="bibr" target="#b6">(7)</ref> as</p><formula xml:id="formula_11">p(Z| λ) = N n=1 ∞ m=1 λ m m-1 l=1 (1 -λ l ) z nm . (<label>8</label></formula><formula xml:id="formula_12">)</formula><p>As previously mentioned in Section II-B, the prior distribution of λ is</p><formula xml:id="formula_13">p( λ| ϕ) = ∞ m=1 Beta(λ m ; 1, ϕ m ) = ∞ m=1 ϕ m (1 -λ m ) ϕ m -1<label>(9)</label></formula><p>where ϕ = (ϕ 1 , ϕ 2 , . . .). Based on (3), we can obtain the expected value of π m . In order to do this, the expected value of λ m will first be calculated as</p><formula xml:id="formula_14">λ m = 1/(1 + ϕ m ). (<label>10</label></formula><formula xml:id="formula_15">)</formula><p>Then, the expected value of π m is denoted as</p><formula xml:id="formula_16">π m = λ m m-1 l=1 (1 -λ l ).<label>(11)</label></formula><p>It is worth to note that, when the value of ϕ m is small, λ m will become large. Therefore, the expected value of the mixing coefficients π m are controlled by the parameters ϕ m , i.e., small value of ϕ m will yield small π m (m &gt; 1) such that the distribution of π m will be sparse.</p><p>As ϕ m is positive, we assume that ϕ follows a product of Gamma prior distributions as:</p><formula xml:id="formula_17">p( ϕ; s, t) = ∞ m=1 Gam(ϕ m ; s m , t m ) = ∞ m=1 t s m m (s m ) ϕ s m -1 m e -t m ϕ m (<label>12</label></formula><formula xml:id="formula_18">)</formula><p>where Gam(•) is the Gamma distribution. s = (s 1 , s 2 , . . .) and t = (t 1 , t 2 , . . .) are the hyperparamters and subject to the constraints s m &gt; 0 and t m &gt; 0.</p><p>Next, we introduce an approximating conjugate prior distribution to parameter in InIDMM. The inverted Dirichlet distribution belongs to the exponential family and its formal conjugate prior can be derived with the Bayesian rule <ref type="bibr" target="#b1">[2]</ref> as</p><formula xml:id="formula_19">p( α| μ 0 , v 0 ) = C( μ 0 , v 0 ) ⎡ ⎣ D+1 d=1 α d D+1 d=1 α d ⎤ ⎦ ν 0 e - μ 0 ( α T - I D+1 ) (<label>13</label></formula><formula xml:id="formula_20">)</formula><p>where μ 0 = [μ 1 0 , . . . μ D+1 0 ] and ν 0 are the hyperparameters in the prior distribution, and</p><formula xml:id="formula_21">C( μ 0 , v 0 ) is a normalization coef- ficient such that p( α| μ 0 , v 0 )d α = 1.</formula><p>I d is a D-dimensional vector with all elements equal to one. Then, we can write the posterior distribution of α as [with N independent identically distributed (i.i.d.)] observations X )</p><formula xml:id="formula_22">f ( α|X ) = iDir(X | α) f ( α| μ 0 , ν 0 ) iDir(X | α) f ( α| μ 0 , ν 0 )d α = C( μ N , ν N ) ⎡ ⎣ D+1 d=1 α d D+1 d=1 (α d ) ⎤ ⎦ ν N e - μ N ( α T - I D+1 ) (<label>14</label></formula><formula xml:id="formula_23">)</formula><p>where the hyperparameters ν N and μ N in the posterior distribution are</p><formula xml:id="formula_24">ν N = ν 0 + N, μ N = μ 0 -[ln X + - I D+1 ln 1 + I T D+1 X + ] I N . (<label>15</label></formula><formula xml:id="formula_25">)</formula><p>In ( <ref type="formula" target="#formula_24">15</ref>), X + is a (D + 1) × N matrix by connecting</p><formula xml:id="formula_26">I T D+1</formula><p>to the bottom of X . However, it is not applicable in our VI framework due to the analytically intractable normalization factor in <ref type="bibr" target="#b43">(44)</ref>. Because is positive, we adopt Gamma prior distributions to approximate conjugate prior for as well.</p><p>By assuming that the parameters of inverted Dirichlet distribution are mutually independent, we have</p><formula xml:id="formula_27">p() = Gam(; U, V ) = ∞ m=1 D+1 d=1 v u md md (u md ) α u md -1 md e -v md α md (16)</formula><p>where all the hyperparameters U = {u md } and V = {v md } are positive.</p><p>With the Bayesian rules and by combining ( <ref type="formula">6</ref>) and ( <ref type="formula" target="#formula_11">8</ref>)-( <ref type="formula">16</ref>) together, we can represent the joint density of the observation X with all the i.i.d. latent variables = (Z, , λ, ϕ) as The structure of the InIDMM can be represented in terms of a graphical model in Fig. <ref type="figure" target="#fig_0">1</ref>.</p><formula xml:id="formula_28">p(X , ) = p(X |Z, ) p(Z| λ) p( λ| ϕ) p( ϕ) p() = N n=1 ∞ m=1 ⎡ ⎣ λ m m-1 j =1 (1 -λ j ) D+1 d=1 α md D+1 d=1 (α md ) × D d=1 x α md -1 nd 1 + D d=1 x nd - D+1 d=1 α md ⎤ ⎥ ⎦ z nm × ∞ m=1 ϕ m (1 -λ m ) ϕ m -1 t s m m (s m ) ϕ s m -1 m e -t m ϕ m × ∞ m=1 D+1 d=1 v u md md (u md ) α u md -1 md e -v md α md . (<label>17</label></formula><formula xml:id="formula_29">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. VARIATIONAL LEARNING FOR INIDMM</head><p>In this section, we develop a variational Bayesian inference framework for learning the InIDMM. With the assistance of recently proposed EVI <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, an analytically tractable algorithm, which prevents numerical sampling during each iteration and facilitates a training procedure, is obtained. The proposed solution is also able to overcome the problem of overfitting and automatically decide the number of mixture components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Extended Variational Inference</head><p>The purpose of the Bayesian analysis is to estimate the values of the hyperparameters as well as the posterior probability distribution of the latent variables. Within the conventional VI framework, the objective function that needs to be maximized is</p><formula xml:id="formula_30">L(q) = E q() [ln p(X , )] -E q() [ln q()]. (<label>18</label></formula><formula xml:id="formula_31">)</formula><p>For most of the non-GMMs (e.g., the BMM <ref type="bibr" target="#b6">[7]</ref>, the DMM <ref type="bibr" target="#b5">[6]</ref>, the beta-Liouville mixture model <ref type="bibr" target="#b38">[39]</ref>, and the IDMM <ref type="bibr" target="#b19">[20]</ref>), the term E q() [ln p(X , )] is analytically intractable such that the lower bound L(q) cannot be maximized directly by a closed-form solution. Therefore, the EVI method <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b45">[46]</ref> was proposed to overcome the aforementioned problem. With an auxiliary function p(X , ) that satisfies</p><formula xml:id="formula_32">E q() [ln p(X , )] ≥ E q() [ln p(X , )]<label>(19)</label></formula><p>and substituting <ref type="bibr" target="#b18">(19)</ref> into <ref type="bibr" target="#b17">(18)</ref>, we can still reach the maximum value of L(q) at some given points by maximizing a lower bound of L(q)</p><formula xml:id="formula_33">L(q) ≥ L(q) = E q() [ln p(X , )] -E q() [ln q()]. (<label>20</label></formula><formula xml:id="formula_34">)</formula><p>If p(X , ) is properly selected, an analytically tractable solution can be obtained. In order to properly formulate the variational posterior q(), we truncate the stick-breaking representation for the InIDMM at a value M as</p><formula xml:id="formula_35">λ M = 1, π m = 0 when m &gt; M,</formula><p>and</p><formula xml:id="formula_36">M m=1 π m = 1. (<label>21</label></formula><formula xml:id="formula_37">)</formula><p>Note that the model is still a full DP mixture. The truncation level M is not a part of our prior infinite mixture model, it is only a variational parameter for pursuing an approximation to the posterior, which can be freely initialized and automatically optimized without yielding overfitting during the learning process. In addition, we make use of the following factorized variational distribution to approximate p(|X ) as:</p><formula xml:id="formula_38">q() = M m=1 q(λ m )q(ϕ m ) N n=1 q(z nm ) D+1 d=1</formula><p>q(α md ), <ref type="bibr" target="#b21">(22)</ref> where the variables in the posterior distribution are assumed to be mutually independent (as illustrated by the graphical model in Fig. <ref type="figure" target="#fig_0">1</ref>). This is the only assumption we introduced to the posterior distribution. No other restrictions are imposed over the mathematical forms of the individual factor distributions <ref type="bibr" target="#b1">[2]</ref>.</p><p>Applying the full factorization formulation and the truncated stick-breaking representation for the proposed model, we can solve the variational learning by maximizing the lower bound L(q) shown in <ref type="bibr" target="#b19">(20)</ref>. The optimal solution in this case is given by ln q s ( s ) = ln p(X , ) j =s + Con.</p><p>(</p><formula xml:id="formula_39">)<label>23</label></formula><p>where • j =s refers to the expectation with respect to all the distributions q j ( j ) except for variable s. In addition, any term that does not include s are absorbed into the additive constant "Con." <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b45">[46]</ref>. In the VI, all factors q s ( s ) need to be suitably initiated, then each factor is updated in turn with a revised value obtained by <ref type="bibr" target="#b22">(23)</ref> using the current values of all the other factors. Convergence is theoretically guaranteed, since the lower bound is a convex with respect to each factor q s ( s ) <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. EVI for the Optimal Posterior Distributions</head><p>According to the principles of EVI, the expectation of the logarithm of the joint distribution, given the joint posterior distributions of the parameters, can be expressed as</p><formula xml:id="formula_40">ln p(X , ) = N n=1 M m=1 z nm R m + D d=1 (α md -1) ln x nd - D+1 d=1 α md 1 + D d=1</formula><p>x nd</p><formula xml:id="formula_41">+ ln λ m + m-1 j =1 ln(1 -λ j ) ⎤ ⎦ + M m=1 [ln ϕ m + (ϕ m -1)ln(1 -λ m )] + M m=1 D+1 d=1 [(u md -1)ln α md -v md α md ] + M m=1 [(s m -1)ln ϕ m -t m ϕ m ] + Con. (<label>24</label></formula><formula xml:id="formula_42">)</formula><p>where</p><formula xml:id="formula_43">R m = ln D+1 d=1 α md D+1 d=1 (α md )</formula><p>.</p><p>With the mathematical expression in <ref type="bibr" target="#b23">(24)</ref>, an analytically tractable solution is not feasible, which is due to the fact that R m cannot be explicitly calculated (although it can be simulated by some numerical sampling methods). In order to apply <ref type="bibr" target="#b22">(23)</ref> to explicitly calculate the optimal posterior distributions and with the principles of the EVI framework, it is required to introduce an auxiliary function Rm such that R m ≥ Rm . According to <ref type="bibr" target="#b5">[6,</ref><ref type="bibr">Eq. 25]</ref>, we can select Rm as</p><formula xml:id="formula_44">Rm = ln D+1 d=1 α md D+1 d=1 (α md ) + D+1 d=1 D+1 k=1 α md -(α md ) × [ln α md -ln α md ] α md ,<label>(25)</label></formula><p>where (•) is the digamma function defined as (a) = ∂ ln (a)/∂a. Substituting ( <ref type="formula" target="#formula_44">25</ref>) into ( <ref type="formula" target="#formula_41">24</ref>), a lower bound to ln p(X , ) can be obtained as ln p(X , )</p><formula xml:id="formula_45">= N n=1 M m=1 z nm Rm + D d=1 (α md -1) ln x nd - D+1 d=1 α md 1 + D d=1 x nd + ln λ m + m-1 j =1 ln(1 -λ j ) ⎤ ⎦ + M m=1 [ln ϕ m + (ϕ m -1)ln(1 -λ m )] + M m=1 D+1 d=1 [(u md -1)ln α md -v md α md ] + M m=1 [(s m -1)ln ϕ m -t m ϕ m ] + Con. (<label>26</label></formula><formula xml:id="formula_46">)</formula><p>With <ref type="bibr" target="#b22">(23)</ref>, we can get analytically tractable solutions for optimally estimating the posterior distributions of Z, λ, ϕ, and . We now consider each of these in more detail.</p><p>1) Posterior Distribution of q(Z): As any term that is independent of z nm can be absorbed into the additive constant, we have ln q * (z nm )</p><formula xml:id="formula_47">= Con. + z nm ⎡ ⎣ R m + ln λ m + m-1 j =1 ln(1 -λ j ) + D d=1 (α md -1) ln x nd + D+1 d=1 α md ln 1 + D d=1</formula><p>x nd <ref type="bibr" target="#b26">(27)</ref> which has the same logarithmic form of the prior distribution (i.e., the categorial distribution). Therefore, we can write ln q * (Z) as</p><formula xml:id="formula_48">ln q * (Z) = N n=1 M m=1</formula><p>z nm ln ρ nm + Con. <ref type="bibr" target="#b27">(28)</ref> with the definition that</p><formula xml:id="formula_49">ln ρ nm = ln λ m + m-1 j =1 ln(1 -λ j ) + Rm + D d=1 (α md -1) ln x nd - D+1 d=1 α md 1 + D d=1 x nd . (<label>29</label></formula><formula xml:id="formula_50">)</formula><p>Recalling that z nm ∈ (0, 1) and M m=1 z nm = 1, we define</p><formula xml:id="formula_51">r nm = ρ nm M m=1 ρ nm . (<label>30</label></formula><formula xml:id="formula_52">)</formula><p>Taking the exponential on both the sides of (28), we have</p><formula xml:id="formula_53">q * (Z) = N n=1 M m=1 r z nm nm (<label>31</label></formula><formula xml:id="formula_54">)</formula><p>which is the optimal posterior distribution of Z.</p><p>The posterior mean z nm can be calculated as z nm = r nm . Actually, the quantities {r nm } are playing a similar role as the responsibilities in the conventional EM <ref type="bibr" target="#b58">[59]</ref> algorithm.</p><p>In the following, we show only the optimal solutions to λ, ϕ, and , respectively. The derivation details can be found in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Posterior Distribution of q(</head><p>λ): The optimal solution to the posterior distribution of λ is characterized as</p><formula xml:id="formula_55">q( λ) = M m=1 Beta(λ m ; g * m , h * m ) (<label>32</label></formula><formula xml:id="formula_56">)</formula><p>where the hyperparameters s * m and q * m are</p><formula xml:id="formula_57">g * m = 1 + N n=1 z nm , h * m = ϕ m + N n=1 M j =m+1 z nj .<label>(33)</label></formula><p>3) Posterior Distribution of q( ϕ): The optimal solution to the posterior distribution of ϕ is</p><formula xml:id="formula_58">q * ( ϕ) = M m=1 Gam(ϕ m ; s * m , t * m ) (<label>34</label></formula><formula xml:id="formula_59">)</formula><p>where the optimal solutions to the hyperparamters s * m and t * m are</p><formula xml:id="formula_60">s * m = 1 + s 0 m , t * m = t 0 m -ln(1 -λ m ) (<label>35</label></formula><formula xml:id="formula_61">)</formula><p>where s 0 m and t 0 m denote the hyperparameters initialized in the prior distribution, respectively. 4) Posterior Distribution of q(): The optimal approximation to the posterior distribution of is</p><formula xml:id="formula_62">q * () = M m=1 D+1 d=1 Gam(α md ; u * md , v * md ) (36)</formula><p>where the optimal solutions to the hyperparameters u * md and v * md are given by</p><formula xml:id="formula_63">u * md = u 0 md + N n=1 z nm × K +1 k=1 α mk -(α md )</formula><p>α md <ref type="bibr" target="#b36">(37)</ref> and</p><formula xml:id="formula_64">v * md = v 0 md - N n=1 z nm ln x nd -ln 1 + D d=1 x nd . (<label>38</label></formula><formula xml:id="formula_65">)</formula><p>In the above-mentioned equations, u 0 md and v 0 md are the hyperparameters in the prior distribution and we set x n,D+1 = 1. The following expectations are needed to calculate the aforementioned update equations:</p><formula xml:id="formula_66">ln(1 -λ m ) = h * m - g * m + h * m ln λ m = g * m - g * m + t * m ln α md = u * md -ln v * md ϕ m = s * m t * m , α md = u * md v * md . (<label>39</label></formula><formula xml:id="formula_67">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Full Variational Learning Algorithm</head><p>As can be observed from the above-mentioned updating process, the optimal solutions for the posterior distributions are dependent on the moments evaluated with respect to the posterior distributions of the other variables. Thus, the variational update equations are mutually coupled. In order to obtain optimal posterior distributions for all the variables, iterative updates are required until convergence. With the obtained posterior distributions, it is straightforward to calculate the lower bound L(q) L(q) = q() ln p(, X ) q() d</p><p>= ln p(X , ) -ln q() = ln p(X , ) -ln q(Z) -ln q( λ)</p><formula xml:id="formula_68">-ln q( ϕ) -ln q() (<label>40</label></formula><formula xml:id="formula_69">)</formula><p>which is helpful in monitoring the convergence. In <ref type="bibr" target="#b39">(40)</ref>, each term with expectation (i.e., •) is evaluated with respect to all the variables in its argument as ln q(Z) = r nm ln r nm <ref type="bibr" target="#b40">(41)</ref> </p><formula xml:id="formula_70">ln q( λ) = M m=1 ln g * m + h * m -ln g * m -ln h * m + g * m -1 ln λ m + h * m -1 ln(1-λ m ) (<label>42</label></formula><formula xml:id="formula_71">)</formula><p>Algorithm 1 Algorithm for EVI-Based Bayesian InIDMM </p><formula xml:id="formula_72">ln q( ϕ) = M m=1 s * m ln t * m -ln (s * m ) + (s * m -1)ln ϕ m -t * m φm<label>(43)</label></formula><p>and</p><formula xml:id="formula_73">ln q( α) = M m=1 D+1 d=1 u * md ln v * md -ln (u * m ) + u * m -1 ln α md -v * md ᾱmd . (<label>44</label></formula><formula xml:id="formula_74">)</formula><p>In addition, ln p(X , ) is given in <ref type="bibr" target="#b25">(26)</ref>.</p><p>The algorithm of the proposed EVI-based Bayesian estimation of InIDMM is summarized in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS AND DISCUSSION</head><p>In this section, both synthesized data and real data are utilized to demonstrate the performance of the proposed algorithm for InIDMM. In the initialization stage of all the experiments, the truncation level M is set to 15 and the hyperparameters of the Gamma prior distributions are chosen as u 0 = s 0 = 1 and v 0 = t 0 = 0.005, which provide noninformative prior distributions. Note that these specific choices were based on our experiments and were found convenient and effective in our case. We take the posterior means as point estimates to the parameters in an InIDMM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Synthesized Data Evaluation</head><p>As shown in the previous studies for EVI-based Bayesian estimation <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, the SLB approximation can guarantee the convergence while the MLB approximation cannot. We use the synthesized data evaluation to compare the Bayesian InIDMM using the SLB approximation (proposed in this paper and 3 When a mixing coefficient is small enough, it converges to 0 faster. Therefore, we can remove components with very small value (less than a threshold). This choice (empirically choosing a threshold) is purely for the convenience of easy implementation. A similar strategy is also widely used and applied in many other sticking-break process-based DP mixture models <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b52">[53]</ref>.  denoted as InIDMM SLB ) with the Bayesian InIDMM using the MLB approximation (proposed in <ref type="bibr" target="#b52">[53]</ref> and denoted as InIDMM MLB ). Three models (see Table <ref type="table" target="#tab_1">I</ref> for details) were selected to generate the synthesized data sets.</p><p>1) Model Selection: One advantage of DP process mixture model is to decide the number of mixture components automatically, based on the training data. Following the instructions in <ref type="bibr" target="#b59">[60]</ref> and for a first check, we ran the proposed EVI-based method for InIDMM SLB . The optimization procedure is carried out without component elimination, i.e., a fixed number of components, M, is chosen and the mixing coefficients are fixed during iteration. (The initial value of the mixing coefficients was obtained from plain EM estimation.) Under this setting, the variational lower bound can be treated as a model selection score and the effect of the number of the mixture components is demonstrated. With synthesized data generated from the aforementioned three models, we plotted the relation between the variaional lower-bounds and the number of mixture components in Fig. <ref type="figure">3</ref>.</p><p>2) Observations of Oscillations: We ran the InIDMM MLB algorithm and monitored the value of the variational objective function during each iteration. It can be observed that the variational objective function was not always increasing in the Bayesian estimation with the InIDMM MLB . Fig. <ref type="figure" target="#fig_1">2</ref> illustrates the decreasing values during iterations. On the other hand, the variational objective function obtained with the InIDMM SLB algorithm was always increasing until convergence, as the SLB approximation insures the convergency theoretically. The observations of oscillations demonstrate that the convergence with MLB approximation cannot be guaranteed. The original variational object function was numerically calculated by employing the sampling method. In order to 3) Quantitative Comparisons: Next, we compare the InIDMM SLB with the InIDMM MLB quantitatively. With a known IDMM, 2000 samples were generated. The InIDMM SLB and the InIDMM MLB were applied to estimate the posterior distributions of the model, respectively. In Table <ref type="table" target="#tab_1">I</ref>, we list the estimated parameters by taking the posterior means. It can be observed that both the InIDMM SLB and the InIDMM MLB can carry out the estimation properly. However, with 20 repeats of the aforementioned "data generation-model estimation" procedure and calculating the variational objective function with sampling method, superior performance of the InIDMM SLB over the InIDMM MLB can be observed from Table <ref type="table" target="#tab_1">II</ref>. The mean values of the objective function obtained by InIDMM SLB are larger than those obtained by the InIDMM SLB , whereas the computational cost (measured in seconds) required by the InIDMM SLB are smaller than those required by the InIDMM MLB . Moreover, smaller KL divergences <ref type="foot" target="#foot_2">4</ref> of the estimated models from the corresponding true models also verify that the InIDMM SLB yields better estimates than the InIDMM MLB . In order to examine if the differences between the InIDMM SLB and the InIDMM MLB are statistically significant, we conducted the student's t-test with the null-hypothesis that the results obtained by these two methods have equal means and equal but unknown variances. All the p-values of in Table II are smaller than the  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Real Data Evaluation</head><p>In the real data evaluations, the proposed InIDMM SLB has been applied for the task of image categorization and object detection. The referred methods for comparisons are the IDMM SLB <ref type="bibr" target="#b60">[61]</ref>, the MCMC-based numerical model estimation (InIDMM MCMC , numerical simulation of the posterior distributions) <ref type="bibr" target="#b57">[58]</ref>, the DP GMM (InGMM, another commonly used statistical model) <ref type="bibr" target="#b61">[62]</ref>, and the SVM-based classifier (discriminant method, implemented with LIBSVM toolbox <ref type="bibr" target="#b62">[63]</ref>).</p><p>1) Data Sets: The evaluations were conducted based on two well-known data sets. The first data set is the Caltech-4 data set <ref type="foot" target="#foot_3">5</ref> . It is a composite of four different categories. There are 1074 images of airplanes from the side, 526 images of cars from the rear, 826 images of motorbikes from the side, and 450 frontal face images from about 27 unique persons. Example images from these four categories are shown in Fig. <ref type="figure" target="#fig_4">6(a)-(d</ref>). The second data set is the ETH-80 data set <ref type="foot" target="#foot_4">6</ref>that consists of eight categories: apple, car, cup, dog, pear, tomato, horse, and cow. Each category has 410 images which are cropped, so that they contain only the object in the center. Examples of images from each category in the ETH-80 data set are shown in Fig. <ref type="figure" target="#fig_6">8</ref>. Our experiments were evaluated on these two commonly used public data sets for the purpose of validating the effectiveness of the proposed method.</p><p>2) Descriptor Extraction: In recent years, many excellent global and local descriptors have been proposed for the purpose of image categorization and object detection. For example, the scale-invariant feature transform <ref type="bibr" target="#b63">[64]</ref> descriptor, the local binary pattern descriptor <ref type="bibr" target="#b64">[65]</ref>, and the histogram of oriented gradient (HOG) descriptor <ref type="bibr" target="#b65">[66]</ref>. The HOG descriptor, among others, has been one of the most popular and effective one for image categorization or detection <ref type="bibr" target="#b66">[67]</ref>, <ref type="bibr" target="#b67">[68]</ref>. In this paper, we employ the rectangular HOG (R-HOG) descriptor <ref type="bibr" target="#b68">[69]</ref>, an variant and improved version of HOG. With the principles of R-HOG and by considering seven windows and nine histogram bins, each image is represented by a 441-D positive feature vector.</p><p>3) Image Categorization: Object categorization refers to classifying a given image into a specific category, such as car, face, motorbike, and airplane. It can also be considered as an image categorization problem <ref type="bibr" target="#b69">[70]</ref>, which is an important and challenging problem in a wide range of application areas, such as multimedia retrieval, pattern recognition, and computer vision. Image categorization and its related applications have attracted considerable attention during the past few years <ref type="bibr" target="#b70">[71]</ref>- <ref type="bibr" target="#b74">[75]</ref>. The reason that image categorization has emerged as one of the most active areas in the fields of image understanding and computer vision is mainly because its large potential in web image research, video retrieval, image database annotation, and medical image mining. Although human usually perform well on the task of image categorization, it remains difficult for computers to achieve similar performance. This is due to the various poses, different scales, and multiple viewpoints.</p><p>Our experiments for image categorization were implemented as follows. First, R-HOG descriptors were extracted from each image. Each image in the data sets was then represented by a 441-D positive vector. Second, the vectors from one category are assumed to be generated from an InIDMM. Each category has been randomly divided into equal training and test sets. For each category, InIDMM was trained based on the training set. Third, the proposed Bayesian InIDMM was employed as a classifier to categorize objects by assigning the test image to a given class that has the highest posterior probability. Table III lists the average categorization accuracies. It can be observed that the proposed InIDMM SLB is superior to all the other referred methods. In order to remove the randomness effect in the results, we conducted 10 rounds of simulations and the mean values with the standard deviations are reported. The accuracy distributions are shown in Fig. <ref type="figure" target="#fig_5">7</ref>.</p><p>4) Object Detection: Object detection is another essential problem in computer vision and has been commonly applied in various applications, such as content-based image retrieval, intelligent traffic management, driver-assistance system, and video surveillance <ref type="bibr" target="#b75">[76]</ref>, <ref type="bibr" target="#b76">[77]</ref>. The main goal of object detection is to find instances of real-world objects, such as car,  face, or bicycle in an image or a video clip. Typical object detection algorithms apply the extracted features and employ the learning algorithms to recognize the instances from an object class. Here, we apply the proposed InIDMM as a classifier and study its performance in object detection. Similar as image categorization, we also applied the R-HOG descriptor to represent an image. Each image in the data set was represented by a 441-D positive feature vector.</p><p>For the experiments on the Caltech-4 data set, we evaluated the detection performance on the four subdata sets mentioned in Section IV-B3. In addition to these four data sets, we used the Caltech background subdata set (451 images) as the nonobject subdata set for these four object subclasses. Sample images from each of these four object classes and the Caltech background data set are shown in Fig. <ref type="figure" target="#fig_4">6</ref>.  The proposed InIDMM is utilized as a classifier to detect the objects through assigning the testing image to a given group (object or nonobject). Table <ref type="table" target="#tab_3">IV</ref> summarizes the detection accuracies. It can be observed from these results that the InIDMM provides the best detection accuracies compared with the other methods. During the evaluations, each of the aforementioned subdata sets were separated randomly into two halves, one for training and the other one for test. Ten rounds of simulations were conducted, and the mean values with the standard deviations are reported. Fig. <ref type="figure" target="#fig_7">9</ref> illustrates the distributions of the detection accuracies.</p><p>5) Computational Efficiency: As emphasized in Section I, one motivation of applying the EVI framework to derive analytically tractable solution for InIDMM such that the computational cost can be reduced, compared with numerical solution. In Table V, we compare the required runtime for InIDMM SLB and InIDMM MCMC . Ten rounds of simulations were conducted, and the mean values are reported. The p-values of the student's t-test with the null-hypothesis that the runtimes of InIDMM SLB and InIDMM MCMC have equal means but unknown variances are listed. It can be concluded that the proposed InIDMM SLB has statistically significantly superior performance in terms of runtime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>The inverted Dirichlet distribution has been widely applied in modeling the positive vector (vector that contains only positive elements). The Dirichlet processing mixture of the IDMM (InIDMM) can provide a good modeling performance to the positive vectors. Compared with the conventional finite IDMM, the InIDMM has more flexible model complexity as the number of mixture components can be automatically determined. Moreover, the overfitting and underfitting problem is avoided by the Bayesian estimation of InIDMM. To obtain an analytically tractable solution for the Bayesian estimation of InIDMM, we utilized the recently proposed EVI framework. With SLB approximation, the convergence of the proposed analytically tractable solution is guaranteed, while the solution obtained via MLB approximations may result in oscillations of the objective function. Extensive synthesized data evaluations and real data evaluations demonstrated the superior performance of the proposed method.</p><p>For the future work, it is of sufficient interest to connect and compare the conventional mixture model with deep neural network, such as deep unfolding for topic model <ref type="bibr" target="#b77">[78]</ref>, <ref type="bibr" target="#b78">[79]</ref>, mixture of probabilistic linear discriminant analysis model <ref type="bibr" target="#b79">[80]</ref>, <ref type="bibr" target="#b80">[81]</ref>. It is also interesting to extend this paper in the task of specific image object recognition, e.g., text object in scene images and web videos <ref type="bibr" target="#b81">[82]</ref>, <ref type="bibr" target="#b82">[83]</ref>. </p><p>In the above-mentioned equations, u 0 md and v 0 md are the hyperparameters in the prior distribution and we set x n,D+1 = 1.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Graphical representation of the variables relationships in the Bayesian inference of an InIDMM. The circles represent variables. Arrows show the relationships between variables. The variables in the box are i.i.d..</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Observations of the objective function's oscillations during iterations. This nonconvergence indicates that the MLB approximation-based method cannot theoretically guarantee convergence. The model settings are the same as in Table I. (a) Model A. (b) Model B. (c) Model C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>3 .Fig. 4 .</head><label>34</label><figDesc>Fig. 4. Illustration of the variational objective function's values obtained by SLB against the number of iterations. (a) Model A. (b) Model B. (c) Model C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Boxplots for comparisons of the objective function values' distributions obtained by SLB and MLB with different models. The model settings are the same as those in Table I. The central mark is the median, and the edges of the box are the 25th and 75th percentiles. The outliers are marked individually. (a) Model A. (b) Model B. (c) Model C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Sample images from the Caltech-4 data set. (a) Airplane. (b) Motorbike. (c) Face. (d) Car. (e) Background.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>7 .</head><label>7</label><figDesc>Boxplots for comparisons of the categorization accuracies' distributions for the Caltech-4 and the ETH-80 data sets. The central mark is the median, and the edges of the box are the 25th and 75th percentiles. The outliers are marked individually. (a) Caltech-4. (b) ETH-80.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Sample images from ETH-80 data set. (a) Apple. (b) Car. (c) Cow. (d) Cup. (e) Dog. (f) Horse. (g) Pear. (h) Tomato.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Boxplots for comparisons of the detection accuracies' distributions for the Caltech-4. The central mark is the median, and the edges of the box are the 25th and 75th percentiles. The outliers are marked individually. (a) Airplane. (b) Face. (c) Car. (d) Motorbike.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>The optimal solution to the posterior distribution of λ m is given by ln q(λ m ) = Con. + ln λ m N n=1 z nm + ln(1 -λ m ) logarithmic form of the beta prior distribution. Hence, the optimal posterior distribution is q( For variable ϕ m , we haveln q * (ϕ m ) = Con. + s m ln ϕ m + [ln(1 -λ m )t m ]ϕ m . (48)It can be observed that (48) has the logarithmic form of the Gamma prior distribution. By taking the exponential on both the sides of (48), we have solutions to the hyperparamters s * m and t * m ares * m = 1 + s 0 m , t * m = t 0 m -ln(1 -λ m ) (50)where s 0 m and t 0 m denote the hyperparameters initialized in the prior distribution, respectively.C. Posterior Distribution of q()Similar to the above-mentioned derivations, for the variable α md , 1 ≤ d ≤ D + 1, the optimal approximation to the posterior distribution is ln q * (α md ) distribution of α md has the logarithmic form of the Gamma distribution, we haveq * () = md ; u * md , v * md ) (52)where the optimal solutions to the hyperparameters u * md and v * md are given by u * md = u 0 md +</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>1: Set the initial truncation level M and the initial values for hyperparameters s 0 m , t 0 m , u 0 md , and v 0 md 2: Initialize the values of r nm by K-means algorithm. Renormalize { π m } to have a unit l 1 norm. 10: Calculate α md = u * md /v * md for all m and d.</figDesc><table><row><cell>3: repeat</cell></row><row><cell>4: Calculate the expectations in (39).</cell></row><row><cell>5: Update the posterior distributions for each variable by</cell></row><row><cell>(47), (50), (53) and (54).</cell></row><row><cell>6: until Stop criterion is reached.</cell></row><row><cell>7: For all m, calculate λ m = s  *  m /(s  *  m + t  *  m ) and substitute</cell></row><row><cell>it back into (11) to get the estimated values of the mixing</cell></row><row><cell>coefficients π m .</cell></row><row><cell>8: Determine the optimum number of components M by</cell></row><row><cell>eliminating the components with mixing weights smaller</cell></row><row><cell>than 10 -5 . 3</cell></row><row><cell>9:</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I COMPARISONS</head><label>I</label><figDesc>OF TRUE AND ESTIMATED MODELS</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III COMPARISONS</head><label>III</label><figDesc>OF IMAGE CATEGORIZATION ACCURACIES (IN %) OBTAINED WITH DIFFERENT MODELS. THE STANDARD DEVIATIONS ARE IN THE BRACKETS. THE p-VALUES OF THE STUDENT'S t-TEST WITH THE NULL-HYPOTHESIS THAT INIDMM SLB AND THE REFERRING METHOD HAVE EQUAL MEANS BUT UNKNOWN VARIANCES ARE LISTED</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV COMPARISONS</head><label>IV</label><figDesc>OF OBJECT DETECTIONS ACCURACIES (IN %) ON CALTECH-4 DATA SET. THE STANDARD DEVIATIONS ARE IN THE BRACKETS. THE p-VALUES OF THE STUDENT'S t-TEST WITH THE NULL-HYPOTHESIS THAT INIDMM SLB AND THE REFERRING METHOD HAVE EQUAL MEANS BUT UNKNOWN VARIANCES ARE LISTED</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V COMPARISONS</head><label>V</label><figDesc>OF RUNTIME (IN s) † FOR INIDMM SLB AND INIDMM MCMC</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Here, model selection means selecting the best of a set of models of different orders</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>To avoid confusion, we use f (x; a) to denote the pdf of x parameterized by parameter a. f (x|a) is used to denote the conditional pdf of x given a, where both x and a are random variables. Both f (x; a) and f (x|a) have exactly the same mathematical expressions.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>Here, the KL divergence is calculated as KL( p(X |) p(X | )) by the sampling method.denotes the point estimate of the parameters from the posterior distribution.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>http://www.vision.caltech.edu/archive.html.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>http://www.d2.mpi-inf.mpg.de/datasets/ETH80.</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the National Key Research and Development Program of China under Grant 2016YFB1001000, in part by the National Natural Science Foundation of China under Grant 61773071, in part by the Beijing Nova Program under Grant Z171100001117049, in part by the Beijing Nova Program Interdisciplinary Cooperation under Project Z181100006218137, in part by the Beijing Natural Science Foundation under Grant 4162044, and in part by the Joint Funding of the Beijing Natural Science Foundation and the Beijing Education Commission under Grant KZ201810009011.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Zhanyu Ma (M'08-SM <ref type="bibr">'17)</ref>   </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Everitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hand</surname></persName>
		</author>
		<title level="m">Finite Mixture Distributions</title>
		<meeting><address><addrLine>London, U.K.</addrLine></address></meeting>
		<imprint>
			<publisher>Chapman &amp; Hall</publisher>
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Pattern Recognition and Machine Learning (Information Science and Statistics)</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised learning of a finite mixture model based on the Dirichlet distribution and its application</title>
		<author>
			<persName><forename type="first">N</forename><surname>Bouguila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ziou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vaillancourt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1533" to="1543" />
			<date type="published" when="2004-11">Nov. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Modeling and analysis of wireless channels via the mixture of Gaussian distribution</title>
		<author>
			<persName><forename type="first">B</forename><surname>Selim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Alhussein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Muhaidat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">K</forename><surname>Karagiannidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Veh. Technol</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="8309" to="8321" />
			<date type="published" when="2016-10">Oct. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Variational Bayesian matrix factorization for bounded support data</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Teschendorff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Leijon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="876" to="889" />
			<date type="published" when="2015-04">Apr. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bayesian estimation of Dirichlet mixture model with variational inference</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Rana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Taghia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Flierl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Leijon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3143" to="3157" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bayesian estimation of beta mixture models with variational inference</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Leijon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2160" to="2173" />
			<date type="published" when="2011-11">Nov. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Supervised hash coding with deep neural network for environment perception of intelligent vehicles</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="284" to="295" />
			<date type="published" when="2018-01">Jan. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Effective uyghur language text detection in complex background images for traffic prompt identification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="220" to="229" />
			<date type="published" when="2018-01">Jan. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Decorrelation of neutral vector variables: Theory and applications</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Leijon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="129" to="143" />
			<date type="published" when="2018-01">Jan. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A highly parallel framework for HEVC coding unit partitioning tree decision on many-core processors</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="573" to="576" />
			<date type="published" when="2014-05">May 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient parallel framework for HEVC motion estimation on many-core processors</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2077" to="2089" />
			<date type="published" when="2014-12">Dec. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Robust text-independent speaker identification using Gaussian mixture speaker models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Rose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Speech Audio Process</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="72" to="83" />
			<date type="published" when="1995-01">Jan. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Variational learning for Gaussian mixture models</title>
		<author>
			<persName><forename type="first">N</forename><surname>Nasios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Bors</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern. B, Cybern</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="849" to="862" />
			<date type="published" when="2006-08">Aug. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Capacity and error probability analysis of diversity reception schemes over generalized-K fading channels using a mixture gamma distribution</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-R</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Wireless Commun</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4721" to="4730" />
			<date type="published" when="2014-09">Sep. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bayesian estimation of the von-Mises Fisher mixture model with variational inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Taghia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Leijon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1701" to="1715" />
			<date type="published" when="2014-09">Sep. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Model-based clustering of DNA methylation array data: A recursive-partitioning algorithm for high-dimensional data arising as a mixture of beta distributions</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Houseman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">365</biblScope>
			<date type="published" when="2008-09">Sep. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Keep it simple with time: A reexamination of probabilistic topic detection models</title>
		<author>
			<persName><forename type="first">Q</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E.-P</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1795" to="1808" />
			<date type="published" when="2010-10">Oct. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Positive vectors clustering using inverted Dirichlet finite mixture models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bdiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bouguila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1869" to="1882" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bayesian learning of inverted Dirichlet mixtures for SVM kernels generation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bdiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bouguila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput. Appl</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1443" to="1458" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Non-negative matrix factorization with sparseness constraints</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1457" to="1469" />
			<date type="published" when="2004-12">Dec. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Effective document-level features for Chinese patent word segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Annu. Meeting Assoc. Comput. Linguistics (ACL)</title>
		<meeting>Annu. Meeting Assoc. Comput. Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="199" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A mobility analytical framework for big mobile data in densely populated area</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Veh. Technol</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1443" to="1455" />
			<date type="published" when="2017-02">Feb. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Characterizing flow, application, and user behavior in mobile networks: A framework for mobile big data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">M</forename><surname>Fadlullah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Wireless Commun</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="40" to="49" />
			<date type="published" when="2018-02">Feb. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Joint patch and multi-label learning for facial action unit detection</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-S</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2015-06">Jun. 2015</date>
			<biblScope unit="page" from="2207" to="2216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Joint patch and multi-label learning for facial action unit and holistic expression recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-S</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3931" to="3946" />
			<date type="published" when="2016-08">Aug. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Joint parsimonious modeling and model order selection for multivariate Gaussian mixtures</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Markley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Signal Process</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="548" to="559" />
			<date type="published" when="2010-06">Jun. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An EM approach to MAP solution of segmenting tissue mixtures: A numerical analysis</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="297" to="310" />
			<date type="published" when="2009-02">Feb. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unsupervised selection of a finite Dirichlet mixture model: An MML-based approach</title>
		<author>
			<persName><forename type="first">N</forename><surname>Bouguila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ziou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="993" to="1009" />
			<date type="published" when="2006-08">Aug. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A review of deterministic approximate inference techniques for Bayesian machine learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput. Appl</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">7-8</biblScope>
			<biblScope unit="page" from="2039" to="2050" />
			<date type="published" when="2013-12">Dec. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Bayesian information criterion for source enumeration in large-scale adaptive antenna array</title>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-K</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Veh. Technol</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="3018" to="3032" />
			<date type="published" when="2016-05">May 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Using Akaike information criterion for selecting the field distribution in a reverberation chamber</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Electromagn. Compat</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="664" to="670" />
			<date type="published" when="2013-08">Aug. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A Bayesian analysis of some nonparametric problems</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Ferguson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Statist</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="209" to="230" />
			<date type="published" when="1973-03">Mar. 1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Mixtures of Dirichlet processes with applications to Bayesian nonparametric problems</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Antoniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Statist</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1152" to="1174" />
			<date type="published" when="1974-11">Nov. 1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">L</forename><surname>Hjort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Walker</surname></persName>
		</author>
		<title level="m">Bayesian Nonparametrics</title>
		<meeting><address><addrLine>Cambridge, U.K.</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge Univ. Press</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Online variational inference for the hierarchical Dirichlet process</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Paisley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="752" to="760" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Truly nonparametric online variational inference for hierarchical Dirichlet processes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bryant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Sudderth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2708" to="2716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A survey of non-exchangeable priors for Bayesian nonparametric models</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Foti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="359" to="371" />
			<date type="published" when="2015-02">Feb. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Online learning of a Dirichlet process mixture of beta-Liouville distributions via variational inference</title>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bouguila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1850" to="1862" />
			<date type="published" when="2013-11">Nov. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The infinite Student&apos;s t-mixture for robust modeling</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal Process</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="page" from="224" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A Dirichlet process mixture of generalized Dirichlet distributions for proportional data modeling</title>
		<author>
			<persName><forename type="first">N</forename><surname>Bouguila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ziou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="107" to="122" />
			<date type="published" when="2010-01">Jan. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The infinite student&apos;s t-factor mixture analyzer for robust clustering and classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4346" to="4357" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The infinite hidden Markov random field model</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Chatzis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tsechpenakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1004" to="1014" />
			<date type="published" when="2010-06">Jun. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Wedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markov</forename><surname>Chain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monte</forename><surname>Carlo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="925" to="930" />
			<pubPlace>Boston, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A survey of stochastic simulation and optimization methods in signal processing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pereyra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Signal Process</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="224" to="241" />
			<date type="published" when="2016-03">Mar. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">An introduction to variational methods for graphical models</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="183" to="233" />
			<date type="published" when="1999-11">Nov. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Variational inference for Watson mixture model</title>
		<author>
			<persName><forename type="first">J</forename><surname>Taghia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Leijon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1886" to="1900" />
			<date type="published" when="2015-09">Sep. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Spoofing detection in automatic speaker verification systems using DNN classifiers and dynamic acoustic features</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNNLS.2017.2771947</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst., to be published</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Cross-modal subspace learning for fine-grained sketchbased image retrieval</title>
		<author>
			<persName><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">278</biblScope>
			<biblScope unit="page" from="75" to="86" />
			<date type="published" when="2018-02">Feb. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Stochastic variational inference</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Paisley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1303" to="1347" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">An adaptive learning rate for stochastic variational inference</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
		<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2013-02">Feb. 2013</date>
			<biblScope unit="page" from="298" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Nested hierarchical Dirichlet processes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Paisley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="256" to="270" />
			<date type="published" when="2015-02">Feb. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Topic novelty detection using infinite variational inverted Dirichlet mixture models</title>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bouguila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf</title>
		<meeting>IEEE Int. Conf</meeting>
		<imprint>
			<date type="published" when="2015-12">Dec. 2015</date>
			<biblScope unit="page" from="70" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Introduction to the Dirichlet distribution and related processes</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Frigyik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kapila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Gupta</surname></persName>
		</author>
		<idno>UWEETR-2010-0006</idno>
	</analytic>
	<monogr>
		<title level="j">Dept. Elect. Eng., Univ. Washington</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<pubPlace>Seattle, WA, USA</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A constructive definition of Dirichlet priors</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sethuraman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statist. Sin</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="639" to="650" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">The student&apos;s t-hidden Markov model with truncated stick-breaking priors</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="355" to="358" />
			<date type="published" when="2011-06">Jun. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Hidden Markov models with stick-breaking priors</title>
		<author>
			<persName><forename type="first">J</forename><surname>Paisley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3905" to="3917" />
			<date type="published" when="2009-10">Oct. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">An infinite mixture of inverted Dirichlet distributions</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bdiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bouguila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Neural Inf. Process</title>
		<meeting>Int. Conf. Neural Inf. ess</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="71" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the EM algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Roy. Stat. Soc</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Variational Bayesian model selection for mixture distribution</title>
		<author>
			<persName><forename type="first">A</forename><surname>Corduneanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 8th Int. Conf</title>
		<meeting>8th Int. Conf</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="27" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Variational Bayesian inference for finite inverted Dirichlet mixture model and its application to object detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chin. J. Electron</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="603" to="610" />
			<date type="published" when="2018-05">May 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Background subtraction with Dirichlet process mixture models</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S F</forename><surname>Haines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="670" to="683" />
			<date type="published" when="2014-04">Apr. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">LIBSVM: A library for support vector machines</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Intell. Syst. Technol</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Local pyramidal descriptors for image recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Seidenari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Serra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Del Bimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1033" to="1040" />
			<date type="published" when="2014-05">May 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Pairwise rotation invariant co-occurrence local binary pattern</title>
		<author>
			<persName><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2199" to="2213" />
			<date type="published" when="2014-11">Nov. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
			<date type="published" when="2005-06">Jun. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Video anomaly detection in real time on a power-aware heterogeneous platform</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Blair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2109" to="2122" />
			<date type="published" when="2016-11">Nov. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Evaluation and acceleration of high-throughput fixed-point object detection on FPGAs</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Najjar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1051" to="1062" />
			<date type="published" when="2015-06">Jun. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Trainable classifier-fusion schemes: An application to pedestrian detection</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">L</forename><surname>Junior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Delgado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gonçalves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Nunes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 12th Int</title>
		<meeting>12th Int</meeting>
		<imprint>
			<date type="published" when="2009-10">Oct. 2009</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Image categorization by learning a propagated graphlet path</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="674" to="685" />
			<date type="published" when="2016-03">Mar. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Attribute-based classification for zero-shot visual object categorization</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="453" to="465" />
			<date type="published" when="2014-03">Mar. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Creating efficient visual codebook ensembles for object categorization</title>
		<author>
			<persName><forename type="first">H.-L</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern. A, Syst. Humans</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="238" to="253" />
			<date type="published" when="2011-03">Mar. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Scale-invariant visual language modeling for object categorization</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">S</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="286" to="294" />
			<date type="published" when="2009-02">Feb. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Sparse color interest points for image retrieval and object categorization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Stottinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2681" to="2692" />
			<date type="published" when="2012-05">May 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Object classification by fusing SVMs and Gaussian mixtures</title>
		<author>
			<persName><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2476" to="2484" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Learning to detect anomalies in surveillance video</title>
		<author>
			<persName><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1477" to="1481" />
			<date type="published" when="2015-09">Sep. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Person surveillance using visual and infrared imagery</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Krotosky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1096" to="1105" />
			<date type="published" when="2008-08">Aug. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Deep unfolding inference for supervised topic model</title>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-T</forename><surname>Chien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP)</title>
		<meeting>IEEE Int. Conf. Acoust., Speech Signal ess. (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2016-03">Mar. 2016</date>
			<biblScope unit="page" from="2279" to="2283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Deep unfolding for topic models</title>
		<author>
			<persName><forename type="first">J.-T</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="318" to="331" />
			<date type="published" when="2018-02">Feb. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Mixture of PLDA for noise robust i-vector speaker verification</title>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Mak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-T</forename><surname>Chien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="130" to="142" />
			<date type="published" when="2016-01">Jan. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Dnn-driven mixture of PLDA for robust speaker verification</title>
		<author>
			<persName><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Mak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-T</forename><surname>Chien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio, Speech, Language Process</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1371" to="1383" />
			<date type="published" when="2017-06">Jun. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Text detection and recognition in imagery: A survey</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1480" to="1500" />
			<date type="published" when="2015-07">Jul. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Text detection, tracking and recognition in video: A comprehensive survey</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2752" to="2773" />
			<date type="published" when="2016-06">Jun. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
