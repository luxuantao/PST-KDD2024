<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hidden State and Reinforcement Learning with Instance-Based State Identification McCallum</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">R</forename><surname>Andrew</surname></persName>
							<email>mccallum@cs.rochester.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Univer-sity of Rochester</orgName>
								<address>
									<postCode>14627-0226</postCode>
									<settlement>Rochester</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hidden State and Reinforcement Learning with Instance-Based State Identification McCallum</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">ED514330606761F699AFB122208A782A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Real robots with real sensors are not omniscient. When a robot's next course of action depends on information that is hidden from the sensors because of problems such as occlusion, restricted range, bounded field of view and limited attention, we say the robot suffers from the hidden state problem. State identification techniques use history information to uncover hidden state. Some previous approaches to encoding history include: finite state machines [12], [28], recurrent neural networks 1251 and genetic programming with indexed memory [49]. A chief disadvantage of all these techniques is their long training time.</p><p>This paper presents instance-based state identification, a new approach to reinforcement learning with state identification that learns with much fewer training steps. Noting that learning with history and learning in continuous spaces both share the property that they begin without knowing the granularity of the state space, the approach applies instance-based (or "memory-based") learning to history sequences-instead of recording instances in a continuous geometrical space, we record instances in actionpercept-reward sequence space.</p><p>The first implementation of this approach, called Nearest Sequence Memory, learns with an order of magnitude fewer steps than several previous approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>HIS paper is about embedded agents that cannot perceive T their whole environment at once, Robotics has seen a surge of interest in reactive robots-agents that use their current sensor values to choose their next action-but as the tasks get more complex, we are finding that some tasks cannot be solved with perception-to-action mappings alone. Sensors are inherently limited, and sometimes the world state information relevant to choosing the next action is hidden by those limitations.</p><p>There are many reasons why important features can be hidden from a robot's perception: sensors have noise, limited range and limited field of view; occlusions hide areas from sensing; limited funds and space prevent equipping the robot with all desired sensors; an exhaustible power supply deters the robot from using all sensors all the time; and the robot has limited computational resources for turning raw sensor data into usable percepts.</p><p>In situations where immediate perception does not supply enough information, short-term memory of past percep-A space station repair robot sees that it needs a replacement part and thrusts across the station toward a supply bin to get the new part. However, once at the bin, occlusions and limited range prevent the robot from still seeing the distant faulty part. Given only the percepts available from its position at the supply bin, the robot cannot know which part it needs. With short-term memory, however, the robot could remember which part was faulty, and pick the correct part. A hospital delivery robot is making its way from the patient's room to the blood lab. During its travel, the robot's limited sensors are not able to distinguish between many of the different hallway intersections, some in which it should turn right and others in which it should turn left. Given this confusion, the robot cannot reliably deliver the sample. With short-term memory, however, the robot could distinguish identical-looking intersections by remembering features of its previous locations. For instance, the robot may know that it should turn right at the upcoming intersection because it remembers that it just passed the elevator doors and because the elevator doors are not next to a left-turn intersection. Unique infrared transceivers installed at the intersections in the hospital might avoid the need for memory, but this installation may not be possible because of another important perceptual limitation listed above: limited funds. The "hardware solution" that uses many transceivers can be considerably more expensive than a "software solution" that uses short-term memory. A robot driver uses a vision system to navigate in traffic. In order to overcome limited computational power for extracting usable perceptual information from its raw image data, (i.e., the last perceptual limitation in the list above), the driver makes use of an active vision system [31, [40]; it has movable binocular cameras, each containing a fovea. The high resolution in the fovea is necessary for recognizing objects, but if the cameras had such fine-grained resolution over a broad field of view, the vision system would be inundated with far more pixels than its limited computational power could handle. The driver should pass a car on the highway only when it is close to the preceding car and the blind spot is clear of cars. Unfortunately, addressing the computational limitation has intensified another limitation: limited field of view. The driver cannot see both the car in front and the blind spot at the same time. Since neither a close-by car ahead nor a clear blind spot alone are enough to tell the robot that it should pull into the passing lane, and since no matter how the robot redirects its active vision system, no immediate perception can include both inputs to the conjunction, the driver cannot decide to pass using immediate perception alone. With short-term memory, the driver would look at the car ahead, remember that it was close enough for passing, look at the blind-spot for a clear way, and once it had both elements of the relevant conjunction, it could make the decision to begin passing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Principles</head><p>We propose a "middle path" between purely reactive agents. which suffer from hidden state problems, and traditional planning agents, which require infeasible complete world models. The agent must represent the world state with imore than its current perceptions, but the agent does not require a full predictive model of the entire environment.</p><p>The work described in this paper addresses the issue of hidden state and short-term memory in conjunction with the following principles:</p><p>Agents should learn their tasks. We would prefer that our robots learn their behaviors rather than have them hard-coded because programming all the details by hand is tedious, we may not know the environment ahead of time, and we want the robot to adapt its behavior as the environment changes. Agents should learn in as few trials as possible. Learning experience is expensive in terms of wall clock time, and dangerous in terms of potential damage to the robot and its surroundings. Experience is often relatively more expensive than storage and computation. Furthermore, the expense of computation and storage will only decrease as advances in computing hardware continue; the cost of experience will not.</p><p>Agents should be able to handle noisy perceptions and actions. Strict optimality is not necessary. We should aim for reasonably good behavior given the complexity of the task. hi many cases, it's not worth running an algorithm that will take ten times more trials to learn good performance, in order to gain only a one-twentieth increase in the that performance. For example, in a changing environment, this is especially true. This paper introduces a new family of methods for reinforcement learning with short-term memory. The first implementation of this approach learns with about an order of magnitude fewer steps than several previous approaches. The paper also suggests how future methods in the family could perform even better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">BACKGROUND AND RELATED WORK</head><p>A reinforcement learning' agent suffers from hidden state if at any time the agent's state representation is missing information needed to determine the next correct action. More formally, we say a reinforcement learning agent suffers from the hidden state problem if the agent's state representation is non-Markovian with respect to actions and utility.</p><p>The hidden state problem arises as a case of perceptual aliasing: the mapping between states of the world and sensations of the agent is not one-to-one <ref type="bibr">[53]</ref>. If the agent's perceptual system produces the same outputs for two world states in which different actions are required, and if the agent's state representation consists only of its percepts, then the agent will fail to choose correct actions.</p><p>Note that even if the agent's representation consists of more than its percepts-that is, the agent maintains some internal state-it is still possible for the agent to suffer from the hidden state problem. This possibility occurs whenever the agent maintains some amount of internal state, but not enough internal state to disambiguate the aliased worId states for the given task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Stateless or Fixed-Memory Agents</head><p>Some approaches to the hidden state problem do not attempt to disambiguate the aliased states.</p><p>The simplest approach is to simply ignore the hidden state problem and apply traditional reinforcement learning methods as if there were no aliased states. Some experience has shown that in certain non-Markovian environments this approach can work [6], however, there are many cases in which ignoring hidden state results in complete failure; see [52], [46], <ref type="bibr" target="#b30">[31]</ref> for examples with explanations.</p><p>A more careful solution to the hidden state problem is to avoid passing through the perceptually aliased states. This is the approach taken in Whitehead's Lion algorithm <ref type="bibr">[52]</ref>. Whenever the agent finds a state that delivers inconsistent reward, it sets that state's utility so low that the policy will never visit it again. The success of this algorithm depends on a deterministic world and on the existence of a path to the goal that consists of only unaliased states.</p><p>Other solutions do not avoid aliased states, but do as best they can given a non-Markovian state representation while also evading the disasters that would result from ignoring the hidden state altogether <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr">[46]</ref>, <ref type="bibr" target="#b20">[21]</ref>. They involve either learning deterministic policies that execute incorrect actions in some aliased states, or learning stochastic policies that, in aliased states, may execute incorrect actions with some probability. These approaches do not depend on a path of unaliased states, but they have other limitations: when faced with many aliased states requiring different actions, the probability of executing an optimal sequence of actions falls precipitously; when faced with potentially harmful results from This paper assumes that the reader is already familiar with the principles of reinforcement learning as developed in the field of machine learning. The algorithm presented is integrally based on previous work in reinforcement learning, especially &amp;-learning <ref type="bibr">[SO]</ref>. No review of reinforcement learning basics is included in this paper, but good introductions to reinforcement learning may be found in [50], [SI, [S2], <ref type="bibr" target="#b29">[30]</ref>.</p><p>incorrect actions, deterministically incorrect or probabilistically incorrect action choice may prove too dangerous; and when faced with performance-critical tasks, inefficiency that is proportional to the amount of aliasing may be unacceptable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Agents That Learn to Use Memory</head><p>The most robust solution to the hidden state problem is to augment the agent's state representation on-line so as to disambiguate the aliased states. State identijkation techniques uncover the hidden state information-that is, they make the agent's internal state space Markovian. This transformation from an imperfect state information model to a perfect state information model has been formalized in the decision and control literature, and involves adding previous percepts and actions to the definition of agent internal state [SI. By augmenting the agent's percepts with history information-shortterm memory of past perceptions, actions and rewards-the agent can distinguish perceptually aliased states, and can then reliably choose correct actions from them.</p><p>The example tasks in Section I illustrated the use of memory to disambiguate hidden state: the robot knows which replacement part to pick because it remembers seeing the faulty part; the robot knows whether to turn right or left at the intersection because it remembers having passed the elevator door; the robot knows to pull into the passing lane because it remembers having looked both in front and behind. Using memory to disambiguate the aliased situations allows the robot to efficiently perform the task independent of the number of aliased states in that path. It allows the robot to choose correct actions reliably, as opposed to making probabilistic guesses whenever faced with ambiguous sensations.</p><p>The problem is how to represent this memory and how the agent should learn what to remember and what to forget. Because the agent begins without knowing how to perform the task at hand, the agent also begins without knowing how much memory will be required.</p><p>Predefined, fixed memory representations such as constantsized perception windows (more formally known as order n Markov models) are often undesirable. When the length of the window is more than needed, they exponentially increase the number of agent internal states for which a policy must be stored and learned; when the length of the memory is less than needed, the agent reverts to the disadvantages of undistinguished hidden state. Even if the agent designer understands the task well enough to know its maximal memory requirements, the agent is at a disadvantage with constanisized windows because, for most tasks, different amounts of memory are needed at different steps of the task.</p><p>Let us emphasize the way in which learning how much to remember is a departure from much other work in machine learning for multi-step tasks. Memory-learning agents must not only learn a mapping from states to actions (which is often difficult enough as it is), but learn the required state space as well. When confronted with hidden state, the agent can no longer rely on perception to completely define the agent's internal state space. The internal states of the agent must depend on both the current perception and some variable, learned amount of memory about past perceptions and actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Related Work in Agents That Learn to Use Memory</head><p>Several reinforcement learning algorithms are based on techniques that adapt the memory representation on-line. The Perceptual Distinctions Approach [ 121 and Utile Distinction Memory <ref type="bibr" target="#b30">[31]</ref> are both based on splitting states of a finite state machine by doing off-line analysis of statistics gathered over many steps. Recurrent-Q <ref type="bibr" target="#b23">[24]</ref> is based on training recurrent neural networks. Indexed Memory [49] uses genetic programming to evolve agents that use load and store instructions on a register bank. A chief disadvantage of all these techniques is that they require a very large number of steps for training.</p><p>Before describing these three approaches in more detail, let us consider some other algorithms that are also related.</p><p>The environment learning approach implemented in Toto</p><p>[27] also uses memory, and is impressively fast-it learns from single presentations. However, the technique is designed specifically for mobile robots inside rooms and corridors. Unlike the above four algorithms it depends integrally on dead reckoning to disambiguate states, and thus is tied to navigation.</p><p>In navigational tasks, the global state is directly correlated with geographical position, which is exactly what dead reckoning provides. A meaningful analogy to global state calculating "dead reckoning" for general, nonnavigational tasks is not apparent. ALECSYS <ref type="bibr" target="#b15">[16]</ref> is a learning classifier system implemented with a genetic algorithm <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr">[9]</ref>. In work by Dorigo and Colombetti [13], ALECSYS learns proper sequences by maintaining internal state indicatmg which subtask the agent should be performing. Transition signals produced by the trainer or environment prompt the agent to switch from one subtask to another. In other work by the same authors [17], ALECSYS is modified to include among its percepts indications of whether a sensor value changed in the last N steps, where N is a parameter of the algorithm set by the robot designer. This is a variation of the constant-sized perception window approach to memory, with the simplification that the agent remembers only the value since the last change, not all sensor values for the last N steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Finite State Muchines:</head><p>The Perceptual Distinctions Approach (PDA) [12] and Utile Distinction Memory (UDM) <ref type="bibr" target="#b30">[31]</ref> both learn partially observable Markov decision processes <ref type="bibr">[7]</ref> via state-splitting strategies. These probabilistic finite state machines (FSM's) represent memory in that certain states aren't likely to be reached except by passing through certain other states previously. So, occupation of the current state can effectively give the agent memory of the perceptions and actions associated with the only available incoming transitions to the current state-the exclusive transitions allow the agent to distinguish states even when the perception associated with those states is identical. The agent can represent arbitrarily long memories by nesting contingent transitions.2</p><p>Other research has studied how to calculate an optimal policy given a partially observable Markov decision process <ref type="bibr">[lo]</ref>. PDA and UDM, on the other hand, both emphasize the learning of the finite state machine itself, then use a simple heuristic for calculating the policy from it. The major difference between PDA and UDM is that UDM only splits states when the statistics show that doing so will help predict reward-in other words, UDM only creates as much memory as needed for the task at hand.</p><p>The finite state machine approaches have the advantage that there is much established formal study in finite state machines, hidden Markov models and partially observable Markov decision processes. One disadvantage is their limited representational power. For instance, if the agent needs to remember only one bit of information, but must remember that bit for a long time, the entire state space covered during the intervening time must be duplicated. A second disadvantage is that UDM and PDA both require an inordinate number of steps to learn the tasks on which they were demonstrated. They are both based on an Expectation-Maximization (EM) algorithm in which the learner alternates between gathering statistics using the current model, and changing the current model based on the statistics [15]. The agent begins with a random or minimal FSM model, then alternately performs trials consisting of 500 or 1000 steps, and performs state splits or joins based on the statistics from the trial. The large number of steps in each trial is required to make the experience statistically significant. Since, as is always the case with EM algorithms, statistics are based on the current (flawed) model, the requirement of some splits can't be detected until other splits are already made. The result is that only very few splits are made at the end of each trial, many trials are required to complete learning, and at 1000 steps per trial, the learning time suffers dramatically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recurrent Neural Networks:</head><p>The Recurrent-Q algorithm <ref type="bibr" target="#b23">[24]</ref> trains a recurrent neural network containing a fixed number of hidden units. The network holds memory in that the backward looping connections from the hidden units feed into the input layer-the current inputs are partially determined by the networks' own context units, which in turn were determined by previous inputs. Repeated looping of activation through these recurrent connections can maintain memory over several steps.</p><p>An advantage of using recurrent neural networks for memory is that their representational efficiency is greater than finite state machines-they can more efficiently memorize one bit We should also mention some related work that, while not associated with reinforcement learning, does provide other examples of learning finite state machines.</p><p>The map learning work described in [ 141 learns finite state machines. It is not applicable to hidden state because it requires a unique, unaliased landmark for each world state, (although the perception of the landmarks is allowed to over many steps, for instance. They also have the potential to handle continuous inputs. One disadvantage of Recurrent-Q is that, unlike finite state machines with state splitting rules, the memory capacity is fixed before learning begins since the number of hidden units is fixed. Recurrent-Q also takes very many steps to learn. Neural networks in general are not well known for quick convergence, and the convergence of recurrent neural networks is typically slower than feed forward networks.</p><p>Genetic Algorithms: Genetic Programming with Indexed Memory [49] uses genetic programming <ref type="bibr" target="#b21">[22]</ref> to evolve programs that apply load and store instructions to a fixed-sized register bank. These registers, which hold values for later retrieval, obviously supply the agent with memory.</p><p>The chief advantage of Genetic Programming with Indexed Memory is its great representational power. In fact, Teller makes a point of emphasizing that the class of potentially evolved programs is Turing complete. Unfortunately, this advantage also causes the algorithm's worst disadvantage-it pays dearly for this flexibility in terms of learning steps required. Genetic Programming with Indexed Memory can take an extremely large number of steps to learn. For example, in Teller's demonstration of the algorithm on a grid world box-pushing task, the population contains 800 individuals, the population must be evolved over 100 generations, after each generation all the individuals must be evaluated on 40 test cases that last for 100 steps each-for a total of over 3 x los steps.</p><p>Our priority is to find a new alternative that learns in fewer steps, with the understanding that we may have to sacrifice flexibility of representation. Agents often cannot afford to take the number of learning steps required for multiple off-line state-splitting tests, or the number of training presentations necessary for a recurrent neural network to converge, or the number of evaluations needed to evolve a population of solutions. Extreme flexibility in memory representation is not always necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="111.">INSTANCE-BASED STATE IDENTIFICATION</head><p>This paper advocates a new solution to the hidden state problem we term instance-based state identification. The approach was inspired by the successes of instance-based (also called "memory-based") methods for learning in continuous perception spaces, (i.e., [2], <ref type="bibr">[35]</ref>). These methods have also had success with physical robots [44], <ref type="bibr">[45]</ref>.</p><p>The term "memory-based" introduces an unfortunate conflict of vocabulary. Here "memory" does not refer to short-term memory of past perceptions and actions used to disambiguate hidden state; it refers to methods that store raw previous experiences directly, such as k-nearest neighbor [ 181 and locally weighted regression [2]. To avoid confusion, we use the phrase "instance-based," which we hope carries the same be noisv): also it does not handle siochasticallv unreliable actions. . learning without knowing the final granularity of the agent's state space. The former learns which regions of continuous input space can be represented uniformly and which areas must be finely divided among many states. The later learns which perceptions can be represented uniformly because they uniquely identify a course of action without the need for memory, and which perceptions must be divided among many states each with their own detailed history to distinguish them from other perceptually aliased world states. The first approach works with a continuous geometrical input space, the second works with a action-percept-reward "sequence" space, (or "history" space). Large continuous regions correspond to less-specified, small memories; small continuous regions correspond to more-specified, large memorie~.~ Furthermore, learning in continuous spaces and sequence spaces both have a lot to gain from instance-based methods. In situations where the state space granularity is unknown, it is especially useful to memorize the raw previous experiences. If the agent incorporates experience merely by averaging it into its current, flawed state space granularity, it is bound to attribute experience to the wrong states; experience attributed to the wrong state turns to garbage and is wasted. When faced with an evolving state space, keeping raw previous experience is the path of least commitment, and thus the most cautious about losing information.</p><p>Keeping records of raw previous experience incur certain expenses in terms of storage and computation time, but these expenses can often be justified. For instance, compare the time required to perform a million multiplications with the time it takes the robot to move down the hallway. Often learning trials are more expensive than computation-expensive in many ways: wall clock time, power consumed, danger to the robot and danger to the surroundings, for example. Techniques such as DYNA [48], Experience Replay <ref type="bibr" target="#b22">[23]</ref>, Transitional Proximity &amp;-learning <ref type="bibr" target="#b28">[29]</ref> and Asynchronous Dynamic Programming [4] are all examples of efforts to substitute world experience with storage and computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. NEAREST SEQUENCE MEMORY</head><p>There are many possible instance-based techniques to choose from, but we wanted to keep the first application simple. With that in mind, this initial algorithm is based on k-nearest neighbor. We call it Nearest Sequence Memory, (NSM). It bears emphasizing that this algorithm is the most straightforward, simple, almost naive combination of instancebased methods and history sequences that one could think of; there are still more sophlsticated instance-based methods to try. The surprising result is that such a simple technique works as well as it does.</p><p>Fig. <ref type="figure" target="#fig_1">1</ref> depicts the analogy between k-nearest neighbor in a continuous geometric space and k nearest neighbor in a sequence space.</p><p>3This analogy between continuous spaces and memory spaces provides a further perspective against implementing memory with constant-sized perception windows: Constant-sized perception windows are undesirable for precisely the same reason constant-sized discretizations of continuous spaces are undesirable. If the discretization is too coarse-grained, the agent will fail; if the discretization is too fine-grained the number of states is exponentially more than needed; different granularity is needed in different regions of state space. Both have a database of instances, and both have a point which we want to query. In each case, the "query point" from which we measue neighborhood relations is indicated with a gray cross, and the three nearest neighbors are indicated with gray shadows In a geometric space (shown on top), instances are the black dots, each indicating some vector of values in multi-dimensional space The neighborhood metric is defined by Euclidean distance In a sequence space (shown on bottom), instances are circles connected by mows, each indicating an action/percept/reward snapshot in tme, with all the snapshots laid out in time sequence order (The numbers in the clrcles indicate actionlperceptireward triples The neighborhood metric is detemned by sequence match length, that is, the number of preceding states that match the states preceding the query point By using this neighborhood function, the agent finds situations from its past that are most simlar to its current situation, where "most simlar" not only includes the current percepts but also the percepts leadmg up to current situation The agent thus has an internd state space with vanable-length short-term memory because when choosmg instances from which to extract expected reward values, it tends to prefer mstances with better matching history Any application of k-nearest neighbor consists of three parts: 1) recording each experience, 2) using some distance metric to find neighbors of the current query point, and 3) extracting output values from those neighbors. We apply these three parts to action-percept-reward sequences and reinforcement learning by Q-learning [50] as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning in</head><p>1) For each step the agent makes in the world, it records the action, perception and reward by adding a new state to a single, long cham of states. Thus, each state in the chain contains a snapshot of the agent's immediate experience at that moment, and all the agent's experiences are laid out in a time-connected history cham. 2 ) When the agent is about to choose an action, it finds states considered to be similar by loohng in its state chain for states with histories similar to the current situation. The longer a state's string of previous experiences matches the agent's most recent experiences, the more likely the state represents where the agent is now. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>v. DETAILS OF THE ALGORITHM</head><p>The interaction between the agent and its environment is described by actions, percepts and rewards. There is a finite set of possible actions, A = u l , a', . . . , U*, a finite set of possible percepts (observations), 0 = ol, 02, . . . , on, and scalar range of possible rewards, R = [z, y], 2, y E !I?. At each time step, t, the agent executes an action, at E A, then as a result receives a new percept, ot E 0, and a reward, rt E R.</p><p>We say that the percept "results" from the action in order to be consistent with models of active perception in which the percept is a function of both the world state and the previous action [3], <ref type="bibr" target="#b29">[30]</ref>. An agent with active perception can execute "perceptual" actions that redirect the agent's focus of attention.</p><p>Note also the atypical grouping of the action, percept and reward in time. In many formulations, the action is given the same time index as the percept and reward that preceded it, i.e., (Ot-1, rt-1, at-1, O t , T t , at, Ot+l, n + l , at+l,. . .). The explanation of NSM is made simpler by giving the action the same lime index as the percept and reward that follow it, i.e., (Ot--l,rt--l,at,ot,rt,at+l,Ot+1,rt+1,at+2,. . .). In this paper we use the later grouping.</p><p>Subscripts on the symbols a, 0, and s indicate the time step associated with specific actions, percepts and states. The subscript t refers the the current time step; the subscripts i and j refer to arbitrary time steps. We may add and subtract constants to indicate offsets from the time step indicated by t , i , or j .</p><p>As with most reinforcement learning, the goal of the agent at each time step is to choose the action that maximizes its expecled discounted sum of future reward, called return, and written rt.</p><p>Jus1 like other instance-based algorithms, Nearest Sequence Memory records each of its raw experiences. The action and environmental state at time t is captured in a "state" data point, written st. Recorded with that state is all the available information associated with time t: the action, at, the resulting percept, ot and resulting reward, rt. Also associated with state st is a slot to hold a single expected future discounted reward estimate, denoted q(st). This value is associated with action at and no other action. We use the notation q to indicate the expected reward value associated with an individual instance; the notation Q refers to the average of the q-values extracted from the k neighboring instances that are closest to the query point.</p><p>The Nearest Sequence Memory algorithm consists of the following steps: 1) Find the 5 nearest neighbor (most similar) states for each possible future action. The state currently at the end of the chain is the "query point" from which we measure all the distances. The neighborhood metric is defined by "string match" length; it is the number of preceding experience records that match the experience records preceding the "query point" state. This metric can be defined recursively. (Here higher values of n(s,, s 3 ) indicate that s, and s3 are closer neighbors; the notation a,-l indicates the action associated with the state preceding state si in the chain.) See (2), shown at the bottom of the page. Considering each of the possible future actions in turn, we find the k nearest neighbors and give them a vote, w( s,). Ties are resolved by preferring states toward the end of the chain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>W ( S i ) =</head><p>1, if n ( s t , 5,) is among the k maxv,31a3=ax n ( s t , sj)'s; 0, otherwise.</p><p>(3)</p><p>In the equation above, st is the current state, and the "if" expression indicates that a state gets vote 1 only if the neighborhood metric between it and the current state is among the k maximum when compared to the neighborhood similarity values of all the other states that have the same outgoing action. Determine the Q-value for -3ch action by averaging the q-values from the k voLllig states (states, s, with w(s) = 1) from which that action was executed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qt(at&gt; = ( 4 s J ) I k ) d s g ) .</head><p>(4) Vs, la3 =a, Select an action by maximum Q-value, or by random exploration. According to an exploration probability, e, either let at+l be randomly chosen from A, or at+1 = argmaxQt(a). a</p><p>(5)</p><p>Execute the action chosen in step 3, and record the resulting experience. Do this by creating a new "state" representing the current state of the environment, and storing the action-percept-reward triple associated with it:</p><p>Increment the time counter: t c t + 1. Create st; record in it at, ot, rt.</p><p>The agent can limit its storage and computational load by limiting the number of instances it maintains. Instead of allowing the number of instances to grow indefinitely with the number of experience steps, the agent c m choose to keep not more than N of them, where N is some reasonably large number like 1000. Once it reaches N instances, it would, with each step, discard the oldest instance and add the new one. The choice of N is balanced by the desire to limit storage and computation, on the one hand, versus the need for the agent to remember experiences from different parts of the environment it visits, on the other. This also provides a way to handle a changing environment. 5) Update the y-values by vote. Perform the dynamic programming step using the standard &amp;-learning rule to update those states that voted for the chosen action.</p><p>Note that this actually involves performing steps 1 and 2 to get the next &amp;-values needed for calculating the utility of the agent's current state, Ut. (Here ,f? is the learning rate.)</p><formula xml:id="formula_0">Ut = maxQt(a) a (6) ('J% I a, = at-1) Y(S2) + (1 -P 4 S d ) d S J + P V ( S % ) ( T Z + rut). (7)</formula><p>Note the use of r, instead of rt in the update of q(s,).</p><p>This helps the agent estimate an accurate average reward The agent augments the above one-step Q-learning by also passing discounted reward backward through its recorded history chain for as long as the propagation increases the states' Q-values. In practice, this does not occur often. The process could be described as a kind of "conservative" TD-X where X = 1. Nearest Sequence Memory performs two kinds of learning. Like all instance-based methods, it learns by populating its input space with raw experience. Unlike most other instancebased methods, it must also perform dynamic programming on values from raw experience. Usually instance-based methods are used as function approximators whose required outputs are the values provided by raw expenence-they are supervised learners. In Nearest Sequence Memory, the required output values are &amp;-values, which are not provided by raw experience, but related to the raw rewards through dynamic programming-Nearest Seqcence Memory is a reinforcement learner. These two kinds of learning need not always occur together. For instance, the agent could stop adding states to the chain, but continue to execute steps in the world and do the dynamic programming as implemented by Q-learning. Section VII-B-3 discusses this distinction further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENTAL RESULTS</head><p>This section shows experimental results comparing the performance of Nearest Sequence Memory to three other algorithms. Performance is demonstrated using the tasks chosen by the other algorithms' designers. In each case, Nearest Sequence Memory learns the task with roughly an order of magnitude fewer steps. Although Nearest Sequence Memory learns good policies quickly, it does not always learn optimal policies. In Section VII-B we will discuss why the policies are not always optimal and how Nearest Sequence Memory could be improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Perceptual Distinctions Approach</head><p>The Perceptual Distinctions Approach <ref type="bibr" target="#b11">[12]</ref> was demonstrated in a space ship docking application with hidden state.</p><p>The task was made difficult by noisy sensors and unreliable actions. Some of the sensors returned incorrect values 30 percent of the time. Various actions failed 70, 30 or 20 percent of the time, and when they failed, resulted in random states.</p><p>The Perceptual Distinctions Approach used learning rate p = 0.1, temporal discount factor y = 0.9, an Expectation-Maximization (EM) cycle of 1000 steps, and an exploration probability e = 0.1. Nearest Sequence Memory used learning rate / 3 = 0.1, temporal discount factor y = 0.9, maximum instance chain length N = 1000, exploration probability e = 0.1, and number of neighbors, k = 8. A higher k was chosen for this experiment than for the others because of the noisy environment. A graph of the performance during learning appears in Fig. <ref type="figure">2</ref>. Performance is indicated using the measure that the agent itself is trying to maximize: sum of future discounted reward (utility). The NSM results have been averaged over five runs. The Perceptual Distinctions Approach takes almost 8000 steps to learn the task. Nearest Sequence Memory learns a good policy in less than 1 000 steps, although the policy is not quite optimal. Not only does NSM learn the task in fewer steps than the Perceptual Distinctions Approach, it also learns the task with less computation. The Baum-Welch procedure [39] is an integral part of the Perceptual Distinctions Approach, and Baum-Welch takes time and space O(ISI21AA(N), where IS1 is the number of states in the model, IAl is the number of actions For both algorithms N is 1000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Utile Distinction Memory</head><p>Utile Distinction Memory [3 11 was demonstrated on several local perception mazes. Unlike most reinforcement learning maze domains, the agent does not perceive global row and column numbers, but only indications of whether there is a barrier to the immediately adjacent up, down, right or left. Therefore, many maze positions are perceived as identical-including positions from which the agent must execute different actions.</p><p>Each time the agent reaches the goal, it is reset to a random location in the maze.</p><p>Utile Distinction Memory used a learning rate p = 0.9, temporal discount factor y = 0.9, and an Expectation-Maxirnization' cycle of 500 steps. Nearest Sequence Memory used learning rate p = 0.9: temporal discount factor y = 0.9, maximum chain length N = 1000, exploration probability e = 0.1, and number of neighbors k = 4. A graph of the number of steps required for learning appears in Fig. <ref type="figure" target="#fig_2">3</ref>. The results are averaged over ten runs. In two of the mazes, Nearest Sequence Memory learns the task in only about 1/20th the time required by Utile Distinction Memory; in the other two, Nearest Sequence Memory learns mazes that Utile Distinction Memory did not solve at all. Although UDM has in principle the capability to learn multi-step memories, in practice they are difficult for UDM to discover [30]. NSM solves these multi-step memory tasks without problem. Like the Perceptual Distinctions Approach, UDM uses the Baum-Welch procedure. Thus, UDM also has worse computational complexity than NSM. 41n a, deterministic environment the agent can handle a much higher learning rate than in the stochastic environment used above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Steps per Trial during Learning</head><p>Recurrent2 ------ C. Recurrent-Q Recurrent-Q [24] was demonstrated on a robot two-cup retrieval task. The environment is deterministic, but the task is made difficult by two nested levels of hidden state and by providing no reward until the task is completely finished. Lin actually tried three different neural network algorithms on this task: Window-Q, Recurrent-Q, and Recurrent-Model; Recurrent-Q is the one that learned this task in the fewest trials. Recurrent-Q used temporal discount y = 0.9, and a trial time-out of 60 steps. It also used TD-X (A = 0.8) with Experience Replay, which is an experience recording and playback technique developed by Lin for speeding up learning. Nearest Sequence Memory used a trial time-out of 60 steps, learning rate p = 0.9, temporal discount factor y = 0.9, maximum instance chain length N = 1000, exploration probability e = 0.1, and number of neighbors IC = 4. A graph of performance during learning appears in Fig. <ref type="figure" target="#fig_3">4</ref>. Nearest Sequence Memory learns good performance in about 15 trials, Recurrent-Q takes about 100 trials to reach equivalent performance.</p><p>A parallel processing implementation of Recurrent-Q could take computation time O(number of layers in the net). A parallel processing implementation of NSM could take computation time O(1). Note also that, like NSM, Lin's Experience Reply mechanism requires the agent to store the chain of raw previous experience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. DISCUSSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Reason for Better Pe$ormance</head><p>Nearest Sequence Memory offers much improved on-line performance and fewer training steps than its predecessors. Why is the improvement so dramatic? I believe the chief reason lies with the inherent advantage of instance-based state identification, as described in Section 111, and restated in the following paragraph.</p><p>The key idea behind Instance-Based State Identification is the recognition that recording raw experience is particularly advantageous when learning to partition a state space, as is the case when the agent is trying to determine how much history is significant for uncovering hidden state. If, instead of using an instance-based technique, the agent simply averages new experiences into its current, flawed state space model, the experiences will be applied to the wrong states, and cannot be reinterpreted when the agent modifies its state space boundaries. Furthermore, data is always interpreted in the context of the flawed state space, always biased in an inappropriate way-not simply recorded, kept uncommitted and open to easy reinterpretation in light of future data.</p><p>This scenario is especially clear in the finite state machine approaches: experiences are averaged into the current FSM; after each state-splitting session, the agent re-gathers experience from scratch; the statistics used for learning are derived from the current, flawed FSM; and the raw data used to obtain these statistics is thrown away-not kept for reinterpretation in light of the new state space topology. The same statements apply to the recurrent neural network, except in this case the agent's internal state space is defined by the weights on connections to the memory-providing hidden units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Areas for Improvement</head><p>The experimental results in this paper bode well for instance-based state identification. Nearest Sequence Memory is simple-if such a simplistic implementation works as well as it does, more sophisticated approaches may work even better. The following subsections discuss some ideas for improvement.</p><p>Better Distance Metric: The agent should use a more sophisticated neighborhood distance metric than exact string match length. A new metric could account for distances between different percepts instead of considering only exact matches. A new metric could also handle continuous-valued inputs.</p><p>There is, however, prior experience showing that neither of these improvements is strictly necessary for work with physical robots. The sequence matching algorithm implemented for Toto <ref type="bibr" target="#b26">[27]</ref> used a small, finite set of percepts and was completely intolerant to noise. The robot wandered rooms and hallways while avoiding obstacles, learned a map of the environment, and could use the map to reliably travel to arbitrary goal locations. Toto's intolerant sequence matching algorithm overcame its quite noisy sensors by using an intermediate perceptual mechanism that translated the continuous, high-dimensional, noisy sensor values into elements of the useful percept set. Using a little domain knowledge and some averaging over time, the robot's intermediate layer produced noise-less percepts suitable for its intolerant string matching algorithm. A robot using Nearest Sequence Memory could make use of precisely the same mechanism. Furthermore, since NSM is somewhat tolerant of noisy percepts and actions, NSM could even continue to work if the intermediate perceptual mechanism were unreliable.</p><p>Unlike finite state machines, Nearest Sequence Memory cannot directly represent cycles in the environment. The agent should use well-chosen or learned abstract action primitives that encapsulate these loops. Some approach based on variable length, fuzzy sequence matching would also help.</p><p>StructureAVoise Distinction: Nearest Sequence Memory demonstrably solves tasks that involve noisy sensation and action, but it could perhaps handle noise even better if it used some technique for explicitly separating noise from structure.</p><p>K-nearest neighbor does not explicitly discriminate between structure and noise <ref type="bibr">[51]</ref>. If the current query point has neighbors with wildly varying output values, there is no way to know if the variations are due to noise, (in which case they should all be averaged), or due to fine-grained structure of the underlying function (in which case only the few closest should be averaged). Because Nearest Sequence Memory is built on k-nearest neighbor. it suffers from the same inability to methodically separate hisLJry differences that are significant for predicting reward and history differences that are not. I believe this is the single most important reason that Nearest Sequence Memory sometimes did not find optimal policies. An interesting manifestation of this noise and stlucture confusion is that occasionally Nearest Sequence Memory exhibits "superstitious behavior" as also displayed by learning animals <ref type="bibr" target="#b18">[19]</ref>, 1381. In certain circumstances, when the agent first finds success by some round-about route, it tends to use the route repeatedly. When exploratory steps uncover a more efficient path to reward, Nearest Sequence Memory will switch to the better path. For instance, in the beginning of a particular learning run for the Space Ship Docking task, the agent first docked successfully after making two redundant 180 degree turns. The agent then chose this behavior consistently, until, several trials later, random exploration discovered that the two extra turns were unnecessary, and the agent began docking without the extra "dance." This is over-specification-thinking more detail is relevant than really is. The extra "dance" is noise, but the agent interprets it as structure. Whether or not the Nearest Sequence Memory exhibits this behavior depends on how densely the state space around the reward is already covered by other instances when it first discovCrs the reward. This occasional tendency toward extra attention to details does imply that Nearest Sequence Memory could respond well to a teacher, because the agent would learn efficiently from a single presentation, and presumably, a teacher would demonstrate the task without the extra "dances."</p><p>Work in progress addresses the structurehoise issue by combining instance-based state identification with the structurehoise separation method from Utile Distinction Memory  <ref type="bibr" target="#b31">[33]</ref> for more details.</p><p>Separating Instance-Based Learning and Dynamic Programming: As pointed out in the section on details of the algorithm, Nearest Sequence Memory performs two kinds of learning: instance-based population of the state space witp raw experience and dynamic programming to calculate expected future reward. Currently these two kinds of learning are intertwined, but I believe the algorithm would perform better with a cleaner separation of the two. The instance-based component should be used to extract the relevant state space, then dynamic programming should be performed only on the extracted states.</p><p>The work in progress mentioned in the preceding section makes use of this scheme.</p><p>Further Reducing Storage and Computation: By using the instance-limiting technique discussed in Section V. Nearest Sequence Memory is already computationally more efficient than its competitors. We may want to find ways of reducing the storage and computational requirements even further.</p><p>As an alternative to keeping the last N instances, the agent could also simply stop adding states to the chain once it thought it had sufficiently covered the possibly significant sequences. The instance chain would remain unchanged while the agent continued to learn the q-values for those instances.</p><p>Once learning is complete, we may want to compile the chain down into a smaller structure. Related work in [47] discusses a technique for compiling example sequences into hidden Markov models. Previous work discusses the application of this technique to building probabilistic finite state machines that include a notion of actions <ref type="bibr" target="#b29">[30]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>~U l l l l l ~.The diversity-based inference procedure for finite automata [42], [41] learns an ingenious representation called an update graph that can efficiently capture regularities in the structure of the environment. The diversity approach finds aTheOf instance-based learning to short-term memory for state identification is driven by the important set of canonical tests using repeatable cycles in the environment. The Rivest-Schapire algorithm depends on a completely deterministic environment. Work in a neural network imulementation of this auuroach can accommodate noisv insight that learning in continuous spaces and learning with hidden feature in common: they both begin have a sensations, but has other disadvantages [37].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. Comparing k-nearest neighbor in a continuous space and in a sequence space. Both have a database of instances, and both have a point which we want to query. In each case, the "query point" from which we measue neighborhood relations is indicated with a gray cross, and the three nearest neighbors are indicated with gray shadows In a geometric space (shown on top), instances are the black dots, each indicating some vector of values in multi-dimensional space The neighborhood metric is defined by Euclidean distance In a sequence space (shown on bottom), instances are circles connected by mows, each indicating an action/percept/reward snapshot in tme, with all the snapshots laid out in time sequence order (The numbers in the clrcles indicate actionlperceptireward triples The neighborhood metric is detemned by sequence match length, that is, the number of preceding states that match the states preceding the query point By using this neighborhood function, the agent finds situations from its past that are most simlar to its current situation, where "most simlar" not only includes the current percepts but also the percepts leadmg up to current situation The agent thus has an internd state space with vanable-length short-term memory because when choosmg instances from which to extract expected reward values, it tends to prefer mstances with better matching history</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Comparing Utile Distinction Memory and Nearest Sequence Memory on a maze task. The agent does not perceive global row and column numbers, but only indications of whether there is a barrier to the immediately adjacent north, south, east or west. Many positions from which the agent should execute different actions are perceived as identical. The goal square is indicated with a "G". The bar graphs show the number of training steps required to learn a perfect policy for all starting points. The results are averaged over ten runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Performance comparison between Recurrent-&amp; and Nearest Se-quence Memory on Lin's 2-cup task. The robot 2-cup retrieval task is difficult because it has two nested levels of hidden state and because it provides no reward until the task is completely finished. The vertical axis shows the number of steps required to complete the 2-cup task twice, once from each of the two possible starting points. Each of these double task completions makes up a single trial. The horizontal axis measures number of trials since the beginning of learning. The learning curves show the mean results from five runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>[ 31 ]</head><label>31</label><figDesc>. The algorithm, called Utile &amp;fix Memory, uses a treestructured representation, and is related to work on Prediction Suffix Trees [43], Parti-game [36], G-algorithm [11], and Variable Resolution Dynamic Programming [ 1 I]. Preliminary results are very promising. See [32],</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>3) Using the states, the agent calculates &amp;-values by averaging together the expected future reward values associated with the k nearest states for each action. The agent then chooses the action with the highest &amp;-value. The regular &amp;-learning update rule is used to update the k states that voted for the chosen action. Choosing to represent short-term memory as a linear trace is a simple, well-established technique. Order N Markov chains use this representation; the fixed-sized time windows used by Lin and Albus are examples of order N Markov chains [24], [I]. Nearest Sequence Memory also uses a linear trace to represent memory, but it differs from these fixed-sized window techniques because it provides a variable memorylength--like k-nearest neighbor, NSM can represent varying resolution in different regions of state space.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Steps to Learn Task 74 1500 number of steps Steps to Learn Task 153 2500 number of steps Steps to Learn Task</head><label></label><figDesc>Fig.2. Comparing perceptual hstmctions approach and nearest sequence memory on Chrisman's dochng task The spaceship dochng task involves noisy sensors and noisy actions. Some sensors returned incorrect values 30% of the tune. Some actions failed 70%, 30%, or 20% of the time, and when they failed, resuked in random states The value plotted is the utility (sum of</figDesc><table><row><cell>UDM</cell><cell></cell></row><row><cell>NSM</cell><cell></cell></row><row><cell>238</cell><cell>&gt;I 0000</cell></row><row><cell></cell><cell>number of steps</cell></row><row><cell></cell><cell>Steps to Learn Task</cell></row><row><cell>UDM</cell><cell></cell></row><row><cell>NSM</cell><cell></cell></row><row><cell>395</cell><cell>&gt;10000</cell></row><row><cell></cell><cell>number of steps</cell></row></table><note><p>average toward recent rewards, where the strength of the bias would have been dictated by the learning rate, p.</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>This work has benefited from discussions with many colleagues, including D. Ballard, A. Moore, J. Schneider, and J. Karlsson. I am grateful to D. Ballard, J. Karlsson, V. de Sa, and the IEEE reviewers for making helpful comments on earlier drafts.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported by NSF under Grant IRI-8903582, and by NIWPHS under Grant 1 R24 RR06853-02.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R. Andrew McCallum</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Outline for a theory of intelligence</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Albus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="473" to="509" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Memory-based approaches to approximating continuous functions</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Atkeson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Nonlinear Modeling and Forecasting</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Casdagli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Eubank</surname></persName>
		</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Principles of animate vision</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Ballard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comp. Vis. Graph. Image Proc.: Image Understanding</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="21" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Real-time learning and control using asynchronous dynamic programming</title>
		<author>
			<persName><forename type="first">A</forename><surname>Barto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bradtke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Univ. of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep. 91-57</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning and sequential decision making</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Barto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J C</forename><surname>Watkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dept. Comp. Inform. Sci., Univ. of Massachusetts</title>
		<editor>
			<persName><forename type="first">Computational</forename><surname>Learning</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Neuroscience</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Gabrial</surname></persName>
		</editor>
		<editor>
			<persName><surname>Moore</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA; Amherst</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Also COINS Tech. Rep. 89-95</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neuron-like elements that can solve difficult learning control problems</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="834" to="846" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Dynamic Programming and Stochastic Control</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1976">1976</date>
			<pubPlace>New York Academic</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Shreve</surname></persName>
		</author>
		<title level="m">Stochastic Optimal Control</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Academic</publisher>
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Classifier systems and genetic algorithms</title>
		<author>
			<persName><forename type="first">L</forename><surname>Booker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Holland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Art$ Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="235" to="282" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Acting optimally in partially observable stochastic domains</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Cassandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Kaelbling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Twelfth Nut. Con$ on ArtiJicial Intelligence</title>
		<meeting>Twelfth Nut. Con$ on ArtiJicial Intelligence<address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning from delayed reinforcement in a complex domain</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chapman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Kaelbling ; M A Addison-Wesley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Twelfth Int. Joint Con$ on ArtiJicial Intelligence, 199 1. Reading</title>
		<meeting>Twelfth Int. Joint Con$ on ArtiJicial Intelligence, 199 1. Reading</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="503" to="521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Reinforcement learning with perceptual aliasing: The perceptual distinctions approach</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chrisman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Tenth Nat. Con5 on Artificial Intelligence</title>
		<meeting>Tenth Nat. Con5 on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="183" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Training agents to perform sequential behavior</title>
		<author>
			<persName><forename type="first">M</forename><surname>Colombetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dorigo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adapt. Behav</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="247" to="275" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Inferring finite automate with stochastic output functions and an application to map learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anglum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Basye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Engelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaelbling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kokkevis</surname></persName>
		</author>
		<author>
			<persName><surname>Maron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Tenth Nut</title>
		<meeting>Tenth Nut</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="208" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the EM algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Royal Stat. Soc. Ser. B</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">ALECSYS and the AutonoMouse: Learning to control a real robot by distributed classifier systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dorigo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<meeting><address><addrLine>Politecnico di Milano, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992">1992. 1995</date>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="209" to="240" />
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep. 92-011</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Robot shaping: Developing autonmous agents through learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dorigo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Colombetti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Art$ Intell</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="321" to="370" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hart</surname></persName>
		</author>
		<title level="m">Pattern Classification and Scene Analysis</title>
		<meeting><address><addrLine>New York Wiley</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Hilgard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Marquis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Condztioning</forename><surname>Learning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Kimble</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1961">1961</date>
			<pubPlace>New York Appleton, 2nd Edition</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adaptation in Natural and Artificial Systems</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Holland</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975">1975</date>
			<publisher>M I Univ. of Michigan Press</publisher>
			<pubPlace>Ann Arbor</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Reinforcement learning algorithm for partially observable Markov decision problems,&apos;&apos; in Advances of Neural Information Processing Systems 7</title>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<pubPlace>San Mateo, C A Morgan Kaufmann</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Genetic Programming: On the Programming of Computers by Means of Natural Selection</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Koza</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Self-improvement based on reinforcement learning, planning and teaching</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eighth Int. Workshop on Machine Learning</title>
		<meeting>Eighth Int. Workshop on Machine Learning</meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Reinforcement learning for robots using neural networks</title>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993-01">Jan. 1993</date>
			<pubPlace>Pittsburgh, PA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>School of Computer Science, Carnegie Mellon</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. Dissertation</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Reinforcement learning with hidden states</title>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Second Int. Con5 on Simulation of Adaptive Behavior: From Animals to Animats</title>
		<meeting>Second Int. Con5 on Simulation of Adaptive Behavior: From Animals to Animats</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="271" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Memoryless policies: Theoretical limitations and practical results</title>
		<author>
			<persName><forename type="first">M</forename><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Third Int. Con$ on Simulation of Adaptive Behavior: From Animals to Animats</title>
		<meeting>Third Int. Con$ on Simulation of Adaptive Behavior: From Animals to Animats</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">A distributed model for mobile robot environment learning and navigation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Mataric</surname></persName>
		</author>
		<idno>AI-TR 1228</idno>
		<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">First results with utile distinction memory for reinforcement learning</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Mccallum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
			<pubPlace>Rochester, NY</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Dept. Computer Science, Univ. of Rochester</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep. 446</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Using transitional proximity for faster reinforcement learning</title>
	</analytic>
	<monogr>
		<title level="m">Proc. Ninth Int. Machine Learning Conference</title>
		<meeting>Ninth Int. Machine Learning Conference</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Learning with incomplete selective perception</title>
		<author>
			<persName><surname>__</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993-03">Mar. 1993</date>
			<pubPlace>Rochester, NY</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Dept. Computer Science, Univ. of Rochester</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep. 453, Ph.D. Thesis proposal</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Utile suffix memory for reinforcement learning with hidden state</title>
	</analytic>
	<monogr>
		<title level="m">Proc. Tenth Int. Machine Learning Conference</title>
		<meeting>Tenth Int. Machine Learning Conference<address><addrLine>Rochester, NY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1321-12">1993. 1321. Dec. 1994</date>
			<biblScope unit="volume">549</biblScope>
		</imprint>
		<respStmt>
			<orgName>Dept. Computer Science, Univ. of Rochester</orgName>
		</respStmt>
	</monogr>
	<note>Overcoming incomplete perception with utile distinction memory</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Instance-based utile distinctions for reinforcement learning</title>
		<author>
			<persName><surname>__</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Twelfth Int. Machine Learning Con$</title>
		<meeting>Twelfth Int. Machine Learning Con$</meeting>
		<imprint>
			<date type="published" when="1977">1995. 1977</date>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">M A Mit</forename><surname>Cambridge</surname></persName>
		</author>
		<author>
			<persName><surname>Press</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
