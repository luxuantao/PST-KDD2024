<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Simple Yet Effective Pretraining Strategy for Graph Few-shot Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-03-29">29 Mar 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">First</forename><surname>Author</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhen</forename><surname>Tan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Arizona State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Second</forename><surname>Author</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Kaize</forename><surname>Ding</surname></persName>
							<email>kding9@asu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Arizona State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Third</forename><surname>Author</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ruocheng</forename><surname>Guo</surname></persName>
							<email>ruocheng.guo@cityu.edu.hk</email>
							<affiliation key="aff1">
								<orgName type="institution">City University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
							<email>huanliu@asu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Arizona State University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Simple Yet Effective Pretraining Strategy for Graph Few-shot Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-03-29">29 Mar 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2203.15936v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, increasing attention has been devoted to the graph few-shot learning problem, where the target novel classes only contain a few labeled nodes. Among many existing endeavors, episodic metalearning has become the most prevailing paradigm, and its episodic emulation of the test environment is believed to equip the graph neural network models with adaptability to novel node classes. However, in the image domain, recent results have shown that feature reuse is more likely to be the key of meta-learning to few-shot extrapolation. Based on such observation, in this work, we propose a simple transductive fine-tuning based framework as a new paradigm for graph few-shot learning. In the proposed paradigm, a graph encoder backbone is pretrained with base classes, and a simple linear classifier is fine-tuned by the few labeled samples and is tasked to classify the unlabeled ones. For pretraining, we propose a supervised contrastive learning framework with data augmentation strategies specific for few-shot node classification to improve the extrapolation of a GNN encoder. Finally, extensive experiments conducted on three benchmark datasets demonstrate the superior advantage of our framework over the state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Due to the strong modeling capability, a spectrum of Graph Neural Networks (GNNs) <ref type="bibr" target="#b4">[Kipf and Welling, 2016;</ref><ref type="bibr">Veli?kovi? et al., 2017;</ref><ref type="bibr" target="#b11">Xu et al., 2018]</ref> have been applied to model a myriad of network-based systems in various domains, such as social networks <ref type="bibr" target="#b2">[Hamilton et al., 2017]</ref>, citation networks <ref type="bibr" target="#b2">[Jiao et al., 2020]</ref>, biomedical brain networks <ref type="bibr" target="#b4">[Liao et al., 2019]</ref>, chemical property discovery <ref type="bibr" target="#b11">[Xu et al., 2018]</ref> and knowledge graphs <ref type="bibr" target="#b4">[Park et al., 2019]</ref>. Following the recurrent mechanism of feature transformation and propagation, GNNs succeed in learning expressive representations to solve problems in these domains.</p><p>Despite the breakthroughs, more recently, it has been revealed that conventional GNNs fail to make accurate predictions when the labels are scarcely available. For the few-shot node classification task <ref type="bibr" target="#b12">[Zhou et al., 2019;</ref><ref type="bibr">Ding</ref>  Node classes in real-world graphs usually follow a highly skewed distribution (e.g., power-law distribution <ref type="bibr" target="#b6">[Stephen and Toubia, 2009]</ref>), which poses a great challenge for learning powerful GNNs. To tackle this problem, meta-learning algorithms (e.g., MAML <ref type="bibr" target="#b2">[Finn et al., 2017]</ref>, Prototypical Network <ref type="bibr" target="#b5">[Snell et al., 2017]</ref>, etc.) have become a prevailing paradigm <ref type="bibr" target="#b12">[Zhou et al., 2019;</ref><ref type="bibr" target="#b9">Ding et al., 2020;</ref><ref type="bibr" target="#b2">Guo et al., 2021]</ref>. By episodically learning from sampled tasks on base classes to emulate the environment in the test phase, a meta-trained GNN model is proposed to transfer the knowledge from base classes to novel classes and adapt to the target task using only a few labeled nodes.</p><p>However, researchers question whether the learned representation from meta-learning is responsible for the fast adaption onto the target few-shot tasks in the test phase <ref type="bibr">[Tian et al., 2020]</ref>. In the image domain, <ref type="bibr" target="#b1">[Dhillon et al., 2019]</ref> implies that feature reuse is the main reason. In other words, with a pretrained encoder backbone, the learned representation will not significantly change during meta-learning. To validate this idea, in the graph domain, we compare the representations of novel classes learned in two manners: (1) directly training a GNN encoder on all the base classes with Cross-Entropy Loss (CEL) and use a separate classifier for classification (2) episodically training one of the state-of-theart meta-learning based graph few-shot learning model on tasks sampled from base classes, <ref type="bibr">GPN [Ding et al., 2020]</ref>. As shown in Figure <ref type="figure" target="#fig_0">1</ref> (a) and (b), we can see that representations in both methods are similarly discriminative. Furthermore, as we demonstrate in Section 5.2, following <ref type="bibr" target="#b1">[Dhillon et al., 2019]</ref>, if we initialize the weight of a separate classifier as the average of the features of support nodes, then training with CEL can achieve comparable results as those state-ofthe-art graph few-shot learning methods. Based on this observation, we argue that, for the graph domain, apart from metalearning, there is another feasible way to tackle the scarce label issue: pretrain a GNN encoder on base classes, and then transductively fine-tune a separate classifier with nodes from the support set in novel classes. To achieve this, it is necessary to design a pretraining strategy to facilitate the encoder with the extrapolation ability to generate discriminative representation embedding for unseen few-shot classes.</p><p>In order to pretrain a powerful GNN encoder, selfsupervised graph contrastive learning has shown to be the most effective way <ref type="bibr">[You et al., 2020;</ref><ref type="bibr" target="#b2">Hassani and Khasahmadi, 2020;</ref><ref type="bibr">Zhu et al., 2021]</ref>. Generally, multiple views of the input graph are first created through predefined transformations (e.g., randomly dropping edges, randomly perturbing node attributes, etc.). Then a contrastive loss is applied to maximize feature consistency under differently augmented views. Despite their effectiveness, we argue that those methods do not consider the unique characteristics of the few-shot node classification problem. First of all, those self-supervised pretraining strategies do not utilize the large portion of the label information in base classes. Following the power-law distribution <ref type="bibr" target="#b6">[Stephen and Toubia, 2009]</ref> in networks, the nodes in base classes also follow a severely skewed distribution. If naively following those graph contrastive learning frameworks, the GNN encoders trained with such unsupervised pretraining schemes will be biased towards the majority classes, since more pairs of views are generated from these classes for contrastive learning. Consequently, some crucial semantics implied in those classes with fewer nodes cannot be well captured. To address this issue, we propose a supervised contrastive learning based framework incorporated with a simple balance sampling strategy. On the one hand, with the label information, for each batch, we treat both original nodes in the same classes and their augmented views as positives. Compared to self-supervised contrastive methods where only a single positive is used, more positive pairs are included to capture the similarity of the nodes in the same classes and contrast them against the negatives (the remainder of the batch). On the other hand, in each batch, we sample identical numbers of nodes to generate views and contrast between them. In this way, the encoder will learn to equally discriminate nodes in different classes and align nodes in the same classes without over-emphasis on those major classes.</p><p>Second, those aforementioned transformations may inject noise that propagates along the graph and then affect the learned representation of nodes in novel classes. Considering the limited number of nodes in each support set, which the decision boundaries depend on, this impact can be amplified in the final classification. To obtain a more robust augmentation, we propose to use node connectivity to sample a subgraph with nodes that are most important to the support node and use an importance-weighted pooling to form the augmented view to better depict the contextual information. Specifically, we consider two methods for measuring node importance, Node Algebraic Distance (NAD) <ref type="bibr">[Chen and Safro, 2009]</ref> and Personalized PageRank (PPR) <ref type="bibr">[Jeh and Widom, 2003]</ref>, that are proved to provide sufficient proximity for importance measurement. The effectiveness of our framework is validated with comprehensive experiments on three real-world datasets. We summarize our contributions here:</p><p>Contributions. We propose a simple yet effective fullysupervised contrastive training based transductive fine-tuning style framework tailored for few-shot classification problems to tackle the aforementioned issues. (1) We are the first to investigate the feasibility of the transductive fine-tuning paradigm for graph few-shot learning problems. (2) We propose a supervised contrastive learning pretraining strategy that is tailored for few-shot node classification tasks. (3) A comparative study is performed to assess the advantage of the transductive fine-tuning based framework over the existing meta-learning based graph few-shot learning methods. ( <ref type="formula" target="#formula_4">4</ref>) Systematic experiments demonstrate the effectiveness and superiority of the proposed framework compared to the existing graph few-shot learning methods or transductive fine-tuning with other graph pretraining strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Graph Few-shot Learning</head><p>Graph Neural Network (GNN) <ref type="bibr" target="#b4">[Kipf and Welling, 2016;</ref><ref type="bibr">Veli?kovi? et al., 2017;</ref><ref type="bibr" target="#b11">Xu et al., 2018]</ref> is a family of deep neural network models tailored for graph-structured data, which exploits a recurrent neighborhood aggregation strategy to preserve the graph structure information and transform the nodes attributes simultaneously. Recently, increasing research attention has been paid to graph few-shot learning problems, where the episodic meta-learning paradigm <ref type="bibr" target="#b2">[Finn et al., 2017]</ref>, has become the most dominant strategy. The key is to train the GNN encoders by explicitly emulating the test environment for few-shot learning <ref type="bibr" target="#b2">[Finn et al., 2017]</ref>, where the encoders are believed to gain the adaptability to extrapolate onto new domains. Based on it, Meta-GNN <ref type="bibr" target="#b12">[Zhou et al., 2019]</ref> and AMM-GNN <ref type="bibr" target="#b9">[Wang et al., 2020]</ref> apply MAML <ref type="bibr" target="#b2">[Finn et al., 2017]</ref> to tackle the low-resource learning issue on graphs. Also, <ref type="bibr">GPN [Ding et al., 2020]</ref> adopts Prototypical Networks <ref type="bibr" target="#b5">[Snell et al., 2017]</ref> to make the classification based on the distance between the node feature and the prototypes. However, more recently, for the image domain, people argue that, the reason of the fast adaptation in the existing works lies in feature reuse rather than those complicated mate-learning algorithms <ref type="bibr" target="#b1">[Dhillon et al., 2019;</ref><ref type="bibr" target="#b7">Tian et al., 2020]</ref>. In other words, with a carefully pretrained encoder, decent accuracy can be obtained through direct fine-tuning a simple classifier on the target domain. Since then, various pretraining strategies <ref type="bibr" target="#b7">[Tian et al., 2020;</ref><ref type="bibr">Liu et al., 2021]</ref> have been put forward to tackle the fewshot image classification problem. However, no research has been done on the graph domain, where there is an important difference that the samples in the base data also have severely skewed distribution due to the power-law manner in networks <ref type="bibr" target="#b6">[Stephen and Toubia, 2009]</ref>. Considering this feature, our work aims at bridging the gap, to design a pretraining scheme specific for graph few-shot learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph Contrastive Learning</head><p>Nowadays, contrastive learning has become the most popular representation learning paradigm in all image <ref type="bibr">[Chen et al., 2020]</ref>, text <ref type="bibr" target="#b10">[Wang et al., 2021]</ref>, and graph <ref type="bibr" target="#b2">[Hassani and Khasahmadi, 2020;</ref><ref type="bibr">Zhu et al., 2021]</ref> domains. Starting from the self-supervised setting, contrastive learning methods are proved to learn discriminative representation by contrasting a predefined distance between positive and negative samples. Usually, those samples are augmented through one or more combined heuristic transformations from original samples in the same or different classes. Specifically, in graph domain, the transformations can be categorized into these following types: (1) graph structure based augmentation, e.g. randomly drop edges or nodes <ref type="bibr" target="#b2">[Hassani and Khasahmadi, 2020;</ref><ref type="bibr">Zhu et al., 2021]</ref>, randomly sample subgraph <ref type="bibr" target="#b2">[Jiao et al., 2020]</ref>. (2) graph feature based augmentation, e.g. randomly mask or perturb attributes of nodes or edges <ref type="bibr" target="#b8">[Tong et al., 2021]</ref>. More recently, for image <ref type="bibr" target="#b3">[Khosla et al., 2020</ref>] and text domains <ref type="bibr" target="#b2">[Gunel et al., 2020]</ref>, people find out that by injecting label information to the contrastive loss to compacted or enlarged the distances of augmentations of instances within the same or different classes, supervised contrastive loss outperforms the original unsupervised version and even crossentropy loss in the setting of transfer learning. All those work has shown the highly discriminative representation learned from supervised contrast in text and image domains. However, no existing work has focused on its extrapolation ability in the graph domain, and especially, under an extremer fewshot situation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem Formulation</head><p>In this paper, we focus on few-shot node classification on a single graph. Formally, given an attributed network G = (V, E, X) = (A, X), where V, E, A and X denote the nodes, edges, adjacency matrix and node features respectively, the few-shot node classification problem assumes the existence of a series of homogeneous node classification tasks</p><formula xml:id="formula_0">T = {D i } I i=1 = {(A C i , X C i )} I i=1</formula><p>, where D i describes the corresponding dataset of a task, X C i denotes the attributes of nodes whose labels belong to the label space C i , and A C i similarly. Following the conventional assumption, we call the classes for training as base classes, C base , and the classes for target test classes as novel classes, C novel , C base ? C novel = ?. Even though with severely skewed distribution, the number of nodes in base classes is sufficiently larger than that of the target novel classes. Specifically, for each novel class ?c ? C novel , we have a set of few labeled nodes, S (support set), and a set of unlabeled target nodes, Q (query set), namely, D i = (S i , Q i ). In each support set S i , if we have N different classes and K labeled nodes per class, then we call this problem a N -way K-shot node classification problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methodology</head><p>Motivation Inspired by <ref type="bibr" target="#b7">[Tian et al., 2020]</ref>, we propose a transductive fine-tuning based framework tailored for fewshot node classification. We first pretrain a GNN encoder in a supervised contrastive learning manner, which can enhance the generalizability of our model by ( <ref type="formula" target="#formula_1">1</ref>) providing more robust contextual augmented data (2) utilizing label information to generate more discriminative representations. Then, for evaluation on the target few-shot novel classes, a simple logistic regression classifier fine-tuned by the support nodes is used to predict the labels of the query nodes. We further illustrate those merits in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">GNN Backbone Pretraining</head><p>Data Augmentation Given a graph G = (V, E, X) = (A, X), following the conventions in contrastive learning methods, a transformation T (?) is used to generate a new view x j from a node j with features X j (?j ? {1, ..., M }, M is the number of nodes):</p><formula xml:id="formula_1">x j = T (x j )<label>(1)</label></formula><p>To accommodate the few-shot learning node classification problem, the key point lies in the design of a transformation function that possesses the following two properties. First, the augmentation method should be relatively robust to noise and outliers. This is because the impact of noise and outliers will be amplified by the few-shot setting <ref type="bibr" target="#b9">[Ding et al., 2020]</ref>. Second, the augmented views from a category of nodes should provide sufficient context information specific to the chosen category <ref type="bibr" target="#b3">[Khosla et al., 2020]</ref>. To this end, we propose to use a node connectivity based subgraph sampling mechanism as the transformation function. Connectivity score can be computed by a family of methods that measures the connection strength between a pair of nodes in a graph without using the nodes attributes <ref type="bibr">[Chen and Safro, 2009]</ref>, and has been shown to provide good proximity for the importance of the relation between two nodes <ref type="bibr" target="#b2">[Jiao et al., 2020]</ref>. Given a graph, we can compute the connectivity scores between each pair of nodes (note that this can be pre-computed before training) by</p><formula xml:id="formula_2">S = Connect(V, E),<label>(2)</label></formula><p>where S ? R M ?M is the matrix of scores between all node pairs, and Connect(?) is the function for calculating the node connectivity. In this paper, we compare two methods: Node Algebraic Distance (NAD) <ref type="bibr">[Chen and Safro, 2009]</ref> and Personalized PageRank (PPR) <ref type="bibr">[Jeh and Widom, 2003]</ref>, see details in Appendix A. Then for each node j, we can sample the top ? -1 nodes with the highest connectivity scores to form a subgraph G s (j) = (A j , X j ). We can assume that the features of the top ? -1 nodes with higher connectivity scores should provide more correlated information and less noise for the center node j, and thus, the subgraph should provide enough context information for node j [Chen and <ref type="bibr">Safro, 2009;</ref><ref type="bibr">Jeh and Widom, 2003]</ref>. The higher ? is, the more contexts are given in the subgraph (We set ? = 20 for memory limitation). Then, we feed the resulting subgraph into a GNN encoder g ? :</p><formula xml:id="formula_3">Z j = g ? (A j , X j ),<label>(3)</label></formula><p>where Z j ? R ??F and F is the embedding size. We normalize the latent features for better performance. As presented in <ref type="bibr" target="#b2">[Hassani and Khasahmadi, 2020]</ref>, contrasting a node j with the sampled subgraph, Gs(j), Gs(r), (j = r), can yield better results than directly contrasting node j with node r or contrasting subgraph Gs(j) with subgraph Gs(r). Based on this, we propose a readout function R(?) which maps the representation of all nodes in the subgraph, Z j , to a vector z j as the subgraph representation. Intuitively, z j is a weighted average of the representations of all nodes in the subgraph. The weights are their normalized connectivity scores ?j ? R ? to node j. We set the connectivity score of a node j to itself as a constant value ? before normalization:</p><formula xml:id="formula_4">z j = R(Z , ?j ) = ?( Z , ?j )<label>(4)</label></formula><p>Where ? is a nonlinear function, and z j ? R F is the desired subgraph representation. The center node embedding z j ? R F can be indexed from Z j . The nodes in the same classes together with their corresponding subgraphs are viewed as postives. Now, given a batch size B, we can define a duo-viewed batch, consisting of the representations of both nodes and their corresponding subgraphs:</p><formula xml:id="formula_5">{(h b , y b )} 2B b = {(z b , y b ), (z b , y b )} B b .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss Function</head><p>To better suit the setting of few-shot node classification, our loss is similar to the Supervised Contrastive loss (Sup-Con) <ref type="bibr" target="#b3">[Khosla et al., 2020]</ref> for pretraining GNN encoders. We call our proposed loss G-SupCon for convenience. Compared to unsupervised contrastive loss (e.g. Deep InfoMax (DIM) <ref type="bibr" target="#b1">[Bachman et al., 2019]</ref>, SimCLR Loss [Chen et al., 2020], Margin Loss (MargL) <ref type="bibr" target="#b2">[Jiao et al., 2020]</ref> etc.), and Cross-Entropy Loss (CEL), G-SupCon has the following advantages: (1) Utilizing the label information to sample minibatches can help mitigate the impact from the biased node class distribution. We adopt the most straightforward yet effective way, to sample identical numbers (B/|C tr |, where |C tr | is the number of classes for pretraining) of nodes per class in each mini-batch for training. We term it Balanced Sampling (BS) for convenience. (2) As our experimental results in Section 5.4 show, even with the same training samples, G-SupCon still outperforms all other loss functions in terms of the accuracy on novel classes. This implies that, by additionally forcing the representation of nodes in the same classes more closely aligned, while pushing clusters of nodes from different classes further apart, the encoder has learned to generate more discriminative representations that allow even a simple classifier to easily learn rigorous boundaries for different classes and extrapolate to unseen novel classes. The loss can be written as:</p><formula xml:id="formula_6">L = b?B -1 |P (b)| p?P (b) log exp (h b ? h p /? ) a?A(b) exp (h b ? h a /? ) ,<label>(5)</label></formula><p>where A(b) is the set of indices from 1 to 2B excluding b, and P (b) is the set of indices of all positives in a duo-viewed batch excluding b. ? ? R + is a scalar temperature parameter defined as ? = ?/ degree(G), where degree(G) is the average degree of the graph, and ? is a hyperparameter. Under this fully-supervised setting, we pretrain our GNN encoder with all the base nodes and fix it during fine-tuning. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Linear Classifier Fine-tuning</head><p>With a pretrained GNN encoder, when fine-tune for a target few-shot node classification dataset D i on novel classes, we tune a separate linear classifier, (e.g. one linear layer, logistic regression, SVM, etc.) with the few labeled nodes in the support set S i , and task it to predict the labels for nodes in the query set Q i . The representations of nodes in D i are obtained through the same procedure mentioned above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we present the experiments. We first introduce the datasets and baseline methods. Then, we show the results and analysis of the comparative study. In addition, we show that the proposed method outperform baselines in clustering unlabeled nodes from novel classes without finetuning. This implies that the representations learned by the proposed method elicits more accurate decision boundaries. Furthermore, to illustrate the advantage of the proposed loss function, we compare three typical candidate loss functions under various settings. Finally, we conduct a comprehensive component analysis to study and validate the effectiveness of individual components in the proposed framework and investigate their characteristics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experiment Settings Evaluation Datasets</head><p>We conduct our experiments on three widely used graph fewshot learning benchmark datasets where a sufficient number of node classes is available for sampling few-shot node classification tasks: CoraFull <ref type="bibr">[Bojchevski and G?nnemann, 2017]</ref>,</p><p>Reddit <ref type="bibr" target="#b2">[Hamilton et al., 2017]</ref>, and Ogbn-arxiv <ref type="bibr" target="#b11">[Xu et al., 2018]</ref>. Their statistics are given in Table <ref type="table" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline Methods</head><p>In this work, we compare our framework with the following types of methods:</p><p>? A naive GNN encoder. For this experiment, we use GCN <ref type="bibr" target="#b4">[Kipf and Welling, 2016]</ref> as the encoder and pretrain it with all nodes from the base classes, and then we finetune a logistic regression model as the classifier for each few-shot node classification task. ? A universal baseline based on the Transductive Fine-Tuning (TFT) <ref type="bibr" target="#b1">[Dhillon et al., 2019]</ref> which is proposed for few-shot image classification. We replace its encoder with a GNN. It proposes to initialize the weight of the separate linear classifier in each few-shot task as a matrix consisting of concatenated prototype vectors of novel classes.</p><p>Table <ref type="table">2</ref>: Comparative Results on the three datasets under different N-way K-shot settings.</p><p>? The state-of-the-art methods for few-shot node classification in a single graph: MAML based Meta-GNN <ref type="bibr" target="#b12">[Zhou et al., 2019]</ref>, AMM-GNN <ref type="bibr" target="#b9">[Wang et al., 2020]</ref>   <ref type="bibr">et al., 2021]</ref>. Following the similar procedure to our framework, these methods are used to pretrain a GNN encoder with nodes from base classes without using the labels. Then we fine-tune the classifiers in a transductive manner on novel support nodes, and report their scores on predicting labels for novel query nodes. The architectures of these classifiers are the same as those for node classification downstream tasks in the original papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>In Table <ref type="table" target="#tab_0">1</ref>, we list the specific data split strategy for each dataset. For a fair comparison, we adopt the same encoder for all compared methods. We further explore the effect of encoder architecture in Section 5.5. For implementation, we implement the proposed framework in PyTorch. Specifically, the graph encoder g ? consists of one GCN layer <ref type="bibr" target="#b4">[Kipf and Welling, 2016]</ref> with PReLU activation. Regarding the classifier for fine-tuning, we use the logistic regression from Sklearn. The encoder is trained with Adam optimizers whose learning rates are set to be 0.001 initially with a weight decay of 0.0005. And the coefficients for computing running averages of gradient and square are set to be ? 1 = 0.9, ? 2 = 0.999. The default values of batch size B and graph temperature parameter ? are set to 500 and 1.0, respectively. For the baseline methods, we use the default parameters provided in their codes.</p><p>Evaluation Protocol: To reduce fluctuation from test task sampling, all the scores reported are accuracy values averaged over 10 random seeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparative Results</head><p>In this section, we present the comparison between our framework and the other four categories of baseline methods described in Section 5.1. To the best of our knowledge, we are the first to investigate the effectiveness of transductive finetuning on Graph Few-shot Learning (GFL) problems. For fair comparison, all methods share the same GNN encoder architecture as the proposed framework, a 1-layer GCN. Also, when experimented on each dataset, they share the same random seeds for data split, leading to identical evaluation data.</p><p>The results are shown in Table <ref type="table">2</ref>. We summarize our findings as the following points:</p><p>Advantage of Transductive Fine-Tuning (transductive fine-tuning) style methods. An evident finding is that almost all the transductive fine-tuning based methods outperform the existing GFL algorithms, which are meta-learning based. Even the most straightforward one, TFT, which only leverages a simple initialization to the separate classifier, can produce comparable scores to the best meta-learning based GFL method, GPN. We have shown that, similar to <ref type="bibr" target="#b1">[Dhillon et al., 2019;</ref><ref type="bibr" target="#b7">Tian et al., 2020]</ref>, the proposed transductive finetuning approach, as a new paradigm to tackle the GFL problem outperforms meta-learning based algorithms by a significant margin.</p><p>Effectiveness of our framework to learn discriminative boundaries for GFL problems. Compared with other existing graph representation pretraining methods, our proposed method outperforms them by a large margin under every setting tested. We attribute this to the following facets: (1) The effectiveness of the proposed node-connectivity-based sampling strategy that can reduce the noise of subgraph sampling, and provide highly correlated context-specific information for the center node. Besides, NAD and PPR can provide similar outcomes. NAD can perform better on higher degree graphs. We leave that for future work. (2) The supervised contrastive objective function G-SupCon can push the GNN encoder to generate more discriminative representations by squeezing the positive while segregating negative representation clusters.</p><p>Robustness to various N -way K-shot settings. Similar to the meta-learning based GFL methods, the performance of transductive fine-tuning based methods also degrades when the K decreases or N increases. However, from Table <ref type="table">2</ref>, we can find that transductive fine-tuning based methods are more robust to the setting change, which means that the encoder has learned more extrapolation ability to generate more discriminative representations of novel classes. To a large extent, the degradation lies in the less accurate classifier due to fewer training nodes. Learning to better measure the classifier under scenarios with extremely scarce support nodes (very small K) is also worth further research.</p><p>To further show the effectiveness of our framework, we present more experiments in the following sections. Without further clarification, the default setting for following experi-  ments is 5-way 5-shot, where we set ? = 1.0, B = 500, and use a single GCN layer as the encoder. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Representation Clustering</head><p>In this section, to demonstrate the high-quality representation generate from the encoder in our framework, we show the clustering results of the representation of the query nodes in a randomly sampled 5 novel classes in Table <ref type="table" target="#tab_1">3</ref> and visualize the embedding in Figure <ref type="figure" target="#fig_2">2</ref> (without fine-tuning on support set). It can be observed that an encoder through our pretraining strategy possesses stronge extrapolation ability to generate highly discriminative boundaries for unseen novel classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Loss Function Analysis</head><p>To better demonstrate the effectiveness of our G-SubCon loss, we design an experiment to compare different loss functions. The SimCLR loss does not consider the label information, so the Balance Sampling (BS) strategy we proposed in Section 4.1 is not feasible for it, but in order to explicitly show the influence from the loss function, we also show the result of SimCLR with the same sampled data splits from BS as our G-SubCon. We list the result from all the candidate loss functions under both settings in Table <ref type="table" target="#tab_2">4</ref>. It can be observable that all the losses suffer from the class imbalance issue, and the simple BS scheme can improve the performance. Also, our proposed G-SupCon outperforms all others even with identical data splits sampled by BS. In all, both the BS scheme and the G-SupCon loss function are effective in terms of the accuracy on few-shot node classification tasks.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Component Analysis</head><p>In this section, we demonstrate how our framework is sensitive to the choice of each component in it. We have considered the architecture of encoder g ? , graph temperature parameter ?, and batch size B.</p><p>Encoder Architecture Our framework is independent from the encoder architecture. We evaluate our framework with three widely-used GNNs: GCN <ref type="bibr" target="#b4">[Kipf and Welling, 2016]</ref>, <ref type="bibr">GAT [Veli?kovi? et al., 2017]</ref>, and GIN <ref type="bibr" target="#b11">[Xu et al., 2018]</ref>, as shown in Table <ref type="table" target="#tab_4">5</ref>.</p><p>The variance between different encoder architectures is insignificant, so we choose GCN as the default encoder.</p><p>Graph Temperature Parameter and Batch Size As presented in Figure <ref type="figure" target="#fig_3">3</ref> (a), we test the sensitivity of our framework regarding the graph temperature parameter ? and batch size B. Observably, our framework is not that sensitive to these two parameters. The best value of ? is 1.0 and we set it as default. Generally, the larger batch size can produce higher scores because more contrast can be made in each batch. We choose 500 as the default batch size for computational efficiency and its sufficiently decent performance. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The t-SNE visualization for the representation of nodes in novel classes from the (a) GNN encoder trained with cross-entropy loss on base classes (b) GPN meta-trained on base classes.</figDesc><graphic url="image-1.png" coords="1,328.27,216.00,218.70,94.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>and prototypical network based GPN [Ding et al., 2020]. ? The state-of-the-art self-supervised pretraining methods on a single graph: MVGRL [Hassani and Khasahmadi, 2020], SUBG-Con [Jiao et al., 2020], GraphCL [You et al., 2020], and GCA [Zhu</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: t-SNE embedding visualization on CoraFull datasets: (a) GPN (b) GCA (C) ours.</figDesc><graphic url="image-2.png" coords="6,54.00,278.97,243.00,81.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: (a) Accuracy vs Graph temperature parameter ? (b) Accuracy vs Batch size B on the three datasets.</figDesc><graphic url="image-3.png" coords="6,315.00,406.84,243.00,89.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the expermental datasets.</figDesc><table><row><cell></cell><cell>CoraFull</cell><cell>Reddit</cell><cell>Ogbn-arxiv</cell></row><row><cell># Nodes</cell><cell>19,793</cell><cell>232,965</cell><cell>169,343</cell></row><row><cell># Edges</cell><cell cols="3">126,842 11,606,919 1,166,243</cell></row><row><cell># Features</cell><cell>8,710</cell><cell>602</cell><cell>128</cell></row><row><cell># Labels</cell><cell>70</cell><cell>41</cell><cell>40</cell></row><row><cell>Pretrain</cell><cell>42</cell><cell>24</cell><cell>24</cell></row><row><cell>Test</cell><cell>28</cell><cell>17</cell><cell>16</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc>Performance on novel classes query node embedding clustering, reported in Normalized Mutual Information (NMI) and Adjusted Rand Index (ARI).</figDesc><table><row><cell>Methods</cell><cell></cell><cell cols="2">CoraFull NMI ARI</cell><cell>Reddit NMI ARI</cell><cell>Ogbn-arxiv NMI ARI</cell></row><row><cell>GPN</cell><cell cols="5">0.5134 0.4327 0.3690 0.3115 0.2905 0.2235</cell></row><row><cell>GCA</cell><cell cols="5">0.7531 0.7351 0.7824 0.7756 0.3786 0.3219</cell></row><row><cell>Ours</cell><cell cols="5">0.8567 0.8229 0.8890 0.8720 0.5280 0.4485</cell></row><row><cell cols="6">Loss Function BS CoraFull (%) Reddit (%) Ogbn-arxiv (%)</cell></row><row><cell cols="3">Cross Entropy No</cell><cell>70.18</cell><cell>69.80</cell><cell>62.25</cell></row><row><cell cols="3">Cross Entropy Yes</cell><cell>73.86</cell><cell>74.08</cell><cell>63.67</cell></row><row><cell>SimCLR</cell><cell></cell><cell>No</cell><cell>87.54</cell><cell>89.60</cell><cell>66.34</cell></row><row><cell>SimCLR</cell><cell></cell><cell>Yes</cell><cell>89.48</cell><cell>92.88</cell><cell>69.98</cell></row><row><cell cols="2">G-SupCon</cell><cell>Yes</cell><cell>93.62</cell><cell>94.12</cell><cell>73.65</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Results of our model trained with different loss functions.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Results of our model with different encoders.</figDesc><table /></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Node Connectivity</head><p>In this section we give details about how to calculate connectivity scores between any pair of nodes in a given graph, without using the node attributes:</p><p>Where S ? R M ?M is the target node connectivity score matrix. Each column S i of S contains the scores between node i and all nodes in the graph. Specially, we set the score between a node and itself as a constant value ? = 0.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Node Algebraic Distance (NAD)</head><p>Following <ref type="bibr">[Chen and Safro, 2009]</ref>, we first randomly assign a random value u i to any node i in the graph to form a vector u ? R M . Then, the algorithm iteratively updates the value of a node by congregating its neighboring weighted values. After a few iterations, the difference between the values of node i and j indicates the coupling between them. The smaller difference stands for a stronger connection: At tth iteration, for node i:</p><p>Where ? is a parameter set to 0.5. The final score matrix we have is:</p><p>Where = 0.01. We column-normalize S and then set the value of S(i, i), (?i ? M ) to ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Personalized PageRank (PPR)</head><p>For PPR we have:</p><p>Where ? is a parameter usually set as 0.15, and D denotes the diagonal matrix: D(i, i) = j A(i, j). We columnnormalize the scores S to make the scores on the diagonal equal to ?.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">5-shot 5-way 5-shot 3-way 1-shot 10-way 5-shot 5-way 5-shot 3-way 1-shot 10-way 5-shot 5-way 5-shot 3-way 1-shot GCN CEL</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Guneet Singh Dhillon, Pratik Chaudhari, Avinash Ravichandran, and Stefano Soatto. A baseline for few-shot image classification</title>
		<author>
			<persName><surname>Bachman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.00910</idno>
		<idno>arXiv:0909.4275</idno>
	</analytic>
	<monogr>
		<title level="m">Jie Chen and Ilya Safro. A measure of the connection strengths between graph vertices with applications</title>
		<imprint>
			<publisher>Kaize Ding</publisher>
			<date type="published" when="2009">2019. 2019. 2017. 2009. 2009. 2020. 2020. 2019. 2019. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>CIKM</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Jeh and Widom, 2003] Glen Jeh and Jennifer Widom. Scaling personalized web search</title>
		<author>
			<persName><surname>Finn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<publisher>WWW</publisher>
			<date type="published" when="2003">2017. 2017. 2020. 2020. 2021. 2021. 2017. 2017. 2020. 2003. 2020</date>
		</imprint>
	</monogr>
	<note>ICDM</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><surname>Khosla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Supervised contrastive learning. NeurIPS, 2020</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Estimating node importance in knowledge graphs using graph neural networks</title>
		<author>
			<persName><forename type="first">Welling ;</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><surname>Liao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<idno>arXiv:1901.01484</idno>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016">2016. 2016. 2019. 2019. 2021. 2021. 2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>KDD</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName><surname>Snell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Explaining the power-law degree distribution in a social commerce network</title>
		<author>
			<persName><forename type="first">Toubia ;</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName><surname>Toubia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Networks</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rethinking few-shot image classification: a good embedding is all you need</title>
		<author>
			<persName><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Veli?kovi? et al., 2017] Petar Veli?kovi?, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks</title>
		<author>
			<persName><surname>Tong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2017">2021. 2021. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Directed graph contrastive learning</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Graph few-shot learning with attribute matching</title>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cline: Contrastive learning with semantic negative examples for natural language understanding</title>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">How powerful are graph neural net</title>
		<author>
			<persName><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">works? arXiv preprint</note>
	<note>You et al., 2020. Graph contrastive learning with augmentations. NeurIPS, 2020</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On few-shot node classification in graph metalearning</title>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<publisher>WWW</publisher>
			<date type="published" when="2019">2019. 2019. 2021</date>
		</imprint>
	</monogr>
	<note>Graph contrastive learning with adaptive augmentation</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
