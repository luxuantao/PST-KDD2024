<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Empirical Investigation of Symbolic Pointer Analysis Algorithms-Preliminary Report</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ben</forename><surname>Hardekopf</surname></persName>
							<email>benh@cs.utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Calvin</forename><surname>Lin</surname></persName>
							<email>lin@cs.utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Doc</forename><surname>Shankar</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Linux Technology Center</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">An Empirical Investigation of Symbolic Pointer Analysis Algorithms-Preliminary Report</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8917A0A58399DBE2BF012CB1CA7F0C84</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Pointer analysis is a necessary prerequisite to many program analyses and transformations. The speed and performance of these client analyses is heavily dependent on the nature of the pointer analysis algorithm used. Due to the high cost of pointer analysis, it is common practice to use only the least precise, and hence fastest, pointer analysis algorithms, with an attendant negative impact on the effectiveness of the client analyses. The research described in this paper explores the application of symbolic pointer analysis to this problem, an approach that promises to dramatically lower the cost of higher-precision pointer analyses. For this preliminary report, we have implemented three different symbolic algorithms for flowand context-insensitive pointer analysis inspired by previously published research. We evaluate the relative performance of these algorithms, as well as the effectiveness of a number of potential optimizations. Our results are encouraging; for example, an interprocedural flow-and context-insensitive Andersenstyle pointer analysis on the entire Linux kernel (consisting of over 1 million lines of code) can be accomplished in approximately 1 minute. A comparative evaluation of these algorithms and optimizations has never been previously published.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Pointer analysis determines, for any particular memory reference in a program, the set of possible memory locations which that reference might access. This information is fundamental to program analysis and transformation in many programming languages, even those in which the programmer never explicitly deals with pointers. Clients requiring this information range from compiler optimizations to software verifiers and program understanding tools. Many of these clients require interprocedural analysis of very large programs; for example, performing software verification on the Linux kernel can require analyzing millions of lines of code. Pointer analysis can be considered a service provided to these various client analyses, which depend on the pointer analysis to determine the flow of information and values through the program. The speed and precision, and hence usefulness, of these clients heavily depend on the precision of the pointer analysis algorithm being used <ref type="bibr" target="#b9">[10]</ref>.</p><p>Because of this dependency by the client, pointer analysis has been extensively studied, and many pointer analysis algorithms have been devised with varying trade-offs between precision and performance. Precise pointer analysis can be very expensive, and due to this high cost it is common practice to use only the least precise, and hence fastest, of these algorithms in most tools (e.g. <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8]</ref>). Those tools which use more precise algorithms pay a heavy cost in performance, and generally do not scale well to large programs.</p><p>Recently a technique called symbolic pointer analysis has been introduced which may hold the key to dramatically lowering the cost of high-precision pointer analyses <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b11">12]</ref>. Symbolic pointer analysis is based on the use of Binary Decision Diagrams (BDDs), a data structure which has been extensively studied and used by the model checking community; however, the relevance of this data structure to program analysis has only recently been realized. The study of symbolic program analysis in general, and symbolic pointer analysis in particular, is still in its infancy, and there is no clear consensus on the limitations of this technique and how best to take advantage of it. The purpose of our research is to explore various approaches to symbolic pointer analysis, in order to both determine its limits and to determine the best methods and algorithms for taking advantage of its benefits.</p><p>The experiments and results described in this paper represent only a preliminary report. Currently, only a subset of the full range of pointer analyses has been implemented, specifically only a flow-and context-insensitive analysis, and only a small set of benchmarks has been studied. Nevertheless, the results have been encouraging. The symbolic algorithms have not only been effective, but are also very easy to implement. Future work includes studying both flow-and context-sensitivity and is described in more detail in the concluding section.</p><p>The remainder of this paper is organized as follows: Section 2 provides some background information on pointer analysis and BDDs; Section 3 describes related work; Section 4 describes the symbolic pointer analysis algorithms we studied and provides some experimental results; finally Section 5 concludes the paper and describes our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Pointer Analysis</head><p>The most precise of the flow-and context-insensitive algorithms is Andersen-style pointer analysis <ref type="bibr" target="#b0">[1]</ref>. This algorithm can be described as a set-constraint problem: each pointer assignment of the form a = b generates a constraint of the form a ⊇ b, i.e. the set of memory locations referenced by a is a superset of the set of memory locations referenced by b. This is true because flow-insensitive analysis ignores strong updates. Each assignment of the form a = &amp;b generates a constraint of the form a ⊇ {b}, i.e. pointer a references memory location b. In a language with explicit pointer manipulation such as C, we can divide these constraints into 3 types: basic constraints, simple constraints, and complex constraints. Basic constraints are those of the form a ⊇ {b}. Simple constraints are those that involve no pointer dereferences, e.g. a ⊇ b. Complex constraints are those that do involve pointer dereferences, e.g. * a ⊇ b or * a ⊇ * b. All of the constraints can be generated by a simple linear pass over the program. The resulting constraint system can be represented as two graphs, the constraint graph G and the pointsto graph P . The nodes of both graphs are the vari-ables of the program. If there is a simple constraint a ⊇ b, then G has an edge from node a to node b. If there is a basic constraint a ⊇ {b}, then P has an edge from node a to node b. Complex constraints are more difficult-since they involve pointer dereferences, we can't add the correct edges to G unless we know the possible values that the pointers might reference; since we need to have performed pointer analysis to answer this question, we have a chickenand-egg problem. The solution is to keep the complex constraints in a list L, and dynamically add edges to G during the pointer analysis as preliminary solutions become available.</p><p>The basic process for solving the system of constraints is conceptually very simple-we propagate the points-to information contained in P along the edges of the constraint graph G by computing the dynamic transitive closure of the graph. If G has an edge from node a to node b, meaning that a's pointsto set is a superset of b's points-to set, and P has an edge from node b to node c, meaning that b references location c, then to satisfy the constraints we need to add an edge in P from node a to node c, meaning that a also references location c. As these values are propagated, they can be used to add new edges to G from the complex constraints in L: e.g. if there is a complex constraint a ⊇ * b, then we simply look at the set of nodes in P directly pointed to by b, which is exactly the set of nodes referenced by *b, and add an edge in G from node a to each element of that set. This entire process is iterated until the solution converges and the graphs no longer change.</p><p>Traditional Andersen-style pointer analyses all have a complexity of O(n 3 ). This common bound arises from the computation of the dynamic transitive closure, for which no lower bound is currently known. It is generally very difficult to scale Andersen-style analyses to large programs, e.g. tens or hundreds of thousands of lines of code, much less millions of lines of code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Binary Decision Diagrams</head><p>BDDs are data structures that are well-suited for compactly representing sets and relations. We are specifically interested in Reduced Ordered BDDs <ref type="bibr" target="#b3">[4]</ref>, which are usually what is meant by the term BDD unless otherwise specified. Suppose we want to maintain a set of up to n objects, labeled 0...n -1. Each object label takes m bits, where m = lg n . Each of the m bit-positions in the label is itself labeled, x 0 ...x m-1 , with x 0 being the most significant bit-position. We can now construct a binary decision tree to specify which of the n possible objects are currently in the set. Starting with x 0 as the root, we draw an edge for each possible value of the bit: x 0 can be 1 or 0, then for each of those possibilities x 1 can be either 1 or 0, and so on until we have constructed a tree of height m. Then for each leaf of the tree, we label it either 1 or 0 depending on whether the path from the root to that leaf constitutes the label of an object in the set.</p><p>This tree is obviously exponentially-sized with respect to m, which doesn't make it very useful. But we can compact the tree by following two simple rules: 1) any two subtrees which are exactly the same can be combined into a single subtree by taking all edges pointing to one of the subtrees and making them point to the second; 2) Any node whose outgoing edges both point to the same node can be eliminated, by having all incoming edges to that node rerouted to the node that the outgoing edges point to. If we iteratively apply these rules until the tree cannot be compacted further, then we have transformed the tree into a DAG called a binary decision diagram. It is important to note that the initial tree doesn't have to be built before building the BDD; the process has been explained this way for purely expository purposes.</p><p>For many (but not all) sets, this process can reduce the exponential-size tree to a polynomial-size DAG, resulting in large space savings. In addition to saving space, this data structure can also save time. Many common logical operations, such as set union and intersection, take time proportional to the size of the BDD, not the number of elements in the set. In other words, the more compact the BDD the faster the set can be manipulated, regardless of how many elements are in the set. In fact, the number of elements in the set has a low correlation with the size of the BDD-adding elements to the set can actually decrease the size of the BDD. The most critical factor determining the size of the BDD is the order in which the bit-positions are laid out when building the initial decision tree. In the example, we labeled each level of the tree from x 0 to x m-1 in sequence. We could rearrange this ordering to label each level of the tree with an arbitrary bit-position, and the particular ordering we choose can greatly affect the size of the resulting BDD.</p><p>We can specify relations in a similar manner. For example, to represent a graph with n nodes as a BDD, we can label each node of the graph with a number 0...n -1, just as we did above. If there is an edge from node a to node b, then we concatenate the label for a with the label for b, resulting in a bit-string with 2m digits. We do this for all edges and then construct a BDD from the resulting bit-strings, just as we did for the set example. Again, the size of the BDD is only loosely correlated with the size of the graph, and many common graph operations, such as reachability, take time proportional to the size of the BDD rather than to the size of the graph itself.</p><p>When dealing with relations we can group bitpositions within the bit-strings into domains, with one domain for each operand of the relation. In the graph example, we have domain V 1 for those bits designating the originating node and domain V 2 for those bits designating the destination node. The BDD can then be described as the relation V 1 × V 2. These domains are simply convenient labels given to various subsets of bit-positions in the bit-string. In the graph example, the first m bit-positions of the bitstring are V 1 and the second m bit-positions are V 2.</p><p>We can now describe several important BDD operations in terms of these domains.</p><p>Existential quantification is one such operation. Given a relation R ⊆ V 1×V 2 describing a graph, we can determine all nodes with an incoming edge simply by existentially quantifying the relation over V 1:</p><formula xml:id="formula_0">S in = exist(R, V 1) = {y | ∃x.(x, y) ∈ R}.</formula><p>Similarly, we can determine all nodes with an outgoing edge by existentially quantifying over V 2:</p><formula xml:id="formula_1">S out = exist(R, V 2) = {x | ∃y.(x, y) ∈ R}.</formula><p>Another frequently used operation is replace. This operation simply moves the information stored in one domain to a different domain. For example, given the same graph relation R as above, suppose we want to find all nodes with both an incoming and outgoing edge. First we use existential quantification to calculate S in and S out as above. Then we want to take the conjunction of S in and S out to find the set of nodes common to both-but S in ⊆ V 2 and S out ⊆ V 1. To fix this, we use the replace operation to change the domain of S in from V 2 to V 1: replace(S in , V 2 → V 1). We can now take their conjunction and get the correct answer.</p><p>Finally, there is the relational product operator. This operator takes two BDDs and a domain as arguments; it computes the conjunction of the BDDs then existentially quantifies out the given domain. Given</p><formula xml:id="formula_2">a relation R1 ⊆ V 1 × V 2 and R2 ⊆ V 2 × V 3, the relational product of R1 and R2 with respect to domain V 2 is S = relprod(R1, R2, V 2) = {(x, z) | ∃y.((x, y) ∈ R1 ∧ (y, z) ∈ R2)}. Essen- tially, this operation calculates transitivity: if (a, b) ∈ R1 and (b, c) ∈ R2, then (a, c) ∈ S.</formula><p>It is also possible to compute the relational product of a set and a relation. For example, if</p><formula xml:id="formula_3">S ⊆ V 1, then relprod(R1, S, V 1) = {y | ∃x.((x, y) ∈ R1 ∧ x ∈ S)}.</formula><p>Andersen-style pointer analysis was first introduced in Andersen's dissertation <ref type="bibr" target="#b0">[1]</ref>. Since then, it has been extensively studied. Fahndrich et al. <ref type="bibr" target="#b4">[5]</ref> developed several optimizations for the analysis, including the idea of cycle elimination, which were crucial for improving the algorithm's scalability. Heintze et al. <ref type="bibr" target="#b6">[7]</ref> futher improved the scalability of the analysis and currently represents the fastest known explicit (i.e. non-symbolic) flow-insensitive Andersenstyle pointer analysis in the literature; however, their method requires an extensive engineering effort to be effective.</p><p>The idea of borrowing BDDs from the model checking community and applying them to pointer analysis was first described by Zhu <ref type="bibr" target="#b13">[14]</ref>. He developed a context-sensitive, flow-insensitive symbolic pointer analysis for C, based on Andersen-style analysis. He later revised and improved his algorithm, providing context-sensitive and context-insensitive versions, both of which utilize symbolic transfer functions to summarize individual procedures <ref type="bibr" target="#b14">[15]</ref>.</p><p>Berndl et al. <ref type="bibr" target="#b2">[3]</ref> were the first to apply the idea of symbolic pointer analysis to Java. They developed a flow-and context-insensitive algorithm, also based on Andersen-style analysis. Whaley and Lam <ref type="bibr" target="#b11">[12]</ref> later extended this idea to provide a context-sensitive analysis for Java, which unlike Zhu's context-sensitive analysis does not utilize transfer functions at all.</p><p>The symbolic algorithm developed by Zhu and the algorithm developed by Berndl et al. and later extended by Whaley and Lam take very different approaches to using BDDs for pointer analysis, even disregarding the fact that they were developed for different languages. The third symbolic algorithm we developed, inspired by Heintze et al.'s explicit approach, differs from both of these. These disparate approaches are what motivates our study of the relative effectiveness of these techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Symbolic Pointer Analysis</head><p>The three symbolic algorithms studied in this paper were inspired by the approaches of Zhu, Berndl et al., and Heintze et al., though the algorithms have been heavily modified. For example, Berndl et al.'s analysis has been modified to work for C rather than Java, and Heintze et al.'s explicit analysis has been transformed into a symbolic analysis.</p><p>The algorithms we describe are all targeted for the C language. They are all field-insensitive. We assume that a constraint generator has already parsed the program and generated the set of constraints; these algorithms are solely concerned with solving the given constraint system. Constraints are given in terms of access paths. An access path is a tuple (a, k), where a is a base variable and k is an integer describing the level of pointer dereferencing. For example, the complex constraint * * a ⊇ b is given as (a, 2) ⊇ (b, 0), and the basic constraint a ⊇ &amp;b is given as (a, 0) ⊇ (b, -1). To simplify the implementation, we ignore function pointers and assume that all pointers are well-typed (i.e. that only variables declared as pointers will hold address information). BDDs are manipulated using the BuDDy BDD package <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The Algorithms</head><p>There are several common data structures that are used by the algorithms described in this section. They are:</p><formula xml:id="formula_4">• G ⊆ V 1 × V 2 -the constraint graph, imple-</formula><p>mented as a BDD. The BDD domain V 1 represents the domain of the constraint relation, and the BDD domain V 2 represents the range of the constraint relation.</p><formula xml:id="formula_5">If (∃a ∈ V 1, b ∈ V 2).((a, b) ∈ G)</formula><p>, then the points-to set of a is a superset of the points-to set of b. G is initialized with the simple constraints.</p><p>• P ⊆ V 2 × V 3 -the points-to graph, implemented as a BDD. The BDD domain V 2 serves double-duty as the domain of the pointsto graph, as well as the range of the constraint graph. The BDD domain V 3 serves as the range of the points-to graph. If (∃a ∈ V 2, b ∈ V 3).((a, b) ∈ P ), then pointer a may point to location b. P is initialized with the basic constraints.</p><p>• L -a list of constraints, implemented as an array. L contains the complex constraints.</p><p>There is also a common operation called the reachability envelope. The reachability envelope function has two arguments: a set of variables S ⊆ V 2, and an integer k. It returns the set of variables which can be reached in P starting from S and following exactly k edges. It is used to resolve pointer dereferences: given the access path (a, k), the value of reach env({a}, k) is the set of variables referenced by that access path. A pseudo-code implementation of the function is shown below. For clarity, each set and relation is subscripted with its type:</p><formula xml:id="formula_6">reach env(S [V 2] , k): for i in 1..k do S [V 3] = relprod(P [V 2×V 3] , S [V 2] , V 2) S [V 2] = replace(S [V 3] , V 3 → V 2) return S [V 2]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Algorithm A</head><p>This algorithm alternates between two phases: using the points-to graph and the complex constraints to add to the constraint graph, and using the constraint graph to add to the points-to graph. The pseudo-code is shown below:</p><formula xml:id="formula_7">do Phase 1 foreach constraint (a, k) ⊇ (b, l) ∈ L do D [V 2] = reach env({a}, k) D [V 1] = replace(D [V 2] , V 2 → V 1) R [V 2] = reach env({b}, l) T [V 1×V 2] = D [V 1] ∧ R [V 2] G [V 1×V 2] = G [V 1×V 2] ∪ T [V 1×V 2] Phase 2 do T [V 1×V 3] = relprod(G [V 1×V 2] , P [V 2×V 3] , V 2) T [V 2×V 3] = replace(T [V 1×V 3] , V 1 → V 2) P [V 2×V 3] = P [V 2×V 3] ∪ T [V 2×V 3] until P [V 2×V 3] doesn't change until P [V 2×V 3] doesn't change</formula><p>Phase 1 iterates through L to process the complex constraints. For each constraint, it uses the current points-to graph to determine the set of variables referenced by (a, k) and the set of variables referenced by (b, l). It then adds the appropriate edges to the constraint graph. Phase 2 propagates the points-to information along the edges of the constraint graph using the relational product operator, repeating until the points-to graph doesn't change. These two phases together are themselves repeated until the points-to graph no longer changes.</p><p>The rationale behind this algorithm is that by using G and P to store the constraint and points-to relations, propagating the points-to information becomes very simple and fast. Each iteration of the do-loop in Phase 2 propagates the points-to information for every variable a distance of one edge in G, all at the same time. The number of iterations of the loop is therefore bounded by the maximum breadth of the constraint graph.</p><p>This algorithm is closely related to the algorithm developed by Berndl et al. <ref type="bibr" target="#b2">[3]</ref>. Since this algorithm targets C we need to deal with complex constraints, which isn't necessary in Java since there are no explicit pointers. However, since Java is object-oriented the analysis developed by Berndl et al. had to be field-sensitive to be useful. Field-sensitivity is more difficult in C, and we have not yet implemented it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Algorithm B</head><p>This algorithm doesn't use the constraint graph at all-the simple constraints are stored in L along with the complex constraints. The algorithm continuously iterates over L, processing the constraints and adding edges to the points-to graph until the points-to graph ceases to change. The pseudo-code is shown below:</p><formula xml:id="formula_8">do foreach constraint (a, k) ⊇ (b, l) ∈ L do D [V 2] = reach env({a}, k) S [V 2] = reach env({b}, l) R [V 2] = reach env(S [V 2] , 1) R [V 3] = replace(R [V 2] , V 2 → V 3) T [V 2×V 3] = D [V 2] ∧ R [V 3] P [V 2×V 3] = P [V 2×V 3] ∪ T [V 2×V 3] until P [V 2×V 3] doesn't change</formula><p>For each constraint in L, the algorithm first determines the set of variables referenced by (a, k) (that is, D) and the set of variables referenced by (b, l) (that is, S). It then finds the set of variables pointed to by S (yielding R). Finally, it adds an edge in P for each variable in D to each variable in R. Essentially, it is just directly interpreting the constraint-the constraint states that the points-to sets of the variables referenced by (a, k) are supersets of the points-to sets of the variables referenced by (b, l). Therefore, the algorithm simply finds the points-to sets of the variables referenced by (b, l) and adds them to the pointsto sets of the variables referenced by (a, k).</p><p>The rationale of this algorithm is that the pointsto graph P usually generates a very small BDD, and hence any operations involving the points-to graph are very fast. The disadvantage with respect to algorithm A is that rather than propagating points-to information for every variable at once along all the constraints, instead we have to propagate the information for each constraint separately. The advantage of discarding the constraint graph is that we no longer incur a cost proportional to the product of the size of G and the size of P when we are propagating information.</p><p>This algorithm is closely related to the algorithm employed by Zhu for summarizing the points-to information of each procedure <ref type="bibr" target="#b13">[14]</ref>. However, rather than creating summaries for each procedure independently and then processing the summaries to compute the final solution for the entire program, instead we solve all the constraints for all the procedures at once.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Algorithm C</head><p>This algorithm uses both P and G, but unlike the previous two it doesn't modify P at all during the analysis. It continuously iterates through the complex constraints in order to add new edges to the constraint graph until G no longer changes. During this process, it uses both G and the initial P to resolve the complex constraints and find the variables being referenced. Once G has converged, the algorithm propagates the initial points-to information in P along the final constraint graph to obtain the final points-to solution. The pseudo-code is shown below:</p><formula xml:id="formula_9">Phase 1 do foreach constraint (a, k) ⊇ (b, l) ∈ L do D [V 1] = resolve(a, k) R [V 1] = resolve(b, l) R [V 2] = replace(R [V 1] , V 1 → V 2) T [V 1×V 2] = D [V 1] ∧ R [V 2] G [V 1×V 2] = G [V 1×V 2] ∪ T [V 1×V 2] until G [V 1×V 2] no longer changes Phase 2 do T [V 1×V 3] = relprod(G [V 1×V 2] , P [V 2×V 3] , V 2) T [V 2×V 3] = replace(T [V 1×V 3] , V 1 → V 2) P [V 2×V 3] = P [V 2×V 3] ∪ T [V 2×V 3] until P [V 2×V 3] doesn't change</formula><p>Phase 1 determines for each constraint in L the set of variables referenced by (a, k) (that is, D)and the set of variables referenced by (b, l) (that is, R), then adds new edges in G from the variables in D to the variables in R. It continues to iterate over L until the constraint graph no longer changes. In phase 2 it then takes the completed constraint graph and uses it to propagate the initial points-to information as it did in algorithm A, the difference being that it does this once at the end rather than for each iteration through L.</p><p>Since P is never updated during phase 1, the algorithm must use another method besides the reachability envelope to calculate the set of variables being referenced for D and R. It uses the resolve function for this purpose, which takes the components of the access path as arguments. The resolve function makes use of another function, reach, which computes reachability in G. The pseudo-code for reach and resolve is shown below:</p><formula xml:id="formula_10">resolve(a, k): N [V 1] = {a} for i in 1..k do D [V 1] = reach(N [V 1] ) D [V 2] = replace(D [V 1] , V 1 → V 2) N [V 2] = reach env(D [V 2] , 1) N [V 1] = replace(N [V 2] , V 2 → V 1) return N [V 1] reach(N [V 1] ): do N [V 2] = relprod(G [V 1×V 2] , N [V 1] , V 1) N [V 1] = replace(N [V 2] , V 2 → V 1) until N [V 1] no longer changes</formula><p>First the resolve function initializes the set N to contain the base variable of the constraint, a. Then for each level of pointer derefence, the resolve function first computes all the variables in G reachable from N using the reach function. The reason it does this is that constraints are transitive: if a ⊇ b ⊇ c, then a ⊇ c. Since the points-to information for the variables isn't being propagated along the constraint graph, then instead the algorithm needs to go find the relevant variables itself so it can use their points-to information as given in the initial points-to graph. Once the function has found all the reachable variables, it uses reach env to determine the set of variables pointed to by the reachable variables, and sets N equal to the result. This process is iterated once for each level of pointer dereference. The end result is the set of variables referenced by the access path (a, k)-the same information we could have gotten just using reach env, as we did for algorithms A and B, if we had been updating the points-to graph during the analysis.</p><p>The rationale for this algorithm is roughly the dual of the rationale for algorithm B: it hopes that the BDD representation of the constraint graph G is small, and that by changing only G (at least until the very end when the final points-to solution is computed) it can avoid the cost of propagating the constraint graph and points-to graph together.</p><p>This algorithm was inspired by the explicit analysis developed by Heintze et al. <ref type="bibr" target="#b6">[7]</ref>. However, the rationales for the two algorithms are very dif-ferent. Heintze et al. developed the algorithm with cycle elimination in mind-the reachability calculation used during the resolve function makes total cycle elimination almost free when G is represented as an explicit graph. This was a large advance from previous cycle elimination algorithms which could only eliminate some of the cycles, due to the large cost of cycle detection. The algorithm developed by Heintze et al. depends on cycle elimination and several other optimizations to be practical, and even still requires extensive engineering and memory management by the programmer to achieve its best performance; this is in contrast to the symbolic algorithm outlined above which requires little more code than that given in the pseudo-code to be effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The Optimizations</head><p>The following sections describe a variety of potential optimizations for the above algorithms. Many of them are only relevant to one or two of the algorithms; this will be noted in the description. Some of the optimizations have been suggested previously, and this has also been noted in the description. Along with each description is a rationale explaining why the optimization might be expected to help increase performance.</p><p>Transitive Closure The performances of both algorithm A and algorithm C are dependent on the breadth of the constraint graph G. Algorithm A propagates the points-to information along G, and the number of loop iterations required depends in G's breadth. Algorithm C computes reachability in G, and the loop iterations required for this also depend on the breadth of G. This fact suggests that by reducing the breadth of G, we could increase the performance of these two algorithms. We can do this by explicitly computing G's transitive closure: for any graph edges (a, b) and (b, c), add an edge (a, c). Iterate until the graph no longer changes. The new graph now has a breadth of 1, and since constraints are transitive the graph still retains the same information as the old graph.</p><p>Implementing this optimization is trivial: at the beginning of each outer loop, insert a call to a procedure tc which will compute the transitive closure of G. The closure can be computed swiftly using the iterative squaring method, as outlined in the pseudocode below:</p><formula xml:id="formula_11">tc(G [V 1×V 2] ): T [V 1×V 3] = replace(G [V 2×V 3] , V 2 → V 3) T [V 2×V 3] = replace(T [V 1×V 3] , V 1 → V 2) do S [V 1×V 3] = relprod(G [V 1×V 2] , T [V 2×V 3] , V 2) G [V 1×V 2] = replace(S [V 1×V 3] , V 3 → V 2) T [V 2×V 3] = replace(S [V 1×V 3] , V 1 → V 2) until G[V 1 × V 2] no longer changes</formula><p>Incrementalization This optimization was first suggested in Berndl et al. <ref type="bibr" target="#b2">[3]</ref>. They noted that when the points-to information was being propagated along the constraint graph, each iteration of the loop was propagating all the information, even the information that had already been propagated in earlier iterations. The optimization is to propagate only the information new to each iteration, by substracting out the information from previous iterations. This tends to greatly decrease the size of the BDD representing the points-to information and thereby speeds up the propagation. The appropriate modification to Phase 2 of algorithm A is shown below:</p><formula xml:id="formula_12">Phase 2 S [V 2×V 3] = P [V 2×V 3] do T [V 1×V 3] = relprod(G [V 1×V 2] , S [V 2×V 3] , V 2) S [V 2×V 3] = replace(T [V 1×V 3] , V 1 → V 2) S [V 2×V 3] = S [V 2×V 3] -P [V 2×V 3] P [V 2×V 3] = P [V 2×V 3] ∪ S [V 2×V 3] until P [V 2×V 3] doesn't change</formula><p>Following the same logic, we can extend this optimization to enhance the reach function of algorithm C:</p><formula xml:id="formula_13">reach(N [V 1] ): S [V 1] = N [V 1] do S [V 2] = relprod(G, S, V 1) S [V 1] = replace(N [V 2] , V 2 → V 1) S [V 1] = S [V 1] -N [V 1] N [V 1] = N [V 1] ∪ S [V 1] until N [V 1] no longer changes</formula><p>Constraint Ordering This optimization is only relevant to algorithm B. This algorithm continously iterates through L, which in this case contains both the simple and complex constraints, in order to add edges to P . The order in which these constraints are processed can have a large affect on the number of iterations required to reach convergence, and hence have an affect on performance. If there are two constraints such as a ⊇ b and b ⊇ c, then we would rather process b ⊇ c before we process a ⊇ b; that way in one iteration we can propagate c's information to a. If they are processed in the reverse order, then it would take an additional iteration through the list to get that information.</p><p>The solution is to order the constraints by their dependencies. In the above example, a depends on b, since a is on the right-hand of a constraint and b is on the left, and similarly b is dependent on c, therefore we can order them a priori to minimize the required iterations. Of course, a total ordering is not always possible since the constraints may contain loops in the dependencies, e.g. it is possible for a to depend on b and also for b to depend on a.</p><p>Ordering is accomplished using the following steps:</p><p>1. Build a constraint dependence graph (an explicit graph, rather than one implemented with BDDs, for reasons that will be explained shortly). The graph has an directed edge from one constraint to another if the second constraint has a dependency on the originating constraint. This step can be accomplished in time linear in the number of constraints.</p><p>2. Find all strongly-connected components in the dependency graph, and create a list containing one representative node from each SCC. This can be accomplished in linear time using Tarjan's SCC algorithm <ref type="bibr" target="#b10">[11]</ref>.</p><p>3. Iterate through the list of SCC representatives. If a particular representative has not yet been numbered, then number the unvisited subgraph rooted in that node in reverse-postorder. This step takes time linear in the number of constraints, since each constraint is visited only once. This step is the reason why BDDs can't be used -each node must get a unique number, and BDDs are not well-suited for dealing with each member of a set individually.</p><p>4. Finally, order the constraints according to the numbers we just assigned. This step also takes linear time, so the over-all complexity of ordering the constraints is linear in the number of constraints.</p><p>Note that this algorithm is only ordering the constraints based on the first part of the access path. This doesn't matter for simple constraints, for which the second part of the access path is 0, but for complex constraints this is suboptimal. However, without pointer analysis it is not possible to correctly order the complex constraints anyway.</p><p>Query Caching One of the optimizations critical to the success of Heintze et al.'s algorithm <ref type="bibr" target="#b6">[7]</ref> is caching the reachability queries. During each iteration of the loop in Phase 1 of algorithm C, the results of the resolve function calls are cached in a table. The table is cleared at the beginning of the next loop iteration. We extended this idea to also cache the results of the reachability envelope function for algorithms A and B. Caching should reduce the number of times we need to access the BDD data structure, thereby increasing performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Program Decomposition</head><p>Even though graph size is only loosely correlated with BDD size, there is some correlation-by dramatically reducing the size of the graph, we would expect the BDD representing that graph to usually also become smaller, though this isn't guaranteed. The smaller we can make G and P , the more efficient the BDD operations become and hence the better the performance. One way to reduce the sizes of G and P is to partition the constraint system into sets, such that each set of constraints can be solved independently.</p><p>A method for doing precisely this was described in a paper by Zhang et al. <ref type="bibr" target="#b12">[13]</ref>, though for a completely different purpose. Their method is intended to partition the constraints into independent sets so that each set could be solved using either flow-insensitive or flow-sensitive analysis, with various heuristics being used to decide for a given set which strategy was most appropriate. Their algorithm is almost linear, using a fast union-find data structure to partition the constraints. For this optimization we first use their algorithm to compute the independent sets, then solve each set independently using our symbolic algorithms.</p><p>Cycle Elimination Cycle elimination has long been known to help the performance of explicit Andersen-style analyses. It was first described by Fahndrich et al. <ref type="bibr" target="#b4">[5]</ref>. The purpose of cycle elimination is to detect strongly-connected components in the constraint graph, then replace those SCCs in the graph with a representative node. Because of the way points-to information is propagated along the constraint graph, all nodes in the same SCC are guaranteed to have the same solution-by replacing SCCs with representative nodes, we both reduce the size of the graph and reduce the number of iterations required to propagate points-to information through the graph.</p><p>Our experiments are the first time cycle elimination has been explored in the context of symbolic pointer analysis. In order to implement this optimization we used a symbolic algorithm designed to detect strongly-connected components within a graph represented as a BDD. This problem has been studied previously in the model checking community, since it is useful in a number of analyses. We implemented the algorithm described in a paper by Gentili et al. <ref type="bibr" target="#b5">[6]</ref>, the best known algorithm for this purpose in terms of computational complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experimental Results</head><p>In order to test the efficacy of the symbolic algorithms and of the optimizations described above, we ran a number of timing experiments. The experiments were conducted on a Pentium4 2.4Ghz computer with 1GB of memory. Times were measured end-toend, from the point immediately after the constraintsystem was read from disk to the point when the final points-to graph was computed. For this preliminary work we have studied only three benchmarks: the apache-1.3.12 core; a minimally configured linux kernel, version 2.4.26 (i.e. configured with most kernel options turned off); and a maximally configured linux kernel (same version, configured with most kernel options turned on). The details of these benchmarks are given in Table <ref type="table" target="#tab_0">1</ref>.</p><p>Figures <ref type="figure" target="#fig_0">1,</ref><ref type="figure" target="#fig_2">2</ref>, and 3 compare the performances of the three symbolic algorithms on each benchmark respectively. The 'baseline' bar for each algorithm represents the performance of the original algorithm with no optimizations. The 'optimized' bar for each algorithm represents the performance of the algorithm with the optimal set of optimizations enabled. To calculate this second value we ran the algorithms on the benchmarks using all possible combinations of the relevant optimizations. For each algorithm we selected the combination of optimizations that performed best. The optimal combination of optimizations chosen for the algorithms are given in table <ref type="table" target="#tab_1">2</ref>.</p><p>The trend is fairly clear: algorithm B is by far the fastest of the algorithms on all of the benchmarks, followed by algorithm A and then algorithm C. The differences between the algorithms are most clearly illustrated by the largest benchmark, lnx-lg. The baseline version of algorithm C wasn't able to complete this benchmark within 60 minutes and was terminated before finishing. The optimized version of algorithm C took over 25 minutes on the same benchmark. Both the baseline and optimized versions of algorithm A took over 6 minutes. The baseline version of algorithm B took a little over 3 minutes, and the optimized version took approximately 1 minute. These results can be explained by looking at the relative sizes of the BDDs involved in the algorithms. Recall that the time required for BDD operations is related to the sizes of the BDDs involved. For all of the benchmarks, the constraint graph BDD is much larger than the points-to graph BDD. Taking the largest benchmark, lnx-lg, as an example, we determined that the constraint graph BDD is 3 times larger than the points-to graph BDD. This means that operations involving the constraint graph take much longer than operations involving the points-to graph. Algorithm A uses both the constraint and points-to graph BDDs, as does algorithm C; however algorithm C makes much more extensive use of the constraint graph than algorithm A. Algorithm B doesn't use the constraint graph at all, and all of it's operations involve only the points-to graph.</p><p>The optimizations did little to help algorithm A, for example the execution time for lnx-lg was decreased by less than 4%. On the other hand, the optimizations were very helpful to both algorithms B and C. For algorithm B the optimizations decreased execution time by 47% for apache, 72% for lnx-sm, and 69% for lnx-lg. For algorithm C the optimizations decreased execution time by 46% for apache and 70% for lnx-sm, and enabled the algorithm to run to completion on lnx-lg within our 60 minute time limit.   None of these optimizations have much of a positive impact on the algorithm, and the last two have very negative affects. Incrementalization, when applied to algorithm A, is used to reduce the size of the points-to graph when propagating the points-to information along the constraint graph. It decreases execution time by only 1-7% on the benchmarks. The reason is that, as explained above, the constraint graph is much larger than the points-to graph. When the constraint graph is the bottleneck, reducing the size of the points-to graph doesn't have much affect. Program decomposition decreases execution time by only 3-10%. It is able to reduce the single original constraint system for each benchmark into hundreds to thousands of indepedent constraint systems; however, for each benchmark there is always one set of constraints that consist of most of the original constraints. There is one set that is 88% of the original constraint system for apache, 92% for lnx-sm, and 91% for lnx-lg. It seems that the almost-linear-time algorithm used to decompose the original constraint system just isn't precise enough to split these large sets up any further.  past results from querying the points-to graph, made while resolving the complex constraints. However, most of algorithm A's time is spent in propagating the constraint and points-to graphs, so reducing the points-to queries while processing the complex constraints has little overall affect.</p><p>Cycle elimination actually increases execution time by a factor of 6-9 for apache and lnx-sm; it is unable to complete within a 60 minute time limit on lnx-lg. Detecting and eliminating cycles in the constraint graph is very expensive, and overwhelms any potential advantages.</p><p>Transitive closure also increases execution time, by a factor of 1.5-2, for apache and lnx-sm. It also is unable to complete lnx-lg within a 60 minute time limit. Given our observation that the size of the constraint graph is a major limiting factor in performance, this result is what we would expectcomputing the transitive closure makes the constraint graph BDD even larger than it already is, overwhelming any advantages from reducing the breadth of the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm B</head><p>The relevant optimizations are constraint ordering, program decomposition, and query caching. As with algorithm A program decomposition has little affect, decreasing execution time by 4-8%, for the same reasons as explained above.</p><p>Query caching has a much greater impact than it did for algorithm A, decreasing execution time by 7-25%. As with algorithm A, query caching is storing past queries to the points-to graph. Since algorithm B spends most of its time in resolving constraints using the points-to graph, it benefits from this optimization more than algorithm A.</p><p>Constraint ordering is the most important optimization for this algorithm, decreasing execution time by 43-71% from the baseline. The initial order of the constraints used in the baseline algorithm is arbitrary, so in order to test the effectiveness of this optimization in the worst case, i.e. when the initial constraints are ordered in exactly the reverse of their dependencies, we ran a second experiment. In this experiment we ordered the constraints as normal, then iterated over them in reverse. Taking this worstcase scenario as our baseline, the constraint ordering optimization decreases execution time by 69-86%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm C</head><p>The relevant optimizations are incrementalization, program decomposition, query caching, cycle elimination, and transitive closure. None of the optimizations applied alone is able to complete the lnx-lg benchmark within the 60 minute time limit, so all of the following remarks concern only the apache and lnx-sm benchmarks.</p><p>Incrementalization, when applied to algorithm C, is used to reduce the size of the constraint graph when computing reachability during the resolve function. As the constraint graph is much larger than the points-to graph and accounts for much of the time spent by the algorithm, this optimization has a very positive affect, decreasing execution time by 33-45%.</p><p>As with algorithms A and B, and for the same reasons, program decomposition had little affect, decreasing execution time by 1-3%.</p><p>Query caching decreases execution time by 19-46%. When applied to algorithm C, query caching stores the past results of reachability queries applied to the constraint graph. The effect is to reduce the number of queries to the constraint graph, which for the same reason as incrementalization results in a large positive impact on performance.</p><p>Cycle elimination was unable to complete within the 60 minute time limit on any of the benchmarks. The cycle detection and elimination algorithms are simply too heavy-weight to be useful.</p><p>Transitive closure does have a positive affect on performance, unlike its affect on algorithm A. It decreases execution time by 9-45%. Since algorithm C makes extensive queries of the constraint graph, much more so than algorithm A, the overhead of computing the transitive closure is compensated for by the benefits of reducing the graph's breadth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>The results of our preliminary investigation into symbolic pointer analysis have been encouraging. The various symbolic algorithms and optimizations were fairly easy to implement, and the results show that symbolic analysis is a promising avenue of investigation for precise and scalable pointer analysis. Algorithm B is clearly the winner for flow-and contextinsensitive analysis of large C programs; the optimized version of this algorithm was able to analyze the Linux kernel, consisting of over 1 million lines of code, in approximately 1 minute.</p><p>In the short term, we plan to broaden our study of the current algorithms by studying more benchmarks and comparing the results of the symbolic Andersen-style analysis with existing implementations of explicit Andersen-style analyses. In the mid-term, we will study both context-sensitivity and flow-sensitivity. Context-sensitivity has already been explored previously in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b11">12]</ref>, but again the approaches taken are very different and study is required to determine the best approach. There are as yet no flow-sensitive symbolic pointer analyses. Flow-sensitive analyses, unlike context-sensitive analyses, cannot be cast in terms of Andersen-style analysis; this is because flow-sensitive analyses must take into account strong updates to pointers. We have several ideas for how to effectively use symbolic analysis in this environment; we will implement and study these ideas to determine their usefulness.</p><p>In the long term, our research is concerned with more than just pointer analysis. We are interested in the implications of being able to perform pointer analysis both quickly and precisely-in other words, what affect does having such an analysis have on client applications? What becomes feasible that was formerly impractical? Up to this point, client applications have had to choose between imprecise results with good performance, and precise results with very bad performance. This has been a stumbling block in many application areas; for example, lack of precision in software verification usually leads to a great many false error reports, making developers loath to use verifiers during the development cycle. Increasing the precision of the verifiers doesn't help because while the number of false error reports decreases, the analysis can take far too long to be practical, again causing the developers to refrain from using the tool. Having an analysis that can be both fast and precise could cause verifiers to become much more accepted in the developer community, with an attendent increase in the correctness and security of software products. This is only one of the areas where this line of research can have a large positive impact.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Apache benchmark</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figures 4 , 5 ,</head><label>45</label><figDesc>Figures 4, 5, and 6 compare the affects of each individual optimization for each algorithm respectively. For each optimization relevant to a particular algorithm, there are three bars representing the three benchmarks. These values are normalized to the baseline performance of the algorithm as given in figures 1, 2, and 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: lnx-sm benchmark</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: lnx-lg benchmark</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5</head><label>5</label><figDesc>Figure 5: Algorithm B</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Benchmark characteristics. Apache is the apache core, lnx-sm is the minimal Linux kernel configuration, and lnx-lg is the maximal Linux kernel configuration. LOC is the number of pre-processed executable lines of code (i.e. not counting variable and type declarations, function prototypes, etc) . The number of variables gives an idea of the number of nodes in the constraint and points-to graphs. The number of constraints is further broken down into basic, simple, and complex constraints.</figDesc><table><row><cell>benchmark</cell><cell cols="4">LOC no. variables no. constraints</cell><cell>basic</cell><cell cols="2">simple complex</cell></row><row><cell>apache</cell><cell>33,862</cell><cell></cell><cell>6,959</cell><cell>12,483</cell><cell>1,155</cell><cell>7,880</cell><cell>3,448</cell></row><row><cell>lnx-sm</cell><cell>239,984</cell><cell></cell><cell>33,306</cell><cell>62,199</cell><cell>8,416</cell><cell>40,377</cell><cell>13,406</cell></row><row><cell cols="2">lnx-lg 1,053,662</cell><cell cols="2">130,060</cell><cell cols="3">254,401 38,828 165,686</cell><cell>49,887</cell></row><row><cell></cell><cell cols="2">algorithm A</cell><cell></cell><cell>algorithm B</cell><cell cols="2">algorithm C</cell></row><row><cell></cell><cell cols="6">incrementalization constraint ordering incrementalization</cell></row><row><cell></cell><cell cols="2">query caching</cell><cell cols="2">decomposition</cell><cell cols="2">query caching</cell></row><row><cell></cell><cell cols="2">decomposition</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Optimization combinations. These are the combinations of optimizations for each algorithm that result in the best possible performance.</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Program Analysis and Specialization for the C Programming Language</title>
		<author>
			<persName><forename type="first">Lars</forename><surname>Ole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andersen</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1994-05">May 1994</date>
		</imprint>
		<respStmt>
			<orgName>DIKU, University of Copenhagen</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automatic predicate abstraction of c programs</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rupak</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><forename type="middle">D</forename><surname>Millstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sriram</forename><forename type="middle">K</forename><surname>Rajamani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGPLAN Conference on Programming Language Design and Implementation</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="203" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Laurie Hendren, and Navindra Umanee. Points-to analysis using bdds</title>
		<author>
			<persName><forename type="first">Marc</forename><surname>Berndl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ondřej</forename><surname>Lhoták</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGPLAN 2003 Conference on Programming Language Design and Inplementation</title>
		<meeting>the ACM SIGPLAN 2003 Conference on Programming Language Design and Inplementation</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="103" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Graph-based algorithms for Boolean function manipulation</title>
		<author>
			<persName><forename type="first">Randal</forename><forename type="middle">E</forename><surname>Bryant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers, C</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="677" to="691" />
			<date type="published" when="1986-08">August 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Partial online cycle elimination in inclusion constraint graphs</title>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Faehndrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">S</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhendong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Aiken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGPLAN Notices</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="85" to="96" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Computing strongly connected components in a linear number of symbolic steps</title>
		<author>
			<persName><forename type="first">Raffaella</forename><surname>Gentilini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carla</forename><surname>Piazza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Policriti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SODA &apos;03: Proceedings of the fourteenth annual ACM-SIAM symposium on Discrete algorithms</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="573" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Ultra-fast aliasing analysis using CLA: A million lines of c code in a second</title>
		<author>
			<persName><forename type="first">Nevin</forename><surname>Heintze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Tardieu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGPLAN Conference on Programming Language Design and Implementation</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="254" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rupak Majumdar, and Gregoire Sutre. Lazy abstraction</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">A</forename><surname>Henzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ranjit</forename><surname>Jhala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Principles of Programming Languages</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="58" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Buddy, a binary decision diagram package</title>
		<author>
			<persName><forename type="first">Jorn</forename><surname>Lind-Nielsen</surname></persName>
		</author>
		<imprint/>
		<respStmt>
			<orgName>Department of Information Technology, Technical Institute of Denmark</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The effects of the precision of pointer analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Horwitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">1302</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Depth-first search and linear graph algorithms</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Endre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tarjan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Comput</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="146" to="160" />
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cloningbased context-sensitive pointer alias analysis using binary decision diagrams</title>
		<author>
			<persName><forename type="first">John</forename><surname>Whaley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monica</forename><forename type="middle">S</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGPLAN 2004 conference on Programming language design and implementation</title>
		<meeting>the ACM SIGPLAN 2004 conference on Programming language design and implementation</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="131" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Program decomposition for pointer aliasing: A step toward practical analyses</title>
		<author>
			<persName><forename type="first">Sean</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barbara</forename><forename type="middle">G</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Landi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Foundations of Software Engineering</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="81" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Symbolic pointer analysis</title>
		<author>
			<persName><forename type="first">Jianwen</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IC-CAD &apos;02: Proceedings of the 2002 IEEE/ACM international conference on Computer-aided design</title>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="150" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Symbolic pointer analysis revisited</title>
		<author>
			<persName><forename type="first">Jianwen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvian</forename><surname>Calman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PLDI &apos;04: Proceedings of the ACM SIGPLAN 2004 conference on Programming language design and implementation</title>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="145" to="157" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
