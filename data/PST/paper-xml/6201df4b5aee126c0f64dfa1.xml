<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Jury Learning: Integrating Dissenting Voices into Machine Learning Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-02-07">7 Feb 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mitchell</forename><forename type="middle">L</forename><surname>Gordon</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Michelle</forename><forename type="middle">S</forename><surname>Lam</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Joon</forename><forename type="middle">Sung</forename><surname>Park</surname></persName>
							<email>joonspk@stanford.edu</email>
						</author>
						<author>
							<persName><forename type="first">Kayur</forename><surname>Patel</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jeffrey</forename><forename type="middle">T</forename><surname>Hancock</surname></persName>
							<email>hancockj@stanford.edu</email>
						</author>
						<author>
							<persName><forename type="first">Tatsunori</forename><surname>Hashimoto</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University Stanford</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University Stanford</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Stanford University Stanford</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Apple Inc. Seattle</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Stanford University Stanford</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Stanford University Stanford</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">Stanford University Stanford</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Jury Learning: Integrating Dissenting Voices into Machine Learning Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-02-07">7 Feb 2022</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3491102.3502004</idno>
					<idno type="arXiv">arXiv:2202.02950v1[cs.HC]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Whose labels should a machine learning (ML) algorithm learn to emulate? For ML tasks ranging from online comment toxicity to misinformation detection to medical diagnosis, different groups in society may have irreconcilable disagreements about ground truth labels. Supervised ML today resolves these label disagreements implicitly using majority vote, which overrides minority groups' labels. We introduce jury learning, a supervised ML approach that resolves these disagreements explicitly through the metaphor of a jury: defining which people or groups, in what proportion, determine the classifier's prediction. For example, a jury learning model</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this dataset, the labeler population consists of labelers who belong to groups A, B, and C</p><p>The decisionmaker composes a jury to rule on input examples (here, they balance representation of groups A, B, and C)</p><p>The jury learning architecture models each individual labeler in the dataset. Jury learning then samples labelers to fill the selected jury composition and predicts each labeler's rating for an example</p><p>To aid a final classification decision, the model surfaces the median jury outcome over multiple trials (each with re-sampled jurors), and the decisionmaker can explore the outcomes of the trials . . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LABELER POPULATION FROM DATASET PREDICTED JUROR LABELS JURY CLASSIFICATION</head><p>Median jury outcome from N trials</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">-7</head><p>N trials</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SELECTED JURY COMPOSITION</head><formula xml:id="formula_0">A 1 B 1 C 1 A 2 B 2 C 2 A 3 B 3 C 3 A 4 B 4 C 4 1 2 3 4 + UNSEEN EXAMPLE</formula><p>Figure <ref type="figure">1</ref>: An overview of jury learning. (1) Given a dataset annotated by labelers from different groups, (2) the machine learning practitioner can compose a jury to rule on an unseen input example by allocating seats to labelers from the dataset with specified characteristics. (3) Then, the jury learning architecture models each individual labeler in the dataset, and performs N trials in which it samples labelers as jurors to populate the specified jury composition and predicts each juror's decision for the example. (4) The system then outputs a median-of-means jury outcome alongside jury outcome exploration visualizations that the decisionmaker can use to reach a classification decision.</p><p>for online toxicity might centrally feature women and Black jurors, who are commonly targets of online harassment. To enable jury learning, we contribute a deep learning architecture that models every annotator in a dataset, samples from annotators' models to populate the jury, then runs inference to classify. Our architecture enables juries that dynamically adapt their composition, explore counterfactuals, and visualize dissent. A field evaluation finds that practitioners construct diverse juries that alter 14% of classification outcomes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Whose voices-whose labels-should a machine learning algorithm learn to emulate? In supervised machine learning today, the answers to these questions are often left implicit in the data collection and training procedure. In a typical procedure, the practitioner pays multiple annotators to label each example <ref type="bibr" target="#b37">[38]</ref>, then aggregates those labels via majority vote <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b76">77]</ref> into a single ground truth label <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b79">80]</ref>. The algorithm then trains on this aggregated ground truth, learning to predict ground truth labels that represent the largest group's point of view. While this majoritarian <ref type="bibr" target="#b54">[55]</ref> procedure succeeded for many early machine learning tasks, it now runs aground on tasks where there is substantial disagreement on what the correct label ought to be <ref type="bibr" target="#b36">[37]</ref>. Tasks with substantial disagreement are common in user-facing contexts, including classification of online comment toxicity <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b82">83]</ref>, news misinformation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b90">91]</ref>, and medical diagnosis <ref type="bibr" target="#b71">[72]</ref>. In these tasks, up to one third of expert annotators disagree with each other when labeling an average example. Properly accounting for labels from non-majority groups in a comment toxicity task, for example, reduces classifier performance from 0.95 ROC AUCnearly solved-to a much less persuasive 0.73 ROC AUC <ref type="bibr" target="#b36">[37]</ref>. This less persuasive number is indicative of the fact that it is impossible to create a classifier that makes every user happy-we have to make a choice.</p><p>Today's supervised learning approach, however, does not afford the technical or interactive tools necessary to resolve annotator disagreements through an explicit, carefully considered choice. One response is to train the model to output a distribution across annotators rather than across classes <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b84">85,</ref><ref type="bibr" target="#b89">90</ref>]-e.g., "40% of annotators will say this comment is toxic, and 60% will not." However, for an HCI researcher or practitioner who is designing a classifier that must make decisions in the face of disagreement, the quantity of interest is rarely just a question of how many people disagree, but one of who disagrees and why <ref type="bibr" target="#b91">[92]</ref>. Reflective practices around dataset generation <ref type="bibr" target="#b33">[34]</ref> can help specify whose voices a classifier should be designed to emulate during the dataset collection stage. However, once a dataset has been collected and the resulting model trained, today's supervised learning pipeline does not afford the ability to reason over disagreement and then change a classifier's voices as tasks change or culture shifts. In most cases we lack even awareness of the need to do so: practitioners are typically unaware of whether stakeholders for a particular deployment or inference will disagree with a classifier's decisions, because they haven't modeled every annotator's or group's opinions. There remains a gap in providing algorithmic and interactive mechanisms that resolve the who, why, and decision rules of machine learning under societal disagreement.</p><p>In this paper we introduce jury learning, a supervised learning architecture that closes this gap through the metaphor of a jury. Jury learning models every individual annotator in the dataset, enabling the practitioner to declaratively define which people or groups from the training dataset, in what proportion, should determine the classifier's prediction. The jury learning model architecture then predicts each juror's label and outputs the joint jury prediction to classify unseen examples. Rather than a typical machine learning classifier outputting a label of, for example, toxic or not toxic, a jury learning classifier might output a prediction such as, "For this jury of six men and six women, which is split evenly between White, Hispanic, AAPI, and Black jurors, 58% of the jury are predicted to agree that comment is toxic." Through jury learning, practitioners can define jury compositions that reflect stakeholders for the task, for example that the toxicity classifier should centrally feature women and Black jurors because they are commonly targets of online harassment <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b65">66]</ref>. The jury can articulate specific individuals, or any group-based annotation in the dataset (e.g., gender identity, political affiliation, racial identity).</p><p>To make a prediction on a new input, jury learning samples jurors from the practitioner's articulated jury composition, predicts each juror's response to the new input, then aggregates those responses into a final prediction. Our jury learning exploratory interface (Figure <ref type="figure" target="#fig_0">2</ref>) visualizes how each juror voted, enabling sensemaking about the nature of disagreement on an input or set of inputs. This approach reconsiders the annotators who label training datasets not as inputs to an aggregation function but as a population of potential jurors. To ensure that no groups are represented as singular and monolithic in their opinions, jury learning does not model groups but instead individual jurors. This model architecture enables visualizations that highlight where each sampled juror falls relative to the distribution of all annotators in that group.</p><p>We contribute algorithms and visualizations that enable jury learning, then demonstrate them on a popular user-facing task of toxicity detection. The core technical challenge: how do we achieve jury-based prediction from a dataset of similar size and scope as those already in use today, and without abandoning the architectures that make modern machine learning models highly performant? We introduce a model architecture that combines state of the art natural language processing pipelines with techniques drawn from recent advances in deep learning based recommender systems <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b85">86]</ref>. This joint model architecture trains the algorithm to predict how every individual person in the training data would label previously unseen examples, much like a movie recommender system might model how a user would react to a movie-but with the added challenge that every inference is on an example previously unseen by anyone in the training data. Our architecture enables this prediction task, and in addition enables visualization of the uncertainty underlying each decision-how many juries with the same composition would have ruled differently?-while highlighting differences between groups' predicted labels. It also facilitates highly expressive jury-based algorithms, for example those that conditionally adapt the jury composition based on the relevant stakeholders for the input (e.g., populating with religious groups when the questionable comment is about religion, and political groups when the questionable comment concerns politics). In addition, by adapting techniques from quadratic programming, we demonstrate that developers can understand how jury composition impacts classifier behavior through counterfactual juries: automatically identifying the smallest change to the jury composition that would reverse a decision.</p><p>In an evaluation, we test whether jury learning changes which groups influence classifications of a machine learning algorithm for toxicity. Moderators of online communities (N=18) were asked to author juries for a comment toxicity classification task. We find that the resulting juries contain 2.9 times the representation of non-White jurors and 31.5 times the representation of non-binary jurors compared to those created implicitly by a large toxicity dataset <ref type="bibr" target="#b48">[49]</ref>. This increased diversity in the jury composition changed the algorithm's classifications on 14% of items, reflecting the fact that jury learning captured those individual jurors' views far better than a baseline, state of the art aggregated model (with an MAE of 0.62 versus 1.05). We further find that our model architecture is more accurate at predicting aggregate test set labels (MAE=0.27) than today's state of the art classifiers (MAE=0.41). This finding, which highlights the inherent instability of ground truth in the standard aggregate labeling approach, means that our model architecture both enables highly performant jury learning verdicts and also offers performance gains in the traditional aggregated task. Both of these are achieved by modeling each individual annotator whose opinions make up an aggregate label or jury verdict.</p><p>Taken together, this work contributes algorithms and interfaces for a machine learning architecture that makes explicit the selection of whose voice, with what weight, determines each prediction. We argue for this approach normatively, demonstrate its predictive accuracy, and produce evidence that practitioners' jury learning classifiers result in material changes in classifier behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this section, we motivate jury learning through an integration of research in human-computer interaction-especially social computing-along with work in machine learning and AI fairness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Engaging stakeholders in algorithm design</head><p>Our work draws on a critique of strict and unexamined majoritarianism in governance <ref type="bibr" target="#b54">[55]</ref>, which tends to exclude the viewpoints of minority groups <ref type="bibr" target="#b19">[20]</ref>. To protect minority rights, governance structures in practice typically include mechanisms that help avoid a tyranny of the majority (e.g., a bicameral structure <ref type="bibr" target="#b68">[69]</ref>). How should machine learning practitioners respond to this challenge? Jury learning presents one possible response, in which we introduce new levers that enable explicit control of how majorities are formed. In doing so, jury learning raises awareness of each potential majorities' consequences and encourages intentionality in their selection. We argue that in the hands of a well-intentioned actor, jury learning represents meaningful progress towards the problems that strict, unexamined majoritarianism can bring in machine learning. Doing so also opens opportunities for participatory and democractic approaches to jury selection.</p><p>Researchers in human-computer interaction and artificial intelligence have long articulated the need for algorithms to balance multiple stakeholders' needs, motivations, and interests, and to help achieve important collective goals <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b87">88,</ref><ref type="bibr" target="#b91">92]</ref>. One such thread, stemming from ethics in AI, focuses on ensuring fairness of outcomes. It demonstrates how machine learning training algorithms can enforce mathematical notions of individual <ref type="bibr" target="#b28">[29]</ref> and group <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b38">39]</ref> fairness in classification tasks such as recidivism prediction. We build on advances in algorithmic fairness that help manage disparate outcomes <ref type="bibr" target="#b58">[59]</ref>, by contributing a technique that instead helps manage disparate beliefs: whose labels we should be learning when there are irreconcilable disagreements among groups in society. For instance: in today's fairness approaches, the developer may normatively decide what a fair outcome looks like: e.g., comments submitted by Black users should be removed just as frequently as comments submitted by White users. Our work focuses on an orthogonal aspect of fairness: while disparate outcomes might focus on how many of these comments should be removed from different groups of users, we ask whose voices should be involved in the decision of whether a comment should be removed.</p><p>Closer to our aims, a second thread of work proposes design guidelines and frameworks to help system designers ensure they are creating algorithms that reflect their stakeholders' values <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b87">88,</ref><ref type="bibr" target="#b91">92]</ref>. These design processes argue for explicit inclusion of appropriate stakeholders in the design and evaluation of the algorithm. For instance, in WeBuildAI <ref type="bibr" target="#b52">[53]</ref>, stakeholders design their own models representing their beliefs, and then a larger algorithm uses each of these models as a single vote when making a decision for the group. We agree that stakeholders' voices should be directly modeled in algorithmic systems. We contribute a jury-based metaphor, along with a model architecture and algorithms designed to empower practitioners to explicitly resolve disagreements between stakeholders while retaining the performance of today's machine learning pipeline.</p><p>In creating our approach, we draw on recent work that adopts a civics and governance metaphor for socio-technical design. Contested platform decisions can be made by juries of platform members, which can increase the perceived legitimacy of the decisions <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b47">48]</ref>. Platforms such as Facebook have recently engaged such models for setting decision-making precedent, as in their Oversight Board <ref type="bibr" target="#b46">[47]</ref>. The PolicyKit toolkit demonstrates how such participatory processes can be encoded directly into the software that powers these platforms <ref type="bibr" target="#b88">[89]</ref>. Our work extends these metaphors to demonstrate their power in fully algorithmic environments as well, where they offer legitimacy and interpretability benefits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Disagreement, datasets, and machine learning</head><p>Across tasks such as identifying toxic comments <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b82">83]</ref>, bot accounts <ref type="bibr" target="#b83">[84]</ref>, and misinformation <ref type="bibr" target="#b90">[91]</ref>, researchers and platforms <ref type="bibr" target="#b44">[45]</ref> increasingly turn to machine learning to aid their efforts <ref type="bibr" target="#b35">[36]</ref>. Specifically, these models are often trained using a supervised learning pipeline where we:</p><p>(1) Collect a large dataset of individual beliefs, either generated through crowdsourcing services that ask several labelers to annotate each item according to policy and then aggregate the result into a single ground truth label (e.g., <ref type="bibr" target="#b20">[21]</ref>), or similarly by asking and then aggregating experts (e.g., <ref type="bibr" target="#b81">[82]</ref>). (2) Use those ground truth labels to train a model that produces either a discrete binary prediction or a continuous probabilistic prediction for any given example.</p><p>For instance, in a Kaggle competition that received over 3,000 submissions, researchers were challenged to discover the bestperforming architecture in a toxicity detection task <ref type="bibr" target="#b43">[44]</ref>. Facebook makes the vast majority of moderation decisions through classifiers <ref type="bibr" target="#b10">[11]</ref>, and YouTube does similarly <ref type="bibr" target="#b13">[14]</ref>.</p><p>Classifiers typically speak with one voice, an aggregated pseudohuman that reflects the majority voice in the dataset they have been trained on <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b72">73]</ref>. This majority-voice outcome can arise for two reasons: (1) majority vote aggregation of the raw crowdsourced annotations overrides minority viewpoints in generating ground truth, or (2) even if training data points are disaggregated, the training algorithm minimizes its loss function by predicting accurately for the opinions held by the largest group of people in the dataset. Unfortunately, while this majority-voice approach to classification has been highly successful in many artificial intelligence (AI) tasks such as image classification <ref type="bibr" target="#b20">[21]</ref>, the results for many tasks in social computing and HCI remain problematic.</p><p>One potential explanation for these problems may be that the voice a model has learned is not the right voice for every deployment, or even every inference within a deployment. To see how this might be true, we can examine annotator disagreement rates in today's datasets: for instance, in a toxicity task, over one third of annotators on average disagree with any toxic classification, even after accounting for label noise <ref type="bibr" target="#b36">[37]</ref>; in a misinformation classification task, three professional fact checkers were unanimous on only half of URLs <ref type="bibr" target="#b4">[5]</ref>. Across countries, content that was perceived as more or less harmful varies significantly <ref type="bibr" target="#b42">[43]</ref>. Such disagreement indicates that there may be multiple competing voices, potentially representing different groups of people or sets of values. Indeed, a toxicity model tuned with a simple positive or negative offset (i.e. baseline) for each annotator achieves far more accurate perannotator results than a standard classifier <ref type="bibr" target="#b48">[49]</ref>.</p><p>We build on research that aims to accurately capture the distribution of annotator opinions <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b45">46]</ref>. Given a dataset with individual annotator labels, machine learning researchers have begun training models to output a distribution of labels rather than a single class label, using loss functions such as cross-entropy compared to the distribution of annotators' labels <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b84">85,</ref><ref type="bibr" target="#b89">90]</ref>. While training models with cross-entropy loss acknowledges the existence of disagreement, it does not tell us who disagrees or why, so we cannot readily act on it. An alternative approach, annotatorlevel modeling, has been shown to yield benefits to uncertainty estimation and majority vote prediction <ref type="bibr" target="#b18">[19]</ref>. In this work, we introduce an annotator-level modeling architecture in the service of the decision rules underlying jury learning. As support for our approach, a Wizard of Oz study found that moving beyond raw distributions and towards AI-provided arguments for competing options resulted in users reviewing more contentious cases themselves <ref type="bibr" target="#b72">[73]</ref>.</p><p>Dataset documentation <ref type="bibr" target="#b33">[34]</ref> and value-sensitive data collection practices <ref type="bibr" target="#b91">[92]</ref> can help specify whose voices a classifier should be designed to emulate. We build on these approaches in two ways. First, we provide an algorithm that makes clear when these voices disagree and provides tools to reflect on and re-weight whose voices are embedded in the model. From this perspective, our work innovates on this literature by directly modeling this information, allowing the machine learning practitioner to understand the nature of the disagreement and make explicit the representation that should resolve it. Second, our work addresses a practical reality of machine learning: while we cannot possibly have a universal set of voices that are appropriate for all models in a given task such as toxicity detection, existing approaches assume practitioners have the resources and motivation to collect new large-scale datasets every time the relevant stakeholders change. In reality, even in the rare cases in which practitioners have the required resources to collect new datasets, they are often unaware of the need to do so: we cannot know whether stakeholders will disagree with a classifier's decisions unless we've modeled every annotator's or group's opinions, leaving many practitioners unaware of the extent to which they are ignoring the opinions of certain annotators or groups. It is therefore not sufficient to have a procedure that requires that requires prior knowledge of the optimal annotator population at the outset. We contribute an approach that can model each relevant individual or group from a dataset similar in size and scope as those already collected today, so that practitioners can reason over and specify which of these individuals or groups their model should and should not reflect, iteratively and reflexively.</p><p>A large body of work in both HCI and machine learning discusses how improved dataset collection practices may result in more performant and ethical classifiers. Often, dataset authors instead strive for a goal of impartiality, so that data is supposedly "unbiased" <ref type="bibr" target="#b74">[75]</ref>. To achieve such a goal, crowdsourcing researchers have proposed a number of methods that aim to resolve annotator disagreement either by making task designs clearer or relying on annotators to resolve disagreement among themselves <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b73">74]</ref>. However, for tasks such as those common in social computing contexts, much of the disagreement is likely irreducible <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b66">67]</ref>, stemming from the socially contested nature of questions such as "What does, and doesn't, cross the line into harassment?". The above methods may help resolve some disagreement in these datasets, but until such an unlikely time as there is ever to be a global consensus on questions such as what constitutes harassment, classifiers must make decisions that represent some users' voices more than others'. Jury learning offers one approach to this decisionmaking.</p><p>An alternative approach is to retrain a model's single voice to represent a desired group <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b32">33]</ref>. If this decision can be made effectively up front, and the practitioner has the substantial budget and resources required to collect their own large-scale dataset, then a single data collection and training pipeline can succeed. Jury learning contributes an approach that allows real-time exploration and tuning of this population without requiring practitioners to collect new and far larger datasets, and makes stronger guarantees about whose voice is being represented in each specific inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Interactive Machine Learning</head><p>Our work draws on a recent thread of research integrating humancentered methods into machine learning systems. Interactive machine learning seeks methods to integrate human insight into the creation of ML models (e.g., <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b30">31]</ref>). One general thrust of such research is to aid the user in providing accurate and useful labels, so that the resulting model is performant <ref type="bibr" target="#b14">[15]</ref>. Another line of work has sought to characterize best practices for designers and organizations developing such classifiers <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8]</ref>. Our work extends this literature, focusing on ameliorating issues that developers and product teams face in reasoning about their models and performance <ref type="bibr" target="#b62">[63]</ref>.</p><p>A third line of works demonstrates that end users struggle to understand and reason about the resulting classifiers. Many are unaware of their existence <ref type="bibr" target="#b29">[30]</ref>, and many others hold informal folk theories as to how they operate <ref type="bibr" target="#b22">[23]</ref>. In response, HCI researchers have engaged in algorithmic audits to help hold algorithmic designers accountable and make their decisions legible to end users <ref type="bibr" target="#b70">[71]</ref>. Our work extends this literature, positioning classifiers as a reflection of many different voices, enabling control over that composition of voices, and enabling both practitioners and users to easily understand which voices are contributing to their models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">JURY LEARNING</head><p>Machine learning often aims to emulate people's labels. Faced with annotator disagreement representing multiple competing voices, which people should we be emulating-whose training labels should a classifier use to make its decisions? We take the position that it is the machine learning practitioner's responsibility to make explicit normative decisions about whose voices their classifiers are reflecting in any given inference. In this section, we describe how we designed jury learning and our motivation in making each of these design decisions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Design goals</head><p>We begin by considering today's approach. Many models return the class label or labels with the highest likelihood (e.g., label = 'toxic', confidence .9). Some models instead predict the distribution of opinions over all annotators: for instance, that 60% of annotators will label a comment as toxic, 30% will label a comment as non-toxic, and 10% will label as unsure. How is a practitioner to act on this information? If their goal is always to satisfy the largest number of annotators, the answer is easy. However, there are many scenarios in which that is not-or should not-be the goal. The practitioner may want to consider different voices (representing different values, experiences, or expertise) depending upon the situation. Consider that a member of the LGBTQ+ community may be more informed about transphobic comments than the population at large. When a comment targets LGBTQ+ issues, or if a community is centered on supporting LGBTQ+ members, a practitioner may wish to more heavily weigh the opinion of these annotators. Or consider that when doctors labeling MRI data disagree about a patient's diagnosis, the practitioner may wish to more heavily weigh opinions from doctors with a particular background or training. Or, it may be the case that the practitioner isn't initially sure who to side with, and so would like to reason over the different decisions that different annotators or groups of annotators would make.</p><p>It is, in theory, possible to achieve some of these goals using today's standard supervised learning pipeline. For instance, a practitioner deploying a classifier to the LGBTQ+ community could collect their own dataset, ensuring that a sufficient portion of annotators identify as LGBTQ+ so that disagreements are resolved by more heavily weighing opinions from LGBTQ+ annotators. In practice, however, such an approach fails to meet our goals. Datasets are expensive and difficult to collect, so practitioners often rely on existing datasets they did not collect, meaning they do not control how disagreements are resolved, and worse: do not even know that voices they care about are dissenting. Without such knowledge, practitioners cannot reason over the different decisions that different annotators or groups of annotators would make. We require a different approach from today's standard supervised learning pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Approach and interaction</head><p>Jury learning is a supervised learning approach that asks practitioners to specify whose voices their classifiers reflect, and in what proportion. To achieve this, jury learning models every individual annotator in a dataset, so that their model may serve as a potential juror. Practitioners then articulate a set of jurors that should be sampled from the groups or individuals in the annotators. That jury's labels determine the classifier's behavior.</p><p>For our purposes, we refer to a jury as a bounded set of individuals whose opinions aggregate into a decision. These individuals are randomly sampled from the population of labelers based on the jury composition that the machine learning practitioner has articulated (e.g., six conservative jurors and six liberal jurors). Jury learning then algorithmically predicts how each of these twelve selected jurors would label the input, and then aggregates those responses into a decision. For many of our examples, we refer to a twelve person jury, which is the default jury size in the American legal system. However, the jury can be any size, if there are enough annotators in each group in the dataset to populate it. If the task is regression rather than classification (e.g., a toxicity score rather than a binary toxic-or-not decision), the outcome is an average of jurors' predicted scores.</p><p>Jury learning enables the creation of many possible classifiers from a single dataset of labels, with the added requirement that the dataset contain information about each annotator for any group or voice that the practitioner wishes to include. For instance, the toxicity dataset we use as our example application domain <ref type="bibr" target="#b48">[49]</ref> includes education, past work experience or qualifications, racial identity, gender identity, political affiliation, age, disability status. Practitioners specify jurors either individually, or using any group membership criteria available in the dataset. If a practitioner selects a juror using group-based data, we demonstrate where that juror fits within the full distribution of all annotators within that group, ensuring that no group is represented as monolithic. Practitioners can interactively explore different jury compositions, gaining an understanding of the consequences of each composition that they try: how are specific annotators or groups of annotators differing in their labels?</p><p>Figure <ref type="figure" target="#fig_0">2</ref> displays the jury learning interface for our example application domain of toxicity detection. In Figure <ref type="figure" target="#fig_0">2(A)</ref>, practitioners specify a jury composition by assigning a juror sheet to each of twelve empty juror slots. A juror sheet defines the characteristics of the annotator who will fill the juror slot. A simple juror sheet may specify only one characteristic, such as a juror identifying as Black, while a more complex juror sheet may articulate an intersectional identity, such as a juror identifying as a Black LGBTQ+ woman. The possible characteristics are dictated by the provided dataset: the set of identities must be broad enough to reflect the relevant stakeholders <ref type="bibr" target="#b75">[76]</ref>. If a characteristic is better captured through open ended text boxes than categories <ref type="bibr" target="#b75">[76]</ref>, the practitioner could explore individual people in the dataset and select a subset for inclusion. The jury composition can be defined interactively via a web interface. The interface also allows the machine learning practitioner to explore different jury compositions and how each might react to different inputs.</p><p>The machine learning practitioner can then apply their jury to an input or set of inputs. Given an input to predict, jury learning makes a prediction for that input for every annotator. It then takes a step not possible when convening real-world juries, but possible with jury learning: it convenes many parallel iterations of the jury, by repeatedly resampling a large number of juries that match the jury specification. Each jury may contain different jurors (annotators from the dataset), and the model will predict different responses to the input for each juror based on that juror's training data in the dataset. The interface disallows selecting any groups with an insufficient number of jurors in the dataset to complete the resampling procedure without replacement, directing practitioners to collect more data for the particular group.</p><p>The system then responds with a jury verdict for the input: the single, final decision of the median jury on that input, shown in Figure 2(B). To identify a verdict, the system samples a set of individual jurors filling the jury specification, predicts each juror's decision, and then determines the aggregate verdict taken as a majority vote (for classification) or mean (for regression). The default decision is calculated as the median jury decision from the set of sampled juries matching the jury specification. This median-of-means estimator <ref type="bibr" target="#b50">[51]</ref>-the median of the mean juror responses across juriesproduces an estimate that is robust both to variance within groups and to potential juror-level modeling errors by the AI. In particular, this approach is resistant to the model being wrong about any small number of jurors, though less effective for systematic errors that may impact most or all jurors.</p><p>The approach also results in a direct measure of uncertainty: how often the outcome changed across the jury samples. For example, the system might communicate that 85% of juries matching the specification resulted in a "toxic" label, and 15% of juries resulted in a "not toxic" label. Or, for a regression task, it might communicate a histogram distribution of jury decisions, as shown via the histogram in Figure <ref type="figure" target="#fig_0">2(B)</ref>.</p><p>Because the system returns a specific jury, the system can visualize each juror in context of the group from which they were sampled (Figure <ref type="figure" target="#fig_0">2(C</ref>)). This contextual information helps the machine learning practitioner better understand the behavior of the jurors chosen for their jury. Specifically, for each juror, we make available all of their annotations and all associated background information that is present in the dataset. This visualization also helps make clear that different members of a group may vote differently, and that despite this individual variation, the overall jury outcome may be stable. In addition, our approach grounds the jurors as individual people with specific characteristics and enables other explainability-related information, such as highlighting how the juror labeled similar inputs in the training data or providing their specific modeling error rate over all of their test examples 2(D)).</p><p>The system is interactive to encourage better sensemaking, but it also provides a code layer for automated systems. The jury definition can be passed as a Python dictionary object, as in Figure <ref type="figure">3</ref>. The response likewise is returned as a Python dictionary object, as in Figure <ref type="figure" target="#fig_1">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Example scenario</head><p>Saanvi has created an online news-sharing social network, and wants to create a classifier to detect any instances of personal attacks on the platform. She finds a popular, publicly available large-scale dataset, trains a model using the traditional supervised learning pipeline, and deploys it to her community. The classifier takes as input the text of a comment, and returns a "toxic" or "not toxic" label. Unfortunately, Saanvi soon begins to notice that both she and many members of her community often disagree with the decisions this classifier makes. Saanvi suspects that perhaps her classifier isn't making decisions in ways that reflect the voices in her community.</p><p>Saanvi's dataset contains characteristics about each annotator, so she switches from the traditional classifier to a classifier created through jury learning. First, Saanvi explores different jury configurations to confirm any group-based differences that she expects to see, inputting comments and exploring how the jurors in each group respond. She confirms that men are more likely to rate borderline comments as not toxic, but notes that there are many women on her platform. By exploring, she also observes that seniors find more comments to be toxic, and 18-35 year olds find fewer comments to be toxic. Saanvi begins by constructing a jury that she believes better represents the members and values of her community. She deploys a private test of it, and notices a significant improvement: the classifier's decisions start making a lot more sense to her and her community members. Saanvi then begins a participatory process to bring in stakeholders from her community, allow them to test different jury configurations, and agree upon a jury to use on their platform.</p><p>Saanvi and the other stakeholders observe that their intuitions of the proper jury composition change based on which groups might be targeted in that post: that when a news article is about women's issues, they want more women on the jury; when a news article concerns LGBTQ+ rights, they want more jurors identifying as LGBTQ+; when an article is about a Black woman, they want more Black women on the jury. So, they agree to dynamically allocate four seats on the jury to the appropriate group based on the news category that the post is shared in (e.g., four women for news articles shared in the womens' rights category). LGBTQ status</p><p>LGBTQ status</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Label distribution</head><p>Juror sheet A -4 of 12 jurors (Race: Hispanic, Gender: Female)</p><p>Juror sheet B -8 of 12 jurors (Race: Black, Age Range: 25-34)  (C) When a user selects a jury, the Jury Trends section is updated. There, they can group by different fields like the juror sheet, decision label, or other demographic attributes to understand patterns in the labels from this jury and contextualize them with respect to the larger population. (D) When a user selects a particular juror, the Juror Details view opens, and they can inspect the predicted label for the juror, the background of this juror, and the juror's annotations. (E) Users can also inspect counterfactual juries that would result in the opposite verdict.</p><formula xml:id="formula_1">0 1 2 3 4 B A 1 A 2 B 1 B 5 B 2 B 6 B 3 B 7 B 4 B 8 A 3 A 4 Toxic Non-toxic</formula><p>Saanvi exports the model and puts it into private testing on her server, where its predictions are not yet shown to users. She and the group of stakeholders continue to monitor its behavior. Eventually, as they build trust in the algorithm, they begin to use it to prioritize comments for human moderators on the platform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">TECHNICAL APPROACH</head><p>Jury learning requires that we predict how each individual annotator would label an unseen example. A jury outcome is then an aggregation of the jurors' (annotators') individual classifications.</p><p>Enabling the broadest set of applications also requires an approach that can make such predictions from a dataset of similar size and structure to those already in use when training supervised standard classifiers: a labeled dataset with a few annotators labeling each item and each annotator labeling a few items, such as those commonly acquired from crowdsourcing services. The only additional assumption we make is that any characteristic used to select jurors (e.g., gender identity) must exist for each juror. This is achievable by adding a small survey when an annotator begins labeling examples. In what follows, we describe our model architecture for jury learning. While we focus our description on  natural language processing tasks (specifically, toxicity detection), the high-level architecture is general and can apply to any inputs that allow content embeddings (e.g., images via Resnet <ref type="bibr" target="#b80">[81]</ref>, screens via Screen2Vec <ref type="bibr" target="#b53">[54]</ref>, or text via BERT <ref type="bibr" target="#b23">[24]</ref>).</p><p>We base our model architecture on the insight that, in trying to predict how each annotator would label an unseen example, we share part our goal with the aim of today's recommender systems. Like recommender systems, we must not only perform well over a range of inputs, but also over a range of individuals. Like recommender systems, we expect that different opinions between annotators can often be partly explained by explicit categorical information about the groups that each annotator belongs to or identify with, but are also partly unique to a particular annotator or explained by unobserved latent factors <ref type="bibr" target="#b39">[40]</ref>. In other words, much like how Netflix might develop a model to predict individual users' opinions on films, our jury-based model will predict individual labelers' perspectives on new inputs.</p><p>Unlike Netflix, however, all of the inputs to our model will be unseen examples (or, in recommender systems language, all examples suffer from the cold start problem), meaning that they have never been seen by any annotators in our training set. This is a standard assumption in any classification task, but not a typical assumption of most recommender systems, which often rely heavily on an item's existing annotations to inform what other users will think of it. We require an approach that relies entirely on an input's featurization: by taking an input and embedding it, we can predict an annotator's label by comparing this input to similar examples they have already annotated. This means that today's hybrid deep learning recommender systems for natural language input, which typically train their own item embeddings <ref type="bibr" target="#b86">[87]</ref>, are insufficient. We propose a model architecture that jointly trains a content model for classification tasks (such as from BERT) alongside a deep recommender system. By combining deep recommender systems' ability to model individuals' opinions with modern pre-trained deep learning models' classification task performance, our architecture takes full advantage of the strengths of each.</p><p>For our recommender system architecture, we select a Deep &amp; Cross Network (DCN) <ref type="bibr" target="#b85">[86]</ref>. DCNs were designed for web-scale collaborative filtering applications in which data are mostly categorical, leading to a large and sparse feature space. While DCNs were created for classic recommender system tasks, our insight is that a modified DCN architecture is strong fit for jury learning. A typical DCN involves three sets of embeddings: content, annotator, and group. The content embedding enables prediction on previously unseen items by mapping those items into a shared space. The group embeddings make use of the data from all annotators who belong to each group, helping overcome sparsity in the dataset. The annotator embedding ensures that the model learns when each annotator differs from the groups they belong to. The DCN learns to combine these embeddings to predict each individual annotator's reaction to an example: the embeddings are concatenated into an input layer, then fed into a cross network containing multiple cross layers that model explicit feature interactions, and then combined with a deep network that models implicit feature interactions <ref type="bibr" target="#b85">[86]</ref>. We modify the DCN architecture to jointly train (or more precisely in the case of a pre-trained models, jointly fine tune) a pre-trained BERT-based model, using its pooler output as the content embeddings. Figure <ref type="figure" target="#fig_2">5</ref> displays a high-level view of our end to end model architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation for toxicity detection</head><p>Having described our high-level approach and architecture, which can be applied to a wide range of tasks, we now turn to the specific task we use to demonstrate jury learning in this paper: toxicity detection.</p><p>4.1.1 Dataset description. We train our model using a publicly available balanced dataset <ref type="bibr" target="#b48">[49]</ref> in which 107,620 social media comments were labeled by five annotators each, from a pool of 17,280 unique annotators. This dataset was collected to understand how user expectations for what constitutes toxic content differ across demographics, beliefs, and personal experiences. Each annotator labeled a minimum of 20 comments, with a small fraction labeling more than 20. Each annotator contained categorical information noting their self-identified gender, race, education, political affiliation, age, whether they're a parent, and whether they consider religion an important part of their lives. Annotators were asked to rate each social media comment's toxicity on a scale from 0 to 4, with 0 being non-toxic, 1 being slightly toxic, and 4 extremely toxic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>If we binarize this task, with a rating of &lt; 1 indicating non-toxic and &gt;= 1 indicating toxic, we find that annotators in this dataset disagree with each other 35.9% of the time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Training.</head><p>We use TensorFlow Recommenders (TFRS) as the basis of our implementation. TFRS natively supports DCNs. We use Huggingface's Tensorflow API to instantiate BERTweet ( a largescale language model pre-trained on English Tweets, released by NVIDIA <ref type="bibr" target="#b61">[62]</ref>) as the pre-trained content embeddings within our recommender system. We adapt the model to the task by performing an initial fine-tuning step on a large-scale toxicity dataset released by Jigsaw <ref type="bibr" target="#b43">[44]</ref>. Initially, we co-train all the model's components together: we fine tune the pre-trained large language model, and we train from randomly initialized values for the annotator embedding, group embeddings, and the DCN. However, while BERT-based models have been shown to quickly overfit after fine tuning for a few epochs, our newly initialized components can benefit from a longer training procedure. We therefore co-train the entire model for two epochs, freeze the large language model, and continue training the remainder of the model for 8 epochs. Further epochs did not noticeably improve the model's performance. We used the Adam optimizer and Mean Squared Error as our loss function.</p><p>We trained our model on one machine with one NVIDIA Titan XP GPU. The majority of the Titan XP's memory is taken up by BERTweet, so most of the DCN itself is stored in the machine's memory during training. We chose standard hyperparameters used when fine tuning BERT-based models: we used learning rate of 2𝑒 − 5, a batch size of 16, and a maximum length of 128 tokens. We set our DCN-specific hyperparameters as follows: we set a constant embedding dimension of 32, a three-layer cross network of size 768, three dense layers of size 768, and a output dense layer of size 1. We selected these sizes and the number of training epochs after performing a small grid search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXTENSIONS</head><p>The architecture of jury learning directly affords new decisionmaking and interpretability techniques that are not available with traditional algorithms. Here we overview two such techniques that we have implemented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Conditional juries</head><p>We might desire different forms of expertise depending on the decision at hand. For example, CHI's peer review process identifies jurors (reviewers) who differ for each paper under review, based on the content of the paper. Likewise, civil society organizations convene different groups of stakeholders depending on who their decisions might impact. In the context of AIs, for example, classifying misogynistic comments may call for a jury with a larger representation of women, whereas classifying racist comments may call for a jury with a larger representation of minoritized racial groups.</p><p>While the default jury learning algorithm focuses on a simple metaphor of a stable jury composition that is used for all decisions, jury composition can be conditional on the item being classified. A simple code conditional might adapt the jury composition: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Counterfactual juries</head><p>When a jury decides a given comment to be non-toxic, it naturally gives rise to the question: what jury composition, if any, would find the comment to be toxic? How different would the jury need to have been to flip the outcome? Jury learning enables this interaction to search for a counterfactual jury, by automatically identifying the minimal change to the jury composition that would result in a different outcome than the current jury (Figure <ref type="figure" target="#fig_0">2(E)</ref>).</p><p>Within the jury learning framework, we frame the search for the counterfactual jury as an optimization problem: flip the classification by making the smallest edit possible to the current jury composition. Formally, we can define this as a quadratic program, solvable via convex optimization. Consider that we have 𝐾 different annotators or groups of annotators, and we have a prediction 𝑠 𝑘 associated with each. We set the size of our jury, 𝑛 𝑗𝑢𝑟𝑜𝑟𝑠 , to 12, meaning that we must assign a value in {0 . . . 𝐾 } to each of the 12 juror slots. To represent a jury composition, we define a jury allocation vector 𝑝 of length 𝐾. Each index of 𝑝 refers to an annotator or group in 𝐾, and the value at each index refers to the number of jurors from the corresponding annotator or group. The jury allocation vector should therefore should sum to 𝑛 𝑗𝑢𝑟𝑜𝑟𝑠 . The classification decision we consider is a threshold on a jury's average prediction, which we define as 𝑣 𝑝 = 𝑘 𝑝 𝑘 𝑠 𝑘 𝑛 𝑗𝑢𝑟𝑜𝑟𝑠 . The final classification is based on whether 𝑣 𝑝 &gt; 1. The problem of identifying a counterfactual jury is now equivalent to a quadratic program. If the current decision is in the negative class 𝑣 𝑝 ≤ 1, then the counterfactual jury that flips this decision is defined as the solution to the following optimization problem This optimization problem can then be solved by off-the-shelf optimization solvers.</p><p>Counterfactual juries can serve as a useful interpretability lens, aiding the community in understanding how dependent the classification outcome was on the jury composition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">MODEL EVALUATION</head><p>Taking our example application of toxicity detection, we evaluate the performance of our proposed model architecture at two levels:</p><p>(1) How accurate are individual juror predictions? (2) How accurate are the final predictions produced by a jury?</p><p>The most important question to test with jury learning is whether the learning algorithm correctly estimates what jurors' opinions are on previously unseen data. Recommender systems make predictions across different individuals by identifying commonalities among annotators and borrowing information. Without an approach that sometimes borrows information, building a jury learning system would require acquiring a large dataset from each group, including each intersectional identity group, which is often infeasible. However, any machine learning approach that borrows information also brings a risk: it is possible to borrow too much information, particularly when we have less data from a specific group or annotator. So, our evaluation seeks to test whether the approach is correctly estimate each juror's labels.</p><p>6.1 Individual juror performance 6.1.1 Performance versus a standard classifier. We first demonstrate that jury learning is substantially more accurate in predicting individual annotator responses when compared to a baseline state-ofthe-art, annotator-agnostic classifier.</p><p>To create a state of the art baseline model, we fine tune BERTweet on the toxicity dataset using the same standard hyperparameters we used to fine tune BERTweet within the jury learning algorithm. As the toxicity dataset provides a regression task in (0, 4), we report the Mean Absolute Error (MAE), comparing each individual annotator's predicted response to their observed response. We design the test set for this evaluation so that all of the comments were never seen by the annotators in the training set. This more challenging prediction task reflects the expected usage of our model, as discussed earlier.</p><p>Our test set contains 5,000 comments and 24,545 annotations.</p><p>We find that our model achieves an MAE of 0.61, and the baseline model achieves an MAE of 0.90. This large improvement is not necessarily surprising: our architecture is the only one that makes use of information about individual annotators. This result demonstrates that our model was able to learn a substantial amount of useful information about each annotator or their groups; if jury learning had learned nothing about either an individual or groups, then its predictions would simply match those of a standard state of the art classifier trained on aggregated labels, which makes one prediction per example.</p><p>6.1.2 Performance versus a group-based classifier. The above performance gains could either have come from learning about individual annotators, the groups they belong to, or both. Our goal with jury learning is to ensure that models are not solely reliant on group membership; we would also like our model learn about how individual annotators diverge within their groups. We therefore now ask: how performant is our model at predicting individual annotators' responses to an example, compared to an ablation of our model that only knows about group membership? If our model performs better using both annotator and group information than solely group information, it has learned specific information about annotators.</p><p>To create a group-specific classifier, we train a model using our proposed architecture with one change: we remove annotator IDs as a feature, meaning that our model can only rely on group-based and content-based features. We find that this model achieves an MAE of 0.81. This score is an improvement over the baseline aggregated classifier's 0.90, indicating that our model learned useful information from group-based features. However, our full individ-ual+group model's MAE of 0.61 is a substantial improvement over both, indicating that our full architecture is reliant on both group and individual annotator features. 6.1.3 Is our model more performant for some groups than others? A recommender-like prediction system may implicitly group 'similar' individuals together (due to its low-rank inductive bias), leading to some unique individual and intersectional perspectives being erased. Such issues could give practitioners false confidence that they are accounting for intersectional opinions, decrease public confidence (as individuals can verify predictions are incorrect for them), and lead to decision systems that are worse than the status quo. In particular, this issue could arise for smaller groups where our model may need to borrow more information. Addressing this issue requires first understanding its extent. We therefore now ask: is performance consistent across groups of varying sizes? This section is not an exhaustive study of intersectional identities in our dataset, which would be infeasible to report. Rather, we focus on three of the most salient group-based categories in our dataset (race, gender, and political affiliation), shown in Table <ref type="table" target="#tab_4">1</ref>. As illustrative examples, we also report results for two intersectional identities.</p><p>We first note that the baseline aggregated model's performance varies substantially between groups. For instance, it achieves an MAE of 0.83 for Asian annotators and a far worse 1.12 for Black annotators, a performance decrease of 35.0%. By comparison, we find that while our model does show differences between groups, but it does so with far smaller magnitudes. It achieves an MAE of 0.62 for Asian annotators and 0.65 for Black annotators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Jury-level performance</head><p>Having shown that our architecture can model individual annotators, we now turn to jury level predictions. Ultimately, these are the most important predictions that our model makes. We ask: how performant is our model at predicting a jury's verdict?</p><p>To evaluate jury-level predictions, we'd like to compare the predicted final value produced by a jury against an observed final value produced by the same jury. Ideally, we would use comments in our test set that have been labeled by at least 12 annotators, and treat those 12 annotators as a de-facto jury.</p><p>While our dataset does not contain comments labeled by twelve annotators, it does contain a subset that were labeled by ten annotators. We rely on this small subset to get a close approximation (though likely a slightly pessimistic estimate) of the MAE of a 12member jury. We define the observed verdict as the mean observed annotation over all 10 annotators, who serve as the de-facto jury. We define the predicted verdict as the mean of our model's individual predictions for those same ten annotators. Over 550 10-annotator juries, we find that our model produces a jury-level MAE of 0.27.</p><p>We have shown in the previous sections that jury learning is very effective when the annotators of interest are different from the distribution of annotators in the original dataset (e.g. intersectional identities). However, we show a surprising result: jury learning is more effective than the current aggregate prediction approach even when the annotator distribution is the same as that of the dataset. We find that the above baseline model produces an MAE of 0.41 over aggregate test labels, notably worse than our model's 0.27. These gains are due to the fact that these examples are annotated by a small group of 10 annotators where the identity of each annotator has a strong influence on the observed verdict, and jury learning can make predictions that account for the identity of these jurors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">USER EVALUATION</head><p>Having demonstrated the technical efficacy of our jury learning architecture in making annotator-level and jury-level inferences, we then sought to evaluate jury learning in the hands of real-world stakeholders in the content moderation setting. Our study aimed to answer the following questions:</p><p>• Q1: What jury compositions do participants select? How diverse are the selected jury compositions with respect to the implicit jury compositions embedded in the original dataset? • Q2: Do participant-specified juries result in different prediction outcomes than those produced by a standard classifier?</p><p>To answer these questions, we targeted our study towards two audiences in the context of our focal task of toxicity classification: content moderators and platform users. Given their expertise in making policy decisions that are tailored to the needs of particular online communities, content moderators are the population most likely to directly utilize our system.</p><p>In the supplementary materials, we replicate this study with everyday platform users who are not involved in content moderation and who might not currently feel that they have a voice in this decision-making, and we also report on survey instruments measuring the perceived legitimacy (willingness to grant deference and authority) of jury learning compared to traditional algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Study design</head><p>We conducted an online study that consisted of a Qualtrics survey with two main components: (1) a jury composition section where participants were asked to design a jury for an online community and answered several short-answer follow-up questions, and (2) a moderation algorithm legitimacy section where they answered questions to assess their perceptions of the legitimacy of a current moderation algorithm and the proposed jury algorithm. To ground the survey in a concrete scenario, we framed all of the questions in terms of a hypothetical online social media platform called Your-Platform that is planning to use algorithmic approaches as a major component of its content moderation strategy. At the start of the survey, we provided a detailed explanation of a current algorithm (a standard machine learning classifier trained on human labels using majority vote label aggregation) and a jury algorithm (an instantiation of our jury learning approach) and explained that YourPlatform was considering using one of these methods.</p><p>For the jury composition task, we displayed one of 5 possible comment sets (generated by random samples from our comment toxicity dataset <ref type="bibr" target="#b48">[49]</ref> stratified by toxicity severity and labeler disagreement) to exemplify the type of content they would need to moderate on YourPlatform. Participants were then shown a simplified jury composition input form that allowed them to allocate 12-person jury slots using three demographic attributes: (1) gender (Female, Male, Non-binary, Other), ( <ref type="formula">2</ref>) race (Black or African American, White, Asian, Hispanic, American Indian or Alaska Native, Native Hawaiian or Pacific Islander, Other) and (3) political affiliation (Conservative, Liberal, Independent, Other). While our approach can accommodate as many categorical values as are associated with labelers, we selected this limited set of axes because they are common demographic attributes that capture a fair amount of variation among users and that were relevant to the topics of the comment sets. Further details on our study procedure and the full survey contents are found in our supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Participant recruitment</head><p>For our content moderator study, we recruited participants who serve as moderators for Discord or Reddit. A member of our research team recruited Discord moderators via a server where many Discord moderators gather to discuss issues around moderation and recruited Reddit moderators of major subreddits via individual solicitation. Due to their domain expertise and relative scarcity, we offered content moderators $40.00 to complete our 30 to 45-minute survey. In total, 18 content moderators participated in our study. These participants moderate on a variety of platforms (17 on Discord, 5 on Reddit, and 2 on Twitch; some participants moderate across multiple platforms and communities). Based on self-reported demographics, we had 9 participants of age 18-24 and 9 participants of age 25-34; we had 4 women, 9 men, 4 non-binary individuals, and 1 participant who did not disclose their gender; we had 12 White, 2 Asian, and 3 multi-racial participants (1 participant did not disclose their racial identity).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Analysis approach</head><p>To analyze our results, for each available demographic attribute value, we compared its representation in participant juries against its corresponding current algorithm implicit jury representation. The current algorithm implicit jury represents the proportion of each demographic group in the original dataset. For each demographic attribute, we calculated the proportion of labelers for each comment who possessed that attribute and computed the average of these per-item proportions across the dataset. These proportions were normalized among the subset of demographic attributes that we selected for this study. The current algorithm implicit jury determined through this process-the annotators in the training data for the current algorithm-is 74% White (see red lines on Figure <ref type="figure" target="#fig_4">6</ref>).</p><p>In addition, both survey sections had open-response questions. The goal of our analysis here was to summarize high-level themes that emerged from these responses, so a member of the research team read through all responses multiple times to generate a set of themes using qualitative open coding <ref type="bibr" target="#b15">[16]</ref>, then coded comments according to these themes.</p><p>As a post-study analysis step, we took participants' jury compositions and performed inference with our jury learning algorithm to compare the jury-based outcome with that of a standard ML algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Results: Jury composition diversity (Q1)</head><p>First, we examined the jury compositions designed by our participants. We had a total of eighteen moderators who completed our survey, of which we were able to analyze sixteen. <ref type="foot" target="#foot_0">1</ref> All possible values for all three attributes were utilized in the study, and participants constructed diverse juries with a mean of 5.7 unique race values (SD=0.85), 3.1 unique gender values (SD=0.56), and 3.4 unique political affiliation values (SD=0.61). This diversity involved the explicit inclusion of non-majority identities (here, defined as values other than White for race, Male or Female for gender, and Liberal or Conservative for political affiliation): on average, participants created juries with 10.31 individuals (SD=1.26) who had one or more non-majority attributes; participants created juries with on average 3.88 individuals (SD=1.76) who had two or more non-majority attributes (e.g., Black and Non-binary).</p><p>We then compared the diversity of the moderator-designed juries relative to the diversity of the current algorithm implicit jury we defined earlier. As summarized in Figure <ref type="figure" target="#fig_4">6</ref>, we observed that for all three demographic attributes, participants juries achieved greater diversity than the current algorithm implicit jury. We performed ). While there are sizeable disparities in group representation in the current algorithm implicit jury (denoted with red lines), the moderator-specified juries generally achieve greater diversity (raising representation for groups with the lowest red lines and lessening the gap in representation among groups).  ). Values denoted with double-asterisks (**) are significant with 𝑝 &lt; 0.01; values denoted with a single asterisk (*) are significant with 𝑝 &lt; 0.05. Most notably, we observe strongly significant increases in the representation of non-White jurors, strongly significant increases in the representation of Non-binary jurors and corresponding strongly significant decreases in the representation of Male and Female jurors.</p><p>one-sample t-tests comparing the representation of demographic attribute values between the current algorithm implicit jury and the participant jury and report the results in Table <ref type="table" target="#tab_6">2</ref>. For racial identity, we observed strongly significantly decreases in the representation of White jurors (𝑝 &lt; 0.001) and strongly significant increases (𝑝 &lt; 0.001) in representation for all non-White race attribute values except for the "Other" category, where we still saw a significant increase in representation (𝑝 &lt; 0.05); participants' juries contained 2.9 times the representation of non-White jurors than the current algorithm implicit jury. For gender identity, we observed a strongly significant reduction in both male and female jurors and a strongly significant increase in the representation of non-binary jurors (𝑝 &lt; 0.001) with 31.5 times the representation of non-binary jurors compared to the current algorithm implicit jury. Finally, for political affiliation, we observed a significant decrease (𝑝 &lt; 0.05) in the representation of Independents (who were oversampled in the original dataset) and a significant increase in the representation of other political affiliations.</p><p>Our qualitative coding shed light on the reasons underlying participants' jury composition decisions. As summarized in Table <ref type="table" target="#tab_7">3</ref>, a vast majority of participants aimed to prioritize diversity and equal representation of juror attributes, and the majority took special care to increase representation of groups who were targeted in the provided comment set. When asked to envision how outcomes of the jury algorithm might differ from those of the current algorithm, many participants felt that it would better capture the views of minority groups and would increase the number of comments rated as toxic. Finally, when explaining which groups had more or less voice in their jury composition, many users stated that they based their decision on whether certain groups had relevant experience with the comment topic or whether certain groups had been historically marginalized or underrepresented.  ) open response questions about their approach to composing juries, the outcomes they anticipated, and justifications for their jury composition decisions. We manually coded their responses to identify themes. The count column indicates the number of participants who mentioned each theme. A majority of participants aimed to prioritize diversity and equal representation of juror attributes, and the majority took special care to increase representation of groups who were targeted in the provided comment set.</p><p>same state of the art BERTweet-based classifier defined earlier in the Model Evaluation section. We first aim to establish that jury learning effectively models the individual jurors selected by participants. We therefore perform a disaggregated analysis in which we randomly sample jurors for each of the diverse, participant-provided jury composition 100 times. We then compute an MAE over all the comments labeled by all selected jurors. We find that jury learning decreases the average error of these diverse participant-provided juror's opinions by 41% when compared to the predictions from our baseline aggregated model, from an MAE of 1.05 to 0.62.</p><p>We then focus on the final predictions produced by jury learning, computed through a median-of-means estimator over 100 resampled juries. We compare these predictions to the predictions from our baseline classifier. To determine whether a jury's prediction caused a toxicity decision to change, we binarize the final regression values found from our median-of-means estimator such that a value &lt; 1, indicates non-toxic, and ≥ 1, corresponding to a value "slightly toxic" or greater in the annotation scheme, indicates toxic. We remove a small number of juror sheets (mean: 4%) because because the participant requested more jurors from an intersectional identity than available in the original dataset.</p><p>Over the 16 moderator-provided juries, we find that a mean of 13.6% of decisions flip, with a standard deviation of 4.1% across moderators. This result suggests that a meaningful number of classifications can change between an off-the-shelf classifier and a jury learning classifier customized for the community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5.2">Do diverse juries flip divisive comments?</head><p>Having established that the diverse juries provided by participants cause toxicity predictions to flip, we now investigate which comments are flipping. Specifically, we ask whether the comments that flip tend to be more divisive among annotators than the comments that do not flip. To make this determination, we compute an annotator disagreement rate for each comment in the test set. We find the annotator disagreement rate by randomly sampling pairs of annotations for the same comment, and computing the percentage of the time that these pairs disagree with each other. Across all comments that participants' proposed juries cause to flip, the annotator disagreement rate is 46.4%. A two-proportion z-test shows this to be a significant increase over the 37.2% disagreement rate for comments that these juries did not flip (𝑧 = 2.89, 𝑝 &lt; .01). This result indicates that jury learning has the biggest impact on comments that are the most divisive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">DISCUSSION</head><p>In this section, we reflect on the contributions, limitations and future opportunities of our approach. We reflect on how designers and product teams might use it in practice. Finally, we reflect on the ethical considerations of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Implications for design</head><p>How do we build artificial intelligence systems that reflect our values? Values are often diverse and heterogeneous across individuals. While the raw datasets that most ML systems rely on are made up of individuals, today's approaches to building machine learning classifiers typically abstract the individuals out of the pipeline. They view differences among annotators as label noise, rather than as genuine differences of opinion that practitioners need to understand and account for. Jury learning is an attempt to re-think the machine learning pipeline so that practitioners make explicit value judgements about the voices that their classifiers should reflect. We believe, and our evaluation suggests, that practitioners and researchers who make these decisions explicitly will include greater diversity than typical models today. Our approach centers individuals at each stage of the pipeline rather than abstracting or aggregating them as in today's ML approaches.</p><p>8.1.1 A new lens for ML interpretability. Today, approaches for machine learning interpretability typically base their explanations around properties of the item in question, aiming to communicate how an item's features or content led the model to make its decision. Our approach affords a new, complementary lens to machine learning interpretability, in which we aim to explain a model's prediction as a function of the properties of its annotators.</p><p>Consider an activist whose social media posts are removed by an AI. They might rightfully wonder if their posts were moderated because the annotators that trained the moderation model had different political views. Such information is currently completely hidden, making it difficult for this activist to trust the outcomes of automated moderation systems. In contrast, jury learning enables new interpretable methods for users to interrogate which groups' opinions are being listened to, which groups' opinions are not, and for what kinds of inputs. Does it weigh men's voices more than women's in its training data? Does this amplify bias for some topics? Jury learning could empower end users to call for greater representation. More broadly, jury learning offers a new way for users and decision makers to communicate and debate normative decisions about whose perspectives should be included.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Ethical considerations</head><p>Compared to today's implicit procedure for selecting a classifier's voice, our explicit approach introduces its own ethical issues and trade-offs. 8.2.1 Making fair and transparent decisions. How do we eradicate harmful biases in machine learning? Existing approaches in the machine learning fairness literature largely take the training data as a given, and then enforce statistical constraints that can introduce notions of fairness on the resulting model's output (e.g., that a model's decisions must be equitable across genders). In other words, the existing fairness literature starts from the assumption that the underlying statistical correlations in the world are flawed, and that they must be corrected through post-hoc adjustments of decisions that were learned from a flawed world. However, these solutions are ultimately band-aids to a problematic input pipeline. A useful distinction is to consider different forms of justice. We can think of jury learning as a form of procedural justice. We do not claim to guarantee the fairness of outcomes, but instead we make claims around the correctness of the process.</p><p>Our work instead takes the position that it is sometimes more desirable or tractable to select specific people whose voices should be emulated. This position comes with its own set of challenges. While jury learning empowers and normatively encourages practitioners to think carefully about whose voices their models represent, it does not inherently enforce notions of fairness. Jury learning can be used to beneficially select the most important voices to a practitioner, or to equitably represent a diverse set of groups. Jury learning can also be used to unintentionally or deliberately make biased decisions that may cause harm. A practitioner could purposely exclude a relevant group's voice, or could unintentionally include a harmful voice. If, for instance, a practitioner unintentionally or intentionally selects racist jurors, then the resulting model will be racist.</p><p>However, unlike fairness approaches that focus on outcomes, the jury learning approach can make use of tools from the humancomputer interaction and social sciences literature that provide established and effective levers to recruit, train, and socialize people such that a practitioner can overcome these challenges and achieve the jury composition that they want. We argue that, if the options are to make decisions by enforcing post-hoc constraints on the decisions learned from large and somewhat random datasets, or the jury learning approach of explicitly selecting people who make decisions, it is often better to go with the latter. In doing so, we can entrust decision-making to the most relevant, qualified people for any task or situation.</p><p>Beyond the juror selection considerations above, we also advocate for transparent juries. Even if jury learning leads to increases in diversity, jury learning is unlikely to dramatically re-order the existing power structures within sociotechnical systems. Rather, the aim of jury learning is to ensure that decision-making regarding issues of power, in particular whose voices are represented in classification tasks, is made explicit and transparent. We therefore propose that any organization deploying a jury-based classifier make their jury composition transparent to relevant stakeholders. In doing so, jury learning enables a new set of conversations between practitioners and stakeholders about precisely whose voices a classifier is emulating, the implications of emulating those voices, and the ability to explore and implement different sets of voices. Such conversations could be considered akin to a Batson challenge, a process in the US legal system in which stakeholders to a case can argue against the removal of particular jurors on impermissible ground. To that end, we also suggest that practitioners make their instantiation of our jury learning interactive interface publicly available as a sandbox so that anyone can understand how different juries might make different decisions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8.2.2</head><p>Addressing the ecological fallacy. Our aim with jury learning is to help practitioners recognize and integrate annotator disagreement in the classifier pipeline. To achieve this, we ask practitioners to create a jury that specifies the individuals or groups their classifiers should emulate. One approach to creating such a classifier might have been to simply model each group as a singular representative voice, akin to personas in traditional HCI methods. However, such an approach would promote an ecological fallacy because it does not demonstrate the extent to which annotators within a group disagree with each other. Our approach instead models individual annotators, enabling tools that inform practitioners about disagreement within groups. The amount of this disagreement depends upon the extent to which the group identities selected by the practitioner can explain disagreement between annotators.</p><p>Another risk arises from the requirement that many machine learning tasks produce a single decision. To make this decision, we must take a position that resolves any disagreement: specifically, we use a median-of-means approach that takes the median jury after randomly sampling 100 juries that match the practitioner's jury composition, ignoring ones that might be outliers. Thus, our system still presents an opportunity promote the ecological fallacy. To ensure that practitioners are aware of this risk, our interactive interface clearly communicates that each jury composition can have many different instantiations, and that a jury's verdict may change depending upon which jurors happened to be selected. Further, we promptly display visualizations that contextualize each individual juror within their larger group, demonstrating where they fall within the distribution of other annotators that may have been chosen in their stead. Finally, as mentioned in our system description, the interface disallows selecting any groups with an insufficient number of annotators in the dataset to complete the resampling procedure without replacement, directing practitioners to collect more data for the particular group.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.3">Accurate representation.</head><p>As with any machine learning system, our approach is only as good as the labels provided to it, and only as good as the model's ability to learn from these labels. If a dataset does not accurately represent the views of its annotators, or does not accurately convey each annotator's group memberships, then our model will emulate those inaccuracies. Users of our system must therefore follow best practices when collecting their datasets. For instance, the dataset we used to demonstrate jury learning relies on self-identifications, which brings its own tradeoffs when compared to an approach that attributes identity characteristics to participants.</p><p>Further, no current model architecture can perfectly emulate the annotators it was trained on. The high stakes nature of social computing settings means that there can be substantial harm from misrepresenting minority perspectives. Good crowdsourcing practices should therefore be paired with participatory methods for auditing the models produced by jury learning, and any performance metrics should be split out by group to ensure that the model's performance is equitable across groups. Future work should also develop new techniques based on robust machine learning to ensure that models are trained to explicitly optimize for performance across all subpopulations rather than on average <ref type="bibr" target="#b40">[41]</ref>.</p><p>8.2.4 Abdication of responsibility. One risk of the jury learning approach is that it may provide a mechanism for platforms to both avoid taking broad policy stances and also evade blame for content moderation decisions. This stems from two aspects of its present design that remain open-ended: (1) the choice of the decisionmaker who wields the jury learning tool to make content moderation decisions, and (2) the meta-policy by which the jury learning outputs are incorporated into an end-to-end content moderation system (answering questions like: what circumstances do and don't warrant the creation of a new jury? How we weight the jury outcomes against other algorithmic tools' outcomes in a standard, principled way? How should we balance the jury outcome against the opinion of a content moderator? How do we select what comments should be sent to a jury?). Ultimately, the organizations deploying classifiers are responsible for the decisions their classifier makes, and should still be held accountable for them. 8.2.5 Annotator privacy. Faithful and accurate representation of jurors potentially requires information collection about the private views and attributes of jurors. Factors such as sexual orientation are highly private, but can be a key part of creating a jury with diverse perspectives. Data recovery and record linkage attacks mean that such information could potentially be leaked to an adversary. Balancing the rights of jurors to privacy with the accountability and transparency benefits of leveraging juror demographics is a challenging open question. Future work in jury learning should investigate methods to disclose potential privacy harms to annotators. For instance, disclosure may require that, when collecting new datasets, we make clear to labelers the possibility that these attributes may be recoverable. Future work should also draw on approaches for differential privacy in AI <ref type="bibr" target="#b0">[1]</ref> to help ensure us that individuals or rare demographic attributes are not rediscoverable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Limitations and future work</head><p>As with any machine learning approach, there are several limitations and future directions worth discussing: 8.3.1 Domains. In this paper, we demonstrated jury learning using a single application domain: toxicity detection. However, our approach is designed to work for any task in which there is annotator disagreement, a dataset denoting each annotator's relevant group memberships, and an existing classification model that produces high quality embeddings for each item. In particular, we hope future work will investigate using jury learning for medical decision making and design tasks, which may rely on different perspectives. For instance: a doctor making use of a model to help them decide between different treatment options might benefit if their model's decisions were based on a jury that reflects a particular patient's preferences in quality of life trade-offs. Or an amateur designer making use of an AI-based tool for poster design might benefit from the ability to create juries reflecting different design sensibilities or artistic schools of thought. 8.3.2 Jury metaphor. Jury learning loosely draws on a metaphor of juries in the US legal system, but we do not intend this rhetorical device to indicate a complete isomorphism. Rather, jury learning draws on two specific aspects of juries: the notion of moving from a single decision maker to a group of voting decision makers, and the idea of some sort of juror selection process.</p><p>Juries in the US legal system are the sites of complex social behaviors facilitated through an intricate legal apparatus <ref type="bibr" target="#b41">[42]</ref>. These behaviors yield benefits and challenges to justice (for instance, group polarization <ref type="bibr" target="#b78">[79]</ref>) and are not the focus of our system. For instance, jury learning does not draw on the deliberative nature of juries, which has been the subject of decades of study in legal literature <ref type="bibr" target="#b21">[22]</ref>. Jury learning's approach to juror selection also contrasts with the approach taken in the US legal system. Jury learning empowers practitioners and end-users to select their own jury composition. In the US legal system, jury selection is not in hands of single individual, but rather jurors are selected through a process in which stakeholders argue to determine its composition. As discussed above in our ethical considerations section, a stakeholder-centered selection process may sometimes be useful in jury learning, and existing work in the HCI literature <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b52">53]</ref> demonstrates how such a process could be put into practice within our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3.3">Group identifiers.</head><p>To demonstrate jury learning, we relied on an existing dataset that provided group membership information for each annotator. This dataset happened to focus on collecting this information for socio-demographic groups. One limitation to note is that the choice to use categories here has consequences. For instance, non-binary individuals find gender dropdown forms problematic unless they include appropriate nonbinary options and an open text box for description when appropriate <ref type="bibr" target="#b75">[76]</ref>. One approach to creating inclusive interfaces in this respect is to ensure that all relevant options are represented in the jury interface. Another would be to allow the practitioner to explore the set of people who used the open-ended textbox and select a subset of them for inclusion as possible jurors.</p><p>Finally, our approach currently relies on datasets that include explicit information about the groups that each annotator belongs to. Future work should investigate unsupervised approaches to finding different voices within datasets <ref type="bibr" target="#b45">[46]</ref>, potentially rendering the jury learning approach possible with any existing dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4">Positionality statement</head><p>The authors represent backgrounds ranging from computer science (HCI, machine learning) to media psychology. We acknowledge critical arguments making thoughtful cases for removing AI from socio-technical systems, as well as arguments substantially increasing human control, oversight and audits of them. Our ideological commitment in this paper is to situations where improvement rather than outright removal of the AI is the appropriate mitigation strategy. We also acknowledge our shaping by the North American normative commitment to decisions being made by a jury of peers. Historically, juries have been sites of both progressive and regressive decision-making. Finally, we recognize that the term "toxic" is non-specific and often used as a catch-all term for a variety of forms of content that people do not wish to see online. In order to be consistent with the process used to collect this dataset, we draw upon this use of the term "toxic. "</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">CONCLUSION</head><p>Machine learning often means learning to imitate people. So whose voices-whose labels-does a machine learning algorithm learn to imitate? Faced with endemic disagreement in user-facing tasks, we have to make a choice. But today's supervised learning pipelines typically abstract individual people out of the pipeline, treating people as abstractions or aggregated pseudo-humans. As a result, we lack the ability to reason over who disagrees and why. Jury learning is an attempt to bridge the realities of machine learning with the realities of contested tasks. Our approach enables practitioners to make explicit value judgements that inform how models resolve disagreement. If successful, we hope that this approach will help developers make more informed and intentional decisions about creating and deploying classifiers in these contexts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: System Overview. (A) In the Jury Selection portion of the system, the user can create juror sheets to populate their jury composition and can provide one or more input examples to evaluate. (B) Then, the system outputs the Jury Learning Results section where they can view a summary of the jury verdict based on a median-of-means estimator of jury outcomes. Here, they can view the full distribution of jury outcomes, select individual juries to view trends, and inspect individual jurors on a jury. (C) When a user selects a jury, the Jury Trends section is updated. There, they can group by different fields like the juror sheet, decision label, or other demographic attributes to understand patterns in the labels from this jury and contextualize them with respect to the larger population. (D) When a user selects a particular juror, the Juror Details view opens, and they can inspect the predicted label for the juror, the background of this juror, and the juror's annotations. (E) Users can also inspect counterfactual juries that would result in the opposite verdict.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The response likewise is returned as a Python dictionary object.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: We introduce a model architecture that jointly fine tunes the practitioner's existing content-based classifier alongside a Deep &amp; Cross Network recommender system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>min 𝑝 * ∈Z + ∑︁ 𝑘 (𝑝 𝑘 − 𝑝 * 𝑘 ) 2 𝑠.𝑡 . ∑︁ 𝑘 𝑝 * 𝑘 = 𝑛 𝑗𝑢𝑟𝑜𝑟𝑠 and 𝑣 𝑝 * &gt; 1 and 𝑝 * 𝑘 ≥ 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure6: Jury composition results (𝑁 = 16). While there are sizeable disparities in group representation in the current algorithm implicit jury (denoted with red lines), the moderator-specified juries generally achieve greater diversity (raising representation for groups with the lowest red lines and lessening the gap in representation among groups).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>7. 5</head><label>5</label><figDesc>Results: Jury prediction outcomes (Q2) 7.5.1 How many classification outcomes flip between toxic and nontoxic? Having established that participants composed a diverse selection of juries, we now ask: do these participant-specified juries result in different prediction outcomes than those produced by a standard classifier? As our standard baseline classifier, we use the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>Performance against individual annotator's test labels for three models: today's standard state-of-the-art aggregate approach (which is annotator-agnostic, and makes one prediction per example), a group-specific version of our proposed architecture, and the full version of our proposed architecture. The standard aggregated model's performance varies substantially between groups. For instance, it achieves an MAE of 0.83 for Asian annotators and 1.12 for Black annotators, a performance decrease of 35.0%. By comparison, we find that our model does show differences between groups, but with far smaller magnitudes. It achieves an MAE of 0.62 for Asian annotators and 0.65 for Black annotators, a performance decrease of 4.9%.</figDesc><table><row><cell></cell><cell cols="2">Full test set Asian</cell><cell>Black</cell><cell>Hispanic</cell><cell>White</cell><cell cols="2">Male Female</cell></row><row><cell>Number unique annotators</cell><cell>11262</cell><cell>817</cell><cell>1774</cell><cell>424</cell><cell>9087</cell><cell cols="2">6077 6985</cell></row><row><cell cols="2">MAE: Baseline aggregated model 0.90</cell><cell>0.83</cell><cell>1.12</cell><cell>0.87</cell><cell>0.87</cell><cell>0.94</cell><cell>0.86</cell></row><row><cell>MAE: Jury learning model</cell><cell>0.61</cell><cell>0.62</cell><cell>0.65</cell><cell>0.57</cell><cell>0.60</cell><cell>0.61</cell><cell>0.60</cell></row><row><cell></cell><cell>Liberal</cell><cell cols="4">Independent Conservative Asian+Female+Liberal Hispanic+Male+Conservative</cell><cell></cell><cell></cell></row><row><cell>Number unique annotators</cell><cell>5388</cell><cell>3764</cell><cell>3687</cell><cell>206</cell><cell>54</cell><cell></cell><cell></cell></row><row><cell cols="2">MAE: Baseline aggregated model 0.86</cell><cell>0.86</cell><cell>1.01</cell><cell>0.84</cell><cell>0.96</cell><cell></cell><cell></cell></row><row><cell>MAE: Jury learning model</cell><cell>0.60</cell><cell>0.58</cell><cell>0.65</cell><cell>0.62</cell><cell>0.64</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>Jury composition one-sample t-test results (𝑁 = 16</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>In a field study, we asked participants (𝑁 = 16</figDesc><table><row><cell>Jury composition approach</cell><cell cols="2">Count Anticipated outcomes</cell><cell cols="2">Count Justifications for increasing/decreasing voice</cell><cell>Count</cell></row><row><cell>Diversity, equal representation, fairness</cell><cell>13</cell><cell>Better capturing views of mi-</cell><cell>8</cell><cell>Extent to which &lt;group&gt; has relevant experience or</cell><cell>7</cell></row><row><cell></cell><cell></cell><cell>nority groups</cell><cell></cell><cell>knowledge about the issues at hand</cell><cell></cell></row><row><cell>Prioritizing groups targeted in sample comments</cell><cell>10</cell><cell>Increase in number of com-</cell><cell>6</cell><cell>Extent to which &lt;group&gt; is marginalized or has experi-</cell><cell>6</cell></row><row><cell></cell><cell></cell><cell>ments rated as toxic</cell><cell></cell><cell>enced historical harms</cell><cell></cell></row><row><cell>Increasing representation of minority groups</cell><cell>4</cell><cell></cell><cell></cell><cell>Extent to which &lt;group&gt; has been targeted by the sam-</cell><cell>4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ple comments</cell><cell></cell></row><row><cell>Decreasing representation of groups that may</cell><cell>1</cell><cell></cell><cell></cell><cell>Extent to which &lt;group&gt; is expected to view as toxic</cell><cell>3</cell></row><row><cell>cause harm to minority groups</cell><cell></cell><cell></cell><cell></cell><cell>the content that other groups would find toxic</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">We exclude two of the moderators' jury composition results: while these participants demonstrated an accurate understanding of the current algorithm and jury algorithm (and thus have valid moderation legitimacy responses), they utilized the "Other" fields to mean "any" or "null," but this field was defined to map to jurors who explicitly self-identified with "Other" for these attributes. Since these responses are not directly comparable, they have been excluded from the quantitative jury composition analysis.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank Joseph Seering for his contributions to our user study. We thank Jane E, Harmanpreet Kaur, Danaë Metaxa, Ranjay Krishna, Matthew Joerke and James Landay for insightful discussions, feedback, and support. We thank Deepak Kumar for providing our toxic content dataset. We thank the reviewers for their helpful comments and suggestions. Mitchell L. Gordon was supported by the Apple Scholars in AI/ML PhD fellowship. This work was supported by the Computer History Museum, Patrick J. McGovern Foundation, and the Stanford Institute for Human-Centered Artificial Intelligence.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep Learning with Differential Privacy</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Brendan Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Mironov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunal</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1145/2976749.2978318</idno>
		<ptr target="https://doi.org/10.1145/2976749.2978318" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security</title>
				<meeting>the 2016 ACM SIGSAC Conference on Computer and Communications Security</meeting>
		<imprint>
			<date type="published" when="2016-10">2016. Oct 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Roles for computing in social change</title>
		<author>
			<persName><forename type="first">Rediet</forename><surname>Abebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Solon</forename><surname>Barocas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manish</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">G</forename><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency</title>
				<meeting>the 2020 Conference on Fairness, Accountability, and Transparency</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="252" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A reductions approach to fair classification</title>
		<author>
			<persName><forename type="first">Alekh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alina</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miroslav</forename><surname>Dudík</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanna</forename><surname>Wallach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="60" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Whose Opinions Matter? Perspective-aware Models to Identify Opinions of Hate Speech Victims in Abusive Language Detection</title>
		<author>
			<persName><forename type="first">Sohail</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valerio</forename><surname>Basile</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viviana</forename><surname>Patti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.15896[cs.CL]</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">N L</forename><surname>Jennifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><forename type="middle">A</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gordon</forename><surname>Arechar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">G</forename><surname>Pennycook</surname></persName>
		</author>
		<author>
			<persName><surname>Rand</surname></persName>
		</author>
		<idno type="DOI">10.31234/osf.io/9qdza</idno>
		<ptr target="https://doi.org/10.31234/osf.io/9qdza" />
		<title level="m">Scaling Up Fact-Checking Using the Wisdom of Crowds</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Software engineering for machine learning: A case study</title>
		<author>
			<persName><forename type="first">Saleema</forename><surname>Amershi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Begel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Deline</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harald</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ece</forename><surname>Kamar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nachiappan</forename><surname>Nagappan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Besmira</forename><surname>Nushi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Zimmermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/ACM 41st International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="291" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Power to the people: The role of humans in interactive machine learning</title>
		<author>
			<persName><forename type="first">Saleema</forename><surname>Amershi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maya</forename><surname>Cakmak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">Bradley</forename><surname>Knox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Kulesza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ai Magazine</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="105" to="120" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Guidelines for human-AI interaction</title>
		<author>
			<persName><forename type="first">Saleema</forename><surname>Amershi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihaela</forename><surname>Vorvoreanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fourney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Besmira</forename><surname>Nushi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Penny</forename><surname>Collisson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jina</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shamsi</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">N</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kori</forename><surname>Inkpen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 chi conference on human factors in computing systems</title>
				<meeting>the 2019 chi conference on human factors in computing systems</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Rehumanized crowdsourcing: a labeling framework addressing bias and ethics in machine learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Natã</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monchu</forename><surname>Barbosa</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2019 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Big data&apos;s disparate impact</title>
		<author>
			<persName><forename type="first">Solon</forename><surname>Barocas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">D</forename><surname>Selbst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Calif. L. Rev</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page">671</biblScope>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Who Moderates the Social Media Giants?</title>
		<author>
			<persName><forename type="first">Barrett</forename><surname>Paul</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<publisher>Center for Business</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Michael S Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Betsy</forename><surname>Magnus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debra</forename><surname>Rajala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charla</forename><surname>Satz</surname></persName>
		</author>
		<author>
			<persName><surname>Waeiss</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.11521</idno>
		<title level="m">ESR: Ethics and Society Review of Artificial Intelligence Research</title>
				<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sprout: Crowd-Powered Task Design for Crowdsourcing</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Bragg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mausam</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology</title>
				<meeting>the 31st Annual ACM Symposium on User Interface Software and Technology<address><addrLine>Berlin, Germany; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="165" to="176" />
		</imprint>
	</monogr>
	<note>UIST &apos;18)</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Tiered governance and demonetization: The shifting terms of labor and compensation in the platform economy</title>
		<author>
			<persName><forename type="first">Robyn</forename><surname>Caplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tarleton</forename><surname>Gillespie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Media+ Society</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">2056305120936636</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Revolt: Collaborative crowdsourcing for labeling machine learning datasets</title>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Chee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Saleema</forename><surname>Amershi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ece</forename><surname>Kamar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2017 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2334" to="2346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Constructing grounded theory: A practical guide through qualitative research</title>
		<author>
			<persName><forename type="first">Kathy</forename><surname>Charmaz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient elicitation approaches to estimate collective crowd answers</title>
		<author>
			<persName><forename type="first">John</forename><surname>Joon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Young</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><forename type="middle">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sindhu</forename><surname>Kutty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungsoo</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juho</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Walter</forename><forename type="middle">S</forename><surname>Lasecki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2019">2019. 2019</date>
			<publisher>CSCW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Design justice: Community-led practices to build the worlds we need</title>
		<author>
			<persName><forename type="first">Sasha</forename><surname>Costanza-Chock</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Dealing with Disagreements: Looking Beyond the Majority Vote in Subjective Annotations</title>
		<author>
			<persName><forename type="first">Aida</forename><surname>Mostafazadeh Davani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Díaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinodkumar</forename><surname>Prabhakaran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.05719[cs.CL]</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Democracy in America</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toqueville</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1835">1835. 1835</date>
			<publisher>A Mentor Book from New American Library</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR09</title>
				<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Jury decision making: 45 years of empirical research on deliberating groups</title>
		<author>
			<persName><forename type="first">Laura</forename><forename type="middle">D</forename><surname>Dennis J Devine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><forename type="middle">B</forename><surname>Clayton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rasmy</forename><surname>Dunford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Seying</surname></persName>
		</author>
		<author>
			<persName><surname>Pryce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychology</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">622</biblScope>
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">How people form folk theories of social media feeds and what it means for how we study self-presentation</title>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Michael A Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffery</forename><forename type="middle">T</forename><surname>Birnholtz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Megan</forename><surname>Hancock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunny</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 CHI conference on human factors in computing systems</title>
				<meeting>the 2018 CHI conference on human factors in computing systems</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Toward a value-analytic approach to information standards</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Dobreski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the Association for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="114" to="122" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Crowdsourcing disagreement for collecting semantic annotation</title>
		<author>
			<persName><forename type="first">Anca</forename><surname>Dumitrache</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Semantic Web Conference</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Crowdsourcing ground truth for medical relation extraction</title>
		<author>
			<persName><forename type="first">Anca</forename><surname>Dumitrache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lora</forename><surname>Aroyo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Welty</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.02185</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Capturing ambiguity in crowdsourcing frame disambiguation</title>
		<author>
			<persName><forename type="first">Anca</forename><surname>Dumitrache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lora</forename><surname>Aroyo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Welty</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.00270</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fairness through awareness</title>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toniann</forename><surname>Pitassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Reingold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd innovations in theoretical computer science conference</title>
				<meeting>the 3rd innovations in theoretical computer science conference</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="214" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">I always assumed that I wasn&apos;t really that close to [her</title>
		<author>
			<persName><forename type="first">Motahhare</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aimee</forename><surname>Rickman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristen</forename><surname>Vaccaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amirhossein</forename><surname>Aleyasen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Vuong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karrie</forename><surname>Karahalios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Sandvig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd annual ACM conference on human factors in computing systems</title>
				<meeting>the 33rd annual ACM conference on human factors in computing systems</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="153" to="162" />
		</imprint>
	</monogr>
	<note>Reasoning about Invisible Algorithms in News Feeds</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Interactive machine learning</title>
		<author>
			<persName><forename type="first">Jerry</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Fails</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><forename type="middle">R</forename><surname>Olsen</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th international conference on Intelligent user interfaces</title>
				<meeting>the 8th international conference on Intelligent user interfaces</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="39" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Digital juries: A civics-oriented approach to platform governance</title>
		<author>
			<persName><forename type="first">Jenny</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><forename type="middle">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 CHI conference on human factors in computing systems</title>
				<meeting>the 2020 CHI conference on human factors in computing systems</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Lessons from Archives: Strategies for Collecting Sociocultural Data in Machine Learning</title>
		<author>
			<persName><forename type="first">Timnit</forename><surname>Gebru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (Virtual Event</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (Virtual Event<address><addrLine>CA, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">3609</biblScope>
		</imprint>
	</monogr>
	<note>KDD &apos;20)</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Timnit</forename><surname>Gebru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Morgenstern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Briana</forename><surname>Vecchione</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><forename type="middle">Wortman</forename><surname>Vaughan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanna</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><surname>Crawford</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.09010</idno>
		<title level="m">Datasheets for datasets</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for toxic comment classification</title>
		<author>
			<persName><surname>Spiros V Georgakopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aristidis</forename><forename type="middle">G</forename><surname>Sotiris K Tasoulis</surname></persName>
		</author>
		<author>
			<persName><surname>Vrahatis</surname></persName>
		</author>
		<author>
			<persName><surname>Vassilis P Plagianakos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th Hellenic Conference on Artificial Intelligence</title>
				<meeting>the 10th Hellenic Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Custodians of the Internet: Platforms, content moderation, and the hidden decisions that shape social media</title>
		<author>
			<persName><forename type="first">Tarleton</forename><surname>Gillespie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Yale University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The disagreement deconvolution: Bringing machine learning performance metrics in line with reality</title>
		<author>
			<persName><forename type="first">Kaitlyn</forename><surname>Mitchell L Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kayur</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsunori</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2021 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Ghost work: How to stop Silicon Valley from building a new global underclass</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><surname>Suri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Eamon Dolan Books</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Equality of opportunity in supervised learning</title>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nati</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3315" to="3323" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Context-Aware Recommender System: A Review of Recent Developmental Process and Future Research Direction</title>
		<author>
			<persName><forename type="first">Khalid</forename><surname>Haruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akmar</forename><surname>Maizatul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhendroyono</forename><surname>Ismail</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damiasih</forename><surname>Suhendroyono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adi</forename><surname>Damiasih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haruna</forename><surname>Cilik Pierewan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tutut</forename><surname>Chiroma</surname></persName>
		</author>
		<author>
			<persName><surname>Herawan</surname></persName>
		</author>
		<idno type="DOI">10.3390/app7121211</idno>
		<ptr target="https://doi.org/10.3390/app7121211" />
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Fairness without demographics in repeated loss minimization</title>
		<author>
			<persName><forename type="first">Tatsunori</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Megha</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongseok</forename><surname>Namkoong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1929" to="1938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Reid</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">D</forename><surname>Penrod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nancy</forename><surname>Pennington</surname></persName>
		</author>
		<title level="m">Inside the jury</title>
				<imprint>
			<publisher>Harvard University Press</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Understanding international perceptions of the severity of harmful content online</title>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Jialun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Casey</forename><surname>Klaus Scheuerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jed</forename><forename type="middle">R</forename><surname>Fiesler</surname></persName>
		</author>
		<author>
			<persName><surname>Brubaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">e0256762</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<ptr target="https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/overview" />
	</analytic>
	<monogr>
		<title level="m">Jigsaw Unintended Bias in Toxicity Classification</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Can Facebook&apos;s new A.I. banish Pepe the Frog?</title>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Kahn</surname></persName>
		</author>
		<ptr target="https://fortune.com/2020/05/12/facebook-a-i-hate-speech-covid-19-misinformation/" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Parting crowds: Characterizing divergent interpretations in crowdsourced annotation tasks</title>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Kairam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Heer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM Conference on Computer-Supported Cooperative Work &amp; Social Computing</title>
				<meeting>the 19th ACM Conference on Computer-Supported Cooperative Work &amp; Social Computing</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1637" to="1648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">The Facebook Oversight Board: Creating an independent institution to adjudicate online free expression</title>
		<author>
			<persName><forename type="first">Kate</forename><surname>Klonick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Yale LJ</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page">2418</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Managing Disruptive Behavior through Non-Hierarchical Governance: Crowdsourcing in League of Legends and Weibo</title>
		<author>
			<persName><forename type="first">Yubo</forename><surname>Kou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinning</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaozeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bonnie</forename><surname>Nardi</surname></persName>
		</author>
		<idno type="DOI">10.1145/3134697</idno>
		<ptr target="https://doi.org/10.1145/3134697" />
	</analytic>
	<monogr>
		<title level="j">Proc. ACM Hum.-Comput. Interact. 1, CSCW, Article</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2017-12">2017. Dec. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">Gage</forename><surname>Kelley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunny</forename><surname>Consolvo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Mason</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elie</forename><surname>Bursztein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zakir</forename><surname>Durumeric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bailey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.04511</idno>
		<title level="m">Designing Toxic Content Classification for a Diversity of Perspectives</title>
				<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">On quality control and machine learning in crowdsourcing</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Lease</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshops at the Twenty-Fifth AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Robust machine learning by medianof-means: theory and practice</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lecué</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Lerasle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="906" to="931" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Crowdsourcing for participatory democracies: Efficient elicitation of social choice functions</title>
		<author>
			<persName><forename type="first">Timothy</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tanja</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helene</forename><surname>Aitamurto</surname></persName>
		</author>
		<author>
			<persName><surname>Landemore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Second AAAI Conference on Human Computation and Crowdsourcing</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">WeBuildAI: Participatory framework for algorithmic governance</title>
		<author>
			<persName><forename type="first">Min</forename><surname>Kyung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Kusbit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anson</forename><surname>Kahng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><forename type="middle">Tae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinran</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allissa</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ritesh</forename><surname>Noothigattu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siheon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Psomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2019">2019. 2019</date>
			<publisher>CSCW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Screen2Vec: Semantic Embedding of GUI Screens and GUI Components</title>
		<author>
			<persName><forename type="first">Toby Jia-Jun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lindsay</forename><surname>Popowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brad</forename><forename type="middle">A</forename><surname>Myers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2021 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">Arend</forename><surname>Lijphart</surname></persName>
		</author>
		<title level="m">Patterns of democracy</title>
				<imprint>
			<publisher>Yale university press</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning to predict population-level label distributions</title>
		<author>
			<persName><forename type="first">Tong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akash</forename><surname>Venkatachalam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pratik</forename><surname>Sanjay Bongale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Homan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Companion Proceedings of The 2019 World Wide Web Conference</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1111" to="1120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Wingit: Efficient refinement of unclear task instructions</title>
		<author>
			<persName><forename type="first">Chaithanya</forename><surname>Vk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Manam</surname></persName>
		</author>
		<author>
			<persName><surname>Quinn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth AAAI Conference on Human Computation and Crowdsourcing</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Algorithmic misogynoir in content moderation practice</title>
		<author>
			<persName><forename type="first">Brandeis</forename><surname>Marshall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">Heinrich-Böll-Stiftung</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A survey on bias and fairness in machine learning</title>
		<author>
			<persName><forename type="first">Ninareh</forename><surname>Mehrabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fred</forename><surname>Morstatter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nripsuta</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aram</forename><surname>Galstyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A paradigm for democratizing artificial intelligence research</title>
		<author>
			<persName><forename type="first">Erwan</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marguerite</forename><surname>Barry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Innovations in Big Data Mining and Embedded Knowledge</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="137" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Designing Ground Truth and the Social Life of Labels</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christine</forename><forename type="middle">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Desmond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Narendra</forename><forename type="middle">Nath</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zahra</forename><surname>Ashktorab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aabhas</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Brimijoin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evelyn</forename><surname>Duesterwald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Casey</forename><surname>Dugan</surname></persName>
		</author>
		<idno type="DOI">10.1145/3411764.3445402</idno>
		<ptr target="https://doi.org/10.1145/3411764.3445402" />
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Association for Computing Machinery</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<author>
			<persName><forename type="first">Thanh</forename><surname>Dat Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><surname>Nguyen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.10200[cs.CL]</idno>
		<title level="m">BERTweet: A pretrained language model for English Tweets</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Examining Difficulties Software Developers Encounter in the Adoption of Statistical Machine Learning</title>
		<author>
			<persName><forename type="first">Kayur</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Fogarty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">A</forename><surname>Landay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beverly</forename><forename type="middle">L</forename><surname>Harrison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1563" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Inherent disagreements in human textual inferences</title>
		<author>
			<persName><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="677" to="694" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Human uncertainty makes classification more robust</title>
		<author>
			<persName><forename type="first">Ruairidh</forename><forename type="middle">M</forename><surname>Joshua C Peterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Battleday</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><surname>Russakovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9617" to="9626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">The State of Online Harassment</title>
		<ptr target="https://www.pewresearch.org/internet/2021/01/13/the-state-of-online-harassment/" />
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Pew Research Center</publisher>
			<pubPlace>Washington, D.C.</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">On Releasing Annotator-Level Labels and Information in Datasets</title>
		<author>
			<persName><forename type="first">Aida</forename><forename type="middle">Mostafazadeh</forename><surname>Vinodkumar Prabhakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Davani</surname></persName>
		</author>
		<author>
			<persName><surname>Diaz</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2021.law-1.14" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Joint 15th Linguistic Annotation Workshop (LAW) and 3rd Designing Meaning Representations (DMR) Workshop</title>
				<meeting>The Joint 15th Linguistic Annotation Workshop (LAW) and 3rd Designing Meaning Representations (DMR) Workshop<address><addrLine>Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="133" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Society-in-the-loop: programming the algorithmic social contract</title>
		<author>
			<persName><forename type="first">Iyad</forename><surname>Rahwan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ethics and Information Technology</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="5" to="14" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">The justification of bicameralism</title>
		<author>
			<persName><surname>William H Riker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Political Science Review</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="101" to="116" />
			<date type="published" when="1992">1992. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<author>
			<persName><forename type="first">Anna</forename><surname>Rogers</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.13947[cs.CL]</idno>
		<title level="m">Changing the World by Changing the Data</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Auditing algorithms: Research methods for detecting discrimination on internet platforms. Data and discrimination: converting critical concerns into productive inquiry</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Sandvig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karrie</forename><surname>Karahalios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cedric</forename><surname>Langbort</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Understanding expert disagreement in medical data analysis through structured adjudication</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schaekermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graeme</forename><surname>Beaton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minahz</forename><surname>Habib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edith</forename><surname>Law</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2019">2019. 2019</date>
			<publisher>CSCW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Ambiguity-aware ai assistants for medical data analysis</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schaekermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graeme</forename><surname>Beaton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elaheh</forename><surname>Sanoubari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 CHI conference on human factors in computing systems</title>
				<meeting>the 2020 CHI conference on human factors in computing systems</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
	<note>Kate Larson, and Edith Law</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Resolvable vs. irresolvable disagreement: A study on worker deliberation in crowd work</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schaekermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joslin</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edith</forename><surname>Law</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note>CSCW</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Do Datasets Have Politics? Disciplinary Values in Computer Vision Dataset Development</title>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Klaus Scheuerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Hanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Denton</surname></persName>
		</author>
		<idno type="DOI">10.1145/3476058</idno>
		<ptr target="https://doi.org/10.1145/3476058" />
	</analytic>
	<monogr>
		<title level="j">Proc. ACM Hum.-Comput. Interact</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">317</biblScope>
			<date type="published" when="2021-10">2021. oct 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Revisiting Gendered Web Forms: An Evaluation of Gender Inputs with (Non-) Binary People</title>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Klaus Scheuerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katta</forename><surname>Spiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jed</forename><forename type="middle">R</forename><surname>Brubaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2021 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Get another label? improving data quality and data mining using multiple, noisy labelers</title>
		<author>
			<persName><forename type="first">Foster</forename><surname>Victor S Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panagiotis</forename><forename type="middle">G</forename><surname>Provost</surname></persName>
		</author>
		<author>
			<persName><surname>Ipeirotis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="614" to="622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Keeping community in the loop: Understanding wikipedia stakeholder values for machine learning-based systems</title>
		<author>
			<persName><forename type="first">Estelle</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anjali</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Halfaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loren</forename><surname>Terveen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiyi</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2020 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">The law of group polarization</title>
		<author>
			<persName><surname>Cass R Sunstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">John M. Olin Law &amp; Economics Working Paper</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<date type="published" when="1999">1999. 1999</date>
		</imprint>
		<respStmt>
			<orgName>University of Chicago Law School</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">A framework for understanding unintended consequences of machine learning</title>
		<author>
			<persName><forename type="first">Harini</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">V</forename><surname>Guttag</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.10002</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-first AAAI conference on artificial intelligence</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Crowdsourcing in medical research: concepts and applications</title>
		<author>
			<persName><forename type="first">Suzanne</forename><surname>Joseph D Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiming</forename><surname>Day</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><surname>Bayus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PeerJ</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">e6762</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Challenges for toxic comment classification: An in-depth error analysis</title>
		<author>
			<persName><forename type="first">Julian</forename><surname>Betty Van Aken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralf</forename><surname>Risch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Krestel</surname></persName>
		</author>
		<author>
			<persName><surname>Löser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.07572</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Detecting spam bots in online social networking sites: a machine learning approach</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Hai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IFIP Annual Conference on Data and Applications Security and Privacy</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="335" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Classification with Label Distribution Learning</title>
		<author>
			<persName><forename type="first">Jing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Geng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Joint Conference on Artificial Intelligence</title>
				<meeting>the 28th International Joint Conference on Artificial Intelligence<address><addrLine>Macao, China</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3712" to="3718" />
		</imprint>
	</monogr>
	<note>IJCAI&apos;19)</note>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">DCN V2: Improved Deep &amp; Cross Network and Practical Lessons for Web-scale Learning to Rank Systems</title>
		<author>
			<persName><forename type="first">Ruoxi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rakesh</forename><surname>Shivanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sagar</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichan</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
				<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1785" to="1797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<author>
			<persName><forename type="first">Tian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuri</forename><forename type="middle">M</forename><surname>Brovman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sriganesh</forename><surname>Madhvanath</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06156</idno>
		<title level="m">Personalized Embedding-based e-Commerce Recommendations at eBay</title>
				<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Keeping designers in the loop: Communicating inherent algorithmic trade-offs across multiple objectives</title>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loren</forename><surname>Terveen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Zhiwei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jodi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiyi</forename><surname>Forlizzi</surname></persName>
		</author>
		<author>
			<persName><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 ACM Designing Interactive Systems Conference</title>
				<meeting>the 2020 ACM Designing Interactive Systems Conference</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1245" to="1257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">PolicyKit: Building Governance in Online Communities</title>
		<author>
			<persName><forename type="first">Amy</forename><forename type="middle">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grant</forename><surname>Hugh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology</title>
				<meeting>the 33rd Annual ACM Symposium on User Interface Software and Technology</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="365" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Predicting the distribution of emotion perception: capturing inter-rater variability</title>
		<author>
			<persName><forename type="first">Biqiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Essl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><forename type="middle">Mower</forename><surname>Provost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM International Conference on Multimodal Interaction</title>
				<meeting>the 19th ACM International Conference on Multimodal Interaction</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="51" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">Fake news: A survey of research, detection methods, and opportunities</title>
		<author>
			<persName><forename type="first">Xinyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reza</forename><surname>Zafarani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00315</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Value-sensitive algorithm design: Method, case study, and lessons</title>
		<author>
			<persName><forename type="first">Haiyi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Halfaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loren</forename><surname>Terveen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2018">2018. 2018</date>
			<publisher>CSCW</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
