<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pre-training is a Hot Topic: Contextualized Document Embeddings Improve Topic Coherence</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Federico</forename><surname>Bianchi</surname></persName>
							<email>f.bianchi@unibocconi.it</email>
							<affiliation key="aff0">
								<orgName type="institution">Bocconi University</orgName>
								<address>
									<addrLine>Via Sarfatti 25</addrLine>
									<postCode>20136</postCode>
									<settlement>Milan</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Silvia</forename><surname>Terragni</surname></persName>
							<email>s.terragni4@campus.unimib.it</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Milan-Bicocca Viale Sarca 336</orgName>
								<address>
									<postCode>20126</postCode>
									<settlement>Milan</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dirk</forename><surname>Hovy</surname></persName>
							<email>dirk.hovy@unibocconi.it</email>
							<affiliation key="aff2">
								<orgName type="institution">Bocconi University</orgName>
								<address>
									<addrLine>Via Sarfatti 25</addrLine>
									<postCode>20136</postCode>
									<settlement>Milan</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Pre-training is a Hot Topic: Contextualized Document Embeddings Improve Topic Coherence</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Topic models extract meaningful groups of words from documents, allowing for a better understanding of data. However, the solutions are often not coherent enough, and thus harder to interpret. Coherence can be improved by adding more contextual knowledge to the model. Recently, neural topic models have become available, while BERT-based representations have further pushed the state of the art of neural models in general. We combine pre-trained representations and neural topic models. Pre-trained BERT sentence embeddings indeed support the generation of more meaningful and coherent topics than either standard LDA or existing neural topic models. Results on four datasets show that our approach effectively increases topic coherence.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Language pre-training is becoming ubiquitous in Natural Language Processing (NLP). Bidirectional Encoder Representations from Transformers (BERT) <ref type="bibr" target="#b5">(Devlin et al., 2018)</ref>, the most prominent architecture in this category, allow us to extract pre-trained document representations to easily reach, and even exceed, state-of-the-art performance across many tasks. Consequentially, researchers use BERT representations in a diverse set of NLP applications <ref type="bibr" target="#b17">(Nozza et al., 2020;</ref><ref type="bibr" target="#b22">Rogers et al., 2020)</ref>.</p><p>However, Topic Modeling is is one NLP application still missing out on the benefit from language model pre-training. This lack is surprising, given that incorporating external knowledge into topic models is know to produce more coherent topics. Coherent topics are easier to interpret by people, and are considered more meaningful. E.g., a topic with "apple, pear, lemon, banana, kiwi" is more coherent than one defined by "apple, knife, lemon, banana, spoon". Coherence can be measured in a variety of ways, from human evaluation via intrusion tests <ref type="bibr" target="#b2">(Chang et al., 2009)</ref> to approximated scores <ref type="bibr" target="#b11">(Lau et al., 2014;</ref><ref type="bibr" target="#b21">Röder et al., 2015)</ref>.</p><p>Topic models have inspired many extensions that incorporate several types of information <ref type="bibr" target="#b27">(Xun et al., 2017;</ref><ref type="bibr" target="#b4">Das et al., 2015;</ref><ref type="bibr" target="#b16">Nguyen et al., 2015;</ref><ref type="bibr" target="#b18">Petterson et al., 2010)</ref>, use word relationships derived from external knowledge bases <ref type="bibr" target="#b3">(Chen et al., 2013;</ref><ref type="bibr" target="#b28">Yang et al., 2015)</ref>, or pre-trained word embeddings <ref type="bibr" target="#b4">(Das et al., 2015;</ref><ref type="bibr" target="#b6">Dieng et al., 2019;</ref><ref type="bibr" target="#b16">Nguyen et al., 2015)</ref>. Even for neural topic models, there exists little work on incorporating external knowledge, e.g., word embeddings <ref type="bibr" target="#b8">(Gupta et al., 2019;</ref><ref type="bibr" target="#b6">Dieng et al., 2019)</ref>. However, most still use Bag of Words (BoW) document representations as model input, rather than contextualized representations. BoW representations, though, disregard the syntactic and semantic relationships among the words in a document. To the best of our knowledge, no work has yet investigated the use of contextual representations derived from pre-trained language models.</p><p>In this paper, we show that adding contextual information to neural topic models enriches the representations, and provides a significant increase in topic coherence. This effect is even more remarkable given that we cannot embed long documents, due to the sentence length limit in BERT. Concretely, we extend ProdLDA <ref type="bibr" target="#b24">(Srivastava and Sutton, 2017)</ref>, a state-of-the-art topic model that implements black-box variational inference <ref type="bibr" target="#b20">(Ranganath et al., 2014)</ref>, to include BERT representations. Our approach leads to consistent significant improvements in topic coherence, and produces competitive results in topic diversity. Neural-ProdLDA is a neural topic modeling approach based on the Variational AutoEncoder (VAE). The neural variational framework trains a neural inference network to directly map the BoW document representation into a continuous latent representation. Then, a decoder network reconstructs the BoW by generating its words from the latent document representation<ref type="foot" target="#foot_0">1</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions</head><p>The framework explicitly approximates the Dirichlet prior using Gaussian distributions, instead of using a Gaussian prior like Neural Variational Document Models <ref type="bibr" target="#b13">(Miao et al., 2016)</ref>. Moreover, Neural-ProdLDA replaces the multinomial distribution over individual words in standard LDA with a product of experts <ref type="bibr" target="#b9">(Hinton, 2002)</ref> (hence the name ProdLDA).</p><p>We extend this model with contextualized document embeddings from SBERT (Reimers and Gurevych, 2019), a recent extension of BERT that allows the quick generation of sentence embeddings. This approach has one limitation: the sentence-length limit of SBERT. If a document is longer than this limit, some information will be lost. The document representations are projected through a hidden layer with the same dimensionality as the vocabulary size, which is concatenated with the BoW representation. Figure <ref type="figure" target="#fig_0">1</ref> briefly sketches the architecture of our model.</p><p>Note that hidden layer size can be tuned. However, an extensive evaluation of different architectures is out of the scope of this paper.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Datasets We experiment on four common datasets for topic modeling: 20NewsGroup<ref type="foot" target="#foot_2">2</ref> , Tweets2011<ref type="foot" target="#foot_3">3</ref> , Google News <ref type="bibr" target="#b19">(Qiang et al., 2019)</ref>, and the StackOverflow dataset <ref type="bibr" target="#b19">(Qiang et al., 2019)</ref>. We use the common pre-processing pipeline for all datasets, removing digits, punctuation, stopwords, and infrequent words. We derive SBERT document representations from the pre-processed text using the pre-trained model fine tuned on NLI.<ref type="foot" target="#foot_4">4</ref> </p><p>Metrics We evaluate the topics found by a model with three different metrics: two metrics for topic coherence (normalized pointwise mutual information and a word embedding based measure), and one metric to quantify the diversity of the topic solutions.</p><p>The first coherence metric is Normalized Point-wise Mutual Information (NPMI) (Lau et al., 2014) (τ ). It measures how much the top-10 words of a topic are related to each other, considering the empirical frequency of the words computed on the original corpus. τ is a symbolic metric and relies on co-occurrence.</p><p>As <ref type="bibr" target="#b7">Ding et al. (2018)</ref> pointed out, though, topic coherence computed on the same data is inherently limited. Coherence computed on an external corpus, on the other hand, correlates much more to human judgment, but it may be expensive to estimate. Thus, our second metric is an external word embeddings topic coherence metric, which we compute by adopting a strategy similar to that described in <ref type="bibr" target="#b7">Ding et al. (2018)</ref>. First, we compute the average pairwise cosine similarity of the word embeddings of the top-10 words in a topic -using <ref type="bibr" target="#b14">(Mikolov et al., 2013)</ref> embeddings. Then, we compute the overall average of those values for all the topics (α).</p><p>Eventually, to evaluate how diverse the topics generated by a single model are, we use the rankbiased overlap (RBO) <ref type="bibr" target="#b26">(Webber et al., 2010)</ref>. RBO compares two topics of the same model. The key qualities of this measure are twofold: it allows disjointedness between the lists of topics (i.e., two topics can have different words in them) and it is weighted on the ranking (i.e., two lists that share some of the same words, albeit at different rankings, are penalized less than two lists that share the same words at the highest ranks). We define ρ as the rankbiased overlap diversity, that we interpret as the reciprocal of the standard RBO. ρ is 0 for identical topics and 1 for completely different topics. Both metrics are computed on the top-k ranked lists. Following the state-of-the-art, we consider k = 10.</p><p>Baselines We compare our approach with the following baselines: (i) Neural-ProdLDA (N-ProdLDA) <ref type="bibr" target="#b24">(Srivastava and Sutton, 2017)</ref>  <ref type="foot" target="#foot_5">5</ref> (the model we extended);<ref type="foot" target="#foot_6">6</ref> (ii) Neural Variational Document Model (NVDM) <ref type="bibr" target="#b13">(Miao et al., 2016)</ref>; and (iii) LDA <ref type="bibr" target="#b0">(Blei et al., 2003)</ref>.</p><p>Configurations We train all models with the same hyper-parameter configurations to maximize comparability. The inference network for both our method and Neural-ProdLDA consists of one hidden layer and 100-dimension of softplus units, which converts the BoW into embeddings. This final representation is again passed through a hidden layer before the variational inference process. We follow <ref type="bibr" target="#b24">(Srivastava and Sutton, 2017)</ref> for the choice of the parameters. The priors over the topic and document distributions are learnable parameters. For LDA, we estimate the hyperparameters that control the document-topic distribution and topic-word distribution via the Expectation-Maximization algorithm.  Quantitative Evaluation We compute all the metrics for 25, 50, 75, 100 and 150 topics. We average results for each metrics over 30 runs (see Table <ref type="table" target="#tab_3">2</ref>). The topics generated with our model are more coherent than the ones of the baselines. Occasionally, though, they are less diverse than Neural-ProdLDA or NVDM. Figures <ref type="figure" target="#fig_2">2a, 2c</ref>, 2b and 2d show the detailed coherence of τ per number of topics, including statistical significance. In general, our model provides the most coherent topics across all corpora and topic settings. <ref type="bibr" target="#b24">Srivastava and Sutton (2017)</ref> reported that NVDM obtains low coherence. We observe that our model suffers the  most on non-standard text like tweets, where the representations generated by BERT do not provide enough information to the model; this might also be the results of generating the embedded representations from pre-processed text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Qualitative Evaluation To provide evidence of the quality of the topics found by our model, we show the top-5 words of some topics in Figure <ref type="figure">3</ref>. These descriptors illustrate the increased coherence of topics obtained with SBERT embeddings. The topic related to football is more coherent in Contextualized TM than in Neural-ProdLDA. LDA's topics seem to be coherent, but are affected by some added noise/mixture of topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Several topic models are based on neural networks <ref type="bibr" target="#b10">(Larochelle and Lauly, 2012;</ref><ref type="bibr" target="#b23">Salakhutdinov and Hinton, 2009)</ref> or neural variational inference <ref type="bibr" target="#b13">(Miao et al., 2016;</ref><ref type="bibr" target="#b15">Mnih and Gregor, 2014;</ref><ref type="bibr" target="#b24">Srivastava and Sutton, 2017;</ref><ref type="bibr" target="#b12">Miao et al., 2017;</ref><ref type="bibr" target="#b7">Ding et al., 2018)</ref>. <ref type="bibr" target="#b13">Miao et al. (2016)</ref>   framework that explicitly approximates the Dirichlet prior, using a Gaussian distribution to obtain more interpretable and coherent topics. Similar to their approach, <ref type="bibr" target="#b12">Miao et al. (2017)</ref> parameterize the multinomial distributions of each document, proposing three variants of neural topic models that are able to exhibit sparse topic distributions. Our approach builds on these works, but extends it by a crucial component to include contextual information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we show that contextualized BERT document embeddings significantly increase the coherence neural topic models. Our results show that context information is an important element to consider when using topic modeling. In the future, we plan to extend the current sentence-length limit of BERT. We provide a Python package of our model at https://github.com/MilaNLProc/ contextualized-topic-models.</p><p>6 Acknowledgement</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: High-level sketch of our Neural Topic Modeling with BERT embeddings. More details of the architecture we extend are found in the original work<ref type="bibr" target="#b24">(Srivastava and Sutton, 2017)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>NPMI for the 20NewsGroup. 50 ♣ , 75 ♣ , 100 ♣ and 150 ♣ . NPMI for the Google News Dataset. 50 ♣ , 75 ♣ , 100 ♣ and 150 ♣ . NPMI for the StackOverflow Dataset. 25 ♣ , 50 ♣ , 75 ♣ , 100 ♣ and 150 ♣ . NPMI for the Tweets2011 Dataset. 50 ♣ , 75 ♣ .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Detailed results of the NPMI coherence of the models for each dataset. Topics for our model with ♣ are significantly better than the others (t-test).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the datasets used.</figDesc><table><row><cell>Dataset</cell><cell cols="2">Docs Vocabulary</cell></row><row><cell cols="2">20NewsGroup 18173</cell><cell>2000</cell></row><row><cell>Tweets2011</cell><cell>2471</cell><cell>5098</cell></row><row><cell cols="2">Google News 11108</cell><cell>8110</cell></row><row><cell cols="2">StackOverflow 16408</cell><cell>2303</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Averaged results over 5 numbers of topics.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Topics sampled from the Google News dataset (topics = 50)</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">For more details see(Srivastava and Sutton,  </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2017" xml:id="foot_1">).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2">http://qwone.com/ ˜jason/20Newsgroups/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3">https://trec.nist.gov/data/tweets/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4">This can be sub-optimal, but many datasets in the literature are already pre-processed.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_5">Note that<ref type="bibr" target="#b24">(Srivastava and Sutton, 2017</ref>) also propose Neural-LDA, which has been found to be scarcely effective in topic modeling by different researchers, though<ref type="bibr" target="#b24">(Srivastava and Sutton, 2017;</ref><ref type="bibr" target="#b25">Wang et al., 2020)</ref>. Our first results with this model were also inconsistent, so we excluded it from the further experiments.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_6">We use the implementation of<ref type="bibr" target="#b1">(Carrow, 2018)</ref>.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We thank our colleague Debora Nozza for providing insightful comments on the first draft of this paper. Federico Bianchi and Dirk Hovy are members of the Bocconi Institute for Data Science and Analytics (BIDSA) and the Data and Marketing Insights (DMI) unit.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">PyTorchAVITM: Open Source AVITM Implementation in PyTorch</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Carrow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Github</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Reading tea leaves: How humans interpret topic models</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><forename type="middle">L</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Gerrish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 22: 23rd Annual Conference on Neural Information Processing Systems 2009. Proceedings of meeting</title>
				<meeting><address><addrLine>Vancouver, British Columbia, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2009-12-10">2009. held 7-10 December 2009</date>
			<biblScope unit="page" from="288" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Discovering coherent topics using general knowledge</title>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arjun</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meichun</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malú</forename><surname>Castellanos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Riddhiman</forename><surname>Ghosh</surname></persName>
		</author>
		<idno type="DOI">10.1145/2505515.2505519</idno>
	</analytic>
	<monogr>
		<title level="m">22nd ACM International Conference on Information and Knowledge Management, CIKM&apos;13</title>
				<meeting><address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013-10-27">2013. October 27 -November 1, 2013</date>
			<biblScope unit="page" from="209" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Gaussian LDA for topic models with word embeddings</title>
		<author>
			<persName><forename type="first">Rajarshi</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/p15-1077</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing</title>
		<title level="s">Long Papers</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>The Association for Computer Linguistics</publisher>
			<date type="published" when="2015-07-26">2015. 2015. July 26-31, 2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="795" to="804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
				<meeting><address><addrLine>Bert</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Francisco</forename><forename type="middle">Jr</forename><surname>Adji B Dieng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><surname>Blei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.04907</idno>
		<title level="m">Topic modeling in embedding spaces</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Ran</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.02687</idno>
		<title level="m">Coherence-aware neural topic modeling</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Document informed neural autoregressive topic models with distributional prior</title>
		<author>
			<persName><forename type="first">Pankaj</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yatin</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Buettner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v33i01.33016505</idno>
		<idno>AAAI2019</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>AAAI Press</publisher>
			<biblScope unit="page" from="6505" to="6512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Training products of experts by minimizing contrastive divergence</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="DOI">10.1162/089976602760128018</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1771" to="1800" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A neural autoregressive topic model</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislas</forename><surname>Lauly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held</title>
				<meeting><address><addrLine>Lake Tahoe, Nevada, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-12-03">2012. December 3-6, 2012</date>
			<biblScope unit="page" from="2717" to="2725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Machine reading tea leaves: Automatically evaluating topic coherence and topic model quality</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Jey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter</title>
				<meeting>the 14th Conference of the European Chapter<address><addrLine>Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2014-04-26">2014. 2014. April 26-30, 2014</date>
			<biblScope unit="page" from="530" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Discovering discrete latent topics with neural variational inference</title>
		<author>
			<persName><forename type="first">Yishu</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning, ICML 2017</title>
				<meeting>the 34th International Conference on Machine Learning, ICML 2017<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017-06-11">2017. 6-11 August 2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2410" to="2419" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Neural variational inference for text processing</title>
		<author>
			<persName><forename type="first">Yishu</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33nd International Conference on Machine Learning, ICML 2016</title>
				<meeting>the 33nd International Conference on Machine Learning, ICML 2016<address><addrLine>New York City, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-19">2016. June 19-24, 2016</date>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1727" to="1736" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neural variational inference and learning in belief networks</title>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31th International Conference on Machine Learning, ICML</title>
				<meeting>the 31th International Conference on Machine Learning, ICML<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06">2014. 2014. June 2014</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1791" to="1799" />
		</imprint>
	</monogr>
	<note>of JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improving topic models with latent feature word representations</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Dat Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lan</forename><surname>Billingsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="299" to="313" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Debora</forename><surname>Nozza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.02912</idno>
		<title level="m">Making Sense of Language-Specific BERT Models</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Word features for latent dirichlet allocation</title>
		<author>
			<persName><forename type="first">James</forename><surname>Petterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tibério</forename><forename type="middle">S</forename><surname>Caetano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shravan</forename><forename type="middle">M</forename><surname>Buntine</surname></persName>
		</author>
		<author>
			<persName><surname>Narayanamurthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 23: 24th Annual Conference on Neural Information Processing Systems 2010. Proceedings of a meeting held 6-9</title>
				<meeting><address><addrLine>Vancouver, British Columbia, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2010-12">2010. December 2010</date>
			<biblScope unit="page" from="1921" to="1929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Jipeng</forename><surname>Qiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Zhenyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhao</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xindong</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07695</idno>
		<title level="m">Short text topic modeling techniques, applications, and performance: A survey</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sentencebert: Sentence embeddings using siamese bertnetworks</title>
		<author>
			<persName><forename type="first">Rajesh</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Gerrish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>Reykjavik, Iceland</addrLine></address></meeting>
		<imprint>
			<publisher>EMNLP-IJCNLP</publisher>
			<date type="published" when="2014-04-22">2014. April 22-25, 2014. 2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3973" to="3983" />
		</imprint>
	</monogr>
	<note>Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Exploring the space of topic coherence measures</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Röder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Both</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Hinneburg</surname></persName>
		</author>
		<idno type="DOI">10.1145/2684822.2685324</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth ACM International Conference on Web Search and Data Mining, WSDM 2015</title>
				<meeting>the Eighth ACM International Conference on Web Search and Data Mining, WSDM 2015<address><addrLine>Shanghai, China</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015-02-02">2015. February 2-6, 2015</date>
			<biblScope unit="page" from="399" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Anna</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Kovaleva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Rumshisky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.12327</idno>
		<title level="m">A primer in bertology: What we know about how bert works</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Replicated softmax: an undirected topic model</title>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 22: 23rd Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>Vancouver, British Columbia, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2009-12-10">2009. 2009. held 7-10 December 2009</date>
			<biblScope unit="page" from="1607" to="1614" />
		</imprint>
	</monogr>
	<note>Proceedings of a meeting</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Autoencoding variational inference for topic models</title>
		<author>
			<persName><forename type="first">Akash</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01488</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Optimising topic coherence with weighted po&apos;lya urn scheme</title>
		<author>
			<persName><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulan</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2019.12.013</idno>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">385</biblScope>
			<biblScope unit="page" from="329" to="339" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A similarity measure for indefinite rankings</title>
		<author>
			<persName><forename type="first">William</forename><surname>Webber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alistair</forename><surname>Moffat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Zobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A correlated topic model using word embeddings</title>
		<author>
			<persName><forename type="first">Guangxu</forename><surname>Xun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidong</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2017/588</idno>
		<ptr target="ij-cai.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI 2017, Melbourne, Australia</title>
				<meeting>the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI 2017, Melbourne, Australia</meeting>
		<imprint>
			<date type="published" when="2017-08-19">2017. August 19-25, 2017</date>
			<biblScope unit="page" from="4207" to="4213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Efficient methods for incorporating knowledge into topic models</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><forename type="middle">L</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d15-1037</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>The Association for Computational Linguistics</publisher>
			<date type="published" when="2015-09-17">2015. 2015. September 17-21, 2015</date>
			<biblScope unit="page" from="308" to="317" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
