<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DeepInf: Social Influence Prediction with Deep Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution" key="instit1">Tsinghua University ‡ Microsoft Research</orgName>
								<orgName type="institution" key="instit2">Redmond ♯ HEC Montreal</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
							<email>jian.tang@hec.ca</email>
							<affiliation key="aff1">
								<orgName type="institution">Montreal Institute for Learning Algorithms</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
							<email>kuansanw@microsoft.com</email>
						</author>
						<author>
							<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
							<email>jietang@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution" key="instit1">Tsinghua University ‡ Microsoft Research</orgName>
								<orgName type="institution" key="instit2">Redmond ♯ HEC Montreal</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DeepInf: Social Influence Prediction with Deep Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3219819.3220077</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>• Information systems → Data mining</term>
					<term>Social networks</term>
					<term>• Applied computing → Sociology</term>
					<term>Representation Learning</term>
					<term>Network Embedding</term>
					<term>Graph Convolution</term>
					<term>Graph Attention</term>
					<term>Social Influence</term>
					<term>Social Networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Social and information networking activities such as on Facebook, Twitter, WeChat, and Weibo have become an indispensable part of our everyday life, where we can easily access friends' behaviors and are in turn influenced by them. Consequently, an effective social influence prediction for each user is critical for a variety of applications such as online recommendation and advertising.</p><p>Conventional social influence prediction approaches typically design various hand-crafted rules to extract user-and networkspecific features. However, their effectiveness heavily relies on the knowledge of domain experts. As a result, it is usually difficult to generalize them into different domains. Inspired by the recent success of deep neural networks in a wide range of computing applications, we design an end-to-end framework, DeepInf 1 , to learn users' latent feature representation for predicting social influence. In general, DeepInf takes a user's local network as the input to a graph neural network for learning her latent social representation. We design strategies to incorporate both network structures and user-specific features into convolutional neural and attention networks. Extensive experiments on Open Academic Graph, Twitter, Weibo, and Digg, representing different types of social and information networks, demonstrate that the proposed end-to-end model, DeepInf, significantly outperforms traditional feature engineeringbased approaches, suggesting the effectiveness of representation learning for social applications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Social influence is everywhere around us, not only in our daily physical life but also on the virtual Web space. The term social influence typically refers to the phenomenon that a person's emotions, opinions, or behaviors are affected by others. With the global penetration of online and mobile social platforms, people have witnessed the impact of social influence in every field, such as presidential elections <ref type="bibr" target="#b6">[7]</ref>, advertising <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b23">24]</ref>, and innovation adoption <ref type="bibr" target="#b41">[42]</ref>. To date, there is little doubt that social influence has become a prevalent, yet complex force that drives our social decisions, making a clear need for methodologies to characterize, understand, and quantify the underlying mechanisms and dynamics of social influence. Indeed, extensive work has been done on social influence prediction in the literature <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref>. For example, Matsubara et al. <ref type="bibr" target="#b31">[32]</ref> studied the dynamics of social influence by carefully designing differential equations extended from the classic 'Susceptible-Infected' (SI) model; Most recently, Li et al. <ref type="bibr" target="#b25">[26]</ref> proposed an end-toend predictor for inferring cascade size by incorporating recurrent neural network (RNN) and representation learning. All these approaches mainly aim to predict the global or aggregated patterns of social influence such as the cascade size within a time-frame. However, in many online applications such as advertising and recommendation, it is critical to effectively predict the social influence for each individual, i.e., user-level social influence prediction.</p><p>In this paper, we focus on the prediction of user-level social influence. We aim to predict the action status of a user given the action statuses of her near neighbors and her local structural information. For example, in Figure <ref type="figure">1</ref>, for the central user v, if some of her friends (black circles) bought a product, will she buy the same product in the future? The problem mentioned above is prevalent in real-world applications whereas its complexity and non-linearity have frequently been observed, such as the "S-shaped" curve in <ref type="bibr" target="#b1">[2]</ref> v Figure <ref type="figure">1</ref>: A motivating example of social influence locality prediction. The goal is to predict v's action status, given 1) the observed action statuses (black and gray circles are used to indicate "active" and "inactive", respectively) of her near neighbors and 2) the local network she is embedded in. and the celebrated "structural diversity" in <ref type="bibr" target="#b45">[46]</ref>. The above observations inspire a lot of user-level influence prediction models, most of which <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b53">54]</ref> consider complicated hand-crafted features, which require extensive knowledge of specific domains and are usually difficult to generalize to different domains.</p><p>Inspired by the recent success of neural networks in representation learning, we design an end-to-end approach to discover hidden and predictive signals in social influence automatically. By architecting network embedding <ref type="bibr" target="#b36">[37]</ref>, graph convolution <ref type="bibr" target="#b24">[25]</ref>, and graph attention mechanism <ref type="bibr" target="#b48">[49]</ref> into a unified framework, we expect that the end-to-end model can achieve better performance than conventional methods with feature engineering. In specific, we propose a deep learning based framework, DeepInf, to represent both influence dynamics and network structures into a latent space. To predict the action status of a user v, we first sample her local neighbors through random walks with restart. After obtaining a local network as shown in Figure <ref type="figure">1</ref>, we leverage both graph convolution and attention techniques to learn latent predictive signals.</p><p>We demonstrate the effectiveness and efficiency of our proposed framework on four social and information networks from different domains-Open Academic Graph (OAG), Digg, Twitter, and Weibo. We compare DeepInf with several conventional methods such as linear models with hand-crafted features <ref type="bibr" target="#b53">[54]</ref> as well as the state-ofthe-art graph classification model <ref type="bibr" target="#b33">[34]</ref>. Experimental results suggest that the DeepInf model can significantly improve the prediction performance, demonstrating the promise of representation learning for social and information network mining tasks.</p><p>Organization The rest of this paper is organized as follows: Section 2 formulates social influence prediction problem. Section 3 introduces the proposed framework in detail. In Section 4 and 5, we conduct extensive experiments and case studies. Finally, Section 6 summarizes related work and Section 7 concludes this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PROBLEM FORMULATION</head><p>In this section, we introduce necessary definitions and then formulate the problem of predicting social influence. Definition 2.1. r -neighbors and r -ego network Let G = (V , E) be a static social network, where V denotes the set of users and E ⊆ V × V denotes the set of relationships<ref type="foot" target="#foot_0">2</ref> . For a user v, its rneighbors are defined as Γ r v = {u : d(u, v) ≤ r } where d(u, v) is the shortest path distance (in terms of the number of hops) between u and v in the network G. The r -ego network of user v is the subnetwork induced by Γ r v , denoted by G r v .</p><p>Definition 2.2. Social Action Users in social networks perform social actions, such as retweet. At each timestamp t, we observe a binary action status of user u, s t u ∈ {0, 1}, where s t u = 1 indicates user u has performed this action before or on the timestamp t, and s t u = 0 indicates that the user has not performed this action yet. Such an action log can be available from many social networks, e.g., the "retweet" action in Twitter and the citation action in academic social networks.</p><p>Given the above definitions, we introduce social influence locality, which amounts to a kind of closed world assumption: users' social decisions and actions are influenced only by their near neighbors within the network, while external sources are assumed to be not present.</p><p>Problem 1. Social Influence Locality <ref type="bibr" target="#b52">[53]</ref> Social influence locality models the probability of v's action status conditioned on her r -ego network G r v and the action states of her r -neighbors. More formally, given G r v and S t v = {s t u : u ∈ Γ r v \ {v}}, social influence locality aims to quantify the activation probability of v after a given time interval ∆t:</p><formula xml:id="formula_0">P s t +∆t v G r v , S t v .</formula><p>Practically, suppose we have N instances, each instance is a 3-tuple (v, a, t), where v is a user, a is a social action and t is a timestamp. For such a 3-tuple (v, a, t), we also know v's r -ego network-G r v , the action statuses of v's r -neighbors-S t v , and v's future action status at t + ∆t, i.e., s t +∆t v . We then formulate social influence prediction as a binary graph classification problem which can be solved by minimizing the following negative log likelihood objective w.r.t model parameters Θ:</p><formula xml:id="formula_1">L(Θ) = − N i =1 log P Θ s t i +∆t v i G r v i , S t i v i .<label>(1)</label></formula><p>Especially, in this work, we assume ∆t is sufficiently large, that is, we want to predict the action status of the ego user v at the end of our observation window.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MODEL FRAMEWORK</head><p>In this section, we formally propose DeepInf, a deep learning based model, to parameterize the probability in Eq. 1 and automatically detect the mechanisms and dynamics of social influence. The framework firstly samples a fixed-size sub-network as the proxy for each r -ego network (see Section 3.1). The sampled sub-networks are then fed into a deep neural network with mini-batch learning (see Section 3.2). Finally, the model output is compared with ground truth to minimize the negative log-likelihood loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sampling Near Neighbors</head><p>Given a user v, a straightforward way to extract her r -ego network G r v is to perform Breadth-First-Search (BFS) starting from user v.  <ref type="table" target="#tab_3">2</ref> for example). (e) A GCN or GAT layer. a vv and a vu indicate the attention coefficients along self-loop (v, v) and edge (v, u), respectively; The value of these attention coefficients can be chosen between Eq. 5 and Eq. 7 according to the choice between GCN and GAT. (f) and (g) Compare model output and ground truth, we get the negative log likelihood loss. In this example, ego user v was finally activated (marked as black).</p><p>However, for different users, G r v 's may have different sizes. Meanwhile, the size (regarding the number of vertices) of G r v 's can be very large due to the small-world property in social networks <ref type="bibr" target="#b49">[50]</ref>. Such variously sized data is unsuited to most deep learning models. To remedy these issues, we sample a fixed-size sub-network from v's r -ego network, instead of directly dealing with the r -ego network.</p><p>A natural choice of the sampling method is to perform random walk with restart (RWR) <ref type="bibr" target="#b44">[45]</ref>. Inspired by <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b45">46]</ref> which suggest that people are more likely to be influenced by active neighbors than inactive ones, we start random walks from either the ego user v or one of her active neighbors randomly. Next, the random walk iteratively travels to its neighborhood with the probability that is proportional to the weight of each edge. Besides, at each step, the walk is assigned a probability to return to the starting node, that is, either the ego user v or one of v's active neighbors. The RWR runs until it successfully collects a fixed number of vertices, denoted by Γr v with Γr v = n. We then regard the sub-network Ḡr v induced by Γr v as a proxy of the r -ego network G r v , and denote St v = {s t u : u ∈ Γr v \ {v}} to be the action statuses of v's sampled neighbors. Therefore, we re-define the optimization objective in Eq. 1 to be:</p><formula xml:id="formula_2">L(Θ) = − N i =1 log P Θ s t i +∆t v i Ḡr v i , S t i v i . (<label>2</label></formula><formula xml:id="formula_3">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Neural Network Model</head><p>With the retrieved Ḡr v and St v for each user, we design an effective neural network model to incorporate both the structural properties in Ḡr v and action statuses in St v . The output of the neural network model is a hidden representation for the ego user v, which is then used to predict her action status-s t +∆t v . As shown in Figure <ref type="figure">2</ref>, the proposed neural network model consists of a network embedding layer, an instance normalization layer, an input layer, several graph convolutional or graph attention layers, and an output layer. In this section, we introduce these layers one by one and build the model step by step.</p><p>Embedding Layer With the recent emergence of representation learning <ref type="bibr" target="#b4">[5]</ref>, the network embedding technique has been extensively studied to discover and encode network structural properties into a low-dimensional latent space. More formally, network embedding learns an embedding matrix X ∈ R D× |V | , with each column corresponding to the representation of a vertex (user) in the network G. In the proposed model, we use a pre-trained embedding layer which maps a user u to her D-dimensional representation x u ∈ R D , as shown in Figure <ref type="figure">2(b)</ref>.</p><p>Instance Normalization <ref type="bibr" target="#b46">[47]</ref> Instance normalization is a recently proposed technique in image style transfer <ref type="bibr" target="#b46">[47]</ref>. We adopt this technique in our social influence prediction task. As shown in Figure <ref type="figure">2</ref>(c), for each user u ∈ Γr v , after retrieving her representation x u from the embedding layer, the instance normalization y u is given by</p><formula xml:id="formula_4">y ud = x ud − µ d σ 2 d + ϵ<label>(3)</label></formula><p>for each embedding dimension d = 1, • • • , D, where</p><formula xml:id="formula_5">µ d = 1 n u ∈ Γr v x ud , σ 2 d = 1 n u ∈ Γr v (x ud − µ d ) 2<label>(4)</label></formula><p>Here µ d and σ d are the mean and variance, and ϵ is a small number for numerical stability. Intuitively, such normalization can remove instance-specific mean and variance, which encourages the downstream model to focus on users' relative positions in latent embedding space rather than their absolute positions. As we will see later in Section 5, instance normalization can help avoid overfitting during training.</p><p>Input Layer As illustrated in Figure <ref type="figure">2</ref>(d), the input layer constructs a feature vector for each user. Besides the normalized lowdimensional embedding comes from up-stream instance normalization layer, it also considers two binary variables. The first variable indicates users' action statuses, and the other indicates whether the user is the ego user. Also, the input layer covers all other customized vertex features such as structural features, content features, and demographic features.</p><p>GCN <ref type="bibr" target="#b24">[25]</ref> Based Network Encoding Graph Convolutional Network (GCN) is a semi-supervised learning algorithm for graphstructured data. The GCN model is built by stacking multiple GCN layers. The input to each GCN layer is a vertex feature matrix, H ∈ R n×F , where n is the number of vertices, and F is the number of features. Each row of H , denoted by h ⊤ i , is associated with a vertex. Generally speaking, the essence of the GCN layer is a nonlinear transformation that outputs H ′ ∈ R n×F ′ as follows:</p><formula xml:id="formula_6">H ′ = GCN(H ) = д A(G)H W ⊤ + b ,<label>(5)</label></formula><p>where</p><formula xml:id="formula_7">W ∈ R F ′ ×F , b ∈ R F ′ are model parameters, д is a non-linear activation function, A(G) is a n × n matrix that captures structural information of graph G. GCN instantiates A(G)</formula><p>to be a static matrix closely related to the normalized graph Laplaican <ref type="bibr" target="#b9">[10]</ref>:</p><formula xml:id="formula_8">A GCN (G) = D −1/2 AD −1/2 , (<label>6</label></formula><formula xml:id="formula_9">)</formula><p>where A is the adjacency matrix 3 of G, and D = diag(A1) is the degree matrix.</p><p>Multi-head Graph Attention <ref type="bibr" target="#b48">[49]</ref> Graph Attention (GAT) is a recent proposed technique that introduces the attention mechanism into GCN. GAT defines matrix A GAT (G) = [a i j ] n×n through a selfattention mechanism. More formally, an attention coefficient e i j is firstly computed by an attention function attn : R F ′ × R F ′ → R, which measures the importance of vertex j to vertex i:</p><formula xml:id="formula_10">e i j = attn W h i , W h j .</formula><p>Different from traditional self-attention mechanisms where the attention coefficients between all pairs of instances will be computed, GAT only evaluates e i j for (i, j) ∈ E( Ḡr v ) or i = j, i.e., (i, j) is either an edge or a self-loop. In doing so, it is able to better leverage and capture the graph structural information. After that, to make coefficients comparable among vertices, a softmax function is adopted to normalize attention coefficients:</p><formula xml:id="formula_11">a i j = softmax(e i j ) = exp (e i j ) k ∈ Γ1 i exp (e ik )</formula><p>.</p><p>Following Velickovic et al. <ref type="bibr" target="#b48">[49]</ref>, the attention function is instantiated with a dot product and a LeakyReLU <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b50">51]</ref> nonlinearity. For an edge or a self-loop (i, j), the dot product is performed between parameter c and the concatenation of the feature vectors of the two end points-W h i and W h j , i.e., e i j = LeakyReLU c ⊤ W h i ||W h j , where the LeakyReLU has negative 3 GCN applies self-loop trick on graph G by adding self-loop on each vertex, i.e., A ← A + I </p><formula xml:id="formula_12">a i j = exp LeakyReLU c ⊤ W h i | |W h j k ∈ Γ1 i exp (LeakyReLU (c ⊤ [W h i | |W h k ])) ,<label>(7)</label></formula><p>where || denotes the vector concatenation operation.</p><p>Once obtained the normalized attention coefficients, i.e., a i j 's, we can plugin A GAT (G) = [a i j ] n×n into Eq. 5. This completes the definition of a single-head graph attention. In addition, we apply multi-head graph attention as suggested by Velickovic et al. <ref type="bibr" target="#b48">[49]</ref> and Vaswani et al. <ref type="bibr" target="#b47">[48]</ref>. The multi-head attention mechanism performs K independent single attention in parallel, i.e., we have</p><formula xml:id="formula_13">K inde- pendent parameters W 1 , • • • ,W K and attention matrix A 1 , • • • , A K .</formula><p>Multi-head attention aggregate the output of K single attention together through an aggregation function:</p><formula xml:id="formula_14">H ′ = д Aggregate A 1 (G)H W ⊤ 1 , • • • , A K (G)H W ⊤ K + b . (8)</formula><p>We concatenate the outputs of each single-head attention to aggregate them except an average operator for the last layer.</p><p>Output Layer and Loss Function This layer (see Figure <ref type="figure">2</ref>(f)) outputs a two-dimension representation for each user, we compare the representation of the ego user with ground truth, and then optimize the log-likelihood loss as described in Eq. 2.</p><p>Mini-batch Learning When sampling from r -ego network, we force the sampled sub-networks to have a fixed size n. Benefiting from such homogeneity, we can apply mini-batch learning here for efficient training. As shown in Figure <ref type="figure">2</ref>(a), in each iteration, we first randomly sample B instances to be a mini-batch. Then we optimize our model w.r.t. the sampled mini-batch. Such method runs much faster than full-batch learning and still introduces enough noise during optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENT SETUP</head><p>We set up our experiments with large-scale real-world datasets to quantitatively evaluate the proposed DeepInf framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>Our experiments are conducted on four social networks from different domains --OAG, Digg, Twitter, and Weibo. Table <ref type="table" target="#tab_1">1</ref> lists statistics of the four datasets.</p><p>OAG<ref type="foot" target="#foot_1">4</ref> OAG (Open Academic Graph) dataset is generated by linking two large academic graphs: Microsoft Academic Graph <ref type="bibr" target="#b14">[15]</ref> and AMiner <ref type="bibr" target="#b43">[44]</ref>. Similar to the treatment in <ref type="bibr" target="#b12">[13]</ref>, we choose 20 popular conferences from data mining, information retrieval, machine learning, natural language processing, computer vision, and database research communities 5 . The social network is defined to be the co-author network, and the social action is defined to be citation behaviors -a researcher cites a paper from the above conferences. We are interested in how one's citation behaviors are influenced by her collaborators.</p><p>Digg <ref type="bibr" target="#b22">[23]</ref> Digg is a news aggregator which allows people to vote web content, a.k.a, story, up or down. The dataset contains data about stories promoted to Digg's front page over a period of a month in 2009. For each story, it contains the list of all Digg users who have voted for the story up to the time of data collection and the time stamp of each vote. The voters' friendship links are also retrieved.</p><p>Twitter <ref type="bibr" target="#b11">[12]</ref> The Twitter dataset was built after monitoring the spreading processes on Twitter before, during and after the announcement of the discovery of a new particle with the features of the elusive Higgs boson on Jul. 4th, 2012. The social network is defined to be the Twitter friendship network, and the social action is defined to be whether a user retweets "Higgs" related tweets.</p><p>Weibo <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b53">54]</ref> Weibo 6 is the most popular Chinese microblogging service. The dataset is from <ref type="bibr" target="#b52">[53]</ref> and can be downloaded here. 7 The complete dataset contains the directed following networks and tweets (posting logs) of 1,776,950 users between Sep. 28th, 2012 and Oct. 29th, 2012. The social action is defined as retweeting behaviors in Weibo -a user forwards (retweets) a post (tweet).</p><p>Data Preparation We process the above four datasets following the practice in existing work <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b53">54]</ref>. More concretely, for a user v who was influenced to perform a social action a at some timestamp t, we generate a positive instance. Next, for each neighbor of the influenced user v, if she was never observed to be active in our observation window, we create a negative instance. Our target is to distinguish positive instances from negative ones. However, the achieved datasets are facing data imbalance problems in two respects. The first comes from the number of active neighbors. As observed by Zhang et al. <ref type="bibr" target="#b53">[54]</ref>, structural features become significantly correlated with social influence locality when the ego user has a relatively large number of active neighbors. However, the number of active neighbors is imbalanced in most social influence data sets. For example, in Weibo, around 80% instances only have one active neighbor and the instances with the number of active neighbors ≥ 3 only occupies 8.57%. Therefore, when we train our model on such imbalanced datasets, the model will be dominated by observations with few active neighbors. To deal with the imbalance issue and show the superiority of our model in capturing local structural information, we filter out observations with few active neighbors. Especially, in each data set, we only consider instances where ego users have ≥ 3 active neighbors. The second problem comes from label imbalance. For example, in the Weibo dataset, the ratio between negative instances and positive instances is about  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Name Description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vertex</head><p>Coreness <ref type="bibr" target="#b3">[4]</ref>. Pagerank <ref type="bibr" target="#b34">[35]</ref>.</p><p>Hub score and authority score <ref type="bibr" target="#b8">[9]</ref>.</p><p>Eigenvector Centrality <ref type="bibr" target="#b5">[6]</ref>.</p><p>Clustering Coefficient <ref type="bibr" target="#b49">[50]</ref>. Rarity (reciprocal of ego user's degree) <ref type="bibr" target="#b0">[1]</ref>. Embedding Pre-trained network embedding (DeepWalk <ref type="bibr" target="#b35">[36]</ref>, 64-dim).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ego</head><p>The number/ratio of active neighbors <ref type="bibr" target="#b1">[2]</ref>. Density of subgnetwork induced by active neighbors <ref type="bibr" target="#b45">[46]</ref>. #Connected components formed by active neighbors <ref type="bibr" target="#b45">[46]</ref>. 300:1. To address this issue, we sample a more balanced dataset with the ratio between negative and positive to be 3:1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Metrics</head><p>To evaluate our framework quantitatively, we use the following performance metrics: Prediction Performance We evaluate the predictive performance of DeepInf in terms of Area Under Curve (AUC) <ref type="bibr" target="#b7">[8]</ref>, Precision (Prec.), Recall (Rec.), and F1-Measure (F1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameter Sensitivity We analyze several hyper-parameters in our model and test how different hyper-parameter choices can influence prediction performance.</head><p>Case Study We use case studies to further demonstrate and explain the effectiveness of our proposed framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison Methods</head><p>We compare DeepInf with several baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Logistic Regression (LR)</head><p>We use logistic regression (LR) to train a classification model. The model considers three categories of features: (1) vertex features for the ego-user; (2) pre-trained network embedding (DeepWalk <ref type="bibr" target="#b35">[36]</ref>) for ego-user; (3) hand-crafted egonetwork features. The features we used are listed in Table <ref type="table" target="#tab_3">2</ref>.</p><p>Support Vector Machine (SVM) <ref type="bibr" target="#b16">[17]</ref> We also use support vector machine (SVM) with linear kernel as the classification model. The model use the same features as logistic regression (LR).</p><p>PSCN <ref type="bibr" target="#b33">[34]</ref> As we model social influence locality prediction as a graph classification problem, we compare our framework with the state-of-the-art graph classification models, PSCN <ref type="bibr" target="#b33">[34]</ref>. For each graph, PSCN selects w vertices according to a user-defined ranking function, e.g., degree and betweenness centrality. Then for each selected vertex, it assembles its top k near neighbors according to breadth-first search order. For each graph, The above process constructs a vertex sequence of length w ×k with F channels, where F is the number of features for each vertex. Finally, PSCN applies 1-dimensional convolutional layers on it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DeepInf and its Variants</head><p>We implement two variants of DeepInf, denoted by DeepInf-GCN and DeepInf-GAT, respectively. DeepInf-GCN uses graph convolutional layer as building blocks of our framework, i.e., setting A(G) = D −1/2 AD −1/2 in Eq. 5. DeepInf-GAT uses graph attention as shown in Eq. 7. However, both DeepInf and PSCN accept vertex-level features only. Due to this limitation, we do not use the ego-network features in these two models. Instead, we expect that DeepInf can discover the ego-network features and other predictive signals automatically.</p><p>Hyper-parameter Setting &amp; Implementation Details As for our framework, DeepInf, we first perform random walk with a restart probability 0.8, and the size of sampled sub-network is set to be 50. For the embedding layer, a 64-dimension network embedding is pre-trained using DeepWalk <ref type="bibr" target="#b35">[36]</ref>. Then we choose to use a three-layer GCN or GAT structure for DeepInf, both the first and second GCN/GAT layers contain 128 hidden units, while the third layer (output layer) contains 2 hidden units for binary prediction. Especially, for DeepInf with multi-head graph attention, both the first and second layer consists of K = 8 attention heads with each computing 16 hidden units (for a total of 8 × 16 = 128 hidden units). For detailed model configuration, we adopt exponential linear units (ELU) <ref type="bibr" target="#b10">[11]</ref> as nonlinearity (function д in Eq. 5). All the parameters are initialized with Glorot initialization <ref type="bibr" target="#b17">[18]</ref> and trained using the Adagrad <ref type="bibr" target="#b15">[16]</ref> optimizer with learning rate 0.1 (0.05 for Digg dataset), weight decay 5e −4 (1e −3 for Digg dataset), and dropout rate 0.2. We use 75%, 12.5%, 12.5% instances for training, validation and test, respectively; the mini-batch size is set to be 1024 across all data sets.</p><p>As for PSCN, in our experiments, we find that the recommended betweenness centrality ranking function does not work well in predicting social influence. We turn to use breadth-first search order starting from the ego user as the ranking function. When BFS order is not unique, we break ties by ranking active users first. We select w = 16 and k = 5 by validation and then apply two 1-dimensional convolutional layers. The first conv layer has 16 output channels, a stride of 5, and a kernel size of 5. The second conv layer has 8 output channels, a stride of 1, and a kernel size of 1. The outputs of the second layer are then fed into a fully-connected layer to predict labels.</p><p>Finally, we allow PSCN and DeepInf to run at most 500 epochs over the training data, and the best model was selected by early stopping using loss on the validation sets. We release the code for PSCN and DeepInf used in this work at https://github.com/xptree/ DeepInf, both implemented with PyTorch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTAL RESULTS</head><p>We compare the prediction performance of all methods across the four datasets in Table <ref type="table" target="#tab_4">3</ref> and list the relative performance gain in Table <ref type="table" target="#tab_5">4</ref>, where the gain is over the closest baseline. In addition, we compare the variants of DeepInf and list the results in Table <ref type="table" target="#tab_6">5</ref>. We have several interesting observations and insights.</p><p>(1) As shown in Figure <ref type="figure" target="#fig_3">3</ref>, DeepInf-GAT achieves significantly better performance over baselines in terms of both AUC and F1, demonstrating the effectiveness of our proposed framework. In OAG and Digg, DeepInf-GAT discovers the hidden mechanism   and dynamics of social influence locality, giving us 3.8% relative performance gain w.r.t. AUC.</p><p>(2) For PSCN, it selects a subset of vertices according to a userdefined ranking function. As mentioned in Section 4, instead of using betweenness centrality, we propose to use BFS order-based ranking function. Such ranking function can be regarded as a predefined graph attention mechanism where the ego user pays much more attention to her active neighbors. PSCN outperform linear predictors such as LR and SVM but does not perform as well as DeepInf-GAT.</p><p>(3) An interesting observation is the inferiority of DeepInf-GCN, as shown in Table <ref type="table" target="#tab_6">5</ref>. Previously, we have seen the success of GCN in may label classification tasks <ref type="bibr" target="#b24">[25]</ref>. However, in this application, DeepInf-GCN achieves the worst performance over all the methods. We attribute its inferiority to the homophily assumption of GCN-similar vertices are more likely to link with each other than dissimilar ones. Under such assumption, for a specific vertex, GCN computes its hidden representation by taking an unweighted average over its neighbors' representations. However, in our application, the homophily assumption may not be true. By averaging over neighbors, GCN may mix predictive signals with noise. On the other hand, as pointed out by <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b45">46]</ref>, active neighbors are more important than inactive neighbors, which also encourages us to use graph attention which treats neighbors differently.</p><p>In experiments shown in Table <ref type="table" target="#tab_4">3</ref>, 4, and 5, we still rely on several vertex features, such as page rank score and clustering coefficient. However, we want to avoid using any hand-crafted features and make DeepInf a "pure" end-to-end learning framework. Quite surprisingly, we can still achieve comparable performance (as shown in Table <ref type="table" target="#tab_7">6</ref>), even we do not consider any hand-crafted features except the pre-trained network embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Parameter Analysis</head><p>In this section, we investigate how the prediction performance varies with the hyper-parameters in sampling near neighbors and the neural network model. We conduct the parameter analyses on the dataset unless otherwise stated.</p><p>Return Probability of Random Walk with Restart When sampling near neighbors, the return probability of random walk with restart (RWR) controls the "shape" of the sampled r -ego network. Figure <ref type="figure" target="#fig_3">3</ref>(a) shows the prediction performance (in terms of AUC and F1) by varying the return probability from 10% to 90%. As the increasing of return probability, the prediction performance also increases slightly, illustrating the locality pattern of social influence.</p><p>Size of Sampled Networks Another parameter that controls the sampled r -ego network is the size of sampled networks. Figure <ref type="figure" target="#fig_3">3(b)</ref> shows the prediction performance (in terms of AUC and F1) by varying the size from 10 to 100. We can observe a slow increase of prediction performance when we sample more near neighbors. This is not surprising because we have more information as the size of sampled networks increases.</p><p>Negative Positive Ratio As we mentioned in Section. 5, the positive and negative observations are imbalanced in our datasets. To investigate how such imbalance influence the prediction performance, we vary the ratio between negative and positive instances from 1 to 10 , and show the performance in Figure <ref type="figure" target="#fig_3">3(c)</ref>. We can observe a decreasing trend w.r.t. the F1 measure, while the AUC score stays stable.</p><p>#Head for Multi-head Attention Another hyper-parameter we analyze is the number of heads used for multi-head attention. For a fair comparison, we fixed the number of total hidden units to be 128. We vary the number of heads to be 1, 2, 4, 8, 16, 32, 64, 128, i.e., each head has 128, 64, 32, 16, 8, 4, 2, 1 hidden units, respectively. As shown in Figure <ref type="figure" target="#fig_3">3</ref>(d), we can see that DeepInf benefits from the multihead mechanism. However, as the decreasing of the number of hidden units associated with each head, the prediction performance decreases. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Instance Normalization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Discussion on GAT and Case Study</head><p>Besides the concatenation-based attention used in GAT (Eq. 7), we also try other popular attention mechanisms, e.g., the dot product attention or the bilinear attention as summarized in <ref type="bibr" target="#b27">[28]</ref>. However, those attention mechanisms do not perform as well as the concatenation-based one. In this section, we introduce the orderpreserving property of GAT <ref type="bibr" target="#b48">[49]</ref>. Based on the property, we attempt to explain the effectiveness of DeepInf-GAT through case studies.</p><p>Observation 1. Order-preserving of Graph Attention Suppose (i, j), (i, k), (i ′ , j) and (i ′ , k) are either edges or self-loops, and a i j , a ik , a i ′ j , a i ′ k are the attention coefficients associated with them. If a i j &gt; a ik then a i ′ j &gt; a i ′ k .</p><p>Proof. As introduced in Eq. 7, the graph attention coefficient for edge (or self-loop) (i, j) is defined as a i j = softmax(e i j ), where</p><formula xml:id="formula_16">e i j = LeakyReLU c ⊤ W h i | |W h j .</formula><p>If we rewrite c ⊤ = p ⊤ q ⊤ , we have</p><formula xml:id="formula_17">e i j = LeakyReLU p ⊤ W h i + q ⊤ W h j .</formula><p>Due to the strict monotonicity of softmax and LeakyReLU, a i j &gt; a ik implies q ⊤ W h j &gt; q ⊤ W h k . Apply the strict monotonicity of LeakyReLU and softmax again, we get a i ′ j &gt; a i ′ k . □</p><p>The above observation shows the following fact-although each vertex only pay attention to its neighbors in GAT (local attention), the attention coefficients have a global ranking, which is determined by q ⊤ W h j only. Thus we can define a score function score(j) = q ⊤ W h j . Then each vertex pays attention to its neighbors according to this score function-a higher score function value indicates a higher attention coefficient. Thus, plotting the value of the scoring function can illustrate where are the "popular areas" or "important areas" of the network. Furthermore, multi-head attention provides a multi-view mechanism-for K heads, we have K score functions, score k (j) = q ⊤ k W k h j , k = 1, • • • , K, highlighting different areas of the network. To better illustrate this mechanism, we perform a few case studies. As shown in Figure <ref type="figure">5</ref>, we choose four instances    from the Digg dataset (each row corresponding to one instance) and select three representative attention heads from the first GAT layer. Quite interestingly, we can observe explainable and heterogeneous patterns discovered by different attention heads. For example, as shown in Figure <ref type="figure">5</ref>, the first attention head tend to focus on the ego-user, while the second and the third highlight active users and inactive users, respectively. However, this property does not hold for other attention mechanisms. Due to the page limit, we do not discuss them here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORK</head><p>Our study is closely related to a large body of literature on social influence analysis <ref type="bibr" target="#b41">[42]</ref> and graph representation learning <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b36">37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Social Influence Analysis</head><p>Most existing work has focused on social influence modeled as a macro-social process (a.k.a., cascade), with a few that have explored the alternative user-level mechanism that considers the locality of social influence in practice. At the macro level, researchers are interested in global patterns of social influence. Such global patterns includes various respects of a cascade and their correlation with the final cascade size, e.g., the rise-and-fall patterns <ref type="bibr" target="#b31">[32]</ref>, external influence sources <ref type="bibr" target="#b32">[33]</ref>, and conformity phenomenon <ref type="bibr" target="#b42">[43]</ref>. Recently, there have been efforts to detect those global patterns automatically using deep learning, e.g., the DeepCas model <ref type="bibr" target="#b25">[26]</ref> which formulate cascade prediction as a sequence problem and solve it with Recurrent Neural Network. Another line of studies focuses on the user-level mechanism in social influence where each user is only influenced by her near neighbors. Examples of such work include pairwise influence <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b38">39]</ref>, topic-level influence <ref type="bibr" target="#b41">[42]</ref>, group formation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b37">38]</ref> and structural diversity <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b45">46]</ref>. Such user-level models act as fundamental building blocks of many real-world problems and applications. For example, in the influence maximization problem <ref type="bibr" target="#b23">[24]</ref>, both independent cascade and linear threshold models assume a pairwise influence model; In social recommendation <ref type="bibr" target="#b29">[30]</ref>, a key assumption is social influence-the ratings and reviews of existing users will influence future customers' decisions through social interaction.</p><formula xml:id="formula_18">v v v v v v v v v v v v v v v v</formula><p>Another example is a large-scale field experiment by Facebook Bond et al. <ref type="bibr" target="#b6">[7]</ref> during the 2010 US congressional elections, the results showed how online social influence changes offline voting behavior.</p><p>Graph Representation Learning Representation learning <ref type="bibr" target="#b4">[5]</ref> has been a hot topic in research communities. In the context of graph mining, there have been many efforts to graph representation learning. One line of studies focus on vertex (node) embedding, i.e., to learn a low-dimensional latent factors for each vertex. Examples include DeepWalk <ref type="bibr" target="#b35">[36]</ref>, LINE <ref type="bibr" target="#b40">[41]</ref>, node2vec <ref type="bibr" target="#b19">[20]</ref>, metap-ath2vec <ref type="bibr" target="#b12">[13]</ref>, NetMF <ref type="bibr" target="#b36">[37]</ref>, etc. Another line of studies pay attention to representation of graphs, i.e., to learn latent representations of sub-structures for graphs, including, graph kernel <ref type="bibr" target="#b39">[40]</ref>, deep graph kernel <ref type="bibr" target="#b51">[52]</ref>, and state-of-the-art method PSCN <ref type="bibr" target="#b33">[34]</ref>. Recently, there have been several attempts to incorporate semi-supervised information into graph representation learning. Typical examples include GCN <ref type="bibr" target="#b24">[25]</ref>, GraphSAGE <ref type="bibr" target="#b20">[21]</ref>, and the state-of-the-art model GAT <ref type="bibr" target="#b48">[49]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this work, we study the social influence locality problem. We formulate this problem from a deep learning perspective and propose a graph-based learning framework DeepInf by incorporating the recently developed network embedding, graph convolution, and self-attention techniques. We test the proposed framework on four social and information networks-OAG, Digg, Twitter, and Weibo. Our extensive experimental analysis shows DeepInf significantly outperforms baselines with rich hand-craft features in predicting social influence locality. This work explores the potential of network representation learning in social influence analysis and gives the very first attempt to explain the dynamics of social influence.</p><p>The general idea behind the proposed DeepInf can be extended to many network mining tasks. Our DeepInf can effectively and efficiently summarize a local area in a network. Such summarized representations can then be fed into various down-stream applications, such as link prediction, similarity search, network alignment, etc. Therefore, we would like to explore this promising direction for future work. Another exciting direction is the sampling of near neighbors. In this work, we perform random walk with restart without considering any side information. Meanwhile, the sampling procedure is loosely coupled with the neural network model. It is also exciting to combine both sampling and learning together by leveraging reinforcement learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>As claimed in Section 3, we use an Instance Normalization (IN) layer to avoid overfitting, especially when training set is small, e.g., Digg. Figure 4(a) and Figure 4(b) illustrate the training loss and test AUC of DeepInf-GAT on the Digg dataset trained with and without IN layer. We can see that IN significantly avoids overfitting and makes the training process more robust.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Parameter analysis. (a) Return probability of random walk with restart. (b) Size of sampled networks. (c) Negative positive ratio. (d) The number of heads used for multi-head attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The (a) training loss/(b) test AUC of DeepInf-GAT on Digg data set trained with and without Instance Normalization, vs. the number of epochs. Instance Normalization helps avoid overfitting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure</head><label></label><figDesc>Figure Case study. How different graph attention heads highlight different areas of the network. (a) Four selected cases from the Digg dataset. Active and inactive users are marked as black and gray, respectively. User v is the egouser that we are interested in. (b)(c)(d) Three representative attention heads.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Summary of datasets. |V | and |E| indicates the number of vertices and edges in graph G = (V , E), while N is the number of social influence locality instances (observations) as described in Section 2.</figDesc><table><row><cell></cell><cell>OAG</cell><cell>Digg</cell><cell>Twitter</cell><cell>Weibo</cell></row><row><cell>|V |</cell><cell>953,675</cell><cell>279,630</cell><cell>456,626</cell><cell>1,776,950</cell></row><row><cell cols="5">|E | 4,151,463 1,548,126 12,508,413 308,489,739</cell></row><row><cell>N</cell><cell>499,848</cell><cell>24,428</cell><cell>499,160</cell><cell>779,164</cell></row><row><cell cols="5">slop 0.2. To sum up, the normalized attention coefficients can be</cell></row><row><cell>expressed as:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>List of features used in this work.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Prediction performance of different methods on the four datasets (%).</figDesc><table><row><cell>Data</cell><cell>Model</cell><cell>AUC Prec. Rec.</cell><cell>F1</cell></row><row><cell></cell><cell>LR</cell><cell cols="2">65.55 32.26 69.97 44.16</cell></row><row><cell></cell><cell>SVM</cell><cell cols="2">65.48 32.17 69.82 44.04</cell></row><row><cell>OAG</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>PSCN</cell><cell cols="2">69.16 36.45 64.64 46.61</cell></row><row><cell></cell><cell cols="3">DeepInf-GAT 71.79 40.77 60.97 48.86</cell></row><row><cell></cell><cell>LR</cell><cell cols="2">84.72 56.78 73.12 63.92</cell></row><row><cell></cell><cell>SVM</cell><cell cols="2">86.01 63.42 67.34 65.32</cell></row><row><cell>Digg</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>PSCN</cell><cell cols="2">87.37 64.75 68.15 66.40</cell></row><row><cell></cell><cell cols="3">DeepInf-GAT 90.65 66.82 78.49 72.19</cell></row><row><cell></cell><cell>LR</cell><cell cols="2">78.07 45.86 69.81 55.36</cell></row><row><cell>Twitter</cell><cell>SVM</cell><cell cols="2">79.42 49.12 67.31 56.79</cell></row><row><cell></cell><cell>PSCN</cell><cell cols="2">78.74 47.36 67.29 55.59</cell></row><row><cell></cell><cell cols="3">DeepInf-GAT 80.22 48.41 69.08 56.93</cell></row><row><cell></cell><cell>LR</cell><cell cols="2">77.10 42.34 72.88 53.56</cell></row><row><cell></cell><cell>SVM</cell><cell cols="2">77.11 43.27 70.79 53.71</cell></row><row><cell>Weibo</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>PSCN</cell><cell cols="2">81.31 47.72 71.53 57.24</cell></row><row><cell></cell><cell cols="3">DeepInf-GAT 82.72 48.53 76.09 59.27</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Relative gain of DeepInf-GAT in terms of AUC against the best baseline.</figDesc><table><row><cell>Method</cell><cell cols="4">OAG Digg Twitter Weibo</cell></row><row><cell>LR</cell><cell cols="2">65.66 84.72</cell><cell>78.07</cell><cell>77.10</cell></row><row><cell>SVM</cell><cell cols="2">65.48 86.01</cell><cell>79.42</cell><cell>77.11</cell></row><row><cell>PSCN</cell><cell cols="2">69.16 87.37</cell><cell>78.74</cell><cell>81.31</cell></row><row><cell cols="3">DeepInf-GAT 71.79 90.65</cell><cell>80.22</cell><cell>82.72</cell></row><row><cell cols="2">Relative Gain 3.8%</cell><cell>3.8%</cell><cell>1.0%</cell><cell>1.7%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Prediction performance of variants of DeepInf (%).</figDesc><table><row><cell>Data</cell><cell>Model</cell><cell>AUC Prec. Rec.</cell><cell>F1</cell></row><row><cell>OAG</cell><cell cols="3">DeepInf-GCN 63.55 30.28 74.36 43.03 DeepInf-GAT 71.79 40.77 60.97 48.86</cell></row><row><cell></cell><cell cols="3">DeepInf-GCN 84.15 58.76 67.61 62.88</cell></row><row><cell>Digg</cell><cell cols="3">DeepInf-GAT 90.65 66.82 78.49 72.19</cell></row><row><cell></cell><cell cols="3">DeepInf-GCN 76.60 44.31 66.74 53.26</cell></row><row><cell>Twitter</cell><cell cols="3">DeepInf-GAT 80.22 48.41 69.08 56.93</cell></row><row><cell></cell><cell cols="3">DeepInf-GCN 76.85 42.44 71.30 53.21</cell></row><row><cell>Weibo</cell><cell cols="3">DeepInf-GAT 82.72 48.53 76.09 59.27</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Prediction performance of DeepInf-GAT (%) with/without vertex features as introduced in Table2.</figDesc><table><row><cell>Data</cell><cell cols="2">Features AUC Prec. Rec.</cell><cell>F1</cell></row><row><cell>OAG</cell><cell>× √</cell><cell cols="2">68.07 34.77 66.87 45.78 71.79 40.77 60.97 48.86</cell></row><row><cell>Digg</cell><cell>× √</cell><cell cols="2">89.39 68.52 72.85 70.62 90.65 66.82 78.49 72.19</cell></row><row><cell>Twitter</cell><cell>× √</cell><cell cols="2">78.30 47.24 65.36 54.84 80.22 48.41 69.08 56.93</cell></row><row><cell>Weibo</cell><cell>× √</cell><cell cols="2">81.47 46.90 75.02 57.71 82.72 48.53 76.09 59.27</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">In this work, we consider undirected relationships. Research Track Paper KDD 2018, August 19-23, 2018, London, United Kingdom</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1">www.openacademic.ai/oag/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. We thank Linjun Zhou, Yutao Zhang, and Jing Zhang for their comments. Jiezhong Qiu and Jie Tang are supported by NSFC 61561130160 and National Basic Research Program of China 2015CB358700.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Friends and neighbors on the web</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eytan</forename><surname>Adamic</surname></persName>
		</author>
		<author>
			<persName><surname>Adar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social networks</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="211" to="230" />
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Group formation in large social networks: membership, growth, and evolution</title>
		<author>
			<persName><forename type="first">Lars</forename><surname>Backstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Huttenlocher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyang</forename><surname>Lan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;06</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="44" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Social influence in social advertising: evidence from field experiments</title>
		<author>
			<persName><forename type="first">Eytan</forename><surname>Bakshy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dean</forename><surname>Eckles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Itamar</forename><surname>Rosenn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EC &apos;12</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="146" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">An O(m) algorithm for cores decomposition of networks</title>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Batagelj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matjaz</forename><surname>Zaversnik</surname></persName>
		</author>
		<idno>arXiv preprint cs/0310049</idno>
		<imprint>
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Power and centrality: A family of measures</title>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Bonacich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American journal of sociology</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="page" from="1170" to="1182" />
			<date type="published" when="1987">1987. 1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A 61-million-person experiment in social influence and political mobilization</title>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">J</forename><surname>Robert M Bond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Fariss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><forename type="middle">Di</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cameron</forename><surname>Kramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><forename type="middle">E</forename><surname>Marlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">H</forename><surname>Settle</surname></persName>
		</author>
		<author>
			<persName><surname>Fowler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">489</biblScope>
			<biblScope unit="page">295</biblScope>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Retrieval evaluation with incomplete information</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR &apos;04</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mining the link structure of the World Wide Web</title>
		<author>
			<persName><forename type="first">Soumen</forename><surname>Chakrabati</surname></persName>
		</author>
		<author>
			<persName><surname>Dom</surname></persName>
		</author>
		<author>
			<persName><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><surname>Rajagopalan</surname></persName>
		</author>
		<author>
			<persName><surname>Tomkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="60" to="67" />
			<date type="published" when="1999">1999. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Spectral graph theory</title>
		<author>
			<persName><forename type="first">Fan</forename><surname>Rk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>American Mathematical Soc</publisher>
			<biblScope unit="volume">92</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Fast and accurate deep network learning by exponential linear units (elus)</title>
		<author>
			<persName><forename type="first">Djork-Arné</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07289</idno>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The anatomy of a scientific rumor</title>
		<author>
			<persName><forename type="first">Manlio</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Domenico</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Lima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Mougel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirco</forename><surname>Musolesi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">2980</biblScope>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">metapath2vec: Scalable Representation Learning for Heterogeneous Networks</title>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitesh</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;17</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="135" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Structural Diversity and Homophily: A Study Across More Than One Hundred Big Networks</title>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reid</forename><forename type="middle">A</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitesh V</forename><surname>Chawla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;17</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="807" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A Century of Science: Globalization of Scientific Collaborations, Citations, and Innovations</title>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;17</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1437" to="1446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07">2011. Jul (2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">LIBLINEAR: A library for large linear classification</title>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Rong-En Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang-Rui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chih-Jen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008-08">2008. Aug (2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS &apos;10</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning influence probabilities in social networks</title>
		<author>
			<persName><forename type="first">Amit</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Bonchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laks</forename><surname>Vs Lakshmanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM &apos;10</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="241" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;16</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS &apos;17</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Rex</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.05584</idno>
		<title level="m">Representation Learning on Graphs: Methods and Applications</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Social dynamics of digg</title>
		<author>
			<persName><forename type="first">Tad</forename><surname>Hogg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Lerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EPJ Data Science</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Maximizing the spread of influence through a social network</title>
		<author>
			<persName><forename type="first">David</forename><surname>Kempe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Éva</forename><surname>Tardos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;03</title>
				<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="137" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">DeepCas: An end-toend predictor of information cascades</title>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW &apos;17</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="577" to="586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Detecting stress based on social interactions in social networks</title>
		<author>
			<persName><forename type="first">Huijie</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangyao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lexing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1820" to="1833" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An experimental study on implicit social recommendation</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR &apos;13</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="73" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning to recommend with social trust ensemble</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><surname>Michael R Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR &apos;09</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="203" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName><forename type="first">Awni</forename><forename type="middle">Y</forename><surname>Andrew L Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName><surname>Ng</surname></persName>
		</author>
		<idno>ICML &apos;13. 3</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Rise and fall patterns of information diffusion: model and implications</title>
		<author>
			<persName><forename type="first">Yasuko</forename><surname>Matsubara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasushi</forename><surname>Sakurai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;12</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="6" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Information diffusion and external influence in networks</title>
		<author>
			<persName><forename type="first">Seth</forename><forename type="middle">A</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;12</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="33" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML &apos;16</title>
				<imprint>
			<date type="published" when="2014">2016. 2014-2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">The PageRank citation ranking: Bringing order to the web</title>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajeev</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terry</forename><surname>Winograd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Stanford InfoLab</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;14</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and node2vec</title>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM &apos;18</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="459" to="467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">The lifecycle and cascade of wechat social messaging groups</title>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">E</forename><surname>Hopcroft</surname></persName>
		</author>
		<idno>WWW &apos;16. 311-320</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Prediction of information diffusion probabilities for independent cascade model</title>
		<author>
			<persName><forename type="first">Kazumi</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryohei</forename><surname>Nakano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masahiro</forename><surname>Kimura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KES &apos;08</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="67" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Efficient graphlet kernels for large graph comparison</title>
		<author>
			<persName><forename type="first">Nino</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Petri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName><surname>Borgwardt</surname></persName>
		</author>
		<idno>AISTATS&apos; 09. 488-495</idno>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">LINE: Large-scale information network embedding</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW &apos;15</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Social influence analysis in large-scale networks</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;09</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="807" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Confluence: Conformity influence in large social networks</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimeng</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;13</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="347" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Arnetminer: extraction and mining of academic social networks</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhong</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;08</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="990" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Fast Random Walk with Restart and Its Applications</title>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia-Yu</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM &apos;06</title>
				<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="613" to="622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Structural diversity in social contagion</title>
		<author>
			<persName><forename type="first">Johan</forename><surname>Ugander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Backstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cameron</forename><surname>Marlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PNAS</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="5962" to="5966" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Instance Normalization: The Missing Ingredient for Fast Stylization</title>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.08022</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS &apos;17</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><surname>Bengio</surname></persName>
		</author>
		<title level="m">Graph Attention Networks. ICLR &apos;18</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Collective dynamics of &apos;small-world&apos; networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Duncan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">H</forename><surname>Watts</surname></persName>
		</author>
		<author>
			<persName><surname>Strogatz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">393</biblScope>
			<biblScope unit="page" from="440" to="442" />
			<date type="published" when="1998">1998. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00853</idno>
		<title level="m">Empirical evaluation of rectified activations in convolutional network</title>
				<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep graph kernels</title>
		<author>
			<persName><forename type="first">Pinar</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;15</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Social Influence Locality for Modeling Retweeting Behaviors</title>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Biao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI&apos; 13</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Who influenced you? predicting retweet via social influence locality</title>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunxiao</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDD</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
