<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GraphAdapter: Tuning Vision-Language Models With Dual Knowledge Graph</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-09-24">24 Sep 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xin</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">National University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dongze</forename><surname>Lian</surname></persName>
							<email>dongze@nus.edu.sg</email>
							<affiliation key="aff1">
								<orgName type="institution">National University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhihe</forename><surname>Lu</surname></persName>
							<email>zhihelu@nus.edu.sg</email>
							<affiliation key="aff1">
								<orgName type="institution">National University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiawang</forename><surname>Bai</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhibo</forename><surname>Chen</surname></persName>
							<email>chenzhibo@ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
							<email>xinchao@nus.edu.sg</email>
							<affiliation key="aff1">
								<orgName type="institution">National University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GraphAdapter: Tuning Vision-Language Models With Dual Knowledge Graph</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-09-24">24 Sep 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2309.13625v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Adapter-style efficient transfer learning (ETL) has shown excellent performance in the tuning of vision-language models (VLMs) under the low-data regime, where only a few additional parameters are introduced to excavate the task-specific knowledge based on the general and powerful representation of VLMs. However, most adapter-style works face two limitations: (i) modeling task-specific knowledge with a single modality only; and (ii) overlooking the exploitation of the interclass relationships in downstream tasks, thereby leading to sub-optimal solutions. To mitigate that, we propose an effective adapter-style tuning strategy, dubbed GraphAdapter, which performs the textual adapter by explicitly modeling the dualmodality structure knowledge (i.e., the correlation of different semantics/classes in textual and visual modalities) with a dual knowledge graph. In particular, the dual knowledge graph is established with two sub-graphs, i.e., a textual knowledge sub-graph, and a visual knowledge sub-graph, where the nodes and edges represent the semantics/classes and their correlations in two modalities, respectively. This enables the textual feature of each prompt to leverage the task-specific structure knowledge from both textual and visual modalities, yielding a more effective classifier for downstream tasks. Extensive experimental results on 11 benchmark datasets reveal that our GraphAdapter significantly outperforms previous adapterbased methods. The code will be released at https://github.com/lixinustc/ GraphAdapter</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent large-scale vision-language models (VLMs), such as CLIP <ref type="bibr" target="#b48">[49]</ref> and ALIGN <ref type="bibr" target="#b24">[25]</ref> have shown their promising representation capability for a series of downstream vision tasks <ref type="bibr" target="#b71">[72]</ref>, such as classification <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b74">75]</ref>, generation <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b53">54]</ref>, and recognition <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b65">66]</ref>. Different from previous pre-training models with ImageNet <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b12">13]</ref>, based on large-scale parameters, VLMs can learn more prosperous, fine-grained, and consistent semantics from billions of text-image pairs with contrastive learning. This enables the VLMs with two advantages: 1) performing superior zero-shot generalization capability <ref type="bibr" target="#b77">[78]</ref> with simple hand-craft prompts like "a photo of a [class]", and 2) possessing impressive transferability for downstream tasks <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b74">75]</ref> by finetuning the VLMs in a low-data regime. Notably, traditional fine-tuning methods typically involve tuning all parameters of the model to adapt to downstream tasks. However, this approach may not be well-suited for fine-tuning large VLMs in resource-constrained scenarios, and result in overfitting due to the limited number of available samples. To mitigate this, efficient transfer learning (ETL) is proposed to transfer the task-relevant knowledge from VLMs to downstream tasks by tuning a few parameters.</p><p>Figure <ref type="figure">1:</ref> The comparison between (a) Zero-shot CLIP <ref type="bibr" target="#b48">[49]</ref>, (b) CLIP-Adapter <ref type="bibr" target="#b16">[17]</ref> (c) TaskRes <ref type="bibr" target="#b70">[71]</ref>, and (d) our proposed GraphAdapter. We can observe that previous works model task-specific knowledge with a single modality and lacks the exploitation of structure knowledge. In contrast, our GraphAdapter aims to exploit the fused vision and language structure knowledge in data (i.e., the inter-class relationships in dual modalities) for textual feature adapter with graph learning.</p><p>There are two popular ETL approaches for VLMs, including prompt tuning <ref type="bibr" target="#b77">[78,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b5">6]</ref>, and adapter-style tuning <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b73">74]</ref>. In particular, prompt tuning aims to adjust the textual classifier adaptively toward downstream tasks by adding learnable prompts on the input side, which outperforms zero-shot CLIP by a large margin with few-shot samples, such as CoOp <ref type="bibr" target="#b77">[78]</ref>, and CoCoOp <ref type="bibr" target="#b76">[77]</ref>. Despite that, there is one limitation in the prompt tuning of VLMs <ref type="bibr" target="#b71">[72,</ref><ref type="bibr" target="#b70">71]</ref>: it needs to pass the data through the textual encoder every iteration during training, yielding a higher demand for resources. In contrast, by using the textual encoder once only, adapter-style works tend to refine the textual classifier or visual features with simple but efficient feature modulation for a specific task on the output side. For instance, CLIP-Adapter <ref type="bibr" target="#b16">[17]</ref> exploits one simple bottleneck layer to adjust the textual and visual embeddings of VLMs, which exceeds the zero-shot CLIP by 3.02% on ImageNet with the one-shot setting. TaskRes <ref type="bibr" target="#b70">[71]</ref> utilizes learnable task-specific parameters as prior-independent residuals to adjust the textual embeddings. Another popular line <ref type="bibr" target="#b74">[75,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b78">79]</ref> seeks to augment the prior knowledge for downstream tasks with the cooperation of CLIP and other pre-trained large vision or language models, such as DINO <ref type="bibr" target="#b4">[5]</ref>, and GPT <ref type="bibr" target="#b3">[4]</ref>.</p><p>However, there are two limitations in most adapter-style works on ETL: 1) only modeling taskspecific knowledge from a single modality perspective, such as CLIP-adapter <ref type="bibr" target="#b16">[17]</ref>, TaskRes <ref type="bibr" target="#b70">[71]</ref> and Tip-Adapter <ref type="bibr" target="#b74">[75]</ref>, where the adaptation is achieved based on the independent visual or textual feature.</p><p>2) overlooking the explicit exploitation of the structure knowledge (i.e., the relation between different semantics/classes) in downstream tasks. A small number of samples in a low-data regime are hard to guide the model to sufficiently excavate the structure knowledge in downstream tasks, leading to the bias for partial attributes in the data, such as color and shape, and causing the sub-optimal transferability and generalization capability <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b40">41]</ref>. It is vital to model the multi-modality structure knowledge of downstream tasks for the tuning of VLMs in the low-data regime.</p><p>To mitigate the above limitations, we propose a brand-new adapter-style tuning strategy, dubbed GraphAdapter, which aims to model task-specific knowledge for downstream tasks with the fused textual and visual structure knowledge. We achieve this by solving two crucial challenges: 1) how to model the structure knowledge (i.e., the inter-class relationship) for downstream tasks, and 2) how to learn the task-specific knowledge by introducing two-modality (i.e., visual and textual) structure knowledge. Recently, graph learning <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b33">34]</ref> has shown the prevailing performance on modeling data knowledge structure. However, the potential of graphs in improving the efficient transfer learning of VLMs has not yet been fully explored. Inspired by this, for the first challenge, we aim to exploit graph learning to model the structure knowledge for downstream tasks when tuning VLMs. Consequently, we propose the dual knowledge graph, which is composed of a textual sub-graph and a visual sub-graph. Particularly, to establish the relationship between different semantics/classes in textual space, we regard the textual feature of a specific class as one node of the textual graph and measure inter-class relationships by measuring their corresponding distance of features as edges of the graph. Similarly, the visual knowledge graph aims to model the relationship between different semantics/classes from the visual perspective, where the nodes are constructed with mean features of the training samples from the same class. In this way, our dual knowledge graph contains two-modality structure knowledge from downstream tasks, which enables the feature adapter with sufficient and reliable perception for downstream tasks.</p><p>For the second challenge, one intuitive strategy is to introduce the textual/visual structure knowledge for the textual/visual feature adapter, separately. However, this strategy still models the task-specific knowledge with a single modality for each textual/visual adapter (e.g., the textual adapter is only aware of textual structure knowledge). To mitigate this limitation, we introduce the dual knowledge graph for textual/visual adapters, and let each adapter be aware of the structure knowledge in the same modality and cross-modality. (Notably, we only exploit the textual adapter in our paper, since the limited gain with the combination of textual and visual adapters. Please see Table <ref type="table">2</ref>.). To exploit the dual knowledge graph effectively, we introduce graph learning to the textual adapter, where each feature of the prompt warps the textual and visual structure knowledge through the graph convolution network (GCN), and then fuse two warped knowledge to modulate the original feature for classification in a residual way. Based on the above strategies, we propose an effective adapter-style tuning strategy, i.e., GraphAdapter, which achieves superior performance in tuning the vision-language models under a low-data regime.</p><p>The contributions of this paper are summarized as follows:</p><p>? We pinpoint that the dual-modality structure knowledge (i.e., the inter-class relationship in textual and visual space) is vital for the efficient transfer learning (ETL) of VLMs in the low-data regime.</p><p>? Based on the above analysis, we propose a brand-new adapter-style tuning strategy, i.e., GraphAdapter, for the ETL of VLMs, which models the dual-modality structure knowledge from the textual and visual space with our proposed dual knowledge graph and enables the feature adapter to leverage the fused visual and language knowledge for better learning of task-specific knowledge from downstream tasks, yielding an effective tuning of VLMs.</p><p>? We evaluate our Graphadapter on 11 popular benchmarks on few-shot classification. The experiments demonstrated that our Graphadapter significantly outperforms previous prompt-based or adapter-style works, even on the challenging fine-grained image classification task, such as FGVCAircraft <ref type="bibr" target="#b41">[42]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Vision-Language Pre-training (VLP) aims to learn the universal textual and visual representation simultaneously with amounts of text-image pairs. A series of works have revealed that pre-trained vision-language representations can help lots of downstream tasks to improve their performances, such as few-shot classification <ref type="bibr" target="#b77">[78,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b70">71]</ref>, cross-modality generation <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b47">48]</ref>, and visual recognition <ref type="bibr" target="#b62">[63]</ref>. The methods of VLP can be roughly divided into three types: 1) dual-encoder architecture, which aligns the textual and visual representation with text-image matching or text-image contrastive learning. One typical work is CLIP <ref type="bibr" target="#b48">[49]</ref>, where the promising visual representation is learned by matching the visual concepts with the corresponding textual representation with contrastive learning.</p><p>2) The second type <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b64">65]</ref> is fusion-encoder architecture, where the textual and visual representation are fused with cross-modal attention. In general, two independent feature extractors are exploited to extract the modal-specific representations, respectively, and the crossmodal transformer is used for learning the text-image shared representation.</p><p>3) The third type is a combination of dual-branches and fusion-branch architectures. For instance, VLMo <ref type="bibr" target="#b0">[1]</ref> proposes the multiway transformer, which consists of the visual-specific branch, text-specific branch, and text-visual branch. Recently, amounts of studies are devoted to exploring how to tune the large vision-language model to downstream tasks with few parameters and data, i.e., efficient transfer learning. This will be clarified in the next section.</p><p>Efficient Transfer Learning (ETL) is proposed to transfer the task-specific knowledge to downstream tasks by tuning the partial parameters <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b34">35]</ref>. There are two prominent directions for the ETL, i.e., prompt tuning <ref type="bibr" target="#b77">[78,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b39">40]</ref> and adapter-style tuning <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b74">75]</ref>. Prompt engineering stems from natural language process, (NLP), which aims to formalize various NLP tasks with different prompt templates <ref type="bibr" target="#b35">[36]</ref>. Based on this development, prompt engineering is introduced to the visual-language task. For instance, Zero-shot CLIP <ref type="bibr" target="#b48">[49]</ref> introduces the simple manual prompt template, like "a photo of a [class]", and outperforms the linear probe by 1.9% on ImageNet <ref type="bibr" target="#b11">[12]</ref>. As the pioneering work, CoOp <ref type="bibr" target="#b77">[78]</ref> for the first time introduces the learnable prompt to transfer the task-specific knowledge to few-shot tasks. Following this, amounts of work <ref type="bibr" target="#b76">[77,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b5">6]</ref> improve the prompt tuning from multiple perspectives, such as generalization <ref type="bibr" target="#b76">[77]</ref>, knowledge prototype <ref type="bibr" target="#b75">[76]</ref>, augmentation <ref type="bibr" target="#b73">[74]</ref>, and diversity <ref type="bibr" target="#b39">[40]</ref>.</p><p>In contrast, adapter-style works tune the VLMs to downstream tasks by adjusting the textual and visual features. Early work CLIP-Adapter <ref type="bibr" target="#b16">[17]</ref> only exploits one bottleneck layer to improve the performance of few-shot classification. TaskRes <ref type="bibr" target="#b70">[71]</ref> introduces the task-independent adapter to decouple the prior knowledge of the pre-trained models and task-specific knowledge. There are also some works <ref type="bibr" target="#b74">[75,</ref><ref type="bibr" target="#b73">74]</ref> that seek extra prior knowledge from other pre-trained large models, such as DINO <ref type="bibr" target="#b4">[5]</ref>, which also yield great performance on adapter-style works. In principle, the above adapter-style methods achieve efficient transfer learning for VLMs from two perspectives: 1) increasing instance-wise or task-wise adaptability based on downstream tasks <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b70">71]</ref>. 2) taking into account the relation between training and test samples. <ref type="bibr" target="#b74">[75]</ref>. However, the above works only model task-specific knowledge with a single modality, and lacks the utilization of structure knowledge in downstream tasks, leading to the sub-optimal solution. In this paper, we aim to excavate the structure knowledge of data with graphs and further improve the performance of adapter-style tuning.</p><p>Graph learning <ref type="bibr" target="#b68">[69,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31]</ref> performs excellent capability in formalizing the topological structure of data with the nodes and edges, which has been broadly applied in various fields, such as biology <ref type="bibr" target="#b32">[33]</ref>, and social networks <ref type="bibr" target="#b52">[53]</ref>. With the emergence of GCN <ref type="bibr" target="#b26">[27]</ref>, some works take a step forward to introduce structure graphs into transfer learning <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b14">15]</ref>. The core challenge for graph learning is how to construct the nodes and edges. Previous works usually construct the nodes from three perspectives, including objects <ref type="bibr" target="#b8">[9]</ref>, semantic features <ref type="bibr" target="#b7">[8]</ref>, or model parameters <ref type="bibr" target="#b6">[7]</ref>. There are also some works that utilize the hypergraph to achieve better learning of structure knowledge <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b69">70]</ref>. In contrast, in this paper, we are the first work to introduce graph learning into the efficient transfer learning (ETL) of vision-language tasks.</p><p>3 Approach</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminaries</head><p>Contrastive Language-Image Pre-training (CLIP) is designed to learn continuous visual representation based on 0.4 million text-image pairs. Different from the pretraining with ImageNet, where the labels are discrete and cannot delicately describe the attributes of images and the relation between different classes, CLIP learns the visual representation with the supervision of textual embedding by contrastive loss. There are two feature extractors in CLIP, i.e., the textual encoder E t and visual encoder E v , which are used to warp the prompts and image into the textual embedding z t and visual embedding z v . In the test process, K prompts like "a photo of a [class]" are inputted to the textual feature extractor to obtain the textual embeddings {z k t } K k=1 as the labels of K classes. The class of each image is achieved with:</p><formula xml:id="formula_0">P (y = c|z v ) = exp(sim(z v , z c t )/? ) K k=1 exp(sim(z v , z k t )/? ) ,<label>(1)</label></formula><p>where c, sim, and ? denote the class, cosine similarity, and learned temperature of CLIP, respectively. Despite CLIP has performed excellently on zero-shot image classification, the efficient transfer learning of CLIP is still required to adapt the pre-trained CLIP to downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Our Approach: GraphAdapter</head><p>To introduce the dual-modality structure knowledge of the downstream tasks to the feature adapter, we introduce dual knowledge graph G = {G t , G v }, which is composed of the textual knowledge sub-graph G t and visual knowledge sub-graph G v to obtain and store the textual structure knowledge of prompts and visual structure knowledge of training samples, respectively. Then the feature of the sample/prompt can warp the structure knowledge of downstream tasks in the two modalities to refine themselves, thereby leveraging the dual structure knowledge for better modeling for downstream tasks. The whole framework of our GraphAdapter is depicted in Fig. <ref type="figure">3</ref>, which is composed of two parts, including the construction of a dual knowledge graph and the adapter-style CLIP-based classification. Particularly, the dual knowledge graph is established only once for the whole training process, and the textual encoder is also only run once to obtain the textual embedding z t of each prompt. In the optimization process, given one textual feature z t , the relation between it and the nodes of the dual knowledge graph will be computed as the condition to warp the knowledge from the dual knowledge graph to adjust the textual embedding in the residual form. We will clarify our GraphAdapter as follows:</p><p>Textual knowledge sub-graph. To excavate the structure knowledge (i.e., the relation between different semantics) of the downstream tasks in textual space, we construct the textual knowledge sub-graph G t = {C t , E t }, where the nodes C t aims to capture the semantics of different classes and the edges {E t } is used to measure the relation of different nodes. Notably, the classifier of CLIP is based on the textual features of the prompts from different classes, where the textual feature of a within-class prompt can represent the semantics of one class. Therefore, given one downstream task with K classes, the nodes set C t = {c i t } K i=1 ? R K?d ) are obtained with the mean feature of the prompts from the different class, where c i t and d denotes the textual feature of the i th class and the dimension of the textual feature. And the edges E t between different nodes are computed with the cosine similarity between different nodes since the classification is achieved by computing the cosine similarity between the features in CLIP.</p><formula xml:id="formula_1">E t = {e i,j t }, e i,j t = c i t c j t T |c i t | ? |c j t | , i, j ? [1, K],<label>(2)</label></formula><p>where {e i,j t } denotes the edge between i th and j th nodes. Visual knowledge sub-graph. Different from the textual knowledge sub-graph, where the structure knowledge is only from the textual concept of a downstream task, the visual knowledge sub-graph can measure more fine-grained relations of different semantics in visual space, since the diverse visual feature of different samples. As shown in Fig. <ref type="figure">3</ref>, to construct the visual knowledge sub-graph G v = {C v , E v }, we pass the augmented image group from the same class into visual encoder to obtain their visual features, and then compute the mean features of them as the nodes</p><formula xml:id="formula_2">C v = {c i v } K i=1 ? R K?d of for visual knowledge graph. The edges E v = {e i,j v |i, j ? [1, K]</formula><p>} are computed with the cosine similarity between different nodes in the visual knowledge sub-graph.</p><p>Adapting with the dual knowledge graph. After constructing the dual knowledge graph G = {G t , G v }, we can achieve the feature adapter by introducing the dual-modality structure knowledge adaptively from the text and image sub-graphs. Notably, previous works on adapter-style tuning only model the task-specific knowledge with a single modality, which lacks the exploitation of crossmodality knowledge. In contrast, our GraphAdapter utilizes both inner-modality and cross-modality structure knowledge for feature adapters. Concretely, given the textual feature z t from the textual encoder of CLIP, we aim to warp the z t into the textual and visual knowledge sub-graphs to extract the textual modality and cross-modality structure knowledge, respectively. One simple strategy is to regard the textual/visual features z t and z v as the query, and then warp the knowledge from the graph nodes C t and C v based on similarity. However, this ignores that the structure knowledge is also required to be optimized to suit the downstream tasks. To achieve this, in the adapting process, we regard the textual feature z t as one node and then warp it to the visual and textual sub-graphs, where the textual features obtain dual-modality structure knowledge by interacting with two sub-graphs in the same graph space with the graph convolutional networks (GCN).</p><p>Specifically, in the optimization process, we expand the textual knowledge sub-graph and visual knowledge sub-graph by concatenating the nodes C t /C v of the textual/visual knowledge sub-graph with the textual features z t , and then compute their edges (i.e., the correlation between different nodes) as:</p><formula xml:id="formula_3">C tt = [z t , C t ], C vt = [z t , C v ], E tt = 1 sim(z t , C t ) sim(C t , z t ) E t , E vt = 1 sim(z t , C v ) sim(C v , z t ) E v .<label>(3)</label></formula><p>In this way, we can warp the textual feature to the dual knowledge graph space, which provides the basis for interaction and knowledge transfer between the textual feature and the dual knowledge graph.</p><p>Then, the Graph Convolution Network (GCN) g tt and g vt are utilized to excavate the knowledge for adapting the texture feature from each node in textual and visual knowledge sub-graphs:</p><formula xml:id="formula_4">C * tt = g tt (C tt , ?tt ) = ?( ?tt C tt W tt ), C * vt = g vt (C tt , ?vt ) = ?( ?vt C vt W vt ),<label>(4)</label></formula><p>where ?tt = D tt E tt D tt and ?vt = D vt E vt D vt are the adjacent matrix for graph learning. ? is an activation function. And the D tt and D vt are the matrices used for Laplace normalization <ref type="bibr" target="#b26">[27]</ref> for the edges as:</p><formula xml:id="formula_5">D tt = diag( K p=1 (E tt + I) p ), D vt = diag( K p=1 (E vt + I) p ).<label>(5)</label></formula><p>The meaning of Eq. 4 is seeking the prior knowledge from other nodes in one graph to refine itself based on the correlation (i.e., the adjacent matrix). And thus, we can obtain the adapted/refined textual feature as z tt = C * tt [0, :] and z vt = C * vt [0, :]. To fuse the inner-modality and cross-modality structure knowledge, we introduce one hyper-parameter ? to fuse these two features as z ? t = ? * z tt +(1-?) * z vt . Then the feature adapter is achieved in a residual form, where the hyper-parameter ? is used to adjust the weights of prior knowledge from the dual knowledge graph and original feature:</p><formula xml:id="formula_6">z * t = ?z t + (1 -?)z ? t .</formula><p>In the whole training process, only the parameters of GCN g tt and g vt are trainable under the constraint of the cross entropy function between the label of samples and the predicted label as Eq. 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Implementation Details</head><p>Datasets. Following previous adapter-style studies <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b74">75]</ref>, we validate our GraphAdapter on 11 few-shot classification tasks, including ImageNet <ref type="bibr" target="#b11">[12]</ref>, StandfordCars <ref type="bibr" target="#b27">[28]</ref>, UCF101 <ref type="bibr" target="#b56">[57]</ref>, Caltech101 <ref type="bibr" target="#b15">[16]</ref>, Flowers102 <ref type="bibr" target="#b44">[45]</ref>, SUN397 <ref type="bibr" target="#b67">[68]</ref>, DTD <ref type="bibr" target="#b10">[11]</ref>, EuroSAT <ref type="bibr" target="#b19">[20]</ref>, FGVCAircraft <ref type="bibr" target="#b41">[42]</ref>, OxfordPets <ref type="bibr" target="#b46">[47]</ref>, and Food101 <ref type="bibr" target="#b2">[3]</ref>. Among them, OxfordPets, Food101, StanfordCars, Flowers102, and FGVCAircraft belong to fine-grained classification tasks, EuroSAT is for remote sensing classification, and DTD is the dataset of texture classification. To investigate the generalization capability of our GraphAdapter, we follow the CoOp <ref type="bibr" target="#b77">[78]</ref> and conduct experiments on ImageNetV2 <ref type="bibr" target="#b51">[52]</ref>, ImageNet-Sketch <ref type="bibr" target="#b61">[62]</ref>, ImageNet-A <ref type="bibr" target="#b21">[22]</ref> and ImageNet-R <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation details.</head><p>If not mentioned, we set the pre-trained backbone ResNet-50 of CLIP <ref type="bibr" target="#b17">[18]</ref> as the base layer to produce the visual feature. To estimate the applicability of our method, we also apply our GraphAdapter to other CLIP visual encoders, including ResNet-101 <ref type="bibr" target="#b18">[19]</ref>, ViT-B/32 <ref type="bibr" target="#b13">[14]</ref>, and ViT-B/16 <ref type="bibr" target="#b13">[14]</ref>, and validate its effectiveness on ImageNet. We optimize our model for 100 epochs Figure <ref type="figure">3</ref>: The performance comparison of our GraphAdapter with the state-of-the-art methods on few-shot learning, including 1-/2-/4-/8-/16-shots on 11 benchmark datasets. We provide all numerical results of this figure in the Supplementary. for 1, 2, 4, 8, and 16-shots. In the training process, we utilize the Adam optimizer with an initial learning rate of 1e -3 , which drops with the cosine learning rate decay schedule. Notably, to achieve stable training, we follow previous works <ref type="bibr" target="#b70">[71,</ref><ref type="bibr" target="#b77">78]</ref> and utilize the warmup strategy for the training, where the small learning rate 1e -5 is applied at the first epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparisons with State-of-the-arts</head><p>Few-shot learning. We compare our proposed GraphAdapter with several state-of-the-art works on ETL, including Zero-shot CLIP <ref type="bibr" target="#b48">[49]</ref>, CoOp <ref type="bibr" target="#b77">[78]</ref>, clip-adapter <ref type="bibr" target="#b16">[17]</ref>, Tip-Adapter-F <ref type="bibr" target="#b74">[75]</ref>, and TaskRes <ref type="bibr" target="#b70">[71]</ref> on 11 benchmark datasets. The experimental results are shown in Fig. <ref type="figure">3</ref>, where we can observe that our GraphAdapter consistently outperforms previous ETL works for 1-/2-/4-/8-/16-shots on the average performance of 11 benchmark datasets. Particularly, on the 16-shot setting, our GraphAdapter achieves an average performance of 76.22%, which exceeds the Tip-Adapter-F [75] by 0.57%, and TaskRes <ref type="bibr" target="#b70">[71]</ref> by 1.12%. Even for the most challenging dataset FGVCAircraft <ref type="bibr" target="#b41">[42]</ref> of the fine-grained classification, our GraphAdapter still performs better than the second-best method Tip-Adapter-F by 1.01% on 16-shot setting, since our GraphAdapter can better exploit the structure knowledge in the downstream tasks with the dual knowledge graph.</p><p>Generalization. We also investigate the generalization capability of our GraphAdapter on four commonly-used datasets i.e., ImageNet-V2 <ref type="bibr" target="#b51">[52]</ref>, ImageNet-Sketch <ref type="bibr" target="#b61">[62]</ref>, ImageNet-A <ref type="bibr" target="#b21">[22]</ref>, ImageNet-R <ref type="bibr" target="#b20">[21]</ref>, with different visual backbones, including the pre-trained ResNet-101 <ref type="bibr" target="#b18">[19]</ref>, ViT-B/16 <ref type="bibr" target="#b12">[13]</ref>,</p><p>ViT-B/32 <ref type="bibr" target="#b12">[13]</ref>. The experimental results are shown in Table <ref type="table" target="#tab_0">1</ref>, where we provide two versions "Ours g " and "Ours" of our GraphAdapter for a fair comparison. Here, "Ours" denotes the version of GraphAdapter used in few-shot learning. Since the over-fitting on few-shot learning will decrease the generalization capability, we increase the weights ? = 0.8 of base textual features z t in the adapted feature z * t as "Ours g " to obviate the over-fitting on ImageNet Dataset. From Table <ref type="table" target="#tab_0">1</ref>, our GraphAdapter achieves the optimal generalization capability on four cross-domain datasets, which outperforms TaskRes <ref type="bibr" target="#b70">[71]</ref> by 0.15% on ResNet-50 <ref type="bibr" target="#b18">[19]</ref>. Meanwhile, it obtains the best performance on the source data ImageNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Studies</head><p>The effects of the different variants of GraphAdapter. It is noteworthy that our GraphAdapter can be applied to the textual and visual adapter by introducing fused visual and textual structure knowledge. Therefore, we investigate the effectiveness of another two variants of our GraphAdapter, i.e., GraphAapter-I, and GraphAdapter-T&amp;I. Here, we denote our GraphAdapter in our paper as GraphAdapter-T, since it is used to achieve the feature adapter of the textual branch. GraphAapter-I denotes that we introduce the GraphAdapter into the visual branch, which adjusts the visual feature to adapt the downstream tasks. GraphAdapter-T&amp;I represents that the GraphAdapter is exploited in the visual and textual branches simultaneously. As shown in Table <ref type="table">2</ref>, we can have the following conclusions: 1) the GraphAdapter is more suitable for the textual branch, which outperforms that in the visual branch by an average of 4.01% on 11 benchmark datasets. 2) Applying the GraphAdapter in both branches of CLIP only achieves a slight gain compared with the GraphAdapter-T. Therefore, we utilize the GraphAdapter-T in our paper.</p><p>The effects of the dual knowledge graph. To investigate the effectiveness of the dual knowledge graph, we conduct experiments on ImageNet by removing the textual knowledge sub-graph (i.e., setting ? as 0.0) and visual knowledge sub-graph (i.e., setting ? as 1.0), respectively. The experimental Table <ref type="table">2</ref>: The ablation study for different variants of our GraphAdapter, where GraphAdapter-T, GraphAdapter-I denotes integrating our proposed GraphAdapter into the textual branch and visual branch, respectively. GraphAdapter-T&amp;I utilizes the GraphAdapter-T and GraphAdapter-I simultaneously.   results are shown in Table <ref type="table" target="#tab_2">3</ref>. We can observe that removing the textual knowledge sub-graph achieves an accuracy of 64.30%, while the performance of removing the visual knowledge sub-graph drops to 64.95%, which reveals that textual structure knowledge is more important for textual feature adapter. Notably, the cooperation of textual and visual structure knowledge can achieve the best performance of 65.70%. This validates the effectiveness of introducing inner-modality and cross-modality structure knowledge for adapter-style tuning.</p><p>The effects of different coefficients ? and ?. There are two hyper-parameters ? and ? in our GraphAdapter, where the ? controls the balance of basic textual features from CLIP and the adapted textual feature obtained with GraphAdapter, and ? is responsible for the weights of textual knowledge graph and visual knowledge graph. As shown in table <ref type="table" target="#tab_2">3</ref>, the performance of GraphAdapter increases first and then drops when the coefficients ? and ? increase. The optimal ? and ? are 0.7, and 0.6, respectively. We also investigate whether a learnable coefficient can further improve the performance of our GraphAdapter. Experimental results show that the learnable coefficients achieve lower performance with our selected fixed coefficients ? and ?, since the coefficients are hard to be optimized with the few-shot samples.</p><p>The effects of different backbones. As stated in Fig. <ref type="figure" target="#fig_1">4</ref>, we also evaluate the effectiveness of our GraphAdapter on different CLIP visual backbones, including ResNet-50, ResNet-101, ViT-B/32, and ViT-B/16. Our GraphAdapter consistently exceeds previous works for both four visual backbones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we comprehensively review the limitations of previous adapter-style tuning methods in the low-data regime as 1) previous works only model the task-specific knowledge with a single modality, and 2) overlooking the exploitation of the structure knowledge (i.e., the relation of different semantics/classes) in downstream tasks, which is vital for data-efficient tasks. Based on the analysis, we propose a brand-new adapter-style tuning strategy for visual-language models, termed as GraphAdapter, which introduces the dual knowledge graph to establish the structure knowledge in the textual and visual modalities. By incorporating the fused textual and visual structure knowledge with Graph Convolutional Network (GCN), the text-based classifier can be adapted to downstream tasks effectively with the inner-modality and cross-modality structure knowledge. Extension experiments on 11 benchmark datasets revealed the effectiveness of our GraphAdapter on few-shot learning and generalization.</p><p>Limitations and Broader Impacts. The limitation of our GraphAdapter stems from the textual structure knowledge modeling. In this paper, we utilize the textual features of default prompts like "a photo of a [class]" to construct the nodes of the textual graph. However, the prompts are simple and lack enough diversity. We believe more diverse and accurate prompts for downstream tasks can achieve better modeling for textual structure knowledge, which can further improve the performance of our GraphAdapter. The possible broader impacts are discussed in the Supplementary. CoOp <ref type="bibr" target="#b77">[78]</ref>. From the table, we can find that on the 16-shot few-shot learning, our GraphAdapter outperforms all previous works except for UCF101 <ref type="bibr" target="#b56">[57]</ref> where its performance is comparable. Depart from that, for the average accuracy of 11 benchmark datasets in the 1-/2-/4-/8-/16-shot few-shot learning, our GraphAdapter surpasses previous works with a consistent improvement of 0.57% to 0.76%. We also make the analysis for the Error Bars by providing the standard deviation (Std) of our experimental results in Table <ref type="table" target="#tab_4">5</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C More Dataset and Implementation Details</head><p>More Dataset Details. In this paper, we follow previous works, e.g., CoOp <ref type="bibr" target="#b77">[78]</ref>, CLIP-Adapter <ref type="bibr" target="#b16">[17]</ref>, TaskRes <ref type="bibr" target="#b70">[71]</ref>, and Tip-Adapter <ref type="bibr" target="#b74">[75]</ref>, and exploit the prompts in Table <ref type="table" target="#tab_5">6</ref> for the tuning and testing.</p><p>More Implementation Details. Our experimental results are achieved by running the algorithm three times with different seeds for each setting. The training and inference are implemented with a single NVIDIA GeForce RTX 3090. In the implementation of GraphAdapter for the ImageNet <ref type="bibr" target="#b11">[12]</ref>, we decouple the sub-graph with 1000 nodes for each modality into four graphs with 256 nodes to alleviate the computational cost.  <ref type="bibr" target="#b19">[20]</ref> 10 "a centered satellite photo of <ref type="bibr">[class]</ref>." FGVCAircraft <ref type="bibr" target="#b41">[42]</ref> 100 "a photo of a [class], a type of aircraft." Flowers102 <ref type="bibr" target="#b44">[45]</ref> 102 "a photo of a [class], a type of flower." Food101 <ref type="bibr" target="#b2">[3]</ref> 101 "a photo of a [class], a type of food." OxfordPets <ref type="bibr" target="#b46">[47]</ref> 37 "a photo of a [class], a type of pet." StanfordCars <ref type="bibr" target="#b27">[28]</ref> 196 "a photo of a [class]." SUN397 <ref type="bibr" target="#b67">[68]</ref> 397 "a photo of a [class]." UCF101 <ref type="bibr" target="#b56">[57]</ref> 101 "a photo of a person doing </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D The Comparison of Time and Model Complexity</head><p>We compute and compare our methods with existing published efficient transfer learning methods on the ImageNet <ref type="bibr" target="#b11">[12]</ref> with the 16-shot setting, from the perspectives of tunable parameters, computational flops, training time, and inference time. All results are measured with the officially released code from GitHub. The experimental results are shown in the Table <ref type="table" target="#tab_6">7</ref>. We can observe our tunable parameters are still less than Tip-Adapter-F <ref type="bibr" target="#b74">[75]</ref>. For computational flops, our GraphAdapter takes about 5.42 GFlops, almost the same as Tip-Adapter-F and TaskRes <ref type="bibr" target="#b70">[71]</ref>, which is far lower than CLIP-Adapter. Moreover, our training and inference times are less than the adapter-based work CLIP-Adapter and the prompt tuning method CoOp <ref type="bibr" target="#b77">[78]</ref>. Therefore, our GraphAdapter satisfies the requirement of efficient transfer learning. Moreover, our GraphAdapter can achieve the best performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Visualization of Graph Nodes</head><p>To demonstrate how our GraphAdapter works for the adapter-style tuning for VLMs, we visualize the graph nodes for textual features before and after the GraphAdapter. As shown in Figure <ref type="figure" target="#fig_3">5</ref>, we randomly sampled 20 classes from ImageNet <ref type="bibr" target="#b11">[12]</ref> and utilize the t-SNE to visualize the distribution of each node corresponding to the textual fracture for classification. We can observe that with our GraphAdapter, the nodes of different classes move in directions that lead to much larger inter-class distances, thereby improving the performance of adapter-style tuning for VLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Broader Impacts</head><p>The adapter-style tuning of VLMs aims to efficiently finetune the VLMs for downstream tasks by optimizing a few parameters in the low-data regime. The possible broader impact of our GraphAdapter stems from the tuning of VLMs itself, which has a heavy dependency on the pre-trained VLMs. The utilization of our GraphAdapter should follow the privacy and safety of datasets and pre-trained models. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The pipeline of GraphAdapter, which is composed of the dual knowledge graph and CLIPbased image classification pipeline. Here, the dual knowledge graph contains the delicately designed sub-graphs for textual and visual knowledge, respectively, where the textual knowledge sub-graph is established with the textual feature from the prompts of the downstream task, and the visual knowledge sub-graph is constructed with the visual features of training samples from the downstream task. In the optimization process, the feature of each prompt used for classification will seek the fused visual and textual structure knowledge from the dual knowledge graph to adjust themselves for better classification. K and d are the number of classes and the dimension of textual/visual features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Comparisons on different backbones.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>templates, including "itap of a [class].", "a bad photo of the [class].", "a origami [class].", "a photo of the large [class].", "a [class] in a video game.", "art of the [class]." and "a photo of the small [class]." .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Visualization of the variance of the graph nodes before and after GraphAdapter. Each node represents the representation of one class. We randomly sampled 20 classes from ImageNet for better visualization. The nodes move toward the direction that leads to much larger inter-class distances after GraphAdapter. The red arrows denote the directions.</figDesc><graphic url="image-19.png" coords="17,168.00,266.58,276.39,208.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-1.png" coords="2,109.98,72.00,392.04,173.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The performance comparison in terms of generalization capability on four CLIP visual backbones. The ETL methods are optimized with the ImageNet dataset on 16-shot setting and tested on cross-domain datasets, including ImageNet-V2, -Sketch, -A, and -R.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>Source ImageNet</cell><cell>-V2</cell><cell>-Sketch</cell><cell>Target -A</cell><cell>-R</cell><cell>Average</cell></row><row><cell>Zero-shot CLIP [49]</cell><cell></cell><cell>58.18</cell><cell>51.34</cell><cell>33.32</cell><cell cols="2">21.65 56.00</cell><cell>40.58</cell></row><row><cell>Linear Probe CLIP [49]</cell><cell></cell><cell>55.87</cell><cell>45.97</cell><cell>19.07</cell><cell cols="2">12.74 28.16</cell><cell>28.16</cell></row><row><cell>CoOp [78] TaskRes [71]</cell><cell>ResNet-50</cell><cell>62.95 64.75</cell><cell>55.11 56.47</cell><cell>32.74 35.83</cell><cell cols="2">22.12 54.96 22.80 60.70</cell><cell>41.23 43.95</cell></row><row><cell>Ours</cell><cell></cell><cell>65.70</cell><cell>56.40</cell><cell>34.50</cell><cell cols="2">21.88 58.94</cell><cell>42.93</cell></row><row><cell>Ours g</cell><cell></cell><cell>64.94</cell><cell>56.58</cell><cell>35.89</cell><cell cols="2">23.07 60.86</cell><cell>44.10</cell></row><row><cell>Zero-shot CLIP [49]</cell><cell></cell><cell>61.62</cell><cell>54.81</cell><cell>38.71</cell><cell cols="2">28.05 64.38</cell><cell>46.49</cell></row><row><cell>Linear Probe CLIP [49]</cell><cell></cell><cell>59.75</cell><cell>50.05</cell><cell>26.80</cell><cell cols="2">19.44 47.19</cell><cell>35.87</cell></row><row><cell>CoOp [78] TaskRes [71]</cell><cell>ResNet-101</cell><cell>66.60 67.70</cell><cell>58.66 59.50</cell><cell>39.08 41.70</cell><cell cols="2">28.89 63.00 29.87 68.07</cell><cell>47.41 49.79</cell></row><row><cell>Ours</cell><cell></cell><cell>68.23</cell><cell>59.60</cell><cell>40.83</cell><cell cols="2">28.77 67.13</cell><cell>49.08</cell></row><row><cell>Ours g</cell><cell></cell><cell>67.87</cell><cell>59.50</cell><cell>41.60</cell><cell cols="2">30.00 68.10</cell><cell>49.80</cell></row><row><cell>Zero-shot CLIP [49]</cell><cell></cell><cell>62.05</cell><cell>54.79</cell><cell>40.82</cell><cell cols="2">29.57 65.99</cell><cell>47.79</cell></row><row><cell>Linear Probe CLIP [49]</cell><cell></cell><cell>59.58</cell><cell>49.73</cell><cell>28.06</cell><cell cols="2">19.67 47.20</cell><cell>36.17</cell></row><row><cell>CoOp [78] TaskRes [71]</cell><cell>ViT-B/32</cell><cell>66.85 68.20</cell><cell>58.08 59.20</cell><cell>40.44 42.50</cell><cell cols="2">30.62 64.45 31.43 69.33</cell><cell>48.40 50.62</cell></row><row><cell>Ours</cell><cell></cell><cell>68.80</cell><cell>59.00</cell><cell>41.70</cell><cell cols="2">29.57 68.67</cell><cell>49.74</cell></row><row><cell>Ours g</cell><cell></cell><cell>68.47</cell><cell>59.10</cell><cell>42.70</cell><cell cols="2">31.73 69.43</cell><cell>50.74</cell></row><row><cell>Zero-shot CLIP [49]</cell><cell></cell><cell>66.73</cell><cell>60.83</cell><cell>46.15</cell><cell cols="2">47.77 73.96</cell><cell>57.18</cell></row><row><cell>Linear Probe CLIP [49]</cell><cell></cell><cell>65.85</cell><cell>56.26</cell><cell>34.77</cell><cell cols="2">35.68 58.43</cell><cell>46.29</cell></row><row><cell>CoOp [78] TaskRes [71]</cell><cell>ViT-B/16</cell><cell>71.92 73.07</cell><cell>64.18 65.30</cell><cell>46.71 49.13</cell><cell cols="2">48.41 74.32 50.37 77.70</cell><cell>58.41 60.63</cell></row><row><cell>Ours</cell><cell></cell><cell>73.68</cell><cell>65.57</cell><cell>48.57</cell><cell cols="2">49.23 77.20</cell><cell>60.14</cell></row><row><cell>Ours g</cell><cell></cell><cell>73.40</cell><cell>65.60</cell><cell>49.23</cell><cell cols="2">50.57 77.73</cell><cell>60.78</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>.90 88.10 74.93 96.10 78.23 33.73 70.30 67.57 82.57 76.87 75.10 Tip-Adapter [75] 65.44 92.63 88.18 75.75 94.23 78.11 35.86 71.00 66.94 84.94 79.03 75.65 Base 58.18 86.29 85.77 55.61 66.14 77.31 17.28 58.52 42.32 37.56 61.46 58.77 GraphAdapter-T 65.70 93.33 88.57 76.23 96.23 78.63 36.87 71.20 67.57 85.27 78.80 76.22 GraphAdapter-I 63.50 92.10 85.86 71.81 93.24 74.55 30.90 67.39 61.75 78.79 74.40 72.21 GraphAdapter-T&amp;I 65.72 93.37 88.59 76.20 96.13 78.40 36.93 71.30 67.90 85.55 78.67 76.25</figDesc><table><row><cell>Method</cell><cell>Dataset</cell><cell>ImageNet</cell><cell>Caltech101</cell><cell>OxfordPets</cell><cell>StanfordCars</cell><cell>Flowers102</cell><cell>Food101</cell><cell>FGVCAircraft</cell><cell>SUN397</cell><cell>DTD</cell><cell>EuroSAT</cell><cell>UCF101</cell><cell>Average</cell></row><row><cell cols="14">CLIP-Adapter [17] 63.59 92.49 87.84 74.01 93.90 78.25 32.10 69.55 65.96 84.43 76.76 74.44</cell></row><row><cell>CoOp [78]</cell><cell></cell><cell cols="12">62.95 91.83 87.01 73.36 94.51 74.67 31.26 69.26 63.58 83.53 75.71 73.42</cell></row><row><cell cols="2">TaskRes [71]</cell><cell cols="2">64.75 92</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>The ablation studies for two coefficients ? and ?.</figDesc><table><row><cell>? (? = 0.6) ImageNet ? (? = 0.7)</cell><cell>0.0 64.30 65.1 65.50 65.70 64.95 0.3 0.5 0.7 1.0 0.5 0.6 0.7 0.8 0.9</cell><cell>Learnable Learnable 64.35</cell><cell>Accuracy (%)</cell><cell>60 65 70</cell><cell>RN50</cell><cell>RN101</cell><cell>ViT-B/32 Zero-shot CoOp CLIP-Adapter</cell><cell>ViT-B/16 Tip-Adapter-F TaskRes GraphAdapter</cell></row><row><cell>ImageNet</cell><cell>65.30 65.70 65.37 64.94 63.03</cell><cell>65.42</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>The experiments for the applicability of our GraphAdapter. For Cafo<ref type="bibr" target="#b73">[74]</ref>, we incorporate our GraphAdapter into the textual classifier. Notably, the TaskRes* exploits the enhanced base classifier. Therefore, TaskRes* + Ours denotes that TaskRes* replace the task residual with our proposed GraphAdapter.</figDesc><table><row><cell>Methods</cell><cell cols="2">1-shot 2-shot 4-shot 8-shot 16-shot</cell></row><row><cell>CaFo [74]</cell><cell>63.80 64.34 65.64 66.86</cell><cell>68.79</cell></row><row><cell>+Ours</cell><cell>63.81 64.97 66.17 67.68</cell><cell>69.30</cell></row><row><cell cols="2">TaskRes* [71] 61.43 62.17 62.93 64.03</cell><cell>64.75</cell></row><row><cell>+Ours</cell><cell>61.73 62.53 63.47 64.57</cell><cell>65.80</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>A numerical comparison between our GraphAdapter and the state-of-the-art methods.</figDesc><table><row><cell>Methods</cell><cell>Setting</cell><cell>Caltech101</cell><cell>DTD</cell><cell>EuroSAT</cell><cell>FGVCAircraft</cell><cell>Flowers102</cell><cell>Food101</cell><cell>ImageNet</cell><cell>OxfordPets</cell><cell>StanfordCars</cell><cell>SUN397</cell><cell>UCF101</cell><cell>Avg.</cell></row><row><cell>Zero-shot CLIP [49]</cell><cell></cell><cell>86.29</cell><cell>42.32</cell><cell>37.56</cell><cell>17.28</cell><cell>66.14</cell><cell>77.31</cell><cell>58.18</cell><cell>85.77</cell><cell>55.61</cell><cell>58.52</cell><cell>61.46</cell><cell>58.77</cell></row><row><cell>CoOp [78]</cell><cell></cell><cell>87.53</cell><cell>44.39</cell><cell>50.63</cell><cell>9.64</cell><cell>68.12</cell><cell>74.32</cell><cell>57.15</cell><cell>85.89</cell><cell>55.59</cell><cell>60.29</cell><cell>61.92</cell><cell>59.59</cell></row><row><cell>CLIP-Adapter [17]</cell><cell></cell><cell>88.60</cell><cell>45.80</cell><cell>61.40</cell><cell>17.49</cell><cell>73.49</cell><cell>76.82</cell><cell>61.20</cell><cell>85.99</cell><cell>55.13</cell><cell>61.30</cell><cell>62.20</cell><cell>62.67</cell></row><row><cell>Tip-Adapter-F [75]</cell><cell>1-shot</cell><cell>88.80</cell><cell>50.49</cell><cell>50.34</cell><cell>19.01</cell><cell>81.17</cell><cell>76.22</cell><cell>60.88</cell><cell>86.04</cell><cell>56.78</cell><cell>61.23</cell><cell>66.19</cell><cell>63.38</cell></row><row><cell>TaskRes [71]</cell><cell></cell><cell>88.80</cell><cell>50.17</cell><cell>61.27</cell><cell>21.20</cell><cell>78.77</cell><cell>74.03</cell><cell>61.43</cell><cell>83.50</cell><cell>58.77</cell><cell>61.93</cell><cell>64.57</cell><cell>64.04</cell></row><row><cell>Ours (w/ Std)</cell><cell></cell><cell cols="12">88.90 (?0.22) (?1.48) (?1.96) (?0.25) (?0.90) (?0.14) (?0.09) (?1.02) (?0.45) (?0.26) (?0.59) (?0.34) 51.77 63.30 20.93 79.98 75.43 61.50 84.40 59.70 61.93 64.93 64.80</cell></row><row><cell>Zero-shot CLIP [49]</cell><cell></cell><cell>86.29</cell><cell>42.32</cell><cell>37.56</cell><cell>17.28</cell><cell>66.14</cell><cell>77.31</cell><cell>58.18</cell><cell>85.77</cell><cell>55.61</cell><cell>58.52</cell><cell>61.46</cell><cell>58.77</cell></row><row><cell>CoOp [78]</cell><cell></cell><cell>87.93</cell><cell>45.15</cell><cell>61.50</cell><cell>18.68</cell><cell>77.51</cell><cell>72.49</cell><cell>57.81</cell><cell>82.64</cell><cell>58.28</cell><cell>59.48</cell><cell>64.09</cell><cell>62.32</cell></row><row><cell>CLIP-Adapter [17]</cell><cell></cell><cell>89.37</cell><cell>51.48</cell><cell>63.90</cell><cell>20.10</cell><cell>81.61</cell><cell>77.22</cell><cell>61.52</cell><cell>86.73</cell><cell>58.74</cell><cell>63.29</cell><cell>67.12</cell><cell>65.55</cell></row><row><cell>Tip-Adapter-F [75]</cell><cell>2-shot</cell><cell>89.61</cell><cell>55.32</cell><cell>64.76</cell><cell>21.76</cell><cell>85.40</cell><cell>77.05</cell><cell>61.57</cell><cell>86.06</cell><cell>61.13</cell><cell>63.19</cell><cell>68.99</cell><cell>66.80</cell></row><row><cell>TaskRes [71]</cell><cell></cell><cell>90.13</cell><cell>54.53</cell><cell>65.77</cell><cell>23.07</cell><cell>85.63</cell><cell>75.30</cell><cell>62.17</cell><cell>84.43</cell><cell>62.77</cell><cell>64.33</cell><cell>69.10</cell><cell>67.02</cell></row><row><cell>Ours (w/ Std)</cell><cell></cell><cell cols="12">90.20 (?0.22) (?1.56) (?1.57) (?0.65) (?0.25) (?0.12) (?0.17) (?0.99) (?0.12) (?0.33) (?0.42) (?0.31) 55.75 67.27 23.80 85.63 76.27 62.32 86.30 63.23 64.60 69.47 67.71</cell></row><row><cell>Zero-shot CLIP [49]</cell><cell></cell><cell>86.29</cell><cell>42.32</cell><cell>37.56</cell><cell>17.28</cell><cell>66.14</cell><cell>77.31</cell><cell>58.18</cell><cell>85.77</cell><cell>55.61</cell><cell>58.52</cell><cell>61.46</cell><cell>58.77</cell></row><row><cell>CoOp [78]</cell><cell></cell><cell>89.55</cell><cell>53.49</cell><cell>70.18</cell><cell>21.87</cell><cell>86.20</cell><cell>73.33</cell><cell>59.99</cell><cell>86.70</cell><cell>62.62</cell><cell>63.47</cell><cell>67.03</cell><cell>66.77</cell></row><row><cell>CLIP-Adapter [17]</cell><cell></cell><cell>89.98</cell><cell>56.86</cell><cell>73.38</cell><cell>22.59</cell><cell>87.17</cell><cell>77.92</cell><cell>61.84</cell><cell>87.46</cell><cell>62.45</cell><cell>65.96</cell><cell>69.05</cell><cell>68.61</cell></row><row><cell>Tip-Adapter-F [75]</cell><cell>4-shot</cell><cell>90.87</cell><cell>60.25</cell><cell>69.66</cell><cell>26.39</cell><cell>89.53</cell><cell>77.46</cell><cell>62.62</cell><cell>86.46</cell><cell>64.86</cell><cell>65.88</cell><cell>72.71</cell><cell>69.70</cell></row><row><cell>TaskRes [71]</cell><cell></cell><cell>90.63</cell><cell>59.50</cell><cell>72.97</cell><cell>24.83</cell><cell>89.50</cell><cell>76.23</cell><cell>62.93</cell><cell>86.27</cell><cell>66.50</cell><cell>66.67</cell><cell>69.70</cell><cell>69.61</cell></row><row><cell>Ours (w/ Std)</cell><cell></cell><cell cols="12">90.97 (?0.05) (?0.39) (?1.37) (?0.29) (?0.19) (?0.26) (?0.19) (?1.47) (?0.29) (?0.28) (?0.16) (?0.27) 59.63 75.20 26.97 89.90 76.77 63.12 86.57 66.53 66.70 71.47 70.35</cell></row><row><cell>Zero-shot CLIP [49]</cell><cell></cell><cell>86.29</cell><cell>42.32</cell><cell>37.56</cell><cell>17.28</cell><cell>66.14</cell><cell>77.31</cell><cell>58.18</cell><cell>85.77</cell><cell>55.61</cell><cell>58.52</cell><cell>61.46</cell><cell>58.77</cell></row><row><cell>CoOp [78]</cell><cell></cell><cell>90.21</cell><cell>59.97</cell><cell>76.73</cell><cell>26.13</cell><cell>91.18</cell><cell>71.82</cell><cell>61.56</cell><cell>85.32</cell><cell>68.43</cell><cell>65.52</cell><cell>71.94</cell><cell>69.89</cell></row><row><cell>CLIP-Adapter [17] Tip-Adapter-F [75]</cell><cell>8-shot</cell><cell>91.40 91.70</cell><cell>61.00 62.93</cell><cell>77.93 79.33</cell><cell>26.25 30.62</cell><cell>91.72 91.00</cell><cell>78.04 77.90</cell><cell>62.68 64.15</cell><cell>87.65 88.28</cell><cell>67.89 69.51</cell><cell>67.50 69.23</cell><cell>73.30 74.76</cell><cell>71.40 72.67</cell></row><row><cell>TaskRes [71]</cell><cell></cell><cell>92.23</cell><cell>64.23</cell><cell>78.07</cell><cell>29.50</cell><cell>94.30</cell><cell>76.90</cell><cell>64.03</cell><cell>87.07</cell><cell>70.57</cell><cell>68.70</cell><cell>74.77</cell><cell>72.76</cell></row><row><cell>Ours (w/ Std)</cell><cell></cell><cell cols="12">92.45 (?0.38) (?0.34) (?1.87) (?0.40) (?0.12) (?0.19) (?0.08) (?0.26) (?0.12) (?0.12) (?0.45) (?0.29) 64.50 80.17 31.37 94.07 77.73 64.23 87.63 70.53 68.97 75.73 73.40</cell></row><row><cell>Zero-shot CLIP [49]</cell><cell></cell><cell>86.29</cell><cell>42.32</cell><cell>37.56</cell><cell>17.28</cell><cell>66.14</cell><cell>77.31</cell><cell>58.18</cell><cell>85.77</cell><cell>55.61</cell><cell>58.52</cell><cell>61.46</cell><cell>58.77</cell></row><row><cell>CoOp [78]</cell><cell></cell><cell>91.83</cell><cell>63.58</cell><cell>83.53</cell><cell>31.26</cell><cell>94.51</cell><cell>74.67</cell><cell>62.95</cell><cell>87.01</cell><cell>73.36</cell><cell>69.26</cell><cell>75.71</cell><cell>73.42</cell></row><row><cell>CLIP-Adapter [17]</cell><cell></cell><cell>92.49</cell><cell>65.96</cell><cell>84.43</cell><cell>32.10</cell><cell>93.90</cell><cell>78.25</cell><cell>63.59</cell><cell>87.84</cell><cell>74.01</cell><cell>69.55</cell><cell>76.76</cell><cell>74.44</cell></row><row><cell>Tip-Adapter-F [75]</cell><cell>16-shot</cell><cell>92.63</cell><cell>66.94</cell><cell>84.94</cell><cell>35.86</cell><cell>94.23</cell><cell>78.11</cell><cell>65.44</cell><cell>88.18</cell><cell>75.75</cell><cell>71.00</cell><cell>79.03</cell><cell>75.65</cell></row><row><cell>TaskRes [71]</cell><cell></cell><cell>92.90</cell><cell>67.57</cell><cell>82.57</cell><cell>33.73</cell><cell>96.10</cell><cell>78.23</cell><cell>64.75</cell><cell>88.10</cell><cell>74.93</cell><cell>70.30</cell><cell>76.87</cell><cell>75.10</cell></row><row><cell>Ours (w/ Std)</cell><cell></cell><cell cols="12">93.33 (?0.08) (?0.09) (?0.29) (?0.50) (?0.16) (?0.08) (?0.08) (?0.51) (?0.17) (?0.08) (?0.26) (?0.11) 67.57 85.27 36.87 96.23 78.63 65.70 88.57 76.23 71.20 78.80 76.22</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>The number of classes and the used prompt temple for each dataset.</figDesc><table><row><cell>Datasets</cell><cell># Classes</cell><cell>Prompt Templet</cell></row><row><cell>Caltech101 [16]</cell><cell>100</cell><cell>"a photo of a [class]."</cell></row><row><cell>DTD [11]</cell><cell>47</cell><cell>"[class] texture."</cell></row><row><cell>EuroSAT</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>A comparison between our GraphAdapter and existing methods on time and model complexity.</figDesc><table><row><cell>Tunable Parameters (M)</cell><cell>0.008</cell><cell>0.524</cell><cell cols="3">16.384 1.024 4.145</cell></row><row><cell>GFlops</cell><cell cols="2">1943.12 1959.44</cell><cell>5.43</cell><cell>5.42</cell><cell>5.42</cell></row><row><cell>Training time (one epoch) (s)</cell><cell>40.91</cell><cell>45.71</cell><cell cols="3">12.36 13.64 23.29</cell></row><row><cell>Inference time (s/100)</cell><cell>119.64</cell><cell>275.22</cell><cell>51.03</cell><cell>4.89</cell><cell>4.91</cell></row><row><cell>Memory Cost (Training)</cell><cell>18.907</cell><cell>9.257</cell><cell cols="3">4.313 6.227 10.75</cell></row><row><cell>Memory Cost (Inference)</cell><cell>7.403</cell><cell>7.615</cell><cell cols="3">4.161 6.225 4.433</cell></row><row><cell>Performance</cell><cell>62.95</cell><cell>63.59</cell><cell cols="3">65.44 64.75 65.70</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Applicability</head><p>To validate the applicability of our GraphAdapter, we select two state-of-the-art adapter-style works, including CaFo <ref type="bibr" target="#b73">[74]</ref> and TaskRes* <ref type="bibr" target="#b70">[71]</ref>. Here, CaFo <ref type="bibr" target="#b73">[74]</ref> incorporates diverse prior knowledge from large pre-trained vision and language models, including DINO's vision-contrastive knowledge, GPT-3's language-generative knowledge, and DALLE's generative capability. The adapting strategy of CaFo <ref type="bibr" target="#b73">[74]</ref> is from the Tip-Adapter <ref type="bibr" target="#b74">[75]</ref>. The TaskRes* denotes the enhanced version of TaskRes <ref type="bibr" target="#b70">[71]</ref>, which exploits the enhanced base classifier instead of the original classifier from CLIP <ref type="bibr" target="#b48">[49]</ref>.</p><p>For CaFo <ref type="bibr" target="#b73">[74]</ref>, we directly incorporate our GraphAdapter into the textual classifier. For TaskRes* <ref type="bibr" target="#b70">[71]</ref>, we replace the task residual with our proposed GraphAdapter and maintain its enhanced textual branch from CLIP. The experimental results on ImageNet <ref type="bibr" target="#b11">[12]</ref> are shown in Table <ref type="table">4</ref>. We can observe that our GraphAdapter can consistently increase the performance of CaFo <ref type="bibr" target="#b73">[74]</ref> and TaskRes* <ref type="bibr" target="#b70">[71]</ref> on few-shot learning with all 1-/2-/4-/8-/16-shots settings. Particularly, on the 16-shot setting, ours improves CaFo <ref type="bibr" target="#b73">[74]</ref> by 0.51%, and TaskRes* by 1.15%, which validates the powerful applicability of our GraphAdapter. Overall, our GraphAdapter is complementary to these prior-augmented methods, and can obtain better performance by integrating ours into them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B More Experimental Results</head><p>We present the numerical results of "Figure <ref type="figure">3</ref> in the main text" as Table <ref type="table">5</ref>. We compare our GraphAdapter with the state-of-the-art works, including the prompt-based method CoOp <ref type="bibr" target="#b77">[78]</ref>, and adapter-style methods, i.e., CLIP-Adapter <ref type="bibr" target="#b16">[17]</ref>, Tip-Adapter-F <ref type="bibr" target="#b74">[75]</ref>, and TaskRes <ref type="bibr" target="#b70">[71]</ref>. Here, the performance of Tip-Adapter-F is reproduced by <ref type="bibr" target="#b70">[71]</ref>, which aims to ensure a fair comparison with</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Vlmo: Unified vision-language pre-training with mixture-of-modality-experts</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">K</forename><surname>Mohammed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Som</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="32897" to="32912" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Food-101-mining discriminative components with random forests</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bossard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2014: 13th European Conference</title>
		<meeting><address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">September 6-12, 2014. 2014</date>
			<biblScope unit="page" from="446" to="461" />
		</imprint>
	</monogr>
	<note>Proceedings, Part VI 13</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9650" to="9660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Plot: Prompt learning with optimal transport for vision-language models</title>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Knowledge graph transfer network for few-shot recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="10575" to="10582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning semantic-specific graph representation for multi-label image recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="522" to="531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Knowledge-embedded routing network for scene graph generation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6163" to="6171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Debiasing vision-language models via biased prompts</title>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.00070</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Describing textures in the wild</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3606" to="3613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Neiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.10862</idno>
		<title level="m">Hypergraph pre-training with graph neural networks</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2004 conference on computer vision and pattern recognition workshop</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="178" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.04544</idno>
		<title level="m">Clip-adapter: Better vision-language models with feature adapters</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Finetune like you pretrain: Improved finetuning of zero-shot vision models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Raghunathan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.00638</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification</title>
		<author>
			<persName><forename type="first">P</forename><surname>Helber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bischke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dengel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Borth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2217" to="2226" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The many faces of robustness: A critical analysis of out-of-distribution generalization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dorundo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Parajuli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8340" to="8349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Natural adversarial examples</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="15262" to="15271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Parameter-efficient transfer learning for nlp</title>
		<author>
			<persName><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Giurgiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Morrone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>De Laroussilhe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gesmundo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Attariyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2790" to="2799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Unsupervised prompt learning for vision-language models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.03649</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Scaling up visual and vision-language representation learning with noisy text supervision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4904" to="4916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">U</forename><surname>Khattak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rasheed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><surname>Maple</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.03117</idno>
		<title level="m">Multi-modal prompt learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision workshops</title>
		<meeting>the IEEE international conference on computer vision workshops</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="554" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Kumari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.13516</idno>
		<title level="m">Ablating concepts in text-to-image diffusion models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Kumari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.04488</idno>
		<title level="m">Multi-concept customization of text-to-image diffusion</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Graph learning regularization and transfer learning for few-shot event detection</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">D</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dernoncourt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2172" to="2176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03557</idno>
		<title level="m">Visualbert: A simple and performant baseline for vision and language</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Graph signal processing, graph neural network and graph learning on biological data: a systematic review</title>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Radfar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Marendy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>O'brien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Casillas-Espinosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Reviews in Biomedical Engineering</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Few-shot real image restoration via distortionrelation guided transfer learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.13078</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Scaling &amp; shifting your features: A new baseline for efficient model tuning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing</title>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Prompt generation networks for efficient adaptation of frozen vision transformers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Loedeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Stol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.06466</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Prompt distribution learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="5206" to="5215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Prediction calibration for generalized few-shot semantic segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.5151</idno>
		<title level="m">Fine-grained visual classification of aircraft</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A survey on visual transfer learning using knowledge graphs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Monka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Halilaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rettinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Semantic Web</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="477" to="510" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Glide: Towards photorealistic image generation and editing with text-guided diffusion models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcgrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.10741</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName><forename type="first">M.-E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth Indian Conference on Computer Vision, Graphics &amp; Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="page" from="722" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Svl-adapter: Self-supervised adapter for vision-language pretrained models</title>
		<author>
			<persName><forename type="first">O</forename><surname>Pantazis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.03794</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Cats and dogs</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="3498" to="3505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Styleclip: Text-driven manipulation of stylegan imagery</title>
		<author>
			<persName><forename type="first">O</forename><surname>Patashnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2085" to="2094" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Hierarchical text-conditional image generation with clip latents</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.06125</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Denseclip: Languageguided dense prediction with context-aware prompting</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="18082" to="18091" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Do imagenet classifiers generalize to imagenet</title>
		<author>
			<persName><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Shankar</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5389" to="5400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Stochastic graph as a model for social networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rezvanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Meybodi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers in Human Behavior</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="621" to="640" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10684" to="10695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Seale</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<title level="m">Continual diffusion: Continual customization of text-to-image diffusion with c-lora</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">2304</biblScope>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Testtime prompt tuning for zero-shot generalization in vision-language models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.07511</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08530</idno>
		<title level="m">Vl-bert: Pre-training of generic visual-linguistic representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Vl-adapter: Parameter-efficient transfer learning for visionand-language tasks</title>
		<author>
			<persName><forename type="first">Y.-L</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="5227" to="5237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><surname>Lxmert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.07490</idno>
		<title level="m">Learning cross-modality encoder representations from transformers</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Motionclip: Exposing human motion generation to clip space</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tevet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hertz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Bermano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2022: 17th European Conference</title>
		<meeting><address><addrLine>Tel Aviv, Israel</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">October 23-27, 2022. 2022</date>
			<biblScope unit="page" from="358" to="374" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XXII</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Learning robust global representations by penalizing local predictive power</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.08472</idno>
		<title level="m">Actionclip: A new paradigm for video action recognition</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.02982</idno>
		<title level="m">Clip-guided prototype modulating for few-shot action recognition</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.10904</idno>
		<title level="m">Simvlm: Simple visual language model pretraining with weak supervision</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Wasim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Naseer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.03307</idno>
		<title level="m">Vita-clip: Video and text adaptive clip via multimodal prompting</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Hierarchical relational learning for few-shot knowledge graph completion</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rajaratnam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.01205</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Sun database: Large-scale scene recognition from abbey to zoo</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE computer society conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="3485" to="3492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">One-shot relational learning for knowledge graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.09040</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Dmh-fsl: Dual-modal hypergraph for few-shot learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Processing Letters</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1317" to="1332" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.10277</idno>
		<title level="m">Task residual for tuning vision-language models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Vision-language models for vision tasks: A survey</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.00685</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Vinvl: Revisiting visual representations in vision-language models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5579" to="5588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Prompt, generate, then cache: Cascade of foundation models makes strong few-shot learners</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.02151</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Tip-adapter: Trainingfree adaption of clip for few-shot classification</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2022: 17th European Conference</title>
		<meeting><address><addrLine>Tel Aviv, Israel</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">October 23-27, 2022. 2022</date>
			<biblScope unit="page" from="493" to="510" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XXXV</note>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Prompting through prototype: A prototype-based prompt learning on pretrained vision-language models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.10841</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Conditional prompt learning for vision-language models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="16816" to="16825" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Learning to prompt for vision-language models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2337" to="2348" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Not all features matter: Enhancing few-shot clip with adaptive prior refinement</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.01195</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
