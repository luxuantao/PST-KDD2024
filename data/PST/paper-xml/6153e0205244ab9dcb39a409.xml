<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pythia: A Customizable Hardware Prefetching Framework Using Online Reinforcement Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-12-19">19 Dec 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Rahul</forename><surname>Bera</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ETH ZÃ¼rich</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Konstantinos</forename><surname>Kanellopoulos</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ETH ZÃ¼rich</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anant</forename><forename type="middle">V</forename><surname>Nori</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Processor Architecture Research Labs</orgName>
								<orgName type="institution">Intel Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Taha</forename><surname>Shahroodi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ETH ZÃ¼rich</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sreenivas</forename><surname>Subramoney</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Processor Architecture Research Labs</orgName>
								<orgName type="institution">Intel Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ETH ZÃ¼rich</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Pythia: A Customizable Hardware Prefetching Framework Using Online Reinforcement Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-12-19">19 Dec 2021</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3466752.3480114</idno>
					<idno type="arXiv">arXiv:2109.12021v4[cs.AR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Past research has proposed numerous hardware prefetching techniques, most of which rely on exploiting one specific type of program context information (e.g., program counter, cacheline address, or delta between cacheline addresses) to predict future memory accesses. These techniques either completely neglect a prefetcher's undesirable effects (e.g., memory bandwidth usage) on the overall system, or incorporate system-level feedback as an afterthought to a system-unaware prefetch algorithm. We show that prior prefetchers often lose their performance benefit over a wide range of workloads and system configurations due to their inherent inability to take multiple different types of program context and system-level feedback information into account while prefetching. In this paper, we make a case for designing a holistic prefetch algorithm that learns to prefetch using multiple different types of program context and system-level feedback information inherent to its design.</p><p>To this end, we propose Pythia, which formulates the prefetcher as a reinforcement learning agent. For every demand request, Pythia observes multiple different types of program context information to make a prefetch decision. For every prefetch decision, Pythia receives a numerical reward that evaluates prefetch quality under the current memory bandwidth usage. Pythia uses this reward to reinforce the correlation between program context information and prefetch decision to generate highly accurate, timely, and systemaware prefetch requests in the future. Our extensive evaluations using simulation and hardware synthesis show that Pythia outperforms two state-of-the-art prefetchers (MLOP and Bingo) by 3.4% and 3.8% in single-core, 7.7% and 9.6% in twelve-core, and 16.9% and 20.2% in bandwidth-constrained core configurations, while incurring only 1.03% area overhead over a desktop-class processor and no software changes in workloads. The source code of Pythia can be freely downloaded from https://github.com/CMU-SAFARI/Pythia.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Prefetching is a well-studied speculation technique that predicts the addresses of long-latency memory requests and fetches the corresponding data from main memory to on-chip caches before the program executing on the processor demands it. A program's repeated accesses over its data structures create patterns in its memory request addresses. A prefetcher tries to identify such memory access patterns from past memory requests to predict the addresses of future memory requests. To quickly identify a memory access pattern, a prefetcher typically uses some program context information to examine only a subset of memory requests. We call this program context a feature. The prefetcher associates a memory access pattern with a feature and generates prefetches following the same pattern when the feature reoccurs during program execution.</p><p>Past research has proposed numerous prefetchers that consistently pushed the limits of prefetch coverage (i.e., the fraction of memory requests predicted by the prefetcher) and accuracy (i.e., the fraction of prefetch requests that are actually demanded by the program) by exploiting various program features, e.g., program counter (PC), cacheline address (Address), page offset of a cacheline (Offset), or a simple combination of such features using simple operations like concatenation (+) <ref type="bibr">[25, 27, 30, 32, 35, 53, 55, 56, 65, 73, 78-80, 90, 103, 106, 111, 112, 122, 123]</ref>. For example, a PC-based stride prefetcher <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b72">73]</ref> uses PC as the feature to learn the constant stride between two consecutive memory accesses caused by the same PC. VLDP <ref type="bibr" target="#b111">[112]</ref> and SPP <ref type="bibr" target="#b77">[78]</ref> use a sequence of cacheline address deltas as the feature to predict the next cacheline address delta. Kumar and Wilkerson <ref type="bibr" target="#b79">[80]</ref> use PC+Address of the first access in a memory region as the feature to predict the spatial memory access footprint in the entire memory region. SMS <ref type="bibr" target="#b121">[122]</ref> empirically finds PC+Offset of the first access in a memory region to be a better feature to predict the memory access footprint. Bingo <ref type="bibr" target="#b26">[27]</ref> combines the features from <ref type="bibr" target="#b79">[80]</ref> and SMS and uses PC+Address and PC+Offset as its features.</p><p>Accurate and timely prefetch requests reduce the long memory access latency experienced by the processor, thereby improving overall system performance. However, speculative prefetch requests can cause undesirable effects on the system (e.g., increased memory bandwidth consumption, cache pollution, memory access interference, etc.), which can reduce or negate the performance improvement gained by hiding memory access latency <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b122">123]</ref>. Thus, a good prefetcher aims to maximize its benefits while minimizing its undesirable effects on the system.</p><p>Even though there is a large number of prefetchers proposed in the literature, we observe three key shortcomings in almost every prior prefetcher design that significantly limits its performance benefits over a wide range of workloads and system configurations:</p><p>(1) the use of mainly a single program feature for prefetch prediction, (2) lack of inherent system awareness, and (3) lack of ability to customize the prefetcher design to seamlessly adapt to a wide range of workload and system configurations.</p><p>Single-feature prefetch prediction. Almost every prior prefetch-er relies on only one program feature to correlate with the program memory access pattern and generate prefetch requests <ref type="bibr">[25, 30, 32, 35, 53, 55, 56, 65, 73, 78-80, 90, 103, 106, 111, 112, 122, 123]</ref>. As a result, a prefetcher typically provides good (or poor) performance benefits in mainly those workloads where the correlation between the feature used by the prefetcher and program's memory access pattern is dominantly present (or absent). To demonstrate this, we show the coverage and overpredictions (i.e., prefetched memory requests that do not get demanded by the processor) of two recently proposed prefetchers, SPP <ref type="bibr" target="#b77">[78]</ref> and Bingo <ref type="bibr" target="#b26">[27]</ref>, and our new proposal Pythia ( Â§4) for six example workloads ( Â§5 discusses our experimental methodology) in Fig. <ref type="figure" target="#fig_0">1(a)</ref>. Fig. <ref type="figure" target="#fig_0">1(b)</ref> shows the performance of SPP, Bingo and Pythia on the same workloads. As we see in Fig. <ref type="figure" target="#fig_0">1</ref>(a), Bingo provides higher prefetch coverage than SPP in sphinx3, PARSEC-Canneal, and PARSEC-Facesim, where the correlation exists between the first access in a memory region and the other accesses in the same region. As a result, Bingo performs better than SPP in these workloads (Fig. <ref type="figure" target="#fig_0">1(b)</ref>). In contrast, for workloads like GemsFDTD that have regular access patterns within a physical page, SPP's sequence of deltas feature provides better coverage and performance than Bingo. performance of two recently-proposed prefetchers, SPP <ref type="bibr" target="#b77">[78]</ref> and Bingo <ref type="bibr" target="#b26">[27]</ref>, and our new proposal, Pythia. Lack of inherent system awareness. All prior prefetchers either completely neglect their undesirable effects on the system (e.g., memory bandwidth usage, cache pollution, memory access interference, system energy consumption, etc.) <ref type="bibr">[25, 27, 32, 35, 53, 55, 56, 65, 73, 78-80, 90, 103, 106, 111, 112, 122]</ref> or incorporate system awareness as an afterthought (i.e., a separate control component) to the underlying system-unaware prefetch algorithm <ref type="bibr">[30, 34, 47-49, 81, 82, 85, 86, 95, 123, 144]</ref>. Due to the lack of inherent system awareness, a prefetcher often loses its performance gain in resource-constrained scenarios. For example, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>(a), Bingo achieves similar prefetch coverage in Ligra-CC as compared to PARSEC-Canneal, while generating significantly lower overpredictions in Ligra-CC than PARSEC-Canneal. However, Bingo loses performance in Ligra-CC by 1.9% compared to a no-prefetching baseline, whereas it improves performance by 6.4% in PARSEC-Canneal (Fig. <ref type="figure" target="#fig_0">1(b)</ref>). This contrasting outcome is due to Bingo's lack of awareness of the memory bandwidth usage. Without prefetching, Ligra-CC consumes higher memory bandwidth than PARSEC-Canneal. As a result, each overprediction made by Bingo in Ligra-CC wastes more precious memory bandwidth and is more detrimental to performance than that in PARSEC-Canneal.</p><p>Lack of online prefetcher design customization. The high design complexity of architecting a multi-feature, system-aware prefetcher has traditionally compelled architects to statically select only one program feature at design time. With every new prefetcher, architects design new rigid hardware structures to exploit the selected program feature. To exploit a new program feature for higher performance benefits, one must design a new prefetcher from scratch and extensively evaluate and verify it both in presilicon and post-silicon realization. Due to the rigid design-time decisions, the hardware structures proposed by prior prefetchers cannot be customized online in silicon either to exploit any other program feature or to change the prefetcher's objective (e.g., to increase/decrease coverage, accuracy, or timeliness) so that it can seamlessly adapt to varying workloads and system configurations.</p><p>Our goal in this work is to design a single prefetching framework that (1) can holistically learn to prefetch using both multiple different types of program features and system-level feedback information that is inherent to the design, and (2) can be easily customized in silicon via simple configuration registers to exploit different types of program features and/or to change the objective of the prefetcher (e.g., increasing/decreasing coverage, accuracy, or timeliness) without any changes to the underlying hardware.</p><p>Key ideas. To this end, we propose Pythia,<ref type="foot" target="#foot_0">1</ref> which formulates hardware prefetching as a reinforcement learning problem. Reinforcement learning (RL) <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b123">124]</ref> is a machine learning paradigm that studies how an autonomous agent can learn to take optimal actions that maximizes a reward function by interacting with a stochastic environment. We formulate Pythia as an RL-agent that autonomously learns to prefetch by interacting with the processor and the memory subsystem. For every new demand request, Pythia extracts a set of program features. It uses the set of features as state information to take a prefetch action based on its prior experience. For every prefetch action (including not to prefetch), Pythia receives a numerical reward which evaluates the accuracy and timeliness of the prefetch action given various system-level feedback information. While Pythia's framework is general enough to incorporate any type of system-level feedback information into its decision making, in this paper we demonstrate Pythia using one major system-level information for prefetching: memory bandwidth usage. Pythia uses the reward received for a prefetch action to reinforce the correlations between various program features and the prefetch action and learn from experience how to generate accurate, timely, and system-aware prefetches in the future. The types of program feature used by Pythia and the reward level values can be easily customized in silicon via configuration registers.</p><p>Novelty and Benefits. Pythia's RL-based design approach requires an architect to only specify which of the possible program features might be useful to design a good prefetcher and what performance goal the prefetcher should target, rather than spending time on designing and implementing a new (likely rigid) prefetch algorithm and accompanying rigid hardware that describes precisely how the prefetcher should exploit the selected features to achieve that performance goal. This approach provides two unique advantages over prior prefetching proposals. First, using the RL framework, Pythia can holistically learn to prefetch using both multiple program features and system-level feedback information inherent to its design. Second, Pythia can be easily customized in silicon via simple configuration registers to exploit different types of program features and/or change the objective of the prefetcher. This gives Pythia the unique benefit of providing even higher performance improvements for a wide variety of workloads and changing system configurations, without any changes to the underlying hardware.</p><p>Results Summary. We evaluate Pythia using a diverse set of memory-intensive workloads spanning SPEC CPU2006 <ref type="bibr" target="#b20">[21]</ref>, SPEC CPU2017 <ref type="bibr" target="#b21">[22]</ref>, PARSEC 2.1 <ref type="bibr" target="#b15">[16]</ref>, Ligra <ref type="bibr" target="#b116">[117]</ref>, and Cloudsuite <ref type="bibr" target="#b50">[51]</ref> benchmarks. We demonstrate four key results. First, Pythia outperforms two state-of-the-art prefetchers (MLOP <ref type="bibr" target="#b110">[111]</ref> and Bingo <ref type="bibr" target="#b26">[27]</ref>) by 3.4% and 3.8% in single-core and 7.7% and 9.6% in twelve-core configurations. This is because Pythia generates lower overpredictions, while simultaneously providing higher prefetch coverage than the prior prefetchers. Second, Pythia's performance benefits increase in bandwidth-constrained system configurations. For example, in a server-like configuration, where a core can have only 1 16 Ã— of the bandwidth of a single-channel DDR4-2400 <ref type="bibr" target="#b14">[15]</ref> DRAM controller, Pythia outperforms MLOP and Bingo by 16.9% and 20.2%. Third, Pythia can be customized further via simple configuration registers to target workload suites to provide even higher performance benefits. We demonstrate that by simply changing the numerical rewards, Pythia provides up to 7.8% (1.9% on average) more performance improvement across all Ligra graph processing workloads over the basic Pythia configuration. Fourth, Pythia's performance benefits come with only modest area and power overheads. Our functionally-verified hardware synthesis for Pythia shows that Pythia only incurs an area and power overhead of 1.03% and 0.37% over a 4-core desktop-class processor.</p><p>We make the following contributions in this paper:</p><p>â€¢ We observe three key shortcomings in prior prefetchers that significantly limits their performance benefits: (1) the use of only a single program feature for prefetch prediction, (2) lack of inherent system awareness, and (3) lack of ability to customize the prefetcher design to seamlessly adapt to a wide range of workloads and system configurations. â€¢ We introduce a new prefetcher called Pythia. Pythia formulates the prefetcher as a reinforcement learning (RL) agent, which takes adaptive prefetch decisions by autonomously learning using both multiple program features and systemlevel feedback information inherent to its design ( Â§3.1). â€¢ We provide a low-overhead, practical implementation of Pythia's RL-based algorithm in hardware, which uses no more complex structures than simple tables ( Â§4.2.1). This design can potentially be used for other hardware structures that can benefit from RL principles. â€¢ By extensive evaluation, we show that Pythia outperforms prior state-of-the-art prefetchers over a wide variety of workloads in a wide range of system configurations.</p><p>â€¢ We open source Pythia and all the workload traces used for performance modeling in our GitHub repository: https: //github.com/CMU-SAFARI/Pythia.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>We first briefly review the basics of reinforcement learning <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b123">124]</ref>.</p><p>We then describe why reinforcement learning is a good framework for designing a hardware prefetcher that fits our goals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Reinforcement Learning</head><p>Reinforcement learning (RL) <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b123">124]</ref>, in its simplest form, is the algorithmic approach to learn how to take an action in a given situation to maximize a numerical reward signal. A typical RL system comprises of two main components: the agent and the environment, as shown in Fig. <ref type="figure" target="#fig_1">2</ref>. The agent is the entity that takes actions. the agent resides in the environment and interacts with it in discrete timesteps. At each timestep ğ‘¡, the agent observes the current state of the environment ğ‘† ğ‘¡ and takes action ğ´ ğ‘¡ . Upon receiving the action, the environment transitions to a new state S t+1 , and emits an immediate reward ğ‘… ğ‘¡ +1 , which is immediately or later delivered to the agent. The reward scheme encapsulates the agent's objective and drives the agent towards taking optimal actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Agent</head><p>Environment The policy of the agent dictates it to take a certain action in a given state. The agent's goal is to find the optimal policy that maximizes the cumulative reward collected from the environment over time. The expected cumulative reward by taking an action ğ´ in a given state ğ‘† is defined as the Q-value of the state-action pair (denoted as ğ‘„ (ğ‘†, ğ´)). At every timestep ğ‘¡, the agent iteratively optimizes its policy in two steps: (1) the agent updates the Q-value of a state-action pair using the reward collected in the current timestep, and (2) the agent optimizes its current policy using the newly updated Q-value.</p><formula xml:id="formula_0">State (S t ) State (S t ) Action (A t ) Action (A t ) Reward (R t+1 ) Reward (R t+1 )</formula><p>Updating Q-values. If at a given timestep ğ‘¡, the agent observes a state ğ‘† ğ‘¡ , takes an action ğ´ ğ‘¡ , while the environment transitions to a new state ğ‘† ğ‘¡ +1 and emits a reward ğ‘… ğ‘¡ +1 and the agent takes action ğ´ ğ‘¡ +1 in the new state, the Q-value of the old state-action pair ğ‘„ (ğ‘† ğ‘¡ , ğ´ ğ‘¡ ) is iteratively optimized using the SARSA <ref type="bibr" target="#b107">[108,</ref><ref type="bibr" target="#b123">124]</ref> algorithm, as shown in Eqn. (1):</p><formula xml:id="formula_1">ğ‘„ (ğ‘† ğ‘¡ , ğ´ ğ‘¡ ) â† ğ‘„ (ğ‘† ğ‘¡ , ğ´ ğ‘¡ ) + ğ›¼ [ğ‘… ğ‘¡ +1 + ğ›¾ğ‘„ (ğ‘† ğ‘¡ +1 , ğ´ ğ‘¡ +1 ) âˆ’ ğ‘„ (ğ‘† ğ‘¡ , ğ´ ğ‘¡ )]<label>(1)</label></formula><p>ğ›¼ is the learning rate parameter that controls the convergence rate of Q-values. ğ›¾ is the discount factor, which is used to assign more weight to the immediate reward received by the agent at any given timestep than to the delayed future rewards. A ğ›¾ value closer to 1 gives a "far-sighted" planning capability to the agent, i.e., the agent can trade off a low immediate reward to gain higher rewards in the future. This is particularly useful in creating an autonomous agent that can anticipate the long-term effects of taking an action to optimize its policy that gets closer to optimal over time.</p><p>Optimizing policy. To find a policy that maximizes the cumulative reward collected over time, a purely-greedy agent always exploits the action ğ´ in a given state ğ‘† that provides the highest Q-value ğ‘„ (ğ‘†, ğ´). However, greedy exploitation can leave the stateaction space under-explored. Thus, in order to strike a balance between exploration and exploitation, an ğœ–-greedy agent stochastically takes a random action with a low probability of ğœ– (called exploration rate); otherwise, it selects the action that provides the highest Q-value <ref type="bibr" target="#b123">[124]</ref>.</p><p>In short, the Q-value serves as the foundational cornerstone of reinforcement learning. By iteratively learning Q-values of stateaction pairs, an RL-agent continuously optimizes its policy to take actions that get closer to optimal over time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Why is RL a Good Fit for Prefetching?</head><p>The RL framework has been recently successfully demonstrated to solve complex problems like mastering human-like control on Atari <ref type="bibr" target="#b91">[92]</ref> and Go <ref type="bibr" target="#b117">[118,</ref><ref type="bibr" target="#b118">119]</ref>. We argue that the RL framework is an inherent fit to model a hardware prefetcher for three key reasons.</p><p>Adaptive learning in a complex state space. As we state in Â§1, the benefits of a prefetcher not only depends on its coverage and accuracy but also on its undesirable effects on the system, like memory bandwidth usage. In other words, it is not sufficient for a prefetcher only to make highly accurate predictions. Instead, a prefetcher should be performance-driven. A prefetcher should have the capability to adaptively trade-off coverage for higher accuracy (and vice-versa) depending on its impact on the overall system to provide a robust performance improvement with varying workloads and system configurations. This adaptive and performance-driven nature of prefetching in a complex state space makes RL a good fit for modeling a prefetcher as an autonomous agent that learns to prefetch by interacting with the system.</p><p>Online learning. An RL agent does not require an expensive offline training phase. Instead, it can continuously learn online by iteratively optimizing its policy using the rewards received from the environment. A hardware prefetcher, similar to an RL agent, also needs to continuously learn from the changing workload behavior and system conditions to provide consistent performance benefits. The online learning requirement of prefetching makes RL an inherent fit to model a hardware prefetcher.</p><p>Ease of implementation. Prior works have evaluated many sophisticated machine learning models like simple neural networks <ref type="bibr" target="#b104">[105]</ref>, LSTMs <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b113">114]</ref>, and Graph Neural Networks (GNNs) <ref type="bibr" target="#b115">[116]</ref> as models for hardware prefetching. Even though these techniques show encouraging results in accurately predicting memory accesses, they fall short especially in two major aspects. First, these models' sizes often exceed even the largest caches in traditional processors <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b104">105,</ref><ref type="bibr" target="#b113">114,</ref><ref type="bibr" target="#b115">116]</ref>, making them impractical (or at best very difficult) to implement. Second, due to the vast amount of computation they require for inference, these models' inference latency is much higher than an acceptable latency of a prefetcher at any cache level. On the other hand, we can efficiently implement an RL-based model, as we demonstrate in this paper ( Â§ 4), that can quickly make predictions and can be relatively easily adopted in a real processor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PYTHIA: KEY IDEA</head><p>In this work, we formulate prefetching as a reinforcement learning problem, as shown in Fig. <ref type="figure" target="#fig_2">3</ref>. Specifically, we formulate Pythia as an RL-agent that learns to make accurate, timely, and system-aware prefetch decisions by interacting with the environment, i.e., the processor and the memory subsystem. Each timestep corresponds to a new demand request seen by Pythia. With every new demand request, Pythia observes the state of the processor and the memory subsystem and takes a prefetch action. For every prefetch action (including not to prefetch), Pythia receives a numerical reward that evaluates the accuracy and timeliness of the prefetch action taking into account various system-level feedback information. Pythia's goal is to find the optimal prefetching policy that would maximize the number of accurate and timely prefetch requests, taking system-level feedback information into account. While Pythia's framework is general enough to incorporate any type of system-level feedback into its decision making, in this paper we demonstrate Pythia using memory bandwidth usage as the system-level feedback information. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Formulation of the RL-based Prefetcher</head><p>We formally define the three pillars of our RL-based prefetcher: the state space, the actions, and the reward scheme.</p><p>State. We define the state as a ğ‘˜-dimensional vector of program features.</p><p>ğ‘† â‰¡ {ğœ™ 1 ğ‘† , ğœ™ 2 ğ‘† , . . . , ğœ™ ğ‘˜ ğ‘† }</p><p>(2) Each program feature is composed of at most two components:</p><p>(1) program control-flow component, and (2) program data-flow component. The control-flow component is further made up of simple information like load-PC (i.e., the PC of a load instruction) or branch-PC (i.e., the PC of a branch instruction that immediately precedes a load instruction), and a history that denotes whether this information is extracted only from the current demand request or a series of past demand requests. Similarly, the data-flow component is made up of simple information like cacheline address, physical page number, page offset, cacheline delta, and its corresponding history. Table <ref type="table" target="#tab_0">1</ref> shows some example program features. Although Pythia can theoretically learn to prefetch using any number of such program features, we fix the state-vector dimension (i.e., ğ‘˜) at design time given a limited storage budget in hardware. However, the exact selection of ğ‘˜ program features out of all possible program features is configurable online using simple configuration registers. In Â§4.3.1, we provide an automated feature selection method to find a vector of program features to be used at design time.</p><p>Action. We define the action of the RL-agent as selecting a prefetch offset (i.e., a delta, "O" in Fig. <ref type="figure" target="#fig_2">3</ref>, between the predicted and We further reduce the action space size by fine tuning, as described in Â§4.3.2. A prefetch offset of zero means no prefetch is generated.</p><p>Reward. The reward structure defines the prefetcher's objective. We define five different reward levels as follows.</p><p>â€¢ Accurate and timely (R ğ´ğ‘‡ ). This reward is assigned to an action whose corresponding prefetch address gets demanded after the prefetch fill. â€¢ Accurate but late (R ğ´ğ¿ ). This reward is assigned to an action whose corresponding prefetch address gets demanded before the prefetch fill. â€¢ Loss of coverage (R ğ¶ğ¿ ). This reward is assigned to an action whose corresponding prefetch address is to a different physical page than the demand access that led to the prefetch. â€¢ Inaccurate (R ğ¼ ğ‘ ). This reward is assigned to an action whose corresponding prefetch address does not get demanded in a temporal window. The reward is classified into two sublevels: inaccurate given low bandwidth usage (R ğ¿ ğ¼ ğ‘ ) and inaccurate given high bandwidth usage (R ğ» ğ¼ ğ‘ ). â€¢ No-prefetch (R ğ‘ ğ‘ƒ ). This reward is assigned when Pythia decides not to prefetch. This reward level is also classified into two sub-levels: no-prefetch given low bandwidth usage (R ğ¿ ğ‘ ğ‘ƒ ) and no-prefetch given high bandwidth usage (R ğ» ğ‘ ğ‘ƒ ). By increasing (decreasing) a reward level value, we reinforce (deter) Pythia to collect such rewards from the environment in the future. R ğ´ğ‘‡ and R ğ´ğ¿ are used to guide Pythia to generate more accurate and timely prefetch requests. R ğ¶ğ¿ is used to guide Pythia to generate prefetches within the physical page of the triggering demand request. R ğ¼ ğ‘ and R ğ‘ ğ‘ƒ are used to define Pythia's prefetching strategy with respect to memory bandwidth usage feedback. In Â§4.3.3, we provide an automated method to configure the reward values. The reward values can be easily customized further for target workload suites to extract higher performance gains ( Â§6.6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PYTHIA: DESIGN</head><p>Fig. <ref type="figure" target="#fig_4">4</ref> shows a high-level overview of Pythia. Pythia is mainly comprised of two hardware structures: Q-Value Store (QVStore) and Evaluation Queue (EQ). The purpose of QVStore is to record Qvalues for all state-action pairs that are observed by Pythia. The purpose of EQ is to maintain a first-in-first-out list of Pythia's recently-taken actions.<ref type="foot" target="#foot_1">2</ref> Every EQ entry holds three pieces of information: (1) the taken action, (2) the prefetch address generated for the corresponding action, and (3) a filled bit. A set filled bit indicates that the prefetch request has been filled into the cache.</p><p>For every new demand request, Pythia first checks the EQ with the demanded memory address ( 1 ). If the address is present in the EQ (i.e., Pythia has issued a prefetch request for this address in the past), it signifies that the prefetch action corresponding to the EQ entry has generated a useful prefetch request. As such, Pythia assigns a reward (either R ğ´ğ‘‡ or R ğ´ğ¿ ) to the EQ entry, based on whether or not the EQ entry's filled bit is set. Next, Pythia extracts the state-vector from the attributes of the demand request (e.g., PC, address, cacheline delta, etc.) ( 2 ) and looks up QVStore to find the action with the maximum Q-value for the given statevector ( 3 ). Pythia selects the action with the maximum Q-value to generate prefetch request and issues the request to the memory hierarchy ( 4 ). At the same time, Pythia inserts the selected prefetch action, its corresponding prefetched memory address, and the statevector into EQ ( 5 ). Note that, a no-prefetch action or an action that prefetches an address beyond the current physical page is also inserted into EQ. The reward for such an action is instantaneously assigned to the EQ entry. When an EQ entry gets evicted, the stateaction pair and the reward stored in the evicted EQ entry are used to update the Q-value in the QVStore <ref type="bibr" target="#b5">( 6 )</ref>. For every prefetch fill in cache, Pythia looks up EQ with the prefetch address and sets the filled bit in the matching EQ entry indicating that the prefetch request has been filled into the cache <ref type="bibr" target="#b6">( 7 )</ref>. Pythia uses this filled bit in 1 to classify actions that generated timely or late prefetches.<ref type="foot" target="#foot_2">3</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>!"#$%#&amp;'()*+%,%, -!+.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>!"#$%&amp; '"()"*+</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">RL-based Prefetching Algorithm</head><p>Algorithm 1 shows Pythia's RL-based prefetching algorithm. Initially, all entries in QVStore are reset to the highest possible Q-value ( 1 1âˆ’ğ›¾ ) and the EQ is cleared (lines 2-3). For every demand request to a cacheline address ğ´ğ‘‘ğ‘‘ğ‘Ÿ , Pythia searches for ğ´ğ‘‘ğ‘‘ğ‘Ÿ in EQ (line 6 ğ‘ ğ‘’ğ‘ğ‘Ÿğ‘â„_ğ‘ğ‘›ğ‘‘_ğ‘šğ‘ğ‘Ÿğ‘˜_ğ¸ğ‘„ (ğ´ğ‘‘ğ‘‘ğ‘Ÿ, FILLED) /* For every prefetch fill, search the address in EQ and mark the corresponding EQ entry as filled */ R ğ´ğ¿ ) based on the filled bit in the EQ entry (lines 8-11). Pythia then extracts the state-vector to stochastically select a prefetching action (Sec. 2) that provides the highest Q-value (lines 13-16). Pythia uses the selected action to generate the prefetch request (line 17) and creates a new EQ entry with the current state-vector, the selected action, and its corresponding prefetched address (line 18). In case of a no-prefetch action, or an action that prefetches beyond the current physical page, Pythia immediately assigns the reward to the newly-created EQ entry (lines <ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref>. The EQ entry is then inserted, which evicts an entry from EQ. If the evicted EQ entry does not already have a reward assigned (indicating that the corresponding prefetch address is not demanded by the processor so far), Pythia assigns the reward R ğ» ğ¼ ğ‘ or R ğ¿ ğ¼ ğ‘ based on the current memory bandwidth usage (lines 25). Finally, the Q-value of the evicted state-action pair is updated via the SARSA algorithm (Sec. 2), using the reward stored in the evicted EQ entry and the Q-value of the state-action pair in the head of the EQ-entry (lines 26-29).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Detailed Design of Pythia</head><p>We describe the organization of QVStore ( Â§ 4.2.1), how Pythia searches QVStore to get the action with the maximum Q-value for a given state-vector ( 3 ) ( Â§4.2.2), how Pythia assigns rewards to each taken action and how it updates Q-values ( 6 ) ( Â§4.2.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.2.1</head><p>Organization of QVStore. The purpose of QVStore is to record Q-values for all state-action pairs that Pythia observes. Unlike prior real-world applications of RL <ref type="bibr" target="#b91">[92,</ref><ref type="bibr" target="#b117">118,</ref><ref type="bibr" target="#b118">119]</ref>, which use deep neural networks to approximately store Q-values of every state-action pair, we propose a new, table-based, hierarchical QVStore organization that is custom-designed to our RL-agent.</p><p>Fig. <ref type="figure" target="#fig_5">5</ref>(a) shows the high-level organization of QVStore and how the Q-value is retrieved from QVStore for a given state ğ‘† (which is a k-dimensional vector of program features, {ğœ™ 1 ğ‘† , ğœ™ 2 ğ‘† , . . . , ğœ™ ğ‘˜ ğ‘† }) and an action ğ´. As the state space grows rapidly with the state-vector dimension (ğ‘˜) and the bits used to represent each feature, we employ a hierarchical organization for QVStore. We organize QVStore in ğ‘˜ partitions, each of which we call a vault. Each vault corresponds to one constituent feature of the state-vector and records the Q-values for the feature-action pair, ğ‘„ (ğœ™ ğ‘– ğ‘† , ğ´). During the Q-value retrieval for a given state-action pair ğ‘„ (ğ‘†, ğ´), Pythia queries each vault in parallel to retrieve the Q-values of constituent feature-action pairs ğ‘„ (ğœ™ ğ‘– ğ‘† , ğ´). The final Q-value of the state-action pair ğ‘„ (ğ‘†, ğ´) is computed as the maximum of all constituent feature-action Qvalues, as Eqn. 3 shows). The maximum operation ensures that the state-action Q-value is driven by the constituent feature of the state-vector that has the highest feature-action Q-value. The vault organization enables QVStore to efficiently scale up to higher statevector dimensions: one can increase the state-vector dimension by simply adding a new vault to the QVStore.</p><formula xml:id="formula_2">ğ‘„ (ğ‘†, ğ´) = max ğ‘– âˆˆ(1,ğ‘˜) ğ‘„ (ğœ™ ğ‘– ğ‘† , ğ´)<label>(3)</label></formula><p>Fig. <ref type="figure" target="#fig_5">5</ref>(a) shows the organization of QVStore as a collection of multiple vaults. The purpose of a vault is to record Q-values of all feature-action pairs that Pythia observes for a specific feature type. A vault can be conceptually visualized as a monolithic twodimensional table (as shown in Fig. <ref type="figure" target="#fig_5">5(a)</ref>), indexed by the feature and action values, that stores Q-value for every feature-action pair. However, the key challenge in implementing vault as a monolithic table is that the size of the table increases exponentially with a linear increase in the number of bits used to represent the feature. This not only makes the monolithic table organization impractical for implementation but also increases the design complexity to satisfy its latency and power requirements.</p><formula xml:id="formula_3">â€¦ Vault k MAX (a) Vault 1 Vault 2 State-action Q-value Plane 1 Shift + # + Feature Index Ï† 1 S Ï† 1 S Ï† 2 S Ï† 2 S Ï† k S Ï† k S Program feature Q(Ï† 1 S , A) Q(Ï† 1 S , A) Q(Ï† 2 S , A) Q(Ï† 2 S , A) Q(Ï† k S , A) Q(Ï† k S , A) Feature-action Q-value Q(S, A) Q(S, A) Ï† k S Ï† k S Q(Ï† k S , A) Q(Ï† k S , A) Feature-action Q-value Ï† k S Ï† k S Index Generation Index Generation Index Generation Action (A) Action (A) Action (A) Action (A) (b) (c)</formula><p>One way to address this challenge is to quantize the feature space into a small number of tiles. Even though feature space quantization can achieve a drastic reduction in the monolithic table size, it requires a compromise between the resolution of a feature value and the generalization of feature values. We draw inspiration from tile coding <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b123">124]</ref> to strike a balance between resolution and generalization. Tile coding uses multiple overlapping hash functions to quantize a feature value into smaller tiles. The quantization achieves generalization of similar feature values, whereas multiple hash functions increase resolution to represent a feature value.</p><p>We leverage the idea of tile coding to organize a vault as a collection of ğ‘ small two-dimensional tables, each of which we call a plane. Each plane entry stores a partial Q-value of a featureaction pair. <ref type="foot" target="#foot_3">4</ref> As Fig. <ref type="figure" target="#fig_5">5</ref>(c) shows, to retrieve a feature-action Q-value ğ‘„ (ğœ™ ğ‘– ğ‘† , ğ´), the given feature is first shifted by a shifting constant (which is randomly selected at design time), followed by a hashing to get the feature index for the given plane. This feature index, along with the action index, is used to retrieve the partial Q-value from the plane. The final feature-action Q-value is computed as the sum of all the partial Q-values from all planes, as shown in Fig. <ref type="figure" target="#fig_5">5(b)</ref>. The use of tile coding provides two key advantages to Pythia. First, the tile coding of a feature enables the sharing of partial Q-values between similar feature values, which shortens prefetcher training time. Second, multiple planes reduces the chance of sharing partial Q-values between widely different feature values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.2.2</head><p>Pipelined Organization of QVStore Search. To generate a prefetch request, Pythia has to (1) look up the QVStore with the state-vector extracted from the current demand request, and (2) search for the action that has the maximum state-action Q-value ( 3 in Fig. <ref type="figure" target="#fig_4">4</ref>). As a result, the search operation lies on Pythia's critical path and directly impacts Pythia's prediction latency. To improve the prediction latency, we pipeline the search operation.  The Q-value search operation is implemented in the following way. For a given state-vector, Pythia iteratively retrieves the Qvalue of each action. Pythia also maintains a variable, ğ‘„ ğ‘šğ‘ğ‘¥ , that tracks the maximum Q-value found so far. ğ‘„ ğ‘šğ‘ğ‘¥ gets compared to every retrieved Q-value. The search operation concludes when Q-values for all possible actions have been retrieved. We pipeline the search operation into five stages as Fig. <ref type="figure" target="#fig_7">6</ref> shows. Pythia first computes the index for each plane and each constituent feature of the given state-vector (Stage 0). In Stage 1, Pythia uses the feature indices and an action index to retrieve the partial Q-values from each plane. In Stage 2, Pythia sums up the partial Q-values to get the feature-action Q-value for each constituent feature. In Stage 3, Pythia computes the maximum of all feature-action Q-values to get the state-action Q-value. In Stage 4, the maximum state-action Q-value found so far is compared against the retrieved state-action Q-value, and the maximum Q-value is updated. Stage 2 (i.e., the partial Q-value summation) is the longest stage of the pipeline and thus it dictates the pipeline's throughput. We accurately measure the area and power overhead of the pipelined implementation of the search operation by modeling Pythia using Chisel <ref type="bibr" target="#b7">[8]</ref> hardware design language and synthesize the model using Synopsys design compiler <ref type="bibr" target="#b22">[23]</ref> and 14-nm library from GlobalFoundries <ref type="bibr" target="#b9">[10]</ref> ( Â§6.7).</p><formula xml:id="formula_4">Ï† 1 S Ï† 1 S Ï† 2 S Ï† 2 S Ï† 3 S Ï† 3 S Q(S, A) Q(S, A) Q ( Ï† 1 S , A ) Q ( Ï† 1 S , A ) Q(Ï† 2 S , A) Q(Ï† 2 S , A) Q (Ï† 3 S , A ) Q (Ï† 3 S , A )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Assigning Rewards and</head><p>Updating Q-values. To track usefulness of the prefetched requests, Pythia maintains a first-infirst-out list of recently taken actions, along with their corresponding prefetch addresses in EQ. Every prefetch action is inserted into EQ. A reward gets assigned to every EQ entry before or when it gets evicted from EQ. During eviction, the reward and the state-action pair associated with the evicted EQ entry are used to update the corresponding Q-value in QVStore ( 6 in Fig. <ref type="figure" target="#fig_4">4</ref>).</p><p>We describe how Pythia appropriately assigns rewards to each EQ entry. We divide the reward assignment into three classes based on when the reward gets assigned to an entry: (1) immediate reward assignment during EQ insertion, (2) reward assignment during EQ residency, and (3) reward assignment during EQ eviction. If Pythia selects the action not to prefetch or one that generates a prefetch request beyond the current physical page, Pythia immediately assigns a reward to the EQ entry. For out-of-page prefetch action, Pythia assigns R ğ¶ğ¿ . For the action not to prefetch, Pythia assigns R ğ» ğ‘ ğ‘ƒ or R ğ¿ ğ‘ ğ‘ƒ , based on whether the current system memory bandwidth usage is high or low. If the address of a demand request matches with the prefetch address stored in an EQ entry during its residency, Pythia assigns R ğ´ğ‘‡ or R ğ´ğ¿ based on the filled bit of the EQ entry. If the filled bit is set, it indicates that the demand request is generated after the prefetch fill. Hence the prefetch is accurate and timely, and Pythia assigns the reward R ğ´ğ‘‡ . Otherwise, Pythia assigns the reward R ğ´ğ¿ . If a reward does not get assigned to an EQ entry until it is going to be evicted, it signifies that the corresponding prefetch address is not yet demanded by the processor. Thus, Pythia assigns a reward R ğ» ğ¼ ğ‘ or R ğ¿ ğ¼ ğ‘ to the entry during eviction based on whether the current system memory bandwidth usage is high or low.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Automated Design-Space Exploration</head><p>We propose an automated, performance-driven approach to systematically explore Pythia's vast design space and derive a basic configuration <ref type="foot" target="#foot_4">5</ref> with appropriate program features, action set, reward and hyperparameters. Table <ref type="table" target="#tab_2">2</ref> shows the basic configuration.  <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b92">93,</ref><ref type="bibr" target="#b108">109]</ref> to create any-one, any-two, and any-three feature-combinations from the set of 32 initial features, each providing a different state-vector. Fourth, we run Pythia with every state-vectors across all single-core workloads ( Â§5) and select the winning state-vector that provides the highest performance gain over no-prefetching baseline. As Table <ref type="table" target="#tab_2">2</ref> shows, the two constituent features of the winning state-vector are PC+Delta and Sequence of last-4 deltas. Rationale behind the winning state-vector. The winning state-vector is intuitive as its constituent features PC+Delta and Sequence of last-4 deltas closely match with the program features exploited by two prior state-of-the-art prefetchers, Bingo <ref type="bibr" target="#b26">[27]</ref> and SPP <ref type="bibr" target="#b77">[78]</ref>, respectively. However, concurrently running SPP and Bingo as a hybrid prefetcher does not provide the same performance 4.3.2 Action Selection. In a system with conventionally-sized 4KB pages and 64B cachelines, Pythia's list of actions (i.e., the list of possible prefetch offsets) contains all prefetch offsets in the range of <ref type="bibr">[âˆ’63, 63]</ref>. However, such a long action list poses two drawbacks. First, a long action list requires more online exploration to find the best prefetch offset given a state-vector, thereby reducing Pythia's performance benefits. Second, a longer action list increases Pythia's storage requirements. To avoid these problems, we prune the action list. We drop each action individually from the full action list [âˆ’63, 63] and measure the performance improvement relative to the performance improvement with the full action list, across all single-core workload traces. We prune any action that does not have any significant impact on the performance. Table <ref type="table" target="#tab_2">2</ref> shows the final pruned action list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.3.3</head><p>Reward and Hyperparameter Tuning. We separately tune seven reward level values (i.e., R ğ´ğ‘‡ , R ğ´ğ¿ , R ğ¶ğ¿ , R ğ» ğ¼ ğ‘ , R ğ¿ ğ¼ ğ‘ , R ğ» ğ‘ ğ‘ƒ , and R ğ¿ ğ‘ ğ‘ƒ ) and three hyperparameters (i.e., learning rate ğ›¼, discount factor ğ›¾, and exploration rate ğœ–) in three steps. First, we create a test trace suite by randomly selecting 10 workload traces from all of our 150 workload traces ( Â§5). Second, we create a list of tuning configurations using the uniform grid search technique <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b82">83]</ref>.</p><p>To do so, we first define a value range for each parameter to be tuned and divide the value range into uniform grids. For example, each of the three hyperparameters (ğ›¼, ğ›¾, and ğœ–) can take a value in the range of [0, 1]. We divide each hyperparameter range into ten exponentially-sized grids (i.e., 1ğ‘’ 0 , 1ğ‘’ âˆ’1 , 1ğ‘’ âˆ’2 , etc.) to obtain 10 Ã— 10 Ã— 10 = 1000 possible tuning configurations. For each tuning configuration, we run Pythia on the test trace suite and select the top-25 highest-performing configurations for the third step. Third, we run Pythia on all single-core workload traces using each of the 25 selected configurations. We select the winning configuration that provides the highest average performance gain. Table <ref type="table" target="#tab_2">2</ref> provides reward level and hyperparameter values of the basic Pythia.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Storage Overhead</head><p>Table <ref type="table" target="#tab_4">4</ref> shows the storage overhead of Pythia in its basic configuration. Pythia requires only 25.5 of metadata storage. QVStore consumes 24KB to store all Q-values. The EQ consumes only 1.5KB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Differences from Prior Work</head><p>The key idea of using RL in prefetching has been previously explored by the context prefetcher (CP) <ref type="bibr" target="#b103">[104]</ref>. Pythia significantly differs from it both in terms of (1) design principles (i.e., the reward, state, and action definition) and (2) the implementation.</p><p>Reward. CP naively defines the agent's reward as a continuous function of prefetch timeliness. Pythia not only considers coverage, â€¢ Entry size = state (21b) + action index (5b) + reward (5b) + filled-bit (1b) + address (16b)</p><p>1.5 KB Total 25.5 KB accuracy, and timeliness but also system-level feedback like memory bandwidth usage to define discrete reward levels. This reward definition provides two key advantages to Pythia. First, unlike CP, Pythia can adaptively trade off prefetch coverage for accuracy (and vice versa) based on memory bandwidth usage. Second, one can easily customize Pythia's objective by changing the reward values via configuration registers to extract even higher performance.</p><p>State. CP relies on compiler-generated hints in its state information. In contrast, Pythia extracts program features purely from hardware (e.g., PC, cacheline delta). Thus, Pythia requires no changes to software and it is easier to adopt into existing microprocessors.</p><p>Action. Unlike Pythia, CP uses a full cacheline address as the agent's action. The use of full cacheline address as action dramatically increases the action space, which results higher storage cost, longer training time, and reduced performance benefits.</p><p>Implementation. Pythia's implementation differs largely from CP in two major ways. First, CP uses the contextual-bandit (CB) algorithm <ref type="bibr" target="#b37">[38]</ref>, a simplified version of RL. The key difference between CB and RL is that a CB-solver cannot take its actions' long-term consequences into account when selecting an action. In contrast, RL-based Pythia weighs each probable prefetch action not only based on the expected immediate reward but also its long-term consequences (e.g., increased bandwidth usage or reduced prefetch accuracy in future) <ref type="bibr" target="#b123">[124]</ref>. As such, Pythia provides more robust and far-sighted predictions than the myopic CB-based CP. Second, Pythia organizes the Q-value storage into multiple vaults, each consisting of multiple planes. This hierarchical QVStore structure (1) enables pipelining the Q-value lookup to achieve high-throughput and low-latency prediction, and (2) easily scales out to support longer state-vectors by simply adding more vaults.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">METHODOLOGY</head><p>We use the trace-driven ChampSim simulator <ref type="bibr" target="#b6">[7]</ref> to evaluate Pythia and compare it to five prior prefetching proposals. We simulate an Intel Skylake <ref type="bibr" target="#b3">[4]</ref>-like multi-core processor that supports up to 12 cores. Table <ref type="table" target="#tab_5">5</ref> provides the key system parameters. For single-core simulations (1ğ¶), we warm up the core using 100 M instructions from each workload and simulate the next 500 M instructions. For multi-core multi-programmed simulations (ğ‘›ğ¶), we use 50 M and 150 M instructions from each workload respectively to warmup and simulate. If a core finishes early, the workload is replayed until every core finishes simulating 150 M instructions. We also implement Pythia using the Chisel <ref type="bibr" target="#b7">[8]</ref> hardware design language (HDL) and functionally verify the resultant register transfer logic (RTL) design to accurately measure Pythia's chip area and power overhead. The source-code of Pythia is freely available at <ref type="bibr" target="#b18">[19]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Workloads</head><p>We evaluate Pythia using a diverse set of memory-intensive workloads spanning SPEC CPU2006 <ref type="bibr" target="#b20">[21]</ref>, SPEC CPU2017 <ref type="bibr" target="#b21">[22]</ref>, PARSEC 2.1 <ref type="bibr" target="#b15">[16]</ref>, Ligra <ref type="bibr" target="#b116">[117]</ref>, and Cloudsuite <ref type="bibr" target="#b50">[51]</ref> benchmark suites. For SPEC CPU2006 and SPEC CPU20017 workloads, we reuse the instruction traces provided by the 2nd and the 3rd data prefetching championships (DPC) [2, 3]. For PARSEC and Ligra workloads, we collect the instruction traces using the Intel Pin dynamic binary instrumentation tool <ref type="bibr" target="#b16">[17]</ref>. We do not consider workload traces that have lower than 3 last-level cache misses per kilo instructions (MPKI) in the baseline system with no prefetching. In all, we present results for 150 workload traces spanning 50 workloads. Table <ref type="table" target="#tab_6">6</ref> shows a categorized view of all the workloads evaluated in this paper. For multi-core multi-programmed simulations, we create both homogeneous and heterogeneous trace mixes from our single-core trace list. For an ğ‘›-core homogeneous trace mix, we run ğ‘› copies of a trace from our single-core trace list, one in each core. For a heterogeneous trace mix, we randomly select ğ‘› traces from our single-core trace list and run one trace in every core. All the single-core traces and multi-programmed trace mixes used in our evaluation are freely available online <ref type="bibr" target="#b18">[19]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Prefetchers</head><p>We compare Pythia to five state-of-the-art prior prefetchers: SPP <ref type="bibr" target="#b77">[78]</ref>, SPP+PPF <ref type="bibr" target="#b31">[32]</ref>, SPP+DSPatch <ref type="bibr" target="#b29">[30]</ref>, Bingo <ref type="bibr" target="#b26">[27]</ref>, and MLOP <ref type="bibr" target="#b110">[111]</ref>. We model each competing prefetcher using the sourcecode provided by their respective authors and fine-tune them in our environment to extract the highest performance gain across all single-core traces. Table <ref type="table">7</ref> shows the parameters of all evaluated prefetchers. Each prefetcher is trained on L1-cache misses and fills prefetched lines into L2 and LLC. We also compare Pythia against multi-level prefetchers found in commercial processors (e.g., stride prefetcher at L1-cache and streamer at L2 <ref type="bibr" target="#b8">[9]</ref>) and IPCP <ref type="bibr" target="#b102">[103]</ref> in Â§6.2.4. For fair comparison, we add a simple PC-based stride prefetcher <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b72">73]</ref> at the L1 level, along with Pythia at the L2 level for such multi-level comparisons.</p><p>Table <ref type="table">7</ref>: Configuration of evaluated prefetchers SPP <ref type="bibr" target="#b77">[78]</ref> 256-entry ST, 512-entry 4-way PT, 8-entry GHR 6.2 KB Bingo <ref type="bibr" target="#b26">[27]</ref> 2KB region, 64/128/4K-entry FT/AT/PHT 46 KB MLOP <ref type="bibr" target="#b110">[111]</ref> 128-entry AMT, 500-update, 16-degree 8 KB DSPatch <ref type="bibr" target="#b29">[30]</ref> Same configuration as in <ref type="bibr" target="#b29">[30]</ref> 3.6 KB PPF <ref type="bibr" target="#b31">[32]</ref> Same configuration as in <ref type="bibr" target="#b31">[32]</ref> 39.3 KB Pythia 2 features, 2 vaults, 3 planes, 16 actions 25.5 KB</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RESULTS</head><p>6.1 Coverage and Overprediction in Single-core Fig. <ref type="figure">7</ref> shows the coverage and overprediction of each prefetcher in the single-core system, as measured at the LLC-main memory boundary. The key takeaway is that Pythia improves prefetch coverage, while simultaneously reducing overprediction compared to state-of-the-art prefetchers. On average, Pythia provides 6.9%, 8.8%, and 14% higher coverage than MLOP, Bingo, and SPP respectively, while generating 83.8%, 78.2%, and 3.6% fewer overpredictions.  <ref type="figure">8</ref>(a) shows the performance improvement of all prefetchers averaged across all traces in single-core to 12-core systems. To realistically model modern commercial multi-core processors, we simulate 1-2 core, 4-6 core, and 8-12 core systems with one, two, and four DDR4-2400 DRAM <ref type="bibr" target="#b14">[15]</ref> channels, respectively. We make two key observations from Figure <ref type="figure">8</ref>(a). First, Pythia consistently outperforms MLOP, Bingo, and SPP in all system configurations. Second, Pythia's performance improvement over prior prefetchers increases as core count increases.</p><p>In the single-core system, Pythia outperforms MLOP, Bingo, SPP, and an aggressive SPP with perceptron filtering (PPF <ref type="bibr" target="#b31">[32]</ref>) by 3.4%, 3.8%, 4.3%, and 1.02% respectively. In four (and twelve) core systems, Pythia outperforms MLOP, Bingo, SPP, and SPP+PPF by 5.8% (7.7%), 8.2% (9.6%), 6.5% (6.9%), and 3.1% (5.2%), respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6.2.2</head><p>Varying DRAM Bandwidth. To evaluate Pythia in bandwidth-constrained, highly-multi-threaded commercial server-class processors, where each core can have only a fraction of a channel's bandwidth, we simulate the single-core single-channel configuration by scaling the DRAM bandwidth (Figure <ref type="figure">8</ref>(b)). Each bandwidth configuration roughly corresponds to the available per-core DRAM bandwidth in various commercial processors (e.g., Intel Xeon Gold <ref type="bibr" target="#b12">[13]</ref>, AMD EPYC Rome <ref type="bibr" target="#b5">[6]</ref>, and AMD Threadripper <ref type="bibr" target="#b4">[5]</ref>).</p><p>The key takeaway is that Pythia consistently outperforms all competing prefetchers in every DRAM bandwidth configuration from 1 16 Ã— to 4Ã— bandwidth of the baseline system. Due to their large overprediction rates, the performance gains of MLOP and Bingo reduce sharply as DRAM bandwidth decreases. By actively trading off prefetch coverage for higher accuracy based on memory bandwidth usage, Pythia outperforms MLOP, Bingo, SPP, and SPP+PPF by 16.9%, 20.2%, 3.7%, and 9.5% respectively in the most bandwidthconstrained configuration with 150 million transfers per second (MTPS). In the 9600-MTPS configuration, every prefetcher enjoys ample DRAM bandwidth. Pythia still outperforms MLOP, Bingo, SPP, and SPP+PPF by 3%, 2.7%, 4.4%, and 0.8%, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6.2.3</head><p>Varying LLC Size. Fig. <ref type="figure">8</ref>(c) shows performance of all prefetchers averaged across all traces in the single-core system while varying the LLC size from 1 8 Ã— to 2Ã— of the baseline 2MB LLC. The key takeaway is that Pythia consistently outperforms all prefetchers in every LLC size configuration. For 256KB (and 4MB) LLC, Pythia outperforms MLOP, Bingo, SPP, and SPP+PPF by 3.6% (3.1%), 5.1% (3.4%), 2.7% (4.8%), and 1.2% (0.8%), respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6.2.4</head><p>Comparison to Multi-level Prefetching Schemes. Figure <ref type="figure">8(d)</ref> shows the performance comparison of Pythia in single-core system with varying DRAM bandwidth against two state-of-the-art multi-level prefetching schemes: (1) stride prefetcher <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b72">73]</ref> at L1 and streamer <ref type="bibr" target="#b34">[35]</ref> at L2 cache found in commercial Intel processors <ref type="bibr" target="#b8">[9]</ref>, and (2) IPCP, the winner of the third data prefetching championship <ref type="bibr">[3]</ref>. For fair comparison, we add a stride prefetcher in the L1 cache along with Pythia in the L2 cache for this experiment and measure performance over the no prefetching baseline. The key takeaway is that Stride+Pythia consistently outperforms stride+streamer and IPCP in every DRAM bandwidth configuration. Stride+Pythia outperforms Stride+Streamer and IPCP by 6.5% and 14.2% in the 150-MTPS configuration and by 2.3% and 1.0% in the 9600-MTPS configuration, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Performance Analysis</head><p>6.3.1 Single-core. Fig. <ref type="figure">9</ref>(a) shows the performance improvement of each individual prefetcher in each workload category in the single-core system. We make two major observations. First, Pythia improves performance by 22.4% on average over a no-prefetching baseline. Pythia outperforms MLOP, Bingo, and SPP by 3.4%, 3.8%, and 4.3% on average, respectively. Second, only Bingo outperforms Pythia only in the PARSEC suite, by 2.3%. However, Bingo's performance comes at the cost of a high overprediction rate, which hurts performance in multi-core systems (see Â§6.3.2).</p><p>To demonstrate the novelty of Pythia's RL-based prefetching approach using multiple program features, Fig. <ref type="figure">9</ref>(b) compares Pythia's performance improvement with the performance improvement of various combinations of prior prefetchers. Pythia not only outperforms all prefetchers (stride, SPP, Bingo, DSPatch, and MLOP) individually, but also outperforms their combination by 1.4% on average, with less than half of the combined storage size of the five prefetchers. We conclude that Pythia's RL-based prefetching approach using multiple program features under one single framework provides higher performance benefit than combining multiple prefetchers, each exploiting only one program feature.</p><p>6.3.2 Four-core. Fig. <ref type="figure" target="#fig_0">10(a)</ref> shows the performance improvement of each individual prefetcher in each workload category in the four-core system. We make two major observations. First, Pythia provides significant performance improvement over all prefetchers</p><formula xml:id="formula_5">!"# !"#$ !"% !"%$ &amp; &amp;"!$ &amp;"&amp; &amp;"&amp;$ &amp;"' &amp;"'$ &amp; ! ! ' ! ! ( ! ! # ! ! &amp; ) ! ! * ' ! ! ) ( ! ! &amp; ' # ! !</formula><p>+,-.,/0123,,4531 -6,710-1 37,8,9:;&lt;0= &gt;?@A1ABCD !"" #$%&amp;' ()*" !""+,!"-./0 !""+""1 "2.0$- in every workload category in the four-core system. On average, Pythia outperforms MLOP, Bingo, and SPP by 5.8%, 8.2%, and 6.5% respectively. Second, unlike in the single-core system, Pythia outperforms Bingo in PARSEC by 3.0% in the four-core system. This is due to Pythia's ability to dynamically increase prefetch accuracy during high DRAM bandwidth usage. Fig. <ref type="figure" target="#fig_0">10(b)</ref> shows that Pythia outperforms the combination of stride, SPP, Bingo, DSPatch, and MLOP prefetchers by 4.9% on average. Unlike in the single-core system, combining more prefetchers on top of stride+SPP in four-core system lowers the overall performance gain. This is due to the additive increase in the overpredictions made by each individual prefetcher, which leads to performance degradation in the bandwidth-constrained four-core system. Pythia's RL-based framework holistically learns to prefetch using multiple program features and generates fewer overpredictions, outperforming all combinations of all individual prefetchers. Figure <ref type="figure" target="#fig_0">10</ref>: Performance in the four-core system. 6.3.3 Benefit of Memory Bandwidth Usage Awareness. To demonstrate the benefit of Pythia's awareness of system memory bandwidth usage, we compare the performance of the full-blown Pythia with a new version of Pythia that is oblivious to system memory bandwidth usage. We create this bandwidth-oblivious version of Pythia by setting the high and low bandwidth usage variants of the rewards R ğ¼ ğ‘ and R ğ‘ ğ‘ƒ to the same value (i.e., essentially removing the bandwidth usage distinction from the reward values). More specifically, we set R ğ» ğ¼ ğ‘ = R ğ¿ ğ¼ ğ‘ = âˆ’8 and R ğ» ğ‘ ğ‘ƒ = R ğ¿ ğ‘ ğ‘ƒ = âˆ’4. Fig. <ref type="figure" target="#fig_10">11</ref> shows the performance benefit of the memory bandwidth-oblivious Pythia normalized to the basic Pythia as we vary the DRAM bandwidth. The key takeaway is that the bandwidth-oblivious Pythia loses performance by up to 4.6% on average across all single-core traces when the available memory bandwidth is low (150-MTPS to 600-MTPS configuration). However, when the available memory bandwidth is high (1200-MTPS to 9600-MTPS), the memory bandwidth-oblivious Pythia provides similar performance improvement to the basic Pythia. We conclude that, memory bandwidth awareness gives Pythia the ability to provide robust performance benefits across a wide range of system configurations.</p><formula xml:id="formula_6">!"! !"!# !"$ !"$# !"% !"%# &amp; $ ' ( ) !&amp; !$ *+,-+./012++3420 ,5+60/,026+7+89:;/&lt; =4-&gt;+60,709,6+1 !"# !"#$ !"% !"%$ &amp; &amp;"!$ &amp;"&amp; &amp;"&amp;$ &amp;"' &amp;"'$ &amp; ! ! ' ! ! ( ! ! # ! ! &amp; ) ! ! * ' ! ! ) ( ! ! &amp; ' # ! ! +,-.,/</formula><p>-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Performance on Unseen Traces</head><p>To demonstrate Pythia's ability to provide performance gains across workload traces that are not used at all to tune Pythia, we evaluate Pythia using an additional 500 traces from the second value prediction championship <ref type="bibr" target="#b19">[20]</ref> on both single-core and four-core systems. These traces are classified into floating-point, integer, crypto, and server categories and each of them has at least 3 LLC MPKI in the baseline without prefetching. No prefetcher, including Pythia, has been tuned on these traces. In the single-core system, Pythia outperforms MLOP, Bingo, and SPP on average by 8.3%, 3.5%, and 4.9%, respectively, across these traces. In the four-core system, Pythia outperforms MLOP, Bingo, and SPP on average by 9.7%, 5.4%, and 6.7%, respectively. We conclude that, Pythia, tuned on a set of workload traces, provides equally high (or even better) performance benefits on unseen traces for which it has not been tuned. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Understanding Pythia Using a Case Study</head><p>We delve deeper into an example workload trace, 459.GemsFDTD-1320B, from SPEC CPU2006 suite to provide more insight into Pythia's prefetching strategy and benefits. In this trace, the top two most selected prefetch offsets by Pythia are +23 and +11, which cumulatively account for nearly 72% of all offset selections. For each of these offsets, we examine the program feature value that selects that offset the most. For simplicity, we only focus on the PC+Delta feature here. The PC+Delta feature values 0x436a81+0 and 0x4377c5+0 select the offsets +23 and +11 the most, respectively. Fig. <ref type="figure" target="#fig_12">13</ref>(a) and (b) show the Q-value curve of different actions for these feature. The x-axis shows the number of Q-value updates to the corresponding feature. Each color-coded line represents the Q-value of the respective action. As Fig. <ref type="figure" target="#fig_12">13</ref>(a) shows, the Q-value of action +23 for feature value 0x436a81+0 consistently stays higher than all other actions (only three other representative actions are shown in 13(a)). This means Pythia actively favors to prefetch using +23 offset whenever the PC 0x436a81 generates the first load to a physical page (hence the delta 0). By dumping the program trace, we indeed find that whenever PC 0x436a81 generates the first load to a physical page, there is only one more address demanded in that page that is 23 cachelines ahead from the first loaded cacheline. In this case, the positive reward for generating a correct prefetch with offset +23 drives the Q-value of +23 much higher than those of other offsets and Pythia successfully uses the offset +23 for prefetch request generation given the feature value 0x436a81+0. We see similar a trend for the feature value 0x4377c5+0 with offset +11 (Fig. <ref type="figure" target="#fig_12">13(b)</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Performance Benefits via Customization</head><p>In this section, we show two examples of Pythia's online customization ability to extract even higher performance gain than the baseline Pythia configuration in target workload suites. First, we customize Pythia's reward level values for the Ligra graph processing workloads. Second, we customize the program features used by Pythia for the SPEC CPU2006 workloads.</p><p>6.6.1 Customizing Reward Levels. For workloads from the Ligra graph processing suite, we observe a general trend that a prefetcher with higher prefetch accuracy typically provides higher performance benefits. This is because any incorrect prefetch request wastes precious main memory bandwidth, which is already heavily used by the demand requests of the workload. Thus, to improve Pythia's performance benefit in the Ligra suite, we create a new strict configuration of Pythia that favors not to prefetch over generating inaccurate prefetches. We create this strict configuration by simply reducing the reward level values for inaccurate prefetch (i.e., R ğ» ğ¼ ğ‘ = âˆ’22 and R ğ¿ ğ¼ ğ‘ = âˆ’20) and increasing the reward level values for no prefetch (i.e., R ğ» ğ‘ ğ‘ƒ = R ğ¿ ğ‘ ğ‘ƒ = 0). Fig. <ref type="figure" target="#fig_4">14</ref> shows the percentage of the total runtime the workload spends in different bandwidth usage buckets in primary y-axis and the overall performance improvement in the secondary y-axis for each competing prefetcher in one example workload from the Ligra suite, Ligra-CC. We make two key observations. First, with MLOP and Bingo prefetchers enabled, Ligra-CC spends a much higher percentage of runtime consuming more than half of the peak DRAM bandwidth than in the no prefetching baseline. As a result, MLOP and Bingo underperforms the no prefetching baseline by 11.8% and 1.8%, respectively. In contrast, basic Pythia leads to only a modest memory bandwidth usage overhead, and outperforms the no prefetching baseline by 6.9%. Second, in the strict configuration, Pythia has even less memory bandwidth usage overhead, and provides 3.5% higher performance than the basic Pythia configuration (10.4% over the no prefetching baseline), without any hardware changes. Fig. <ref type="figure" target="#fig_5">15</ref> shows the performance benefits of the basic and strict Pythia configurations for all workloads from Ligra. The key takeaway is that by simply changing the reward level values via configuration registers on the silicon, strict Pythia provides up to 7.8% (2.0% on average) higher performance than basic Pythia. We conclude that the objectives of Pythia can be easily customized via simple configuration registers for target workload suites to extract even higher performance benefits, without any changes to the underlying hardware.  6.6.2 Customizing Feature Selection. To maximize the performance benefits of Pythia on the SPEC CPU2006 workload suite, we run all one-combination and two-combination of program features from the initial set of 32 supported features. For each workload, we fine-tune Pythia using the feature combination that provides the highest performance benefit. We call this the feature-optimized configuration of Pythia for SPEC CPU2006 suite. Fig. <ref type="figure" target="#fig_14">16</ref> shows the performance benefits of the basic and optimized configurations of Pythia for all SPEC CPU2006 workloads. The key takeaway is that by simply fine-tuning the program feature selection, Pythia delivers up to 5.1% (1.5% on average) performance improvement on top of the basic Pythia configuration. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.7">Overhead Analysis</head><p>To accurately estimate Pythia's chip area and power overheads, we implement the full-blown Pythia, including all fixed-point adders, multipliers, and the pipelined QVStore search operation ( Â§4.2.2), using the Chisel <ref type="bibr" target="#b7">[8]</ref> hardware design language (HDL). We extensively verify the functional correctness of the resultant register transfer logic (RTL) design and synthesize the RTL design using Synopsys Design Compiler <ref type="bibr" target="#b22">[23]</ref> and 14-nm library from GlobalFoundries <ref type="bibr" target="#b9">[10]</ref> to estimate Pythia's area and power overhead. Pythia consumes 0.33 mm2 of area and 55.11 mW of power in each core. The QVStore component consumes 90.4% and 95.6% of the total area and power of Pythia, respectively. With respect to the overall die area and power consumption of a 4-core desktop-class Skylake processor with the lowest TDP budget <ref type="bibr" target="#b10">[11]</ref>, and a 28-core server-class Skylake processor with the highest TDP budget, Pythia (implemented in all cores) incurs area &amp; power overheads of only 1.03% &amp; 0.4%, and 1.33% &amp; 0.75%, respectively. We conclude that Pythia's performance benefits come at a very modest cost in area and power overheads across a variety of commercial processors. 4-core Skylake D-2123IT, 60W TDP <ref type="bibr" target="#b10">[11]</ref> 1.03% 0.37% 18-core Skylake 6150, 165W TDP <ref type="bibr" target="#b11">[12]</ref> 1.24% 0.60% 28-core Skylake 8180M, 205W TDP <ref type="bibr" target="#b13">[14]</ref> 1.33% 0.75%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">OTHER RELATED WORKS</head><p>To our knowledge, Pythia is the first RL-based customizable prefetching framework that can learn to prefetch using multiple different program features and system-level feedback information inherent to its design, to provide performance benefits across a wide range of workloads and changing system configurations. We already compare Pythia against five state-of-the-art prefetching proposals <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b110">111]</ref> in Â§6. In this section, we qualitatively compare Pythia against other prior prefetching techniques. Traditional Prefetchers. We divide the traditional prefetching algorithms into three broad categories: precomputation-based, temporal, and spatial. Precomputation-based prefetchers (e.g., runahead <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b95">[96]</ref><ref type="bibr" target="#b96">[97]</ref><ref type="bibr" target="#b97">[98]</ref><ref type="bibr" target="#b98">[99]</ref><ref type="bibr" target="#b99">[100]</ref><ref type="bibr" target="#b100">[101]</ref><ref type="bibr" target="#b101">[102]</ref> and helper-thread execution <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b87">88,</ref><ref type="bibr" target="#b106">107,</ref><ref type="bibr" target="#b119">120,</ref><ref type="bibr" target="#b128">129,</ref><ref type="bibr" target="#b141">142,</ref><ref type="bibr" target="#b144">145]</ref>) pre-execute program code to generate future memory requests. These prefetchers can generate highly-accurate prefetches even when no recognizable pattern exists in program memory requests. However, precomputation-based prefetchers usually have high design complexity. Pythia is not a precomputation-based proposal. It finds patterns in past memory request addressed to generate prefetch requests.</p><p>Temporal prefetchers <ref type="bibr">[26, 29, 36, 37, 42, 52, 62, 66, 72, 77, 121, 130-132, 134, 135]</ref> memorize long sequences of cacheline addresses demanded by the processor. When a previously-seen address is encountered again, a temporal prefetcher prefetch requests to addresses that previously followed the currently-seen cacheline address. However, temporal prefetchers usually have high storage requirements (often multi-megabytes of metadata storage, which necessitates storing metadata in memory <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b120">121,</ref><ref type="bibr" target="#b129">130]</ref>). Pythia requires only 25.5KB of storage, which can easily fit inside a core.</p><p>Spatial prefetchers <ref type="bibr">[25, 27, 30, 32, 35, 56, 65, 73, 78-80, 90, 103, 106, 111, 112, 122, 123]</ref> predict a cacheline delta or spatial bitpattern by learning program access patterns over different spatial memory regions. Spatial prefetchers provide high-accuracy prefetches, usually with lower storage overhead than temporal prefetchers. We already compare Pythia with other spatial prefetchers <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b110">111]</ref> and show higher performance benefits.</p><p>Machine Learning (ML) in Computer Architecture. Prior works apply ML techniques in computer architecture in two major ways: (1) to design adaptive, data-driven algorithms, and (2) to explore the large microarchitectural design-space. Researchers have proposed ML-based algorithms for various microarchitectural tasks like memory scheduling <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b93">94]</ref>, cache management <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b86">87,</ref><ref type="bibr" target="#b109">110,</ref><ref type="bibr" target="#b112">113,</ref><ref type="bibr" target="#b126">127]</ref>, branch prediction <ref type="bibr">[57, 67-70, 125, 126, 139, 140, 146]</ref>, address translation <ref type="bibr" target="#b88">[89]</ref> and hardware prefetching <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b103">104,</ref><ref type="bibr" target="#b104">105,</ref><ref type="bibr" target="#b113">[114]</ref><ref type="bibr" target="#b114">[115]</ref><ref type="bibr" target="#b115">[116]</ref><ref type="bibr" target="#b140">141]</ref>. Pythia provides three key advantages over prior MLbased prefetchers. First, Pythia can learn to prefetch from multiple program features and system-level feedback information inherent to its design. Second, Pythia can be customized online. Third, Pythia incurs low hardware overhead. Researchers have also explored ML techniques to explore the large microarchitectural design space, e.g., NoC design <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b83">84,</ref><ref type="bibr" target="#b127">128,</ref><ref type="bibr" target="#b135">136,</ref><ref type="bibr" target="#b136">137,</ref><ref type="bibr" target="#b142">143]</ref>, chip placement optimization <ref type="bibr" target="#b90">[91]</ref>, hardware resource assignment for accelerators <ref type="bibr" target="#b75">[76]</ref>. These works are orthogonal to Pythia.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>We introduce Pythia, the first customizable prefetching framework that formulates prefetching as a reinforcement learning (RL) problem. Pythia autonomously learns to prefetch using multiple program features and system-level feedback information to predict memory accesses. Our extensive evaluations show that Pythia not only outperforms five state-of-the-art prefetchers but also provides robust performance benefits across a wide-range of workloads and system configurations. Pythia's benefits come with very modest area and power overheads. We believe and hope that Pythia would encourage the next generation of data-driven autonomous prefetchers that automatically learn far-sighted prefetching policies by interacting with the system. Such prefetchers can not only improve performance and efficiency under a wide variety of workloads and system configurations, but also reduce the system architect's burden in designing sophisticated prefetching mechanisms. baseline without prefetching in all but one four-core trace mix. Pythia provides the highest performance gain in 437.leslie3d-271B (2.1Ã—) and lowest performance gain in 429.mcf-184B (-3.5%) over the no-prefetching baseline. Second, Pythia also outperforms (or matches performance) all competing prefetchers in majority of trace mixes. Pythia underperforms Bingo in the 462.libquantum homogeneous trace mix due to the very regular streaming access pattern. On the other hand, Pythia significantly outperforms Bingo in Ligra workloads (e.g., pagerank) due to its adaptive prefetching strategy to trade-off coverage for accuracy in high memory bandwidth usage. We conclude that Pythia provides a consistent performance gain over multiple prior state-of-the-art prefetchers over a wide range of workloads even in bandwidth-constrained multi-core systems.  </p><formula xml:id="formula_7">!"! !"# $"! $"# %"! %"# &amp;"! $ $$ %$ &amp;$ '$ #$ ($ )$ *$ +$ $!$ $$$ $%$ $&amp;$ $'$ $#$ $($ $)$ $*$ $+$ %!$ %$$ %%$ %&amp;$ %'$ %#$ %($ ,-../0-</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Performance with Different Features</head><p>Fig. <ref type="figure" target="#fig_0">19</ref> shows the performance, coverage, and overprediction of Pythia averaged across all single-core traces with different feature combinations during automated feature selection ( Â§4.3.1). For brevity, we show results for all experiments with any-one and any-two combinations of 20 features taken from the full list of 32 features. Both graphs are sorted in ascending order of performance improvement of Pythia over the baseline without prefetching. We make three key observations. First, Pythia's performance gain over the no-prefetching baseline improves from 20.7% to 22.4% by varying the feature combination. We select the feature combination that provides the highest performance gain as the basic Pythia configuration (Table <ref type="table" target="#tab_2">2</ref>). Second, Pythia's coverage and overprediction also change significantly with varying feature combination. Pythia's coverage improves from 66.2% to 71.5%, whereas overprediction improves from 32.2% to 26.7% by changing feature combination. Third, Pythia's performance gain positively correlates with Pythia's coverage in single-core configuration. We conclude that automatic design-space exploration can significantly optimize Pythia's performance, coverage, and overpredictions.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Comparison to the Context Prefetcher</head><p>As we discuss in Section 4.5, unlike Pythia, the context prefetcher (CP <ref type="bibr" target="#b103">[104]</ref>) relies on both hardware and software contexts. A tailor-made compiler needs to encode the software contexts using special NOP instructions, which are decoded by the core front-end to pass the context to the CP. For a fair comparison, we implement the context prefetcher using hardware contexts (CP-HW) and show the performance comparison of Pythia and CP-HW in Figure <ref type="figure" target="#fig_18">21</ref>. The key takeaway is that Pythia outperforms the CP-HW prefetcher by 5.3% and 7.6% in single-core and four-core configurations, respectively. Pythia's performance improvement over CP-HW mainly comes from two key aspects: (1) Pythia's ability to take memory bandwidth usage into consideration while taking prefetch actions, and (2) the far-sighted predictions made by Pythia as opposed to myopic predictions by CP-HW.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 Comparison to the IBM POWER7 Adaptive Prefetcher</head><p>Fig. <ref type="figure" target="#fig_19">22</ref> compares Pythia against the IBM POWER7 adaptive prefetcher <ref type="bibr" target="#b70">[71]</ref>. The POWER7 prefetcher dynamically tunes its prefetch aggressiveness (e.g., selecting prefetch depth, enabling stride-based prefetching) by monitoring program performance. We make two observations from Fig. <ref type="figure" target="#fig_19">22</ref>. First, Pythia outperforms the POWER7 prefetcher by 4.5% in single-core system. This is mostly due to Pythia's ability to capture different types of address patterns than just streaming/stride patterns. Second, Pythia outperforms POWER7 prefetcher by 6.5% in four-core and 6.1% in eight-core systems (not plotted), respectively. The increase in performance improvement from single to four (or eight) core configuration suggests that Pythia is more adaptive than the POWER7 prefetcher. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.6 Performance Sensitivity to Number of Warmup Instructions</head><p>Fig. <ref type="figure" target="#fig_20">23</ref> shows performance sensitivity of all prefetchers to the number of warmup instructions averaged across all single-core traces. Our baseline simulation configuration uses 100 million warmup instructions. The key takeaway from Fig. <ref type="figure" target="#fig_20">23</ref> is that Pythia consistently outperforms prior prefetchers in a wide range of simulation configurations using different number of warmup instructions. In the baseline simulation configuration using 100M warmup instructions, Pythia outperforms MLOP, Bingo, and SPP by 3.4%, 3.8%, and 4.4% respectively. In a simulation configuration with no warmup instruction, Pythia continues to outperform MLOP, Bingo, and SPP by 2.8%, 3.7%, and 4.2% respectively. We conclude that, Pythia can quickly learn to prefetch from a program's memory access pattern and provides higher performance than other heuristics-based prefetching techniques over a wide range of simulation configurations using different number of warmup instructions.</p><p>1.15</p><p>1.17 </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Comparison of (a) coverage, overprediction, and (b) performance of two recently-proposed prefetchers, SPP<ref type="bibr" target="#b77">[78]</ref> and Bingo<ref type="bibr" target="#b26">[27]</ref>, and our new proposal, Pythia.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Interaction between an agent and the environment in a reinforcement learning system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Formulating the prefetcher as an RL-agent.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>!% 32</head><label>32</label><figDesc>!""#$%&amp;'()*'+&amp;,-&amp; .-''("/-%+#%$ 01&amp;(%,'',&amp;/'(9(,.:&amp;*.,#-%&amp;;&amp; 7,*,(&lt;!.,#-%&amp;/*#'&amp;#%&amp;01 ,&amp;01&amp;(%,'2&amp;*%+&amp; 5/+*,(&amp;167,-'( &amp; ?#%+&amp;,:(&amp;!.,#-%&amp;)#,:&amp;@*A&amp;1&lt;6*B5( ' &amp;9#BB(+&amp;C#,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Overview of Pythia.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: (a) The QVStore is comprised of multiple vaults. (b) Each vault is comprised of multiple planes. (c) Index generation from feature value.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>all feature-action Q-values to compute state-action Q-value Track maximum state-action Q-value</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Pipelined organization of QVStore search operation. The illustration depicts three program features, each having three planes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc>Figure 8: Average performance improvement of prefetchers in systems with varying (a) number of cores, (b) DRAM million transfers per second (MTPS), (c) LLC size, and (d) prefetching level. Each DRAM bandwidth configuration roughly matches MTPS/core of various commercial processors [5, 6, 13]. The baseline bandwidth/LLC configuration is marked in red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Performance of memory bandwidth-oblivious Pythia versus the basic Pythia.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Performance on unseen traces.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Q-value curves of PC+Delta feature values (a) 0x436a81+0 and (b) 0x4377c5+0 in 459.GemsFDTD-1320B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 14 :Figure 15 :</head><label>1415</label><figDesc>Figure 14: Performance and main memory bandwidth usage of prefetchers in Ligra-CC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 16 :</head><label>16</label><figDesc>Figure 16: Performance of the basic and feature-optimized Pythia on the SPEC CPU2006 suite.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 18 :</head><label>18</label><figDesc>Figure 18: Performance line graph of 272 four-core trace mixes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 19 :Figure 20 :</head><label>1920</label><figDesc>Figure 19: Performance, coverage, and overprediction of Pythia with different feature combinations. The x-axis shows experiments with different feature combinations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 21 :</head><label>21</label><figDesc>Figure 21: Performance of Pythia vs. the context prefetcher [104] using hardware contexts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 22 :</head><label>22</label><figDesc>Figure 22: Performance comparison against IBM POWER7 prefetcher [71].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 23 :</head><label>23</label><figDesc>Figure 23: Performance sensitivity of all prefetchers to number of warmup instructions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Example program featuresAs every post-L1-cache prefetcher generates prefetch requests within a physical page[27, 30, 32, 53, 65, 78-80, 90, 103, 106,  111, 112, 122, 123], the list of prefetch offsets only contains values in the range of [âˆ’63, 63] for a system with a traditionally-sized 4KB page and 64B cacheline. Using prefetch offsets as actions (instead of full cacheline addresses) drastically reduces the action space size.</figDesc><table><row><cell></cell><cell cols="2">Control-flow</cell><cell>Data-flow</cell><cell></cell></row><row><cell>Feature</cell><cell>Info.</cell><cell>History</cell><cell>Info.</cell><cell>History</cell></row><row><cell>Last 3-PCs</cell><cell>PC</cell><cell>last 3</cell><cell>âœ–</cell><cell>âœ–</cell></row><row><cell>Last 4-deltas</cell><cell>âœ–</cell><cell>âœ–</cell><cell>Cacheline delta</cell><cell>last 4</cell></row><row><cell>PC+Delta</cell><cell>PC</cell><cell>current</cell><cell>Cacheline delta</cell><cell>current</cell></row><row><cell>Last 4-PCs+Page no.</cell><cell>PC</cell><cell>last 4</cell><cell>Page no.</cell><cell>current</cell></row><row><cell cols="5">the demanded cacheline address) from a set of candidate prefetch</cell></row><row><cell>offsets.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>ğ‘’ğ‘›ğ‘¡ğ‘Ÿ ğ‘¦.ğ‘Ÿğ‘’ğ‘¤ğ‘ğ‘Ÿğ‘‘ â† R ğ´ğ‘‡ /* If the filled bit is set, i.e., the demand access came after the prefetch fill, assign reward R ğ´ğ‘‡ */ ğ‘‚ ğ‘“ ğ‘“ ğ‘ ğ‘’ğ‘¡ [ğ‘ğ‘ğ‘¡ğ‘–ğ‘œğ‘›]) /* Add the selected prefetch offset to the current demand address to generate prefetch address */ 18:ğ‘’ğ‘›ğ‘¡ğ‘Ÿ ğ‘¦ â† ğ‘ğ‘Ÿğ‘’ğ‘ğ‘¡ğ‘’_ğ¸ğ‘„_ğ‘’ğ‘›ğ‘¡ğ‘Ÿ ğ‘¦ (ğ‘†, ğ‘ğ‘ğ‘¡ğ‘–ğ‘œğ‘›, ğ´ğ‘‘ğ‘‘ğ‘Ÿ + ğ‘‚ ğ‘“ ğ‘“ ğ‘ ğ‘’ğ‘¡ [ğ‘ğ‘ğ‘¡ğ‘–ğ‘œğ‘›]) /* Create new EQ entry using the current state-vector, the selected action, and the prefetch address */</figDesc><table><row><cell></cell><cell>1âˆ’ğ›¾</cell><cell></cell></row><row><cell>3:</cell><cell>clear EQ</cell><cell></cell></row><row><cell>4:</cell><cell></cell><cell></cell></row><row><cell cols="2">5: procedure Train_and_Predict(Addr)</cell><cell>/* Called for every demand request */</cell></row><row><cell>6:</cell><cell>ğ‘’ğ‘›ğ‘¡ğ‘Ÿ ğ‘¦ â† ğ‘ ğ‘’ğ‘ğ‘Ÿğ‘â„_ğ¸ğ‘„ (ğ´ğ‘‘ğ‘‘ğ‘Ÿ )</cell><cell>/* For a demand request to ğ´ğ‘‘ğ‘‘ğ‘Ÿ , search EQ with the demand address */</cell></row><row><cell>7:</cell><cell>if entry is valid then</cell><cell></cell></row><row><cell>8:</cell><cell>if ğ‘’ğ‘›ğ‘¡ğ‘Ÿ ğ‘¦.ğ‘“ ğ‘–ğ‘™ğ‘™ğ‘’ğ‘‘ == ğ‘¡ğ‘Ÿğ‘¢ğ‘’ then</cell><cell></cell></row><row><cell>9:</cell><cell></cell><cell></cell></row><row><cell>10:</cell><cell>else</cell><cell></cell></row><row><cell>11:</cell><cell>ğ‘’ğ‘›ğ‘¡ğ‘Ÿ ğ‘¦.ğ‘Ÿğ‘’ğ‘¤ğ‘ğ‘Ÿğ‘‘ â† R ğ´ğ¿</cell><cell>/* Otherwise, assign R ğ´ğ¿ */</cell></row><row><cell>12:</cell><cell>ğ‘† â† ğ‘”ğ‘’ğ‘¡ _ğ‘ ğ‘¡ğ‘ğ‘¡ğ‘’ ()</cell><cell>/* Extract the state-vector from the attributes of current demand request */</cell></row><row><cell>13:</cell><cell>if ğ‘Ÿğ‘ğ‘›ğ‘‘ () â‰¤ ğœ– then</cell><cell></cell></row><row><cell>14:</cell><cell>ğ‘ğ‘ğ‘¡ğ‘–ğ‘œğ‘› â† ğ‘”ğ‘’ğ‘¡ _ğ‘Ÿğ‘ğ‘›ğ‘‘ğ‘œğ‘š_ğ‘ğ‘ğ‘¡ğ‘–ğ‘œğ‘› ()</cell><cell>/* Select a random action with a low probability ğœ– to explore the state-action space */</cell></row><row><cell>15:</cell><cell>else</cell><cell></cell></row><row><cell>16:</cell><cell>ğ‘ğ‘ğ‘¡ğ‘–ğ‘œğ‘› â† argmax ğ‘ ğ‘„ (ğ‘†, ğ‘)</cell><cell>/* Otherwise, select the action with the highest Q-value */</cell></row><row><cell cols="2">17: ğ‘ğ‘Ÿğ‘’ ğ‘“ ğ‘’ğ‘¡ğ‘â„ (ğ´ğ‘‘ğ‘‘ğ‘Ÿ + 19: if no prefetch action then</cell><cell></cell></row><row><cell>20:</cell><cell>ğ‘’ğ‘›ğ‘¡ğ‘Ÿ ğ‘¦.ğ‘Ÿğ‘’ğ‘¤ğ‘ğ‘Ÿğ‘‘ â† R ğ» ğ‘ ğ‘ƒ or R ğ¿ ğ‘ ğ‘ƒ</cell><cell>/* In case of no-prefetch action, immediately assign reward ğ‘… ğ» ğ‘ ğ‘ƒ or ğ‘… ğ¿ ğ‘ ğ‘ƒ based on current memory bandwidth usage */</cell></row><row><cell>21:</cell><cell>else if out-of-page prefetch then</cell><cell></cell></row><row><cell>22:</cell><cell></cell><cell></cell></row></table><note>). If a matching entry is found, Pythia assigns a reward (either R ğ´ğ‘‡ or Algorithm 1 Pythia's reinforcement learning based prefetching algorithm 1: procedure Initialize 2: initialize QVStore: ğ‘„ (ğ‘†, ğ´) â† 1 ğ‘’ğ‘›ğ‘¡ğ‘Ÿ ğ‘¦.ğ‘Ÿğ‘’ğ‘¤ğ‘ğ‘Ÿğ‘‘ â† R ğ¶ğ¿ /* In case of out-of-page prefetch action, immediately assign reward ğ‘… ğ¶ğ¿ */ 23: ğ‘’ğ‘£ğ‘–ğ‘ğ‘¡ _ğ‘’ğ‘›ğ‘¡ğ‘Ÿ ğ‘¦ â† ğ‘–ğ‘›ğ‘ ğ‘’ğ‘Ÿğ‘¡ _ğ¸ğ‘„ (ğ‘’ğ‘›ğ‘¡ğ‘Ÿ ğ‘¦) /* Insert the entry. Get the evicted EQ entry. */ 24: if â„ğ‘ğ‘ _ğ‘Ÿğ‘’ğ‘¤ğ‘ğ‘Ÿğ‘‘ (ğ‘‘ğ‘_ğ‘’ğ‘›ğ‘¡ğ‘Ÿ ğ‘¦) == ğ‘“ ğ‘ğ‘™ğ‘ ğ‘’ then 25: ğ‘‘ğ‘_ğ‘’ğ‘›ğ‘¡ğ‘Ÿ ğ‘¦.ğ‘Ÿğ‘’ğ‘¤ğ‘ğ‘Ÿğ‘‘ â† R ğ» ğ¼ ğ‘ or R ğ¿ ğ¼ ğ‘ /* If the evicted entry does not have a reward yet, assign the reward R ğ» ğ¼ ğ‘ or R ğ¿ ğ¼ ğ‘ based on current memory bandwidth usage */ 26: ğ‘… â† ğ‘‘ğ‘_ğ‘’ğ‘›ğ‘¡ğ‘Ÿ ğ‘¦.ğ‘Ÿğ‘’ğ‘¤ğ‘ğ‘Ÿğ‘‘ /* Get the reward stored in the evicted entry */ 27: ğ‘† 1 â† ğ‘‘ğ‘_ğ‘’ğ‘›ğ‘¡ğ‘Ÿ ğ‘¦.ğ‘ ğ‘¡ğ‘ğ‘¡ğ‘’; ğ´ 1 â† ğ‘‘ğ‘_ğ‘’ğ‘›ğ‘¡ğ‘Ÿ ğ‘¦.ğ‘ğ‘ğ‘¡ğ‘–ğ‘œğ‘› /* Get the state-vector and the action from the evicted EQ entry */ 28: ğ‘† 2 â† ğ¸ğ‘„.â„ğ‘’ğ‘ğ‘‘.ğ‘ ğ‘¡ğ‘ğ‘¡ğ‘’; ğ´ 2 â† ğ¸ğ‘„.â„ğ‘’ğ‘ğ‘‘.ğ‘ğ‘ğ‘¡ğ‘–ğ‘œğ‘› /* Get the state-vector and the action from the entry at the head of the EQ */ 29: ğ‘„ (ğ‘† 1 , ğ´ 1 ) â† ğ‘„ (ğ‘† 1 , ğ´ 1 ) + ğ›¼ [ğ‘… + ğ›¾ğ‘„ (ğ‘† 2 , ğ´ 2 ) âˆ’ ğ‘„ (ğ‘† 1 , ğ´ 1 ) ] /* Perform the SARSA update */ 30: 31: procedure Prefetch_Fill(Addr) 32:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Basic Pythia configuration derived from our automated design-space exploration</figDesc><table><row><cell>Features</cell><cell>PC+Delta, Sequence of last-4 deltas</cell></row><row><cell>Prefetch Action List</cell><cell>{-6,-3,-1,0,1,3,4,5,10,11,12,16,22,23,30,32}</cell></row><row><cell>Reward Level Values</cell><cell>R</cell></row></table><note>ğ´ğ‘‡ =20, R ğ´ğ¿ =12, R ğ¶ğ¿ =âˆ’12, R ğ» ğ¼ ğ‘ =âˆ’14, R ğ¿ ğ¼ ğ‘ =âˆ’8, R ğ» ğ‘ ğ‘ƒ =âˆ’2,R ğ¿ ğ‘ ğ‘ƒ =âˆ’4 Hyperparameters ğ›¼ = 0.0065, ğ›¾ = 0.556, ğœ– = 0.002 4.3.1 Feature Selection. We derive a list of possible program features for feature-space exploration in four steps. First, we derive a list of 4 control-flow components, and 8 data-flow components, which are mentioned in Table 3. Second, we combine each controlflow component with each data-flow component with the concatenation operation, to obtain a total of 32 possible program features. Third, we use the linear regression technique</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>List of program control-flow and data-flow components used to derive the list of features for exploration</figDesc><table><row><cell>Control-flow Component</cell><cell>Data-flow Component</cell></row><row><cell></cell><cell>(1) Load cacheline address</cell></row><row><cell></cell><cell>(2) Page number</cell></row><row><cell>(1) PC of load request</cell><cell>(3) Page offset</cell></row><row><cell>(2) PC-path (XOR-ed last-3 PCs)</cell><cell>(4) Load address delta</cell></row><row><cell>(3) PC XOR-ed branch-PC</cell><cell>(5) Sequence of last-4 offsets</cell></row><row><cell>(4) None</cell><cell>(6) Sequence of last-4 deltas</cell></row><row><cell></cell><cell>(7) Offset XOR-ed with delta</cell></row><row><cell></cell><cell>(8) None</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Storage overhead of Pythia</figDesc><table><row><cell>Structure</cell><cell>Description</cell><cell>Size</cell></row><row><cell></cell><cell>â€¢ # vaults = 2</cell><cell></cell></row><row><cell></cell><cell>â€¢ # planes in each vault = 3</cell><cell></cell></row><row><cell>QVStore</cell><cell>â€¢ # entries in each plane = feature dimension (128) Ã—</cell><cell>24 KB</cell></row><row><cell></cell><cell>action dimension (16)</cell><cell></cell></row><row><cell></cell><cell>â€¢ Entry size = Q-value width (16b)</cell><cell></cell></row><row><cell></cell><cell>â€¢ # entries = 256</cell><cell></cell></row><row><cell>EQ</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Simulated system parameters</figDesc><table><row><cell>Core</cell><cell>1-12 cores, 4-wide OoO, 256-entry ROB, 72/56-entry LQ/SQ</cell></row><row><cell cols="2">Branch Pred. Perceptron-based [69], 20-cycle misprediction penalty</cell></row><row><cell>L1/L2</cell><cell>Private, 32KB/256KB, 64B line, 8 way, LRU, 16/32 MSHRs, 4-</cell></row><row><cell>Caches</cell><cell>cycle/14-cycle round-trip latency</cell></row><row><cell></cell><cell>2MB/core, 64B line, 16 way, SHiP [133], 64 MSHRs per LLC Bank,</cell></row><row><cell>LLC</cell><cell></cell></row><row><cell></cell><cell>34-cycle round-trip latency</cell></row><row><cell></cell><cell>1C: Single channel, 1 rank/channel; 4C: Dual channel, 2</cell></row><row><cell>Main Memory</cell><cell>ranks/channel; 8C: Quad channel, 2 ranks/channel;</cell></row><row><cell></cell><cell>8 banks/rank, 2400 MTPS, 64b data-bus/channel, 2KB row</cell></row><row><cell></cell><cell>buffer/bank, tRCD=15ns, tRP=15ns, tCAS=12.5ns</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Workloads used for evaluation</figDesc><table><row><cell>Suite</cell><cell cols="3"># Workloads # Traces Example Workloads</cell></row><row><cell>SPEC06</cell><cell>16</cell><cell>28</cell><cell>gcc, mcf, cactusADM, lbm, ...</cell></row><row><cell>SPEC17</cell><cell>12</cell><cell>18</cell><cell>gcc, mcf, pop2, fotonik3d, ...</cell></row><row><cell>PARSEC</cell><cell>5</cell><cell>11</cell><cell>canneal, facesim, raytrace, ...</cell></row><row><cell>Ligra</cell><cell>13</cell><cell>40</cell><cell>BFS, PageRank, Bellman-ford, ...</cell></row><row><cell>Cloudsuite</cell><cell>4</cell><cell>53</cell><cell>cassandra, cloud9, nutch, ...</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Area and power overhead of Pythia</figDesc><table><row><cell cols="3">Pythia's area: 0.33 mm2/core; Pythia's power: 55.11 mW/core</cell></row><row><cell>Overhead compared to real systems</cell><cell>Area</cell><cell>Power</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Pythia, according to Greek mythology, is the oracle of Delphi who is known for accurate prophecies<ref type="bibr" target="#b17">[18]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">Pythia keeps track of recently-taken actions because it cannot always immediately assign a reward to an action, as the usefulness of the generated prefetch request (i.e., if and when the prefetched address is demanded by the processor) is not immediately known while the action is being taken. During EQ residency, if the address of a demand request matches with the prefetch address stored in an EQ entry, the corresponding action is considered to have generated a useful prefetch request.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">In this paper, we define prefetch timeliness as a binary value due to its measurement simplicity. One can easily make the definition non-binary by storing three timestamps per EQ entry: (1) when the prefetch is issued (ğ‘¡ ğ‘–ğ‘ ğ‘ ğ‘¢ğ‘’ ), (2) when the prefetch is filled (ğ‘¡ ğ‘“ ğ‘–ğ‘™ğ‘™ ), and (3) when a demand is generated for the same prefetched address (ğ‘¡ ğ‘‘ğ‘’ğ‘šğ‘ğ‘›ğ‘‘ ).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">Our application of tile coding is similar to that used in the self-optimizing memory controller (RLMC)<ref type="bibr" target="#b63">[64]</ref>. The key difference is that RLMC uses a hybrid combination of feature and action values to index single-dimensional planes, whereas Pythia uses feature and action values separately to index two-dimensional planes.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">Using a compute-grid with ten 28-core machines, the automated exploration across 150 workload traces (mentioned in detail in Â§5) takes 44 hours to complete. benefit as Pythia, as we show in Â§6.3.1. This is because combining SPP with Bingo not only improves their prefetch coverage, but also combines their prefetch overpredictions, leading to performance degradation, especially in resource-constrained systems. In contrast, Pythia's RL-based learning strategy that inherently uses the same two features successfully increases prefetch coverage, while maintaining high prefetch accuracy. As a result, Pythia not only outperforms SPP and Bingo individually, but also outperforms the combination of the two prefetchers.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank the anonymous reviewers for their feedback. We thank all SAFARI Research Group members, especially Skanda Koppula, for insightful feedback. We acknowledge the generous gifts provided by our industrial partners: Google, Huawei, Intel, Microsoft, and VMware. This work is also in part supported by SRC research grant as a part of AI Hardware program. The first author thanks his departed father, whom he lost in COVID-19 pandemic.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A ARTIFACT APPENDIX A.1 Abstract</head><p>We implement Pythia using ChampSim simulator <ref type="bibr" target="#b6">[7]</ref>. In this artifact, we provide the source code of Pythia and necessary instructions to reproduce its key performance results. We identify four key results to demonstrate Pythia's novelty:</p><p>â€¢ Workload category-wise performance speedup of all competing prefetchers (Fig. <ref type="figure">9</ref>(a)). â€¢ Workload category-wise coverage and overpredictions of all competing prefetchers (Fig. <ref type="figure">7</ref>). â€¢ Geomean performance comparison with varying DRAM bandwidth from 150-MTPS to 9600-MTPS (Fig. <ref type="figure">8(b)</ref>). â€¢ Workload category-wise performance speedup of all competing prefetchers (Fig. <ref type="figure">10(a)</ref>).</p><p>The artifact can be executed in any machine with a generalpurpose CPU and 52 GB disk space. However, we strongly recommend running the artifact on a compute cluster with slurm <ref type="bibr" target="#b137">[138]</ref> support for bulk experimentation.</p><p>A.2 Artifact Check-list (Meta-information)</p><p>â€¢ Compilation: G++ v6.3.0 or above.</p><p>â€¢ Data set: Download traces using the supplied script. A.3.4 Data Sets. The ChampSim traces required to evaluate Pythia can be downloaded using the supplied script. Our implementation of Pythia is fully compatible with prior ChampSim traces that are used in previous cache replacement (CRC-2 [1]), data prefetching (DPC-3 [3]) and value-prediction (CVP-2 <ref type="bibr" target="#b19">[20]</ref>) championships. We are also releasing a new set of ChampSim traces extracted from Ligra <ref type="bibr" target="#b116">[117]</ref> and PARSEC-2.1 <ref type="bibr" target="#b15">[16]</ref> suites.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Installation</head><p>(1) Clone Pythia from GitHub repository:</p><p>$ git clone https://github.com/CMU-SAFARI/Pythia.git</p><p>(2) Clone Bloomfilter library inside Pythia home and build:</p><p>$ cd Pythia/ $ git clone https://github.com/mavam/libbf.git libbf/ $ cd libbf/ $ mkdir build &amp;&amp; cd build/ &amp;&amp; cmake ../ $ make clean &amp;&amp; make</p><p>(3) Build Pythia for single-core and four-core configurations:</p><p>$ cd $PYTHIA_HOME $ ./build_champsim.sh multi multi no 1 $ ./build_champsim.sh multi multi no 4 (4) Please make sure to set environment variables as: $ source setvars.sh</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Experiment Workflow</head><p>This section describes steps to generate, and execute necessary experiments. We recommend the reader to follow the README file to know more about each script used in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5.1 Preparing Traces.</head><p>(1) Download necessary traces as follows:</p><p>$ mkdir $PYTHIA_HOME/traces $ cd $PYTHIA_HOME/scripts $ perl download_traces.pl -csv artifact_traces.csv -dir $PYTHIA_HOME/traces/ (2) If the traces are downloaded in other path, please update the full path in MICRO21_1C.tlist and MICRO21_4C.tlist inside $PYTHIA_HOME/experiments directory appropriately.</p><p>A.5.2 Launching Experiments. The following instructions will launch all experiments required to reproduce key results in a local machine. We strongly recommend using a compute cluster with slurm support to efficiently launch experiments in bulk. To launch experiments using slurm, please provide -local 0 (tested using slurm v16.05.9).</p><p>(1) Launch single-core experiments as follows: (3) Please make sure the paths used in tlist and exp files are appropriately changed before creating the experiment files.</p><p>A.5.3 Rolling-up Statistics. We will use rollup.pl script to rollup statistics from outputs of all experiments. To automate the process, we will use the following instructions. This will create three comma-separated-value (CSV) files in experiments directory which will be used for evaluation in appendix A.6.</p><p>$ cd $PYTHIA_HOME/experiments $ bash automate_rollup.sh</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 Evaluation</head><p>For single-core baseline configuration experiments, we will evaluate three metrics: performance, coverage, and overprediction of each prefetcher. For single-core experiments varying DRAM-bandwidth and four-core experiments, we will evaluate only performance. The performance, coverage and overprediction of a prefetcher X is measured by following equations:</p><p>To easily calculate the metrics, we are providing a Microsoft Excel template to post-process the rolled-up CSV files. The template has four sheets, three of which bear the same name of the rolled up CSV files. Each sheet is already populated with our collected results, necessary formulas, pivot tables, and charts to reproduce the results presented in the paper. Please follow the instructions to reproduce the results from your own CSV statistics files:</p><p>(1) Copy and paste each CSV file in the corresponding sheet's top left corner (i.e., cell A1). (2) Immediately after pasting, convert the comma-separated rows into columns by going to Data -&gt; Text-to-Coloumns -&gt; Selecting comma as a delimiter. This will replace the already existing data in the sheet with the newly collected data. (3) Refresh each pivot table in each sheet by clicking on them and then clicking Pivot-Table <ref type="table">-</ref>Analyse -&gt; Refresh.</p><p>The reader can also use any other data processor (e.g., Python pandas) to reproduce the same result. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7 Expected Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.8 Experiment Customization</head><p>â€¢ The configuration of every prefetcher can be customized by changing the ini files inside the config directory.  <ref type="figure">17</ref> shows the performance line graph of all prefetchers for the 150 single-core workload traces. The workload traces are sorted in ascending order of performance improvement of Pythia over the baseline without prefetching. We make three key observations. First, Pythia outperforms the no-prefetching baseline in every single-core trace, except 623.xalancbmk-592B (where it underperforms the baseline by 2.1%). 603.bwaves-2931B enjoys the highest performance improvement of 2.2Ã— over the baseline. Performance of the top 80% of traces improve by at least 4.2% over the baseline. Second, Pythia underperforms Bingo in workloads like libquantum due to the heavy streaming nature of memory accesses. As libquantum streams through all physical pages, Bingo simply prefetches all cachelines of a page at once just by seeing the first access to the page. As a result Bingo achieves higher timeliness and higher performance than Pythia. Third, Pythia significantly outperforms every competing prefetcher in workloads with irregular access patterns (e.g., mcf, pagerank). We conclude that Pythia provides consistent performance gains over the no-prefetching baseline and multiple prior state-of-the-art prefetchers over a wide range of workloads. We share a table depicting the single-core performance of every competing prefetcher considered in this paper over the no-prefetching baseline in our GitHub repository: https://github.com/CMU-SAFARI/Pythia.  B.1.2 Four-core. Fig. <ref type="figure">18</ref> shows the performance line graph of all prefetchers for 272 four-core workload trace mixes (including both homogeneous and heterogeneous mixes). The workload mixes are sorted in ascending order of performance improvement of Pythia over the baseline without prefetching. We make two key observations. First, Pythia outperforms the</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="https://crc2.ece.tamu.edu" />
		<title level="m">Cache Replacement Championship</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<ptr target="http://comparch-conf.gatech.edu/dpc2/" />
		<title level="m">Data Prefetching Championship</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<ptr target="https://dpc3.compas.cs.stonybrook.edu" />
		<title level="m">Data Prefetching Championship</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<ptr target="https://www.intel.com/content/www/us/en/processors/core/desktop-6th-gen-core-family-spec-update.html" />
		<title level="m">6th Generation IntelÂ® Processor Family</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<ptr target="https://en.wikichip.org/wiki/amd/ryzen_threadripper/3990x" />
		<title level="m">AMD Ryzen Threadripper 3990X</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<ptr target="https://en.wikichip.org/wiki/amd/epyc/7702p" />
		<title level="m">AMD Zen2 EPYC 7702P</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<ptr target="https://github.com/ChampSim/ChampSim" />
		<title level="m">ChampSim</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<ptr target="https://www.chisel-lang.org" />
		<title level="m">Chisel/FIRRTL Hardware Compiler Framework</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<ptr target="https://software.intel.com/content/www/us/en/develop/articles/disclosure-of-hw-prefetcher-control-on-some-intel-processors.html" />
		<title level="m">Disclosure of Hardware Prefetcher Control on Some IntelÂ® Processors</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<ptr target="https://www.globalfoundries.com/sites/default/files/product-briefs/pb-14lpp.pdf" />
		<title level="m">GlobalFoundries 14nm FinFET Technology</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<ptr target="https://en.wikichip.org/wiki/intel/xeon_d/d-2123it" />
		<title level="m">Intel Xeon D-2123IT</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title/>
		<ptr target="https://en.wikichip.org/wiki/intel/xeon_gold/6150" />
	</analytic>
	<monogr>
		<title level="j">Intel Xeon Gold</title>
		<imprint>
			<biblScope unit="volume">6150</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<ptr target="https://en.wikichip.org/wiki/intel/xeon_gold/6258r" />
		<title level="m">Intel Xeon Gold 6258R</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<ptr target="https://en.wikichip.org/wiki/intel/xeon_platinum/8180m" />
		<title level="m">Intel Xeon Platinum 8180M</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName><surname>Jedec-Ddr4</surname></persName>
		</author>
		<ptr target="https://www.jedec.org/sites/default/files/docs/JESD79-4.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<ptr target="http://parsec.cs.princeton.edu/" />
		<title level="m">PARSEC</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<ptr target="https://software.intel.com/en-us/articles/pin-a-dynamic-binary-instrumentation-tool" />
		<title level="m">Dynamic Binary Instrumentation Tool</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<ptr target="https://en.wikipedia.org/wiki/Pythia" />
		<title level="m">Pythia</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Pythia</forename><surname>Github</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Repository</forename></persName>
		</author>
		<ptr target="https://github.com/CMU-SAFARI/Pythia" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<ptr target="https://www.microarch.org/cvp1/cvp2/rules.html" />
		<title level="m">Second Championship Value Prediction</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<ptr target="https://www.spec.org/cpu2006/" />
		<title level="m">SPEC CPU 2006</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<ptr target="https://www.spec.org/cpu2017/" />
		<title level="m">SPEC CPU 2017</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Synopsys</surname></persName>
		</author>
		<author>
			<persName><surname>Ultra</surname></persName>
		</author>
		<ptr target="https://www.synopsys.com/implementation-and-signoff/rtl-synthesis-test/dc-ultra.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A New Approach to Manipulator Control: The Cerebellar Model Articulation Controller (CMAC)</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Albus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Dynamic Systems, Measurement, and Control</title>
		<imprint>
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An Effective On-chip Preloading Scheme to Reduce Data Access Penalty</title>
		<author>
			<persName><forename type="first">Jean-Loup</forename><surname>Baer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tien-Fu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SC</title>
				<imprint>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Domino Temporal Data Prefetcher</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Bakhshalipour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pejman</forename><surname>Lotfi-Kamran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamid</forename><surname>Sarbazi-Azad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bingo Spatial Data Prefetcher</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Bakhshalipour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehran</forename><surname>Shakerinava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pejman</forename><surname>Lotfi-Kamran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamid</forename><surname>Sarbazi-Azad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Accelerating Deep Learning Inference via Learned Caches</title>
		<author>
			<persName><forename type="first">Arjun</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adarsh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivaram</forename><surname>Venkataraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Akella</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Correlated load-address predictors</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bekerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Jourdan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronny</forename><surname>Ronen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gilad</forename><surname>Kirshenboim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihu</forename><surname>Rappoport</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adi</forename><surname>Yoaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISCA</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">DSPatch: Dual Spatial Pattern Prefetcher</title>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Bera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Anant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Nori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sreenivas</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><surname>Subramoney</surname></persName>
		</author>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Random Search for Hyper-parameter Optimization</title>
		<author>
			<persName><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Perceptron-Based Prefetch Filtering</title>
		<author>
			<persName><forename type="first">Eshan</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gino</forename><surname>Chacon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seth</forename><surname>Pugsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elvira</forename><surname>Teran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">V</forename><surname>Gratz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">A</forename><surname>JimÃ©nez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Simultaneous Subordinate Microthreading (SSMT)</title>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">S</forename><surname>Chappell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sangwook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yale</forename><forename type="middle">N</forename><surname>Reinhardt</surname></persName>
		</author>
		<author>
			<persName><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
				<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Prefetching and Memory System Behavior of the SPEC95 Benchmark Suite</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Charney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Puzak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research and Development</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Effective hardware-based data prefetching for high-performance processors</title>
		<author>
			<persName><forename type="first">Tien-Fu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Loup</forename><surname>Baer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE TC</title>
				<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dynamic Hot Data Stream Prefetching for General-Purpose Programs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Trishul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Chilimbi</surname></persName>
		</author>
		<author>
			<persName><surname>Hirzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PLDI</title>
				<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Low-cost Epoch-based Correlation Prefetching for Commercial Applications</title>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Chou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICRO</title>
				<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Contextual bandits with linear payoff functions</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lev</forename><surname>Reyzin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics</title>
				<meeting>the Fourteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="208" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">LEAD: Learningenabled Energy-aware Dynamic Voltage/Frequency Scaling in NoCs</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avinash</forename><surname>Kodi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Louri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DAC</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Dynamic Speculative Precomputation</title>
		<author>
			<persName><forename type="first">Jamison</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dean</forename><forename type="middle">M</forename><surname>Tullsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">P</forename><surname>Shen</surname></persName>
		</author>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Speculative precomputation: Long-range prefetching of delinquent loads</title>
		<author>
			<persName><forename type="first">Hong</forename><surname>Jamison D Collins</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Dean M Tullsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong-Fong</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Lavery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
				<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">A stateless, contentdirected data prefetching mechanism</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Cooksey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Jourdan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Grunwald</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>ASPLOS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Dynamic Error Mitigation in NoCs Using Intelligent Prediction Techniques</title>
		<author>
			<persName><forename type="first">Dominic</forename><surname>Ditomaso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Travis</forename><surname>Boraten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avinash</forename><surname>Kodi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Louri</surname></persName>
		</author>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Machine Learning Enabled Power-aware Network-on-chip Design</title>
		<author>
			<persName><forename type="first">Dominic</forename><surname>Ditomaso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashif</forename><surname>Sikder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avinash</forename><surname>Kodi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Louri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Michel</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName><surname>Song</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
		<respStmt>
			<orgName>University of Southern California CENG</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note>Assisted Execution</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Improving Data Cache Performance by Pre-executing Instructions Under a Cache Miss</title>
		<author>
			<persName><forename type="first">James</forename><surname>Dundas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Mudge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICS</title>
				<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Prefetch-aware Shared Resource Management for Multi-core Systems</title>
		<author>
			<persName><forename type="first">Eiman</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><forename type="middle">Joo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yale</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Coordinated Control of Multiple Prefetchers in Multi-core Systems</title>
		<author>
			<persName><forename type="first">Eiman</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><forename type="middle">Joo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yale</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Techniques for Bandwidthefficient Prefetching of Linked Data Structures in Hybrid Prefetching Systems</title>
		<author>
			<persName><forename type="first">Eiman</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yale</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
				<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">HARAQ: Congestion-aware Learning Model for Highly Adaptive Routing Algorithm in On-Chip Networks</title>
		<author>
			<persName><forename type="first">Masoumeh</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masoud</forename><surname>Daneshtalab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fahimeh</forename><surname>Farahnakian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juha</forename><surname>Plosila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pasi</forename><surname>Liljeberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maurizio</forename><surname>Palesi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NOCS</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Clearing the Clouds: A Study of Emerging Scale-out Workloads on Modern Hardware</title>
		<author>
			<persName><forename type="first">Almutaz</forename><surname>Michael Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Adileh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stavros</forename><surname>Kocberber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Volos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Djordje</forename><surname>Alisafaee</surname></persName>
		</author>
		<author>
			<persName><surname>Jevdjic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>ASPLOS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Last-touch Correlated Data Streaming</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Babak</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISPASS</title>
				<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Spatial Memory Streaming with Rotated Patterns</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Michael Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Babak</forename><surname>Somogyi</surname></persName>
		</author>
		<author>
			<persName><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JILP Data Prefetching Championship</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Dynamic Voltage and Frequency Scaling in NoCs with Supervised and Reinforcement Learning techniques</title>
		<author>
			<persName><forename type="first">Quintin</forename><surname>Fettes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avinash</forename><surname>Karanth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Louri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TC</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Data Prefetching in Multiprocessor Vector Cache Memories</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">C</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janak</forename><forename type="middle">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
				<imprint>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Stride Directed Prefetching in Scalar Processors</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">C</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janak</forename><forename type="middle">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bob</forename><forename type="middle">L</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><surname>Janssens</surname></persName>
		</author>
		<idno>MICRO. 1992</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Bit-level Perceptron Prediction for Indirect Branches</title>
		<author>
			<persName><forename type="first">Elba</forename><surname>Garza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samira</forename><surname>Mirbagher-Ajorpaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tahsin</forename><surname>Ahmad Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">A</forename><surname>JimÃ©nez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">An Introduction to Variable and Feature Selection</title>
		<author>
			<persName><forename type="first">Isabelle</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">AndrÃ©</forename><surname>Elisseeff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="2003-03">Mar. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Continuous Runahead: Transparent Hardware Acceleration for Memory Intensive Workloads</title>
		<author>
			<persName><forename type="first">Milad</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yale</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Filtered Runahead Execution with a Runahead Buffer</title>
		<author>
			<persName><forename type="first">Milad</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yale</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICRO</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Learning Memory Access Patterns</title>
		<author>
			<persName><forename type="first">Milad</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grant</forename><surname>Ayers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiner</forename><surname>Litz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jichuan</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">TCP: Tag Correlating Prefetchers</title>
		<author>
			<persName><forename type="first">Zhigang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Martonosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanos</forename><surname>Kaxiras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
				<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Effective stream-based and execution-based data prefetching</title>
		<author>
			<persName><forename type="first">Sorin</forename><surname>Iacobovici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Spracklen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudarshan</forename><surname>Kadambi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santosh G</forename><surname>Abraham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICS</title>
				<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Self-Optimizing Memory Controllers: A Reinforcement Learning Approach</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ipek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>MartÃ­nez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
				<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Access Map Pattern Matching for Data Cache Prefetch</title>
		<author>
			<persName><forename type="first">Yasuo</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mary</forename><surname>Inaba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kei</forename><surname>Hiraki</surname></persName>
		</author>
		<idno>ISC. 2009</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Linearizing Irregular Memory Accesses for Improved Correlated Prefetching</title>
		<author>
			<persName><forename type="first">Akanksha</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Calvin</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICRO</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Fast path-based neural branch prediction</title>
		<author>
			<persName><forename type="first">JimÃ©nez</forename><surname>Daniel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICRO</title>
				<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Multiperspective Perceptron Predictor</title>
		<author>
			<persName><forename type="first">JimÃ©nez</forename><surname>Daniel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th JILP Workshop on Computer Architecture Competitions (JWAC-5): Championship Branch Prediction (CBP-5)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Dynamic Branch Prediction with Perceptrons</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
				<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Neural methods for dynamic branch prediction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Calvin</forename><surname>JimÃ©nez</surname></persName>
		</author>
		<author>
			<persName><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOCS</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Adaptive Prefetching on POWER7: Improving Performance and Power Consumption</title>
		<author>
			<persName><forename type="first">VÃ­ctor</forename><surname>JimÃ©nez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><forename type="middle">J</forename><surname>Cazorla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Gioiosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alper</forename><surname>Buyuktosunoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradip</forename><surname>Bose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P O'</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName><surname>Connell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>TOPC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Prefetching using Markov predictors</title>
		<author>
			<persName><forename type="first">Doug</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Grunwald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
				<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Improving Direct-mapped Cache Performance by the Addition of a Small Fully-associative Cache and Prefetch Buffers</title>
		<author>
			<persName><forename type="first">Norman</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
				<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Helper thread prefetching for loosely-coupled multiprocessor systems</title>
		<author>
			<persName><forename type="first">Changhee</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daeseob</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaejin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Solihin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IPDPS</title>
				<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">B-fetch: Branch Prediction Directed Prefetching for Chipmultiprocessors</title>
		<author>
			<persName><forename type="first">David</forename><surname>Kadjo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinchun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prabal</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reena</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Gratz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Jimenez</surname></persName>
		</author>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">ConfuciuX: Autonomous Hardware Resource Assignment for DNN Accelerators using Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Sheng-Chun</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geonhwa</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Krishna</surname></persName>
		</author>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">A prefetching technique for irregular accesses to linked data structures</title>
		<author>
			<persName><forename type="first">Magnus</forename><surname>Karlsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fredrik</forename><surname>Dahlgren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Per</forename><surname>Stenstrom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA)</title>
				<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Path Confidence based Lookahead Prefetching</title>
		<author>
			<persName><forename type="first">Jinchun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seth</forename><forename type="middle">H</forename><surname>Pugsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">V</forename><surname>Gratz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeshan</forename><surname>Chishti</surname></persName>
		</author>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Division of labor: A more effective approach to prefetching</title>
		<author>
			<persName><forename type="first">Sushant</forename><surname>Kondguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Exploiting Spatial Locality in Data Caches using Spatial Footprints</title>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Wilkerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
				<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Prefetch-Aware DRAM Controllers</title>
		<author>
			<persName><forename type="first">Chang</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yale</forename><forename type="middle">N</forename><surname>Veynu Narasiman</surname></persName>
		</author>
		<author>
			<persName><surname>Patt</surname></persName>
		</author>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Improving Memory Bank-Level Parallelism in the Presence of Prefetching</title>
		<author>
			<persName><forename type="first">Chang</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Veynu</forename><surname>Narasiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yale</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Fitting Segmented Regression Models by Grid Search</title>
		<author>
			<persName><surname>Pm Lerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series C (Applied Statistics)</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">A Deep Reinforcement Learning Framework for Architectural Exploration: A Routerless NoC Case Study</title>
		<author>
			<persName><forename type="first">Ting-Ru</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Drew</forename><surname>Penney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massoud</forename><surname>Pedram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Reducing DRAM Latencies with an Integrated Memory Hierarchy Design</title>
		<author>
			<persName><forename type="first">Wei-Fen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Reinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
				<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">Filtering superfluous prefetches using density vectors</title>
		<author>
			<persName><forename type="first">Wei-Fen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Reinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Puzak</surname></persName>
		</author>
		<idno>ICCD. 2001</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">An imitation learning approach for cache replacement</title>
		<author>
			<persName><forename type="first">Evan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milad</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junwhan</forename><surname>Parthasarathy Ranganathan</surname></persName>
		</author>
		<author>
			<persName><surname>Ahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Tolerating Memory Latency Through Software-controlled Pre-execution in Simultaneous Multithreading Processors</title>
		<author>
			<persName><forename type="first">Chi-Keung</forename><surname>Luk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
				<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Virtual address translation via learned page table indexes</title>
		<author>
			<persName><forename type="first">Artemiy</forename><surname>Margaritov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitrii</forename><surname>Ustiugov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Bugnion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Grot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ML for Systems at NeurIPS</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Best-offset Hardware Prefetching</title>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Michaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">A Graph Placement Methodology for Fast Chip Design</title>
		<author>
			<persName><forename type="first">Azalia</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Goldie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Yazgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><forename type="middle">Wenjie</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ebrahim</forename><surname>Songhori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shen</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Elizabeth</forename><forename type="middle">A</forename><surname>Douglas C Montgomery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Peck</surname></persName>
		</author>
		<author>
			<persName><surname>Vining</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>John Wiley &amp; Sons</publisher>
			<biblScope unit="volume">821</biblScope>
		</imprint>
	</monogr>
	<note>Introduction to linear regression analysis</note>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">MORSE: Multi-objective Reconfigurable Self-optimizing Memory Scheduler</title>
		<author>
			<persName><forename type="first">Janani</forename><surname>Mukundan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><forename type="middle">F</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Using the First-Level Caches as Filters to Reduce the Pollution Caused by Speculative Memory References</title>
		<author>
			<persName><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyesoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">N</forename><surname>Armstrong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yale</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJPP</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">Address-value Delta (AVD) prediction: Increasing the Effectiveness of Runahead Execution by Exploiting Regular Memory Allocation Patterns</title>
		<author>
			<persName><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyesoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yale</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Techniques for Efficient Processing in Runahead Execution Engines</title>
		<author>
			<persName><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyesoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yale</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
				<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Efficient Runahead Execution: Power-efficient Memory Latency Tolerance</title>
		<author>
			<persName><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyesoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yale</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Micro</title>
				<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title level="m" type="main">On reusing the results of pre-executed instructions in a runahead execution processor</title>
		<author>
			<persName><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyesoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yale</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>IEEE CAL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Runahead Execution: An Alternative to Very Large Instruction Windows for Out-of-order Processors</title>
		<author>
			<persName><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yale</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
				<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Runahead Execution: An Effective Alternative to Large Instruction Windows</title>
		<author>
			<persName><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yale</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Micro</title>
				<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Vector Runahead</title>
		<author>
			<persName><forename type="first">Ajeya</forename><surname>Naithani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Ainsworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lieven</forename><surname>Eeckhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Bouquet of Instruction Pointers: Instruction Pointer Classifier-based Spatial Hardware Prefetching</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pakalapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Panda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Semantic Locality and Contextbased Prefetching using Reinforcement Learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Peled</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Weiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Etsion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<title level="m" type="main">A neural network memory prefetcher using semantic locality</title>
		<author>
			<persName><forename type="first">Leeor</forename><surname>Peled</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Weiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Etsion</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.00478</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Sandbox Prefetching: Safe Run-time Evaluation of Aggressive Prefetchers</title>
		<author>
			<persName><forename type="first">Zeshan</forename><surname>Seth H Pugsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Chishti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng-Fei</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aamer</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><surname>Jaleel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Runahead Threads to Improve SMT Performance</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pajuelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">J</forename><surname>Santana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
				<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<monogr>
		<title level="m" type="main">On-line Q-learning using connectionist systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gavin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahesan</forename><surname>Rummery</surname></persName>
		</author>
		<author>
			<persName><surname>Niranjan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
		<respStmt>
			<orgName>University of Cambridge, Department of Engineering Cambridge, UK.</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<monogr>
		<title level="m" type="main">Linear regression analysis</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">J</forename><surname>Seber</surname></persName>
		</author>
		<author>
			<persName><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>John Wiley &amp; Sons</publisher>
			<biblScope unit="volume">329</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Designing a Cost-Effective Cache Replacement Policy using Machine Learning</title>
		<author>
			<persName><forename type="first">Subhash</forename><surname>Sethumurugan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jieming</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Sartori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<monogr>
		<title level="m" type="main">Multi-lookahead offset prefetching</title>
		<author>
			<persName><forename type="first">Mehran</forename><surname>Shakerinava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Bakhshalipour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pejman</forename><surname>Lotfi-Kamran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamid</forename><surname>Sarbazi-Azad</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<monogr>
		<title level="m" type="main">Efficiently Prefetching Complex Address Patterns</title>
		<author>
			<persName><forename type="first">Manjunath</forename><surname>Shevgoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sahil</forename><surname>Koladiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajeev</forename><surname>Balasubramonian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seth</forename><forename type="middle">H</forename><surname>Pugsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeshan</forename><surname>Chishti</surname></persName>
		</author>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<monogr>
		<title level="m" type="main">Applying Deep learning to The Cache Replacement Problem</title>
		<author>
			<persName><forename type="first">Zhan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangru</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akanksha</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Calvin</forename><surname>Lin</surname></persName>
		</author>
		<idno>MICRO. 413-425. 2019</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">A Neural Hierarchical Sequence Model for Irregular Data Prefetching</title>
		<author>
			<persName><forename type="first">Zhan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akanksha</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milad</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parthasarathy</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Calvin</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ML For Systems Workshop, NeurIPS</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">A Hierarchical Neural Model of Data Prefetching</title>
		<author>
			<persName><forename type="first">Zhan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akanksha</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milad</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parthasarathy</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Calvin</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<monogr>
		<title level="m" type="main">Learning execution through neural code fusion</title>
		<author>
			<persName><forename type="first">Zhan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milad</forename><surname>Parthasarathy Ranganathan</surname></persName>
		</author>
		<author>
			<persName><surname>Hashemi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07181</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Ligra: A Lightweight Graph Processing Framework for Shared Memory</title>
		<author>
			<persName><forename type="first">Julian</forename><surname>Shun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><forename type="middle">E</forename><surname>Blelloch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM SIGPLAN symposium on Principles and practice of parallel programming</title>
				<meeting>the 18th ACM SIGPLAN symposium on Principles and practice of parallel programming</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="135" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Mastering the game of Go with deep neural networks and tree search</title>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aja</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><surname>Driessche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play</title>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Using a user-level memory thread for correlation prefetching</title>
		<author>
			<persName><forename type="first">Yan</forename><surname>Solihin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaejin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josep</forename><surname>Torrellas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
				<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Spatio-Temporal Memory Streaming</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Somogyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasia</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Babak</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
				<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Babak Falsafi, and Andreas Moshovos. Spatial Memory Streaming</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Somogyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastassia</forename><surname>Ailamaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
				<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Feedback Directed Prefetching: Improving the Performance and Bandwidth-efficiency of Hardware Prefetchers</title>
		<author>
			<persName><forename type="first">Santhosh</forename><surname>Srinath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyesoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yale</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
				<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<monogr>
		<title level="m" type="main">Reinforcement Learning: An Introduction</title>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<monogr>
		<title level="m" type="main">Merging path and gshare indexing in perceptron branch prediction</title>
		<author>
			<persName><forename type="first">David</forename><surname>Tarjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Skadron</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>TACO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<monogr>
		<title level="m" type="main">Improving branch prediction by modeling global history with convolutional neural networks</title>
		<author>
			<persName><forename type="first">Chit-Kwan</forename><surname>Stephen J Tarsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gokce</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautham</forename><surname>Keskin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Chinya</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.09889</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Perceptron Learning for Reuse Prediction</title>
		<author>
			<persName><forename type="first">Elvira</forename><surname>Teran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">A</forename><surname>JimÃ©nez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICRO</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Extending the Power-efficiency and Performance of Photonic Interconnects for Heterogeneous Multicores with Machine Learning</title>
		<author>
			<persName><forename type="first">Scott</forename><surname>Van Winkle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avinash</forename><surname>Karanth Kodi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Louri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Helper Threads via Virtual Multithreading on an Experimental ItaniumÂ®2 Processor-based Platform</title>
		<author>
			<persName><forename type="first">Perry</forename><forename type="middle">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamison</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongkeun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Greene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Ming</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS</title>
				<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Practical Off-chip Meta-data for Temporal Memory Streaming</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Thomas F Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasia</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Babak</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><surname>Moshovos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
				<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<monogr>
		<title level="m" type="main">Making Address-correlated Prefetching Practical</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Thomas F Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasia</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Babak</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><surname>Moshovos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>IEEE</publisher>
			<pubPlace>Micro</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Temporal streaming of shared memory</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Thomas F Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolaos</forename><surname>Somogyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jangwoo</forename><surname>Hardavellas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastassia</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Babak</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
				<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<monogr>
		<title level="m" type="main">SHiP: Signature-based Hit Predictor for High Performance Caching</title>
		<author>
			<persName><forename type="first">Carole-Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aamer</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Hasenplaugh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Martonosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><forename type="middle">C</forename><surname>Steely</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Emer</surname></persName>
		</author>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<monogr>
		<title level="m" type="main">Temporal Prefetching Without the Off-Chip Metadata</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishnendra</forename><surname>Nathella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Pusdesris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dam</forename><surname>Sunwoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akanksha</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Calvin</forename><surname>Lin</surname></persName>
		</author>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Efficient Metadata Management for Irregular Data Prefetching</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nathella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sunwoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<monogr>
		<title level="m" type="main">Toward More Efficient NoC Arbitration: A Deep Reinforcement Learning Approach</title>
		<author>
			<persName><forename type="first">Jieming</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasuko</forename><surname>Eckert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Oskin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><forename type="middle">H</forename><surname>Loh</surname></persName>
		</author>
		<editor>AIDArc</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Experiences with ML-Driven Design: A NoC Case Study</title>
		<author>
			<persName><forename type="first">Jieming</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhash</forename><surname>Sethumurugan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasuko</forename><surname>Eckert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chintan</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Morton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Slurm: Simple Linux Utility for Resource Management</title>
		<author>
			<persName><forename type="first">Andy</forename><forename type="middle">B</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morris</forename><forename type="middle">A</forename><surname>Jette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Grondona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on job scheduling strategies for parallel processing</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="44" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<monogr>
		<title level="m" type="main">BranchNet : Using Offline Deep Learning To Predict Hard-To-Predict Branches</title>
		<author>
			<persName><forename type="first">Siavash</forename><surname>Zangeneh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Pruett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sangkug</forename><surname>Lym</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yale</forename><surname>Patt</surname></persName>
		</author>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<monogr>
		<title level="m" type="main">Branch prediction with multilayer neural networks: The value of specialization. Machine Learning for Computer Architecture and Systems</title>
		<author>
			<persName><forename type="first">Siavash</forename><surname>Zangeneh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Pruett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yale</forename><surname>Patt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Long Short Term Memory based Hardware Prefetcher: A Case Study</title>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaochen</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MEMSYS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Accelerating and Adapting Precomputation Threads for Effcient Prefetching</title>
		<author>
			<persName><forename type="first">Weifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dean</forename><forename type="middle">M</forename><surname>Tullsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brad</forename><surname>Calder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
				<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">An Energy-efficient Network-on-chip Design using Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Louri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DAC</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">A Hardware-based Cache Pollution Filtering Mechanism for Aggressive Prefetches</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-H</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPP</title>
				<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Execution-based Prediction Using Speculative Slices</title>
		<author>
			<persName><forename type="first">Craig</forename><surname>Zilles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gurindar</forename><surname>Sohi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
				<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<monogr>
		<title level="m" type="main">Branch Prediction as a Reinforcement Learning Problem: Why, How and Case Studies</title>
		<author>
			<persName><forename type="first">Anastasios</forename><surname>Zouzias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kleovoulos</forename><surname>Kalaitzidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Grot</surname></persName>
		</author>
		<idno>ArXiv abs/2106.13429. 2021</idno>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
