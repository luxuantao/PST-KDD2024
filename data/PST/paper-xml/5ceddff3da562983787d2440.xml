<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generative Neural Networks for Anomaly Detection in Crowded Scenes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Tian</forename><surname>Wang</surname></persName>
							<email>wangtian@buaa.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Meina</forename><surname>Qiao</surname></persName>
							<email>meinaqiao@buaa.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Zhiwei</forename><surname>Lin</surname></persName>
							<email>z.lin@ulster.ac.uk</email>
						</author>
						<author>
							<persName><forename type="first">Ce</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hichem</forename><surname>Snoussi</surname></persName>
							<email>hichem.snoussi@utt.fr</email>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Zhe</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Chang</forename><surname>Choi</surname></persName>
						</author>
						<author>
							<persName><forename type="middle">C</forename><surname>Choi</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Automation Science and Electrical Engineering</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">Ulster University</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">College of Electrical and Information Engineering</orgName>
								<orgName type="institution">Lanzhou University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="laboratory">Institute Charles Delaunay-LM2S-UMR STMR 6279</orgName>
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">University of Technology of Troyes</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Nanjing University of Aeronautics and Astronautics</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution" key="instit1">Computer Engineering and IT research Institute</orgName>
								<orgName type="institution" key="instit2">Chosun University</orgName>
								<address>
									<country>Rep. of Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Generative Neural Networks for Anomaly Detection in Crowded Scenes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4D1D2070E0F556DA6AD033778713D1E7</idno>
					<idno type="DOI">10.1109/TIFS.2018.2878538</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIFS.2018.2878538, IEEE Transactions on Information Forensics and Security This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIFS.2018.2878538, IEEE Transactions on Information Forensics and Security</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Spatio-temporal</term>
					<term>anomaly detection</term>
					<term>Variational AutoEncoder</term>
					<term>loss function</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Security surveillance is critical to social harmony and people's peaceful life. It has a great impact on strengthening social stability and life safeguarding. Detecting anomaly timely, effectively and efficiently in video surveillance remains challenging. This paper proposes a new approach, called S 2 -VAE, for anomaly detection from video data. The S 2 -VAE consists of two proposed neural networks: a Stacked Fully Connected Variational AutoEncoder (SF -VAE) and a Skip Convolutional VAE (SC -VAE). The SF -VAE is a shallow generative network to obtain a model like Gaussian mixture to fit the distribution of the actual data. The SC -VAE, as a key component of S 2 -VAE, is a deep generative network to take advantages of CNN, VAE and skip connections. Both SF -VAE and SC -VAE are efficient and effective generative networks and they can achieve better performance for detecting both local abnormal events and global abnormal events. The proposed S 2 -VAE is evaluated using four public datasets. The experimental results show that the S 2 -VAE outperforms the state-of-the-art algorithms. The code is available publicly at https://github.com/tianwangbuaa/.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>V IDEO surveillance is a key tool to maintain the security and stability of public scene <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. Densely crowded environments (such as shopping centers, train stations, etc.), are equipped with CCTV cameras to meet the increasing challenges of security issues in these public areas. The surveillance systems generate a large amount of video data. Detecting abnormal events timely, effectively and efficiently from a large amount of video data, without human interaction and monitoring, has become a crucial task in video surveillance.</p><p>In video surveillance, abnormal events can be classified into global abnormal event (GAE) or local abnormal event (LAE) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. We assume that abnormal events happen in the foreground. Most of current research focuses on detecting abnormal events from foreground. The detection of GAE is to identify the frames with an anomaly, while the task of detecting LAE, beyond identifying the frames with an anomaly, is to locate the individuals with abnormal behaviors in the frames. It is more challenging to detect LAE than GAE.</p><p>In this paper, we aim to improve the detection of the LAE and GAE. To this end, we propose to use a self-supervised learning method so that the detection task can be achieved more accurately and efficiently. The proposed algorithm, called S 2 -VAE, includes 2 stages: the first stage is a shallow network, called S F -VAE, with a low resolution input. And the second stage is a deep neural network, called S C -VAE, with a high resolution input. The shallow network S F -VAE was designed to filter out some palpable normal samples quickly, so that the next stage network S C -VAE can learn a model from the remaining samples more effectively and more efficiently.</p><p>Inspired by the Gaussian mixture model (GMM), we design S F -VAE, a new Variational AutoEncoder (VAE) model, so that the GMM-like distributions can be learned with S F -VAE for the raw input data. In our experiments, this S F -VAE is used to learn several latent variables to overcome the limitation of a single latent variable in traditional VAE. The purpose of using S F -VAE is to filter out some obvious normal samples from the original samples, which can significantly reduce the training and testing time in the next stage.</p><p>In the second stage of S 2 -VAE network, the remaining samples are firstly enlarged, and the enlarged samples are fed into S C -VAE. This S C -VAE, is a deep generative network with skip-connection between downsampling layers and upsampling layers. The convolutional operation in S C -VAE can learn hierarchical features and a local relationship from the input, which can not be achieved by the fully connected layers in S F -VAE. This deep S C -VAE network can also integrate low/mid/high level features, and therefore it has stronger learning ability than shallow networks. Finally, from the information theory, the fusion of low-level and highlevel information achieved by skip-connection can reduce the information loss caused by the transmission across layers in the generative network. From the feature representation perspective, the low-level feature can be treated as the auxiliary feature to the high-level feature <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>.</p><p>We show how the proposed S 2 -VAE can be used for anomaly detection in video data in the experiments. Four public datasets are used to evaluate the algorithm's effectiveness and efficiency by comparing with state-of-the-art approaches. From the experimental results, we find that our S 2 -VAE outperforms the state-of-the-art algorithms consistently.</p><p>The contributions of this paper are as follows:</p><p>• a shallow generative neural network built based on VAE, called S F -VAE network is proposed. This network can help to reduce unnecessary normal samples, which helps to improve the speed of the anomaly detection. • a deep generative network with more powerful learning ability, called S C -VAE, is proposed to detect the abnormal event from video data. This S C -VAE network has a skipped encoder-decoder structure, with a build-in VAE.</p><p>The S C -VAE makes full use of the advantages of both CNN and VAE. The network fuses the feature between the encoder layer and the decoder layer, which helps to reduce information loss due to the transmission across layers.</p><p>• the proposed approach was evaluated by using four public datasets. The results show that the proposed approach outperforms state-of-the-art algorithms. The rest of the paper is organized as follows. Section II reviews the related work. Section III presents our S 2 -VAE. The performance of S 2 -VAE is evaluated in Section IV. This paper is concluded in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>The state-of-the-art methods of the abnormal detection can be categorized into: 1) motion based models, and 2) spatiotemporal approaches combining motion with appearance information.</p><p>In the motion based models, the trajectory based method was used to detect motions <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>, since such representations can preserve the temporal structure of the abnormal events. The computational cost rose significantly due to occlusion in complex scenes. Thus, the no-tracking based methods were favored. The descriptors such as quantized optical flow <ref type="bibr" target="#b8">[9]</ref>, social model <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>, co-occurrence matrix based on frame intensity <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>, sptiao-temporal context representations <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>, etc had been proposed. For instance, an algorithm monitoring optical flow in a set of fixed local spatial positions was presented in paper <ref type="bibr" target="#b15">[16]</ref>. The sum of squared differences was transformed into a probability distribution. The likelihood of observations respected to the probability distribution of the observations was calculated, and the likelihood falling below a preset threshold was detected as an alert. The sparse reconstruction cost (SRC) model was introduced in paper <ref type="bibr" target="#b16">[17]</ref> over the multi-scale histogram of optical flow. Due to the insufficient performance of huge training samples in paper <ref type="bibr" target="#b16">[17]</ref>, the weighted orthogonal matching pursuit was adopted in <ref type="bibr" target="#b17">[18]</ref> to improve the ability of the model for handling large samples. With suitable communication technology, the anomaly detection method can used for application <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>. The main limitation of the motion based approach is that it cannot detect abnormal events with a sequence of similar normal actions, and it cannot distinguish among the appearance characteristics.</p><p>The spatio-temporal approaches combining motion with appearance have been very successful in anomaly detection <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>. These models provided a more comprehensive representations than the motion based method. In paper <ref type="bibr" target="#b22">[23]</ref> the video was described by the nearby spatio-temporal interest points (STIPs), then Gaussian process regression (GPR) was adopted to cluster, learn, and infer the appearance and position relationship of the STIPs, finally the abnormal event was detected with competing performance while maintaining lower space-time complexity. The mixture of dynamic textures (MDT) was proposed in paper <ref type="bibr" target="#b23">[24]</ref>. Moreover, a hierarchical mixture of dynamic textures (HMDT) was proposed for handling the high computational cost of paper <ref type="bibr" target="#b23">[24]</ref> later. The events of low-probability were handled using discriminant saliency. The high hierarchical levels and long-range dynamics are important for event representation. Although several models have already been proposed, handcrafted features meet the challenge of universality. The efficient and effective abnormal event detection method consisting of a feature descriptor with a suitable pattern classification method remains an open problem.</p><p>The most recent research in this area is driven by deep neural networks <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>, with some significant achievements in abnormal event detection <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>. The work in paper <ref type="bibr" target="#b28">[29]</ref> used both normal and abnormal events to construct the training samples, and the spatio-temporal information had been taken into account in a convolution neural network in order to fuse the appearance and movement information in video frames. The work in paper <ref type="bibr" target="#b26">[27]</ref> first proposed a fully-connected autoencoder with the handcrafted histograms of gradients (HOG) and histograms of optical flows (HOF) features as input. Then in consideration of feature representation, the video clips were used as input, in order to extract features automatically by the fully convolutional autoencoder. Despite the better performance that deep neural networks gain compared with handcrafted features, the robustness of the feature representation is still needed to be improved.</p><p>It is recognised that the deep neural network, especially the generative models (e.g, VAE) can yield better performance for abnormal event detection. We aim to design new generative models to extract more robust features, so that the LAE and GAE can be detected simultaneously by using the same architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. MODEL ARCHITECTURE</head><p>This section presents our approach for abnormal event detection from video sequences. Fig. <ref type="figure" target="#fig_0">1</ref> presents the workflow and visualization of our approach, including Fully Convolutional Neural network (FCN) <ref type="bibr" target="#b29">[30]</ref> for foreground extraction and our proposed abnormal detection of S F -VAE and S C -VAE. The first row in Fig. <ref type="figure" target="#fig_0">1</ref> is our network, and the second row is the samples and results from the network.</p><p>Suppose we have N + 1 video frames {X i } N +1 i=1 , the first step in this model is to extract the foregrounds {G i } N +1 i=1 from this N + 1 frames by using FCN. The FCN used in this paper is FCN-16s, which is built based on VGG-16 and pre-trained on Pascal VOC 2012 <ref type="bibr" target="#b30">[31]</ref>. Two consecutive foregrounds G i and G i+1 are used to calculate motion feature with the optical flow algorithm <ref type="bibr" target="#b31">[32]</ref>, which results in a set of N motion images {O i } N i=1 represented by the Munsell Color System <ref type="bibr" target="#b31">[32]</ref>. Now, both G i and O i will be used as input to S F -VAE. The S F -VAE </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The S F -VAE network</head><p>The proposed S F -VAE network is used to learn a Gaussian mixture like model. The study of VAE shows that a VAE is a perfect combination of neural network and variational inference <ref type="bibr" target="#b32">[33]</ref>. From the neural network perspective, a VAE is an encoder-decoder architecture and from the variational inference perspective, it consists of an inference procedure and a generation procedure.</p><p>Let x and z be the inputs, where x is the data input to VAE. z as the latent representation of x, is learned by VAE. A VAE can be used to learn a Gaussian model such that p(z|x) ∼ N (µ, σ 2 I) for approximating x. The loss function of VAE is shown as follows:</p><formula xml:id="formula_0">L = -E z∼p θ (z|x) [log q ϕ (x|z)] + KL(p θ (z|x)||p(z)),<label>(1)</label></formula><p>where θ and ϕ are the corresponding parameters to be trained in the encoder and decoder in the network. The first term is the reconstruction error between the input x and the output decoded from z. The second term is the KL (Kullback-Leibler) divergence measuring the similarity between the distribution of z and a known distribution where Gaussian distribution is mostly used.</p><p>Although VAE performs well in several applications, a VAE network with single latent variable may have limited capacity. Therefore, we propose to embed n latent variables in a VAE network for abnormal event detection (shown in Fig. <ref type="figure" target="#fig_1">2</ref>). The solid boxes represent all of the neurons in the corresponding layers, and the black dashed boxes represent the neurons of </p><formula xml:id="formula_1">z 1 z 2 z 3 μ 1 μ 2 μ 3 σ 1 σ 2 σ 3 Input Cell</formula><formula xml:id="formula_2">z i |x i ) ∼ N (µ i , σ 2 i ) where 1 ≤ i ≤ n.</formula><p>We define the loss function of the S F -VAE as:</p><formula xml:id="formula_3">L = -E z∼p θ (z|x) [log q ϕ (x|z 1 , • • • , z n )] + 1 n ∑ n i=1 KL(p θ (z i |x)||p(z i )),<label>(2)</label></formula><p>where the first term is the log-likelihood of the data, or the reconstruction error, and the second term is the average KL divergence between the distribution of the encoded n-latent variable and normal Gaussian distribution p(z i ) ∼ N (0, 1).</p><p>Here, θ and ϕ are similar to the corresponding parameters in the Eq. 1.</p><p>The proposed S F -VAE is inspired by the mixture of several Gaussian distributions. According to the theory of pattern recognition and machine learning, a simple Gaussian distribution does not have the ability to describe complex structures <ref type="bibr" target="#b33">[34]</ref>. However, the mixture of Gaussian distribution is more powerful to fit the distribution of actual data. We will demonstrate this proposed S F -VAE's ability for modeling data in our experiments. The shallow network S F -VAE was designed to filter out some palpable normal samples, so that the next stage network S C -VAE can learn a model from the remaining samples more effectively and more efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The S C -VAE network</head><p>Despite the strength of Gaussian mixture like model in the first stage, it can only filter some normal samples out of the data samples. Since the S F -VAE is still a shallow network, and the input is the direct flatten of the samples without considering the position relationship of the pixels. Then in the second stage of S 2 -VAE, we build a deep network to extract more local relationship and hierarchical features from the input. And at the same time, in order to reduce the information loss across layers, we add a skip connection between low-level features and high-level features by using the concatenation of the feature map along the dimension of channel. Take the two feature maps shown in Fig. <ref type="figure" target="#fig_2">3</ref> with size 20 × 16 × 16 as an example. The two feature maps are in the encoder step and decoder step. 20 and 16 are the height and width. The second 16 is the number of channels. After the skip connection which is labeled as 'M', the new feature map is with size 20×16×32. It is the output of the skip connection and is also the input to the next decoder layer. The feature information is passed across the layers in the end-to-end network. The reason for adding the features from the encoder layer to the decoder layer is that information loss is inevitable in the decoding process. Therefore, it makes sense to combine the low-level features and the high-level features to reduce information loss.</p><p>The S C -VAE network is built by combining U-net <ref type="bibr" target="#b34">[35]</ref>, and VAE (shown in the green rectangle in Fig. <ref type="figure" target="#fig_2">3</ref>). The S C -VAE network can not only extract local relationship and latent variables of the input data, but also integrate the feature maps with same resolution in the downsampling layers and upsampling layers, in order to obtain more accurate pixel-wise reconstruction.</p><p>Since the S C -VAE network is to reconstruct the input data. The loss function for S C -VAE for N training samples is proposed as:</p><formula xml:id="formula_4">L= 1 N N ∑ i=1 ( (x i -xi ) 2 ) +KL (p(z|x)||p(z))+γ ∥w∥ 2 2 , (<label>3</label></formula><formula xml:id="formula_5">)</formula><p>where the first term is the average reconstruction error of the training samples, x is the input of the network, x i is the pixel value of one sample, xi is the output of the network (the reconstruction of x i ). The second term limits the latent variable distribution to be a Gaussian distribution. The last term is a regularizer to avoid over-fit.</p><p>There are also other methods to reduce information loss including highway network <ref type="bibr" target="#b35">[36]</ref>, ResNet network <ref type="bibr" target="#b36">[37]</ref> and so on. They are quite effective but they require very deep architecture. Our network is effective but it is not as deep as them <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>. Therefore the skip connection proposed in this paper is more efficient for training our network. In addition, the built-in VAE network is not a general fully connected network consisting of layers with the same number of neurons, but is a reconstruction of its input. This is also beneficial to reduce information loss. On the other hand, the skip connection is an auxiliary feature added to the highlevel features. The S C -VAE network is a powerful generative network with less information loss and we will demonstrate its ability in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Anomaly detection</head><p>After we use S F -VAE to process the input samples of G i and O i , the output from the S F -VAE network will be resized and the resized images will be the input to the S C -VAE network. For example, if we have 16 × 12 images from S F -VAE, we can resize them to 80 × 60, which is then fed to the S C -VAE network. The convolution operation is similar to VGGNet <ref type="bibr" target="#b37">[38]</ref>. Here, we have an example of how the S C -VAE network operates on a resized images: In this structure, I(i, j, k) is the input data, meaning that k channels of i × j pixels; C is a convolution operation; CC is to perform the same convolution operation twice. P is max-Pooling; Z is Zero-padding; F is to flatten the feature map after the convolution operation, F C represents fullyconnected; R is to reshape the output of the fully-connected layer to a suitable format as input to the latter operation; U is Upsampling; M is to concatenate additional link between the downsampling layers and upsampling layers, as shown in the red rectangle in Fig. <ref type="figure" target="#fig_2">3</ref>. This operation concatenates the lowlevel features and high-level features which have the same resolution.</p><formula xml:id="formula_6">I(80,</formula><p>For accurate detection of an abnormal event, we use both motion and appearance features of the samples. In order to extract robust features, we train the network in every stage twice, one for motion feature extraction, and one for appearance feature extraction. The input samples are optical flow and intensity of the pixels, respectively. After getting the training samples, we then feed them into the S 2 -VAE to represent both motion and appearance features. Since both of the S F -VAE and S C -VAE are generative models, the abnormal event is detected by the reconstruction error of the input with a threshold set by the highest reconstruction cost during training. The final decision is the union set of the motion and appearance anomaly detection results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this section, we conduct experiments to validate the proposed networks. All the experiments are run on an NVIDIA GTX-1080 GPU. We use four benchmark datasets: UCSD <ref type="bibr" target="#b38">[39]</ref>, Avenue <ref type="bibr" target="#b14">[15]</ref>, UMN <ref type="bibr" target="#b39">[40]</ref> and PETS <ref type="bibr" target="#b40">[41]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Pre-processing</head><p>For each video frame X i , its foreground is extracted by using FCN as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. Fig. <ref type="figure" target="#fig_3">4</ref> shows an example of the foreground extraction for the original image.</p><p>For each foreground, blocks are extracted to cover every area in the foreground. To do so, we suppose each block has height b h and weight b w and each block contains cell units, where the size of each cell unit is c h × c w . As such, one block will have b h c h × bw cw cell units. For example, in Fig. <ref type="figure" target="#fig_3">4</ref>, the size of the foreground image is 158 × 238. If the size of each cell is 16 × 12, which is shown in the little red filled rectangle, then we can get at most 9 × 19 ( 158 16 × 238 12 ) cell units in the block, which covers all the pixels between (1, 1) and (16×9, 12×19).</p><p>In order to cover the remaining pixels, we shift the block by a stride of 2 pixels to obtain different blocks so that all the pixels will be covered by a set of blocks. For example, in Fig. <ref type="figure" target="#fig_3">4</ref>, the remaining pixels at the right and at the bottom are 10 pixels and 14 pixels respectively. We will shift the block to the right by 2 pixels to cover the pixels between (1, 1 + 2) and (16 × 9, 12 × 19 + 2), which results in a new block. By continuing this shift to the right or to the bottom, we could obtain 35 blocks in total, so that all the pixels will be covered at least by one block.</p><p>In our experiments, we only keep the cells whose area is covered at least 40% with the foreground. And, the size of the cell is defined as 16 × 12 empirically so that it can cover an action and at the same time reduce the scale of the training samples. The extracted cells will then be used for LAE detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation criteria</head><p>For abnormal event detection, the frame-level criterion is commonly used to evaluate both GAE and LAE detections. But this is not good enough for LAE evaluation. The pixellevel criteria have been proposed for evaluating LAE detection <ref type="bibr" target="#b23">[24]</ref>.</p><p>Frame-level Criterion: The frame-level criterion is the accuracy of detecting abnormal events. A frame is classified as abnormal if an abnormal event is found in this frame, and the frame-level criterion only takes the whole frame into account. For frame-level evaluation, the equal error rate (EER), a tradeoff between accuracy and recall, and the Receiver Operating Characteristic (ROC) curve, will be used. It is defined as the percentage of misclassified frames when the false positive rate equals the false negative rate.</p><p>This frame-level criterion is not an accurate evaluation method. For example, for an abnormal frame with a car on a walkway street as the abnormal event, a model may correctly classify this frame as abnormal but this decision was made based on the wrong detection that classifies a walking person as abnormal. Therefore, the frame-level is not accurate enough to locate a local abnormal event. As a result, we need pixellevel to fill this gap.</p><p>Pixel-level Criterion: This is to locate the abnormal events in a frame, rather than just tell if the frame contains abnormal</p><p>In this case, a frame is classified as abnormal only if the detected abnormal events have more than 40% overlapping with the pixel-level ground truth. So the pixel-level criterion is a more accurate measure for evaluating the quality of the algorithm.</p><p>For pixel-level evaluation, the ROC curve, Rate of Detection (RD), and Area Under receiver operating characteristic Curve (AUC) are used. The RD is defined as the detection rate at equal error. The AUC is the area under the ROC curve. Therefore, if an algorithm is robust enough, then it will have low EER, high RD and high AUC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experimental results</head><p>In this section, we will introduce the procedure and performance comparison of the experiment. The result estimation includes the comparison between the performance of networks with different architectures, and the comparison between proposed S 2 -VAE network and state-of-the-art methods. In the S F -VAE, 3 latent variables are constructed. For the network we propose, we make experiments on different networks similar with the proposed networks, and compare the results among them to make sure the proposed networks have gained the best performance in the aspect of the network. For comprehensive comparison, we compare our algorithm with autoencoder based model such as Conv-AE <ref type="bibr" target="#b26">[27]</ref>, and other state-of-the-art methods, such as Sparse <ref type="bibr" target="#b16">[17]</ref>, MDT <ref type="bibr" target="#b23">[24]</ref>, SF <ref type="bibr" target="#b41">[42]</ref>, MPPCA <ref type="bibr" target="#b42">[43]</ref>, MPPCA+SF <ref type="bibr" target="#b23">[24]</ref>, Adam <ref type="bibr" target="#b15">[16]</ref>, Feng <ref type="bibr" target="#b43">[44]</ref> and so on. And this is to make sure the proposed algorithm has outperformed others in the detection of abnormal event in the aspect of algorithm.</p><p>1) The UCSD dataset: This is an LAE detection dataset, containing sequences taken on a walkway street by a stationary camera <ref type="bibr" target="#b38">[39]</ref>. The density of the pedestrians varies from low to high. Each sequence contains 200 frames, and the resolution of each frame is 158 × 238. The normal events used for training are human walking, while the abnormal events are the frames with moving bikes, cars, wheelchairs and so on.</p><p>For the UCSD PED1 dataset, we first extract the foreground information by FCN network. Then we extract foreground blocks based on the cell units with size 16 × 12 in each frame and calculate their optical flow images. For each sequence in the dataset, as there are 200 frames, optical flow of 199 frames are extracted. We can get 9 × 19 × 199 = 34, 029 cells in the block of one position. After foreground extraction, we can get about 11,000 cells, including both normal and abnormal cells. Then for each cell unit, their raw pixels and their optical flow images are first fed to the S F -VAE network to filter out some normal samples in the first stage, respectively. After S F -VAE, we enlarge the height and weight of the remaining samples to 80 × 60, which are input into S C -VAE in the second stage. The final decision is made based on the union of the motion feature and the appearance feature. The activations used in all of the neural network are Relu <ref type="bibr" target="#b44">[45]</ref>, and the optimizer is Adam with learning rate of 1e-4. The results are shown in the 3D figures Fig. <ref type="figure" target="#fig_5">5</ref>. In this dataset, each frame has 35 blocks, meaning that a pixel will be contained in at most 35 cells. The value for each pixel in Fig. <ref type="figure" target="#fig_5">5 (e,</ref><ref type="figure">f,</ref><ref type="figure">g,</ref><ref type="figure">h</ref>) is calculated based on the number of cells which contains the pixel and are classified  To show the advantage of using both S F -VAE and S C -VAE networks, we do experiments to prove the effectiveness of the proposed networks. The results are shown in Table <ref type="table" target="#tab_1">I</ref> and II. In Table <ref type="table" target="#tab_1">I</ref>, we compare the performance on the proposed S F -VAE with general VAE network by using the filter rate, where the filter rate is the proportion of filtered normal samples in all of the testing samples. We find that S F -VAE has higher filter rate than the normal VAE. For stage 2, after using the S F -VAE network for the first stage, we compare the performance on the proposed S C -VAE with networks: 1) without the skipconnection in S C -VAE; 2) without VAE, namely F (640) → F C(640) → F C(640) in the architecture. The ROC of them is shown in Fig. <ref type="figure" target="#fig_4">6</ref> and the AUC is shown in Table <ref type="table" target="#tab_2">II</ref>. The pixellevel AUC of the 3 networks is 0.9425, 0.7629, and 0.9303, respectively, which proves the advantage of using the S C -VAE network.</p><p>We also compare the proposed approach with the state-ofthe-art algorithms, shown in Fig. <ref type="figure">7</ref> for the ROC curve, in Table <ref type="table" target="#tab_3">III</ref> for EER, RD and Our approach has lower EER, higher RD and higher AUC, compared to state-of-the-art   2) The Avenue dataset: The Avenue dataset is an anomaly detection dataset provided by Lu et al. <ref type="bibr" target="#b14">[15]</ref>. Since the ground truth of the Avenue dataset has been labeled by rectangles, it can be treated as an LAE dataset. There are <ref type="bibr" target="#b15">16</ref>    3) The UMN dataset: This is a GAE detection dataset with three scenes: lawn, indoor and plaza. The image resolution of the dataset is 240 × 320. The global frame is handled by the proposed S 2 -VAE method. In this dataset, the normal scenes are the events of people walking around, while abnormal scenes are the events of people running.</p><p>For the UMN dataset, as the behaviors (people running) of the abnormal events are similar in the scenes, we aim to train a model only on the lawn scene and then transfer this model to the indoor and plaza scenes. We show the experimental results in Table <ref type="table" target="#tab_6">V</ref>. From the results, we find that our approach gains higher AUC, which also means our model is transferable.</p><p>4) The PETS dataset: This is a GAE detection dataset, captured by multiple cameras. The image resolution of PETS dataset is 576 × 768. This dataset has been applied to different tasks: event recognition, tracking, etc <ref type="bibr" target="#b40">[41]</ref>. There are 2 different scenarios of abnormal events in this scene. In the first scenario, the normal events are defined as people walking in different directions, while the abnormal events are defined as people gathering and walking ahead in the same direction. In the second scenario, the normal events are defined as people walking in one queue, while the abnormal events are defined as people leaving the queue.</p><p>For the PETS dataset, since the abnormal events are different in different scenarios, we train the model by the normal samples in each of the scenarios. Similar to UMN dataset, the proposed algorithm also works well on the PETS dataset. The results are shown in Table <ref type="table" target="#tab_7">VI</ref>.</p><p>All of the experiments on different networks comparison demonstrate the superiority of our proposed network. The proposed network exploits the advantage of the robust feature extraction of CNN, the data representation of VAE, and the fusion of skip connections. This can reduce the information loss and gain a finer reconstruction of the input. As a result, the S 2 -VAE gains excellent performance on abnormal event detection. And this superiority is also proved obviously by the latter experiments of both the comparison among similar networks and the comparison among state-of-the-art algorithms on the detection of LAE as well as GAE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>Abnormal event detection from video sequences remains very challenging, due to the complexity of the video data. In this paper, a 2-stage algorithm, i.e. S 2 -VAE, is proposed for the detection of both local abnormal event and global abnormal event. The proposed algorithm consists of 2 networks: S F -VAE and S C -VAE. The S F -VAE network in the first stage is a shallow generative network for the powerful description of data distribution. It is used to filter out some unnecessary normal samples quickly. Then the S C -VAE in the second stage is a deep generative network for accurately locating the abnormal events. The skip connection in S C -VAE is to make the low-level features added to the high-level features as auxiliary features. In addition, the skip connection can also be viewed as the fusion of the information between the encoder and decoder, which can reduce the information loss across layers. And the VAE in the hidden layer also has the same effect. Finally, we show the effectiveness and efficiency of our proposed algorithm by comparing with state-of-the-art approaches with four public datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig.1:The architecture for anomaly detection with a fully convolutional network (FCN), S F -VAE and S C -VAE. This architecture aims to detect LAE and GAE. For example, in LAE detection, it should be able to identify the abnormal objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: The architecture of S F -VAE in the first stage. The appearance of the region of the interest is taken as the training and testing samples in this figure. The motion based feature can also be handled in the same architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: The structure of the S C -VAE in the second stage. C: Convolution. CC: Convolution twice. P: max-Pooling. F: Flatten. R: Reshape. U: Upsampling. Z: Zero-padding. M: Merge link between the downsampling layers and upsampling layers.</figDesc><graphic coords="5,198.47,212.31,51.70,51.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: The pre-processing for foreground detection and cell extraction.</figDesc><graphic coords="5,440.83,347.25,102.79,69.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: The network comparison in the second stage on the UCSD dataset. Pixel-level ROC comparison between the S C -VAE and other similar networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Examples of LAE detection via our proposed algorithm. The hikes in (e), (f), (g) and (h) indicate objects being identified as abnormal. The values of the hikes are the number of times for a pixel being identified as abnormal in cells.</figDesc><graphic coords="7,98.18,151.01,100.39,75.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>60, 3) → CC(80, 60, 64) → P (40, 30, 64) → Z(40, 32, 64) → CC(40, 32, 32) → P (20, 16, 32) → CC(20, 16, 16) → P (10, 8, 16) → CC(10, 8, 8) → F (640) → F C(6) → F C(640) → R(10, 8, 8) → U (20, 16, 8) → C(20, 16, 16) → M (20, 16, 32) → CC(20, 16, 32) → U (40, 32, 32) → C(40, 32, 32) → M (40, 32, 64) → CC(40, 32, 64) → C(40, 30, 64) → U (80, 60, 64) → C(80, 60, 64) → M (80, 60, 128) → CC(80, 60, 128) → C(80, 60, 3).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>The network comparison in the first stage. Comparison of our S F -VAE network with general VAE networks.</figDesc><table><row><cell>Stage 1</cell><cell cols="2">UCSD result</cell></row><row><cell>Filter rate</cell><cell>S F -VAE 5.7778 %</cell><cell>VAE 1.1858 %</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>The network comparison in the second stage. Comparison of our S C network with other similar networks. It is obvious that pixels of the hikes in the 3D figures are the objects which are identified as abnormal.</figDesc><table><row><cell>Stage 2</cell><cell>UCSD result</cell><cell></cell></row><row><cell>Pixel-level AUC</cell><cell>S C -VAE No skip 0.9425 0.7629</cell><cell>FC 0.9303</cell></row><row><cell>as abnormal.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III :</head><label>III</label><figDesc>The algorithm comparison in our experiment. Comparison of our method with state-of-the-art methods for LAE of UCSD PED1 dataset. The best performances are shown in bold font. The F and P in brackets represents that the criterion is for the frame-level or the pixel-level. methods. In the S 2 -VAE, on one hand, the S C -VAE exploits robust feature extraction of CNN and data representation of VAE; on the other hand, the skip connections designed in the S C -VAE can reduce information loss to gain a finer reconstruction of the input. Also, the first stage of S F -VAE contributes to the detection of abnormal events by filtering out some normal samples effectively and reduce the number of input samples to the S C -VAE. Thus, the performance of S C -VAE can be improved by training without unnecessary normal samples.</figDesc><table><row><cell>Method</cell><cell cols="3">Evaluation Criteria EER (F) RD (P) AUC (P)</cell></row><row><cell>Sparse [17]</cell><cell>19 %</cell><cell>46 %</cell><cell>46.1 %</cell></row><row><cell>Adam [16, 24]</cell><cell>38 %</cell><cell>24 %</cell><cell>13.3 %</cell></row><row><cell>MPPCA+SF[24]</cell><cell>32 %</cell><cell>27 %</cell><cell>21.3 %</cell></row><row><cell>SF [42]</cell><cell>31 %</cell><cell>21 %</cell><cell>17.9 %</cell></row><row><cell>MPPCA [24, 43]</cell><cell>40 %</cell><cell>18 %</cell><cell>20.5 %</cell></row><row><cell>MDT [24]</cell><cell>25 %</cell><cell>45 %</cell><cell>44.1 %</cell></row><row><cell>HOG+HOS [12]</cell><cell>27.02 %</cell><cell>78.87 %</cell><cell>-</cell></row><row><cell>Conv-AE [27]</cell><cell>27.9 %</cell><cell>-</cell><cell>81.0 %</cell></row><row><cell>Lu [15]</cell><cell>-</cell><cell>59.1%</cell><cell>63.8%</cell></row><row><cell>sRNN [9]</cell><cell>12.5 %</cell><cell>-</cell><cell>89.9 %</cell></row><row><cell>Feng [44]</cell><cell>-</cell><cell>64.9 %</cell><cell>69.9 %</cell></row><row><cell>S 2 -VAE (ours)</cell><cell>14.3 %</cell><cell>87.4 %</cell><cell>94.25 %</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>video clips for training, and 21 video clips for testing. The abnormal events</figDesc><table><row><cell></cell><cell>1</cell><cell></cell><cell cols="2">UCSD Frame-level ROC</cell><cell></cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>True Positive Rate</cell><cell>0.4 0.6</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Sparse</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Adam</cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell cols="2">MPPCA+SF SF</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">MPPCA</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">MDT</cell></row><row><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell>S</cell><cell>2 -VAE</cell></row><row><cell></cell><cell>0</cell><cell>0.2</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">False Positive Rate</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">(a) Frame level ROC</cell><cell></cell></row><row><cell></cell><cell>1</cell><cell></cell><cell cols="2">UCSD Pixel-level ROC</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Sparse</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Adam</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">MPPCA+SF</cell><cell></cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell cols="2">SF</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">MPPCA</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">MDT</cell><cell></cell></row><row><cell>True Positive</cell><cell>0.4 0.6</cell><cell></cell><cell>S</cell><cell>2 -VAE</cell><cell></cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>0.2</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">False Positive</cell><cell></cell></row></table><note><p>(b) Pixel level ROC Fig. 7: ROC comparisons of UCSD PED1 dataset. (a) Framelevel ROC for UCSD dataset. (b) Pixel-level ROC for UCSD dataset. The ROC of the compared algorithm is extracted from [17, 24].</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV :</head><label>IV</label><figDesc>The algorithm comparison in our experiment. Comparison of our method with state-of-the-art methods for LAE of Avenue dataset. The best performance is shown in bold font.</figDesc><table><row><cell>Method</cell><cell>AUC</cell></row><row><cell>Conv-AE [27]</cell><cell>70.2 %</cell></row><row><cell>Lu [15]</cell><cell>80.5 %</cell></row><row><cell>sRNN [9]</cell><cell>81.71 %</cell></row><row><cell>Spatiotemporal-AE [46]</cell><cell>80.3 %</cell></row><row><cell>Feng [44]</cell><cell>75.4 %</cell></row><row><cell>S 2 -VAE (ours)</cell><cell>87.6 %</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V :</head><label>V</label><figDesc>The algorithm comparison in our experiment. Comparison of our method with state-of-the-art methods for GAE of UMN dataset. The best performances are shown in bold font. Compared with UCSD dataset, the Avenue dataset has higher resolution. The comparison, according to frame-level AUC, between our S 2 -VAE with other algorithms is shown in TableIV. As can be found in the table, our method has better results than state-of-the-art methods.</figDesc><table><row><cell>Method</cell><cell>lawn</cell><cell>AUC indoor</cell><cell>plaza</cell></row><row><cell>Social Force [42]</cell><cell></cell><cell>96 %</cell><cell></cell></row><row><cell>NN [17]</cell><cell></cell><cell>93 %</cell><cell></cell></row><row><cell>HOG+HOS [12]</cell><cell></cell><cell>97.02 %</cell><cell></cell></row><row><cell>SRC [17]</cell><cell>99.5 %</cell><cell>97.5 %</cell><cell>96.4 %</cell></row><row><cell>HOFO [9]</cell><cell>98.45 %</cell><cell>90.37 %</cell><cell>98.15 %</cell></row><row><cell>CLP [47]</cell><cell>98.72 %</cell><cell>95.21 %</cell><cell>99.34 %</cell></row><row><cell>S 2 -VAE (ours)</cell><cell>100 %</cell><cell cols="2">99.92 % 99.51 %</cell></row><row><cell cols="4">include running, abnormal direction and so on. The resolution</cell></row><row><cell cols="2">of each frame is 360 × 640.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VI :</head><label>VI</label><figDesc>The algorithm comparison in our experiment. Comparison of our method with state-of-the-art methods for GAE on the Time14-17 scene and the Time14-31 scene in the PETS dataset.</figDesc><table><row><cell>Method</cell><cell cols="2">Detection accuracy Time 1417 Time 1431</cell></row><row><cell>DT [48]</cell><cell cols="2">93.8 %</cell></row><row><cell>BoTG [48]</cell><cell cols="2">91.2 %</cell></row><row><cell>HOFO [9]</cell><cell>97.8 %</cell><cell>94.6 %</cell></row><row><cell>S 2 -VAE (ours)</cell><cell>99.3 %</cell><cell>98.8 %</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>JOURNAL OF L A T E X CLASS FILES, VOL. 14, NO. 8, OCTOBER 2018</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>This work is partially supported by the National Natural Science Foundation of China (No. 61503017, U1435220, 61866022, 61802180), the Aeronautical Science Foundation of China (No. 2016ZC51022), the SURECAP CPER project, the EU Horizon 2020 research and innovation programme under grant agreement (No. 690238) for DESIREE project, the UK EPSRC (No. EP/P031668/1), the BT Ireland Innovation Centre (BTIIC), and the Platform CAPSEC funded by Région Champagne-Ardenne and FEDER. Also, this work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (Ministry of Science and ICT) (No. 2017R1E1A1A01077913). Specifically, thank Dr. Yi Zhao, from Department of Computer Science and Technology, Tsinghua University, for his useful advices on experiment design and comments in manuscript preparation.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Zhiwei Lin received his DPhil from Ulster University in 2010. Following his PhD degree, he became a researcher in business intelligence at SAP before moving to British Sky Broadcasting as a data scientist. Dr. Lin completed his industrial career with Oracle before he returned to Ulster University as a lecturer in computer science. Since then, his research is focused on machine learning and data analytics. His research interests include intelligent information processing, semantic web, smart IoT system and intelligent system security.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ce</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semantic face signatures: Recognizing and retrieving faces by verbal descriptions</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">Y</forename><surname>Almudhahka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Nixon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Hare</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="706" to="716" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Abandoned object detection via temporal consistency modeling and back-tracing verification for visual surveillance</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-P</forename><surname>Hung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1359" to="1370" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A review of novelty detection</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Pimentel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Clifton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Clifton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tarassenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="215" to="249" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Video-based abnormal human behavior recognition-a review</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">P</forename><surname>Popoola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics, Part C: Applications and Reviews</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="865" to="878" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Training hierarchical feed-forward visual recognition models using transfer learning from pseudo-tasks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="69" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">3d convolutional neural networks for human action recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="221" to="231" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Abnormal event detection via covariance matrix for optical flow based feature</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Snoussi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia Tools and Applications</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Chaotic invariants of lagrangian particle trajectories for anomaly detection in crowded scenes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2054" to="2060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Detection of abnormal visual events via global optical flow orientation histogram</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Snoussi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="988" to="998" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Social network model for crowd anomaly detection and localization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Chaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">Al</forename><surname>Aghbari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">N</forename><surname>Junejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="266" to="281" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Abnormal crowd behavior detection by social force optimization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Raghavendra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Del Bue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Behavior Understanding</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="134" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Swarm intelligence for detecting interesting events in crowded environments</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kaltsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Briassouli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kompatsiaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Hadjileontiadis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Strintzis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2153" to="2166" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Real-time abnormal event detection in complicated scenes</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Pattern Recognition (ICPR)</title>
		<meeting>International Conference on Pattern Recognition (ICPR)<address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="3653" to="3656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Quaternion discrete cosine transformation signature analysis in crowd scenes for abnormal event detection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">204</biblScope>
			<biblScope unit="page" from="106" to="115" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Abnormal event detection at 150 fps in matlab</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Robust real-time unusual event detection using multiple fixed-location monitors</title>
		<author>
			<persName><forename type="first">A</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Shimshoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Reinitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="555" to="560" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sparse reconstruction cost for abnormal event detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="3449" to="3456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Abnormal event detection in crowded scenes using sparse representation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="1851" to="1864" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Interference cancellation for non-orthogonal multiple access used in future wireless mobile networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Wireless Communications and Networking</title>
		<imprint>
			<biblScope unit="volume">2016</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">231</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Intelligent approaches for security technologies</title>
		<author>
			<persName><forename type="first">C</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Ogiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Concurrency and Computation: Practice and Experience</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Anomaly detection and localization in crowded scenes</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="18" to="32" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Combining motion and appearance cues for anomaly detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="443" to="452" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Gaussian process regression-based video anomaly detection and localization with hierarchical feature representation</title>
		<author>
			<persName><forename type="first">K.-W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-H</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5288" to="5301" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Anomaly detection in crowded scenes</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bhalodia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1975" to="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Internal transfer learning for improving performance in human action recognition for small datasets</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Snoussi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">633</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A fast and robust convolutional neural network-based defect detection model in product quality control</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Snoussi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Advanced Manufacturing Technology</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">9-12</biblScope>
			<biblScope unit="page" from="3465" to="3471" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning temporal regularity in video sequences</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="733" to="742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sparse canonical temporal alignment with deep tensor decomposition for action recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="738" to="750" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Spatial-temporal convolutional neural networks for anomaly detection and localization in crowded scenes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing: Image Communication</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="358" to="368" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><surname>Voc</surname></persName>
		</author>
		<ptr target="http://host.robots.ox.ac.uk/pascal/voc/voc2012/" />
		<title level="m">Pascal voc 2012 dataset</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Secrets of optical flow estimation and their principles</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2432" to="2439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Tutorial on variational autoencoders</title>
		<author>
			<persName><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05908</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pattern recognition and machine learning</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">P</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<date type="published" when="2006">2006</date>
			<publisher>springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<meeting>International Conference on Medical Image Computing and Computer-Assisted Intervention</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00387</idno>
		<title level="m">Highway networks</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">UCSD anomaly detection dataset</title>
		<author>
			<persName><surname>Ucsd</surname></persName>
		</author>
		<ptr target="http://www.svcl.ucsd.edu/projects/anoma-ly/dataset.html" />
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><surname>Umn</surname></persName>
		</author>
		<ptr target="http://mha.cs.umn.edu/movies/crowd-activity-all.avi" />
		<title level="m">Unusual crowd activity dataset of University of Minnesota</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Performance evaluation of tracking and surveillance (PETS) 2009 benchmark data. multisensor sequences containing different crowd activities</title>
		<author>
			<persName><surname>Pets</surname></persName>
		</author>
		<ptr target="http://www.cvg.rdg.ac.uk/pets2009/a.html" />
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Abnormal crowd behavior detection using social force model</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mehran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="935" to="942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Observe locally, infer globally: a space-time mrf for detecting abnormal activities with incremental updates</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="2921" to="2928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning deep event models for crowd anomaly detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">219</biblScope>
			<biblScope unit="page" from="548" to="556" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Abnormal event detection in videos using spatiotemporal autoencoder</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="189" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Crowd behavior analysis using local mid-level visual descriptors</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fradi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Luvison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">C</forename><surname>Pham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="589" to="602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Crowd behavior recognition using dense trajectories</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Khokher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bouzerdoum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Phung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Digital lmage Computing: Techniques and Applications (DlCTA)</title>
		<meeting>International Conference on Digital lmage Computing: Techniques and Applications (DlCTA)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">His research interests include computer vision and pattern recognition. Meina Qiao is currently pursuing the M.S. degree in School of Automation Science and Electrical Engineering in Beihang University. She is involved in abnormal events detection and video surveillance. Her academic interests are computer vision and machine learning</title>
	</analytic>
	<monogr>
		<title level="m">France in 2010 and 2014, respectively</title>
		<imprint>
			<publisher>Tian Wang received the M.S. degree and Ph</publisher>
		</imprint>
		<respStmt>
			<orgName>Xian Jiaotong University, China and University of Technology of Troyes</orgName>
		</respStmt>
	</monogr>
	<note>He is an assistant professor at the School of Automation of Science and Electrical Engineering, Beihang University</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
