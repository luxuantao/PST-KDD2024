<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Motion-based Recognition of People in EigenGait Space</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chiraz</forename><surname>Benabdelkaderý</surname></persName>
							<email>chiraz@umiacs.umd.edu</email>
						</author>
						<author>
							<persName><forename type="first">Ross</forename><surname>Cutlerþ</surname></persName>
							<email>rcutler@microsoft.com</email>
						</author>
						<author>
							<persName><forename type="first">Larry</forename><surname>Davisý</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Ý University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">ÞMicrosoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Motion-based Recognition of People in EigenGait Space</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">61852040D1AC297C8FCEFC660A00AD7A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A motion-based, correspondence-free technique for human gait recognition in monocular video is presented. We contend that the planar dynamics of a walking person are encoded in a 2D plot consisting of the pairwise image similarities of the sequence of images of the person, and that gait recognition can be achieved via standard pattern classification of these plots. We use background modelling to track the person for a number of frames and extract a sequence of segmented images of the person. The self-similarity plot is computed via correlation of each pair of images in this sequence. For recognition, the method applies Principal Component Analysis to reduce the dimensionality of the plots, then uses the k-nearest neighbor rule in this reduced space to classify an unknown person. This method is robust to tracking and segmentation errors, and to variation in clothing and background. It is also invariant to small changes in camera viewpoint and walking speed. The method is tested on outdoor sequences of 44 people with 4 sequences of each taken on two different days, and achieves a classification rate of 77%. It is also tested on indoor sequences of 7 people walking on a treadmill, taken from 8 different viewpoints and on 7 different days. A classification rate of 78% is obtained for near-fronto-parallel views, and 65% on average over all view.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, gait recognition has received growing interest within the computer vision community, due to its emergent importance as a biometric. The term gait recognition is typically used to signify the identification of individuals in image sequences 'by the way they walk'. Gait classification is the recognition of different types of human locomotion, such as running, limping, hopping, etc. Because human ambulation is one form of human movement, gait recognition is closely related to vision methods that detect, track and analyze human movement in general.</p><p>Gait recognition research has largely been motivated by Johansson's experiments <ref type="bibr" target="#b18">[19]</ref> and the ability of humans to perceive motion from Moving Light Displays (MLDs). In these experiments, human subjects were able to recognize the type of move-ment of a person solely from observing the 2D motion pattern generated by light bulbs attached to the person. Similar experiments later showed some evidence that the identity of a familiar person ('a friend') <ref type="bibr" target="#b0">[1]</ref>, as well as the gender of the person <ref type="bibr" target="#b8">[9]</ref> might be recognizable from MLDs, though in the latter case a recognition rate of 60% is hardly significantly better than chance (50%).</p><p>Despite the agreement that humans can perceive motion from MLDs, there is still no consensus on how humans interpret this MLD-type stimuli (i.e. how it is they use it to achieve motion recognition). Two main theories exist: the first maintains that people use motion information in the MLDs to recover the 3D structure of the moving object (person), and subsequently use the structure for recognition; and the second theory states that motion information is directly used to recognize a motion <ref type="bibr" target="#b6">[7]</ref>.</p><p>The dynamics of gait can be fully characterized via the kinematics of a handful of body landmarks such as limbs and joints <ref type="bibr" target="#b17">[18]</ref>. Indeed, one method of motion-based recognition is to first explicitly extract the dynamics of points on a moving object (person). Consider a point È ´Øµ ´Ü´Øµ Ý ´Øµ Þ ´Øµµ on a moving object as a function of time Ø. The dynamics of the point can be represented by the phase plot ´ È ´Øµ È ´Øµ Ø µ. Since we wish to recognize different types of motions (viz. gaits), it is important to know what can be determined from the projection of È ´Øµ onto an image plane, ´Ù Úµ</p><p>´ È µ. Under orthographic projection, and if È ´Øµ is constrained to planar motion, the object dy- namics are completely preserved up to a scalar factor. That is, the phase space for the point constructed from ´Ù Úµ is identical (up to a scalar factor) to the phase space constructed from È ´Øµ. However, if the motion is not constrained to a plane, then the dynamics are not preserved. Under perspective projection, the dynamics of planar and arbitrary motion are in general not preserved.</p><p>Fortunately, planar motion is an important class of motion, and includes "biological motion" <ref type="bibr" target="#b15">[16]</ref>. In addition, if the person is sufficiently far from the camera, the camera projection becomes approximately orthographic (with scaling). In this case, and assuming we can accurately track a point È ´Øµ in the image plane, then we can completely reconstruct the phase space of the dynamic system (up to a scalar factor). The phase space can then be used directly to classify the object motion (e.g., <ref type="bibr" target="#b5">[6]</ref>).</p><p>In general, point correspondence is not always possible in realistic image sequences (without the use of special markers), due to occlusion boundaries, lighting changes, insufficient texture, image noise, etc. However, for classifying motions, we do not necessarily have to extract the complete dynamics of the system; qualitative measures may suffice to distinguish a class of motions from each other. In this paper, we use a correspondence-free image feature for motion-based gait recognition.</p><p>Our method maps a sequence of images of a walking person to a similarity plot (SP), defined as the matrix of self-similarities between each pair of images of the person in the sequence. We contend that this 2D feature encodes a projection of the planar dynamics of gait, and hence a signature of gait dynamics. The feature vectors used for classification consist of the contiguous square blocks, termed Units of Self-Similarity (USS), in the SP of size one gait period each. Our method treats a USS much the same way that the Eigenfaces technique <ref type="bibr" target="#b27">[28]</ref> treats a face image; it uses Principal Components Analysis (PCA) to reduce the dimensionality of the feature space, then applies some supervised pattern classification technique (k-nearest neighbor rule in our case) in the reduced feature space for recognition, termed the Eigengait.</p><p>This method is invariant to background texture and lighting, and clothing, and is robust to segmentation errors. It assumes that people walk on a known plane with constant velocity for about 3-4 seconds (the time it takes to walk 5-8 steps at normal speed), the frame rate is greater than twice the frequency of walking, and the camera is static.</p><p>The rest of the paper is organized as follows. Section 2 reviews recent vision literature related to gait recognition. In Section 3 we describe the method in detail. Section 4 describes the experimental methodology and results, and finally in Section 5 we conclude with a brief summary and discussion of future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>We review vision methods used in detection, tracking and recognition of human movement in general, as they are closely related to gait recognition ( <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b12">13]</ref> are good surveys on this topic). These methods can be divided into two main categories: methods that recover high-level structure of the body and use this structure for motion recognition, and those that directly model how the person moves. We shall describe the latter in more detail as it is more relevant to the gait recognition approach proposed in this paper.</p><p>Structure-free methods characterize its motion pattern, without regard to its underlying structure. They can be further divided into two main classes. The first class of methods consider the human action or gait to be comprised of a sequence of poses of the moving person, and recognize it by recognizing a sequence of static configurations of the body in each pose <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b14">15]</ref>. The second class of methods characterizes the spatiotemporal distribution generated by the motion in its continuum, and hence analyze the spatial and temporal dimensions simultaneously <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b7">8]</ref>.</p><p>State-space methods represent human movement as a sequence of static configurations. Each configuration is recognized by learning the appearance of the body (as a function of its color/texture, shape or motion flow) in the corresponding pose. Murase and Sakai <ref type="bibr" target="#b22">[23]</ref> describe a template matching method which uses the parametric eigenspace representation as applied in face recognition <ref type="bibr" target="#b27">[28]</ref>. Huang et al. <ref type="bibr" target="#b16">[17]</ref> use a similar technique, as they apply PCA to map the binary silhouette of the moving figure to a low dimensional feature space. The gait of an individual person is represented as a cluster in this space, and gait recognition is done by determining if all the input silhouettes belong to this cluster. He and Debrunner <ref type="bibr" target="#b14">[15]</ref> recognize individual gaits via an HMM that uses a quantized vector of Hu moments computed from the person's binary silhouette as input.</p><p>In spatiotemporal methods the action or motion is characterized via the entire 3D spatiotemporal (XYT) data volume spanned by the moving person in the image. It could for example consist of a sequence of grey-scale images, optical flow images, or binary silhouettes of the person. Of particular interest is the work by Cutler and Davis <ref type="bibr" target="#b7">[8]</ref> which is closely related to our method. They use similarity plots to characterize periodicity of human motion, and thereby detect humans in video (and not for gait recognition).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>An overview diagram of the method is shown in Figure <ref type="figure" target="#fig_0">1</ref>. An input image sequence is first processed to segment the moving person from the background and track it in each frame. The obtained sequence of blobs are then properly aligned and scaled to a uniform height, to account for detection/tracking errors and any depth changes that occur in non-fronto-parallel walking. A selfsimilarity plot (SP) of the person is computed by correlating each pair of these blobs, and a set of normalized feature vectors, we call Units of Self-Similarity, are then extracted from this SP and used for gait recognition via standard statistical pattern classification technique. In the following two sections, we explain our methods for feature extraction/selection and classification in more detail. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Feature Extraction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Self-similarity Plots</head><p>Given a sequence of images obtained from a static camera, we detect and track the moving person, extract an image template corresponding to the person's motion blob in each frame, then compute the self-similarity plot from the obtained sequence of templates.</p><p>For this, we use the method described in <ref type="bibr" target="#b7">[8]</ref>, except that we use background subtraction for foreground detection since the camera is assumed to be stationary <ref type="bibr" target="#b11">[12]</ref>.</p><p>Once a person has been tracked for AE consecutive frames, the corresponding sequence of AE image templates are scaled to a uniform height, as their sizes may vary due to depth changes and segmentation errors, and the self-similarity plot Ë is obtained by:</p><formula xml:id="formula_0">Ë´Ø½ Ø ¾µ Ñ Ò Ü Ý Ö ´Ü Ýµ¾ Ø ½ ÇØ ½ ´Ü • Ü Ý • Ýµ ÇØ ¾ ´Ü Ýµ</formula><p>where ½ Ø½ Ø ¾ AE , Ø ½ is the bounding box of the person blob in frame Ø½, Ö is a small search radius, and ÇØ ½ Ç Ø ¾ ÇØ AE are the scaled templates. Note that these templates can be either (1) foreground images or (2) binary silhouettes, as shown in Figure <ref type="figure" target="#fig_1">2</ref>. There are clearly competing tradeoffs to using either type of template in measuring image similarity: the latter is more robust to clothing and lighting variations than the former, but is less robust to segmentation errors. We shall later compare these two image similarity measures empirically in the experiments (Section 4). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Properties of Self-similarity Plots</head><p>The self-similarity plot Ë of a walking person has some useful properties <ref type="bibr" target="#b7">[8]</ref>. For example, the intersections of its off-diagonals and cross-diagonals, which are also its local minima, encode the frequency and phase of walking. Specifically, each intersection corresponds to a combination of the following four key poses of gait: (i) when the two legs are furthest apart and the left leg is leading, (ii) when the two legs are joined together and the right leg is leading, (iii) when the two legs are furthest apart and the left leg is leading, and (iv) when the two legs are joined together and the left leg is leading, as illustrated by Figure <ref type="figure" target="#fig_2">3</ref>. We shall denote these poses as , , , and , respectively. Note that diagonals corresponding to and only exist in the similarity plot of near fronto-parallel walking. Intuitively, this is because poses and , and poses and are very similar in appearance only if the person is walking nearly fronto-parallel to the camera and the person's gait is almost bilaterally symmetrical (i.e. the right and left leg are functionally indistinguishable which is not the case when the person has a limp).</p><p>Thus the frequency and phase of gait can be simply computed by finding the local minima of Ë. However, we can only resolve the phase of gait up to half a period, hence poses and and poses and are indistinguishable. We currently have no way of determining whether the left or right leg is leading, which is a difficult visual problem in itself (it can be achieved in principle by first determining the direction of walking, then carefully segmenting the two legs to determine which leg occludes the other, i.e. which leg is closer to the camera). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Units of Self-Similarity</head><p>Because gait consists of periodic contiguous steps, the similarity plot can be tiled into contiguous rectangular (square for constantspeed walking) blocks, termed Units of Self-Similarity (USS), each of which consists of the person's self-similarity over two periods of gait (Figure <ref type="figure" target="#fig_3">4</ref>). Clearly a different such tiling is obtained for each starting phase of the periods.</p><p>In this paper, we propose to use the set of all USS's starting at key-pose or (respectively the blue and green tiles in Figure <ref type="figure" target="#fig_3">4</ref>) as our input feature vectors for gait recognition. We only use the USS's in the top half of the similarity plot (the solid tiles), since they are symmetric to the USS's in the bottom half (the brokenline tiles). Hence, for a sequence containing gait periods ( in the Figure <ref type="figure" target="#fig_3">4</ref>), we can extract ¾ ´ •½µ ¾ ´ • ½ µ such USS's. Note, however, that since we can only resolve the phase of gait up to half a period (as discussed in Section 3.1.2), the USS's starting at pose are in fact indistinguishable from those starting at pose , which is why we extract both for recognition. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">Normalization</head><p>In any pattern classifier, it is important to determine which sources of variation in the input data are irrelevant to classification, and normalize them prior to classification <ref type="bibr" target="#b10">[11]</ref>. In our case, a USS of the same walking person will vary with: (i) clothing, (ii) the background scene, (iii) number of pixels on target, (iv) camera viewpoint, and (v) walking speed. By using background subtraction to obtain person templates, we effectively normalize for background variations. A simple way to normalize for variation due to clothing and lighting is by using a color-invariant image similarity measure, such as absolute correlation of binary silhouettes or chamfer matching of the edge maps. However this will not normalize for the style of clothing (for example pants vs. skirts), nor for detection/segmentation errors.</p><p>The number of pixels-on-target (POT) is a function of camera depth and image resolution. Assuming the roles of these two are interchangeable, the POT is normalized by scaling down each template such that ¡ ÔÝ is some fixed constant, where is the height of the person in the scaled image and ÔÝ is the effective Ý dimension of a pixel in the frame grabber <ref type="bibr" target="#b26">[27]</ref>.</p><p>Since the size of a USS (equal to one period gait) may vary depending on the actual gait period (speed of walking), we scale them to a uniform Ì xÌ square tile. This is equivalent to temporalwarping. Note, however, that this does not normalize for the different walking speeds in any qualitative way. It only transforms all USS's to feature vectors of equal length (Ì ¾ -dimensional), to be able to use them as input to the same statistical pattern classifier. Since gait dynamics are at root not invariant to speed of walking, there is no (direct) way to normalize for this variation qualitatively.</p><p>Similarly, the USS's corresponding to different camera viewpoints are qualitatively different, since a different (planar) projection of gait dynamics is captured in the image plane from any one camera viewpoint. We currently also have no way of normalizing for this variation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Gait Classifier</head><p>As mentioned in the previous section, the similarity plot is a projection of the dynamics of the walking person that preserves the frequency and phase of the gait. The question then arises as to whether this projection preserves more detailed (higherdimensional) aspects of gait dynamics, that capture the unique way an individual person walks.</p><p>We build a gait pattern classifier that takes USS's as input feature vectors. Our classifier is very much analogous to the 'Eigenface' approach <ref type="bibr" target="#b27">[28]</ref>, in that we treat a USS much the same way that a face image is treated in that method. Specifically, we apply principal components analysis (PCA) to reduce the dimensionality of the input feature vectors, and use a simple non-parametric pattern classification technique to classify new feature vectors in the subspace spanned by the first few principal components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Training the Classifier</head><p>Let Í½ Í ½ ÍÅ be a given training set of Å labelled (i.e. each corresponding to a known person) USS's, of size Ì xÌ each, and let Ù be the vector of length Ì ¾ corresponding to Í (obtained by concatenating all its rows). Note, however, that strictly speaking the feature vectors (i.e. the USS's) extracted from the same walking sequence are not independent.</p><p>We then compute the principal components <ref type="bibr" target="#b19">[20]</ref> of the space spanned by Ù½ ÙÅ by computing the eigenvalue decomposition (also called Karhunen-Loeve expansion) of their covariance matrix</p><formula xml:id="formula_1">Ù ½ Å È Å ½ ´Ù Ù µ´Ù Ù µ Ì</formula><p>, where Ù is the simple mean of all training vectors Ù½ ÙÅ . This can be efficiently computed in Ç´Å µ time (instead of the brute force Ç´Ì ¾ µ) <ref type="bibr" target="#b27">[28]</ref>. The subspace spanned by the most significant eigenvectors, Ú½ Ú , that account for (say) ± of the variation in the training vectors, is denoted the Eigengait.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Classification</head><p>Gait recognition now reduces to standard pattern classification in a -dimensional Eigengait space. Let Ü½ Ü ¾ Ü Ð be the feature vectors corresponding to the Ð USS's extracted from a given sequence of an unknown walking person. To classify this sequence, we project each of these vectors Ü to a -dimensional vector in Eigengait space, and determine its class, denoted , based on the k-nearest neighbor rule <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b25">26]</ref>. We then decide the class of the sequence itself as the most frequent .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We test our method on four different data sets in order to evaluate its performance across natural variability of individual walking, as well as its sensitivity when other factors are varied: camera viewpoint, walking cadence, image similarity measure, and the KNN parameter . The classifier is trained and tested separately for each combination of the above factors. The person templates are invariably scaled to a height of 50 pixels before computing the SP, and the USS's are each normalized to a size 32x32, hence spanning a 1024-dimensional feature space. We use the leave-one-out cross-validation to estimate the classification error rate <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b28">29]</ref>.</p><p>Since the USS's are not normalized for variation caused by different camera viewpoint and walking cadence, classification is indexed by cadence and camera viewpoint. That is, a different gait classifier is built for each camera viewpoints (for the Keck multiview dataset) and range of cadences (for the CMU MoBo dataset).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Fronto-parallel Datasets</head><p>The method is first tested on two different sets of frontoparallel sequences. The first dataset is the same used by Little and Boyd in <ref type="bibr" target="#b20">[21]</ref>, and consists of 42 image sequences with six different subjects (4 males and two females) and 7 sequences of each, taken from a static camera. The second datatset contains 108 fronto-parallel sequences taken in an outdoor environment on 2 different days and with 44 different subjects (10 females and 34 males), hence 2 sequences per subject per day. The sequences were captured at 20 fps and a full color resolution of 644x484. Each subject walked a fixed straight path back and forth at their natural pace, as shown in Figure <ref type="figure" target="#fig_4">5</ref>.   <ref type="figure">½ ¿</ref> .</p><p>Table <ref type="table" target="#tab_0">1</ref> gives the recognition rates when using the different image similarity measures and values of the KNN parameter . Note that correlation of binary silhouettes (denoted BC) gave slightly better results than FC for the first dataset, and almost the reverse is true for the second dataset. Also, the performance slightly degrades for higher values of , which maybe because the training points of any one person form multiple clusters in Eigengait space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Keck Lab Multiview Dataset</head><p>Here we test the method on a database consisting of 7 people (3 females and 4 males) walking on a treadmill, taken on 7 different days and captured simultaneously from 8 different cameras. An average of 56 sequences is provided for each subject. The multiple viewpoints correspond to different pan angles of the camera that are at 15 degree intervals and span a range of about 120 degrees of the camera field of regard. Figure <ref type="figure" target="#fig_5">6</ref> illustrates the eight camera viewpoints used in this experiment. The data sequences were captured in the Keck multi-perspective lab at a frame of 60 fps and using greyscale 644x488 images <ref type="bibr" target="#b3">[4]</ref>.</p><p>The treadmill speed was set to match the natural walking pace for each subject, which typically varied between 2.5 and 3.5 miles per hour. The results are shown in Figure <ref type="figure" target="#fig_6">7</ref>. Note that performance is significantly better at nearly fronto-parallel views (95 and 115</p><p>) and that the best performance of 78% is obtained at a 95 with ½ and binary correlation.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">CMU MoBo Dataset</head><p>To investigate the effect of variation in walking speed, we used an existing dataset <ref type="bibr" target="#b13">[14]</ref> of 50 sequences and 25 people walking on a treadmill at a slow (2.06 miles/hr) and moderate (2.82 miles/hr) pace. A recognition rate of 12% was obtained when training on slow-speed sequences and testing on moderate-speed sequences, or vice versa. However, when we both train and test on slow sequences, we obtain a recognition rate of 72%, and 76% for fast sequences. This seems to confirm the expectation that our gait recognition method is sensitive to large changes in walking-speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and future work</head><p>In this paper, we have used a correspondence-free motionbased method to recognize the gaits of small populations of people. The method is view dependent, and performs best when fronto-parallel images are used. Clothing, lighting, and other variations may degrade the performance of the classifier When tested with fronto-parallel sequences, the method achieved a recognition rate of 93% on a small dataset of 6 people and 7 sequences each taken on the same day, and 77% on a dataset of 44 subjects and 4 sequences each taken on two different days. The method was also tested on multi-view sequences of 7 people captured from 8 different viewpoints and taken on different days. The best recognition result (78%) was achieved using correlation on binary silhouettes from a near-fronto-parallel viewpoint.</p><p>The classification rate has significantly improved compared to the previous version of this method <ref type="bibr" target="#b1">[2]</ref>, in which we used a (slightly) different feature vector, that consisted of the similarity plot computed over some fixed number of gait periods (typically 3) and starting at some fixed phase. This method achieved a classification rate of at most 28% for the fronto-parallel outdoor datatset (compared to 77% with the new method), and at most 65% for the Keck dataset (compared to 78% with the new method).</p><p>We plan to study the sensitivity of this method to changes in camera viewpoint and walking cadence, as well as investigate ways to compensate for this dependence via interpolation techniques in Eigengait space. We are also working to combine Eigengait features obtained from this method with other parametric gait features that can be robustly computed from video, such as cadence, stride length and stature.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Overview of Method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. From left-to-right: original image, foreground template and binary template, of a walking person.</figDesc><graphic coords="3,97.32,425.23,141.70,71.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Combinations of key poses of human gait correspond with local minima of the self-similarity plot.</figDesc><graphic coords="3,312.36,261.28,113.30,170.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Extracting units of self-similarity from the similarity plot. Blue and green USS's start at pose and , respectively.</figDesc><graphic coords="4,69.00,108.16,198.40,198.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. An example of 4 outdoor walking sequences of one person. The top and bottom two sequences were each taken on two different days.</figDesc><graphic coords="5,83.16,378.50,170.00,127.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Eight camera viewpoints of the sequences in second test data set.</figDesc><graphic coords="5,349.08,383.33,155.90,112.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Classification rates for Dataset 3 for the 8 viewpoints with</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Classification rates for the two fronto-parallel</figDesc><table><row><cell></cell><cell cols="2">Little and Boyd</cell><cell cols="2">UMD2 Dataset</cell></row><row><cell cols="3">ÃÒ BC Rate FC Rate</cell><cell cols="2">BC Rate FC Rate</cell></row><row><cell>1</cell><cell>.93</cell><cell>.90</cell><cell>.75</cell><cell>.77</cell></row><row><cell>3</cell><cell>.90</cell><cell>.90</cell><cell>.72</cell><cell>.72</cell></row><row><cell>5</cell><cell>.93</cell><cell>.87</cell><cell>.73</cell><cell>.70</cell></row><row><cell cols="5">datasets with two different measures of image similarity,</cell></row><row><cell cols="5">Foreground (FC) and Binary (BC), and three different val-</cell></row><row><cell cols="3">ues of for the KNN classifier,</cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>We would like to thank Dr. Jeffrey E. Boyd of the Department of Computer Science at the University of Calgary, Canada, for providing their gait dataset. The support of DARPA (Human ID project, grant No. 5-28944) is also gratefully acknowledged.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Temporal and spatial factors in gait perception that influence gender recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Barclay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cutting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kozlowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception and Psychophysics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="145" to="152" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Gait as a biometric for person identification in video sequences</title>
		<author>
			<persName><forename type="first">C</forename><surname>Benabdelkader</surname></persName>
		</author>
		<idno>4289</idno>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
		<respStmt>
			<orgName>University of Maryland College Park</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Neural Networks for Pattern Recognition. Oxford</title>
		<imprint>
			<publisher>Clarendon Press</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Multiperspective analysis of human actions</title>
		<author>
			<persName><forename type="first">E</forename><surname>Borovikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cutler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Horprasert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Human motion analysis: a review</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Computer Society Workshop on Motion of Non-Rigid and Articulated Objects</title>
		<meeting>of IEEE Computer Society Workshop on Motion of Non-Rigid and Articulated Objects</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Recognition of human body motion using phase space constraints</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">W</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bobick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A survey of motion analysis from moving light displays</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cedras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Robust real-time periodic motion detection, analysis and applications</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cutler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Recognizing friends by their walk: Gait perception without familiarity cues</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cutting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kozlowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bulletin Psychonomic Soc</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="353" to="356" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The representation and recognition of action using temporal templates</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Bobick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stork</surname></persName>
		</author>
		<title level="m">Pattern Classification</title>
		<imprint>
			<publisher>John Wiley and Sons</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Non-parametric model for background subtraction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Elgammal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Harwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The visual analysis of human movement: a survey</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gavrila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The cmu motion of body (mobo) database</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
		<respStmt>
			<orgName>Robotics Institute, Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Individual recognition from periodic activity using hidden markov models</title>
		<author>
			<persName><forename type="first">Q</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Debrunner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Human Motion</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The interpretation of biological motion</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Flinchbaugh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Cybernetics</title>
		<imprint>
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Comparing different template features for recognizing people by their gait</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Nixon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">V</forename><surname>Inman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Ralston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Todd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1981">1981</date>
			<publisher>Human Walking. Williams and Wilkins</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Visual motion perception. Scientific American</title>
		<author>
			<persName><forename type="first">G</forename><surname>Johansson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">T</forename><surname>Joliffe</surname></persName>
		</author>
		<title level="m">Principal Component Analysis</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recognizing people by their gait: the shape of motion</title>
		<author>
			<persName><forename type="first">J</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Boyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Videre</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Finding periodicity in space and time</title>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Moving object recognition in eigenspace representation: gait analysis and lip reading</title>
		<author>
			<persName><forename type="first">H</forename><surname>Murase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sakai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PRL</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Analyzing and recognizing walking figures in XYT</title>
		<author>
			<persName><forename type="first">S</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Detection and recognition of periodic, non-rigid motion</title>
		<author>
			<persName><forename type="first">R</forename><surname>Polana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Pattern Recognition and Neural Networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ripley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">An efficient and accurate camera calibration technique for 3d machine vision</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tsai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Face recognition using eigenfaces</title>
		<author>
			<persName><forename type="first">M</forename><surname>Turk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Computer Systems that Learn</title>
		<author>
			<persName><forename type="first">S</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kulikowski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>Morgan Kaufman</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
