<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NOT ALL TASKS ARE BORN EQUAL: UNDERSTANDING ZERO-SHOT GENERALIZATION</title>
				<funder ref="#_JHZhS6u">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jing</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Interdisciplinary Information Sciences (IIIS)</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zongyu</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Interdisciplinary Information Sciences (IIIS)</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yanan</forename><surname>Zheng</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jian</forename><surname>Li</surname></persName>
							<email>lijian83@mail.tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Interdisciplinary Information Sciences (IIIS)</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
							<email>zhiliny@mail.tsinghua.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">Shanghai Artificial Intelligence Laboratory</orgName>
								<address>
									<addrLine>4 Shanghai Qi Zhi Institute</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">NOT ALL TASKS ARE BORN EQUAL: UNDERSTANDING ZERO-SHOT GENERALIZATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Post-hoc Cosmos QA</term>
					<term>Social IQA</term>
					<term>PAWS</term>
					<term>QuAIL</term>
					<term>Wiki QA</term>
					<term>QuaRTz</term>
					<term>QASC</term>
					<term>ROPES Prior-detect Cosmos QA</term>
					<term>Adv./DBiDAF</term>
					<term>Adv./DRoBERTa</term>
					<term>QuaRTz</term>
					<term>Social IQA</term>
					<term>Hotpot QA</term>
					<term>Adv./DBERT</term>
					<term>ROPES</term>
					<term>QuAIL</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent work has achieved remarkable zero-shot performance with multi-task prompted pretraining, but little has been understood. For the first time, we show that training on a small number of key tasks beats using all the training tasks, while removing these key tasks substantially hurts performance. We also find that these key tasks are mostly question answering (QA) tasks. These novel findings combined deepen our understanding about zero-shot generalization-training on certain tasks such as QA encodes general knowledge transferable to a wide range of tasks. In addition, to automate this procedure, we devise a method that (1) identifies key training tasks without observing the test tasks by examining the pairwise generalization results and (2) resamples training tasks for better data distribution. Empirically, our approach achieves improved results across various model scales and tasks. 1 * Corresponding Authors. 1 Our code is released at https://github.com/zhouj8553/Improving-T0.</p><p>1 Published as a conference paper at ICLR 2023 any test tasks, and then performs resampling by upsampling key tasks or downsampling non-critical tasks, as shown in Figure <ref type="figure">3</ref>. In this way, we build a better mixture of multi-task training sets to highlight those key tasks with better general transfer ability. Experiments show that task resampling consistently outperforms the previous approach T0 <ref type="bibr" target="#b31">(Sanh et al., 2022)</ref> across three different model scales and on test tasks of various types.</p><p>To sum up, our contributions are as follows.</p><p>1. We conduct experiments to understand and reveal how multi-task training for zero-shot generalization works-(1) Only a small number of training tasks dominate zero-shot generalization; (2) Some key tasks provide general transfer ability and can be detected using pairwise generalization results.</p><p>2. We devise a novel method, task resampling, to improve zero-shot generalization by (1) first automatically identifying key training tasks based on pairwise training and evaluation without observing any test tasks, and (2) resampling training tasks using upsampling and downsampling strategies.</p><p>3. Experiments show that our approach achieves new state-of-the-art results across various model scales and tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>2.1 ZERO-SHOT LEARNING IN NLP Zero-Shot Learning denotes the setting when no data correlated with the test set is available during the training stage. The early definition of zero-shot learning referred to predicting samples with unseen classes, so traditional methods require prior information such as semantic knowledge (Zhang et al., 2019a) or knowledge graph (Chen et al., 2021) for an unseen class so that model can predict that class without training data. Meta-learning (Zhang et al., 2022) and reinforcement learning (Ye et al., 2020) methods are also used for zero-shot learning. Recently, more work focuses on the setting of predicting samples with unseen tasks, supported by the development and prevalence of pre-trained language models (PLMs), as well as multi-task training. McCann et al. (2018) unifies NLP tasks into QA-format to perform multi-task learning. Liu et al. (2019) designs a multi-task deep neural network for natural language understanding tasks. Aghajanyan et al. (2021) designs an intermediate training stage between pretraining and finetuning using around 50 tasks. Most recently, with the combination of the above two approaches, T0 and FLAN (Sanh et al., 2022;<ref type="bibr" target="#b39">Wei et al., 2022)</ref> have shown that explicit multi-task prompted training where all tasks are unified by the natural language prompts can vastly promote zero-shot task generalization. We build upon previous work within this new paradigm and devote ourselves to improving the recipe of multi-task prompted training by revealing the mechanism of generalization in Section 3 and further enhancing its performance in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">THE INTERPRETATION OF PROMPTED LEARNING</head><p>Recent work has shown an increased interest in how the prompts help the model generalize to unseen tasks. Some researchers <ref type="bibr" target="#b39">(Wei et al., 2022;</ref><ref type="bibr" target="#b33">Schick &amp; Sch?tze, 2021;</ref><ref type="bibr" target="#b21">Mishra et al., 2022)</ref> suggest that the model learns to understand what they are doing through prompts. While some work <ref type="bibr" target="#b38">(Webson &amp; Pavlick, 2022;</ref><ref type="bibr" target="#b19">Logan IV et al., 2022)</ref> challenges this assumption, revealing that sometimes we could get comparable performance without prompts or even with wrong prompts.</p><p>T0 <ref type="bibr" target="#b31">(Sanh et al., 2022)</ref> claims that they only empirically witness the transfer phenomenon, but it is unclear why it happens. We provide new insights into the reason for generalization. We challenge the idea that the model learns the task through instructions, based on the observation that deleting a small but important set of tasks will lead to transfer failure. We suggest that most generalization ability comes from key tasks, which could be divided into specific and general transfer abilities. We hope our discovery could promote the development of this field.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Recent work <ref type="bibr">(Brown et al., 2020;</ref><ref type="bibr" target="#b2">Artetxe et al., 2022;</ref><ref type="bibr" target="#b26">Rae et al., 2021)</ref> has demonstrated the potential of leveraging pretrained language models (PLMs) to perform zero-shot generalization. Zero-shot generalization enables PLMs to adapt to a variety of natural language processing (NLP) tasks without relying on any annotated data, which opens the possibility towards generic systems.</p><p>Pretrained models, such as <ref type="bibr">GPT-3 (Brown et al., 2020)</ref>, BERT <ref type="bibr" target="#b8">(Devlin et al., 2019)</ref> and T5 <ref type="bibr" target="#b27">(Raffel et al., 2020)</ref>, can perform zero-shot inference on unseen test tasks by leveraging natural language prompts and formulating NLP tasks into language modeling tasks. More recent advances <ref type="bibr" target="#b39">(Wei et al., 2022;</ref><ref type="bibr" target="#b31">Sanh et al., 2022)</ref> performed multi-task prompted training on PLMs and further enhanced the zero-shot performance to a large extent. Despite the substantial progress, few works have studied how multi-task prompted training boosts the zero-shot performance. The lack of understanding hinders further improvement of the field.</p><p>To this end, we take a further step to understand multi-task training for zero-shot generalization (1) by selecting only a small number of key training tasks and performing multi-task training and (2) by studying the characteristics of tasks with general transfer ability. The results reveal several interesting findings -First of all, only a small number of training tasks dominate the performance of zero-shot generalization. In other words, using only these key training tasks to perform multi-task training leads to good results, while removing these key tasks would drastically hurt the zero-shot performance. Secondly, not all tasks are born equal, and some tasks show general transfer ability by providing widely useful knowledge. Moreover, key tasks with general transfer ability can be automatically detected using pairwise generalization results.</p><p>In addition, based on the findings, we propose an improved method, task resampling, which improves multi-task training for zero-shot generalization to a large extent. Task resampling first automatically identifies a set of key training tasks based on pairwise training and evaluation without peeking forward</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">TRANSFER RELATIONSHIPS IN MULTI-TASK LEARNING</head><p>Based on the observation that transfer ability comes from key tasks, we would like to further improve the zero-shot performance by exploring the transfer relationships in multi-task learning. Some previous works learn transfer relationships in supervised multi-task training. Taskonomy <ref type="bibr" target="#b43">(Zamir et al., 2018)</ref> builds a transfer structure of a series of computer vision tasks by training task-specific encoders on each dataset and retraining the decoders on target datasets. <ref type="bibr" target="#b9">Dwivedi &amp; Roig (2019)</ref> and <ref type="bibr" target="#b34">Song et al. (2019)</ref> obtain the transfer relationship based on the assumption that transferable task-specific models have similar representations or embeddings on the same data. <ref type="bibr" target="#b36">Vu et al. (2020)</ref> learns the embedding of each NLP task and tries to predict the transfer relationship between different datasets. ExT5 <ref type="bibr" target="#b1">(Aribandi et al., 2022)</ref> learns the transferability by co-training the source task and target task and evaluate on the target task. UnifiedQA <ref type="bibr" target="#b14">(Khashabi et al., 2020)</ref> explores the transferability between different QA tasks. The major challenge lies in that we could not get access to the test task at the training stage, thus it is hard to figure out in advance which training tasks can be more useful for unseen tasks. To address this challenge, we propose a reweighting method based on the transfer performance of pairwise training tasks, which will be discussed in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">DATA AUGMENTATION IN NLP</head><p>Data Augmentation is widely used in NLP to strengthen the robustness and diversity of the data distribution and promote the model performance. However, most of the approaches are conducted in word-level <ref type="bibr" target="#b47">(Zhang et al., 2015;</ref><ref type="bibr" target="#b37">Wang &amp; Yang, 2015;</ref><ref type="bibr" target="#b40">Wei &amp; Zou, 2019</ref>) and sentence-level <ref type="bibr" target="#b12">(Kafle et al., 2017;</ref><ref type="bibr" target="#b10">Hou et al., 2018;</ref><ref type="bibr" target="#b13">Khashabi et al., 2018;</ref><ref type="bibr" target="#b46">Zhang et al., 2018)</ref>, or in other words, instancelevel. In our paper, to naturally cater for the multi-task setting, we adopt a new perspective and design a cross-domain data augmentation which intersects each domain data with different kinds of tasks and significantly improves the diversity of data distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">UNDERSTANDING ZERO-SHOT TASK GENERALIZATION</head><p>This section explores how multi-task training contributes to zero-shot generalization. By revealing the mechanism of task transfer, we provide some insights for improving zero-shot performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">DATA</head><p>We followed the setting of T0 <ref type="bibr" target="#b31">(Sanh et al., 2022</ref>) and adopted the tasks therein. There are 38 training tasks across 8 task types, and 11 test tasks ranging from natural language inference (RTE <ref type="bibr" target="#b4">(Candela et al., 2006)</ref>, CB <ref type="bibr" target="#b7">(De Marneffe et al., 2019)</ref>, ANLI/R1-R3 <ref type="bibr" target="#b23">(Nie et al., 2020)</ref>), coreference resolution (WSC <ref type="bibr" target="#b16">(Levesque et al., 2012)</ref>, Winogrande <ref type="bibr" target="#b30">(Sakaguchi et al., 2020)</ref>), sentence completion (COPA <ref type="bibr" target="#b28">(Roemmele et al., 2011)</ref>, StoryCloze <ref type="bibr" target="#b22">(Mostafazadeh et al., 2017)</ref>, Hellaswag <ref type="bibr" target="#b44">(Zellers et al., 2019)</ref>), to word disambiguation <ref type="bibr">(WiC (Pilehvar &amp; Camacho-Collados, 2019)</ref>). Both training and test sets are disjoint in task types, thus guaranteeing the zero-shot setting. We report the mean and median accuracy over multiple prompts for each test task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">A SMALL NUMBER OF KEY TASKS DOMINATE PERFORMANCE</head><p>Since there has been little agreement on where the excellent zero-shot generalization performance comes from, we would like to conduct experiments to explore its mechanism. Some researchers believe that the model understands the prompts through multi-task training <ref type="bibr" target="#b39">(Wei et al., 2022;</ref><ref type="bibr" target="#b33">Schick &amp; Sch?tze, 2021;</ref><ref type="bibr" target="#b21">Mishra et al., 2022)</ref>, while we take an orthogonal perspective and hypothesize that there might be a few key tasks that are crucial for the zero-shot generalization performance. For a straightforward verification of the above hypothesis, we conduct two experiments.</p><p>Single Task Shows Zero-Shot Transfer Ability We set up an experiment to study the pairwise transfer results between all pairs of tasks. Specifically, for any pair of tasks in the T0 collection, we train on one task and evaluate on the other. We expect to decouple the effects of multi-task learning and observe the transfer ability of single tasks. Results are shown in Figure <ref type="figure" target="#fig_0">1</ref>. On the held-out test tasks, the performance gap between multi-task training and single-task training is less than 5 points on average. We also conduct experiments by training on top-3 tasks for each test dataset. The detailed results are presented in Appendix D.1.  <ref type="table" target="#tab_10">8 82 61 52 45 31 17 54 58 43 32 38 74 35 53 62 47 5 9 54 38 80 77 30 5 3 6 11 3 4 6 43 12 37 49 47 33 34 33 56 26 53 52 56 50</ref> Extractive QA <ref type="table" target="#tab_10">32 62 54 9 78 56 51 45 30 16 57 60 38 32 43 71 37 54 67 47 5 9 56 38 70 68 30 5 4 7 12 2 4 6 49 23 30 49 38 33 33 33 61 26 55 52 55 50  Adv./DRoberTa 32 62 54 8 78 57 49 43 28 16 58 57 42 32 43 72 36 54 68 47 5 7 51 39 77 74 32 5 2 6 12 2 4 6 49 31 42 48 40 33 33 33 62 27 56 53 55</ref>   The entry at row i and column j denotes the average performance when the model is trained on task i and evaluated on task j. For each entry, the value is the average score of different prompts. (Accuracy if only Accuracy is calculated, and otherwise the mean of Accuracy and F1.) Only those prompts related to the original tasks are included for evaluation. We highlight those entries with high scores for each task (Red is the Top-1). The horizontal and vertical lines denote the boundary of task-type groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adv./DBERT</head><p>A Small Set of Tasks Dominates Performance on the Test Tasks We manually select the top-8 tasks out of all training tasks of T0. These selected tasks empirically demonstrate good generalization to the test tasks according to preliminary experiments. Specifically, we first select the top-3 key tasks for each test task. Then, we select the tasks which appear at least twice in the top-3 support tasks. As a result, the number of selected key tasks is exactly 8. The top-8 selected tasks are CosmosQA <ref type="bibr" target="#b11">(Huang et al., 2019)</ref>, Social IQA <ref type="bibr" target="#b32">(Sap et al., 2019)</ref>, PAWS <ref type="bibr">(Zhang et al., 2019b)</ref>, QuAIL <ref type="bibr" target="#b29">(Rogers et al., 2020)</ref>, Wiki QA <ref type="bibr">(Yang et al., 2015)</ref>, QuaRTz <ref type="bibr" target="#b35">(Tafjord et al., 2019)</ref>, QASC <ref type="bibr" target="#b15">(Khot et al., 2020)</ref>, and ROPES <ref type="bibr" target="#b17">(Lin et al., 2019)</ref>.</p><p>For comparison, we experimented with three different variants by training a T5-Large model using only top-8 tasks ("Top-8 Only") and all but the top-8 tasks ("T0 Tasks w/o Top-8"), respectively. We also experimented using backbones with different scales (XL) or architectures (decoder-only model), and results are presented in Appendix C.</p><p>Results on T5-Large are shown in Table <ref type="table" target="#tab_2">1</ref>. We observe that training with only the top-8 tasks outperforms training with all tasks when tested on 11 test tasks, while training with all but the top-8 tasks drastically decreases performance. It proves that a few key tasks contribute to zero-shot task generalization. Training with key tasks selected by post-hoc results achieves much better zero-shot performance than training with all tasks. It is generally agreed that training with more tasks should lead to better learning of prompts, but our experiments show that this is not the key reason for the performance improvement. The information contained in the key tasks plays a significant role in improving the few-shot generalization. This result raises a further question whether the model learns to read, understand and react to instructions broadly from all tasks, or just benefits from several key tasks which have strong general transferability.   Only" means using only the top-8 tasks. "T0 Tasks w/o Top-8" means using the T0 tasks with top-8 tasks removed. Results that are comparable to or outperform the T0 baseline are denoted in bold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">GENERAL TRANSFER AND SPECIFIC TRANSFER</head><p>First of all, we divide the transfer ability into specific transfer ability and general transfer ability according to the scope of target tasks.</p><p>Specific transfer ability means that the task can only provide special knowledge for a small set of tasks with certain kinds of patterns. A typical example is the sentiment analysis task, which helps the sentiment analysis tasks of different domains a lot, but has little effect on other complex NLU tasks.</p><p>The specific transfer ability is relatively stronger among the tasks at the diagonal blocks in Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>General transfer ability means that the task can provide knowledge that is required by most downstream tasks. The more it provides beyond the knowledge captured by the pretrained model, the better it will contribute to the overall transfer ability of the model. For example, adding question answering (QA) tasks will improve the performance on most of the downstream tasks, which could be because they provide valuable commonsense knowledge and reasoning skills.</p><p>After introducing the division of transfer ability, we raise two questions based on the experimental results in Section 3.2. (1) Do the key tasks embrace the ability of general transfer or specific transfer?</p><p>(2) Can we reveal the common patterns shared by the tasks with general transfer ability?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">NOT ALL TASKS ARE BORN EQUAL</head><p>Some QA Tasks Show General Transfer Ability For the first question, from Figure <ref type="figure" target="#fig_0">1</ref>, most of the tasks selected in post-hoc experiments bring improvements on a wide range of tasks, and thus have a certain degree of general transfer ability. In addition, most of the tasks that show general transfer ability are QA tasks.</p><p>Two notable concepts are QA format and QA tasks. QA tasks indeed take the QA format. However, QA-formatted data are not necessarily QA tasks (e.g., prompted sentiment analysis data taking the QA format is not a QA task.). Here, QA tasks refer to those tasks that require reasoning skills, such as reading comprehension. We conduct an experiment by formatting the sentiment analysis task into a multiple-choice QA task. Specifically, we convert the format of a sentiment analysis task: Yelp Review Full into the multiple choice QA format (see Figure <ref type="figure" target="#fig_2">2</ref>). We compare the zeroshot performance on 11 unseen tasks between training on raw Yelp Review Full and training on QA-formatted Yelp Review Full, which is displayed in Table <ref type="table" target="#tab_3">2</ref>. Results show that simply using QA-formatted non-QA-tasks does not benefit zero-shot performance, proving that it is not simply the QA format that results in the zero-shot ability.   Why QA Tasks Show General Transfer Ability? Experiments show that some QA tasks demonstrate general transfer ability, and we would like to explore the underlying reasons. We suspect some QA tasks provide some knowledge that is not captured in the pretraining process.</p><p>From the examples in Table <ref type="table" target="#tab_5">3</ref>, we can see that both CosmosQA and Social IQA require some simple reasoning ability in the general domain, which is required for a wide range of NLP tasks. More importantly, it is difficult to learn this knowledge in the pretraining stage, so an additional supplement is necessary to make the model have good cognitive ability. As a result, those tasks show better general transfer ability. There may be some quantitative methods to evaluate the knowledge provided by these tasks. A possible solution is to design some probe tasks, as is done in <ref type="bibr" target="#b25">Pruksachatkun et al. (2020)</ref>, and we leave this for future work.  Which Kinds of QA Tasks Work? Another important observation is that not all QA tasks show general transfer ability. Specifically, CosmosQA, Social IQA, and QuAIL show outstanding transfer ability, while some tasks such as WikiHop, and WiQA do not. Through a careful examination of the datasets (see Table <ref type="table" target="#tab_5">3</ref> for reference), we conjecture that there are two reasons. First of all, the domain type matters a lot. Taking an extreme case as an example, training on datasets full of math problems is not likely to provide general transfer ability to other tasks. All of CosmosQA, Social IQA, and QuAIL require commonsense knowledge that is useful in the general domain. However, the WikiHop dataset urges the model to remember specific knowledge that is mainly required for knowledge contests. Another possible factor is the text format. In detail, the expressions of WikiHop/WiQA seem much more artificially-constructed than Social IQA/CosmosQA, thus showcase limited transfer ability.</p><p>So far, we have analyzed phenomena about the zero-shot task generalization ability. At the same time, three problems remain unsolved: (1) The test set cannot be seen in advance.</p><p>(2) We need to further distinguish which QA tasks are useful when a large number of QA tasks are provided. (3) When the training set provided is changed, we need to distinguish new tasks with general transfer ability. For the above three reasons, we propose a general data-driven approach to identify tasks with general transfer capability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">AN IMPROVED METHOD: TASK RESAMPLING</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">METHOD</head><p>We have arrived at several findings in Section 3, revealing that (1) only a small number of training tasks dominate the performance of zero-shot generalization and (2) some key tasks with general transfer ability can be detected through pairwise generalization. Following the findings, we hypothesize that one of the key aspects of improving zero-shot performance is to appropriately adjust the weight of different training tasks, such that the model is trained with an optimized mixture of multi-task datasets. To this end, we devise a novel method, task resampling, that first automatically identifies a set of key training tasks based on pairwise training and evaluation without observing any test tasks, and then adjusts the training data distribution through upsampling or downsampling. Figure <ref type="figure" target="#fig_3">3</ref> shows an overview of our method.</p><p>Formally, we are given a set of training tasks T = {t i } i where t i is a task, and a pretrained model M. Each task is formulated as t i = {x i,j , y i,j } j , consisting of a prompted input x i,j and a prompted target y i,j . Our goal is to assign appropriate weights {w i } i for each training task, and use the optimized mixture of training tasks to train the model M in a multi-task manner such that it performs well on unseen test tasks.</p><p>Generally, our method consists of three major steps.</p><p>1. Pre-detection of key tasks. Identify key training tasks based on pairwise training and evaluation without relying on any test tasks. This can be viewed as a prior approximation to the post-hoc method in Section 3.2.</p><p>2. Task resampling. Resample different training tasks by upsampling key tasks or downsampling non-key tasks.</p><p>3. Multi-task training. Train a model using the resampled mixture of multi-task datasets.</p><p>Pre-detection of Key Tasks Without observing the test tasks, the main idea of our approach is to use pairwise training and evaluation within the training tasks. To identify the key tasks, we first train a model on each training task and evaluate it on all the training tasks. This results in an N ? N (N is the number of training tasks) table, which is part of the results in Figure <ref type="figure" target="#fig_0">1</ref>. Then we design a method to select key training tasks based on this N ? N table. For each task pair A and B, let f (A, B) be the performance of training on A and evaluating on B. We let g(A, B) = 1 if the following conditions are satisfied (and otherwise g(A, B) = 0):</p><p>1. A and B are of different task types.</p><p>2. The performance f (A, B) is high enough:</p><formula xml:id="formula_0">f (A, B) ? max A ? ? =B f (A ? , B) -TH 1 and f (A, B) ? mean A ? ? =B f (A ? , B) + TH 2 .</formula><p>Here TH 1 and TH 2 are two constants which control the tolerant distance between f(A, B) and maximum performance and average performance respectively. The value g(A, B) indicates whether A is a high-performing training task for B. We constrain that A and B are of different types because we eventually target cross task generalization. We then aggregate g(A) = B g(A, B) to represent how many times A is a high-performing training task for another task, and use the tasks with the largest g(A) values as the key tasks. In our implementation, we apply thresholding on g(A) to obtain the set of key tasks.</p><p>Task Resampling by Upsampling or Downsampling After detecting the key tasks using only the training tasks, we propose a simple yet effective resampling approach to optimize the mixture of multi-task data. We either perform upsampling or downsampling strategy. For upsampling, we upsample the key tasks by N u times. For downsampling, we cap the number of samples to be N d for each non-key task and use the original sample size for the key tasks. In our preliminary experiments, we found task resampling more robust than using the key tasks only because it takes a softer approach to highlight the importance of key tasks while maintaining knowledge from other tasks.</p><p>Data Augmentation In the above sections, we have discussed an important discovery that certain tasks are crucial for zero-shot performance. However, some of these key tasks might be limited in terms of labeled data. Thus, we further propose a data augmentation method to create as many samples as possible for each task. Specifically, given tasks A and B, we apply the prompts of A to the data of B to obtain additional augmented data. In other words, we have more data from task B that are used to perform the task A. We use a trained T0 to predict the labels of the augmented samples, which is similar to self-training. Note that data augmentation is optional and independent of task resampling. We do ablation studies to investigate the effectiveness of this component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-task Training</head><p>Our training procedure is the same as T0. The only difference is that we employ an optimized mixture of datasets (and optionally with data augmentation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">EXPERIMENTAL SETUP</head><p>Following the same training and evaluation setting as T0 <ref type="bibr" target="#b31">(Sanh et al., 2022)</ref>, we finetune the T5-LM-Adapt model on 38 training tasks, which has been discussed in detail in Section 3.1. For data preprocessing, following T0, to balance the number of data for different tasks, we restrict the maximum number of data examples for each training task to 500,000.</p><p>Based on our resampling strategy, we set the pre-detection parameters as TH 1 = 5, TH 2 = 10, and then choose the datasets which are counted as the key tasks at least twice (i.e., all tasks A with g(A) ? 2). Given each key task D with data size |D|, we duplicate D by 5 times (N u = 5) for the upsampling strategy and empirically start from 50,000 samples for each dataset. For the downsampling strategy, we downsample each non-key task to N d = min(50, 000, |D|) samples. We provide detailed statistics about the datasets in Appendix B.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">RESULTS</head><p>Post-hoc v.s. Prior Detection of Key Tasks One vital part of our approach is the detection of key tasks, so we list the key tasks selected by post-hoc (i.e., observing the test tasks) and pre-detection methods in Table <ref type="table">4</ref>. There are five common tasks shared by the two methods, indicating that our approach can detect most of the key tasks. Moreover, even though the two sets of key tasks are not exactly matched, our experiments demonstrate that this does not affect performance.</p><p>and T0 with downsampling non-key tasks (DS-T0). Besides, we display the performance of the task-resampled T0 with augmented data, dubbed as US+DA-T0 and DS+DA-T0, respectively. We summarize the following key observations from Table <ref type="table" target="#tab_6">5</ref>: Zero-shot performance for our improved T0 and original T0 at three different scales. Results with ? are reported by Sanh et al., and results with ? are reproduced in our experiments. US-T0 means T0 with upsampling key tasks, DS-T0 means T0 with downsampling non-key tasks, and DS+DA-T0 / US+DA-T0 represents DS-T0 / US-T0 with augmented data. "Our Best" is achieved with the US+DA-T0 setup.</p><p>1. Advantage of Task Resampling. Our task-resampled T0 with both upsampling and downsampling strategies (US-T0 and DS-T0) boosts the performance of reproduced T0. Specifically, DS-T0 outperforms T0 (*) by 3.2% at Large scale and 2.0% at XL scale, and US-T0 outperforms T0 (*) by 3.9% at Large scale and 0.6% at XL scale.</p><p>2. Advantage of Data Augmentation. Task-resampled T0 achieves better performance with augmented data. Specifically, downsampling T0 (DS-T0) increases by 1.3% with augmented data at Large scale and upsampling T0 (US-T0) increases by 1.4% with augmented data at XL scale. And T0 with both task resampling and augmented data achieves 0.8% gain with reproduced T0. It indicates that data augmentation can further strengthen the mixture of multi-task data.</p><p>3. Advantage of Our Implementation Framework. Our reproduced T0 result is better than the reported T0 <ref type="bibr" target="#b31">(Sanh et al., 2022)</ref> by 9.4% at XL scale and 3.2% at XXL scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>This work studies the principles of zero-shot generalization through pairwise experiments, and reveals that a small number of training tasks dominate performance. We further divide the transfer relationship into specific transfer and general transfer, and find that adding those tasks with general transfer ability will contribute to the performance gain for most tasks. Moreover, those tasks with general transfer ability can be identified by examining the pairwise generalization results. Based on the findings, we propose the task resampling method to improve the zero-shot performance. Extensive experiments demonstrate the effectiveness of our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A EXPERIMENTAL DETAILS</head><p>A.1 HYPER-PARAMETERS SELECTION For all the experiments, we adopt the ADAM optimizer and use a learning rate of 1e-4. Considering the amount of data, the batch size and training steps are different for different settings in our experiments. We don't search the training hyper-parameters anymore, because we find that the performance is similar as long as we train for sufficient epochs in our preliminary experiments. We use different batch sizes and training steps for different amounts of data for time-saving. The hyper-parameters of our experiments are in Table <ref type="table" target="#tab_7">6</ref>. For other hyper-parameters we selected, we predefine them using some preliminary experiments on T5-Large, and then apply them directly on larger models for the sake of time. We first select the key tasks by searching T H 1 , T H 2 using the upsampling strategy with N u = 5, the whole search space is T H 1 = {5}, T H 2 = {5, 10}. We choose G(A) ? 2 because if you draw the distribution of G(A) values, you can clearly see that the tasks with G(A) ? 1 are the long-tailed part. When the key tasks are selected, we then search the hyper-parameters for upsampling and downsampling using the search space N u = {2, 5}, N d = {5, 10}, and choose the best one on T5-Large. We find that those hyper-parameters don't affect the results a lot, i.e., the gap among them is much smaller than the gap between them and the baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment</head><p>We only report the best result on T5-XXL, because we are unable to run all the experiments due to the limitation of computing resources. Our best result is achieved using US+DA-T0.</p><p>Published as a conference paper at ICLR 2023</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 DATA AUGMENTATION DETAILS</head><p>We propose two algorithms for the domain-task intersection. The first one is based on a human-written taxonomy tree, and another relies on universal fields. We combine the two techniques and achieve a balance between quality and diversity.</p><p>Taxonomy Tree Based Domain-Task Intersection To build a general taxonomy tree to cover as many prompts as possible, we take both task format and task content into consideration to develop a series of guidelines similar to a Decision Tree. Then, at the end of each branch, we intersect the source data lying in that branch with the related prompts belonging to that branch. In this way, we might produce a reasonable combination of the source data and prompt. For example, classification tasks like IMDB can also do tasks like title generation.</p><p>Universal Domain-Task Generation To further improve the diversity of the augmented data, we get rid of the man-made restriction and propose universal domain-task generation. In detail, we define the unified fields for data from all domains, which are utilized for various tasks. Obviously, each original domain data lacks certain kinds of fields, e.g., AG NEWS data only have two fields: category label and text. Therefore, we leverage the T0 to predict the missing fields in order to conduct different kinds of tasks using the prompts having already been trained in T0. For some tasks, we also train a specific model for prediction to get better performance. After that, we filter samples according to the confidence score (i.e., the probability output by the model).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B MORE EXPLORATIONS ON THE TRANSFER ABILITY B.1 STATISTICS OF THE CURRECT DATASETS</head><p>We report the size of the dataset in our experiments in Table <ref type="table" target="#tab_8">7</ref>. To explore more statistical features of the original dataset, we further provide some statistical indicators. Specifically, we calculate the average sequence length (SLEN) and the mean segmental TTR (MSTTR) of the prompted input of training datasets. MSTTR is calculated with a window size of 50. We calculate those statistical values using at most 10000 examples for each prompted task for the sake of time. Results are in Table <ref type="table" target="#tab_8">7</ref>.</p><p>An interesting phenomenon is that datasets with fewer training examples are more likely to show general transfer ability. We speculate that it is because a lot of manual effort is needed to make up those datasets with sufficient knowledge. It is too costly to make these datasets extremely large. It seems that the statistics such as MSTTR or data length alone cannot be good indicators for transfer ability. More explorations about the influence factors of transfer ability are left for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C MORE RESULTS ON OTHER MODELS AND OTHER BASELINES C.1 RESULTS ON LARGER MODEL</head><p>To verify whether the observation that a small number of key tasks dominate zero-shot performance still holds true on larger models, we conduct the top-8 tasks experiments on T5-XL, similar to Section 3.2. Results are in Table <ref type="table" target="#tab_9">8</ref>. From the results, we can see that the model trained on top-8 only slightly outperforms the baseline, while greatly defeating the performance of the model trained on all T0 tasks without the top-8 tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 RESULTS ON DECODER-ONLY ARCHITECTURE</head><p>To verify whether the results still hold true on more architectures, we experiment on more architectures.</p><p>Considering that the encoder-only model is not suitable for language modeling tasks, here we only consider the decoder-only architecture. In specific, we conduct the top-8 experiments on GPT-Neo-1.3B <ref type="bibr" target="#b3">(Black et al., 2021)</ref>. Results are in Table <ref type="table" target="#tab_10">9</ref>. From the results, we can see that, the observation that a small number of key tasks dominate zero-shot performance mentioned in Section 3.2 still holds true. We train with a prefix-LM loss.  C.3 RESULTS ON OTHER BASELINES most tasks in T0 are defined as QA tasks, the observation that QA tasks are important might not be fair enough. Thus, we want to investigate whether the statement that "some general transfer classes dominate the zero-shot performance" still holds with a totally different mixture of datasets. Therefore, we consider conducting the experiment on the prompted datasets of FLAN <ref type="bibr" target="#b39">(Wei et al., 2022)</ref>. Noted that reading comprehension is one of the task types used in FLAN, which can be regarded as narrative QA (in this way, we classify QA tasks based on the content rather than the format), so we hope to explore what will happen if we remove all reading comprehension tasks in the mixture of datasets used in FLAN.</p><p>We use exactly the same datasets and prompts as FLAN <ref type="bibr" target="#b39">(Wei et al., 2022)</ref>, except that we include three dialogue datasets in the same way as in <ref type="bibr">FLAN-T5 (Chung et al., 2022)</ref>, and exclude the translation datasets. We leave the NLI and Commonsense Reasoning as the hold-out test set, which follows FLAN.</p><p>We conduct the experiments as follows: 1. Training with all remaining tasks in FLAN. (43 tasks in total); 2. Training with only the reading comprehension tasks in FLAN. (7 tasks in total); 3. Training without the reading comprehension tasks in FLAN. (36 tasks in total)</p><p>As can be seen in Table <ref type="table" target="#tab_11">10</ref>, the model which is trained on reading comprehension tasks greatly outperforms the model trained on FLAN tasks without reading comprehension tasks. Therefore, these experiments serve as supplementary to our main experiment conducted on T0 datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Met.</head><p>Natural The results when training on the top-3 key datasets for each test task are in Table <ref type="table" target="#tab_2">11</ref> and Table <ref type="table" target="#tab_3">12</ref>. We can see that the model trained on top-3 key datasets shows comparable results with the T0 baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 FULL RESULTS EVALUATED ON HELD-OUT TEST DATASETS</head><p>Full results evaluated on held-out test datasets are in Table <ref type="table" target="#tab_5">13</ref> and<ref type="table" target="#tab_2">Table 14.</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Pairwise transfer relationships on T5-XL. The entry at row i and column j denotes the average performance when the model is trained on task i and evaluated on task j. For each entry, the value is the average score of different prompts. (Accuracy if only Accuracy is calculated, and otherwise the mean of Accuracy and F1.) Only those prompts related to the original tasks are included for evaluation. We highlight those entries with high scores for each task (Red is the Top-1). The horizontal and vertical lines denote the boundary of task-type groups.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>context and choose the best option to answer the question \n Context: {{text}} \n Question: {{question}} \n Options: {{answer_choices | join("\\n-")}} \n 1 star ||| 2 stars ||| 3 stars ||| 4 stars ||| 5 stars (a) Predict the full choice. context and choose the best option to answer the question \n Context: {{text}} \n Question: {{question}} \n Options: \nA. 1 star \nB. 2 stars \nC. 3 stars \nD. 4 stars \nE.5 stars \n A ||| B ||| C ||| D ||| E (b) Predict the option label.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An illustration of multiple-choice QA formatted sentiment analysis prompts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The pipeline of the improved multi-task prompted training recipe. We detect key tasks by examining the pairwise generalization results between training tasks. These key tasks are upsampled, or non-key tasks are downsampled to form an optimized mixture of datasets.</figDesc><graphic url="image-1.png" coords="7,212.22,117.14,105.51,70.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Zero-shot performance of training with/without top-8 tasks (out of 38) on T5-Large. The top-8 tasks are CosmosQA, Social IQA, PAWS, QuAIL, Wiki QA, QuaRTz, QASC, and ROPES.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">Natural Language Inference</cell><cell></cell><cell cols="2">Sentence Completion</cell><cell>Co-Reference WSD.</cell></row><row><cell>Train Tasks</cell><cell>Met.</cell><cell>RTE</cell><cell>CB</cell><cell cols="6">ANLI1 ANLI2 ANLI3 COPA Hella. Story. WSC Wino.</cell><cell>WiC</cell><cell>Avg.</cell></row><row><cell></cell><cell cols="3">Mean 72.53 50.60</cell><cell>30.93</cell><cell>31.96</cell><cell>32.23</cell><cell>82.20</cell><cell cols="2">27.16 92.05 62.21 52.00</cell><cell>50.14</cell><cell>53.09</cell></row><row><cell>All T0 Tasks</cell><cell cols="3">Med. 74.01 57.14</cell><cell>30.40</cell><cell>31.60</cell><cell>31.75</cell><cell>83.00</cell><cell cols="2">27.60 91.77 62.98 52.33</cell><cell>50.00</cell><cell>54.87</cell></row><row><cell></cell><cell cols="3">Mean 73.10 66.55</cell><cell>33.55</cell><cell>32.46</cell><cell>36.34</cell><cell>84.62</cell><cell cols="2">30.93 94.51 64.04 53.02</cell><cell>50.52</cell><cell>56.33</cell></row><row><cell>Top-8 Only</cell><cell cols="3">Med. 74.91 71.42</cell><cell>33.10</cell><cell>32.10</cell><cell>36.42</cell><cell>84.50</cell><cell cols="2">30.96 94.76 65.38 52.96</cell><cell>50.16</cell><cell>56.97</cell></row><row><cell></cell><cell cols="3">Mean 60.47 44.17</cell><cell>30.68</cell><cell>32.81</cell><cell>32.87</cell><cell>67.07</cell><cell cols="2">26.46 68.81 51.83 51.29</cell><cell>50.92</cell><cell>47.03</cell></row><row><cell>T0 Tasks w/o Top-8</cell><cell cols="3">Med. 60.29 44.64</cell><cell>30.80</cell><cell>33.00</cell><cell>33.33</cell><cell>66.83</cell><cell cols="2">26.65 72.53 47.12 51.54</cell><cell>50.71</cell><cell>47.04</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">Natural Language Inference</cell><cell></cell><cell>Sentence Completion</cell><cell>Co-Reference WSD</cell></row><row><cell>Task</cell><cell>Met.</cell><cell>RTE</cell><cell>CB</cell><cell cols="4">ANLI1 ANLI2 ANLI3 COPA Hella. Story. WSC Wino.</cell><cell>WiC</cell><cell>Avg.</cell></row><row><cell></cell><cell cols="3">Mean 52.71 32.50</cell><cell>33.38</cell><cell>33.59</cell><cell>33.38</cell><cell>53.30</cell><cell>25.65 46.69 36.54 49.93 49.97 40.69</cell></row><row><cell>Yelp</cell><cell cols="3">Med. 52.71 39.29</cell><cell>33.40</cell><cell>33.40</cell><cell>33.50</cell><cell>54.00</cell><cell>25.82 46.77 36.54 50.08 50.00 41.41</cell></row><row><cell></cell><cell cols="3">Mean 52.78 37.02</cell><cell>33.27</cell><cell>33.49</cell><cell>33.57</cell><cell>57.37</cell><cell>24.93 50.24 36.54 50.09 49.86 41.74</cell></row><row><cell>Yelp2QA</cell><cell cols="3">Med. 52.71 41.07</cell><cell>33.40</cell><cell>33.40</cell><cell>33.50</cell><cell>58.00</cell><cell>24.96 50.45 36.54 50.28 50.00 42.21</cell></row></table><note><p><p>Zero-shot performance of T0-XL trained on Yelp Review Full and QA-formatted Yelp Review Full respectively.</p>Considering that QA tasks account for a large portion of the T0 benchmark, which might affect our conclusion, we also validated it in another setting (i.e., QA tasks only account for 7/43 of all training tasks) and presented the results in Appendix C.3. The results verify the effectiveness of QA tasks.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Examples of part of the QA tasks. CosmosQA and Social IQA both show excellect general transfer ability, while WikiHop QA and WiQA don't. The biggest difference between them is the knowledge domain.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc></figDesc><table><row><cell>Model</cell><cell>Met.</cell><cell>RTE</cell><cell cols="4">Natural Language Inference CB ANLI1 ANLI2 ANLI3 COPA Hella. Story. WSC Wino. Sentence Completion Co-Reference WSD. WiC</cell><cell>Avg.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">T5-Large-LM-Adapt (770M)</cell></row><row><cell>T0 (*)</cell><cell cols="3">Mean 72.53 50.60 Med. 74.01 57.14</cell><cell>30.93 30.40</cell><cell>31.96 31.60</cell><cell>32.23 31.75</cell><cell>82.20 27.16 92.05 62.21 52.00 50.14 53.09 83.00 27.60 91.77 62.98 52.33 50.00 54.87</cell></row><row><cell>DS-T0</cell><cell cols="3">Mean 74.22 60.95 Med. 75.45 66.07</cell><cell>35.65 36.00</cell><cell>32.57 32.40</cell><cell>35.88 36.50</cell><cell>87.64 28.29 94.12 63.75 54.89 51.60 56.33 87.50 28.38 94.01 64.42 55.33 51.41 57.04</cell></row><row><cell>DS+DA-T0</cell><cell cols="3">Mean 80.72 71.90 Med. 81.23 80.36</cell><cell>36.00 36.40</cell><cell>34.80 35.20</cell><cell>38.18 39.33</cell><cell>84.10 26.00 94.00 63.27 54.54 50.58 57.65 85.21 26.06 94.39 63.94 54.38 50.31 58.80</cell></row><row><cell>US-T0</cell><cell cols="3">Mean 78.30 61.55 Med. 79.00 69.60</cell><cell>36.05 36.10</cell><cell>34.06 33.90</cell><cell>36.50 36.75</cell><cell>87.79 28.05 94.97 62.40 55.52 51.46 56.97 89.00 28.14 94.92 64.42 56.43 50.39 58.06</cell></row><row><cell>US+DA-T0</cell><cell cols="3">Mean 80.69 70.95 Med. 80.69 80.35</cell><cell>37.38 38.00</cell><cell>34.20 34.20</cell><cell>39.43 40.33</cell><cell>87.97 26.73 93.71 63.27 55.58 51.47 58.31 89.29 26.98 93.91 64.42 55.72 51.25 59.56</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">T5-XL-LM-Adapt (3B)</cell></row><row><cell>T0 ( ?)</cell><cell cols="3">Mean 64.55 45.36 Med. 64.08 50.00</cell><cell>33.84 33.65</cell><cell>33.11 33.40</cell><cell>33.33 33.33</cell><cell>72.40 27.29 84.03 65.10 50.97 50.69 50.97 74.92 27.51 85.09 64.42 50.51 50.39 51.57</cell></row><row><cell>T0 (*)</cell><cell cols="3">Mean 80.72 67.62 Med. 80.14 75.00</cell><cell>41.09 42.80</cell><cell>37.79 39.20</cell><cell>40.38 41.75</cell><cell>91.92 32.03 97.27 65.96 57.84 50.14 60.37 92.00 32.29 97.22 68.27 58.41 50.00 61.62</cell></row><row><cell>DS-T0</cell><cell cols="3">Mean 83.21 73.33 Med. 82.67 82.14</cell><cell>44.38 45.40</cell><cell>38.84 39.70</cell><cell>43.72 45.58</cell><cell>94.17 31.21 97.72 64.42 62.67 52.01 62.34 94.50 32.03 97.70 64.42 63.38 51.33 63.53</cell></row><row><cell>DS+DA-T0</cell><cell cols="3">Mean 84.77 74.40 Med. 84.66 82.14</cell><cell>43.25 46.30</cell><cell>39.17 39.70</cell><cell>43.22 45.75</cell><cell>94.93 27.01 97.65 62.02 66.74 53.09 62.39 95.00 27.00 97.65 62.98 65.35 52.90 63.58</cell></row><row><cell>US-T0</cell><cell cols="3">Mean 82.41 69.38 Med. 82.34 82.81</cell><cell>43.20 45.41</cell><cell>38.40 39.94</cell><cell>40.72 42.60</cell><cell>93.55 30.30 97.25 61.41 60.56 53.66 60.99 93.75 30.38 97.34 63.67 62.27 52.66 63.02</cell></row><row><cell>US+DA-T0</cell><cell cols="3">Mean 83.29 75.83 Med. 84.48 82.14</cell><cell>44.80 47.90</cell><cell>39.09 39.90</cell><cell>43.68 47.25</cell><cell>94.81 26.29 96.94 61.73 66.03 53.28 62.34 94.50 26.17 97.06 64.90 65.11 52.98 63.85</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">T5-XXL-LM-Adapt (11B)</cell></row><row><cell>T0 ( ?)</cell><cell cols="3">Mean 80.83 70.12 Med. 81.23 78.57</cell><cell>43.56 44.70</cell><cell>38.68 39.40</cell><cell>41.26 42.42</cell><cell>90.02 33.58 92.40 61.45 59.94 56.58 60.77 90.79 33.65 94.71 64.42 60.46 57.21 62.51</cell></row><row><cell>T0 (*)</cell><cell cols="3">Mean 84.01 72.26 Med. 85.02 83.93</cell><cell>47.89 49.00</cell><cell>42.80 44.00</cell><cell>46.49 48.58</cell><cell>91.60 35.27 98.15 62.69 69.46 54.83 63.98 95.00 34.62 98.24 66.35 70.24 52.35 66.12</cell></row><row><cell>Our Best</cell><cell cols="4">Mean 85.56 72.50 Med. 85.74 82.14 51.60 48.28</cell><cell>43.81 46.40</cell><cell>47.62 51.25</cell><cell>95.18 27.91 97.46 67.02 71.41 56.29 64.82 95.00 27.71 97.54 66.35 71.19 58.46 66.67</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Training hyper-parameters for our experiments.</figDesc><table><row><cell></cell><cell cols="2">Batch Size Steps</cell></row><row><cell>Single Task Transfer</cell><cell>512</cell><cell>1000</cell></row><row><cell>Top-3 Task Transfer</cell><cell>1024</cell><cell>2000</cell></row><row><cell>Top-8 Task Transfer</cell><cell>1024</cell><cell>10000</cell></row><row><cell>Full Dataset</cell><cell>1024</cell><cell>20000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Statistics of the training sets. "Orig Num" denotes the size of the original dataset. "T0 Num" denotes the size of prompted data in the T0 baseline. "US Num" denotes the size of prompted data we used in our upsampling experiments. "DS Num" denotes the size of prompted data we used in our downsampling experiments. "MSTTR" denotes the mean segmental TTR, which is an indicator to reflect lexical diversity. "In Len" denotes the average length of prompted input data. "Out Len" denotes the average length of target prompted target data.</figDesc><table><row><cell>Task Name</cell><cell></cell><cell cols="7">Orig Num T0 Num US Num DS Num MSTTR In Len Out Len</cell></row><row><cell>MRPC</cell><cell></cell><cell cols="2">3668</cell><cell>23288</cell><cell></cell><cell>23288</cell><cell cols="2">23288</cell><cell>0.74</cell><cell>53</cell><cell>1</cell></row><row><cell>QQP</cell><cell></cell><cell cols="2">363846</cell><cell>2183076</cell><cell></cell><cell>49998</cell><cell cols="2">50000</cell><cell>0.67</cell><cell>30</cell><cell>1</cell></row><row><cell>PAWS</cell><cell></cell><cell cols="2">49401</cell><cell>565240</cell><cell cols="2">565240</cell><cell cols="2">50000</cell><cell>0.64</cell><cell>47</cell><cell>1</cell></row><row><cell>Hotpot QA</cell><cell></cell><cell cols="2">88869</cell><cell>444345</cell><cell cols="2">250000</cell><cell cols="2">444345</cell><cell>0.75</cell><cell>18</cell><cell>2</cell></row><row><cell>Wiki QA</cell><cell></cell><cell cols="2">20360</cell><cell>108040</cell><cell cols="2">108040</cell><cell cols="2">50000</cell><cell>0.69</cell><cell>9</cell><cell>27</cell></row><row><cell>Adv./DBidaf</cell><cell></cell><cell cols="2">10000</cell><cell>50000</cell><cell cols="2">250000</cell><cell cols="2">50000</cell><cell>0.78</cell><cell>130</cell><cell>4</cell></row><row><cell>Adv./DBERT</cell><cell></cell><cell cols="2">10000</cell><cell>50000</cell><cell cols="2">250000</cell><cell cols="2">50000</cell><cell>0.78</cell><cell>146</cell><cell>4</cell></row><row><cell cols="2">Adv./DRoberTa</cell><cell cols="2">10000</cell><cell>50000</cell><cell cols="2">250000</cell><cell cols="2">50000</cell><cell>0.78</cell><cell>143</cell><cell>4</cell></row><row><cell>DuoRC/Self.</cell><cell></cell><cell cols="2">60721</cell><cell>545235</cell><cell></cell><cell>49995</cell><cell cols="2">50000</cell><cell>0.79</cell><cell>372</cell><cell>2</cell></row><row><cell>DuoRC/Para.</cell><cell></cell><cell cols="2">69524</cell><cell>604172</cell><cell></cell><cell>49995</cell><cell cols="2">50000</cell><cell>0.79</cell><cell>356</cell><cell>7</cell></row><row><cell>ROPES</cell><cell></cell><cell cols="2">10924</cell><cell>131088</cell><cell cols="2">655440</cell><cell cols="2">131088</cell><cell>0.76</cell><cell>211</cell><cell>1</cell></row><row><cell>Quoref</cell><cell></cell><cell cols="2">19399</cell><cell>213389</cell><cell cols="2">213389</cell><cell cols="2">50000</cell><cell>0.80</cell><cell>328</cell><cell>2</cell></row><row><cell>Cos E</cell><cell></cell><cell cols="2">9741</cell><cell>107151</cell><cell cols="2">107151</cell><cell cols="2">50000</cell><cell>0.74</cell><cell>30</cell><cell>1</cell></row><row><cell>Cosmos QA</cell><cell></cell><cell cols="2">25262</cell><cell cols="5">328406 1642030 328406</cell><cell>0.76</cell><cell>10</cell><cell>8</cell></row><row><cell>DREAM</cell><cell></cell><cell cols="2">6116</cell><cell>30580</cell><cell></cell><cell>30580</cell><cell cols="2">30580</cell><cell>0.73</cell><cell>141</cell><cell>4</cell></row><row><cell>QASC</cell><cell></cell><cell cols="2">8134</cell><cell>65072</cell><cell></cell><cell>65072</cell><cell cols="2">50000</cell><cell>0.66</cell><cell>63</cell><cell>2</cell></row><row><cell>QuAIL</cell><cell></cell><cell cols="2">10246</cell><cell>133198</cell><cell cols="2">665990</cell><cell cols="2">133198</cell><cell>0.80</cell><cell>372</cell><cell>5</cell></row><row><cell>QuaRel</cell><cell></cell><cell cols="2">1941</cell><cell>9705</cell><cell></cell><cell>9705</cell><cell cols="2">9705</cell><cell>0.72</cell><cell>57</cell><cell>2</cell></row><row><cell>QuaRTz</cell><cell></cell><cell cols="2">2696</cell><cell>21568</cell><cell cols="2">107840</cell><cell cols="2">21568</cell><cell>0.70</cell><cell>47</cell><cell>1</cell></row><row><cell>SciQ</cell><cell></cell><cell cols="2">11679</cell><cell>58395</cell><cell></cell><cell>58395</cell><cell cols="2">50000</cell><cell>0.73</cell><cell>89</cell><cell>2</cell></row><row><cell>Social IQA</cell><cell></cell><cell cols="2">33410</cell><cell cols="5">200460 1002300 200460</cell><cell>0.73</cell><cell>49</cell><cell>1</cell></row><row><cell>Wiki Hop</cell><cell></cell><cell cols="2">43738</cell><cell>393642</cell><cell cols="2">393642</cell><cell cols="2">50000</cell><cell>0.75</cell><cell>1248</cell><cell>2</cell></row><row><cell>WiQA</cell><cell></cell><cell cols="2">29808</cell><cell>238464</cell><cell cols="2">238464</cell><cell cols="2">50000</cell><cell>0.68</cell><cell>94</cell><cell>1</cell></row><row><cell>Amazon</cell><cell></cell><cell cols="2">3600000</cell><cell>499995</cell><cell></cell><cell>49995</cell><cell cols="2">50000</cell><cell>0.79</cell><cell>95</cell><cell>1</cell></row><row><cell>App Reviews</cell><cell></cell><cell cols="2">288065</cell><cell>1152260</cell><cell></cell><cell>50000</cell><cell cols="2">50000</cell><cell>0.56</cell><cell>33</cell><cell>1</cell></row><row><cell>IMDB</cell><cell></cell><cell cols="2">25000</cell><cell>275000</cell><cell cols="2">275000</cell><cell cols="2">50000</cell><cell>0.81</cell><cell>209</cell><cell>4</cell></row><row><cell cols="2">Rotten Tomatoes</cell><cell cols="2">8530</cell><cell>85300</cell><cell></cell><cell>85300</cell><cell cols="2">50000</cell><cell>0.73</cell><cell>29</cell><cell>1</cell></row><row><cell>Yelp</cell><cell></cell><cell cols="2">650000</cell><cell>499996</cell><cell></cell><cell>49994</cell><cell cols="2">50000</cell><cell>0.80</cell><cell>132</cell><cell>2</cell></row><row><cell>Common Gen</cell><cell></cell><cell cols="2">67389</cell><cell>606501</cell><cell></cell><cell>49995</cell><cell cols="2">50000</cell><cell>0.46</cell><cell>16</cell><cell>11</cell></row><row><cell>Wiki Bio</cell><cell></cell><cell cols="2">582659</cell><cell>500000</cell><cell></cell><cell>50000</cell><cell cols="2">50000</cell><cell>0.80</cell><cell>107</cell><cell>82</cell></row><row><cell cols="2">CNN Daily Mail</cell><cell cols="2">287113</cell><cell>2584017</cell><cell></cell><cell>49995</cell><cell cols="2">50000</cell><cell>0.83</cell><cell>66</cell><cell>338</cell></row><row><cell>Gigaword</cell><cell></cell><cell cols="2">3803957</cell><cell>499995</cell><cell></cell><cell>49995</cell><cell cols="2">50000</cell><cell>0.81</cell><cell>17</cell><cell>31</cell></row><row><cell>MultiNews</cell><cell></cell><cell cols="2">44972</cell><cell>269832</cell><cell cols="2">269832</cell><cell cols="2">50000</cell><cell>0.82</cell><cell>769</cell><cell>216</cell></row><row><cell>SamSum</cell><cell></cell><cell cols="2">14732</cell><cell>103124</cell><cell cols="2">103124</cell><cell cols="2">50000</cell><cell>0.78</cell><cell>98</cell><cell>20</cell></row><row><cell>XSum</cell><cell></cell><cell cols="2">204045</cell><cell>2040450</cell><cell></cell><cell>50000</cell><cell cols="2">50000</cell><cell>0.82</cell><cell>257</cell><cell>21</cell></row><row><cell>AG News</cell><cell></cell><cell cols="2">120000</cell><cell>840000</cell><cell></cell><cell>49994</cell><cell cols="2">50000</cell><cell>0.84</cell><cell>45</cell><cell>2</cell></row><row><cell>DBPedia</cell><cell></cell><cell cols="2">560000</cell><cell>500000</cell><cell></cell><cell>50000</cell><cell cols="2">50000</cell><cell>0.80</cell><cell>36</cell><cell>1</cell></row><row><cell>TREC</cell><cell></cell><cell cols="2">5452</cell><cell>47818</cell><cell></cell><cell>47818</cell><cell cols="2">47818</cell><cell>0.58</cell><cell>20</cell><cell>1</cell></row><row><cell>Train Tasks</cell><cell>Met.</cell><cell>RTE</cell><cell cols="6">Natural Language Inference CB ANLI1 ANLI2 ANLI3 COPA Hella. Story. WSC Wino. Sentence Completion Co-Reference WSD WiC</cell><cell>Avg.</cell></row><row><cell>Baseline T0</cell><cell cols="3">Mean 80.72 67.62 Med. 80.14 75.00</cell><cell>41.09 42.80</cell><cell>37.79 39.20</cell><cell>40.38 41.75</cell><cell>91.92 92.00</cell><cell>32.03 97.27 65.96 57.84 50.14 60.37 32.29 97.22 68.27 58.41 50.00 61.62</cell></row><row><cell>Top-8 Only</cell><cell cols="3">Mean 81.37 75.36 Med. 81.59 73.21</cell><cell>41.66 41.70</cell><cell>37.40 37.50</cell><cell>42.26 43.50</cell><cell>93.22 93.00</cell><cell>33.95 96.93 59.64 64.81 54.01 61.87 33.36 96.90 60.06 65.87 54.94 61.97</cell></row><row><cell>T0 Tasks w/o Top-8</cell><cell cols="3">Mean 55.45 45.24 Med. 54.33 46.43</cell><cell>34.60 33.80</cell><cell>34.05 33.90</cell><cell>34.81 34.33</cell><cell>84.68 85.00</cell><cell>28.73 86.50 54.29 42.50 55.94 50.62 29.12 90.11 54.14 37.02 55.72 50.35</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table><row><cell>Train Tasks</cell><cell>Met.</cell><cell>RTE</cell><cell cols="5">Natural Language Inference CB ANLI1 ANLI2 ANLI3 COPA Hella. Story. WSC Wino. Sentence Completion Co-Reference WSD WiC</cell><cell>Avg.</cell></row><row><cell>Baseline T0</cell><cell cols="3">Mean 49.28 42.74 Med. 47.83 50.00</cell><cell>32.95 33.20</cell><cell>33.25 33.30</cell><cell>32.98 33.00</cell><cell>60.59 60.02</cell><cell>25.93 66.05 62.31 49.76 50.24 46.01 26.07 68.36 63.46 49.49 50.16 46.81</cell></row><row><cell>Top-8 Only</cell><cell cols="3">Mean 62.92 61.07 Med. 62.64 66.07</cell><cell>30.24 29.30</cell><cell>32.07 32.00</cell><cell>32.18 32.17</cell><cell>68.20 70.00</cell><cell>26.06 71.23 58.17 49.41 51.27 49.35 26.26 72.96 61.06 49.41 50.94 50.25</cell></row><row><cell>T0 Tasks w/o Top-8</cell><cell cols="3">Mean 56.46 43.93 Med. 55.78 50.00</cell><cell>32.93 32.90</cell><cell>33.26 33.30</cell><cell>33.40 33.17</cell><cell>57.92 57.50</cell><cell>25.72 52.04 53.85 48.97 50.74 44.47 25.70 52.70 57.69 48.86 50.63 45.29</cell></row></table><note><p>Zero-shot performance of training with/without top-8 tasks (out of 38) on T5-XL. The top-8 tasks are CosmosQA, SocialIQA, PAWS, QuAIL, Wiki QA, QuaRTz, QASC, and ROPES. "Top-8 Only" means using only the top-8 tasks. "T0 Tasks w/o Top-8" means using the T0 tasks with top-8 tasks removed.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Zero-shot performance of training with/without top-8 tasks (out of 38) on GPT-Neo. The top-8 tasks are CosmosQA, SocialIQA, PAWS, QuAIL, Wiki QA, QuaRTz, QASC, and ROPES. "Top-8 Only" means using only the top-8 tasks. "T0 Tasks w/o Top-8" means using the T0 tasks with top-8 tasks removed.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>Results of training on FLAN. We train a T5-Large model using the datasets of FLAN. "RC" denotes Reading Comprehension. "ARC/E." denotes "ARC/Easy", and "ARC/C." denotes "ARC/Challenge".</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Language Inference</cell><cell></cell><cell></cell><cell cols="2">Sentence Completion</cell><cell cols="2">Co-Reference</cell></row><row><cell></cell><cell>RTE</cell><cell>CB</cell><cell cols="10">ANLI1 ANLI2 ANLI3 SNLI MNLI WNLI QNLI COPA Hella. PiQA Story. ARC/E. ARC/C.</cell><cell>Avg.</cell></row><row><cell>Baseline</cell><cell cols="2">Mean 73.24 78.57 Med. 77.26 82.14</cell><cell>42.08 42.30</cell><cell>40.08 40.00</cell><cell>43.17 43.67</cell><cell>58.33 61.04 58.55 61.03</cell><cell>52.54 52.82</cell><cell>74.58 76.97</cell><cell>82.88 83.00</cell><cell>41.42 67.38 90.61 41.46 67.66 90.30</cell><cell>58.39 58.60</cell><cell>40.58 40.80</cell><cell>60.33 61.10</cell></row><row><cell>RC. Only</cell><cell cols="2">Mean 70.88 66.07 Med. 75.81 67.86</cell><cell>38.44 38.8</cell><cell>37.47 37.40</cell><cell>42.13 42.42</cell><cell>58.89 53.88 59.23 58.58</cell><cell>46.90 46.48</cell><cell>62.29 65.73</cell><cell>90.92 90.70</cell><cell>38.94 67.92 90.92 38.99 68.25 90.70</cell><cell>60.06 60.18</cell><cell>43.59 43.31</cell><cell>56.91 57.95</cell></row><row><cell>w/o RC.</cell><cell cols="2">Mean 65.98 71.03 Med. 70.40 71.43</cell><cell>36.52 36.00</cell><cell>36.29 36.40</cell><cell>39.07 39.25</cell><cell>51.15 56.63 50.04 57.09</cell><cell>50.70 50.70</cell><cell>63.13 64.54</cell><cell>62.75 62.00</cell><cell>28.28 55.51 50.68 28.41 55.58 49.65</cell><cell>41.78 41.67</cell><cell>27.65 27.76</cell><cell>49.14 49.39</cell></row><row><cell cols="4">D FULL RESULTS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">D.1 FULL RESULTS ON TOP-3 KEY DATASETS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p><rs type="person">Jian Li</rs> and <rs type="person">Jing Zhou</rs> are supported in part by the <rs type="funder">National Natural Science Foundation of China</rs> Grant <rs type="grantNumber">62161146004</rs>, <rs type="projectName">Turing AI Institute of Nanjing and Xi</rs>'an <rs type="institution">Institute for Interdisciplinary Information Core Technology</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_JHZhS6u">
					<idno type="grant-number">62161146004</idno>
					<orgName type="project" subtype="full">Turing AI Institute of Nanjing and Xi</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Muppet: Massive multi-task representations with pre-finetuning</title>
		<author>
			<persName><forename type="first">Armen</forename><surname>Aghajanyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anchit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshat</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sonal</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5799" to="5811" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Ext5: Towards extreme multi-task scaling for transfer learning</title>
		<author>
			<persName><forename type="first">Vamsi</forename><surname>Aribandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaixiu</forename><surname>Steven Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanket</forename><surname>Vaibhav Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglei</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Vinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dara</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmo</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jai</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Prakash Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><surname>Metzler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>In ICLR. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Efficient large scale language modeling with mixtures of experts</title>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Bhosale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Victoria Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srinivasan</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramakanth</forename><surname>Pasunuru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giridharan</forename><surname>Anantharaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Halil</forename><surname>Akin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandeep</forename><surname>Baines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Punit</forename><surname>Singh Koura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian O'</forename><surname>Horo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><forename type="middle">T</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="11699" to="11732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">If you use this software, please cite it using these metadata</title>
		<author>
			<persName><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><surname>Leahy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Gpt-Neo ; Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><surname>Litwin</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.5297715</idno>
		<ptr target="https://doi.org/10.5281/zenodo.5297715" />
	</analytic>
	<monogr>
		<title level="m">Large Scale Autoregressive Language Modeling with Mesh-Tensorflow</title>
		<meeting><address><addrLine>Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">March 2021. 2020</date>
		</imprint>
	</monogr>
	<note>NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<author>
			<persName><forename type="first">Joaquin</forename><surname>Qui?onero Candela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning Challenges, Evaluating Predictive Uncertainty, Visual Object Classification and Recognizing Textual Entailment, First PASCAL Machine Learning Challenges Workshop, MLCW 2005</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</editor>
		<editor>
			<persName><surname>Florence D'alch?-Buc</surname></persName>
		</editor>
		<meeting><address><addrLine>Southampton, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">April 11-13, 2005. 2006</date>
			<biblScope unit="volume">3944</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Zero-shot text classification via knowledge graph embedding for social media data</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaizhu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frans</forename><surname>Coenen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet of Things Journal</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Scaling instruction-finetuned language models</title>
		<author>
			<persName><forename type="first">Chung</forename><surname>Hyung Won</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shayne</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhartha</forename><surname>Brahma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shane</forename><surname>Shixiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuyun</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirac</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyun</forename><surname>Suzgun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adams</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><forename type="middle">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongkun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Slav</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Wei</surname></persName>
		</author>
		<idno>CoRR, abs/2210.11416</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The commitmentbank: Investigating projection in naturally occurring discourse</title>
		<author>
			<persName><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandy</forename><surname>Simons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judith</forename><surname>Tonhauser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of Sinn und Bedeutung</title>
		<meeting>Sinn und Bedeutung</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="107" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT (1)</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Representation similarity analysis for efficient task taxonomy &amp; transfer learning</title>
		<author>
			<persName><forename type="first">Kshitij</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gemma</forename><surname>Roig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision Foundation / IEEE</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="12387" to="12396" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sequence-to-sequence data augmentation for dialogue language understanding</title>
		<author>
			<persName><forename type="first">Yutai</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1234" to="1245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cosmos QA: machine reading comprehension with contextual commonsense reasoning</title>
		<author>
			<persName><forename type="first">Lifu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP/IJCNLP</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2391" to="2401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Data for visual question answering</title>
		<author>
			<persName><forename type="first">Kushal</forename><surname>Kafle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><forename type="middle">A</forename><surname>Yousefhussien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Kanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">198-202, 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Looking beyond the surface: A challenge set for reading comprehension over multiple sentences</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Snigdha</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shyam</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unifiedqa: Crossing format boundaries with a single QA system</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP (Findings), volume EMNLP 2020 of Findings of ACL</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1896" to="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">QASC: A dataset for question answering via sentence composition</title>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Guerquin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8082" to="8090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The winograd schema challenge</title>
		<author>
			<persName><forename type="first">Hector</forename><surname>Levesque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ernest</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leora</forename><surname>Morgenstern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Reasoning over paragraph effects in situations</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MRQA@EMNLP</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="58" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-task deep neural networks for natural language understanding</title>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4487" to="4496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cutting down on prompts and parameters: Simple few-shot learning with language models</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename><surname>Robert L Logan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivana</forename><surname>Bala?evi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (Findings)</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2824" to="2835" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The natural language decathlon: Multitask learning as question answering</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno>CoRR, abs/1806.08730</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cross-task generalization via natural language crowdsourcing instructions</title>
		<author>
			<persName><forename type="first">Swaroop</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chitta</forename><surname>Baral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="3470" to="3487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Lsdsem 2017 shared task: The story cloze test</title>
		<author>
			<persName><forename type="first">Nasrin</forename><surname>Mostafazadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Annie</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LSDSem@EACL</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="46" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adversarial NLI: A new benchmark for natural language understanding</title>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4885" to="4901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Wic: the word-in-context dataset for evaluating context-sensitive meaning representations</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Taher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pilehvar</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jos?</forename><surname>Camacho-Collados</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Intermediate-task transfer learning with pretrained models for natural language understanding: When and why does it work?</title>
		<author>
			<persName><forename type="first">Yada</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haokun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mon</forename><surname>Phu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyi</forename><surname>Htut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">Yuanzhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katharina</forename><surname>Vania</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Kann</surname></persName>
		</author>
		<author>
			<persName><surname>Bowman</surname></persName>
		</author>
		<idno>CoRR, abs/2005.00628</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Scaling language models: Methods, analysis &amp; insights from training gopher</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Jack W Rae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Aslanides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susannah</forename><surname>Ring</surname></persName>
		</author>
		<author>
			<persName><surname>Young</surname></persName>
		</author>
		<idno>CoRR, abs/2112.11446</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">67</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Choice of plausible alternatives: An evaluation of commonsense causal reasoning</title>
		<author>
			<persName><forename type="first">Melissa</forename><surname>Roemmele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cosmin</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Bejan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">S</forename><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning</title>
		<imprint>
			<publisher>AAAI</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="90" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Getting closer to AI complete question answering: A set of prerequisite real tasks</title>
		<author>
			<persName><forename type="first">Anna</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Kovaleva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Rumshisky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8722" to="8731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Winogrande: An adversarial winograd schema challenge at scale</title>
		<author>
			<persName><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8732" to="8740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Multitask prompted training enables zero-shot task generalization</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lintang</forename><surname>Sutawika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaid</forename><surname>Alyafeai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Chaffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnaud</forename><surname>Stiegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Raja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manan</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saiful Bari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Urmish</forename><surname>Thakker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanya</forename><surname>Sharma Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Szczechla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taewoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunjan</forename><surname>Chhablani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nihal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debajyoti</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><surname>Tian-Jian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Manica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng Xin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harshit</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Bawden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
		<editor>Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault F?vry, Jason Alan Fries, Ryan Teehan, Teven Le Scao</editor>
		<imprint>
			<date type="published" when="2022">2022</date>
			<pubPlace>Stella Biderman, Leo Gao, Thomas Wolf</pubPlace>
		</imprint>
	</monogr>
	<note>In ICLR. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Social iqa: Commonsense reasoning about social interactions</title>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Le Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP/IJCNLP</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4462" to="4472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">It&apos;s not just size that matters: Small language models are also few-shot learners</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2339" to="2352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep model transferability from attribution maps</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengchao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingli</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6179" to="6189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Quartz: An open-domain dataset of qualitative relationship questions</title>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP/IJCNLP</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5940" to="5945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Exploring and predicting transferability across NLP tasks</title>
		<author>
			<persName><forename type="first">Tu</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsendsuren</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mattarella-Micke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7882" to="7926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">That&apos;s so annoying!!!: A lexical and frame-semantic embedding based data augmentation approach to automatic categorization of annoying behaviors using #petpeeve tweets</title>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<publisher>The Association for Computational Linguistics</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2557" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Do prompt-based models really understand the meaning of their prompts?</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2300" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Finetuned language models are zero-shot learners</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>In ICLR. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">EDA: easy data augmentation techniques for boosting performance on text classification tasks</title>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP/IJCNLP</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6381" to="6387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Wikiqa: A challenge dataset for open-domain question answering</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<publisher>The Association for Computational Linguistics</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2013" to="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Zero-shot text classification via reinforced self-training</title>
		<author>
			<persName><forename type="first">Zhiquan</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxia</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaoyan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoxiao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3014" to="3024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Taskonomy: Disentangling task transfer learning</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">B</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE Computer Society</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3712" to="3722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Hellaswag: Can a machine really finish your sentence?</title>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4791" to="4800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Integrating semantic knowledge to tackle zero-shot text classification</title>
		<author>
			<persName><forename type="first">Jingqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piyawat</forename><surname>Lertvittayakumjorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yike</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT (1)</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1031" to="1040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Record: Bridging the gap between human and machine commonsense reading comprehension</title>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<idno>CoRR, abs/1810.12885</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junbo</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jake</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learn to adapt for generalized zero-shot text classification</title>
		<author>
			<persName><forename type="first">Yiwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caixia</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongbin</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="517" to="527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Tasks Met. Natural Language Inference Sentence Completion Co-Reference WSD</title>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT (1)</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1298" to="1308" />
		</imprint>
	</monogr>
	<note>PAWS: paraphrase adversaries from word scrambling</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">ANLI1 ANLI2 ANLI3 COPA Hella. Story. WSC Wino. WiC Baseline T0 Mean</title>
		<author>
			<persName><surname>Rte Cb</surname></persName>
		</author>
		<idno>71.94 56.46 32.81 32.29 34.24 84.77 27.09 93.45 64.30 54.33 50.45</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title/>
		<author>
			<persName><surname>Rte Key Mean</surname></persName>
		</author>
		<idno>72.42 54.17 32.08 31.87 34.22 69.41 28.37 82.43 64.52 51.78 50.67</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Key Mean</surname></persName>
		</author>
		<idno>56.86 60.48 33.17 32.94 34.98 81.04 28.48 93.34 44.81 54.57 51.79</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Copa</forename><surname>Key Mean</surname></persName>
		</author>
		<idno>59.49 40.36 31.73 31.65 32.06 77.52 29.31 93.32 51.63 50.92 49.82</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title/>
		<author>
			<persName><surname>Hella</surname></persName>
		</author>
		<idno>63.10 49.05 32.93 34.03 35.48 81.61 30.15 94.43 47.79 51.55 49.88</idno>
		<imprint/>
	</monogr>
	<note type="report_type">KEY Mean</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title/>
		<author>
			<persName><surname>Story</surname></persName>
		</author>
		<idno>KEY Mean 59.49 40.36 31.73 31.65 32.06 77.52 29.31 93.32 51.63 50.92 49.82</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title/>
		<author>
			<persName><surname>Wsc Key Mean</surname></persName>
		</author>
		<idno>55.09 42.62 31.65 32.30 32.39 50.45 23.92 48.77 63.75 50.88 50.99</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Zero-shot results when training with the top-3 key tasks for each test task. The experiments are conducted on the T5-Large model. The entry at (row i, column j) denotes the model performance on test task j after being trained with the top-3 key tasks of task-i. Mean denotes the mean performance on all prompts, and Med. denotes the median performance on all prompts. Bold denotes the best performance for each evaluation dataset</title>
	</analytic>
	<monogr>
		<title level="j">Tasks Met. Natural Language Inference Sentence Completion Co-Reference WSD</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">RTE CB ANLI1 ANLI2 ANLI3 COPA Hella. Story. WSC Wino</title>
		<idno>WiC Baseline T0 Mean 80.72 67.62</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title/>
		<author>
			<persName><surname>Rte Key Mean</surname></persName>
		</author>
		<idno>79.63 65.95 38.95 37.77 37.90 72.53 31.31 83.14 59.71 52.88 52.13</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Key Mean</surname></persName>
		</author>
		<idno>77.22 63.93 41.25 38.86 40.62 87.50 35.43 95.28 61.44 56.02 51.77</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Copa</forename><surname>Key Mean</surname></persName>
		</author>
		<idno>60.97 58.21 37.07 35.16 37.07 83.95 32.45 95.47 49.04 54.10 51.54</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title/>
		<author>
			<persName><surname>Hella Key Mean</surname></persName>
		</author>
		<idno>69.42 65.36 38.85 37.20 38.50 87.07 33.66 97.24 50.38 55.44 51.03</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title/>
		<author>
			<persName><surname>Story Key Mean</surname></persName>
		</author>
		<idno>69.42 65.36 38.85 37.20 38.50 87.07 33.66 97.24 50.38 55.44 51.03</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title/>
		<author>
			<persName><surname>Wsc Key Mean</surname></persName>
		</author>
		<idno>60.36 52.38 33.73 33.97 33.31 53.50 26.39 51.43 63.75 51.70 52.02</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title/>
		<author>
			<persName><surname>Wino Key Mean</surname></persName>
		</author>
		<idno>66.21 69.88 34.95 35.17 27.73 75.27 25.37 67.48 50.58 61.50 50.20</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title/>
		<author>
			<persName><surname>Wic Key Mean</surname></persName>
		</author>
		<idno>56.03 52.86 34.64 34.07 33.98 87.20 31.23 95.28 47.31 53.91 50.83</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Zero-shot results when training with the top-3 key tasks for each test task. The experiments are conducted on the T5-XL model. The entry at (row i, column j) denotes the model performance on test task j after being trained with the top-3 key tasks of task-i. Mean denotes the mean performance on all prompts, and Med. denotes the median performance on all prompts. Bold denotes the best performance for each evaluation dataset</title>
	</analytic>
	<monogr>
		<title level="j">Task Names Met. Natural Language Inference Sentence Completion Co-Reference WSD</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">ANLI1 ANLI2 ANLI3 COPA Hella. Story. WSC Wino. WiC MRPC Mean</title>
		<author>
			<persName><surname>Rte Cb</surname></persName>
		</author>
		<idno>49.96 47.74 33.12 33.21 32.94 53.9 23.39 48.06 61.92 50.4 50.13</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title/>
		<author>
			<persName><surname>Hotpot Qa Mean</surname></persName>
		</author>
		<idno>57.11 39.4 33.55 34.41 33.17 44.02 24.95 46.82 59.71 51.57 50.38 Med. 56.32 46.43 33.5 34.7 33.08 45.03 24.76 46.93 62.02 51.3 50.47</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title/>
		<author>
			<persName><surname>Wiki Qa Mean</surname></persName>
		</author>
		<idno>63.18 47.26 29.99 30.88 33.14 53.74 22.79 48.84 59.52 49.72 51.3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">/</forename><surname>Duorc</surname></persName>
		</author>
		<author>
			<persName><surname>Para</surname></persName>
		</author>
		<idno>Mean 53.21 40.95 32.25 33.13 32.72 46.89 25.4 46.44 46.44 50.31 49.97</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title/>
		<author>
			<persName><surname>Quoref</surname></persName>
		</author>
		<idno>Mean 51.05 26.79 33.85 33.91 33.16 51.77 25.46 51.1 58.37 51.25 51.76 Med. 50.36 26.79 33.6 33.7 33.33 53.0 25.47 50.13 62.5 51.22 50.78</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title/>
		<idno>50.14 38.93 32.68 33.54 33.34 66.54 27.2 63.55 50.19 51.81 52.23</idno>
	</analytic>
	<monogr>
		<title level="j">Cos E Mean</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title/>
		<author>
			<persName><surname>Quarel</surname></persName>
		</author>
		<idno>Mean 47.58 29.05 33.33 34.59 33.51 58.04 25.0 50.8 44.9 49.69 50.56 Med. 46.75 35.71 33.4 34.6 33.42 58.5 25.13 51.04 44.23 49.72 50.55</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title/>
		<author>
			<persName><surname>Sciq</surname></persName>
		</author>
		<idno>Mean 52.96 20.36 33.15 32.97 33.28 50.8 24.63 45.09 45.19 51.25 48.71 Med. 52.71 8.93 33.3 33.3 33.5 50.0 24.62 45.06 37.02 51.3 48.98</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Wiki</forename><surname>Hop</surname></persName>
		</author>
		<idno>Mean 52.96 34.76 33.53 33.53 33.29 52.58 25.82 48.79 39.52 50.07 50.03</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title/>
		<author>
			<persName><surname>Wiqa</surname></persName>
		</author>
		<idno>Mean 52.64 37.98 32.93 33.46 33.56 48.96 21.52 40.5 37.98 49.6 51.22</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title/>
		<idno>52.31 31.07 32.95 33.15 33.27 55.14 23.98 53.75 42.6 49.64 50.08</idno>
	</analytic>
	<monogr>
		<title level="j">App Reviews Mean</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title/>
		<idno>47.4 37.5 33.38 33.27 33.31 53.47 26.32 49.3 61.92 49.57 50.03</idno>
	</analytic>
	<monogr>
		<title level="j">Common Gen Mean</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Wiki</forename><surname>Bio</surname></persName>
		</author>
		<idno>Mean 47.83 42.98 32.93 33.65 33.06 50.6 23.08 44.76 55.58 49.5 49.84</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Mail</forename><surname>Daily</surname></persName>
		</author>
		<idno>Mean 48.27 40.48 32.85 33.8 33.09 46.69 25.77 51.82 56.63 49.27 50.02 Med. 47.29 50.0 33.2 33.4 33.0 45.42 25.85 51.63 63.46 49.17 50.0</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title/>
		<author>
			<persName><surname>Gigaword</surname></persName>
		</author>
		<idno>Mean 53.03 26.31 33.34 34.19 33.46 57.45 23.28 44.5 49.81 49.98 49.4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title/>
		<author>
			<persName><surname>Samsum</surname></persName>
		</author>
		<idno>Mean 47.69 27.98 33.63 32.87 32.89 51.55 25.31 46.89 52.69 50.99 50.39 Med. 46.93 28.57 33.6 33.2 33.08 52.5 25.34 46.87 59.13 50.75 50.08</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title/>
		<author>
			<persName><surname>Xsum</surname></persName>
		</author>
		<idno>Mean 48.81 42.62 33.43 33.05 33.08 57.86 22.35 45.51 57.31 49.72 52.45</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>News</surname></persName>
		</author>
		<idno>Mean 53.5 34.05 32.95 33.66 33.72 54.01 23.54 51.35 38.27 50.01 51.05</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title/>
		<author>
			<persName><surname>Dbpedia</surname></persName>
		</author>
		<idno>Mean 52.78 35.12 33.24 33.33 33.67 54.86 26.45 52.9 38.46 51.18 51.29</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">The entry at row i and column j denotes the average performance when the model is trained on task i and evaluated on task j. Mean denotes the mean performance on all prompts, and Med. denotes the median performance on all prompts</title>
	</analytic>
	<monogr>
		<title level="m">Experiments on T5-Large</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
	<note>Table</note>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">ANLI1 ANLI2 ANLI3 COPA Hella. Story. WSC Wino. WiC MRPC Mean</title>
		<author>
			<persName><surname>Rte Cb</surname></persName>
		</author>
		<idno>54.44 36.79 33.43 32.87 33.38 55.02 25.83 49.77 61.92 51.68 51.35</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title/>
		<author>
			<persName><surname>Hotpot Qa Mean</surname></persName>
		</author>
		<idno>57.22 21.9 34.55 34.56 34.48 58.82 24.45 58.98 58.37 52.58 50.53 Med. 55.6 10.71 34.3 35.0 34.25 60.71 24.2 58.69 59.62 52.17 50.24</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title/>
		<author>
			<persName><surname>Wiki Qa Mean</surname></persName>
		</author>
		<idno>61.05 52.26 31.49 32.96 32.69 52.81 26.74 58.4 62.69 51.1 50.02</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">/</forename><surname>Duorc</surname></persName>
		</author>
		<author>
			<persName><surname>Para</surname></persName>
		</author>
		<idno>Mean 53.61 39.17 33.82 34.15 33.74 53.83 26.66 47.66 38.17 52.83 50.41 Med. 53.25 41.07 33.5 33.9 33.5 54.5 27.01 46.93 37.02 52.33 50.24</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title/>
		<idno>47.44 34.17 33.98 34.52 33.06 51.1 25.84 51.26 60.77 53.76 49.72</idno>
	</analytic>
	<monogr>
		<title level="j">Quoref Mean</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title/>
		<idno>52.45 38.21 33.65 32.91 33.53 66.72 28.35 51.36 38.56 53.69 50.89 Med. 51.81 39.29 33.5 33.0 33.42 66.83 29.31 50.94 36.54 52.88 51.02</idno>
	</analytic>
	<monogr>
		<title level="j">Cos E Mean</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title/>
		<author>
			<persName><surname>Quarel</surname></persName>
		</author>
		<idno>Mean 52.45 31.43 33.06 33.42 33.46 60.95 27.85 50.33 43.08 50.36 47.96 Med. 52.71 33.93 33.1 33.3 33.5 58.5 28.49 50.24 39.9 50.12 48.04</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title/>
		<author>
			<persName><surname>Sciq</surname></persName>
		</author>
		<idno>Mean 53.18 25.95 32.98 33.31 33.86 56.69 24.15 48.67 37.4 51.76 50.99 Med. 52.71 8.93 33.3 33.3 33.5 55.38 23.62 48.53 36.54 51.46 50.0</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title/>
		<author>
			<persName><surname>Wiqa</surname></persName>
		</author>
		<idno>Mean 53.68 42.74 34.51 34.69 36.19 56.24 22.8 45.12 36.63 53.42 50.71 Med. 52.71 39.29 34.5 34.5 36.0 56.5 21.86 45.22 36.54 53.51 50.0</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title/>
		<idno>54.37 31.43 32.94 33.82 33.09 58.23 23.74 56.73 46.92 49.58 51.88</idno>
	</analytic>
	<monogr>
		<title level="j">App Reviews Mean</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title/>
		<idno>48.12 36.55 33.47 33.72 33.49 50.77 26.09 50.82 58.85 49.77 49.92</idno>
	</analytic>
	<monogr>
		<title level="j">Common Gen Mean</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Wiki</forename><surname>Bio</surname></persName>
		</author>
		<idno>Mean 48.38 34.17 32.73 33.69 33.08 55.03 22.72 46.79 56.83 51.78 50.2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Mail</forename><surname>Daily</surname></persName>
		</author>
		<idno>Mean 48.88 42.38 33.15 33.54 33.08 51.22 28.44 54.16 57.5 50.32 50.02 Med. 47.65 48.21 33.3 33.4 33.0 51.46 28.94 54.36 63.46 50.2 50.0</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<title/>
		<author>
			<persName><surname>Gigaword</surname></persName>
		</author>
		<idno>Mean 53.1 32.38 33.37 33.84 34.18 59.66 23.41 47.15 37.4 50.67 49.86 Med. 52.71 35.71 33.4 33.6 34.33 60.0 22.95 47.3 36.54 50.36 49.84</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<title/>
		<author>
			<persName><surname>Multinews</surname></persName>
		</author>
		<idno>Mean 48.66 41.19 32.99 33.36 32.97 52.9 23.94 48.94 51.63 51.21 50.64 Med. 48.38 41.07 33.2 33.3 32.92 53.04 24.1 48.85 57.69 51.07 50.63</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
		<title/>
		<author>
			<persName><surname>Samsum</surname></persName>
		</author>
		<idno>Mean 47.11 33.21 31.99 32.73 32.18 53.41 24.0 51.69 50.48 50.59 51.68 Med. 46.57 35.71 31.7 32.8 31.83 52.0 24.01 52.11 51.92 50.99 51.25</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<title/>
		<author>
			<persName><surname>Xsum</surname></persName>
		</author>
		<idno>Mean 47.04 43.21 33.31 34.01 32.94 52.43 20.31 48.13 58.65 49.85 49.98</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>News</surname></persName>
		</author>
		<idno>Mean 52.24 47.98 33.41 33.07 33.38 59.38 25.64 47.4 41.92 50.32 50.2 Med. 52.35 53.57 33.4 33.3 33.5 59.31 25.68 47.35 43.27 50.51 50.24</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<monogr>
		<title/>
		<author>
			<persName><surname>Dbpedia</surname></persName>
		</author>
		<idno>Mean 52.74 18.45 33.3 33.52 33.39 61.82 24.87 47.48 46.25 50.83 51.14</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">The entry at row i and column j denotes the average performance when the model is trained on task i and evaluated on task j. Mean denotes the mean performance on all prompts</title>
	</analytic>
	<monogr>
		<title level="m">Experiments on T5-XL</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
	<note>and Med. denotes the median performance on all prompts</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
