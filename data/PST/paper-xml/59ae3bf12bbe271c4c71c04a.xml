<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SSGAN: Secure Steganography Based on Generative Adversarial Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Haichao</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Cyber Security</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jing</forename><surname>Dong</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Center for Research on Intelligent Perception and Computing</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Center for Research on Intelligent Perception and Computing</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yinlong</forename><surname>Qian</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Center for Research on Intelligent Perception and Computing</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Xiaoyu</forename><surname>Zhang</surname></persName>
							<email>zhangxiaoyu@iie.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SSGAN: Secure Steganography Based on Generative Adversarial Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4771551FBA534D84C0A58D49EAD99983</idno>
					<idno type="DOI">10.1007/978-3-319-77380-3_51</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T02:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Steganography</term>
					<term>Steganalysis</term>
					<term>Generative adversarial networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, a novel strategy of Secure Steganography based on Generative Adversarial Networks is proposed to generate suitable and secure covers for steganography. The proposed architecture has one generative network, and two discriminative networks. The generative network mainly evaluates the visual quality of the generated images for steganography, and the discriminative networks are utilized to assess their suitableness for information hiding. Different from the existing work which adopts Deep Convolutional Generative Adversarial Networks, we utilize another form of generative adversarial networks. By using this new form of generative adversarial networks, significant improvements are made on the convergence speed, the training stability and the image quality. Furthermore, a sophisticated steganalysis network is reconstructed for the discriminative network, and the network can better evaluate the performance of the generated images. Numerous experiments are conducted on the publicly available datasets to demonstrate the effectiveness and robustness of the proposed method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Steganography is the task of concealing a message within a medium such that the presence of the hidden message cannot be detected. It is one of the hot topics in information security and has drawn lots of attention in recent years. Steganography is often used in secret communications. Especially in the fast-growing social networks, there are an abundance of images and videos, which provide more opportunities and challenges for steganography. Therefore, the design of a secure steganography scheme is of critical importance.</p><p>How to design a secure steganography method is the problem that researchers have always been concerned about. Existing steganographic schemes usually require the prior of probability distribution on cover objects which is difficult to obtain in practice. Conventionally, the steganography method is designed in a heuristic way which does not take the steganalysis into account fully and automatically. For the sake of the steganography safety, we consider the steganalysis into the design of steganography.</p><p>At present, the image-based steganography algorithm is mainly divided into two categories. The one is based on the spatial domain, the other is based on the DCT domain. In our work, we focus on the spatial domain steganography.</p><p>Least Significant Bit (LSB) <ref type="bibr" target="#b10">[11]</ref> is one of the most popular embedding methods in spatial domain steganography. If LSB is adopted as the steganography method, the statistical features of the image are destroyed. And it is easy to detect by the steganalyzer. For convenience and simple implementation, the LSB algorithm hides the secret to the least significant bits in the given image's channel of each pixel. Mostly, the modification of the LSB algorithm is called ±1-embedding <ref type="bibr" target="#b1">[2]</ref>. It randomly adds or subtracts 1 from the channel pixel, so the last bits would match the ones needed. So we consider the ±1-embedding algorithm in this paper.</p><p>Besides the LSB algorithm, some sophisticated steganographic schemes use a distortion function which is used for selecting the embedding localization of the image. We called them the image content-adaptive steganography. These algorithms are the most popular and the most secure image steganography in spatial domain, such as HUGO (Highly Undetectable steGO), WOW (Wavelet Obtained Weights), S-UNIWARD, etc.</p><p>HUGO <ref type="bibr" target="#b11">[12]</ref> is a steganographic scheme that defines a distortion function domain by assigning costs to pixels based on the effect of embedding some information within a pixel. It uses a weighted norm function to represent the feature space. HUGO is considered to be one of the most secure steganographic techniques, which we will use in this paper to demonstrate our method's security. WOW (Wavelet Obtained Weights) <ref type="bibr" target="#b7">[8]</ref> is another content-adaptive steganographic method that embeds information into a cover image according to textural complexity of regions. In WOW shows that the more complex the image region is, the more pixel values will be modified in this region. S-UNIWARD <ref type="bibr" target="#b8">[9]</ref> introduces a universal distortion function that is independent of the embedded domain. Despite the diverse implementation details, the ultimate goals are identical, i.e. they are all devoted to minimize this distortion function, to embed the information into the noise area or complex texture, and to avoid the smooth image coverage area.</p><p>So far as we know, when people design steganography algorithm, they usually heuristically consider the steganalysis side. For example, the message should embed into the noise and texture region of image which is more secure. In this paper, we propose a novel SSGAN algorithm, which implements secure steganography based on the generative adversarial networks. We consider it under adversarial learning framework, inspired by the work of Denis and Burnaev <ref type="bibr" target="#b4">[5]</ref>. We use WGAN <ref type="bibr" target="#b2">[3]</ref> to improve the security of steganography by generating more suitable covers. In the proposed SSGAN, covers are generated firstly using the generative network. Then we adapt the state-of-the-art embedding algorithm, like HUGO, to embed message into the generated image. Finally, we use the GNCNN <ref type="bibr" target="#b16">[17]</ref> to detect on images whether there is a steganographic operation.</p><p>The contributions of our work can be concluded as follows:</p><p>Perceptibility. In this paper, we use WGAN instead of DCGAN to generate cover images to achieve generative images with higher visual quality and ensure faster training process.</p><p>Security. We use a more sophisticated network called GNCNN to assess the suitableness of the generated images instead of the steganalysis network proposed by <ref type="bibr" target="#b4">[5]</ref>.</p><p>Diversity. We also use GNCNN to compete against the generative network, which can make the generated images more suitable for embedding.</p><p>The rest of the paper is structured as follows: In Sect. 2, we discuss the related work of adversarial learning and elaborate the proposed method. In Sect. 3, experiments are conducted to demonstrate the effectiveness and security of the proposed method. In Sect. 4, we draw conclusions.</p><p>2 Secure Steganography Based on Generative Adversarial Networks</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Adversarial Learning</head><p>Adversarial learning using game theory, and is combined with unsupervised way to jointly train the model. The independent model is trained to compete with each other, iteratively improving the output of each model. In Generative Adversarial Networks, the generative model G tries to train the noise to samples, while the discriminative model D tries to distinguish between the samples output by G and the real samples.</p><p>Based on fooling the D, the weight of G is updated, and at the same time, the D's weight is updated by distinguishing between the fake and real samples. Recent years, GANs have been successfully applied to image generation tasks [14] using convolutional neural networks (CNNs) for both G and D. But in traditional GANs, they are considered difficult to train. Because there is no obvious relationship between the convergence of the loss function and the sample quality. Typically, people choose to stop the training by visually checking the generated samples. So, a design arises recently, WGAN <ref type="bibr" target="#b2">[3]</ref>, using the Wasserstein distance instead of the Jensen-Shannon divergence, to make the data set distribution compared with the learning distribution from G. Obviously, they show that the sample quality is closely related with the network's convergence and the training rate is really improved.</p><p>Adversarial training has also been applied to steganography. The adversarial training process can be described as a minimax game:</p><formula xml:id="formula_0">min G max D J D; G ð Þ¼E x $ p data x ð Þ log D x ð Þ ð ÞþE z $ p noise z ð Þ log 1 À D G z ð Þ ð Þ ð Þ ð<label>1Þ</label></formula><p>where D(x) represents the probability that x is a real image rather than synthetic, and G (z) is a synthetic image for input noise z. In this process, there are two networks, the G and the D, trained simultaneously:</p><p>• Generative Network, its input is a noise z from the prior distribution p noise z ð Þ, and transform it from the data distribution p data x ð Þ, to generate a data sample which is similar to p data x ð Þ. • Discriminative Network, its input are the real data and the fake data generated from the Generative Network, and determine the difference between the real and fake data samples.</p><p>To solve the minimax problem, in each iteration of the mini-batch stochastic gradient optimization, we first perform the gradient ascent step on D and then perform the gradient descent step on G. So we let x N represents the neural network N, then we can see the optimization step:</p><formula xml:id="formula_1">• We let the D fixed to update the model G by x G x G À c G r G J where r G J ¼ @ @x G E z $ p noise z ð Þ log 1 À D G z; x G ð Þ; x D ð Þ ð Þ ð<label>2Þ</label></formula><p>• We let the G fixed to update the model D by</p><formula xml:id="formula_2">x D x D þ c D r D J where r D J ¼ @ @x D E x $ p data x ð Þ log D x; x D ð Þ ð ÞþE z $ p noise z ð Þ log 1 À D G z; x G ð Þ; x D ð Þ ð Þ È É<label>ð3Þ</label></formula><p>In this paper, we use WGANs to verify the advantages of generating and discriminating the image using adversarial training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Model Design</head><p>We introduce a model that we called SSGAN, which contains a generative network, and two discriminative networks. The model can be described as Fig. <ref type="figure">1:</ref> Since we want the G to generate realistic images that could be used as secure covers for steganography, we force G to compete against the D and S at the same time. We use S x ð Þ to represent the output of the steganalysis network, then the game can be shown as follows:</p><formula xml:id="formula_3">min G max D max S J ¼ a E x $ p data x ð Þ log D x ð Þ ð ÞþE z $ p noise z ð Þ log 1 À D G z ð Þ ð Þ ð Þ À Á þ 1 À a ð ÞE z $ p noise z ð Þ logS Stego G z ð Þ ð Þ ð Þ þlog 1 À S G z ð Þ ð Þ ð Þ ½<label>ð4Þ</label></formula><p>To control the trade-off between the realistic of the generated images and the evaluation of the steganalysis, we use a convex combination which includes the D and S network with parameters a 2 0; 1 ½ . And we show that when we give a 0:7, the results are closer to the noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Generator G</head><p>For the generator G, it is used to generate the secure covers. And we use a fully connected layer, and four fractionally-strided convolution layers, and then a Hyperbolic tangent function layer. This network structure can be described as Fig. <ref type="figure">2</ref>:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Discriminator D</head><p>For the discriminator D, it is used to evaluate the visual quality of the generated images. And we use four convolutional layers, and then a fully connected layer. This network structure can be described as Fig. <ref type="figure">3</ref>: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Discriminator S</head><p>For the S network, it is used to assess the suitableness of the generated images. And we first use a predefined high-pass filter to make a filtering operation, which is mainly for steganalysis. And then four convolutional layers. Finally we use a classification layer, which includes several fully connected layers. This network structure can be described as Fig. <ref type="figure">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.4">Update Rules</head><p>And we can show that the SGD update rules: • For the discriminator S:</p><formula xml:id="formula_4">• For the generator G: x G x G À c G r G J it is calculated by r G J ¼ @ @x G E z $ p noise z ð Þ log 1 À D G z; x G ð Þ; x D ð Þ ð Þ ½ þ @ @x G 1 À a ð ÞE z $ p noise z ð Þ log S Stego G z; x G ð Þ; x S ð Þ ð Þ ð Þ ½ þ @ @x G 1 À a ð ÞE z $ p noise z ð Þ log 1 À S G z; x G ð Þ; x S ð Þ ð Þ ½<label>ð5Þ</label></formula><formula xml:id="formula_5">• For the discriminator D: x D x D þ c D r G J it is calculated by r G J ¼ @ @x D E z $ pdata x ð Þ logD x; x D ð Þ ½ þ E z $ pnoise z ð Þ log 1 À D G z; x G ð Þ; x D ð Þ ð Þ ½ È É<label>ð6Þ</label></formula><formula xml:id="formula_6">x S x S þ c S r S J it is calculated by r S J ¼ @ @x S E z $ p noise z ð Þ logS Stego G z; x G ð Þ ð Þ ; x S ð Þ þ log 1 À S G z; x G ð Þ; x S ð Þ ð Þ ½<label>ð7Þ</label></formula><p>We update G to not only maximize the errors of D, but the normalization errors of D and S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Preparation</head><p>All experiments are performed in TensorFlow <ref type="bibr" target="#b0">[1]</ref>, on a workstation with a Titan X GPU. In our experiments, we use the CelebA dataset which contains more than 200,000 images.</p><p>We pre-process the image, and all images are cropped to 64 Â 64 pixels.</p><p>For the purpose of the steganalysis, we use 90% of the data to construct a training set, and regard the rest as testing set. The training set is denoted by TRAIN, and the testing set is denoted by TEST. We use Stego(x) to represent the steganographic algorithm used to hide information. Two datasets are involved in the experiments. One is TRAIN + Stego(TRAIN), where Stego(TRAIN) is the training set embedded in some secret information, and the other is TEST + Stego(TEST). Finally, we got 380,000 images for steganography training, 20,000 for testing. In order to train the model, we use 200,000 cropped images to generate images. After seven epochs, the images generated by our generative model and the model proposed in <ref type="bibr" target="#b4">[5]</ref>, denoted by SGAN, are shown in the Fig. <ref type="figure" target="#fig_2">5</ref>. Meanwhile, we use the LSB Matching algorithm which is ±1-embedding algorithm with a payload size to 0.4 bits per pixel to embed information, which we use a text from casual articles.</p><p>The experimental results are as follows, the visual quality of images generated by our SSGAN model are higher than its counterpart.</p><p>Experimental results show that through the use of WGAN, the convergence speed is faster than DCGAN, and the effect is more obvious, as is shown in Table <ref type="table" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental Setup</head><p>The generative network G is designed with one fully connected layer, four Fractionally À Strided Conv2D ! Batch Normalization ! Leaky ReLU layers, and one Hyperbolic tangent function layer. The discriminative network D comprises four Conv2D ! Batch Normalization ! Leaky ReLU layers, one fully connected layer. As for the steganalyser network S, we use a predefined high-pass filter to make a filtering operation, which is kept fixed while training, and four convolutional layers corresponding to three kinds of operations, i.e. convolution, non-linearity, and pooling, followed by a fully connected layer and a softmax layer for classification.</p><p>We train the model using RMSProp optimization algorithm with the learning rate 2 Â 10 À4 and update parameters b 1 ¼ 0:5 and b 2 ¼ 0:99. We update the weights of D and S once, while update weights of G twice in each mini-batch. In steganalysis, we call S the Steganalyser. In addition to the S network, we also use an independent network which is called S* <ref type="bibr" target="#b16">[17]</ref>. To make the network S effective, we also use the filter in <ref type="bibr" target="#b16">[17]</ref>, which defines as follows:</p><formula xml:id="formula_7">F 0 ð Þ ¼ À1 2 À2 2 À1 2 À6 8 À6 2 À2 8 À12 8 À2 2 À6 8 À6 2 À1 2 À2 2 À1 0 B B B B @ 1 C C C C A</formula><p>We also train the S* network using RMSProp optimization algorithm with the learning rate 5 Â 10 À6 and update parameters b 1 ¼ 0:9 and b 2 ¼ 0:99. And we use the format of binary cross-entropy to calculate the loss.</p><p>We conduct experiments on real images and generative images respectively, and the results are as follows:</p><p>In our first experiment, we compare the security of the generated images with that of real images. We embed secret message into real images and use the steganalysis network on real images first, then we embed secret message into generative images and use the steganalysis network on generative images.</p><p>We train the S network on real images and from the results we conclude that even if the usual WGAN can generate a synthetic image, it can be easily detected by a steganalyser. And our method increase the results of the classification error rates, which means the images generated by our model is more secure to be the carrier of the steganography images (Table <ref type="table" target="#tab_1">2</ref>).</p><p>We also use HUGO steganography scheme on real images and generative images. In our second experiment, we investigate the security of generated images under different seed values. We conduct the experiment on generative images generated by different setups.</p><p>In this group of experiments, we use Qian's network <ref type="bibr" target="#b16">[17]</ref> which is called steganalyser S* on generated images generated by SSGAN. The input is the prior noise distribution p noise z ð Þ for some fixed seed value. We test the S* on images. The experimental setups are as follows: S1. We use same seed value; S2. We use some randomly selected seed value. S3. We use the same seed value, as in S2, and we additionally tune the WGAN model for several epochs.</p><p>As is shown in Table <ref type="table" target="#tab_2">3</ref>, we can see that through using different seed values when generating images, can make it easier to deceive the steganalysis network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Discussion</head><p>As demonstrated in the experiments, the method proposed in <ref type="bibr" target="#b4">[5]</ref> has some limitations, the experiments show that the steganography is not secure enough. The experiments show that the steganalysis network in Denis's article is suitable for embedding with the same key, but when using the random key, their network is not so useful. So we use GNCNN <ref type="bibr" target="#b16">[17]</ref>. And we have confirmed that the steganalysis network they used was not so useful. Our results show that on the one hand, the generated images are more difficult to detect, indicating that the security performance is higher. On the other hand, the generated images' visual quality is better and more realistic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion and Future Work</head><p>In this paper, we introduce generative adversarial networks for steganography to generate more suitable and secure covers for steganography. Based on the WGANs, we have proposed a model called SSGAN for steganography. The proposed model is efficient to generate images, which have higher visual quality. And our model is suitable for embedding with the random key. Mostly, it can generate more secure covers for steganography. We have evaluated the performance of our model using CelebA datasets. Results show the effectiveness of the SSGAN model through the classification accuracy, and we think it could be used for adaptive steganographic algorithm for social network in the future. We believe that, by exploring more steganography properties, better performance can be achieved.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .Fig. 1 .</head><label>21</label><figDesc>Fig. 2. The generative network structure</figDesc><graphic coords="5,42.12,345.26,341.01,121.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .Fig. 4 .</head><label>34</label><figDesc>Fig. 3. The discriminative network structure</figDesc><graphic coords="6,56.07,59.53,341.46,101.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Examples of images, generated by SSGAN and SGAN after training for 7 epochs on the CelebA dataset, the left is generated by SSGAN, the right is generated by SGAN.</figDesc><graphic coords="7,64.97,424.91,295.31,146.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The contrast of two methods' time for running for seven epochs</figDesc><table><row><cell>Method Time (mins)</cell></row><row><cell>SSGAN 227.5</cell></row><row><cell>SGAN 240.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Accuracy of the steganalyser trained on real images</figDesc><table><row><cell>Type of images</cell><cell cols="2">SSGANs SGANs</cell></row><row><cell>Real images</cell><cell>0.87</cell><cell>0.92</cell></row><row><cell cols="2">Generated images 0.72</cell><cell>0.90</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Trained on generated images according to experimental conditions S1-S3</figDesc><table><row><cell cols="2">Experimental conditions Accuracy</cell></row><row><cell>S1</cell><cell>0.87</cell></row><row><cell>S2</cell><cell>0.72</cell></row><row><cell>S3</cell><cell>0.71</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. This work is supported by the National Natural Science Foundation of China (No. 61501457, U1536120, U1636201, 61502496) and the National Key Research and Development Program of China (No. 2016YFB1001003).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<title level="m">TensorFlow: large-scale machine learning on heterogeneous distributed systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>a</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Resampling and the detection of LSB matching in color bitmaps</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Ker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eletronic Imaging</title>
		<imprint>
			<publisher>International Society for Optics and Photonics</publisher>
			<date type="published" when="2005">2005. 2005</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-01">January 2017</date>
			<publisher>Wasserstein GAN</publisher>
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for image processing: an application in robot vision</title>
		<author>
			<persName><forename type="first">M</forename><surname>Browne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Ghidary</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-540-24581-0_55</idno>
		<ptr target="https://doi.org/10.1007/978-3-540-24581-0_55" />
	</analytic>
	<monogr>
		<title level="m">AI 2003. LNCS (LNAI)</title>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Gedeon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">C C</forename><surname>Fung</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">2903</biblScope>
			<biblScope unit="page" from="641" to="652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generative adversarial networks for image steganography</title>
		<author>
			<persName><forename type="first">D</forename><surname>Volkhonskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Borisenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Burnaev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2016 Open Review</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Detecting LSB steganography in color, and gray-scale images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fridrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Goljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Multimedia</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="22" to="28" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Rich models for steganalysis of digital images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fridrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kodovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Forensics Secur</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="868" to="882" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Designing steganographic distortion using directional filters</title>
		<author>
			<persName><forename type="first">V</forename><surname>Holub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fridrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE International Workshop on Information Forensics and Security (WIFS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="234" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Universal distortion function for steganography in an arbitrary domain</title>
		<author>
			<persName><forename type="first">V</forename><surname>Holub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fridrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Denemark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP J. Inf. Secur</title>
		<imprint>
			<biblScope unit="volume">2014</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Generative adversarial nets</publisher>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">LSB matching revisited</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mielikainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="285" to="287" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Using high-dimensional image models to perform highly undetectable steganography</title>
		<author>
			<persName><forename type="first">T</forename><surname>Pevný</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Filler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bas</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-16435-4_13</idno>
		<ptr target="https://doi.org/10.1007/978-3-642-16435-4_13" />
	</analytic>
	<monogr>
		<title level="m">IH 2010</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Böhme</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">W L</forename><surname>Fong</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Safavi-Naini</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">6387</biblScope>
			<biblScope unit="page" from="161" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised learning of invariant feature hierarchies with applications to object recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-L</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno>CoRR, abs/1511.06434</idno>
		<ptr target="http://arxiv.org/abs/1511.06434" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Textural features for steganalysis</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sutthiwan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-36373-3_5</idno>
		<ptr target="https://doi.org/10.1007/978-3-642-36373-3_5" />
	</analytic>
	<monogr>
		<title level="m">IH 2012</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Kirchner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Ghosal</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">7692</biblScope>
			<biblScope unit="page" from="63" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.05396</idno>
		<title level="m">Generative adversarial text to image synthesis</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Deep learning for steganalysis via convolutional neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>International Society for Optics and Photonics</publisher>
			<biblScope unit="page">94090</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
