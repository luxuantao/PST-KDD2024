<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Scalar Quantization for Large Scale Image Search</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
							<email>wengang.zhou@utsa.edu</email>
						</author>
						<author>
							<persName><forename type="first">Yijuan</forename><surname>Lu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Houqiang</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Qi</forename><surname>Tian</surname></persName>
							<email>qitian@cs.utsa.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">University of Texas</orgName>
								<address>
									<addrLine>San Antonio 1</addrLine>
									<postCode>78249</postCode>
									<settlement>Texas</settlement>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">Texas State University 2</orgName>
								<address>
									<postCode>78666</postCode>
									<settlement>Texas</settlement>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Dept. of EEIS</orgName>
								<orgName type="institution">University of Science and Technology of China 3</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Scalar Quantization for Large Scale Image Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D35A57886B9E35657415BA5AC4646C35</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T07:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>I.2.10 [Vision and Scene Understanding]: VISION Algorithms</term>
					<term>Experimentation</term>
					<term>Verification Large-scale image retrieval</term>
					<term>scalar quantization</term>
					<term>SIFT</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Bag-of-Words (BoW) model based on SIFT has been widely used in large scale image retrieval applications. Feature quantization plays a crucial role in BoW model, which generates visual words from the high dimensional SIFT features, so as to adapt to the inverted file structure for indexing. Traditional feature quantization approaches suffer several problems: 1) high computational cost-visual words generation (codebook construction) is time consuming especially with large amount of features; 2) limited reliability-different collections of images may produce totally different codebooks and quantization error is hard to be controlled; 3) update inefficiency-once the codebook is constructed, it is not easy to be updated. In this paper, a novel feature quantization algorithm, scalar quantization, is proposed. With scalar quantization, a SIFT feature is quantized to a descriptive and discriminative bit-vector, of which the first tens of bits are taken out as code word. Our quantizer is independent of collections of images. In addition, the result of scalar quantization naturally lends itself to adapt to the classic inverted file structure for image indexing. Moreover, the quantization error can be flexibly reduced and controlled by efficiently enumerating nearest neighbors of code words.</p><p>The performance of scalar quantization has been evaluated in partial-duplicate Web image search on a database of one million images. Experiments reveal that the proposed scalar quantization achieves a relatively 42% improvement in mean average precision over the baseline (hierarchical visual vocabulary tree approach), and also outperforms the state-of-the-art Hamming Embedding approach and soft assignment method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The last decade has witnessed the great advance in content-based image retrieval on large-scale database. Most state-of-the-art approaches utilize SIFT features <ref type="bibr" target="#b1">[1]</ref> to represent images and leverage the BoW model <ref type="bibr" target="#b2">[2]</ref> to index large-scale image dataset for scalable retrieval. Some post-processing techniques, such as spatial verification <ref type="bibr" target="#b3">[3]</ref> [4] and query expansion <ref type="bibr" target="#b5">[5]</ref>  <ref type="bibr" target="#b24">[24]</ref>, are also explored to further boost the retrieval accuracy. Of them, one of the key steps is feature quantization, which first generates visual words from the high dimensional SIFT features, and then quantize features to the corresponding visual words for indexing.</p><p>The most popular feature quantization method is vector quantization. Originally used in lossy data compression, vector quantization divides a large set of training SIFT features into groups. Each group corresponds to a sub-space in the feature space, and is represented by its center, which is called visual word <ref type="bibr" target="#b2">[2]</ref>. All visual words constitute a visual codebook. Then, given a novel feature, vector quantization assigns it the visual word ID of the sub-space where the feature falls in. The most popular visual codebook generation approach is k-means <ref type="bibr" target="#b2">[2]</ref> clustering. When the visual codebook size becomes very large (e.g. 1 million), it is infeasible to train the codebook with k-means, and hierarchical kmeans <ref type="bibr" target="#b6">[6]</ref> is more preferred to improve codebook generation speed and enhance feature quantization efficiency.</p><p>Traditional vector quantization suffers several problems. 1) High computational cost: visual codebook generation is computationally expensive especially with a large amount of features. For example, in order to train a large visual codebook containing 1 million visual words, usually about 50 million SIFT features may need, considering both feature coverage and affordable memory size. However, for the SIFT descriptor space with as large as 128 dimensions, it is still unknown whether 50 million SIFT features are enough to capture the feature distribution. Even if the memory would afford several orders of magnitude more training features, it would take intolerable time cost to finish the clustering for codebook generation. 2) Limited reliability: codebook construction in vector quantization relies on the collection of image features and codebook generation methods. Different collections of image features may produce totally different codebooks. Even with the same collection of images and the same clustering methods, generated codebook may be still different due to the variability of k-means. Therefore, quantization error is hard to be controlled. 3) Update inefficiency: with many new features collected, the codebook/quantizer should be updated accordingly. However, the codebook updating needs lots of effort. The huge amount of features have to be re-clustered, which is computationally inefficient.</p><p>To address the above problems, in this paper, a novel quantization strategy, scalar quantization, is proposed. Distinguished from the traditional vector quantization methods, the proposed scalar quantization approach does not involve any form of visual codebook training or clustering. Instead, it transforms each feature to a bit-vector with a quantizer, which is independent of collections of image features. Our quantization operation is very simple and requires low computational cost. The bit-vector generated by scalar quantization achieves more compact representation of the original SIFT descriptors, but still keeps the discriminative power of SIFT feature. Since our quantization method is independent of collections of images, even with new collected features, there is no need to update our quantizer.</p><p>Moreover, scalar quantization can index features to the classic inverted file structure easily by extracting the first tens of bits from the quantized bit-vector to generate code word. And the remaining bits of the quantized bit-vector are stored in the inverted file list for matching verification. Furthermore, a novel soft quantization strategy is applied in scalar quantization to address the quantization loss by enumerating the nearest neighbors of the code word. Consequently, more candidate matches are included for matching verification, which greatly boost the retrieval accuracy in large-scale image database.</p><p>In this paper, we only focus on the feature quantization step. To further improve the image retrieval performance, our approach can also be flexibly integrated with many other algorithms, such as weak geometric consistency <ref type="bibr" target="#b15">[15]</ref>, fast spatial matching <ref type="bibr" target="#b3">[3]</ref>, geometric verification <ref type="bibr" target="#b4">[4]</ref>  <ref type="bibr" target="#b17">[17]</ref>, and query expansion <ref type="bibr" target="#b5">[5]</ref>, etc.</p><p>To summarize, the main contributions of this paper lie in three aspects: <ref type="bibr" target="#b1">(1)</ref> We propose a new scalar quantization method to quantize the SIFT descriptor to a compact bit-vector, with the discriminative power kept. No visual codebook is needed to be trained in our quantization scheme.</p><p>(2) We adapt the quantized bit-vectors to the popular inverted index file structure for scalable image search.</p><p>(3) We propose a soft quantization scheme based on our indexing structure to reduce the quantization error.</p><p>The rest of the paper is organized as follows. Section 2 reviews related work in large-scale image search. Section 3 discusses the proposed algorithm in details. Experimental results are given in Section 4. Section 5 makes discussions on three issues. Finally, the conclusion is provided in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>In large-scale content-based image search applications, Bag-of-Words (BoW) model based on local features has been widely adopted. Generally, in those BoW-based approaches, there are four major key components: local feature representation, feature quantization, index strategy, and post-processing. In this section, we make a review of related work in each component.</p><p>Local Feature Representation Extraction of local feature usually involves two steps, i.e. interest point detection and feature description. The detected interest points are expected to have high repeatability over various changes. Popular detectors include Difference of Gaussian (DoG) <ref type="bibr" target="#b1">[1]</ref>, MSER <ref type="bibr" target="#b7">[7]</ref>, and Hessian affine <ref type="bibr">[8]</ref>. After interest point detection, a descriptor is extracted to represent the visual appearance of the local region centered at the interest point. Usually, the descriptor should be invariant to rotation and scale, and also robust to affine distortion, addition of noise, and illumination changes, etc. The most popular choice with the above merits is SFIT feature <ref type="bibr" target="#b1">[1]</ref>. As a variation, SURF <ref type="bibr" target="#b9">[9]</ref> demonstrates good performance but achieves better efficiency.</p><p>Recently, a binary feature BRIEF <ref type="bibr" target="#b10">[10]</ref> and its variation ORB <ref type="bibr" target="#b11">[11]</ref> have been proposed and attracted lots of attention.</p><p>Feature Quantization Usually, several hundred or thousand local features are extracted from a single image. To achieve a compact representation, high-dimensional local features are quantized to visual words, and an image can be represented as a "bag" of visual words. Therefore, a visual codebook containing visual words needs to be generated first. The most intuitive visual codebook generation method is k-means <ref type="bibr" target="#b2">[2]</ref> or hierarchical kmeans <ref type="bibr" target="#b6">[6]</ref> for large size visual codebook generation.</p><p>With visual codebook defined, feature quantization is to assign a visual word ID to each feature. The most naive choice is finding the closest (the most similar) visual word of a given feature by linear scan, which, however, suffers expensive computational cost. Usually, approximate nearest neighbor (ANN) search methods are adopted to speed up the searching process, with sacrifice of accuracy to some extent. In <ref type="bibr" target="#b1">[1]</ref>, a k-d tree <ref type="bibr" target="#b21">[21]</ref> is utilized with a best-bin-first modification to find approximate nearest neighbors to the descriptor vector of the query. In <ref type="bibr" target="#b6">[6]</ref>, based on hierarchical vocabulary tree, an efficient approximate nearest neighbor search is achieved by propagating the query feature vector from the root node down the tree by comparing the corresponding child nodes and choosing the closest one. In <ref type="bibr" target="#b12">[12]</ref>, a k-d forest approximation algorithm is proposed with reduced time complexity. To reduce the quantization loss, a descriptordependent soft assignment scheme <ref type="bibr" target="#b13">[13]</ref> is proposed to map a feature vector to a weighted combination of several visual words.</p><p>In <ref type="bibr" target="#b14">[14]</ref>, the high dimensional SIFT descriptor space is partitioned into regular lattices. Although demonstrated to work well in image classification, in <ref type="bibr" target="#b13">[13]</ref>, regular lattice quantization is reported working significant worse than <ref type="bibr" target="#b6">[6]</ref>[13] in large-scale image search.</p><p>In <ref type="bibr" target="#b27">[27]</ref>, a novel scheme is proposed to jointly optimize the dimension reduction and indexing. In <ref type="bibr" target="#b28">[28]</ref>, a compact image signature, called Residual Enhanced Visual Vector, is designed via quantization residue aggregation and classification-aware dimensionality reduction. In <ref type="bibr" target="#b29">[29]</ref> and <ref type="bibr" target="#b30">[30]</ref>, descriptive and contextual visual vocabularies are generated respectively for large-scale image applications, such as image search.</p><p>In <ref type="bibr" target="#b15">[15]</ref>, for each feature quantized to a visual word, feature dimension is further performed and a binary signature is generated with a pre-trained median vector. Such binary signature will be used for feature matching verification in on-line retrieval. In <ref type="bibr" target="#b26">[26]</ref>, a variation of Hamming Embedding <ref type="bibr" target="#b15">[15]</ref>, i.e., the Asymmetric Hamming Embedding scheme, is proposed to better exploit the information conveyed by the binary signature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Index Strategy</head><p>Inspired by the success of text search engines, inverted file structure <ref type="bibr" target="#b23">[23]</ref> has been successfully used for large- <ref type="bibr" target="#b16">[16]</ref>. In essence, inverted file structure is a compact representation of a sparse matrix, whose row and column denote visual word and image, respectively. In on-line retrieval, only those images sharing common visual words with the query image need to be checked. Therefore, the number of candidate images to be compared is greatly reduced, achieving efficient response.</p><formula xml:id="formula_0">scale image search [2][3][4][5][6][13][15]</formula><p>In inverted file structure, each visual word is followed by an inverted file list of entries. Each entry stores the ID of image where the visual word appears, and some other clues for verification or similarity measurement. For instance, Hamming Embedding <ref type="bibr" target="#b15">[15]</ref> generates a 64-bit Hamming code for each feature to verify descriptor matching. Bundled Feature <ref type="bibr" target="#b16">[16]</ref> stores the x-order and y-order of each SIFT feature located in the bundled area. The geometric clues, such as feature position, scale, and orientation, are also stored in inverted file list for verification of geometric consistency <ref type="bibr" target="#b3">[3]</ref>[4] <ref type="bibr" target="#b15">[15]</ref>[16] <ref type="bibr" target="#b17">[17]</ref>.</p><p>To further reduce the memory cost of inverted file structure, a visual word vector is mapped to a low-dimensional representation by a group of min-hash functions <ref type="bibr">[18] [19]</ref>. Consequently, only a small constant amount of data per image needs to be stored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Post Processing</head><p>The initially returned result list can be further refined by exploring the spatial context or enhancing the original query. Spatial verification <ref type="bibr">[3] [4]</ref> [15] <ref type="bibr" target="#b17">[17]</ref> [19] and query expansion <ref type="bibr">[5] [24]</ref> are two of the most successful post-processing techniques to boost the accuracy of large-scale image search.</p><p>Spatial context is an important clue to remove false positive visual matches. Lots of work has been done on spatial verification. In <ref type="bibr" target="#b2">[2]</ref>, locally spatial consistency is imposed to filter visual-word matches with low support. In <ref type="bibr" target="#b15">[15]</ref>, weak geometric consistency on scale and orientation is imposed to quickly filter potential false matches. In <ref type="bibr" target="#b3">[3]</ref>, global spatial verification is performed based on a variation of RANSAC <ref type="bibr" target="#b20">[20]</ref>. An affine model is estimated to filter local matches that fail to fit the model. In <ref type="bibr" target="#b4">[4]</ref> [17], the geometric context among local features is encoded into binary maps. And then it recursively removes geometrically inconsistent matches by analyzing those coding maps.</p><p>Query expansion, leveraged from text retrieval, reissues the initial highly-ranked results to generate new queries. Some relevant features, which are not present in the original query, can be used to enrich the original query to further improve the recall performance. Several strategies, such as average query expansion, transitive closure expansion, recursive expansion, intra-expansion, and inter-expansion, etc. have been discussed in <ref type="bibr" target="#b5">[5]</ref> [24].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">METHOD</head><p>In this paper, we focus on feature quantization, which plays a key role of BoW model. We first introduce our scalar quantization strategy in Section 3.1. Then, in Section 3.2, we discuss how to adapt the scalar quantization result to the classic inverted file structure for scalable image search. In Section 3.3, we discuss a soft quantization scheme to further reduce the quantization error in on-line query stage. Finally, a summary of our quantization algorithm is given in Section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Scalar Quantization</head><p>High dimensional SIFT descriptors (L 2 -normalized 128-D vectors <ref type="bibr" target="#b1">[1]</ref>) are extracted from images for discrimination. Each dimension of the descriptor vector corresponds to a bin of concatenated orientation histograms. Generally, similar SIFT features have relatively smaller distances than different features. Features from the same source, e.g. image patch, may not be exactly same due to image noise. But their values on the 128 bins usually share some common patterns, e.g., the pair-wise differences between most of bins are similar and stable. Therefore, it can be easily extended that the differences between bins and a predefined threshold are stable for most bins. Based on such observation, we propose a scalar quantization strategy.</p><p>Given a high dimensional feature vector</p><formula xml:id="formula_1">d T d R f f f f   ) , , ,<label>( 2 1</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head></head><p>, where</p><formula xml:id="formula_2">) , , 2 , 1 ( , d i R f i   </formula><p>, and d denotes the feature dimension size. For SIFT descriptor,</p><formula xml:id="formula_3">128  d . We define a quantization function ) ( q to transform f to a bit vector T d b b b b ) , , ,<label>( 2 1</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head></head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head></head><p>, as follows:</p><formula xml:id="formula_4">        f f f f b i i i ˆ if 0 ˆ if 1 ) , , 2 , 1 ( d i   (1)</formula><p>where f ˆ is a threshold determined by vector f . The threshold f ˆ is an important parameter, which determines the discriminative power of the quantization results. If the discriminative power of SIFT is well kept in scalar quantization, the Hamming distance between scalar vectors b should be consistent with the L 2 distance between original feature vectors f. There are many methods to choose the threshold f ˆ. In this paper, we choose f ˆ as the median value of vector f . The philosophy behind it is that, the median value is relatively stable to changes in some bins of a long vector. The quantization function ) ( q is a kind of hashing. Unlike classic LSH methods involving many hashing tables and functions <ref type="bibr">[8]</ref> [19], our scheme needs only one hash function and therefore is much simpler and more efficient.</p><p>With each high dimension feature quantized to a bit-stream vector, the feature comparison is transformed to the comparison of binary vectors, which can be efficiently accomplished by exclusive-OR operation and measured by Hamming distance. SIFT descriptor pairs, which include every SIFT pair extracted from image pairs randomly sampled from a large image dataset. For each descriptor pair, its L 2 distance before scalar quantization and Hamming distance after scalar quantization are calculated. As shown in Fig. <ref type="figure" target="#fig_11">1</ref>(a), the distribution of Hamming distances between these descriptors exhibits a Gaussian-like distribution. From Fig. <ref type="figure" target="#fig_11">1</ref> (b) and Fig. <ref type="figure" target="#fig_11">1</ref>(c), it is observed that the Hamming distance between our quantized bitvectors is consistent with the average L 2 -distance, with relatively small standard deviation (computed on the unit-normalized descriptors). To further reduce the deviation, we use a variation of Eq. ( <ref type="formula">1</ref>) and transform the descriptor vector to a 256-bit vector, which will be discussed at the end of this section. It should be noted that our approach is different from the SIFT quantization methods proposed in lattice quantization <ref type="bibr" target="#b14">[14]</ref> and Hamming Embedding <ref type="bibr" target="#b15">[15]</ref>. In <ref type="bibr" target="#b14">[14]</ref>, the descriptor space is arbitrarily split along dimension axes into regular lattice. In <ref type="bibr" target="#b15">[15]</ref>, for each bin/dimension, a median value of all training features on that bin in the reduced dimensional space is computed for binarizing the corresponding dimension. Both two approaches ignore the unique property of every individual SIFT descriptor.</p><p>As shown in Fig. <ref type="figure" target="#fig_11">1</ref>, the original features' difference in Euclidean distance can be well captured by their Hamming distance after scalar quantization. Fig. <ref type="figure" target="#fig_1">2</ref> shows a real instance of local descriptor match across two images with scalar quantization. From Fig. <ref type="figure" target="#fig_1">2</ref>(b), it can be observed that these two SIFT descriptors have similar magnitude in the corresponding bins with some small variations before quantization. After scalar quantization, they differ from each other in six bins. With a proper threshold, it can be easily determined whether the local match is true or false just by the exclusive-OR (XOR) operation between the quantized bit-vectors. Obviously, the error in the exclusive-OR result is likely to occur in those bins with magnitude around the median value. Intuitively, the median threshold could be increased to some upper level, which can make the Hamming distance between similar SIFT descriptors smaller. However, such modification will also reduce the Hamming distance between irrelevant descriptors and cause false descriptor matches.  Another statistical study on the distribution of median value of SIFT descriptor is also performed. 100 million SIFT descriptors are sampled from a large dataset, and the median value of each 128-D descriptor vector is computed. As shown in Fig. <ref type="figure" target="#fig_2">3</ref>, the median value of most SIFT descriptors is relatively small, around 10, but the maximum magnitude in some bins still can reach more than 140. This may incur potential quantization loss since those bins with magnitude above the median are not well distinguished.</p><p>To address this issue, the same scalar quantization strategy could be conducted again on those bins with magnitude above the median. Intuitively, such operation can be performed recursively. However, it will cause additional storage cost. In our implementation, we only perform the scalar quantization twice, i.e., first on the whole 128 elements, and second on those elements with magnitude above the median value. Consequently, a SIFT descriptor</p><formula xml:id="formula_5">128 128 2 1 ) , , , ( R f f f f T    is quantized to a 256-bit vector T b b b b ) , , , ( ~256 2 1  </formula><p>, as follows: With scalar quantization by Eq. ( <ref type="formula" target="#formula_1">2</ref>), the comparison of SIFT descriptors in L 2 -distance is captured by the Hamming distance of the corresponding 256-bit vectors. Since our target is large-scale image search, how to adapt our scalar quantization result to the classic inverted file structure for scalable image search needs to be explored.</p><formula xml:id="formula_6">               1 2 1 2 128 ˆ if ) 0 , 0 ( ˆ if ) 0 , 1 ( ˆ if ) 1 , 1 ( , f f f f f f f b b i i i i i ) 128 , , 2 , 1 (   i (2) where 2 ˆ , 2 ˆ33 32 2 65 64 1 g g f g g f     , ) , , , (</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Indexing with Inverted File Structure</head><p>In image search, the problem of feature matching between images can be regarded as finding feature's nearest or approximatenearest neighbors. When the feature amount becomes very large, say, over one billion, it is too computationally expensive to find the nearest neighbors by linearly comparing all features' binary vectors. To address this problem, leveraged from text retrieval, inverted file structure can be used for scalable indexing of largescale image dataset.</p><p>In traditional inverted file structure for image search, a group of visual words are pre-trained. And each visual word is followed with an entry list of image features, which are quantized to this followed visual word. Each indexed feature in the list records its image ID and some other clues.</p><p>To adapt to the classic inverted file structure to index image features, we define code word 1 by the first t bits of the binary code generated by scalar quantization result. Then, the rest bits of features are recorded in the entry list of the corresponding code word. In fact, any other t bits of the binary code are expected to be equivalent. A toy example is shown in Fig. <ref type="figure" target="#fig_0">4</ref>. Intuitively, if a code word is represented with t bits, the total number of code 1 It should be noted that our code word is different from the traditional visual word <ref type="bibr">[2] [6]</ref>.</p><p>words could be amounted up to t 2 . However, it is found from experiments that, when t increases above 20, the amount of nonempty code words becomes much smaller than t 2 , as shown in Fig. <ref type="figure">5</ref>. For example, when t increases to 32, the total number of code words could be up to 9 32 10 4 2   (4 billion). However, the number of unique code words generated by scalar quantization (on one million image database) is even much less than 8  10 .</p><p>Generally, the more code words are generated, the shorter the average length of indexed feature list becomes, and the less the time cost is needed to query a new feature. However, in our method, we will introduce a soft quantization scheme (Section 3.3) to expand more code words for each query feature. And the number of expanded indexed feature lists is polynomial to t . To make a tradeoff, in our experiments, we select 32  t , and 46.5 million "code words" are obtained. Fig. <ref type="figure" target="#fig_6">6</ref> shows the distribution of code word occurrence on one million image database. It can be observed that, of the 46.5 million code words, only the top few thousand code words have very high frequency. Those code words are prevalent in many images, and their distinctive power is weak. As suggested by <ref type="bibr" target="#b2">[2]</ref>, we apply a stop-list to ignore those high frequency code words that occur in more than 0.11% of the total image dataset. Experiments reveal that a proper stop-list may not affect the search accuracy, but does avoid checking many code word lists and achieves gain in efficiency.  Once all features of an image dataset have been indexed with the inverted file structure, given a new SIFT descriptor, it will be first quantized to a 256-bit vector with scalar quantization. Then through the top 32 bits, the corresponding code word can be located. And only the indexed features following the matched code word will be checked. Therefore, the searching space is greatly reduced. Finally, the exclusive-OR operation is performed on the remained 224 bits of the query vector and those of indexed features recorded in the entry list of the matched code word. A threshold  on the Hamming distance between 256-bit vectors needs to be set for true-match judgment, such that those matches with Hamming distance no larger than  will be accepted as true matches. The impact of  will be studied in Section 4.1.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Reduction of Quantization Error</head><p>In Section 3.2, we define code word by the top 32 bits of the bit vector from scalar quantization. However, such simple processing will exclude some candidate features that have some flipping bits among the top 32 bits (e.g., 0 changes to 1) due to noise. To address this issue, we propose a soft strategy to reduce the quantization error. Assuming such flipping happens only to very few dimensions, features before and after the flipping should be still very similar, i.e., small Hamming distance. To identify these candidate features, it is desired to quickly enumerate all of its possible nearest neighbors within a predefined Hamming distance d , just by alternatively flipping some bits. This is equivalent to a tolerant expansion of the original code word. The impact of expansion-bit number d will be studied in Section 4.1.</p><p>As shown in the toy example in Fig. <ref type="figure" target="#fig_8">7</ref>, the code word of a new query feature is a bit-vector 100, i.e., CW4 in pink color. To identify all of candidate features, its possible nearest neighbors (e.g., Hamming distance d =1) will be obtained by flipping one bit in turn, which generates three additional code words (in green color): CW0 (000), CW5 (101) and CW6 (110). These code words are nearest neighbors of CW4 in the Hamming space. Then, besides CW4, the indexed feature lists of these three expanded code words will be also considered as candidate true matches, and all features in these expanded lists will be further compared on their rest bit-codes.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Algorithm Summary</head><p>Overall, the proposed scalar quantization consists of two stages: offline indexing and on-line querying. In this section, we summarize the general steps of these two stages in Fig. <ref type="figure" target="#fig_9">8</ref> and Fig. <ref type="figure" target="#fig_10">9</ref>, respectively. Given a query image, after looking up the index 2) Identify its code word ID q V by the first 32 bits of the 256-bit vector.</p><p>3) For each feature in the inverted image list linked to q V , compare its indexed 224-bit vector with the query feature. If the total Hamming distance in 256-bit is not greater than  , accept the indexed feature as true match.</p><p>4) Expand the q V to include its nearest code words</p><formula xml:id="formula_7">   , 2 , 1 ,  i V i q</formula><p>with Hamming distance no greater than d . 5) For each i q V , repeat step (3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Off-line Indexing with Scalar Quantization</head><p>1) Given a 128-D SIFT descriptor from an index image, convert it to a 256-bit vector by Eq. ( <ref type="formula" target="#formula_1">2</ref>);</p><p>2) Identify code word ID d V by the first 32 bits of the 256-bit vector.</p><p>3) In the inverted image list of d V , store both the ID of the image where the feature appears, and the remaining 224 bits of the quantized 256-bit vector.</p><p>file with the proposed on-line querying steps, we can obtain the matching results between the query image and target images. Finally, we formulate image retrieval as a voting problem and define the similarity between two images by the cardinality of matched feature set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head><p>Our basic dataset is built by crawling one million images from the Web. We take the partial-duplicate image dataset released in <ref type="bibr" target="#b17">[17]</ref> as the ground-truth dataset, which contains 1104 images from 33 groups, including "Mona Lisa", "KFC logo", "American Gothic Painting", "Seven-eleven logo", etc. To evaluate the performance with respect to the size of dataset, we construct three smaller datasets (50K, 200K, and 500K) by sampling the basic dataset. From the ground truth dataset, 108 representative query images are randomly selected for evaluation comparison. Similar to <ref type="bibr" target="#b3">[3]</ref>[14] <ref type="bibr" target="#b16">[16]</ref>, mean average precision (mAP) is selected to evaluate the accuracy performance of all methods.</p><p>We use the standard SIFT feature <ref type="bibr" target="#b1">[1]</ref> for image representation. Key points are detected with the Difference-of-Gaussian (DoG) detector. A 128-D orientation histogram (SIFT descriptor) is extracted to capture the visual appearance of local patch centered at each key point. Before feature extraction, large images are scaled to have a maximum axis size of 400.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Parameter Analysis</head><p>There are two parameters in our approach: Hamming distance threshold  and expansion-bit number d. To study the impact of these two parameters on search performance and computational cost, we compare the mAP performance and average time cost per query under different parameter settings of  and d on the 1-M image dataset. The results are shown in Fig. <ref type="figure" target="#fig_12">10</ref>.</p><p>From Fig. <ref type="figure" target="#fig_12">10</ref>(a), it can be observed that when the Hamming distance threshold  increases, the mAP performance first increases and then keeps stable and gradually drops a little after it reaches the peak, where 24 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head></head><p>. This is intuitive, since increasing  always includes more candidate true matches, but when  is too large, many noisy matches are also included and pollute the results. On the other hand, when expansion-bit number d increases, the mAP gradually increases. This is due to the fact that more candidate code word lists are involved in matching verification, and more true matches will be kept.</p><p>In terms of efficiency, as shown in Fig. <ref type="figure" target="#fig_12">10</ref>(b), the average time cost per query increases when  increases. This is due to that, when  is larger, we have to make more exclusive-OR operations, until the Hamming distance between two 224-bit vectors is above a threshold. As d increases, the querying time cost rises significantly. This is because the expanded code word list number is exponential to the expansion-bit number d.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Considering the tradeoff between mAP performance and time cost,</head><p> is set as 24 and d is set as 2 in the rest experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation</head><p>Comparison Algorithms: We compare our approach with three state-of-the-art feature quantization algorithms in large-scale image search. The BoW approach with visual vocabulary tree <ref type="bibr" target="#b6">[6]</ref> is selected as the "baseline" method. We test various sizes of visual word vocabulary, and the 1-million vocabulary gives the best overall performance. As suggested in <ref type="bibr" target="#b2">[2]</ref>[6], the stop-list strategy is also adopted to improve efficiency. To enhance the baseline, two other algorithms, i.e., soft assignment <ref type="bibr" target="#b13">[13]</ref> and Hamming embedding <ref type="bibr" target="#b15">[15]</ref>, are also compared. Soft assignment <ref type="bibr" target="#b13">[13]</ref> identifies a local feature with a weighted combination of three nearby visual words. We use the default parameters as set in <ref type="bibr" target="#b13">[13]</ref>. The "nearby" visual words for a given feature are found by the approximate nearest neighbor search algorithm k-d tree <ref type="bibr" target="#b21">[21]</ref> <ref type="bibr" target="#b25">[25]</ref>. A public library for approximate nearest neighbor (ANN) searching <ref type="bibr" target="#b22">[22]</ref> is used in our experiments. To make a tradeoff between accuracy and efficiency, we select the error bound parameter <ref type="bibr" target="#b22">[22]</ref> as five.</p><p>Hamming embedding <ref type="bibr" target="#b15">[15]</ref> generates additional Hamming codes (64 bits) to filter more candidate features which are quantized to the same visual word but have large hamming distance to the query feature. We denote this method as "HE". We have tested different thresholds for the Hamming distance in HE, and the best performance is achieved when the threshold is selected as 12. Since the focus of this paper is feature quantization, the weak geometric consistency scheme proposed in the Hamming embedding approach is not added in the experiments.</p><p>Accuracy: From Fig. <ref type="figure" target="#fig_14">11</ref>, it can be observed that our approach outperforms all the other three methods on large image databases. On the 1-million dataset, The mAP of the baseline is 0.38. Our approach hits 0.54, a relatively 42.1% improvement. Since Hamming codes can effectively filter false featurs, the Hamming Embdding approach achieves a mAP of 0.43, but still 11% lower than our approach. The mAP improvement of soft assignment approach is higher than HE. It reaches a mAP of 0.48. Compared with soft assignment, our approach still enjoys a relatively 12.5% improvement. Such improvement stems from the distance based thresholding for matching verification of our approach. It is interesting to note that, when the database size decreases to 50 K, our approach is worse than the soft assignment. This is due to our tradeoff selection on the expansion bit number and efficiency. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Efficiency:</head><p>The experiments are performed on a server with 3.4 GHz CPU and 16 GB memory. We compare efficiency in both off-line indexing and on-line query. From Table <ref type="table" target="#tab_0">1</ref>, it can be observed that our approach is the most efficient one in indexing image features. It takes our approach 18.86 seconds to index one million SIFT features, which is 2 times, 2.5 times, and 40 times faster than the baseline, HE and soft assignment approach, respectively. Fig. <ref type="figure" target="#fig_15">12</ref> shows the average time cost per query of all four approaches. It should be noted that the time cost of SIFT feature extraction is not included for all approaches. It takes the baseline 0.12 second in average to perform one query. HE is the most time-efficient one and costs only 0.05 second to finish one query in average. Soft assignment is the most time-consuming approach, consuming 0.52 second in average per query. Although our approach costs more time than the baseline approach, it may still meet user's expectation of fast response time (average 0.48 second per query) but with much higher search accuracy. It is slightly more efficient than the soft assignment approach, with 0.04 second less in average per query.</p><p>It should be noted that a distinctive characteristic of our approach from other three comparison methods is that, no visual codebook needed to be trained before feature quantization, which could save a lot of computational time. As a contrast, all three comparison algorithms have to train a large visual codebook containing as many as one million visual words, which usually costs days of time. In order to train a visual codebook of one million in size, usually about 100 million SFIT descriptors are needed as training samples. However, even with so many training samples, it is still unclear whether these training samples are enough to generate desired visual words to capture the sample distribution in the so large 128-D descriptor space.</p><p>Moreover, when more new features are indexed, it may be necessary to update the visual codebook accordingly (e.g., re-cluster all the features), which is always time consuming and computationally expensive. On the contrary, our scalar quantization just needs to incrementally add new code words to the existing visual codebook (code word set).   Memory Cost: We compare memory cost of all approaches on both indexed feature and quantizer, as listed in Table <ref type="table" target="#tab_1">2</ref>. In terms of memory cost per indexed feature, for each feature, the baseline approach needs 4 bytes to store image ID and another 4 bytes to store the tf-idf weight. The soft assignment has to store each indexed features in three visual word lists, therefore it costs 24 bytes, three times the memory cost of the baseline approach. In Hamming Embedding approach, it allocates 4 bytes on image ID and 8 bytes on the 64-bit Hamming code. Compared with the above three methods, our approach consumes more memory. It takes 4 bytes to store image ID and additional 28 bytes to store another 224 bits from quantization results.</p><p>Besides indexed feature, all the three comparison methods have to load a large quantizer into main memory. A hierarchical visual vocabulary tree (about 142M bytes) is required for both the baseline and HE. Besides, HE has to store a 64-D median vector (floating point values) for each leaf node. As for soft assignment approach, besides the visual words (leaf nodes of the vocabulary tree, 128M bytes), it also needs to generate a k-d tree (about 378M bytes)to quantize features. As a contrast, our approach needs no memory cost on quantizer. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Sample Results</head><p>In Fig. <ref type="figure" target="#fig_11">13</ref>, a sample edited image "Apollo" is selected as a query to demonstrate the search performance of all four approaches on the 1-million dataset. For this query, compared with the baseline approach, our approach improves the mAP from 0.36 to 0.846, with relatively 135% improvement. Fig. <ref type="figure" target="#fig_11">13 (c</ref>) and (d) show the top images returned by the baseline approach and our approach, respectively. Due to the poor quality of the query image, the returned results of the baseline are polluted by irrelevant images. As for our approach, more relevant images are ranked to the top. Fig. <ref type="figure" target="#fig_11">14</ref> shows a difficult image sample on which our method works poorly. The query image is the first image with label rank-1 in Fig. <ref type="figure" target="#fig_11">14(a)</ref>. Since the query contains a large portion of patches containing rich text, the returned results are polluted by many screenshot images of documents or web pages. The matching result between the query and the fifth returned image is shown in Fig. <ref type="figure" target="#fig_11">14(b</ref>). Although irrelevant in visual content, they still share many local similar patterns. Such query is also very challenging for the other three comparison methods. To filter such false positives, spatial verification algorithms <ref type="bibr" target="#b3">[3]</ref> [17] could be combined with our scalar quantization to re-rank the results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Threshold in Quantization</head><p>In our scalar quantization, the threshold f ˆ in Eq. ( <ref type="formula">1</ref>) is selected as the median value of the feature descriptor, so as in Eq. ( <ref type="formula" target="#formula_1">2</ref>). However, it is still an open question whether the median value is the best choice. There are many alternatives for it, such as mean value of vector f . Alternatively, we can also cluster the 128 elements of a SIFT descriptor into two clusters and choose the cluster boundary as the threshold f ˆ. For some specific applications, f ˆcan also be learned by training. For example, some positive and negative matching pairs are manually labeled beforehand. Then, the threshold f ˆcan be learned to yield the best classification performance for discrimination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Dimension Reduction</head><p>Our approach consumes more memory to index each feature (28 bytes) than other three approaches. It is desired that the memory cost can be further reduced without much sacrifice of search accuracy. As discussed in Section 3.1, it can be inferred that the size of scalar quantization result is proportional to the dimension of SIFT descriptors. If SIFT descriptors can be reduced in dimension, the quantization result will be more compact. There are two possible ways to achieve this goal. One method is to project SIFT descriptors to a low dimensional space by PCA. The dimension-reduced features are expected to keep the characteristics of the original SIFT descriptors, so that our scalar quantization can be applied in the new feature space as well. The second possible method is to change the formulation of SIFT descriptors, instead of performing dimension reduction. The original SIFT descriptor is designed as a combination of 8-D orientation histogram on 16 (4 4) patches. It is possible to reformulate it in a simple way, such as dividing the local region into 3 3 patches instead of 4 4, and extracting an 8-D orientation histogram on each patch. And SIFT descriptor dimension will be reduced from 128 to 72. It is demonstrated by Lowe <ref type="bibr" target="#b1">[1]</ref> that, such a design only causes a small drop on the closest neighbor matching. Its impact on large-scale image search still needs to be further studied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Extension to General Features</head><p>In this paper, we present our scalar quantization based on SIFT descriptors. An intuitive question "can the proposed scalar quantization algorithm be extended to general feature vectors?" may be interesting. SIFT descriptors are very distinctive, with most energy concentrated on relatively few bins, as shown by a typical example in Fig. <ref type="figure" target="#fig_2">3(a)</ref>. Besides, it captures the local visual appearance with very strict representation. Such property makes our approach work well on SIFT descriptors. Therefore, our scalar quantization can be generally extended to other features with similar property as SIFT feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION</head><p>In this paper, a novel quantization scheme "scalar quantization" is proposed on SIFT descriptors for large-scale image search. Scalar quantization quantizes a SIFT descriptor to a 256-bit vector, which can be easily adapted to the classic inverted file structure for indexing. Distinguished from the traditional vector quantization approaches, the proposed scalar quantization approach does not involve any kind of visual codebook training or clustering. The quantizer is defined by an individual feature itself and is independent of collections of images. Further, soft quantization is proposed to efficiently enumerate the nearest code words for quantization error reduction. Experiments on large-scale image search demonstrate the superiority of scalar quantization on retrieval accuracy over other state-of-the-art methods In the future, investigation will be performed on developing more compact bit-vector representation in scalar quantization. Moreover, the flipping behavior of bit-vectors of similar SIFT descriptors will be explored. Some insights are expected to be obtained from this study, which may be beneficial for searching space reduction in soft quantization step and consequently improve retrieval efficiency. Further, various choices for threshold selection in scalar quantization will be studied.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>4 </head><label>4</label><figDesc>pairs of SIFT descriptors. (a) Descriptor pair frequency vs. Hamming distance; (b) The average L 2 -distance vs. Hamming distance; (c) The average standard deviation vs. Hamming distance.To demonstrate the discriminative power of SIFT descriptors is well kept in our scalar quantization, we have made a</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Example of feature matches. (a) A local match between two images. The endpoints of the green line denote the key point positions of two SIFT features. The radius of the red circle centered at the key points is proportional to the SIFT feature's characteristic scale. (b) top: the 128-D descriptor of the matched SIFT feature in the left image; middle: the 128-D descriptor of the matched SIFT feature in the right image; bottom: the XOR result of the binary SIFT features from the two matched SIFT features. The red horizontal lines in the "top" and "bottom" figure denote the median values of the two SIFT descriptors, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Statistics of SIFT descriptors. (a) A typical SIFT descriptor with bins sorted by magnitude in each dimension; (b) The frequency distribution of median value of the descriptor vector among 100 million SIFT descriptors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>order. With Eq. (2), each dimension of SIFT descriptor is divided into three parts, and two bits are used to encode each part.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .Figure 5 .</head><label>45</label><figDesc>Figure 4. A toy example of image feature indexed with inverted file structure. The scalar quantization result of the indexed feature is an 8-bit vector (1001 0101). The first three bits denote its code word ID (100), and the remaining 5 bits (10101) are stored in the inverted file list.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Frequency of code words among one million images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. A toy example of soft quantization with bit-stream code words. There are eight code words, each represented with a three-bit vector. Each code word is followed by an indexed image feature list. (Best viewed in color PDF)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. The general steps of off-line indexing with scalar quantization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. The general steps of on-line querying with scalar quantization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>1 )</head><label>1</label><figDesc>Given a 128-D SIFT descriptor in a query image, convert it to a 256-bit vector by Eq. (2);</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 10 .</head><label>10</label><figDesc>(a)The mAP performance and (b) average time cost per query under different parameter settings of  and d on the 1-million image dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. Performance (mAP) comparison of different methods with different database sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 12 .</head><label>12</label><figDesc>Figure 12. Comparison of average query time cost of different methods on the 1M database. (Not including the time cost for SIFT feature extraction)</figDesc><graphic coords="8,361.92,218.34,189.12,109.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 13 .Figure 14 .</head><label>1314</label><figDesc>Sample results comparing the baseline and our approach. (a) Query image; (b) Comparison of the precisionrecall curves of all four approaches; (c) The top images (rank 11 to rank 20) returned by the baseline; (d) The top images returned by our approach (rank 11 to rank 20). The first 10 images are true positive for both baseline and our approach and therefore are not shown. The false positives are shown with red dashed bounding boxes. (Best viewed in color PDF) Retrieval examples. (a) Retrieval results of the query (Rank 1). The number of local matches is shown in the bracket below each image. (b) Feature matching between the query and the rank 5 result. (Best viewed in color PDF)5. DISCUSSION</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 . Time cost to index 1 million SIFT features for four approaches in off-line stage.</head><label>1</label><figDesc></figDesc><table><row><cell cols="2">Method</cell><cell>baseline</cell><cell>HE</cell><cell>soft assignment</cell><cell>our approach</cell></row><row><cell cols="2">Time cost (second)</cell><cell>53.72</cell><cell>64.82</cell><cell>771.09</cell><cell>18.86</cell></row><row><cell>average time cost per query(second)</cell><cell>0.00 0.10 0.20 0.30 0.40 0.50 0.60</cell><cell>0.12</cell><cell>0.05</cell><cell>0.52</cell><cell>0.48</cell></row><row><cell></cell><cell></cell><cell>baseline</cell><cell>HE</cell><cell cols="2">soft assignment our approach</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 . Memory cost for four approaches.</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell>Memory cost per</cell><cell>Memory cost for</cell></row><row><cell></cell><cell>indexed feature (byte)</cell><cell>quantizer (byte)</cell></row><row><cell>baseline</cell><cell>8</cell><cell>142M</cell></row><row><cell>HE</cell><cell>12</cell><cell>398M</cell></row><row><cell>soft assignment</cell><cell>24</cell><cell>506M</cell></row><row><cell>our approach</cell><cell>32</cell><cell>0</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">ACKNOWLEDGMENTS</head><p>This work was supported in part to Dr. Lu by Research Enhancement Program (REP), start-up funding from the Texas State University and DoD HBCU/MI grant W911NF-12-1-0057, in part to Dr. Li by NSFC general project "Intelligent Video Processing and Coding Based on Cloud Computing", and in part to Dr. Tian by ARO grant W911NF-12-1-0057, NSF IIS 1052851, Faculty Research Awards by Google, NEC Laboratories of America and FXPAL, UTSA START-R award, respectively.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Distinctive image features form scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Video Google: a text retrieval approach to object matching in videos</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Object retrieval with large vocabularies and fast spatial matching</title>
		<author>
			<persName><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Spatial coding for large scale partial-duplicate Web image search</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Multimedia</title>
		<meeting>ACM Multimedia</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Total recall: Automatic query expansion with a generative feature model for object retrieval</title>
		<author>
			<persName><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scalable recognition with a vocabulary tree</title>
		<author>
			<persName><forename type="first">D</forename><surname>Nister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Stewenius</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Robust wide baseline stereo from maximally stable extremal regions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Scale &amp; affine invariant interest point detectors</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">60</biblScope>
			<biblScope unit="page" from="63" to="86" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">SURF: Speeded up robust features</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">BRIEF: binary robust independent elementary features</title>
		<author>
			<persName><forename type="first">M</forename><surname>Calonder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">ORB: an efficient alternative to SIFT or SURF</title>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Rublee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Localization using an image map</title>
		<author>
			<persName><forename type="first">C</forename><surname>Silpa-Anan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Australasian Conf. on Robo. and Auto</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Lost in quantization: Improving particular object retrieval in large scale image databases</title>
		<author>
			<persName><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Vector quantizing feature space with a regular lattice</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hamming embedding and weak geometric consistency for large scale image search</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bundling features for large scale partialduplicate Web image search</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Large scale image search with geometric coding</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Multimedia</title>
		<meeting>ACM Multimedia</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Near duplicate image detection: min-Hash and tf-idf weighting</title>
		<author>
			<persName><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Geometric min-Hashing: finding a (thick) needle in a haystack</title>
		<author>
			<persName><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Perdoch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Random Sample Consensus: A paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Fischler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comm. of the ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="381" to="395" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">K-d trees for semidynamic point sets</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Bentley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 6th Ann</title>
		<meeting>6th Ann</meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="187" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Ann: Library for approximate nearest neighbor searching</title>
		<author>
			<persName><forename type="first">S</forename><surname>Arya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mount</surname></persName>
		</author>
		<ptr target="http://www.cs.umd.edu/~mount/ANN/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Baeza-Yates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ribeiro-Neto</surname></persName>
		</author>
		<title level="m">Modern information retrieval</title>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Query expansion for hash-based image object retrieval</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Multimedia</title>
		<meeting>ACM Multimedia</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An algorithm for finding best matches in logarithmic expected time</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Bentley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Finkel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Mathematical Software</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="209" to="226" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Asymmetric Hamming embedding: taking the best of our bits for large scale image search</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Multimedia</title>
		<meeting>ACM Multimedia</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Aggregating local descriptors into a compact image representation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Residual enhanced visual vectors for ondevice image matching</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tsai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asilomar Conference on Signals, Systems, and Computers</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Generating descriptive visual words and visual phrases for large-scale image applications</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2664" to="2677" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Building contextual visual vocabulary for largescale image applications</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Multimedia</title>
		<meeting>ACM Multimedia</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
