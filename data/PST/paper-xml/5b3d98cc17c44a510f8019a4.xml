<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generalizing to Unseen Domains via Adversarial Data Augmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Riccardo</forename><surname>Volpi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hongseok</forename><surname>Namkoong</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ozan</forename><surname>Sener</surname></persName>
						</author>
						<author>
							<persName><forename type="first">John</forename><surname>Duchi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Vittorio</forename><surname>Murino</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Istituto Italiano di Tecnologia</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Intel Labs</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Istituto Italiano di Tecnologia</orgName>
								<orgName type="institution">Università di Verona</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Generalizing to Unseen Domains via Adversarial Data Augmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1F6AF98BB8DD50C4EAD3FF93AEA50268</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T04:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We are concerned with learning models that generalize well to different unseen domains. We consider a worst-case formulation over data distributions that are near the source domain in the feature space. Only using training data from a single source distribution, we propose an iterative procedure that augments the dataset with examples from a fictitious target domain that is "hard" under the current model. We show that our iterative scheme is an adaptive data augmentation method where we append adversarial examples at each iteration. For softmax losses, we show that our method is a data-dependent regularization scheme that behaves differently from classical regularizers that regularize towards zero (e.g., ridge or lasso). On digit recognition and semantic segmentation tasks, our method learns models improve performance across a range of a priori unknown target domains. * Equal contribution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In many modern applications of machine learning, we wish to learn a system that can perform uniformly well across multiple populations. Due to high costs of data acquisition, however, it is often the case that datasets consist of a limited number of population sources. Standard models that perform well when evaluated on the validation dataset-usually collected from the same population as the training dataset-often perform poorly on populations different from that of the training data <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b37">38]</ref>. In this paper, we are concerned with generalizing to populations different from the training distribution, in settings where we have no access to any data from the unknown target distributions. For example, consider a module for self-driving cars that needs to generalize well across weather conditions and city environments unexplored during training.</p><p>A number of authors have proposed domain adaptation methods (for example, see <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b39">40]</ref>) in settings where a fully labeled source dataset and an unlabeled (or partially labeled) set of examples from fixed target distributions are available. Although such algorithms can successfully learn models that perform well on known target distributions, the assumption of a priori fixed target distributions can be restrictive in practical scenarios. For example, consider a semantic segmentation algorithm used by a robot: every task, robot, environment and camera configuration will result in a different target distribution, and these diverse scenarios can be identified only after the model is trained and deployed, making it difficult to collect samples from them.</p><p>In this work, we develop methods that can learn to better generalize to new unknown domains. We consider the restrictive setting where training data only comes from a single source domain. Inspired by recent developments in distributionally robust optimization and adversarial training <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b11">12]</ref>, we consider the following worst-case problem around the (training) source distribution P 0 minimize θ∈Θ sup P :D(P,P0)≤ρ</p><formula xml:id="formula_0">E P [ (θ; (X, Y ))].<label>(1)</label></formula><p>Here, θ ∈ Θ is the model, (X, Y ) ∈ X × Y is a source data point with its labeling, : X × Y → R is the loss function, and D(P, Q) is a distance metric on the space of probability distributions.</p><p>The solution to worst-case problem <ref type="bibr" target="#b0">(1)</ref> guarantees good performance against data distributions that are distance ρ away from the source domain P 0 . To allow data distributions that have different support to that of the source P 0 , we use Wasserstein distances as our metric D. Our distance will be defined on the semantic space<ref type="foot" target="#foot_0">3</ref> , so that target populations P satisfying D(P, P 0 ) ≤ ρ represent realistic covariate shifts that preserve the same semantic representation of the source (e.g., adding color to a greyscale image). In this regard, we expect the solution to the worst-case problem (1)-the model that we wish to learn-to have favorable performance across covariate shifts in the semantic space.</p><p>We propose an iterative procedure that aims to solve the problem (1) for a small value of ρ at a time, and does stochastic gradient updates to the model θ with respect to these fictitious worst-case target distributions (Section 2). Each iteration of our method uses small values of ρ, and we provide a number of theoretical interpretations of our method. First, we show that our iterative algorithm is an adaptive data augmentation method where we add adversarially perturbed samples-at the current model-to the dataset (Section 3). More precisely, our adversarially generated samples roughly correspond to Tikhonov regularized Newton-steps <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b24">25]</ref> on the loss in the semantic space. Further, we show that for softmax losses, each iteration of our method can be thought of as a data-dependent regularization scheme where we regularize towards the parameter vector corresponding to the true label, instead of regularizing towards zero like classical regularizers such as ridge or lasso.</p><p>From a practical viewpoint, a key difficulty in applying the worst-case formulation <ref type="bibr" target="#b0">(1)</ref> is that the magnitude of the covariate shift ρ is a priori unknown. We propose to learn an ensemble of models that correspond to different distances ρ. In other words, our iterative method generates a collection of datasets, each corresponding to a different inter-dataset distance level ρ, and we learn a model for each of them. At test time, we use a heuristic method to choose an appropriate model from the ensemble.</p><p>We test our approaches on a simple digit recognition task, and a more realistic semantic segmentation task across different seasons and weather conditions. In both settings, we observe that our method allows to learn models that improve performance across a priori unknown target distributions that have varying distance from the original source domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related work</head><p>The literature on adversarial training <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b11">12]</ref> is closely related to our work, since the main goal is to devise training procedures that learn models robust to fluctuations in the input. Departing from imperceptible attacks considered in adversarial training, we aim to learn models that are resistant to larger perturbations, namely out-of-distribution samples. Sinha et al. <ref type="bibr" target="#b33">[34]</ref> proposes a principled adversarial training procedure, where new images that maximize some risk are generated and the model parameters are optimized with respect to those adversarial images. Being devised for defense against imperceptible adversarial attacks, the new images are learned with a loss that penalizes differences between the original and the new ones. In this work, we rely on a minimax game similar to the one proposed by Sinha et al. <ref type="bibr" target="#b33">[34]</ref>, but we impose the constraint in the semantic space, in order to allow our adversarial samples from a fictitious distribution to be different at the pixel level, while sharing the same semantics.</p><p>There is a substantial body of work on domain adaptation <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b39">40]</ref>, which aims to better generalize to a priori fixed target domains whose labels are unknown at training time. This setup is different from ours in that these algorithms require access to samples from the target distribution during training. Domain generalization methods <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b23">24]</ref> that propose different ways to better generalize to unknown domains are also related to our work. These algorithms require the training samples to be drawn from different domains (while having access to the domain labels during training), not a single source, a limitation that our method does not have. In this sense, one could interpret our problem setting as unsupervised domain generalization. Tobin et al. <ref type="bibr" target="#b36">[37]</ref> proposes domain randomization, which applies to simulated data and creates a variety of random renderings with the simulator, hoping that the real world will be interpreted as one of them. Our goal is the same, since we aim at obtaining data distributions more similar to the real world ones, but we accomplish it by actually learning new data points, and thus making our approach applicable to any data source and without the need of a simulator.</p><p>Hendrycks and Gimpel <ref type="bibr" target="#b12">[13]</ref> suggest that a good empirical way to detect whether a test sample is out-of-distribution for a given model is to evaluate the statistics of the softmax outputs. We adapt this idea in our setting, learning ensemble of models trained with our method and choosing at test time the model with the greatest maximum softmax value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>The worst-case formulation (1) over domains around the source P 0 hinges on the notion of distance D(P, P 0 ), that characterizes the set of unknown populations we wish to generalize to. Conventional notions of Wasserstein distance used for adversarial training <ref type="bibr" target="#b33">[34]</ref> are defined with respect to the original input space X , which for images corresponds to raw pixels. Since our goal is to consider fictitious target distributions corresponding to realistic covariate shifts, we define our distance on the semantic space. Before properly defining our setup, we first give a few notations. Letting p the dimension of output of the last hidden layer, we denote θ = (θ c , θ f ) where θ c ∈ R p×m is the set of weights of the final layer, and θ f is the rest of the weights of the network. We denote by g(θ f ; x) the output of the embedding layer of our neural network. For example, in the classification setting, m is the number of classes and we consider the softmax loss</p><formula xml:id="formula_1">(θ; (x, y)) := -log exp θ c,y g(θ f ; x) m j=1 exp θ c,j g(θ f ; x)<label>(2)</label></formula><p>where θ c,j is the j-th column of the classification layer weights θ c ∈ R p×m .</p><p>Wasserstein distance on the semantic space On the space R p × Y, consider the following transportation cost c-cost of moving mass from (z, y) to (z , y ) c((z, y), (z , y</p><formula xml:id="formula_2">)) := 1 2 z -z 2 2 + ∞ • 1 {y = y } .</formula><p>The transportation cost takes value ∞ for data points with different labels, since we are only interested in perturbation to the marginal distribution of Z. We now define our notion of distance on the semantic space. For inputs coming from the original space X × Y, we consider the transportation cost c θ defined with respect to the output of the last hidden layer</p><formula xml:id="formula_3">c θ ((x, y), (x , y )) := c((g(θ f ; x), y), (g(θ f ; x ), y ))</formula><p>so that c θ measures distance with respect to the feature mapping g(θ f ; x). For probability measures</p><formula xml:id="formula_4">P and Q both supported on X × Y, let Π(P, Q) denote their couplings, meaning measures M with M (A, X × Y) = P (A) and M (X × Y, A) = Q(A).</formula><p>Then, we define our notion of distance by</p><formula xml:id="formula_5">D θ (P, Q) := inf M ∈Π(P,Q) E M [c θ ((X, Y ), (X , Y ))].<label>(3)</label></formula><p>Armed with this notion of distance on the semantic space, we now consider a variant of the worst-case problem (1) where we replace the distance with D θ (3), our adaptive notion of distance defined on the semantic space for i = 1, . . . , n do 8:</p><formula xml:id="formula_6">X k i ← X i 9:</formula><p>for t = 1, . . . , T max do 10:</p><formula xml:id="formula_7">X k i ← X k i + η∇ x (θ; (X k i , Y i )) -γc θ ((X k i , Y i ), (X i , Y i )) 11: Append (X k i , Y k i ) to dataset 12: for t = 1, . . . , T do 13:</formula><p>Sample (X, Y ) uniformly from dataset 14:</p><formula xml:id="formula_8">θ ← θ -α∇ θ (θ; (X, Y ))</formula><p>Taking the dual reformulation of the penalty relaxation (4), we can obtain an efficient solution procedure. The following result is a minor adaptation of [2, Theorem 1]; to ease notation, let us define the robust surrogate loss</p><formula xml:id="formula_9">φ γ (θ; (x 0 , y 0 )) := sup x∈X { (θ; (x, y 0 )) -γc θ ((x, y 0 ), (x 0 , y 0 ))} .<label>(5)</label></formula><p>Lemma 1. Let : Θ × (X × Y) → R be continuous. For any distribution Q and any γ ≥ 0, we have sup</p><formula xml:id="formula_10">P {E P [ (θ; (X, Y ))] -γD θ (P, Q)} = E Q [φ γ (θ; (X, Y ))].<label>(6)</label></formula><p>In order to solve the penalty problem (4), we can now perform stochastic gradient descent procedures on the robust surrogate loss φ γ . Under suitable conditions <ref type="bibr" target="#b4">[5]</ref>, we have</p><formula xml:id="formula_11">∇ θ φ γ (θ; (x 0 , y 0 )) = ∇ θ (θ; (x γ , y 0 )),<label>(7)</label></formula><p>where x γ = arg max x∈X { (θ; (x, y 0 )) -γc θ ((x, y 0 ), (x 0 , y 0 ))} is an adversarial perturbation of x 0 at the current model θ. Hence, computing gradients of the robust surrogate φ γ requires solving the maximization problem <ref type="bibr" target="#b4">(5)</ref>. Below, we consider an (heuristic) iterative procedure that iteratively performs stochastic gradient steps on the robust surrogate φ γ .</p><p>Iterative Procedure We propose an iterative training procedure where two phases are alternated: a maximization phase where new data points are learned by computing the inner maximization problem (5) and a minimization phase, where the model parameters are updated according to stochastic gradients of the loss evaluated on the adversarial examples generated from the maximization phase. The latter step is equivalent to stochastic gradient steps on the robust surrogate loss φ γ , which motivates its name. The main idea here is to iteratively learn "hard" data points from fictitious target distributions, while preserving the semantic features of the original data points.</p><p>Concretely, in the k-th maximization phase, we compute n adversarially perturbed samples at the current model θ ∈ Θ</p><formula xml:id="formula_12">X k i ∈ arg max x∈X (θ; (x, Y i )) -γc θ ((x, Y i ), (X k-1 i , Y i ))<label>(8)</label></formula><p>where X 0 i are the original samples from the source distribution P 0 . The minimization phase then performs repeated stochastic gradient steps on the augmented dataset {X k i , Y i } 0≤k≤K,1≤i≤n} . The maximization phase (8) can be efficiently computed for smooth losses if <ref type="bibr">Theorem 2]</ref>; for example, this is provably true for any linear network. In practice, we use gradient ascent steps to solve for worst-case examples <ref type="bibr" target="#b7">(8)</ref>; see Algorithm 1 for the full description of our algorithm.</p><formula xml:id="formula_13">x → c θ k-1 ((x, Y i ), (X k-1 i , Y i )) is strongly convex [34,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ensembles for classification</head><p>The hyperparameter γ-which is inversely proportional to ρ, the distance between the fictitious target distribution and the source-controls the ability to generalize outside the source domain. Since target domains are unknown, it is difficult to choose an appropriate level of γ a priori. We propose a heuristic ensemble approach where we train s models θ 0 , ..., θ s . Each model is associated with a different value of γ, and thus to fictitious target distributions with varying distances from the source P 0 . To select the best model at test time-inspired by Hendrycks and Gimpel <ref type="bibr" target="#b12">[13]</ref>-given a sample x, we select the model θ u (x) with the greatest softmax score u (x) := arg max</p><formula xml:id="formula_14">1≤u≤s max 1≤j≤k θ u c,j g(θ u f ; x).<label>(9)</label></formula><p>3 Theoretical Motivation</p><p>In our iterative algorithm (Algorithm 1), the maximization phase (8) was a key step that augmented the dataset with adversarially perturbed data points, which was followed by standard stochastic gradient updates to the model parameters. In this section, we provide some theoretical understanding of the augmentation step <ref type="bibr" target="#b7">(8)</ref>. First, we show that the augmented data points (8) can be interpreted as Tikhonov regularized Newton-steps <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b24">25]</ref> in the semantic space under the current model. Roughly speaking, this gives the sense in which Algorithm 1 is an adaptive data augmentation algorithm that adds data points from fictitious "hard" target distributions. Secondly, recall the robust surrogate loss (5) whose stochastic gradients were used to update the model parameters θ in the minimization step (Eq (7)). In the classification setting, we show that the robust surrogate (5) roughly corresponds to a novel data-dependent regularization scheme on the softmax loss . Instead of penalizing towards zero like classical regularizers (e.g., ridge or lasso), our data-dependent regularization term penalizes deviations from the parameter vector corresponding to that of the true label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Adaptive Data Augmentation</head><p>We now give an interpretation for the augmented data points in the maximization phase <ref type="bibr" target="#b7">(8)</ref>. Concretely, we fix θ ∈ Θ, x 0 ∈ X , y 0 ∈ Y, and consider an -maximizer</p><p>x ∈ -arg max x∈X { (θ; (x, y 0 )) -γc θ ((x, y 0 ), (x 0 , y 0 ))} .</p><p>We let z 0 := g(θ f ; x 0 ) ∈ R p , and abuse notation by using (θ; (z 0 , y 0 )) := (θ; (x 0 , y 0 )). In what follows, we show that the feature mapping g(θ f ; x ) satisfies</p><formula xml:id="formula_15">g(θ f ; x ) = g(θ f ; x 0 ) + 1 γ I - 1 γ ∇ zz (θ; (z 0 , y 0 )) -1 ∇ z (θ; (z 0 , y 0 )) =: gnewton(θ f ;x0) +O γ + 1 γ 2 .</formula><p>(10) Intuitively, this implies that the adversarially perturbed sample x is drawn from a fictitious target distribution where probability mass on z 0 = g(θ f ; x 0 ) was transported to g newton (θ f ; x 0 ). We note that the transported point in the semantic space corresponds to a Tikhonov regularized Newton-step <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b24">25]</ref> on the loss z → (θ; (z, y 0 )) at the current model θ. Noting that computing g newton (θ f ; x 0 ) involves backsolves on a large dense matrix, we can interpret our gradient ascent updates in the maximization phase (8) as an iterative scheme for approximating this quantity.</p><p>We assume sufficient smoothness, where we use H to denote the 2 -operator norm of a matrix H. Assumption 1. There exists L 0 , L 1 &gt; 0 such that, for all z, z ∈ R p , we have</p><formula xml:id="formula_16">| (θ; (z, y 0 )) - (θ; (z , y 0 ))| ≤ L 0 z -z 2 and ∇ z (θ; (z, y 0 )) -∇ z (θ; (z , y 0 )) 2 ≤ L 1 z -z 2 .</formula><p>Assumption 2. There exists L 2 &gt; 0 such that, for all z, z ∈ R p , we have</p><formula xml:id="formula_17">∇ zz (θ; (z, y 0 )) -∇ zz (θ; (z , y 0 )) ≤ L 2 z -z 2 .</formula><p>Then, we have the following bound <ref type="bibr" target="#b9">(10)</ref> whose proof we defer to Appendix A.1.</p><formula xml:id="formula_18">Theorem 1. Let Assumptions 1, 2 hold. If Im(g(θ f ; •)) = R p and γ &gt; L 1 , then g(θ f ; x ) -g newton (θ f ; x 0 ) 2 2 ≤ 2 γ -L 1 + L 2 3(γ -L 1 ) 5L 0 γ 3 + L 0 γ -L 1 3 + 2 γ 3 2</formula><p>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Data-Dependent Regularization</head><p>In this section, we argue that under suitable conditions on the loss,</p><formula xml:id="formula_19">φγ(θ; (z, y)) = (θ; (z, y)) + 1 γ ∇z (θ; (z, y)) 2 2 + O 1 γ 2 .</formula><p>For classification problems, we show that the robust surrogate loss ( <ref type="formula" target="#formula_9">5</ref>) corresponds to a particular data-dependent regularization scheme. Let (θ; (x, y)) be the m-class softmax loss (2) given by (θ; (x, y)) = -log py(θ, x) where pj(θ, x) := exp(θ c,j g(θ, x))</p><formula xml:id="formula_20">m l=1 exp(θ c,l g(θ f ; x))</formula><p>.</p><p>where θ c,j ∈ R p is the j-th row of the classification layer weight θ c ∈ R p×m . Then, the robust surrogate φ γ is an approximate regularizer on the classification layer weights θ c φγ(θ;</p><formula xml:id="formula_21">(x, y)) = (θ; (x, y)) + 1 γ θc,y - m j=1 pj(θ, x)θc,j 2 2 + O 1 γ 2 . (<label>11</label></formula><formula xml:id="formula_22">)</formula><p>The expansion <ref type="bibr" target="#b10">(11)</ref> shows that the robust surrogate ( <ref type="formula" target="#formula_9">5</ref>) is roughly equivalent to data-dependent regularization where we minimize the distance between m j=1 p j (θ, x)θ c,j , our "average estimated linear classifier", to θ c,y , the linear classifier corresponding to the true label y. Concretely, for any fixed θ ∈ Θ, we have the following result where we use L(θ) := 2 max 1≤j ≤m θ c,j 2 m j=1 θ c,j 2 to ease notation. See Appendix A.3 for the proof. Theorem 2. If Im(g(θ f ; •)) = R p and γ &gt; L(θ), the softmax loss (2) satisfies</p><formula xml:id="formula_23">1 γ + L(θ) θc,y - m j=1 pj(θ, x)θc,j 2 2 ≤ φγ(θ, (x, y))-(θ, (x, y)) ≤ 1 γ -L(θ) θc,y - m j=1 pj(θ, x)θc,j<label>2 2</label></formula><p>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate our method for both classification and semantic segmentation settings, following the evaluation scenarios of domain adaptation techniques <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b13">14]</ref>, though in our case the target domains are unknown at training time. We summarize our experimental setup including implementation details, evaluation metrics and datasets for each task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Digit classification</head><p>We train on MNIST <ref type="bibr" target="#b18">[19]</ref> dataset and test on MNIST-M <ref type="bibr" target="#b8">[9]</ref>, SVHN <ref type="bibr" target="#b29">[30]</ref>, SYN <ref type="bibr" target="#b8">[9]</ref> and USPS <ref type="bibr" target="#b5">[6]</ref>. We use 10, 000 digit samples for training and evaluate our models on the respective test sets of the different target domains, using accuracy as a metric. In order to work with comparable datasets, we resized all the images to 32×32, and treated images from MNIST and USPS as RGB. We use a ConvNet <ref type="bibr" target="#b17">[18]</ref> with architecture conv-pool-conv-pool-fc-fc-softmax and set the hyperparameters α = 0.0001, η = 1.0, T min = 100 and T max = 15. In the minimization phase, we use Adam <ref type="bibr" target="#b16">[17]</ref> with batch size equal to 32 <ref type="foot" target="#foot_1">4</ref> . We compare our method against the Empirical Risk Minimization (ERM) baseline and different regularization techniques (Dropout <ref type="bibr" target="#b34">[35]</ref>, ridge).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic scene segmentation</head><p>We use the SYTHIA <ref type="bibr" target="#b30">[31]</ref> dataset for semantic segmentation. The dataset contains images from different locations (we use Highway, New York-like City and Old European Town), and different weather/time/date conditions (we use Dawn, Fog, Night, Spring and Winter. We train models on a source domain and test on other domains, using the standard mean Intersection Over Union (mIoU) metric to evaluate our performance <ref type="bibr" target="#b7">[8]</ref>. We arbitrarily chose images from the left front camera throughout our experiments. For each one, we sample 900 random images (resized to 192 × 320 pixels) from the training set. We use a Fully Convolutional Network (FCN) <ref type="bibr" target="#b22">[23]</ref>, with a ResNet-50 <ref type="bibr" target="#b10">[11]</ref> body and set the hyperparameters α = 0.0001, η = 2.0, T min = 500 and T max = 50. For the minimization phase, we use Adam <ref type="bibr" target="#b16">[17]</ref> with batch size equal to 8. We compare our method against the ERM baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Results on Digit Classification</head><p>In this section, we present and discuss the results on the digit classification experiment. Firstly, we are interested in analyzing the role of the semantic constraint we impose. Figure <ref type="figure" target="#fig_0">1a</ref> (top) shows performances associated with models trained with Algorithm 1 with K = 1 and γ = 10 4 , with the constraint in the semantic space (as discussed in Section 2) and in the pixel space <ref type="bibr" target="#b33">[34]</ref> (blue and yellow bars, respectively). Figure <ref type="figure" target="#fig_0">1a</ref> (bottom) shows performances of models trained with our method using different values of the hyperparameter γ (with K = 2) and with ERM (blue bars and red lines, respectively). These plots show (i) that moving the constraint on the semantic space carries benefits when models are tested on unseen domains and (ii) that models trained with Algorithm 1 outperform models train with ERM for any value of γ on out-of-sample domains (SVHN, MNIST-M and SYN). The latter result is a rather desired achievement, since this hyperparameter cannot be properly cross-validated. On USPS, our method causes accuracy to drop since MNIST and USPS are very similar datasets, thus the image domain that USPS belongs to is not explored by our algorithm during the training procedure, which optimizes for worst case performance.</p><p>Figure <ref type="figure" target="#fig_0">1b</ref> (top) reports results related to models trained with our method (blue bars), varying the number of iterations K and fixing γ = 1.0, and results related to ERM (red bars) and Dropout <ref type="bibr" target="#b34">[35]</ref> (yellow bars). We observe that our method improves performances on SVHN, MNIST-M and SYN, outperforming both ERM and Dropout <ref type="bibr" target="#b34">[35]</ref> statistically significantly. In Figure <ref type="figure" target="#fig_0">1b</ref> (middle), we compare models trained with ridge regularization (green bars) with models trained with Algorithm 1 (with K = 1 and γ = 1.0) and ridge regularization (blue bars); these results show that our method can potentially benefit from other regularization approaches, as in this case we observed that the two effects sum up. We further report in Appendix B a comparison between our method and an unsupervised domain adaptation algorithm (ADDA <ref type="bibr" target="#b38">[39]</ref>), and results associated with different values of the hyperparameters γ and K.</p><p>Finally, we report the results obtained by learning an ensemble of models. Since the hyperparameter γ is nontrivial to set a priori, we use the softmax confidences <ref type="bibr" target="#b8">(9)</ref> to choose which model to use at test time. We learn ensemble of models, each of which is trained by running Algorithm 1 with different values of the γ as γ = 10 -i , with i = 0, 1, 2, 3, 4, 5, 6 . Figure <ref type="figure" target="#fig_0">1b</ref> (bottom) shows the comparison between our method with different numbers of iterations K and ERM (blue and red bars, respectively). In order to separate the role of ensemble learning, we learn an ensemble of baseline models each corresponding to a different initialization. We fix the number of models in the ensemble to be the same for both the baseline (ERM) and our method. Comparing Figure <ref type="figure" target="#fig_0">1b</ref> (bottom) with Figure <ref type="figure" target="#fig_0">1b</ref> (top) and Figure <ref type="figure" target="#fig_0">1a</ref> (bottom), our ensemble approach achieves higher accuracy in different testing scenarios. We observe that our out-of-sample performance improves as the number of iterations K gets large. Also in the ensemble setting, for the USPS dataset we do not see any improvement, which we conjecture to be an artifact of the trade-off between good performance on domains far away from training, and those closer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results on Semantic Scene Segmentation</head><p>We report a comparison between models trained with ERM and models trained with our method (Algorithm 1 with K = 1). We set γ = 1.0 in every experiment, but stress that this is an arbitrary value; we did not observe a strong correlation between the different values of γ and the general behavior of the models in this case. Its role was more meaningful in the ensemble setting where each model is associated with a different level of robustness, as discussed in Section 2. In this setting, we do not apply the ensemble approach, but only evaluate the performances of the single models. The main reason for this choice is the fact that the heuristics developed to choose the correct model at test time in effect cannot be applied in a straightforward fashion to a semantic segmentation problem.</p><p>Figure <ref type="figure" target="#fig_1">2</ref> reports numerical results obtained. Specifically, leftmost plots report results associated with models trained on sequences from the Highway split and tested on the New York-like City and the Old European Town splits (top-left and bottom-left, respectively); rightmost plots report results associated with models trained on sequences from the New York-like City split and tested on the Highway and the Old European Town splits (top-right and bottom-right, respectively). The training sequences (Dawn, Fog, Night, Spring and Winter) are indicated on the x-axis. Red and blue bars indicate average mIoUs achieved by models trained with ERM and by models trained with our method, respectively. These results were calculated by averaging over the mIoUs obtained with each model on the different conditions of the test set. As can be observed, models trained with our method mostly better generalize to unknown data distributions. In particular, our method always outperforms the baseline by a statistically significant margin when the training images are from Night scenarios. This is since the baseline models trained on images from Night are strongly biased towards dark scenery, while, as a consequence of training over worst-case distributions, our models can overcome this strong bias and better generalize across different unseen domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>We study a new adversarial data augmentation procedure that learns to better generalize across unseen data distributions, and define an ensemble method to exploit this technique in a classification framework. This is in contrast to domain adaptation algorithms, which require a sufficient number of samples from a known, a priori fixed target distribution. Our experimental results show that our iterative procedure provides broad generalization behavior on digit recognition and cross-season and cross-weather semantic segmentation tasks.</p><p>For future work, we hope to extend the ensemble methods by defining novel decision rules. The proposed heuristics (9) only apply to classification settings, and extending them to a broad realm of tasks including semantic segmentation is an important direction. Many theoretical questions still remain. For instance, quantifying the behavior of data-dependent regularization schemes presented in Section 3 would help us better understand adversarial training methods in general.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Results associated with models trained with 10, 000 MNIST samples and tested on SVHN, MNIST-M, SYN and USPS (1 st , 2 nd , 3 rd and 4 th columns, respectively). Panel (a), top: comparison between distances in the pixel space (yellow) and in the semantic space (blue), with γ = 10 4 and K = 1. Panel (a), bottom: comparison between our method with K = 2 and different γ values (blue bars) and ERM (red line). Panel (b), top: comparison between our method with γ = 1.0 and different number of iterations K (blue), ERM (red) and Dropout [35] (yellow). Panel (b), middle: comparison between models regularized with ridge (green) and with ridge + our method with γ = 1.0 and K = 1 (blue). Panel (b), bottom: results related to the ensemble method, using models trained with our methods with different number of iterations K (blue) and using models trained via ERM (red). The reported results are obtained by averaging over 10 different runs; black bars indicate the range of accuracy spanned.</figDesc><graphic coords="7,137.34,72.00,337.33,437.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Results obtained with semantic segmentation models trained with ERM (red) and our method with K = 1 and γ = 1.0 (blue). Leftmost panels are associated with models trained on Highway, rightmost panels are associated with models trained on New York-like City. Test datasets are Highway, New York-like City and Old European Town.</figDesc><graphic coords="8,108.00,72.00,396.03,166.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Adversarial Data Augmentation Input: original dataset {X i , Y i } i=1,...,n and initialized weights θ 0 Output: learned weights θ</figDesc><table><row><cell cols="2">1: Initialize: θ ← θ 0</cell></row><row><cell cols="2">2: for k = 1, ..., K do</cell><cell>Run the minimax procedure K times</cell></row><row><cell>3:</cell><cell>for t = 1, ..., T min do</cell></row><row><cell>4:</cell><cell cols="2">Sample (X t , Y t ) uniformly from dataset</cell></row><row><cell>5:</cell><cell></cell></row><row><cell></cell><cell>minimize θ∈Θ</cell><cell>sup</cell></row></table><note><p><p><p><p><p><p><p><p>P</p>{E P [ (θ; (X, Y ))] : D θ (P, P 0 ) ≤ ρ} .</p>Computationally, the above supremum over probability distributions is intractable. Hence, we consider the following Lagrangian relaxation with penalty parameter γ minimize θ∈Θ sup</p>P {E P [ (θ; (X, Y ))] -γD θ (P, P 0 )} .</p>(4)</p>Algorithm 1</p>θ ← θ -α∇ θ (θ; (X t , Y t )) 6:</p>Sample {X i , Y i } i=1,...,n uniformly from the dataset 7:</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>By semantic space we mean learned representations since recent works<ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b15">16]</ref> suggest that distances in the space of learned representations of high capacity models typically correspond to semantic distances in visual space.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>Models were implemented using Tensorflow, and training procedures were performed on NVIDIA GPUs. Code is available at https://github.com/ricvolpi/generalize-unseen-domains</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>† Work done while author was a Visiting Student Researcher at Stanford University. 32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montréal, Canada.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Analysis of representations for domain adaptation</title>
		<author>
			<persName><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 19</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hoffman</surname></persName>
		</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="137" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Quantifying distributional model risk via optimal transport</title>
		<author>
			<persName><forename type="first">Jose</forename><surname>Blanchet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthyek</forename><surname>Murthy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.01446</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>math.PR</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Domain adaptation with structural correspondence learning</title>
		<author>
			<persName><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;06</title>
		<meeting>the 2006 Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;06<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="120" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Perturbation analysis of optimization problems</title>
		<author>
			<persName><forename type="first">Frédéric</forename><surname>Bonnans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Shapiro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lieven</forename><surname>Vandenberghe</surname></persName>
		</author>
		<title level="m">Convex Optimization</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Graf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Baird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<title level="m">Advances in neural information processing systems 1. chapter Neural Network Recognizer for Hand-written Zip Code Digits</title>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="323" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generating images with perceptual similarity metrics based on deep networks</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2008/workshop/index.html" />
		<title level="m">The PASCAL Visual Object Classes Challenge 2008 (VOC2008) Results</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning, ICML 2015</title>
		<meeting>the 32nd International Conference on Machine Learning, ICML 2015<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07-11">6-11 July 2015. 2015</date>
			<biblScope unit="page" from="1180" to="1189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno>CoRR, abs/1512.03385</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Conditional variance penalties and domain shift robustness</title>
		<author>
			<persName><forename type="first">Christina</forename><surname>Heinze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Deml</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolai</forename><surname>Meinshausen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.11469</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A baseline for detecting misclassified and out-of-distribution examples in neural networks</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno>CoRR, abs/1610.02136</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Fcns in the wild: Pixel-level adversarial and constraint-based adaptation</title>
		<author>
			<persName><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno>CoRR, abs/1612.02649</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Domain adaptation for statistical classifiers</title>
		<author>
			<persName><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<idno>CoRR, abs/1109.6341</idno>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno>CoRR, abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><forename type="middle">E</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donnie</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">E</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Minimax statistical learning and domain adaptation with wasserstein distances</title>
		<author>
			<persName><forename type="first">Jaeho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Raginsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07815</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A method for the solution of certain problems in least squares</title>
		<author>
			<persName><surname>Levenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">quart. appl. math</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="1944">1944</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Deeper, broader and artier domain generalization</title>
		<author>
			<persName><forename type="first">Da</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<idno>CoRR, abs/1710.03077</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno>CoRR, abs/1411.4038</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Robust place categorization with deep domain generalization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mancini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Bulò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2093" to="2100" />
			<date type="published" when="2018-07">July 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An algorithm for least-squares estimation of nonlinear parameters</title>
		<author>
			<persName><forename type="first">Donald</forename><forename type="middle">W</forename><surname>Marquardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the society for Industrial and Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="431" to="441" />
			<date type="published" when="1963">1963</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Minimal-entropy correlation alignment for unsupervised deep domain adaptation</title>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Morerio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacopo</forename><surname>Cavazza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vittorio</forename><surname>Murino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unified deep supervised domain adaptation and generalization</title>
		<author>
			<persName><forename type="first">Saeid</forename><surname>Motiian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Piccirilli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><forename type="middle">A</forename><surname>Adjeroh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gianfranco</forename><surname>Doretto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10">Oct 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Domain generalization via invariant feature representation</title>
		<author>
			<persName><forename type="first">Krikamol</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">Sanjoy</forename><surname>Dasgupta</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">David</forename><surname>Mcallester</surname></persName>
		</editor>
		<meeting>the 30th International Conference on Machine Learning<address><addrLine>Atlanta, Georgia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06">Jun 2013</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="17" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cubic regularization of newton method and its global performance</title>
		<author>
			<persName><forename type="first">Yurii</forename><surname>Nesterov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><forename type="middle">T</forename><surname>Polyak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="177" to="205" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Deep Learning and Unsupervised Feature Learning</title>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName><forename type="first">German</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06">June 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Adapting visual category models to new domains</title>
		<author>
			<persName><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th European Conference on Computer Vision: Part IV, ECCV&apos;10</title>
		<meeting>the 11th European Conference on Computer Vision: Part IV, ECCV&apos;10<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="213" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Generalizing across domains via cross-gradient training</title>
		<author>
			<persName><forename type="first">Shiv</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vihari</forename><surname>Piratla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumen</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhartha</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preethi</forename><surname>Jyothi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunita</forename><surname>Sarawagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Certifiable distributional robustness with principled adversarial training</title>
		<author>
			<persName><forename type="first">Aman</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongseok</forename><surname>Namkoong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2014-01">January 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep CORAL: correlation alignment for deep domain adaptation</title>
		<author>
			<persName><forename type="first">Baochen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Domain randomization for transferring deep neural networks from simulation to the real world</title>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Tobin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<idno>CoRR, abs/1703.06907</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unbiased look at dataset bias</title>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 IEEE Conference on Computer Vision and Pattern Recognition, CVPR &apos;11</title>
		<meeting>the 2011 IEEE Conference on Computer Vision and Pattern Recognition, CVPR &apos;11<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1521" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07">July 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Adversarial feature augmentation for unsupervised domain adaptation</title>
		<author>
			<persName><forename type="first">Riccardo</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Morerio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vittorio</forename><surname>Murino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06">June 2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
