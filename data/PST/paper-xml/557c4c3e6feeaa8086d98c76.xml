<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Segmentation of Blood Vessels and Optic Disc in Retinal Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ana</forename><surname>Salazar-Gonzalez</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Djibril</forename><surname>Kaba</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yongmin</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xiaohui</forename><surname>Liu</surname></persName>
						</author>
						<title level="a" type="main">Segmentation of Blood Vessels and Optic Disc in Retinal Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BDB27B77D2AEC4BA04C8E3F06394782A</idno>
					<idno type="DOI">10.1109/JBHI.2014.2302749</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/JBHI.2014.2302749, IEEE Journal of Biomedical and Health Informatics</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Retinal images</term>
					<term>vessel segmentation</term>
					<term>optic disc segmentation</term>
					<term>graph cut segmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Retinal image analysis is increasingly prominent as a non-intrusive diagnosis method in modern ophthalmology. In this paper, we present a novel method to segment blood vessels and optic disc in the fundus retinal images. The method could be used to support non-intrusive diagnosis in modern ophthalmology since the morphology of the blood vessel and the optic disc is an important indicator for diseases like diabetic retinopathy, glaucoma and hypertension. Our method takes as first step the extraction of the retina vascular tree using the graph cut technique. The blood vessel information is then used to estimate the location of the optic disc. The optic disc segmentation is performed using two alternative methods. The Markov Random Field (MRF) image reconstruction method segments the optic disc by removing vessels from the optic disc region and the Compensation Factor method segments the optic disc using prior local intensity knowledge of the vessels. The proposed method is tested on three public data sets, DIARETDB1, DRIVE and STARE. The results and comparison with alternative methods show that our method achieved exceptional performance in segmenting the blood vessel and optic disc.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>T HE Segmentation of retinal image structures has been of great interest because it could be used as a non-intrusive diagnosis in modern ophthalmology. The morphology of the retinal blood vessel and the optic disc is an important structural indicator for assessing the presence and severity of retinal diseases such as diabetic retinopathy, hypertension, glaucoma, haemorrhages, vein occlusion and neo-vascularisation. However to assess the diameter and tortuosity of retinal blood vessel or the shape of the optic disc, manual planimetry has commonly been used by ophthalmologist, which is generally time consuming and prone with human error, especially when the vessel structure are complicated or a large number of images are acquired to be labelled by hand. Therefore, a reliable automated method for retinal blood vessel and optic disc segmentation, which preserves various vessel and optic disc characteristics is attractive in computer aided-diagnosis.</p><p>An automated segmentation and inspection of retinal blood vessel features such as diameter, colour and tortuosity as well as the optic disc morphology allows ophthalmologist and eye care specialists to perform mass vision screening exams for early detection of retinal diseases and treatment evaluation. This could prevent and reduce vision impairments; age related diseases and many cardiovascular diseases as well as reducing the cost of the screening.</p><p>Over the past few years, several segmentation techniques have been employed for the segmentation of retinal structures such as blood vessels and optic disc and diseases like lesions in fundus retinal images. However the acquisition of fundus retinal images under different conditions of illumination, resolution and field of view (FOV) and the overlapping tissue in the retina cause a significant degradation to the performance of automated blood vessel and optic disc segmentations. Thus, there is a need for a reliable technique for retinal vascular tree extraction and optic disc detection, which preserves various vessel and optic disc shapes. In the following segment, we briefly review the previous studies on blood vessel segmentation and optic disc segmentation separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head><p>Two different approaches have been deployed to segment the vessels of the retina: the pixel processing based methods and tracking based methods <ref type="bibr" target="#b0">[1]</ref>.</p><p>The pixel processing based approach performs the vessel segmentation in a two-pass operation. First the appearance of the vessel is enhanced using detection process such as morphological pre-processing techniques and adaptive filtering. The second operation is the recognition of the vessel structure using thinning or branch point operations to classify a pixel as a vessel or background. These approaches process every pixel in the image and apply multiple operations on each pixel. Some pixel processing methods use neutral networks and frequency analysis to define pixels in the image as vessel pixels and background pixels. Typical pixel processing operations are shown in Hoover et al. <ref type="bibr" target="#b1">[2]</ref>, Mendoca et al. <ref type="bibr" target="#b2">[3]</ref>, Soares et al. <ref type="bibr" target="#b3">[4]</ref>, Staal et al. <ref type="bibr" target="#b4">[5]</ref>, Chaudhuri et al. <ref type="bibr" target="#b5">[6]</ref> and Zana et al. <ref type="bibr" target="#b6">[7]</ref>.</p><p>The second set of approaches to vessel segmentation are referred to as vessel tracking, vectorial tracking or tracing <ref type="bibr" target="#b0">[1]</ref>. In contrast to the pixel processing based approaches, the tracking methods detect first initial vessel seed points, and then track the rest of the vessel pixels through the image by measuring the continuity proprieties of the blood vessels. This technique is used as a single pass operation, where the detection of the vessel structures and the recognition of the structures are simultaneously performed.</p><p>The tracking based approaches included semi automated tracing and automated tracing. In the semi automated tracing methods, the user manually selects the initial vessel seed point. These methods are generally used in quantitative coronary angiography analysis and they generally provide accurate segmentation of the vessels. In fully automated tracing, the algorithms automatically select the initial vessel points and most methods use Gaussian functions to characterise a vessel profile model, which locates a vessel points for the vessel tracing. They are computationally efficient and more suitable for retinal image processing. Examples of the tracking based approaches are presented in Xu et al. <ref type="bibr" target="#b7">[8]</ref>, Maritiner-perez et al. <ref type="bibr" target="#b8">[9]</ref>, Staal et al. <ref type="bibr" target="#b4">[5]</ref>, Zhou et al <ref type="bibr" target="#b9">[10]</ref>.</p><p>Both pixel processing and tracking approaches have their own advantages and limitations over each other. The pixel processing approaches can provide a complete extraction of the vascular tree in the retinal image since they search all the possible vessel pixels across the whole image. However these techniques are computationally expensive and require special hardware to be suitable for large image dataset. The presence of noise and lesions in some retinal images causes a significant degradation in the performance of the pixel processing approaches as the enhancement operation may pick up some noise and lesions as vessel pixels. This could lead to false vessel detection in the recognition operation. On the other hand, the tracking approaches are computationally efficient and much faster than the pixels processing methods because they perform the vessel segmentation using only the pixels in the neighbourhood of the vessels structure and avoid the processing of every pixel in the image. Nevertheless, these methods lack in extracting a complete vascular tree in the case where there are discontinuities in the vessel branches. Further more, the semi automated tracking segmentation methods need manual input, which requires time.</p><p>The optic nerve head is described as the brightest round area in the retina where the blood vessels converge with a shape that is approximately elliptical and has a width of 1.8 ± 0.2 mm and height 1.9 ± 0.2 mm <ref type="bibr" target="#b10">[11]</ref>. The convergence feature of blood vessels into the optic disc region is generally used to estimate the location of the optic disc and segment it from the retinal image. But the intrusion of vessels in the optic disc region constitutes computational complexity for the optic disc segmentation as it is breaking the continuity of its boundary. To address this problem, several methods have been employed such as Chrastek et al. <ref type="bibr" target="#b11">[12]</ref>, Lowell et al. <ref type="bibr" target="#b12">[13]</ref>, Welfer et al. <ref type="bibr" target="#b13">[14]</ref> and Aquino et al. <ref type="bibr" target="#b14">[15]</ref> . Chrastek et al. <ref type="bibr" target="#b11">[12]</ref> presented an automated segmentation of the optic nerve head for diagnosis of glaucoma. The method removes the blood vessel by using a distance map algorithm, then the optic disc is segmented by combining a morphological operation, Hough Transform and an anchored active contour model. Lowell et al. <ref type="bibr" target="#b12">[13]</ref> proposed a deformable contour model to segment the optic nerve head boundary in low resolution retinal images. The approach localises the optic disc using a specialised template matching and a directionally-sensitive gradient to eliminate the obstruction of the vessel in the optic disc region before performing the segmentation. Welfer et al. <ref type="bibr" target="#b13">[14]</ref> proposed an automated segmentation of the optic disc in colour eye fundus image using an adaptive morphological operation. The method uses a watershed transform markers to define the optic disc boundary and the vessel obstruction is minimised by morphological erosion.</p><p>These techniques are performed using morphological operations to eliminate the blood vessels from the retinal image. However, the application of morphological operations can modify the image by corrupting some useful information.</p><p>In our optic disc segmentation process, the convergence feature of vessels into the optic disc region is used to estimate its location. We then use two automated methods (Markov Random field image reconstruction and Compensation Factor) to segment the optic disc.</p><p>The rest of the paper is organised as follow. The blood vessel segmentation is discussed in Section III. Section IV provides the detailed description of the optic disc segmentation. Section V presents the experimental results of our method with comparisons to other methods. Conclusions are drawn in Section VI. The preliminary results of the three components of the approach, namely the blood vessel segmentation, optic disc segmentation using the Graph Cut and Markov Random Field respectively, were presented separately in <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>. More details of the approach can be found in the PhD thesis <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. BLOOD VESSELS SEGMENTATION</head><p>Blood vessels can be seen as thin elongated structures in the retina, with variation in width and length. In order to segment the blood vessel from the fundus retinal image, we have implemented a pre-processing technique, which consists of effective adaptive histogram equalisation (AHE) and robust distance transform. This operation improves the robustness and the accuracy of the graph cut algorithm. Fig. <ref type="figure" target="#fig_0">1</ref> shows the illustration of the vessel segmentation algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Pre-processing</head><p>We apply a contrast enhancement process to the green channel image similar to the work presented in <ref type="bibr" target="#b19">[20]</ref>. The intensity of the image is inverted, and the illumination is equalised. The resulting image is enhanced using an adaptive histogram equaliser, given by: </p><formula xml:id="formula_0">I Enhanced =   p ∈R(p) s (I (p) -I (p )) h 2   r • M (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where I is the green channel of the fundus retinal colour image, p denotes a pixel and p is the neighbourhood pixel around p. p ∈ R(p) is the square window neighbourhood with length h. s(d) = 1 if d &gt; 0, and s(d) = 0 otherwise with d = s (I (p) -I (p )). M = 255 value of the maximum intensity in the image. r is a parameter to control the level of enhancement. Increasing the value of r would also increase the contrast between vessel pixels and the background (see Fig. <ref type="figure" target="#fig_1">2</ref>). The experimental values of the window length was set to h = 81 and r = 6.</p><p>A binary morphological open process is applied to prune the enhanced image, which discards all the misclassified pixels see (Fig. <ref type="figure" target="#fig_1">2 (d)</ref>). This approach significantly reduces the false positive, since the enhanced image will be used to construct the graph for segmentation.</p><p>A distance map image is created using the distance transform algorithm. This is used to calculate the direction and magnitude of the vessel gradient. Fig. <ref type="figure" target="#fig_1">2 (e)</ref> and<ref type="figure">(f)</ref> show the distance map of the whole image and a sample vessel with arrows indicating the direction of the gradients respectively. From the sample vessel image, we can see the centre line with the brightest pixels, which are progressively reduced in intensity in the direction of the edges (image gradients). The arrows in Fig. <ref type="figure" target="#fig_1">2</ref> (f) referred as vector field, which is used to construct the graph in the next Sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Graph construction for vessel segmentation</head><p>The graph cut is an energy based object segmentation approach. The technique is characterised by an optimisation A graph G (ν, ) is defined as a set of nodes (pixels) ν and a set of undirected edges that connect these neighbouring nodes. The graph included two special nodes, a foreground terminal (source S) and a background terminal (sink T). includes two types of undirected edges: neighbourhood links (n-links) and terminal links (t-links). Each pixel p ∈ P (a set of pixels) in the graph presents two t-links {p, S} and {p, T } connecting it to each terminal while a pair of neighbouring pixels {p, q} ∈ N (number of pixel neighbour) is connect by a n-links <ref type="bibr" target="#b20">[21]</ref>. Thus:</p><formula xml:id="formula_2">= N p∈P {{p, S}, {p, T }, ν = P ∪ {S, T }}<label>(2)</label></formula><p>An edge e ∈ is assigned a weight (cost) W e &gt; 0. A cut is defined by a subset of edges C ∈ where G (c) = ν, \C separating the graph into two foreground and background with C defined as |C| = e∈C W e The Max-Flow algorithm is used to cut the graph and find the optimal segmentation. Table <ref type="table">I</ref> assigns weight to the edges in the graph <ref type="bibr" target="#b20">[21]</ref>. </p><formula xml:id="formula_3">λ • Rp(F g) p ∈ P, p / ∈ F ∪ B K p ∈ F 0 p ∈ B { p, T} (Background) λ • Rp(Bg) p ∈ P, p / ∈ F ∪ B 0 p ∈ F K p ∈ B where K = 1 + max p∈P {p,q} B p,q<label>(3)</label></formula><p>F and B represent the subsets of pixels selected as foreground and background respectively. Thus F ⊂ P and B ⊂ P such that F ∩ B = ø. B p,q defines the discontinuity between neighbouring pixels, and its value is large when the pixel intensities. λ &gt; 0 is a constant coefficient, which we will define in the energy formulation of the graph.</p><p>The graph cut technique is used in our segmentation because it allows the incorporation of prior knowledge into the graph formulation in order to guide the model and find the optimal segmentation. Let assume A = (A 1 , A p , . . . A P ) a binary vector set of labels assigned to each pixel p in the image, where A p indicate assignments to pixels p in P . Therefore, each assignment A p is either in foreground (F g) or background (Bg). Thus the segmentation is obtained by the binary vector A and the constraints imposed on the regional and boundary proprieties of vector A are derived by the energy formulation of the graph defined as</p><formula xml:id="formula_4">E (A) = λ • R (A) + B (A)<label>(4)</label></formula><p>where the positive coefficient λ indicates the relative importance of the regional term (likelihoods of foreground and background) R A against the boundary term (relationship between neighbourhood pixels) B A . The regional or the likelihood of the foreground and background is given by</p><formula xml:id="formula_5">R (A) = p∈P R p (A p )<label>(5)</label></formula><p>and the boundary constraints is defined as</p><formula xml:id="formula_6">B (A) = p,q∈N B p,q • φ (A p , A q )<label>(6)</label></formula><p>where φ (A p , A q ) = 1 for A p = A q and 0 Otherwise.</p><formula xml:id="formula_7">B p,q = exp(- (I p -Iq) 2 2σ 2 ) • 1 dist(p, q)<label>(7)</label></formula><p>R p (A p ) specifies the assignment of pixel p to either the foreground (F g) or the background (Bg). B p,q defines the discontinuity between neighbouring pixels, and its value is large when the pixel intensities I p and I q are similar and close to zero when they different. The value of B p,q is also affected by the Euclidean distance dist(p, q) between pixels p and q.</p><p>During the minimisation of the graph energy formulation in (4) to segment thin objects like blood vessels, the second term (boundary term) in (4) has a tendency to follow short edges known as "the shrinking bias" <ref type="bibr" target="#b21">[22]</ref>. This problem causes a significant degradation on the performance of the graph cut algorithm on thin elongated structures like the blood vessels. Fig. <ref type="figure" target="#fig_2">3</ref> shows an example of the blood vessel segmentation using the traditional graph formulation <ref type="bibr" target="#b22">[23]</ref>. From Fig. <ref type="figure" target="#fig_2">3</ref>, it can be seen that the blood vessel segmentation follows short edges, and tends to shrink in the searching for the cheapest cost. It can also be noticed that λ in (4) controls the relation between boundary and regional terms. Increasing the value of λ, the likelihood of the pixels belonging to foreground and background (t-links) gains strength over the regional term (n-links), which slightly improved the segmentation result see Fig. <ref type="figure" target="#fig_2">3 (d)</ref>. To address the above problem, the segmentation of blood vessels using the graph cut requires special graph formulation. One of the method used to address the shrinking bias problem is to impose an additional connectivity prior, where the user marks the constrain connectivity <ref type="bibr" target="#b21">[22]</ref>. In order to achieve full automated segmentation, we used the method presented in <ref type="bibr" target="#b22">[23]</ref>, which overcomes the "the shrinking bias" by adding the mechanism of vectors flux into the construction of the graph. The incorporation of vectors flux can improve edge alignment and allows the segmentation of thin objects like blood vessels by keeping a balance between shrinking (length) and stretching (vectors flux) along the boundary. Fig. <ref type="figure" target="#fig_3">4</ref> shows flux of vectors v passing through a given surface S. Our method takes the image gradients of rough blood vessels from the pre-processing step as vectors v (see Fig. <ref type="figure" target="#fig_1">2 (f</ref>)), and the flux (magnitude, and direction) of these vectors is incorporated into the graph construction and optimised. Thus the shrinking effect of the minimization energy on the boundary term is equilibrated with the spreading effect of vectors v flux. It is been shown in <ref type="bibr" target="#b22">[23]</ref> that the class of Finsler metrics can described geometric proprieties of the discrete cut metric on regular grids and Finsler length can be represented by the sum of two terms. Those terms represent the symmetric and anti-symmetric parts of the cut metric. The symmetric part of the cut defines the standard geometric length of contour and it is independent of its orientation. The anti-symmetric part of the cut metric represents the flux of a given vector field through the contour <ref type="bibr" target="#b22">[23]</ref>.</p><p>To address "the shrinking bias" problem seen in Fig. <ref type="figure" target="#fig_2">3</ref>, we have constructed a graph consisting of a symmetric part g + (shrinking) and an anti-symmetric part g -(stretching) by incorporating the flux of vector v into the graph construction. The symmetric part g + of the graph corresponds to a cut geometric length and is related directly with the n-link connections and the anti-symmetric part g -is equal to flux of vector field v over the cut geometric and it is used to derive the t-links. Thus the the blood vessels can be segment by keeping a good balance between shrinking and stretching (flux) throughout the image boundary.</p><p>1) The symmetric part of the graph: is used to assign weights on the n-link connections (edges between neighbouring pixels). Let consider a neighbour system of a graph described by a set of edges e k , where 1 ≤ k ≤ N , for N number of neighbours. Let us define e k as the shortest vector connecting two pixels in the direction of k, W + k (p) the weight of the edge e k at pixel p and W + k (p) a set of the edge weights at pixel p for all directions. The corresponding edge weights are defined by</p><formula xml:id="formula_8">ω + = 1 2 D × g + (<label>8</label></formula><formula xml:id="formula_9">)</formula><p>where D is a N x N matrix with entries defined as</p><formula xml:id="formula_10">D ii = - sin(α i+1 -α i-1 ) sin(α i+1 -α i )sin(α i -α i-1 )<label>(9)</label></formula><formula xml:id="formula_11">D ij = 1 sin(αj -αi) if j + 1 ± 1 mod N 0 f or other entries</formula><p>where α k is the angle of the edge e k with respect to the positive axis X (see Fig. <ref type="figure" target="#fig_4">5</ref>).</p><p>In our implementation, we consider a grid map of 16 neighbours with edges e k , k = 1, 2, ..., 16 (see Fig. <ref type="figure" target="#fig_4">5</ref>). For each pixel p in the green channel image, the edge weight W + k (p) is computed according to (8). g + is calculated using the pixel intensity difference between two given nodes by:</p><formula xml:id="formula_12">g + = K • exp -(I p -I q ) 2 σ 2<label>(10)</label></formula><p>g + has a high value for pixels of similar intensities, when I p -I q &lt; σ. However if the pixels are very different I p -I q &gt; σ the value of g + is small, which represents a poor relation between the pixels, hence they belong to different terminals <ref type="bibr" target="#b23">[24]</ref>. 2) The anti-symmetric part of the graph : We used the term Anti-Symmetry because, the flux (stretching) of vector field v over the cut geometric balanced the shrinking of blood vessels during the segmentation. This anti-symmetric part of the graph is defined by the flux of vector field v over the cut geometric. It is used to assign weights on the t-links (edges between a given pixel and the terminals) to balance the shrinking effect seen in Fig. <ref type="figure" target="#fig_2">3</ref>. Specific weights for t-links are obtained based on the deposition of vector v. Different decompositions of vectorv may result in different t-links whose weights can be interpreted as an estimation of divergence. In our implementation, we decomposed the vector v along grid edges with the n-links oriented along the main axes, X and Y direction. Thus vector v can be decomposed as v = v x u x + v y u y where u x andu y are unit vectors in X and Y direction respectively. This decomposition leads to the t-link weights defined as</p><formula xml:id="formula_13">t p = δ 2 2 [ v right x -v lef t x + v up y -v down x ]<label>(11)</label></formula><p>where v right x and v right x are the components of vector v in X direction taken at the right and left neighbour of pixel P respectively. v up y and v down y are the Y of vector v taken at the top and down of of pixel P . δ is the size of the cell in the grid map (see Fig. <ref type="figure" target="#fig_4">5</ref>). We add edge (s → p) with weight C * (-tp) if tp &lt; 0, or edge (p → t) with weight C * tp otherwise. The parameter C is related to the magnitude of the vector v, thus pixels in the centre of the blood vessel have a higher connection to the source (foreground) than pixels in the edge of the blood vessels. Because the distance map is calculated on the pruned image and vector v is only defined for the pixels detected as blood vessels in the rough segmentation. For the rest of the pixels in the image, the initialisation of t-link weights is set as (p → s) with weight t = 0 and (p → t) with weight t = K, where K is the maximum weight sum for a pixel in the symmetric construction. Fig. <ref type="figure" target="#fig_5">6</ref> shows the segmentation results of the blood vessels using different decomposition of the vector v generating different t-link weights. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. OPTIC DISC SEGMENTATION</head><p>The optic disc segmentation starts by defining the location of the optic disc. This process used the convergence feature of vessels into the optic disc to estimate its location. The disc area is then segmented using two different automated methods (Markov Random field image reconstruction and Compensation Factor). Both methods use the convergence feature of the vessels to identify the position of the disc. The Markov Random Field (MRF) method is applied to eliminate the vessel from the optic disc region. This process is known as image reconstruction and it is performed only on the vessel pixels to avoid the modification of other structures of the image. The reconstructed image is free of vessel and it is used to segment the optic disc via graph cut. In contrast to MRF method, the Compensation Factor approach segments the optic disc using prior local intensity knowledge of the vessels. Fig. <ref type="figure" target="#fig_6">7</ref> shows the overview of both the MRF and the Compensation Factor method process. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Optic Disc Location</head><p>Inspired by the method proposed in <ref type="bibr" target="#b13">[14]</ref>, which effectively locates the optic disc using the vessels. we use the binary image of vessels segmented in Section III to find the location of the optic disc. The process iteratively trace towards the centroid of the optic disc. The vessel image is pruned using a morphological open process to eliminate thin vessels and keep the main arcade. The centroid of the arcade is calculated using the following formulation:</p><formula xml:id="formula_14">C x = K i=1 x i K C y = K i=1 y i K (<label>12</label></formula><formula xml:id="formula_15">)</formula><p>where x i and y i are the coordinates of the pixel in the binary image and K is the number of pixels set to 1 (pixels marked as blood vessels) in the binary image.</p><p>Given the gray scale intensity of a retinal image, we select 1% of the brightest region. The algorithm detects the brightest region with the most number of pixels to determine the location of the optic disc with respect to the centroid point (right, left, up or down). The algorithm adjusts the centroid point iteratively until it reaches the vessel convergence point or centre of the main arcade (centre of the optic disc) by reducing the distance from one centroid point to next one in the direction of the brightest region, and correcting the central position inside the arcade accordingly. Fig. <ref type="figure" target="#fig_7">8</ref> shows the process of estimating the location of the of optic disc in a retinal image. It is important to notice that, the vessel convergence point must be detected accurately, since this point is used to automatically mark foreground seeds. A point on the border of the optic disc may result in some false foreground seeds. After the detection of the vessel convergence point, the image is constrained a region of interest (ROI) including the whole area of the optic disc to minimize the processing time. This ROI is set to a square of 200 by 200 pixels concentric with the detected optic disc centre. Then an automatic initialisation of seeds (foreground and background) for the graph is performed. A neighbourhood of 20 pixels of radius around the centre of the optic disc area is marked as foreground pixels and a band of pixels around the perimeter of the image are selected as background seeds (see Fig. <ref type="figure" target="#fig_8">9</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Optic Disc Segmentation with Markov Random Field Image Reconstruction</head><p>The high contrast of blood vessels inside the optic disc presented the main difficulty for it segmentation as it misguides the segmentation through a short path, breaking the continuity of the optic disc boundary. To address this problem, the MRF based reconstruction method presented in <ref type="bibr" target="#b24">[25]</ref> is adapted in our work. We have selected this approach because of its robustness. The objective of our algorithm is to find a best match for some missing pixels in the image, however one of the weaknesses of MRF based reconstruction is the requirement of intensive computation. To overcome this problem, we have limited the reconstruction to the region of interest (ROI) and using prior segmented retina vascular tree, the reconstruction was performed in the ROI. An overview diagram of the optic disc segmentation with Markov Random Field Image Reconstruction is shown in Fig. <ref type="figure" target="#fig_6">7</ref>.  Let us consider a pixel neighbourhood w(p) define as a square window of size W , where pixel p is the centre of the neighbourhood . I is the image to be reconstructed and some of the pixels in I are missing. Our objective is to find the best approximate values for the missing pixels in I. So let d(w1, w2) represent a perceptual distance between two patches that defines their similarity. The exact matching patch corresponds to d(w , w(p)) = 0. If we define a set of these patches as Ω(p) = {ω ⊂ I : d(ω , ω(p)) = 0} the probability density function of p can be estimated with a histogram of all centre pixel values in Ω(p). However we are considering a finite neighbourhood for p and the searching is limited to the image area, there might not be any exact matches for a patch. For this reason, we find a collection of patches, which match falls between the best match and a threshold. The closest match is calculated as ω best = argmin ω d(ω(p), ω) ⊂ I. All the patches ω with d(ω(p), ω) &lt; (1 + )d(ω(p), ω best ) are included in the collection ω . d(w , w(p)) is defined as the sum of the absolute differences of the intensities between patches, so identical patches will result in d(w , w(p)) = 0. Using the collection of patches, we create an histogram and select the one with the highest mode. Fig. <ref type="figure" target="#fig_9">10</ref> shows sample results of the reconstruction. The foreground F g s and the background Bg s seeds are initialised in the reconstructed image, which are then used in graph cut formulation to segment the optic disc. Similar to the (Fig. <ref type="figure" target="#fig_8">9</ref>), the initialisation of the foreground F g s and background Bg s seeds is performed using the reconstructed image. The graph cut algorithm descripted in section III-B is used to separate the foreground and the background by minimising the energy function over the graph and producing the optimal segmentation of the optic disc in the image. The energy function of the graph in (4) consists of regional and boundary terms. The regional term (likelihoods of foreground and background) is calculated using (5), while the boundary term (relationship between neighbouring pixels) is derived using <ref type="bibr" target="#b5">(6)</ref>. A grid of 16 neighbours N is selected to create links between pixels in the image Im. The Max-Flow algorithm is used to cut the graph and find the optimal segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Optic Disc Segmentation With Compensation Factor</head><p>In contrast to MRF image reconstruction, we have incorporated the blood vessels into the graph cut formulation by introducing a compensation factor V ad. This factor is derived using prior information of blood vessel.</p><p>The energy function of the graph cut algorithm generally comprises a boundary and regional terms. The boundary term defined in (6) is used to assign weights on the edges (nlinks) to measure the similarity between neighbouring pixels with respect to the pixel proprieties (intensity, texture, colour). Therefore pixels with similar intensities have a strong connection. The regional term in (5) is derived to define the likelihood of the pixel belonging to the background or the foreground by assigning weights on the edges (t-link) between image pixels and the two terminals background and foreground seeds. In order to incorporated the blood vessels into the graph cut formulation, we derived the t-link as follows:</p><formula xml:id="formula_16">S link = -ln P r (I p \F g seeds ) if p = vessel -ln P r (I p \F g seeds ) + V ad if p = vessel<label>(13)</label></formula><p>T link = -ln P r (I p \Bg seeds ) if p = vessel -ln P r (I p \Bg seeds ) if p = vessel <ref type="bibr" target="#b13">(14)</ref> where p is the pixel in the image, F g seeds is the intensity distribution of the foreground seeds, Bg seeds represents the intensity distribution of the background seeds and V ad is the compensation factor given as:</p><p>V ad = max p∈vessel {-ln P r (I p \Bg seeds )}</p><p>The intensity distribution of the blood vessel pixels in the region around the optic disc makes them more likely to belong to background pixels than the foreground (or the optic disc pixels). Therefore the vessels inside the disc have weak connections with neighbouring pixels making them likely to be segmented by the graph cut as background. We introduce in (13) a compensation vector to all t-links of the foreground for pixels belong to the vascular tree to address this behaviour. Consequently, vessels inside the optic disc are classified with respect to their neighbourhood connections instead of their likelihood with the terminals foreground and background seeds. Fig. <ref type="figure" target="#fig_10">11</ref> shows sample of images segmented by Compensation Factor. The segmentation of the disc is affected by the value of V ad, the method achieves poor segmentation results for low value of V ad. However when the value of the V ad increases, the performance improves until the value of V ad is high enough to segment the rest of the vessels as foreground. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. RESULTS</head><p>For the vessel segmentation method, we tested our algorithm on two public datasets, DRIVE <ref type="bibr" target="#b4">[5]</ref>, STARE <ref type="bibr" target="#b1">[2]</ref> with a total of 60 images. The optic disc segmentation algorithm was tested on DRIVE <ref type="bibr" target="#b4">[5]</ref> and DIARETDB1 <ref type="bibr" target="#b25">[26]</ref>, consisting of 129 images in total. The performance of both methods is tested against a number of alternative methods.</p><p>The DRIVE consists of 40 digital images which were captured from a Canon CR5 non-mydriatic 3CCD camera at 45 • field of view (FOV). The images have a size of 768 × 584 pixels. The dataset includes masks to separate the FOV from the rest of the image. It included two sets hand labelled images (set A and set B) for the blood vessel. The set A offers the manually labelled images for all the images in the dataset, whereas the set B provides the manually labelled images for half of the dataset. To test our method we adopt the set A hand labelling as the benchmark. We manually delimited the optic disc to test the performance of optic disc segmentation algorithm.</p><p>The STARE dataset consists of 20 images captured by a TopCon TRV-50 fundus camera at 35 • FOV. The size of the images is 700 × 605 pixels. We calculated the mask image for this dataset using a simple threshold technique for each colour channel. The STARE dataset included images with retinal diseases selected by Hoover et al <ref type="bibr" target="#b1">[2]</ref>. It also provides two sets of hand labelled images performed by two human experts. The first expert labelled fewer vessel pixels than the second one. To test our method we adopt the first expert hand labelling as the ground truth.</p><p>The DIARETDB1 dataset consist of 89 colour images with 84 of them contain at last one indication of lesion. The images were captured with digital fundus camera at 50 degree filed of view and have a size of 1500 × 1152 pixels. Hand labelled lesion regions are provided in this dataset by four human experts. However the DIARETDB1 dataset only includes the hand labelled ground truth of lesions but not the blood vessels and the optic disc. For this reason, we were unable to compare the performance of the blood vessel segmentation on the DIARETDB1 dataset. Nevertheless we were able to create the hand labelled ground truth of optic disc to test the performance of the optic disc segmentation.</p><p>To facilitate the performance comparison between our method and alternative retinal blood vessels segmentation approaches, parameters such as the true positive rate (TPR), the false positive rate (FPR) and the accuracy rate (ACC) are derived to measure the performance of the segmentation <ref type="bibr" target="#b4">[5]</ref>. The accuracy rate is defined as the sum of the true positives (pixels correctly classified as vessel points) and the true negatives (non-vessel pixels correctly identified as non vessel points), divided by the total number of pixel in the images. True Positive Rate (TPR) is defined as the total number of true positives, divided by the number of blood vessel pixel marked in the ground true image. False Positive Rate (FPR) is calculated as the total number of false positives divided by the number of pixels marked as non-vessel in the ground true image. It is worth mentioning that a perfect segmentation would have a FPR of 0 and a TPR of 1. Our method and all the alternative methods used the first expert hand labelled images as performance reference.</p><p>Most of the alternative methods use the whole image to measure the performance. In <ref type="bibr" target="#b4">[5]</ref> all the experiments are done on the FOV without considering the performance in the dark area outside the FOV. The method in <ref type="bibr" target="#b2">[3]</ref> measures the performance on both the whole image and the FOV. The dark background outside the FOV in the retinal image is easy to segment. It is an advantage in measuring the true negatives pixels when the whole image is considered. We have calculated the percentage of pixels outside the FOV in the images for the two datasets, which represents approximately the 25% of the pixels in the whole image. However, it does not affect all the measurement metrics, except when the true negative value is involved (e.g. accuracy rate). On the other hand, most of the methods use the whole image to measure their performance, making the comparison fair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Results of Blood Vessel Segmentation Algorithm on STARE dataset</head><p>Tables II and III show performance comparison results of our approach with recent alternative methods in terms of TPR, FPR and ACC on STARE dataset. The performance results of the second expert hand labelled and the method of Martinez-Perez et al. <ref type="bibr" target="#b8">[9]</ref> and Staal et al. <ref type="bibr" target="#b4">[5]</ref> are taken from <ref type="bibr" target="#b8">[9]</ref>. The results of the methods proposed by Mendonca et al. <ref type="bibr" target="#b2">[3]</ref> and Hoover et al. <ref type="bibr" target="#b1">[2]</ref> are taken from <ref type="bibr" target="#b2">[3]</ref> and the approaches of Chaudhuri et al. <ref type="bibr" target="#b5">[6]</ref>, Kaba et al. <ref type="bibr" target="#b26">[27]</ref> and Marin et al. <ref type="bibr" target="#b27">[28]</ref> and Zhang et al. <ref type="bibr" target="#b28">[29]</ref> were generated from their original manuscripts. The performance of the segmentation results for Zhang et al. <ref type="bibr" target="#b28">[29]</ref>, Chaudhuri et al. <ref type="bibr" target="#b5">[6]</ref> and Soares et al. <ref type="bibr" target="#b29">[30]</ref> on both healthy and unhealthy images were taken from <ref type="bibr" target="#b28">[29]</ref>. The testing includes all the 20 fundus images except the method proposed by <ref type="bibr">Staal [5]</ref> which used 19 out of the 20 (10 healthy and 9 unhealthy) images.</p><p>In Tables II the second human expert hand labelled image is considered as the target performance level with average (T P R = 0.7887) given the first human expert hand labelled as benchmark. Thus our method needs an improvement of 10.64% in average true positive whereas Mendona et al., <ref type="bibr">Staal et</ref>  Considering the value of average TPR as performance measure, our proposed approach reaches better performance than all the other methods. However with the average accuracy rate, our method is only marginally inferior to the methods presented by Staal et al. <ref type="bibr" target="#b4">[5]</ref>, Kaba et al. <ref type="bibr" target="#b26">[27]</ref>, Marin et al. <ref type="bibr" target="#b27">[28]</ref> and Zhang et al. <ref type="bibr" target="#b28">[29]</ref> but as mentioned above, Staal et al. <ref type="bibr" target="#b4">[5]</ref> used 19 of the 20 images. Compared to the methods proposed by Hoover et al. <ref type="bibr" target="#b1">[2]</ref>, Martinez-Perez et al. <ref type="bibr" target="#b8">[9]</ref> and Chaudhuri et al. <ref type="bibr" target="#b5">[6]</ref>, our approach outperforms the accuracy rate of these techniques and it has approximately the same value of Acc as Mendonca et al. <ref type="bibr" target="#b2">[3]</ref>.</p><p>Table III compares the performance of the healthy subject images against the unhealthy subject images on STARE dataset. The results of the experiments show the unhealthy ocular images cause a significant degradation to the performance of automated blood vessels segmentation techniques. An overview of the results shows on both healthy and unhealthy images, our proposed method achieves better overall average TPR performance than all the methods. However the average Acc value is comparable to the performance of Soares et al. <ref type="bibr" target="#b29">[30]</ref> and Zhang et al. <ref type="bibr" target="#b28">[29]</ref>. It outperforms the ACC of Mendonca et al. <ref type="bibr" target="#b2">[3]</ref> , Hoover et al. <ref type="bibr" target="#b1">[2]</ref> and Chaudhuri et al. <ref type="bibr" target="#b5">[6]</ref> on both healthy and unhealthy images.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Results of Blood Vessel Segmentation Algorithm on DRIVE dataset</head><p>The performance of the segmentation our method on DRIVE dataset is compared with alternative methods: Zhang et al. <ref type="bibr" target="#b28">[29]</ref>, Soares et al. <ref type="bibr" target="#b29">[30]</ref>, Zana et al. <ref type="bibr" target="#b6">[7]</ref>, Garg et al. <ref type="bibr" target="#b30">[31]</ref>, Perfetti et al. <ref type="bibr" target="#b31">[32]</ref>, Al-Rawi et al. <ref type="bibr" target="#b32">[33]</ref> taken from <ref type="bibr" target="#b28">[29]</ref>. The results of the second human expert B and the method proposed by Niemeijer et al. <ref type="bibr" target="#b33">[34]</ref>, Mendonca et al. <ref type="bibr" target="#b2">[3]</ref> and Staal et al. <ref type="bibr" target="#b4">[5]</ref> were acquired from <ref type="bibr" target="#b2">[3]</ref>. Cinsdikici et al. <ref type="bibr" target="#b34">[35]</ref> and Jiang et al. <ref type="bibr" target="#b35">[36]</ref> were generated from Marin et al. <ref type="bibr" target="#b27">[28]</ref> and finally Ricci et al. <ref type="bibr" target="#b36">[37]</ref>, Soares et al. <ref type="bibr" target="#b29">[30]</ref>and Martinez-Perez et al. <ref type="bibr" target="#b8">[9]</ref> were used from their orignal manuscripts.</p><p>The second human expert B hand labelled <ref type="bibr" target="#b2">[3]</ref> is considered as the target performance level with average (T P R = 0.7761andACC = 0.9473) given the first human expert A hand labelled as reference (benchmark). Tables IV shows the performance of our method against the above methods on DRIVE dataset. Our method needs an overall improvement of 2.49% and 0.61% in average true positive rate and average accuracy rate respectively.</p><p>Whereas with an average TPR rate of 0.7512, our method achieves better performance than all the other methods with respect to the average TPR value. The average accuracy achieved with our approach on DRIVE outperforms Jiang et al. <ref type="bibr" target="#b35">[36]</ref>, Cinsdikici et al. <ref type="bibr" target="#b34">[35]</ref>, Zana et al. <ref type="bibr" target="#b6">[7]</ref>, Garg et al. <ref type="bibr" target="#b30">[31]</ref>, Zhang et al. <ref type="bibr" target="#b28">[29]</ref> and Martinez et al. <ref type="bibr" target="#b8">[9]</ref>. But it is marginally inferior to the methods proposed by Al-Rawi et al. <ref type="bibr" target="#b32">[33]</ref>, Ricci et al. <ref type="bibr" target="#b36">[37]</ref> and Mendonca et al. <ref type="bibr" target="#b2">[3]</ref> and it is comparable to Soares et al. <ref type="bibr" target="#b29">[30]</ref>, Marin et al. <ref type="bibr" target="#b27">[28]</ref>, Niemeijer et al. <ref type="bibr" target="#b33">[34]</ref> and Staal et al. <ref type="bibr" target="#b4">[5]</ref>.</p><p>It is important to note that the methods presented by Ricci et al. <ref type="bibr" target="#b36">[37]</ref>, Soares et al. <ref type="bibr" target="#b29">[30]</ref>, Marin et al. <ref type="bibr" target="#b27">[28]</ref>, Niemeijer et al. <ref type="bibr" target="#b33">[34]</ref> and Staal et al. <ref type="bibr" target="#b4">[5]</ref> used supervised techniques that generally depend on the training datasets, thus to achieve a good results, classifier re-training is required before performing any experimentation on new datasets.</p><p>An overview of the testing results on DRIVE show that our method offers a reliable and robust segmentation solution for blood vessels. It is clearly observed that our approach reaches better performance in terms average true positive rate. The performance results of our approach are compared to the alternative methods: The adaptive morphological approach  The optic disc segmentation performance is evaluated by the overlapping ratio Oratio and the mean absolute distance M AD. The overlapping ratio is defined to measure the common area between the optic disc region in the ground truth and the optic disc region segmented by our method. It is defined by the following formulation:</p><formula xml:id="formula_18">Oratio = G S G S (<label>16</label></formula><formula xml:id="formula_19">)</formula><p>where G represents the true optic disc boundary (manually labelled region) and S is the optic disc boundary segmented by our method. MAD is defined as:</p><formula xml:id="formula_20">M AD (G c , S c ) = 1 2 1 n n i=1 d(g ci , S) + 1 m m i=1 d(s ci , G)<label>(17)</label></formula><p>where G c and S c are the contours of the segmented regions of the ground truth and our algorithm respectively. d(a i , B) is the minimum distance from the position of the pixel a i on the contour A to the contour B. A good segmentation implies a high overlapping ratio and a low M AD value.</p><p>The sensitivity of our method on DIARETDB1 and DRIVE, it is defined as:</p><formula xml:id="formula_21">Sensitivity = T p T p + F n (<label>18</label></formula><formula xml:id="formula_22">)</formula><p>where T p and F n are the number of true positives and the number of false negatives respectively. The sensitivity indicates the detection of the foreground pixels by the segmentation method.</p><p>Fig. <ref type="figure" target="#fig_14">14</ref> (a) and (b) show the optic disc segmentation results of topology cut technique <ref type="bibr" target="#b37">[38]</ref>, traditional graph cut technique <ref type="bibr" target="#b23">[24]</ref> and both our methods the optic disc segmentation with compensation factor and the optic disc segmentation with Markov Random field image reconstruction on DIARETDB1 and DRIVE respectively. Considering the ground truth images, It is clear that both our methods perform better than alternative methods topology cut technique <ref type="bibr" target="#b37">[38]</ref> and traditional graph cut technique <ref type="bibr" target="#b23">[24]</ref>. The topology cut technique achieved acceptable results in the brighter images, characterised by vessels that are more likely to belong to the foreground (similar intensity as the optic disc). However, the traditional graph cut technique tends to segment only the brightest region of the disc, this is due to the intrusion of the blood vessels in the optic disc region, which misguide the segmentation algorithm to follow a short path.</p><p>Table <ref type="table" target="#tab_5">V</ref> shows the performance of our proposed methods with alternative methods on DIARETDB1 images. The compensation factor vad and the MRF image reconstruction segmentation algorithms achieve the overlapping ratio of 0.7594 and 0.7850 and outperform the approaches in <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b37">[38]</ref> and <ref type="bibr" target="#b23">[24]</ref>. However considering the performance in terms of a mean absolute distance, MRF image reconstruction algorithm reaches the lowest value 6.55 and performs better than all the other methods. Both our methods achieve the highest average sensitivity with 87.50% for MRF image reconstruction and 86.75% for compensation factor V ad in 96.7% on the DIARETDB1 images. Table VI shows the performance results of our methods with other alternative methods in terms of Oratio, M AD and Sensitivity on DRIVE images. An overview of the segmentation results shows our proposed methods achieved the highest overlapping ratio with the minimum M AD value compare to the traditional graph cut method <ref type="bibr" target="#b23">[24]</ref>, the topology cut method <ref type="bibr" target="#b37">[38]</ref>, except the Adaptive morphologic <ref type="bibr" target="#b13">[14]</ref>, which is marginally inferior to the compensation factor algorithm in terms of M AD. However, an increase in the overlapping ratio does not necessarily mean a decrease on M AD value. Thus the value of M AD alone is not enough to measure the performance of segmentation results, but it provides a good reference of the contour matching with the ground truth contour reference.  For further performance comparison, we used the cumulative histogram to compare the overlapping ratio of our proposed method against Topology Cut <ref type="bibr" target="#b37">[38]</ref> and Graph cut <ref type="bibr" target="#b23">[24]</ref>. This is done by performing each segmentation method against the human expert hand labelled, and the cumulative histogram represents the frequency of the Oratio value. A perfect segmentation is achieved when the value of Oratio = 1 and the area under the curve is equal to zero. Fig. <ref type="figure" target="#fig_15">15</ref> and<ref type="figure" target="#fig_16">16</ref> show the plotted of the cumulative histograms of overlapping ratio for Topology Cut <ref type="bibr" target="#b37">[38]</ref> and Graph cut <ref type="bibr" target="#b23">[24]</ref>, Compensation Factor and MRF Image Reconstruction on DIARETDB1 and DRIVE datasets respectively. The overview of the graphs show that the Compensation Factor and MRF Image Reconstruction methods achieve the minimum area under the graph, hence our method outperforms all other methods. In general the MRF Image Reconstruction method reaches better results on DRIVE images, while the Compensation Factor method produces better segmentation results on DIARETDB1 dataset. Based on the assumption in Niemeijer et al. <ref type="bibr" target="#b38">[39]</ref> which consider a minimum overlapping ratio Oratio &gt; 50% as a successful segmentation, the compensation factor algorithm with 86.52% success performs better on DRIVE than DI-ARETDB1 and the segmentation of MRF Image Reconstruction with 90.00% achieves better results than the compensation factor algorithm on DRIVE .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. DISCUSSIONS AND CONCLUSIONS</head><p>We have presented a novel approach for blood vessels and optic disc segmentation in retinal images by integrating the mechanism of flux, MRF image reconstruction and compensation factor into the graph cut method. The process We have evaluated the performance of vessel segmentation against 10 other methods including human manual labelling on the STARE dataset and 15 other methods including human manual labelling on the DRIVE dataset. For the optic disc segmentation, we have evaluated the performance of our method against three other methods on the DRIVE and DIARETDB1 datasets.</p><p>Tables II, III and IV show performance comparison in terms of average true positive rate, false positive rate and accuracy rate. According to these results, our vessel segmentation algorithm reaches a acceptable results and outperforms all other methods in terms of average true positive rate on both STARE and DRIVE images. In terms of average accuracy, our method outperforms Hoover et al. <ref type="bibr" target="#b1">[2]</ref>, Martinez-Perez et al. <ref type="bibr" target="#b8">[9]</ref> and Chaudhuri et al. <ref type="bibr" target="#b5">[6]</ref> on Stare images. On DRIVE it performs better than Jiang et al. <ref type="bibr" target="#b35">[36]</ref>, Cinsdikici et al. <ref type="bibr" target="#b34">[35]</ref>, Zana et al. <ref type="bibr" target="#b6">[7]</ref>, Garg et al. <ref type="bibr" target="#b30">[31]</ref>, Zhang et al. <ref type="bibr" target="#b28">[29]</ref> and Martinez et al. <ref type="bibr" target="#b8">[9]</ref>. Nevertheless our method is marginally inferior to the methods presented by Staal et al. <ref type="bibr" target="#b4">[5]</ref>, Kaba et al. <ref type="bibr" target="#b26">[27]</ref>, Marin et al. <ref type="bibr" target="#b27">[28]</ref> and Zhang et al. <ref type="bibr" target="#b28">[29]</ref> on STARE and Al-Rawi et al. <ref type="bibr" target="#b32">[33]</ref>, Ricci et al. <ref type="bibr" target="#b36">[37]</ref> and Mendonca et al. <ref type="bibr" target="#b2">[3]</ref>, Soares et al. <ref type="bibr" target="#b29">[30]</ref>, Marin et al. <ref type="bibr" target="#b27">[28]</ref> and Staal et al. <ref type="bibr" target="#b4">[5]</ref> on DRIVE. Although Soares et al. <ref type="bibr" target="#b29">[30]</ref>, Marin et al. <ref type="bibr" target="#b27">[28]</ref>, Staal et al. <ref type="bibr" target="#b4">[5]</ref> and Ricci et al. <ref type="bibr" target="#b36">[37]</ref> seems to achieve higher accuracy, as supervised techniques, they generally depend on the training datasets, thus to achieve excellent results, classifier re-training is required before performing any experimentation on new datasets. Further studies in <ref type="bibr" target="#b27">[28]</ref> proved that these methods perform well when both training and testing are applied on the same dataset but the performance deteriorated when the method is tested and trained on different datasets. Since these methods are sensitive to the training datasets, deploying them for practical use in retinal blood vessel segmentation would need further improvements as segmentation algorithms must work on retinal images taken under different conditions to be effective.</p><p>Our propose method incorporates prior knowledge of blood vessels to perform the segmentation and it can be applied on retinal images from multiple sources and under different conditions without a need for training. This can be seen in the results achieved by this method on both STARE and DRIVE datasets.</p><p>For the optic disc segmentation Tables V and VI present the performance of our method on DIARETDB1 and DRIVE images. The results show that our methods of using (the Compensation factor and the MRF image reconstruction) achieved the best overall performance. The results also show that, the MRF image reconstruction algorithm outperforms the Compensation factor algorithm by 2.56% and 11.5% on DIARETDB1 and DRIVE images respectively. However it is important to notice that, the MRF image reconstruction algorithm depends on the vessel segmentation algorithm, for example if the vessel segmentation algorithm achieved a low performance on severely damage retinal image, the reconstruction would not define a meaningful optic disc region, hence the segmentation will fail.</p><p>Further more, the proposed method addresses one of the main issues in medical image analysis, "the overlapping tissue segmentation". Since the blood vessels converse into the optic disc area and misguide the graph cut algorithm through a short path, breaking the optic disc boundary. To achieve a good segmentation results, the MRF image reconstruction algorithm eliminates vessels in the optic disc area without any modification of the image structures before segmenting the optic disc. On the other hand the compensation factor incorporates vessels using local intensity characteristic to perform the optic disc segmentation. Thus our method can be applied in other medical image analysis applications to overcome "the overlapping tissue segmentation."</p><p>Our future research will be based on the segmentation of retinal diseases (lesions) known as "exudates" using the segmented structures of the retina (blood vessels and optic disc).Thus a background template can be created using these structures. Then this template can be used to perform the detection of suspicious areas (lesions) in the retinal images.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Vessel segmentation algorithm</figDesc><graphic coords="3,48.96,56.72,255.11,198.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Pre-processing. (a) h = 45, r = 3, (b) h = 45, r = 6, (c) h = 81, r = 3, (d)h = 81, r = 6 (e) distance map, (f) sample of a vessel with arrows indicating the vessel gradients</figDesc><graphic coords="3,317.03,56.72,240.94,155.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Retinal blood vessel segmentation using the traditional graph. (a) seeds initialisation of the input image, (b) λ = 20, (c) λ = 50, (d) λ = 100</figDesc><graphic coords="4,331.21,114.90,212.59,184.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The flux of vectors v passing through a given surface S.</figDesc><graphic coords="4,345.38,591.18,184.25,113.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Neighborhood system for a grid in the graph.</figDesc><graphic coords="5,359.55,56.72,155.90,141.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Vessel segmentation using the decomposition of vector v: (a) input retinal image, (b) Blood vessel segmentation using horizontal (X axis) decomposition of vector v, (c) Blood vessel segmentation using vertical (Y axis) decomposition of vector v, (d) Blood vessel segmentation result using the decomposition of vector v along X and Y axes.</figDesc><graphic coords="6,68.19,56.72,212.59,184.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. (a) Markov Random Field Image Reconstruction method diagram, (b) Compensation Factor method diagram</figDesc><graphic coords="6,54.03,552.32,240.93,99.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Optic disc detection. a) retinal image green channel with 1% of the brightest region selected in green colour , b) binary segmented blood vessel c) binary segmented blood vessel after pruning and d) sequence of points from the centroid to vessels convergence point (optic disc location).</figDesc><graphic coords="7,68.19,56.73,212.59,184.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Optic disc detection. a) ROI image b) initialisation of the foreground F and the background B of the ROI image.</figDesc><graphic coords="7,68.19,306.15,212.59,113.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. MRF reconstruction applied to retinal images. (Top) original gray scale images. (Bottom) reconstructed images using the MRF based method.</figDesc><graphic coords="7,338.29,138.48,198.43,141.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Optic disc segmentation with Compensation Factor V ad method: (a) V ad = 20, (b) V ad = 100, (c) V ad = 150, (d) V ad = 250</figDesc><graphic coords="8,61.11,406.55,226.77,155.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 12</head><label>12</label><figDesc>Fig. 12 and 13 show the segmented images and the manually labelled images for the DRIVE and the STARE datasets respectively.</figDesc><graphic coords="9,312.78,211.22,249.44,155.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. The DRIVE dataset: a) and d) retinal images, b) and e) our segmentation results, and c) and e) manually labelled results.</figDesc><graphic coords="9,312.78,434.14,249.44,155.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. The STARE dataset: a) and d) retinal images, b) and e) our segmentation results, and c) and e) manually labelled results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. (a) Optic disc segmentation results of DIARETDB1 images: First row Topology cut, second row Graph cut, third row Compensation factor algorithm, fourth row Markov Random field image reconstruction algorithm, fifth row Hand labelled. (b) Optic disc segmentation results of DRIVE images: First row Topology cut, second row Graph cut, third row Compensation factor algorithm, fourth row Markov Random field image reconstruction algorithm, fifth row Hand labelled</figDesc><graphic coords="11,317.04,175.10,240.93,170.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 15 .</head><label>15</label><figDesc>Fig. 15. Cumulative histogram for overlapping ratio of DIARETDB1 images</figDesc><graphic coords="12,61.10,419.30,226.76,141.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 16 .</head><label>16</label><figDesc>Fig. 16. Cumulative histogram for overlapping ratio of DRIVE images</figDesc><graphic coords="12,324.12,56.72,226.76,141.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>al., Chaudhuri et al., Hoover et al., Kaba et al., Martinez-Perez et al. and Zhang et al. have a room of improvement of 19.55%, 19.81%, 28.17%, 22.00%, 23.06%, 14.45% and 17.74% respectively.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II PERFORMANCE</head><label>II</label><figDesc>COMPARISON ON THE STARE DATASET.</figDesc><table><row><cell>Method</cell><cell>TPR</cell><cell>FPR</cell><cell>Accuracy</cell></row><row><cell>2 nd human expert [9]</cell><cell>0.8951</cell><cell>0.0438</cell><cell>0.9522</cell></row><row><cell>Hoover[2]</cell><cell>0.6751</cell><cell>0.0433</cell><cell>0.9267</cell></row><row><cell>Staal[5]</cell><cell>0.6970</cell><cell>0.0190</cell><cell>0.9541</cell></row><row><cell>Mendonca[3]</cell><cell>0.6996</cell><cell>0.0270</cell><cell>0.9440</cell></row><row><cell>Martinez[9]</cell><cell>0.7506</cell><cell>0.0431</cell><cell>0.9410</cell></row><row><cell>Chaudhuri[6]</cell><cell>0.6134</cell><cell>0.0245</cell><cell>0.9384</cell></row><row><cell>Kaba [27]</cell><cell>0.6645</cell><cell>0.0216</cell><cell>0.9450</cell></row><row><cell>Zhang [29]</cell><cell>0.7177</cell><cell>0.0247</cell><cell>0.9484</cell></row><row><cell>Marin [28]</cell><cell>-</cell><cell>-</cell><cell>0.9526</cell></row><row><cell>Our method</cell><cell>0.7887</cell><cell>0.0367</cell><cell>0.9441</cell></row><row><cell cols="4">C. Results of Optic Disc Segmentation on DIARETDB1 and</cell></row><row><cell>DRIVE datasets</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III PERFORMANCE</head><label>III</label><figDesc>COMPARISON OF HEALTHY VERSUS DISEASE IMAGES IN STARE DATASET.</figDesc><table><row><cell>Heathy images</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>TPR</cell><cell>FPR</cell><cell>Accuracy</cell></row><row><cell>Mendonca[3]</cell><cell>0.7258</cell><cell>0.0209</cell><cell>0.9492</cell></row><row><cell>Hoover[2]</cell><cell>0.6766</cell><cell>0.0338</cell><cell>0.9324</cell></row><row><cell>Chaudhuri[6]</cell><cell>0.7335</cell><cell>0.0218</cell><cell>0.9486</cell></row><row><cell>Zhang [29]</cell><cell>0.7526</cell><cell>0.0221</cell><cell>0.9510</cell></row><row><cell>Soares [30]</cell><cell>0.7554</cell><cell>0.0188</cell><cell>0.9542</cell></row><row><cell>Our method</cell><cell>0.8717</cell><cell>0.0364</cell><cell>0.9513</cell></row><row><cell>Unhealthy images</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>TPR</cell><cell>FPR</cell><cell>Accuracy</cell></row><row><cell>Mendonca[3]</cell><cell>0.6733</cell><cell>0.0331</cell><cell>0.9388</cell></row><row><cell>Hoover[2]</cell><cell>0.6736</cell><cell>0.0528</cell><cell>0.9211</cell></row><row><cell>Chaudhuri[6]</cell><cell>0.5881</cell><cell>0.0384</cell><cell>0.9276</cell></row><row><cell>Zhang [29]</cell><cell>0.7166</cell><cell>0.0327</cell><cell>0.9439</cell></row><row><cell>Soares [30]</cell><cell>0.6869</cell><cell>0.0318</cell><cell>0.9416</cell></row><row><cell>Our method</cell><cell>0.7057</cell><cell>0.0371</cell><cell>0.9369</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV PERFORMANCE</head><label>IV</label><figDesc>COMPARISON ON THE DRIVE DATASET.</figDesc><table><row><cell>Method</cell><cell>TPR</cell><cell>FPR</cell><cell>Accuracy</cell></row><row><cell>Human expert B[3]</cell><cell>0.7761</cell><cell>0.0275</cell><cell>0.9473</cell></row><row><cell>Staal[5]</cell><cell>0.7194</cell><cell>0.0227</cell><cell>0.9442</cell></row><row><cell>Mendonca[3]</cell><cell>0.7344</cell><cell>0.0236</cell><cell>0.9452</cell></row><row><cell>Niemeijer[34]</cell><cell>0.6898</cell><cell>0.0304</cell><cell>0.9417</cell></row><row><cell>Jiang[36]</cell><cell>-</cell><cell>-</cell><cell>0.8911</cell></row><row><cell>Cinsdikici [35]</cell><cell>-</cell><cell>-</cell><cell>0.9293</cell></row><row><cell>Marin [28]</cell><cell>-</cell><cell>-</cell><cell>0.9452</cell></row><row><cell>Ricci[37]</cell><cell>-</cell><cell>-</cell><cell>0.9633</cell></row><row><cell>Zana[7]</cell><cell>-</cell><cell>-</cell><cell>0.9377</cell></row><row><cell>Garg[31]</cell><cell>-</cell><cell>-</cell><cell>0.9361</cell></row><row><cell>Perfetti[32]</cell><cell>-</cell><cell>-</cell><cell>0.9261</cell></row><row><cell>Al-Rawi[33]</cell><cell>-</cell><cell>-</cell><cell>0.9510</cell></row><row><cell>Soares[30]</cell><cell>-</cell><cell>-</cell><cell>0.9466</cell></row><row><cell>Zhang[29]</cell><cell>0.7120</cell><cell>0.0276</cell><cell>0.9382</cell></row><row><cell>Martinez[9]</cell><cell>0.7246</cell><cell>0.0345</cell><cell>0.9344</cell></row><row><cell>Our method</cell><cell>0.7512</cell><cell>0.0316</cell><cell>0.9412</cell></row><row><cell cols="4">by Welfer et al. [14], the traditional graph cut technique by</cell></row><row><cell cols="4">Boykov et al. [24] and the topology cut technique proposed</cell></row><row><cell cols="4">by Zeng et al. [38]. Unfortunately it was not possible to test</cell></row><row><cell cols="4">our method against a large number of alternative methods,</cell></row><row><cell cols="4">since most of the methods do use a unique benchmark to</cell></row><row><cell cols="4">measure the results of the optic disc segmentation, therefore</cell></row><row><cell cols="4">this makes the comparison of the results difficult. Further</cell></row><row><cell cols="4">comparison is made between our two optic disc segmentation</cell></row><row><cell cols="4">methods (the Compensation factor and the Markov Random</cell></row><row><cell cols="4">field image reconstruction). All the methods are tested on</cell></row><row><cell cols="4">the same datasets (DIARETDB1 and DRIVE) of 109 fundus</cell></row><row><cell cols="4">retinal images in total, including those with discernable optic</cell></row><row><cell>disc.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V PERFORMANCE</head><label>V</label><figDesc>COMPARISON ON DIARETDB1 DATASET.</figDesc><table><row><cell></cell><cell>Average</cell><cell>Average</cell><cell>Average</cell></row><row><cell>Method</cell><cell>ORatio</cell><cell>MAD</cell><cell>Sensitivity</cell></row><row><cell>Topology cut [38]</cell><cell>0.3843</cell><cell>17.49</cell><cell>0.5530</cell></row><row><cell>Adaptive morphologic [14]</cell><cell>0.4365</cell><cell>8.31</cell><cell>-</cell></row><row><cell>Graph cut [24]</cell><cell>0.5403</cell><cell>10.74</cell><cell>0.7635</cell></row><row><cell>Compensation Factor</cell><cell>0.7594</cell><cell>6.18</cell><cell>0.8675</cell></row><row><cell>MRF Image Reconstruction</cell><cell>0.7850</cell><cell>6.55</cell><cell>0.8750</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI PERFORMANCE</head><label>VI</label><figDesc>COMPARISON ON DRIVE DATASET.</figDesc><table><row><cell></cell><cell>Average</cell><cell>Average</cell><cell>Average</cell></row><row><cell>Method</cell><cell>ORatio</cell><cell>MAD</cell><cell>Sensitivity</cell></row><row><cell>Topology Cut [38]</cell><cell>0.5591</cell><cell>10.24</cell><cell>0.6512</cell></row><row><cell>Adaptive morphologic [14]</cell><cell>0.4147</cell><cell>5.74</cell><cell>-</cell></row><row><cell>Graph cut [24]</cell><cell>0.5532</cell><cell>9.97</cell><cell>0.7398</cell></row><row><cell>Compensation Factor</cell><cell>0.709</cell><cell>6.48</cell><cell>0.8464</cell></row><row><cell>MRF Image Reconstruction</cell><cell>0.8240</cell><cell>3.39</cell><cell>0.9819</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank V. Kolmogorov for providing the software MaxFlow-v3.01 to compute the graph cut.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>He is a Charted Engineer, Life Member of the Association for the Advancement of Artificial Intelligence, Fellow of the Royal Statistical Society, and Fellow of the British Computer Society. Professor Liu has over 100 high quality journal publications in biomedical informatics, complex systems, computational intelligence and data mining. His H-index is over 40</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automated model based segmentation, tracing and analysis of retinal vasculature from digital fundus images</title>
		<author>
			<persName><forename type="first">K</forename><surname>Fritzsche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Can</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tanenbuam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Roysam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Suri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laxminarayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">State-of-The-Art Angiography, Applications and Plaque Imaging Using MR, CT, Ultrasound and X-rays</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="225" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Locating blood vessels in retinal images by piecewise threshold probing of a matched filter response</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hoover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kouznetsova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Goldbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="203" to="210" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Segmentation of retinal blood vessels by combining the detection of centerlines and morphological reconstruction</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Mendonca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Campilho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1200" to="1213" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Retinal vessel segmentation using the 2-d gabor wavelet and supervised classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leandro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cesar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jelinek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cree</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1214" to="1222" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Ridge-based vessel segmentation in color images of the retina</title>
		<author>
			<persName><forename type="first">J</forename><surname>Staal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Abramoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Niemeijer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Viergever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="501" to="509" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Detection of blood vessels in retinal images using two-dimensional matched filters</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Goldbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="263" to="269" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Segmentation of vessel-like patterns using mathematical morphology and curvature evaluation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-C</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1010" to="1019" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note>Image Processing</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A novel method for blood vessel detection from retinal images</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomedical engineering online</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Segmentation of blood vessels from red-free and fluorescein retinal images</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Martinez-Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Thom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Bharath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Parker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="47" to="61" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The detection and quantification of retinopathy using digital angiograms</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Rzeszotarski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Singerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Chokreff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="619" to="626" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
	<note>Medical Imaging</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automated localisation of the optic disc, fovea, and retinal blood vessels from digital colour fundus images</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sinthanayothin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Boyce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">British Journal of Ophthalmology</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="902" to="910" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automated segmentation of the optic nerve head for diagnosis of glaucoma</title>
		<author>
			<persName><forename type="first">R</forename><surname>Chrastek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Donath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Niemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hothorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Mardin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Michelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="297" to="314" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Optic nerve head segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lowell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hunter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Steel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fletcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kennedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="256" to="264" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Segmentation of the optic disc in color eye fundus images using an adaptive morphological approach</title>
		<author>
			<persName><forename type="first">D</forename><surname>Welfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Scharcanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kitamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Pizzol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ludwig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Marinho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers in Biology and Medicine</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="124" to="137" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Detecting the optic disc boundary in digital fundus images using morphological, edge detection, and feature extraction techniques</title>
		<author>
			<persName><forename type="first">A</forename><surname>Aquino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Gegúndez-Arias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Marín</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1860" to="1869" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>Medical Imaging</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Optic disc segmentation by incorporating blood vessel compensation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Salazar-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE SSCI, International Workshop on Computational Intelligence In Medical Imaging</title>
		<meeting>IEEE SSCI, International Workshop on Computational Intelligence In Medical Imaging</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Retinal blood vessel segmentation via graph cut</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Salazar-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Control Automation Robotics &amp; Vision (ICARCV), 2010 11th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="225" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mrf reconstruction of retinal images for the optic disc segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Salazar-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kaba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Health Information Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="88" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Structure analysis and lesion detection from retinal fundus images</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G S</forename><surname>Gonzalez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
		<respStmt>
			<orgName>Brunel University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On the adaptive detection of blood vessels in retinal images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="341" to="343" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Interactive graph cuts for optimal boundary &amp; region segmentation of objects in nd images</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-P</forename><surname>Jolly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV 2001. Proceedings. Eighth IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="105" to="112" />
		</imprint>
	</monogr>
	<note>Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Graph cut based image segmentation with connectivity priors</title>
		<author>
			<persName><forename type="first">S</forename><surname>Vicente</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">What metrics can be approximated by geo-cuts, or global optimization of length/area and flux</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Tenth IEEE International Conference on Computer Vision, ICCV</title>
		<meeting>Tenth IEEE International Conference on Computer Vision, ICCV</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="564" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Graph cuts and efficient n-d image segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Funka-Lea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="109" to="131" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Texture synthesis by non-parametric sampling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICCV</title>
		<meeting>the ICCV</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="1033" to="1038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Diaretdb1 diabetic retinopathy database and evaluation protocol</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kauppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kalesnykiene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kamarainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lensu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sorri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Raninen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Voitilainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Uusitalo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kalviainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pietila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of British Machine Vision Conference</title>
		<meeting>British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Segmentation of retinal blood vessels using gaussian mixture models and expectation maximisation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kaba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Salazar-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Serag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Health Information Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="105" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A new supervised method for blood vessel segmentation in retinal images by using gray-level and moment invariants-based features</title>
		<author>
			<persName><forename type="first">D</forename><surname>Marín</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aquino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Gegúndez-Arias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Bravo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="146" to="158" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>Medical Imaging</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Retinal vessel extraction by matched filter with first-order derivative of gaussian</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Karray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers in biology and medicine</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="438" to="445" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Retinal vessel segmentation using the 2-d gabor wavelet and supervised classification</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Leandro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Cesar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">F</forename><surname>Jelinek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Cree</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1214" to="1222" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note>Medical Imaging</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unsupervised curvature-based retinal vessel segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivaswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chandra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Biomedical Imaging: From Nano to Macro, 2007. ISBI 2007. 4th IEEE International Symposium</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="344" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Cellular neural networks with virtual template expansion for retinal vessel segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Perfetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Casali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Costantini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="141" to="145" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note>Circuits and Systems II: Express Briefs</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">An improved matched filter for blood vessel detection of digital retinal images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Al-Rawi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qutaishat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Arrar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers in Biology and Medicine</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="262" to="267" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Comparative study of retinal vessel segmentation methods on a new publicly available database</title>
		<author>
			<persName><forename type="first">M</forename><surname>Niemeijer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Staal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Loog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Abramoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Society for Optics and Photonics</title>
		<imprint>
			<biblScope unit="page" from="648" to="656" />
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
	<note>Medical Imaging</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Detection of blood vessels in ophthalmoscope images using mf/ant (matched filter/ant colony) algorithm</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Cinsdikici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Aydın</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer methods and programs in biomedicine</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="85" to="95" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Adaptive local thresholding by verificationbased multithreshold probing with application to vessel detection in retinal images</title>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mojon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="131" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Retinal blood vessel segmentation using line operators and support vector classification</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Perfetti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1357" to="1365" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note>Medical Imaging</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Topology cuts: a novel min-cut/max-flow algorithm for topology preserving segmentation in nd images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computer vision and image understanding</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="81" to="90" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Segmentation of the optic disc, macula and vascular arch in fundus photographs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Niemeijer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Abràmoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="116" to="127" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note>Medical Imaging</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
