<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Measuring Fairness in Ranked Outputs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ke</forename><surname>Yang</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Julia</forename><surname>Stoyanovich</surname></persName>
							<email>stoyanovich@drexel.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Drexel University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Drexel University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<addrLine>SSDBM &apos;17, June 27-29</addrLine>
									<postCode>2017</postCode>
									<settlement>Chicago</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Measuring Fairness in Ranked Outputs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9020978E2F189A7D6E52736A119B1592</idno>
					<idno type="DOI">10.1145/3085504.3085526</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Data, Responsibly</term>
					<term>Fairness</term>
					<term>Accountability</term>
					<term>Transparency</term>
					<term>Data Science for Social Good</term>
					<term>Data Ethics</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Ranking and scoring are ubiquitous. We consider the setting in which an institution, called a ranker, evaluates a set of individuals based on demographic, behavioral or other characteristics. The final output is a ranking that represents the relative quality of the individuals. While automatic and therefore seemingly objective, rankers can, and often do, discriminate against individuals and systematically disadvantage members of protected groups. This warrants a careful study of the fairness of a ranking scheme, to enable data science for social good applications, among others.</p><p>In this paper we propose fairness measures for ranked outputs. We develop a data generation procedure that allows us to systematically control the degree of unfairness in the output, and study the behavior of our measures on these datasets. We then apply our proposed measures to several real datasets, and detect cases of bias. Finally, we show preliminary results of incorporating our ranked fairness measures into an optimization framework, and show potential for improving fairness of ranked outputs while maintaining accuracy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Algorithmic decisions often result in scoring and ranking individuals, to determine credit worthiness, desirability for college admissions and employment, and attractiveness as dating partners. Scoring and ranking systems encode ideas of what counts as the best schools, neighborhoods, and technologies. Rankings are used as the basis of important decisions including college admissions, hiring, promotion, grant making, and lending, and have a potentially enormous impact on the livelihood and well-being of individuals.</p><p>Specifically, we consider the setting in which an institution (called a ranker), evaluates a set of items (typically individuals, but also colleges, restaurants, websites, or products) based on demographic, behavioral or other characteristics. The output is a ranking that represents the relative quality of the items. While automatic and therefore seemingly objective, rankers can, and often do, discriminate against individuals, systematically disadvantage members of protected groups <ref type="bibr" target="#b1">[2]</ref>, and exhibit lack of diversity at high ranks <ref type="bibr" target="#b7">[8]</ref>. This warrants a careful study of the fairness of a ranking scheme, a topic on which we focus in this paper.</p><p>Fairness can be broadly defined as impartial or just treatment of individuals and of demographic groups. Algorithmic fairness has increasingly been receiving attention in the literature, and a multitude of interpretations and technical definitions have been given <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12]</ref>. However, despite a recent spike of interest in fairness, and despite the ubiquity and the importance of ranking, we are not aware of work that quantifies fairness in a ranking scheme. The primary objective of this paper is to fill this gap.</p><p>A useful dichotomy is between individual fairness -a requirement that similar individuals are treated similarly, and group fairness, also known as statistical parity -a requirement that demographics of those receiving a particular outcome are identical to the demographics of the population as a whole <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. The focus of this paper is on statistical parity, and in particular on the case where a subset of the population belongs to a legally protected group. Members of a legally protected group share a characteristic that cannot be targeted for discrimination. In the US, such characteristics include race, gender and disability status, among others.</p><p>We make a simplifying assumption and consider membership in one protected group at a time (i.e., we consider only gender or disability status or membership in a minority group). Further, we assume that membership in a protected group is binary (minority vs. majority group, rather than a break-down by ethnicity). This is a reasonable starting point, and is in line with much recent literature <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>.</p><p>In this paper we propose several measures that quantify statistical parity, or lack thereof, in ranked outputs. The reasons to consider this problem are two-fold. First, having insight into the properties of a ranked output, or more generally of an output of an algorithmic process, helps make the process transparent, interpretable and accountable <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9]</ref>. Second, principled fairness quantification mechanisms allow us to engineer better algorithmic rankers, correcting for bias in the input data that is due to the effects of historical discrimination against members of a protected group.</p><p>To reason about fair assignment of outcomes to groups, we must first understand the meaning of a positive outcome in our context. There is a large body of work on measuring discrimination in machine learning, see <ref type="bibr" target="#b11">[12]</ref> for a survey. The most commonly considered algorithmic task is binary classification, where items assigned to the positive class are assumed to receive the positive outcome. In contrast to classification, a ranking is, by definition, relative, and so outcomes for items being ranked are not strictly positive or negative. The outcome for an item ranked among the top-5 is at least as good as the outcome for an item ranked among the top-10, which is in turn at least as good as the outcome for an item ranked among the top-20, etc.</p><p>Basic idea. Our formulation of fairness measures is based on the following intuition: Because it is more beneficial for an item to be ranked higher, it is also more important to achieve statistical parity at higher ranks. The idea is to take several well-known statistical parity measures proposed in the literature <ref type="bibr" target="#b11">[12]</ref>, and make them rank-aware by placing them within the nDCG framework <ref type="bibr" target="#b4">[5]</ref>. nDCG is commonly used for evaluating the quality of ranked lists in information retrieval, and is appropriate here because it provides a principled weighted discount mechanism. Specifically, we calculate set-wise parity at a series of cut-off points in a ranking, and progressively sum these values with a position-based discount.</p><p>The rest of this paper is organized as follows. We fix our notation and describe a procedure for generating synthetic rankings of varying degrees of fairness in Section 2. In Section 3 we present novel ranked fairness measures and show their behavior on synthetic and real datasets. In Section 4 we show that fairness in rankings can be improved through an optimization framework, but that additional work is needed to integrate the fairness and accuracy measures into the framework more tightly, to achieve better performance. We conclude in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES</head><p>We are given a database I (k, s, x 1 , . . . , x m ) of items. In this database, an item identified by the key attribute k is associated with a boolean attribute s that denotes membership in a binary protected group, and with descriptive attributes x 1 , . . . , x m . We denote by S + ⊆ I the items that belong to the protected group, and by S -= I \ S + the remaining items.</p><p>A ranking over I is a bijection τ : I → {1, . . . , N }, where τ (k) is the position of the item with identifier k in τ . We refer to the item at position i in τ by τ i . For a pair of items</p><formula xml:id="formula_0">k 1 , k 2 , we say that k 1 is preferred to k 2 in τ , denoted k 1 ≻ τ k 2 if τ (k 1 ) &lt; τ (k 2 )</formula><p>. Figure <ref type="figure" target="#fig_0">1</ref> gives an example of rankings of individuals, with gender=F as the protected group.</p><p>We denote by τ 1..i the top-i portion of τ , and by S + 1...i the items from the protected group S + that appear among the top-i: S + ∩τ 1..i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data generator</head><p>To systematically study the behavior of the proposed measures, we generate synthetic ranked datasets of varying degrees of fairness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>rank k gender income</head><formula xml:id="formula_1">1 X204 F 350K 2 Z912 M 250K 3 J314 M 210K 4 E098 M 210K 5 L125 F 180K 6 S994 M 175K 7 L813 M 175K 8 X909 F 150K 9 G305 F 120K 10 F881 F 110K … … … … rank k gender 1 X204 F 2 L125 F 3 X909 F 4 G305 F 5 F881 F 6 Z912 M 7 J314 M 8 E098 M 9 S994 M 10 L813 M</formula><p>fairness parameter original ranking rank gender</p><formula xml:id="formula_2">1 M 2 M 3 F 4 M 5 M 6 F 7 M 8 F 9 F 10 F</formula><p>rank gender</p><formula xml:id="formula_3">1 M 2 M 3 M 4 M 5 M 6 F 7 F 8 F 9 F 10 F</formula><p>rank gender Algorithm 1 presents our data generation procedure. This algorithm takes two inputs. The first is a ranking τ , which may be a random permutation of the items in I , or it may be generated by the vendor according to their usual process, e.g., in a score-based manner. The second input is the fairness probability f ∈ [0, 1], which specifies the relative preference between items in S + and in S -. When f = 0.5, groups S + and S -are mixed in equal proportion for as long as there are items in both groups. When f &gt; 0.5, members of S + are preferred, and when f &lt; 0.5 members of S -are preferred. In extreme cases, when f = 0, all members of S -will be placed in the output ranking σ before any members of S + (first all male individuals, then all female in Figure <ref type="figure" target="#fig_0">1</ref>); when f = 1, the opposite will be true: all of S + will be ranked higher than S -in the output σ .</p><formula xml:id="formula_4">1 M 2 F 3 M 4 F 5 M 6 F 7 M 8 F 9 M 10 F f = 0.3 f = 0.5 f = 0</formula><p>The following additional property holds for a pair</p><formula xml:id="formula_5">k 1 , k 2 ∈ I : if (k 1 .s = k 2 .s) ∧ (k 1 ≻ τ k 2 ) then k 1 ≻ σ k 2 .</formula><p>That is, Algorithm 1 does not change the relative order among a pair of items that are both in S + or in S -.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Ranking generator</head><p>Require: Ranking τ , fairness probability f . {Initialize the output ranking σ .}</p><formula xml:id="formula_6">1: σ ← ∅ 2: τ + = τ ∩ S + 3: τ -= τ ∩ S - 4: while (τ + ∅) ∧ (τ -∅) do 5: p = random([0, 1]) 6: if p &lt; f then 7:</formula><p>Pop an item from the top of the list τ + . end if 13: end while 14: σ ← τ + 15: σ ← τ - 16: return σ Figure <ref type="figure" target="#fig_1">2</ref> gives examples of three ranked lists of 10 individuals, with the specified fairness probabilities, and with дender = F as the protected group. For f = 0, all male individuals are placed ahead of the females. For f = 0.3, males and females are mixed, but males occur more frequently at top ranks. For f = 0.5, males and females are mixed in equal proportion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">FAIRNESS MEASURES</head><p>A ranking scheme exhibits statistical parity if membership in a protected group does not influence an item's position in the output. We now present three measures of statistical parity that capture this intuition.</p><p>Our measures quantify the relative representation of the protected group S + in a ranking τ . For all measures, we compute set-based fairness at discrete points in the ranking (top-10, top-20, etc), and compound these values with a logarithmic discount. In this way, we express that higher positions in the ranking are more important, i.e., that it is more important to be fair at the top-10 than at the top-100. Our logarithmic discounting method is inspired by that used in nDCG <ref type="bibr" target="#b4">[5]</ref>, a ranked quality measure in Information Retrieval.</p><p>All measures presented in this section are normalized to the [0, 1] range for ease of interpretation. All measures have their best (most fair) value at 0, and their worst value at 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Normalized discounted difference (rND)</head><p>Normalized discounted difference (rND) (Equation <ref type="formula" target="#formula_7">1</ref>), computes the difference in the proportion of members of the protected group at top-i and in the over-all population.</p><p>Values are accumulated at discrete points in the ranking with a logarithmic discount, and finally normalized. Normalizer Z is computed as the highest possible value of rND for the given number of items N and protected group size</p><formula xml:id="formula_7">|S + |. rND(τ ) = 1 Z N i=10,20, ... 1 loд 2 i |S + 1...i | i - |S + | N<label>(1)</label></formula><p>Figure <ref type="figure">3</ref> plots the behavior of rND on synthetic datasets of 1000 items, with 200, 500 and 800 items in S + , as a function of fairness probability. We make four observations: (1) Groups S + and S -are treated symmetrically -a low proportion of either S + or S -at high ranks leads to a high (unfair) rND score. <ref type="bibr" target="#b1">(2)</ref> The best (lowest) value of rND is achieved when fairness parameter is set to the value matching the proportion of S + in I : 0.2 for 200 protected group members out of 1000, 0.5 for 500 members, and 0.8 for 800 members.</p><p>(3) rND is convex and continuous. (4) rND is not differentiable at 0.</p><p>We argued in the introduction that fairness measures are important not only as a means to observe properties of the output, but also because they allow us to engineer processes that are more fair. A common approach is to specify an optimization problem in which some notion of accuracy or utility is traded against some notion of fairness <ref type="bibr" target="#b10">[11]</ref>. While rND presented above is convex and continuous, it is not differentiable, limiting its usefulness in an optimization framework. This consideration motivates us to develop an alternative measure, rKL, described next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Normalized discounted KL-divergence (rKL)</head><p>Kullback-Leibler (KL) divergence measures the expectation of the logarithmic difference between two discrete probability distributions P and Q:</p><formula xml:id="formula_8">D K L (P ||Q) = i P(i)loд P(i) Q(i)<label>(2)</label></formula><p>We use KL-divergence to compute the expectation of the difference between protected group membership at top-i vs. in the over-all population. We take:</p><formula xml:id="formula_9">P = |S + 1...i | i , |S - 1...i | i , Q = |S + | N , |S -| N<label>(3)</label></formula><p>and define normalized discounted KL-divergence (rKL) as:</p><formula xml:id="formula_10">rKL(τ ) = 1 Z N i=10,20, ... D K L (P ||Q) loд 2 i<label>(4)</label></formula><p>Figure <ref type="figure">4</ref> plots the behavior of rKL on synthetic datasets of 1000 items, with 200, 500 and 800 items in S + , as a function of fairness probability. We observe that this measure has similar behavior as rND (Equation 1 and Figure <ref type="figure" target="#fig_0">1</ref>), but that it appears smoother, and so may be more convenient to optimize robustly. An additional advantage of rKL is that it can be used without modification to go beyond binary protected group membership, e.g., to capture proportions of different racial groups, or age groups, in a population.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Normalized discounted ratio (rRD)</head><p>Our final measure, normalized discounted ratio (rRD), is formulated similarly to rND, with the difference in the denominator of the fractions: the size of S + 1...i is compared to the size of S - 1...i , not to i (and similarly for the second term, S + ). When either the numerator or the denominator of a fraction is 0, we set the value of the fraction to 0. 5: rRD on 1,000 items</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1RUPDOL]HGGLVFRXQWHGGLIIHUHQFH</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>200LQSURWHFWHGJURXS 500LQSURWHFWHGJURXS 800LQSURWHFWHGJURXS</head><p>Behavior of the rRD measure on a synthetic dataset is presented in Figure <ref type="figure">5</ref>. We observe that rRD reaches its best (lowest) value at the same points as do rND and rKL, but that it shows different trends. Most importantly, because rRD does not treat S + and S - symmetrically, its behavior when protected group represents the majority of the over-all population (800 protected group members out of 1000 in Figure <ref type="figure">5</ref>), or when S + is preferred to S -(fairness probability &gt; 0.5), is not meaningful. We conclude that rRD is only applicable when the protected group is the minority group, i.e., when S + corresponds to at most 50% of the underlying population, and when fairness probability is below 0.5.</p><formula xml:id="formula_11">rRD(τ ) = 1 Z N i=10,20, ... 1 loд 2 i |S + 1...i | |S - 1...i | - |S + | |S -|<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation with real datasets</head><p>We used several real datasets to study the relative behavior of our measures, and to see whether our measures signal any fairness violations in that data. We highlight results on two datasets: ProPublica <ref type="bibr" target="#b0">[1]</ref> and German Credit <ref type="bibr" target="#b6">[7]</ref>.</p><p>ProPublica is the recidivism data analyzed by <ref type="bibr" target="#b0">[1]</ref> and retrieved from https://github.com/propublica/compas-analysis. This dataset contains close to 7,000 criminal defendant records. The goal of the analysis done by <ref type="bibr" target="#b0">[1]</ref> was to establish whether there is racial bias in the software that computes predictions of future criminal activity. Racial bias was indeed ascertained, as was gender bias, see <ref type="bibr" target="#b0">[1]</ref> for details. In our analysis we set out to check whether ranking on the values of recidivism score, violent recidivism score, and on the number of prior arrests shows parity w.r.t. race (black as the protected group, 51% of the dataset) and gender (female as the protected group, 19% of the dataset).</p><p>Using race = black as the protected group, we found rND = 0.44 for ranking on recidivism and rND = 0.44. We found lower but still noticeable rKL values for these rankings (0.17 and 0.18, respectively). Interestingly, ranking by the number of prior arrests produced rND = 0.23 but rKL = 0.04, showing much higher unfairness according to the rND measure than to the rKL. Note that rRD is inapplicable here, since the protected group constitutes the majority of the population.</p><p>Using дender = F as the protected group, we found rND = 0.15 for ranking on recidivism score, rND = 0.12 for violent recidivism and rND = 0.11 for ranking on prior arrests. The rKL was lowbetween 0.01 and 0.02 for these cases. We measured rRD = 0.20 for recidivism ranking, rRD = 0.14 for violent recidivism and rRD = 0.16 for prior arrests.</p><p>German Credit is a dataset from <ref type="bibr" target="#b6">[7]</ref> with financial information about 1,000 individuals applying for loans. This dataset is typically used for classification tasks. Nonetheless, several of the attributes that are part of this dataset can be used for ranking, including duration (month), credit amount, status of existing account, and employment length. We experimented with ranking on individual attributes duration (months) and credit amount, and also used a score-based ranker that computes the score of each individual by normalizing the value of each attribute, and then summing them with equal weights. We used all attributes that are either continuous or discrete but ordered.</p><p>As protected group, we used дender = F (69% of the dataset), aдe &lt; 25(15% of the dataset), and aдe &lt; 35 (55% of the dataset). For these cases, rKL ranged between 0.01 and 0.15, and rND ranged between 0.05 and 0.41. rRD was only applicable to age below 25, and ranged between 0.08 and 0.12. We will show in Section 4 results of optimizing fairness for this dataset, with aдe &lt; 25 as the protected group.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">LEARNING FAIR RANKINGS</head><p>In this section we describe an optimization method for improving the fairness of ranked outputs. Our approach is based closely on the fair representations framework of Zemel et. al <ref type="bibr" target="#b10">[11]</ref>, which focuses on making classification tasks fair. We first briefly describe their framework, and then explain our modifications that make it applicable to rankings.</p><p>The main idea in <ref type="bibr" target="#b10">[11]</ref> is to introduce a level of indirection between the input space X that represents individuals and the output space Y in which classification outcomes are assigned to individuals. Specifically, they introduce a multinomial random variable Z of size k, with each of the k values representing a "prototype" (cluster) in the space of X . The goal is to learn a mapping from X to Z that   preserves information about attributes that are useful for the classification task at hand, while at the same time hiding information about protected group membership of x. Statistical parity in this framework is formulated as</p><formula xml:id="formula_12">P(Z = k | x ∈ S + ) = P(Z = k | x ∈ S -), ∀k.</formula><p>The goal is to learn a mapping that satisfies statistical parity and at the same time preserves utility. For this, Z must be a good description of X (distances from x ∈ X to its representation in x should be small) and predictions based on x should be accurate. This formulation gives rise to the following multi-criteria optimization problem, with the loss function L = A x L x +A y L y +A z L z , where L x , L y and L z are loss functions and A x , A y , A z are hyper-parameters governing the trade-offs. We keep the terms responsible for statistical parity (L z ) and distance in the input space (L x ) as in the original framework, and redefine the term that represents accuracy (L y ) as appropriate for ranked outcomes. We show here results that use average per-item score difference between the groundtruth ranking τ and its estimate τ to quantify accuracy. We also experimented with position accuracy (per-item rank difference), Kendall-τ distance, and Spearman and Pearson's correlation coefficients, but omit these results due to space limitations, and also because more work is needed to understand convergence properties of the optimization under these measures.</p><p>We evaluated our optimization approach on the German Credit dataset <ref type="bibr" target="#b6">[7]</ref>. We were unable to evaluate these techniques on the ProPublica dataset <ref type="bibr" target="#b0">[1]</ref> because that dataset is ranked based on the value of an already-computed attribute, and so there is no opportunity to adjust the ranked order.</p><p>Results of our preliminary evaluation on the German Credit dataset (see Section 3), with aдe &lt; 25 as the protected group, are presented in Figures 6 (ranked on sum of all attributes, normalized, with equal weights) and 7 (ranked on the credit amount attribute). We show convergence of accuracy (normalized average score difference) and fairness (rND, rKL, rRD, and the statistical parity component L z as "group fairness in optimization"). We observe that all fairness measures converge to a low (fair) value, but that accuracy is optimized more effectively in Figure <ref type="figure" target="#fig_4">6</ref> than in Figure <ref type="figure" target="#fig_5">7</ref>, where the score difference is considerable. We also show convergence of accuracy using Kendall-τ distance in Figure <ref type="figure" target="#fig_6">8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS AND FUTURE WORK</head><p>In this paper we presented novel measures that can be used to quantify statistical parity in rankings. We evaluated these measures in synthetic and real datasets, and showed preliminary optimization results. Much future work remains on the evaluation of these measures -understanding their applicability in real settings, formally establishing their mathematical properties, incorporating them into an optimization framework, and ensuring that the framework is able to both preserve accuracy and improve fairness. One of the bottlenecks in this process is the running time, which we are working to optimize both by looking for ways to make fairness measures more computationally friendly and by careful engineering.</p><p>The code implementing all parts of this work is publicly available at https://github.com/DataResponsibly/FairRank.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A ranked list, sorted in descending order of income; дender = F is the protected group.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Three ranked lists, дender = F is the protected group.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>an item from the top of the list τ -. 11: σ ← pop(τ -) 12:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: rND on 1,000 items</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Accuracy and fairness on German Credit, ranked by sum of normalized attribute values, with k = 10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Accuracy and fairness on German Credit, ranked by credit amount, with k = 10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Accuracy (Kendall-τ ) and fairness on German Credit, ranked by sum of normalized attribute values, with k = 10.</figDesc></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>* This work was supported in part by NSF Grants No. 1464327 and 1539856 and BSF Grant No. 2014391.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Machine Bias</title>
		<author>
			<persName><forename type="first">Julia</forename><surname>Angwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Mattu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lauren</forename><surname>Kirchner</surname></persName>
		</author>
		<ptr target="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing" />
	</analytic>
	<monogr>
		<title level="j">ProPublica</title>
		<imprint>
			<date type="published" when="2016-05-23">2016. May 23 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The Scored Society: Due Process for Automated Predictions</title>
		<author>
			<persName><forename type="first">Danielle</forename><forename type="middle">K</forename><surname>Citron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><forename type="middle">A</forename><surname>Pasquale</surname></persName>
		</author>
		<ptr target="http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2376209" />
	</analytic>
	<monogr>
		<title level="j">Washington Law Review</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fairness through awareness</title>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toniann</forename><surname>Pitassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Reingold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="DOI">10.1145/2090236.2090255</idno>
		<ptr target="http://dx.doi.org/10.1145/2090236.2090255" />
	</analytic>
	<monogr>
		<title level="m">Innovations in Theoretical Computer Science</title>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-01-08">2012. 2012. January 8-10, 2012</date>
			<biblScope unit="page" from="214" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">On the (im)possibility of fairness</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sorelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Friedler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suresh</forename><surname>Scheidegger</surname></persName>
		</author>
		<author>
			<persName><surname>Venkatasubramanian</surname></persName>
		</author>
		<idno>CoRR abs/1609.07236</idno>
		<ptr target="http://arxiv.org/abs/1609.07236" />
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cumulated gain-based evaluation of IR techniques</title>
		<author>
			<persName><forename type="first">Kalervo</forename><surname>Järvelin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaana</forename><surname>Kekäläinen</surname></persName>
		</author>
		<idno type="DOI">10.1145/582415.582418</idno>
		<ptr target="http://dx.doi.org/10.1145/582415.582418" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="422" to="446" />
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">A</forename><surname>Kroll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joanna</forename><surname>Huey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Solon</forename><surname>Barocas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">W</forename><surname>Felten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><forename type="middle">R</forename><surname>Reidenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">G</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harlan</forename><surname>Yu</surname></persName>
		</author>
		<ptr target="http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2765268" />
	</analytic>
	<monogr>
		<title level="j">Accountable Algorithms. University of Pennsylvania Law Review</title>
		<imprint>
			<biblScope unit="volume">165</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Lichman</surname></persName>
		</author>
		<ptr target="http://archive.ics.uci.edu/ml" />
		<title level="m">UCI Machine Learning Repository</title>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Making intervalbased clustering rank-aware</title>
		<author>
			<persName><forename type="first">Julia</forename><surname>Stoyanovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sihem</forename><surname>Amer-Yahia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tova</forename><surname>Milo</surname></persName>
		</author>
		<idno type="DOI">10.1145/1951365.1951417</idno>
		<ptr target="http://dx.doi.org/10.1145/1951365.1951417" />
	</analytic>
	<monogr>
		<title level="m">EDBT 2011, 14th International Conference on Extending Database Technology</title>
		<meeting><address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-03-21">2011. March 21-24, 2011</date>
		</imprint>
	</monogr>
	<note>Proceedings. 437-448</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Julia</forename><surname>Stoyanovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellen</forename><forename type="middle">P</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Revealing Algorithmic Rankers. Freedom to Tinker</title>
		<imprint>
			<date type="published" when="2016-08-05">2016. August 5 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Florian</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vaggelis</forename><surname>Atlidakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roxana</forename><surname>Geambasu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">J</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Pierre</forename><surname>Hubaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Humbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Juels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huang</forename><surname>Lin</surname></persName>
		</author>
		<idno>CoRR abs/1510.02377</idno>
		<ptr target="http://arxiv.org/abs/1510.02377" />
		<title level="m">Discovering Unwarranted Associations in Data-Driven Applications with the FairTest Testing Toolkit</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning Fair Representations</title>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toniann</forename><surname>Pitassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Dwork</surname></persName>
		</author>
		<ptr target="http://jmlr.org/proceedings/papers/v28/zemel13.html" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A survey on measuring indirect discrimination in machine learning</title>
		<author>
			<persName><forename type="first">Indre</forename><surname>Zliobaite</surname></persName>
		</author>
		<idno>CoRR abs/1511.00148</idno>
		<ptr target="http://arxiv.org/abs/1511.00148" />
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
