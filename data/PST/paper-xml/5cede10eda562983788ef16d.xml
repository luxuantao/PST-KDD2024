<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Meta-SR: A Magnification-Arbitrary Network for Super-Resolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xuecai</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Center for Research on Intelligent Perception and Computing</orgName>
								<orgName type="institution">NLPR</orgName>
								<address>
									<region>CASIA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haoyuan</forename><surname>Mu</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
							<email>zhangxiangyu@megvii.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Megvii Inc (Face++</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zilei</forename><surname>Wang</surname></persName>
							<email>zlwang@ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Center for Research on Intelligent Perception and Computing</orgName>
								<orgName type="institution">NLPR</orgName>
								<address>
									<region>CASIA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
							<email>sunjian@megvii.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Megvii Inc (Face++</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Meta-SR: A Magnification-Arbitrary Network for Super-Resolution</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recent research on super-resolution has achieved great success due to the development of deep convolutional neural networks (DCNNs). However, super-resolution of arbitrary scale factor has been ignored for a long time. Most previous researchers regard super-resolution of different scale factors as independent tasks. They train a specific model for each scale factor which is inefficient in computing, and prior work only take the super-resolution of several integer scale factors into consideration. In this work, we propose a novel method called Meta-SR to firstly solve super-resolution of arbitrary scale factor (including noninteger scale factors</head><p>) with a single model. In our Meta-SR, the Meta-Upscale Module is proposed to replace the traditional upscale module. For arbitrary scale factor, the Meta-Upscale Module dynamically predicts the weights of the upscale filters by taking the scale factor as input and use these weights to generate the HR image of arbitrary size. For any low-resolution image, our Meta-SR can continuously zoom in it with arbitrary scale factor by only using a single model. We evaluated the proposed method through extensive experiments on widely used benchmark datasets on single image super-resolution. The experimental results show the superiority of our Meta-Upscale.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Single image super-resolution (SISR) aims to reconstruct a visually natural high-resolution image from its degraded low-resolution (LR) image. And it has very wide application on security and surveillance imaging <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b36">37]</ref>, medical imaging <ref type="bibr" target="#b22">[23]</ref>, as well as satellite and aerial imaging <ref type="bibr" target="#b31">[32]</ref>. In real-world scenarios, it is very common and necessary for SISR to zoom in the LR image with the user-customized scale factor. As with the common image viewer, the user can arbitrarily zoom in the viewed image by rolling the mouse wheel to see the local details of the viewed image. The customized scale factor for super-resolution also can be any positive number. And it should not be fixed to some certain integers. Thus, a method to solve super-resolution of arbitrary scale factor is important for putting the SISR into more practical use. If we train a specific model for each positive scale factor, it is impossible to store all these models and it is inefficient in computing. Thus, the more important thing is that whether we can solve the super-resolution of arbitrary scale factor with a single model.</p><p>However, as we all known, the most existing SISR methods only consider super-resolution of some certain integer scale factors (X2, X3, X4). And these methods treat superresolution of different scale factors as independent tasks. Few previous work has discussed how to implement superresolution of arbitrary scale factor. As for the state-of-theart SISR methods, such as ESPCNN <ref type="bibr" target="#b21">[22]</ref>, EDSR <ref type="bibr" target="#b17">[18]</ref>, RDN <ref type="bibr" target="#b35">[36]</ref> and RCAN <ref type="bibr" target="#b34">[35]</ref>, these methods zoom in the feature maps at the end of networks with the sub-pixel convolution <ref type="bibr" target="#b17">[18]</ref>. Unfortunately, these methods have to design a specific upscale module for each scale factor. Each upscale module can only zoom in the image with the fixed integer scale factor. And the sub-pixel convolution only works for the integer scale factors. These disadvantages limit the use of SISR to real-world scenarios. Although, we could implement super-resolution of non-integer scale factors by properly upscaling the input image. However, the repeated computation and the upscaled input make these methods very time-consuming and hard to put into pratical use.</p><p>To solve these drawbacks and put SISR into more practical use, an efficient and novel method for super-resolution of arbitrary scale factor with a single model is necessary. If we want to solve the super-resolution of arbitrary scale factor with a single model, a group of weights for upscale filters is necessary for each scale factor. Inspired by the meta-learning, we propose a network to dynamically pre-dict the weights of filters for each scale factor. Thus, we no longer need to store weights for each scale factor. Compared with storing the weights for each scale factor, storing the small weight prediction network is more convenient.</p><p>We call this method Meta-SR. There are two modules in our Meta-SR, the Feature Learning Module and the Meta-Upscale Module. The Meta-Upscale Module is proposed to replace the typical upscale module. For each pixel (i, j) on the generated HR image, we project it onto the LR image based on the scale factor r. The projection coordinate is (⌊ i r ⌋, ⌊ j r ⌋) on the LR image. Our Meta-Upscale Module takes this coordinate-related and scale-related vector as input and predicts the weights for the filters. For each pixel (i, j) on the generated SR image, a convolution operation is conducted between the feature at the corresponding projection coordinate on the LR image and the weights of the filters to generate the pixel value on (i, j). The proposed Meta-Upscale Module could dynamically predict the variant number of weights for the convolution filters by taking a sequence of scale-related and coordinate-related vectors as input. Through this way , our Meta-Upscale Module can zoom in the feature maps of arbitrary scale factor with a single model. Actually, our Meta-Upscale Module can be incorporated into most previous methods <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b17">18]</ref> by replacing the typical upscale module.</p><p>We present extensive experiments on multiple benchmark datasets for single image super-resolution to evaluate our method. We show that: 1) For super-resolution of single integer scale factor, our Meta-SR could achieve the comparable results with the corresponding baseline which re-trained the model for each integer scale factor. Note that our Meta-SR is trained a single model for super-resolution of arbitrary scale factor together. 2) For super-resolution of arbitrary scale factor with a single model, our Meta-SR is better than these methods based on properly zooming in the input images or the output images, or interpolating on the feature maps. 3) Our Meta-Upscale Module only consists of several fully connected layers and it is fast enough. The running time of our Meta-Upscale is about 1% of the time consumed by the Feature Learning Module (RDN <ref type="bibr" target="#b35">[36]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Single Image Super Resolution</head><p>Early SISR methods are exemplar or dictionary based super-resolution <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b24">25]</ref>. These methods require a database of external images and generate the highresolution images by transfering the relevant patches in the database images. The performance is limited by the size of the database or the dictionary. These traditional methods are very time-consuming and they have limited performance.</p><p>With the rapid development of the deep learning, numerous deep learning based methods have been proposed.</p><p>A three-layers convolutional neural network is firstly proposed by Dong et al. <ref type="bibr" target="#b3">[4]</ref> called SRCNN. The SRCNN upscaled the low-resolution image with bicubic interpolation before feeding into the network. Kim et al. <ref type="bibr" target="#b13">[14]</ref> increased the depth of the network and used the residual learning for stable training. Kim et al. <ref type="bibr" target="#b14">[15]</ref> firstly introduced the recursive learning to SISR, called DRCN. Tai et al. <ref type="bibr" target="#b23">[24]</ref> proposed DRRN by introducing the recursive blocks with shared parameters to make the training stable. Tai et al. also introduced the memory block called Memnet <ref type="bibr" target="#b24">[25]</ref>. However, the input of these networks have the same size as the final highresolution image, these methods are time-consuming.</p><p>Shi et al. <ref type="bibr" target="#b21">[22]</ref> firstly proposed a real-time superresolution algorithm ESPCNN by proposing the sub-pixel convolution layer. The ESPCNN <ref type="bibr" target="#b21">[22]</ref> upscaled the image at the end of the network to reduce the computation. Ledig et al. <ref type="bibr" target="#b15">[16]</ref> introduced the residual block and the adversarial learning <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b5">6]</ref> to make the generated images more realistic and natural. Lim et al. <ref type="bibr" target="#b17">[18]</ref> used the deeper and wider residual networks called EDSR. The EDSR <ref type="bibr" target="#b17">[18]</ref> removed the BN layer and used the residual scaling to speedup the training. Lim also firstly trained single model for multiple scale factors (X2, X3, X4) called MDSR. The MDSR has different image processing blocks and upscale modules for each scale factor. Zhang et al. <ref type="bibr" target="#b35">[36]</ref> proposed a residual dense network (RDN) which combines the advantage of the residual block and the dense connected block. Then Zhang et al. <ref type="bibr" target="#b34">[35]</ref> introduced the residual channel attention to the SR framework. Wang et al. <ref type="bibr" target="#b27">[28]</ref> proposed a novel deep spatial feature transform to recover textures conditioned on the categorical priors. Both DBPN <ref type="bibr" target="#b9">[10]</ref> and DSRN <ref type="bibr" target="#b8">[9]</ref> made use of the mutual dependencies of low-and high-resolution images. DBPN exploited iterative up-sampling and down-sampling layers to provide an error feedback mechanism for each stage. Jo et al. <ref type="bibr" target="#b12">[13]</ref> introduced the dynamic upsampling filters for video super-resolution. The dynamic upsampling filters were generated locally and dynamically depending on the spatial-temporal neighborhood of each pixel in LR frames. Different from this work, our Meta-Upscale Module predicted the weights of the convolution kernel depending on the varying scale factors for SISR. Moreover, our Meta-Upscale could generate variant number and variant weights of the convolution kernel depend on the scale factor. Instead of using the spatial-temporal feature blocks, the input of the our Meta-Upscale Module is the scale-related and coordinate-related vector. Moreover, our Meta-Upscale Module is proposed to solve the arbitrary scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Meta-Learning</head><p>The meta-learning, or learning to learn, is the science of observing how different machine learning approaches perform on a wide range of learning tasks, and then learning from this experience, or meta-data. The meta-learning is mainly used in few-shot/zero-shot learning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b20">21]</ref> and transfer learning <ref type="bibr" target="#b28">[29]</ref>. The detailed survey of the metalearning can be found in <ref type="bibr" target="#b16">[17]</ref>. Here we only discuss the weight prediction related work.</p><p>The weight prediction is one of meta-learning strategy in the neural network <ref type="bibr" target="#b16">[17]</ref>. The weights of the neural network are predicted by another neural network rather than directly learned from the training dataset. Cai et al. <ref type="bibr" target="#b1">[2]</ref> predicted the parameters W of the classifier to adapt to the new categories without back propagation for few-shot learning. The parameters were predicted conditioned on the memory of the support set. In the object detection task, Hu et al. <ref type="bibr" target="#b10">[11]</ref> proposed to predict the mask weights from box weights. And Yang et al. <ref type="bibr" target="#b30">[31]</ref> proposed a novel and flexible anchor mechanism for object detection. The anchor functions could be dynamically generated from the arbitrary customized prior boxes. In the video super resolution, Jo et al. <ref type="bibr" target="#b12">[13]</ref> proposed a dynamic upsampling filters. The dynamic upsampling filters were generated locally and dynamically depending on the spatial-temporal neighborhood of each pixel in multiple LR frames. Unlike this work, we take the advantage of the meta-learning to predict weights of the filters for each scale factor. We no long need to store the weights of the filters for each scale factor. Our Meta-SR can train a single model for super-resolution of arbitrary scale. It is convenient and efficient for practical use.</p><p>The most related work is the Parameterized Image Oper-ators <ref type="bibr" target="#b4">[5]</ref> which took advantage of the weight prediction to dynamically adjust the weights of a deep network for image operators (image filtering or image restoration). Different from this work, our Meta-SR focuses on reformulation of the Upscale Module by taking both the coordinate and scale factor as inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Approach</head><p>In this section, we describe the proposed model architectures. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>. In our Meta-SR, the Feature Learning Module extracts the feature of the low-resolution image and the Meta-Upscale Module upscales the feature map with arbitrary scale factor. We introduce our Meta-Upscale at first, then we describe the architecture details of our Meta-SR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Meta-Upscale Formulation</head><p>Given an LR image I LR which is downscaled from the corresponding original HR image I HR , the task of SISR is to generate a HR image I SR whose ground-truth is I HR . We choose the RDN <ref type="bibr" target="#b35">[36]</ref> as our Feature Learning Module. As shown in Fig. <ref type="figure" target="#fig_0">1(b)</ref>. Here, we focus on formulating the Meta-Upscale Module.</p><p>Let F LR denote the feature extracted by the Feature Learning Module. Suppose the scale factor is r. For each pixel (i, j) on the SR image, we think that it is decided by the feature of the pixel (i ′ , j ′ ) on the LR image and the weights of the corresponding filter. From this perspective, the upscale module can be seen as a mapping function to map I SR and F LR . At first, the upscale module should map the pixel (i, j) to the pixel (i ′ , j ′ ). Then, the upscale module needs a specific filter to map the feature of the pixel (i ′ , j ′ ) to generate the value of this pixel (i, j). We formulate the upscale module as:</p><formula xml:id="formula_0">I SR (i, j) = Φ(F LR (i ′ , j ′ ), W(i, j))<label>(1)</label></formula><p>where I SR (i, j) denotes the pixel value at (i, j) on SR image. F LR (i ′ , j ′ ) denotes the feature of pixel (i ′ , j ′ ) on the LR image. W(i, j) is the weights of filter for pixel (i, j). Φ(.) is the feature mapping function to calculate the pixel value.</p><p>Since each pixel on the SR image corresponds to a filter. For different scale factors, both the number of the filters and the weights of the filters are different from the other scale factor. In order to solve the super-resolution of arbitrary scale factor with a single model, we propose the Meta-Upscale Module to dynamically predict the weights W(i, j) based on the scale factor and coordinate information.</p><p>For the Meta-Upscale Module, there are three important functions. That is, the Location Projection, the Weight Prediction and the Feature Mapping. As shown in the Fig. Location Projection For each pixel (i, j) on the SR image, the location projection is to find the (i ′ , j ′ ) on the LR image. We think the value of the pixel (i, j) is decided by the feature of (i ′ , j ′ ) on the LR image. We do the following projection operator to map these two pixels:</p><formula xml:id="formula_1">(i ′ , j ′ ) = T (i, j) = i r , j r<label>(2)</label></formula><p>where T is the transformation function. ⌊⌋ is floor function.</p><p>The Location Projection can be seen as a kind of variable fractional stride <ref type="bibr" target="#b18">[19]</ref> mechanism which could upscale the feature maps with arbitrary scale factor. As shown in the Fig <ref type="figure" target="#fig_2">2</ref>, if the scale factor r is 2, each pixel (i ′ , j ′ ) determines two points. However, if the scale factor is non-integer, such as r = 1.5, some pixels determine two pixels and some pixels determine one pixel. For each pixel (i, j) on the SR image, we could find a unique pixel (i ′ , j ′ ) on the LR image and we think these two pixels are most related.</p><p>Weight Prediction For the typical upscale module, it predefines the number of filters for each scale factor and learns W from the training dataset. Different from the typ- ical upscale module, our Meta-Upscale Module uses a network to predict the weights of the filters. We can formulate the weight prediction as:</p><formula xml:id="formula_2">W(i, j) = ϕ(v ij ; θ)<label>(3)</label></formula><p>where W(i, j) are the weights of filter for pixel (i, j) on the SR image, v ij is a vector related with i, j. ϕ(.) is weight prediction network and takes the v ij as input. θ is the parameters of the weight prediction network.</p><p>As for the input of ϕ(.) for pixel (i, j), the proper choice is the relative offset to the (i ′ , j ′ ), the v ij can be formulated as:</p><formula xml:id="formula_3">v ij = i r − i r , j r − j r<label>(4)</label></formula><p>In order to train the multiple scale factor together, it is better to add the scale factor into the v ij to differentiate the weights for different scale factor. For example, if we want to upscale the image with scale factor 2 and 4, and we denote them as I SR is the subimage of the I SR 4 . It would limit the performance. Thus, we redefine the v ij as:</p><formula xml:id="formula_4">v ij = i r − i r , j r − j r , 1 r<label>(5)</label></formula><p>Feature Mapping We extract the feature of the (i ′ , j ′ ) on the LR image from F LR . And we predict the weights of the filters with weight prediction network. The last thing we need to do is mapping feature to the value of the pixel on the SR image. We choose the matrix product as the Feature Mapping function. We formulate the Φ(.)as:</p><formula xml:id="formula_5">Φ(F LR (i ′ , j ′ ), W(i, j)) = F LR (i ′ , j ′ )W(i, j)<label>(6)</label></formula><p>Our Meta-Upscale Module is shown in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Meta Upscale Module</head><p>Input: scale:r, the size of input image: (inH, inW ), the weight prediction function:W, the feature: F LR Output: the upscale image 1: Calculate the output size outH = int(inH × r), outW = int(inW × r) 2: for i = 0 : 1 : outH do 3:</p><p>for j = 0 : 1 : outW do 4:</p><formula xml:id="formula_6">v ij = ( i r − ⌊ i r ⌋, j r − ⌊ j r ⌋, 1 r ) 5: (i ′ , j ′ ) = (⌊ i r ⌋, ⌊ j r ⌋) 6:</formula><p>the feature on (i ′ , j ′ ):</p><formula xml:id="formula_7">F LR (i ′ , j ′ ) 7:</formula><p>weight predicted by ϕ: W(i, j)</p><formula xml:id="formula_8">8: pv = F LR (i ′ , j ′ ) • W(i, j) 9:</formula><p>the pixel value on (i, j) is pv 10:</p><p>end for 11: end for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Architecture Details of Meta-SR</head><p>There are two modules in our Meta-SR network, the Feature Learning Module, and the Meta-Upscale Module. Most the state-of-the-art methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b33">34]</ref> could be selected as our Feature Learning Module. The proposed Meta-Upscale Module could be applied to these networks by simply replacing the traditional upscale module (sub-pixel convolution <ref type="bibr" target="#b21">[22]</ref>). We choose the state-of-the-art SISR network, called residual dense network ( RDN <ref type="bibr" target="#b35">[36]</ref> ) as our Feature Learning Module. Note that our Meta-SR can also work with EDSR or MDSR <ref type="bibr" target="#b17">[18]</ref> or RCAN <ref type="bibr" target="#b34">[35]</ref>. For the RDN <ref type="bibr" target="#b35">[36]</ref>, there are 3 convolutional layers and 16 residual dense blocks (RDBs). Each RDB has 8 convolutional layers. The growth rate for the dense block is 64. And the extracted feature map has 64 channels. The detailed structure is shown in Fig. <ref type="figure" target="#fig_0">1</ref>. More details can be found in RDN <ref type="bibr" target="#b35">[36]</ref>.</p><p>For the Meta-Upscale Module, it consists of several fully connected layers and several activation layers. Each input will output one group of weights with the shape (inC, outC, k, k). Here the inC is the number of channels of the extracted feature map, and the inC = 64 in the paper. The outC is the number of channels of the predicted HR image. Generally, outC = 3 for color images and outC = 1 for grayscale image. The k represents the size of the convolution kernel.</p><p>Here we want to describe the parameters of the proposed Meta-Upscale Module including the number of hidden neurons, the number of the fully connected layers, the choice of activation function and the kernel size of the convolution layer. Since the output size (k 2 × inC × outC) is very large compared with the input size (3), we set the number of the hidden neurons as 256. Continuing to increase the number of the hidden neurons has no improvements. And the activation function is ReLU. We conduct experiments and find that the best number of the fully connected layer is 2 with the balance of the speed and the performance. As for the kernel size, 3 × 3 is the best size . Conducting 5 × 5 convolution operation on the large feature maps is more time-consuming.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Metrics</head><p>In the NTIRE 2017 Challenge on Single Image Super Resolution, a high-quality dataset DIV2K <ref type="bibr" target="#b25">[26]</ref> is newly released. There are 1000 images in DIV2K database, 800 images for training, 100 images for validation and 100 images for test. All of our models are trained with DIV2K training images set. For testing, we use four standard benchmark datasets: Set14 <ref type="bibr" target="#b32">[33]</ref>, B100 <ref type="bibr" target="#b19">[20]</ref>, Manga109 <ref type="bibr" target="#b11">[12]</ref> and DIV2K <ref type="bibr" target="#b25">[26]</ref> . Note that the ground truth of the DIV2K test set is not publicly available. Therefore, we report the results on the DIV2K validation set. The super-resolution results are evaluated with PSNR and SSIM <ref type="bibr" target="#b29">[30]</ref>. Following the setting in <ref type="bibr" target="#b35">[36]</ref>, we only consider the PSNR and SSIM <ref type="bibr" target="#b29">[30]</ref> on the Y channel of the transformed YCbCr color space.</p><p>As for the degradation methods to generate the lowresolution images, following <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b35">36]</ref>, we use the bicubic interpolation by adopting the Matlab function imresize to simulate the LR images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training Details</head><p>In the single image super-resolution, the traditional loss function is L2 loss. Following the setting of <ref type="bibr" target="#b17">[18]</ref>, we train our network using L1 loss instead of the L2 for better convergence.</p><p>During training the network, we randomly extract 16 LR RGB patches with the size of 50*50 as a batch input. Following the setting in <ref type="bibr" target="#b35">[36]</ref>, we randomly augment the patches by flipping horizontally or vertically and rotating 90 • . The optimizer is Adam. The learning rate is initialized to 10 −4 for all the layers and decreases by half for every 200 epochs. All experiments run in parallel on 4 GPUs. The training scale factors of the Meta-SR vary from 1 to 4 with stride 0.1, and the distribution of the scale factors is uniform. Each patch image in a batch has the same scale factor. Our Meta-SR is trained with Meta-Upscale Module from scratch. Table <ref type="table">1</ref>. Results of arbitrary upscale on different methods. The EDSR is based on residual block. And the RDN is based on the dense connection block. The test dataset is B100 <ref type="bibr" target="#b19">[20]</ref>. The Best results is black.</p><p>scale. Thus we could input the LR image into the network G to generate the HR image. And then we downscale the HR image with scale factor r k to predict the final results. If the k = 2, we call them RDN(x2), and EDSR(x2) respectively. For the scale r &gt; k, we have to upscale the LR image before feeding it into the network. If the k = 4, it is the fourth baseline. We call them RDN(x4) and EDSR(x4) respectively.</p><p>In order to prove the superiority of the Weight Prediction and the Location Projection, we design the fifth baseline (BicuConv): we use the interpolation to upscale the final feature maps, the upscale module is fixed convolution layer for all scale factor. And the sixth baseline (Meta-Bicu) is interpolating the feature maps to the needed size. We use the Weight Prediction network to predict the weights of the convolution filter for each scale factor. We train all these models on arbitrary scale factor together.</p><p>The experimental results are shown in Table <ref type="table">1</ref>. For the bicubic interpolation baseline, simply upscaling the LR image with bicubic interpolation could not introduce any texture or details to the HR images. It has very limited performance. For the RDN(x1) and EDSR(x1), it has low-performance on the large scale factors. And the upscaled input makes it time-consuming. For the RDN(x4) and EDSR(x4), the performance has huge gap between our Meta-RDN and RDN(x4) (or Meta-EDSR and EDSR(x4)) for scale factor close to 1. Moreover, EDSR(x4) and RDN(x4) also have to upscale the LR image before feeding it into the network when the the scale r &gt; k.</p><p>Thanks to the Weight Prediction, both Meta-Bicu and our Meta-SR could learn the best weights of the filter for each scale factor while BicuConv shares the same weights of the filter for all the scale factors. The experimental results show that Meta-Bicu is significantly better than BicuConv which proves the superiority of the Weight Prediction Module. At the same time, our Meta-RDN is also better than the Meta-Bicu. For the interpolation on the feature maps, the larger the scale factor is, the smaller the valid Filed Of View (FOV) is. However, each scale factor has the same FOV in our Meta-SR methods. Benefited from the proposed Meta-Upscale, our Meta-RDN achieves the better performance on almost all scale factors than the other baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">The Inference Time</head><p>SISR is low-level image processing task and has very high practical use. In real-world scenarios, the time requirements are very important and strict for SISR. We measure the computing efficiency using Tesla P40 with Intel Xeon E5-2670v3@2.30GHz. We choose the B100 <ref type="bibr" target="#b19">[20]</ref> as the test dataset. Here, we do not take the image pre-processing time into consideration.</p><p>We conduct experiments to calculate the running time of each module in our Meta-SR and the baselines. As shown Although the computing efficiency of our Meta-SR on single scale has no advantage when we compare with the baselines RDN(x1), RDN(x2) and RDN(x4) on scale r = 2. If we increase the scale factor to 8 or 16, our Meta-SR is less time-consuming than these baselines. Moreover, if we want to continuously zoom in the same image with different scale factors like the common image viewer, our Meta-SR is the fastest. Since our Meta-SR method only need to run the Meta-Upscale Module for each scale factor. RDN(x1) and RDN(x2) have to upscale the input image at first. And then they feed the upscaled image into whole network for each scale factor. Thus, we claim that our Meta-RDN is more efficient and has better performance than these baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Comparison With The SOTA Methods</head><p>We apply the proposed Meta-Upscale Module to the RDN <ref type="bibr" target="#b35">[36]</ref> by replacing the typical upscale module, called Meta-RDN . We train our Meta-RDN on the DIV2K training images with random scale factor r ∈ <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b3">4]</ref>. We compare the Meta-RDN with the corresponding baseline RDN <ref type="bibr" target="#b35">[36]</ref>. For fair comparison, we also try to finetune our Meta-RDN for each single scale factor. However, fine-tuning on each single integer scale factor have few improvement to the final performance. And the RDN re-trained the model for each scale factor with different upscale module, including X2, X3, X4. We test our Meta-RDN on four different benchmarks with PSNR and SSIM metrics.</p><p>As shown in Table <ref type="table">3</ref>, the Meta-RDN achieve the comparable or even better results compared with the corresponding baseline RDN <ref type="bibr" target="#b35">[36]</ref>. Since the proposed Meta-Upscale could dynamically predict weights of the filter for each scale and thanks to the weight prediction, we could train a single model for multiple scale factors and work well at the arbitrary scale factor. Moreover, our Meta-SR network only need to save one model for test, but the typical model needs to save several models. Our Meta-SR network is more efficient for SR of multiple scale factors. Table <ref type="table">3</ref>. Compared with the state-of-the-art methods on X2,X3,X4. The reported results of the state-of-the art methods are re-trained for each scale factor. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Visual Results</head><p>In this section, we show the visual results in Fig. <ref type="figure" target="#fig_4">3</ref> and Fig. <ref type="figure" target="#fig_3">4</ref> . As shown in Fig. <ref type="figure" target="#fig_3">4</ref>, we compare with the RDN(x1), RDN(x2) and RDN(x4) for super-resolution of arbitrary scale factor. Our Meta-SR has better performance for the structure part. Since the RDN(x1), RDN(x2) and RDN(x4) share the same the weights of filters for all the scale factors, the texture of the SR image generated by these baseline methods is worse than our Meta-RDN. Thanks to the weight prediction, our Meta-SR can predict a group of independent weights for each scale factor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We propose a novel upscale module named Meta-Upscale to solve the super-resolution of arbitrary scale factor with a single model. The proposed Meta-Upscale Module could dynamically predict the weights of the filters. For each scale factor, the proposed Meta-Upscale Module generates a group of weights for the upscale module. By doing convolution operation between the feature maps and the filters, we generate the HR image of arbitrary size. Thanks to the weight prediction, we can train a single model for super-resolution of arbitrary scale factor. Especially, our Meta-SR can continuously zoom in the same image with multiple scale factors.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. An instance of our Meta-SR based on RDN [36]. We also call the network Meta-RDN. (a) The Residual Dense Block proposed by RDN [36]. (b) The Feature Learning Module which generates the shared feature maps for arbitrary scale factor. (c) For each pixel on the SR image, we project it onto the LR image. The proposed Meta-Upscale Module takes a sequence of coordinate-related and scale-related vectors as input to predict the weights for convolution filters. By doing the convolution operation, our Meta-Upscale finally generate the HR image.</figDesc><graphic url="image-1.png" coords="3,62.49,72.00,470.25,230.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>2. The Location Projection projects pixel onto the LR image. And the Weight Prediction Module predicts the weights of the filter for each pixel on the SR image. At last, the Feature Mapping function maps the feature on the LR image with the predicted weights back to the SR image to calculate the value of the pixel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. The schematic diagram for how to upscale the feature map with the non-integer scale factor r = 1.5. Here we only show the one-dimensional case for simplify.</figDesc><graphic url="image-2.png" coords="4,332.49,72.00,189.00,208.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>2 and I SR 4 respectively.</head><label>4</label><figDesc>The pixel (i, j) on I SR 2 would have the same weights of the filter and the same projection coordinate with the pixel (2i, 2j) on I SR 4 . That means that I SR2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. The visual comparison of an single image upsampling with different scale factors by our Meta-RDN.</figDesc><graphic url="image-10.png" coords="8,314.77,387.08,90.50,72.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. The visual comparison with four baselines. Our Meta-RDN has better performance.</figDesc><graphic url="image-17.png" coords="8,314.77,467.95,96.81,64.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>Comparison running time with the baselines. FL represents the Feature Learning Module. WP is Weight Prediction Module of our Meta-SR. Upscale is the Upscale Module. We test on the B100 and the test scale factor is 2. Table.2. Compared with the Feature Learning Module, the running time of our Weight Prodiction Module can be neglected. Because there are only two fully connected layers in our Meta-Upscale Module.</figDesc><table><row><cell>Methods</cell><cell>FL</cell><cell>WP</cell><cell>Upscale</cell></row><row><cell>RDN(x1)</cell><cell>3.66e-2s</cell><cell>-</cell><cell>1.7e-4s</cell></row><row><cell>RDN(x2)</cell><cell>3.29e-2s</cell><cell>-</cell><cell>1.9e-4s</cell></row><row><cell>RDN(x4)</cell><cell>3.13e-2s</cell><cell>-</cell><cell>3.2e-4s</cell></row><row><cell cols="4">Meta-RDN 3.28e-2s 1.5e-4s 3.6e-4s</cell></row><row><cell>in the</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This research is supported by National Key R&amp;D Program of China (2017YFA0700800, 2016YFB1001002, 2016YFB1001000), National Natural Science Foundation of China (61525306, 61633021, 61721004, 61420106015), Capital Science and Technology Leading Talent Training Project (Z181100006318030), and Beijing Science and Technology Project (Z181100008918010).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Single Model For Arbitrary Scale Factor</head><p>Since no previous approach has focused on the superresolution of arbitrary scale factor with a single model, we need to design several baselines. We compare our approach with these baselines to prove the superiority of Meta-SR.</p><p>Suppose we want to zoom in the LR image with scale r ∈ <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b3">4]</ref>. Before we feed it into the network, we could upscale it with bicubic interpolation. Thus, the first baseline simply upscales the LR image with bicubic interpolation as the final HR image, called bicubic baseline. The second approach upscales the LR image r times at first, and then input it into a CNN to generate the final HR image, called EDSR(x1) and RDN(x1) respectively. These two methods are very time-consuming and hard to put into practical use.</p><p>The third baseline downscales the generated HR image. Suppose there is a network G to implement the k times up- </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to learn by gradient descent by gradient descent</title>
		<author>
			<persName><forename type="first">M</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shillingford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">De</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Memory matching networks for one-shot image recognition</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2003">June 2018. 3</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Super-resolution through neighbor embedding</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2004. Proceedings of the 2004 IEEE Computer Society Conference on</title>
				<imprint>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Decouple learning for parameterized image operators</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
				<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="442" to="458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.7495</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Super-resolution reconstruction of compressed video using transform-domain statistics</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Gunturk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Altunbasak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Mersereau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Image super-resolution via dual-state recurrent networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Witbrock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2002">June 2018. 2</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep backprojection networks for super-resolution</title>
		<author>
			<persName><forename type="first">M</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ukita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2002">June 2018. 2</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning to segment every thing</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cornell University arXiv Institution: Ithaca</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Single image superresolution from transformed self-exemplars</title>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep video super-resolution network using dynamic upsampling filters without explicit motion compensation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Wug</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Joo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2018-06">June 2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Accurate image superresolution using very deep convolutional networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deeply-recursive convolutional network for image super-resolution</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Metalearning: a survey of trends and technologies</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lemke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Budka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gabrys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence review</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE conference on computer vision and pattern recognition (CVPR) workshops</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, 2001. ICCV 2001. Proceedings</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Optimization as a model for fewshot learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cardiac image super-resolution with global correspondence using multi-atlas patchmatch</title>
		<author>
			<persName><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M S M</forename><surname>De Marvao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dawes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Oregan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Image super-resolution via deep recursive residual network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Memnet: A persistent memory network for image restoration</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4539" to="4547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ntire 2017 challenge on single image super-resolution: Methods and results</title>
		<author>
			<persName><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops (CVPRW), 2017 IEEE Conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1110" to="1121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Anchored neighborhood regression for fast example-based super-resolution</title>
		<author>
			<persName><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">De</forename><surname>Smet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
				<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1920" to="1927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Recovering realistic texture in image super-resolution by deep spatial feature transform</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2002">June 2018. 2</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning to learn: Model regression networks for easy small sample learning</title>
		<author>
			<persName><forename type="first">Y.-X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="616" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Metaanchor: Learning to detect objects with customized anchors</title>
		<author>
			<persName><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A novel image fusion method using ikonos satellite images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yıldırım</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Güngör</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Geodesy and Geoinformation</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On single image scale-up using sparse-representations</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zeyde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Protter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on curves and surfaces</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="711" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning a single convolutional super-resolution network for multiple degradations</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2005">June 2018. 5</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Image super-resolution using very deep residual channel attention networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.02758</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Residual dense network for image super-resolution</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2007">2018. 1, 2, 3, 5, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Very low resolution face recognition problem</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
