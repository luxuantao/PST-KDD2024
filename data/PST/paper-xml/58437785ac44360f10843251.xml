<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-cue fusion for emotion recognition in the wild</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jingwei</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Key Laboratory of Child Development and Learning Science of Ministry of Education</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Biological Science and Medical Engineering</orgName>
								<orgName type="institution">Southeast University</orgName>
								<address>
									<postCode>210096</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Wenming</forename><surname>Zheng</surname></persName>
							<email>wenming_zheng@seu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Key Laboratory of Child Development and Learning Science of Ministry of Education</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Biological Science and Medical Engineering</orgName>
								<orgName type="institution">Southeast University</orgName>
								<address>
									<postCode>210096</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhen</forename><surname>Cui</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
								<address>
									<postCode>210094</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chuangao</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Key Laboratory of Child Development and Learning Science of Ministry of Education</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Biological Science and Medical Engineering</orgName>
								<orgName type="institution">Southeast University</orgName>
								<address>
									<postCode>210096</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">School of Information Science and Engineering</orgName>
								<orgName type="institution">Southeast University</orgName>
								<address>
									<postCode>210096</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuan</forename><surname>Zong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Key Laboratory of Child Development and Learning Science of Ministry of Education</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Biological Science and Medical Engineering</orgName>
								<orgName type="institution">Southeast University</orgName>
								<address>
									<postCode>210096</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-cue fusion for emotion recognition in the wild</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">267D4379A8FDF5F40A8DF3902885414D</idno>
					<idno type="DOI">10.1016/j.neucom.2018.03.068</idno>
					<note type="submission">Received date: 22 July 2017 Revised date: 1 March 2018 Accepted date: 28 March 2018 Preprint submitted to Neurocomputing May 3, 2018 ACCEPTED MANUSCRIPT</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Emotion recognition</term>
					<term>Convolutional neural network (CNN)</term>
					<term>Facial landmark action</term>
					<term>Multi-cue fusion</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Emotion recognition has become a hot research topic in the past several years due to the large demand of this technology in many practical situations. One challenging task in this topic is to recognize emotion types in a given video clip collected in the wild. In order to solve this problem we propose a multi-cue fusion emotion recognition (MCFER) framework by modeling human emotions from three complementary cues, i.e., facial texture, facial landmark action and audio signal, and then fusing them together. To capture the dynamic change of facial texture we employ a cascaded convolutional neutral network (CNN) and bidirectional recurrent neutral network (BRNN) architecture where facial image from each frame is first fed into CNN to extract high-level texture feature, and then the feature sequence is traversed into BRNN to learn the changes within it.</p><p>Facial landmark action models the movement of facial muscles explicitly. SVM and CNN are deployed to explore the emotion related patterns in it. Audio signal is also modeled with CNN by extracting low-level acoustic features from segmented clips and then stacking them as an image-like matrix. We fuse these models at both feature level and decision level to further boost the overall performance. Experimental results on two challenging databases demonstrate</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Analyzing human emotion plays a key role in a growing number of occasions.</p><p>For example, based on emotion and other physiological states we can do lie detection. In computational advertising domain, recognizing consumers' emotion help enterprises evaluate their advertisements. Doctors make use of emotion 5 recognition technology to do researches on Parkinson's disease. With such great demand in human-computer interaction, eduction, medical and health service <ref type="bibr" target="#b0">[1]</ref>, recent years have witnessed more and more attention in this area, especially in real-life cases like emotion recognition in the wild <ref type="foot" target="#foot_0">1</ref> and recognizing spontaneous emotions. <ref type="bibr" target="#b10">10</ref> Traditional emotion recognition researches are based on databases collected under lab-controlled environment, such as CK+ <ref type="bibr" target="#b1">[2]</ref> and multipie <ref type="bibr" target="#b2">[3]</ref>. The participants were asked to pose certain emotions directly. As a result some of the samples are not natural and even quite exaggerated. On the contrary, emotion recognition in the wild utilizes databases gathered from real life or similar sce-15 narios such as video clips in movies, TV dramas and reality shows, which makes it much closer to practical applications. In recent years, such databases have appeared in several emotion related competitions due to their challenging difficulty. In 18th ACM International Conference on Multimodal Interaction (ICMI 2016), Acted Facial Expressions in the Wild (AFEW 6.0) which is composed of 20 video clips from hollywood movies and reality TV shows is deployed <ref type="bibr" target="#b3">[4]</ref>. <ref type="bibr">In</ref>  Database (CHEAVD) which consists of video clips selected from Chinese movies and TV programs is used <ref type="bibr" target="#b4">[5]</ref>. Compared to the traditional databases, these ones 25 contain complex background information, various illumination conditions and different kinds of occlusions and noises.</p><p>In this paper, for the purpose of emotion recognition in the wild, we propose a multi-cue fusion emotion recognition (MCFER) framework which models human emotion from three complementary aspects, i.e., facial texture, facial landmark 30 action and audio signal. Facial texture is the most obvious and important cue that reflects human emotions directly. We employ a cascaded CNN-BRNN architecture to capture high-level facial texture features and the dynamic changes within sequential video frames. In this architecture, facial image on each frame is first fed into a transferred VGG-Face network <ref type="bibr" target="#b5">[6]</ref>. And then output of the  Researchers usually take video and audio modalities into consideration separately. Then efficient approaches like multi-modal factorized bilinear pooling <ref type="bibr" target="#b13">[13]</ref> or its generalized version <ref type="bibr" target="#b14">[14]</ref> can further be employed to fuse the multimodal features. Alternatively, results of different modalities can be fused at decision level. For the aspect of video, CNN and RNN are extensively employed hidden units, to model the feature sequences. Instead of RNN, Fan et al. <ref type="bibr" target="#b16">[16]</ref> utilized long short-term memory (LSTM) <ref type="bibr" target="#b17">[17]</ref> to model the temporal relation within feature sequences which were extracted by a fine-tuned CNN. Besides 100 that they also deployed 3D convolutional neutral network (C3D) <ref type="bibr" target="#b18">[18]</ref> to model video sequences directly. Different combinations of models were evaluated to test the overall performance. Yao et al. <ref type="bibr" target="#b19">[19]</ref> designed a new type of CNN named HoloNet which was inspired by GoogLeNet <ref type="bibr" target="#b21">[20]</ref> and deep residual network <ref type="bibr" target="#b22">[21]</ref>.</p><p>In their framework inception block and residual block were used repeatedly to  Once the transferred VGG-Face model is obtained, we take facial images from each video as input and output of the last fully connected layer of VGG-Face network, i.e., fc7, which is a 4096-dimension vector, as the extracted highlevel facial texture feature. Thus every video clip is transformed into a sequence of 4096-dimension feature vectors. Then we employ RNN to capture the dynamic differences within the facial feature sequence so that variations of facial components can be modeled to recognize facial expressions. In classical RNN structure data sequence is fed in chronologically. At time step t the hidden state h t can be formulated as:</p><formula xml:id="formula_0">h t = Φ(Uh t-1 + Wx t ),<label>(1)</label></formula><p>where U and W are weight matrices corresponding to the previous hidden state h t-1 and the input x t . Φ is the non-linear activation function. Equation <ref type="formula" target="#formula_0">1</ref>shows that RNN is capable of memorizing a series of previous states and combine them with the current input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>190</head><p>Usually we treat the video clip as a single directional frame sequence, i.e., the frames are displayed in chronological order. However as we observe through the whole process of human facial expression, from the very beginning to the end, a simple fact is that the intensity of facial expression changes from weak to strong, strong enough to reach the peak, and then decays gradually till the expression vanishes. This observation offers us another point of view to look into facial expressions. Assume the frame sequence is reversed, there is still a sequence of frames which can be treated as ordinary facial expression. One small difference is that expression intensity from the beginning to the peak usually takes longer time than the decay process. Therefore, we can utilize both the forward and backward sequence together to reveal more temporal relations inside the facial feature sequence. To this end, we employ Bidirectional RNN (BRNN) <ref type="bibr" target="#b6">[7]</ref> to learn from both directions at the same time. Here two directional hidden states are calculated separately as follows.</p><formula xml:id="formula_1">h f t = Φ(U f h f t-1 + W f x f t ), h b t = Φ(U b h b t-1 + W b x b t ).<label>(2)</label></formula><p>The notation with superscript f indicates the forward sequence order, while b indicates the opposite. Figure <ref type="figure" target="#fig_5">2</ref> shows the framework of BRNN. The red dotted box contains hidden states from both directions. Note that if the backward process is removed, it will degenerate into an ordinary RNN model. After we get h f t and h b t , the two directional hidden states are added up at the corresponding time step:</p><formula xml:id="formula_2">h t = h f t + h b T -t+1 ,<label>(3)</label></formula><p>where T is the length of the video clip. Back propagation through time is employed to train the network. The flowchart of CNN-BRNN is shown in the top dotted box of Figure <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">SVM and CNN model for facial landmark action</head><p>Besides facial texture there is more implicit emotion related knowledge con- Landmark coordinates on the ith frame are constructed into a vector:</p><formula xml:id="formula_3">p (i) = [x (i) 1 , y (i) 1 , ..., x (i) m , y (i) m , ..., x<label>(i)</label></formula><p>51 , y</p><formula xml:id="formula_4">(i) 51 ] ,<label>(4) where (x (i) m , y (i)</label></formula><p>m ) is the mth landmark coordinate. We normalize the elements in p (i) using the following equations:</p><formula xml:id="formula_5">x(i) m = x (i) m -x (i) c δ (i) x , ỹ(i) m = y (i) m -y (i) c δ (i) y .<label>(5)</label></formula><p>Here (x</p><formula xml:id="formula_6">(i) c , y<label>(i)</label></formula><p>c ) is the central landmark coordinate, i.e., apex of nose, and (δ</p><formula xml:id="formula_7">(i) x , δ (i)</formula><p>y ) is the standard deviation of (x, y) coordinates at the ith frame. After we get the normalized vector of landmark coordinates for each frame, a fixed number of these vectors are concatenated together as the feature of this video clip: p = [p (1) , p(2) , ..., p(n) ] ,</p><p>where n is the number of frames picked from every video according to the average video length in the database. After we obtain a high-dimensional feature vector for each video, an SVM model is then trained to classify the samples.</p><p>As facial muscle motions indicate the expressions directly and explicitly, we believe there is more information encoded in the movement of landmarks that 235 is worth exploring. Therefore a CNN based landmark action model is proposed.</p><p>For each video clip a fixed number of n frames are randomly selected. A slight difference with the SVM based method is that we adopt all the 68 landmarks here because using more landmarks would be more advantageous for CNN to capture the local spatial information of the face. Then the coordinates are 240 stretched into a 136-dimension vector for every frame as equation 4 does. After that we join these vectors together to form a n * 136 image-like matrix as the input of CNN. As shown in Figure <ref type="figure" target="#fig_7">4</ref>, a CNN model of 4 convolutional layers and 2 fully connected layers is designed to explore high-level features in facial landmark action. Since the irrelevant sound in audio signal is difficult to remove, the recognition 255 rate is not satisfactory. However the audio cue can still help boost the overall performance in the fusion procedure.</p><p>Inspired by the work <ref type="bibr" target="#b31">[30]</ref>, we propose a similar framework which employs CNN to deal with audio features. For each audio signal, as shown on the bottom of Figure <ref type="figure" target="#fig_0">1</ref>, we first segment it into many overlapping small clips by employ- Framework of the designed CNN is similar to Figure <ref type="figure" target="#fig_7">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Multi-cue fusion</head><p>Details of the three cues and associated models are presented previously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>270</head><p>As facial textures, facial landmark action and audio signal help to recognize emotions from different aspects, it is reasonable to consider them to be complementary to some extent. In order to combine the expression related information explored form different cues together, two fusion strategies, i.e., from feature level and from decision level, can be adopted. </p><formula xml:id="formula_9">S = ω 1 S V ideo + ω 2 S Landmark SV M + ω 3 S Landmark CN N + ω 4 S Audio ,<label>(7)</label></formula><p>where ω i is the weight of each model and </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We conduct experiments on two challenging databases released for the purpose of competition, i.e., AFEW 6.0 and CHEAVD. Both competitions provide Due to the scale of AFEW 6.0 database is relatively small, we put the train-325 ing and validation set together and employ 3-fold cross validation to tune our models, including configurations of the networks and the weights of score ma-      </p><formula xml:id="formula_10">A C C E P T E D M A N U S C R I P T</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper we propose multi-cue fusion emotion recognition (MCFER)</p><p>to address the problem of emotion recognition in the wild based on three vital cues, i.e., facial texture, facial landmark action and audio signal. For facial </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FinetunedFigure 1 :</head><label>1</label><figDesc>Figure 1: An overview of our proposed multi-cue fusion emotion recognition (MCFER) framework. From the top to bottom three red dashed boxes correspond to three cues which are facial texture, facial landmark action and audio signal. They are respectively modeled by the CNN-BRNN model, landmark action based CNN &amp; SVM model, and a CNN model based on temporal-pyramid features. Results from these models are finally fused together to predict the emotion category.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>35( 1 )( 2 )( 3 )</head><label>123</label><figDesc>last fully connected layer, which is treated as the extracted high-level featureA C C E P T E D M A N U S C R I P Tof each frame, is sequentially transmitted into a bidirectional recurrent neural network (BRNN)<ref type="bibr" target="#b6">[7]</ref> in order to model the dynamic changes of facial textures.To make the most of facial information, we develop a new modality called facial landmark action and use it as our second cue. Facial landmark action, namely 40 trajectories of landmark points, can be regarded as a derivative of action units (AUs)<ref type="bibr" target="#b7">[8]</ref>. It aims to utilize trajectories of landmark points to describe facial muscle movements in the video sequence. Moreover, different from AUs, landmark action can be presented explicitly and accurately based on the landmark coordinates on each frame, which makes it an alternative effective approach to 45 describe emotions. Therefore we transform the facial landmark actions in each video into an image-like matrix and employ classical CNN to learn the patterns contained in it. Meanwhile audio signal can serve as an auxiliary cue. To obtain acoustic features of different temporal regions, we segment the audio sequence into several clips by employing a temporal-pyramid structure and then extract 50 low-level acoustic features from each one. These features of all clips within an audio sequence are stacked into an image-like matrix. Similar to the landmark action model we deploy CNN to explore the acoustic patterns. Finally, we fuse the models at both feature level and decision level to boost the performance of emotion recognition. The entire MCFER framework is shown in Figure 1. From 55 the top to bottom three dashed boxes depict our method for three cues respectively. Experimental results on AFEW 6.0 and CHEAVD show the superiority of our proposed framework. Our main contributions are summarized as follows. We deploy BRNN when modelling facial texture changes. In contrast to conventional RNN, BRNN can capture more dynamic change information 60 in the sequence without adding much computational cost. We investigate a new modality called facial landmark action which is close related to facial expression. After normalization the landmark movement can be presented explicitly and precisely to describe the motion of facial muscles so as to recognize emotions. 65 When dealing with audio signal, a simple yet effective audio-CNN struc-A C C E P T E D M A N U S C R I P T ture is proposed. Despite that audio signal contains a certain amount of noise, fusion result shows that they can still boost the performance of our framework. The rest of this paper is organized as follows. Section 2 presents a series of 70 related work on this problem. Section 3 describes our proposed MCFER framework in detail. Section 4 demonstrates our experiments on two wild datasets respectively. Finally Section 5 concludes the paper. 2. Related work In the past several decades, emotion recognition, including facial expres-75 sion recognition (FER) and speech emotion recognition (SER), has been widely studied by many dedicated researchers [8, 9, 10]. Most of the works focus on traditional situations where the image or video samples are under controlled environment such as fixed illumination, clean background and no occlusion. When comes to complicated tasks like emotion recognition in the wild, many of these 80 methods can not be employed directly or perform poorly due to different kinds of disturbances. On the other hand, based on a variety of deep neutral networks, deep learning has achieved huge successes in many research areas such as image classification, object segmentation and face recognition [11, 12]. Among various network architectures, convolutional neutral network (CNN) and recurrent 85 neural network (RNN) are two most fundamental and important network architectures which have shown great superiority in extracting discriminative image features and modeling temporal relationship within sequences. Consequently in order to solve the challenging task of emotion recognition in the wild, more and more CNN and RNN based deep learning methods are proposed recently.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>90</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>S C R I P T to deal with the problem. Kahou et al. [15] made use of three mainstream CNN structures to extract frame features and then deployed RNN with rectified linear</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>125</head><label></label><figDesc>each video. And then a word2vec model<ref type="bibr" target="#b26">[25]</ref> was trained to obtain the text A C C E P T E D M A N U S C R I P T feature. Thus the audio modality was transformed into a new text modality for emotion recognition. Different from these works above, in our framework audio signal is investigated with a designed CNN structure based on the stacked low-level features extracted from segmented audio clips.1303. Proposed methodEmotion recognition in the wild is quite a challenge task. Information from different aspects should be taken into consideration as much as possible. From image to audio, we explore three essential cues, i.e., facial texture, facial landmark action and audio signal, to address this problem. Based on these, a multi-135 cue fusion emotion recognition (MCFER) framework is proposed. In this section we will present MCFER in detail. As shown in Figure1after some preprocessing procedures three methods derived from separate cues are stated. Features or scores of these models are fused together to get the final emotion category. 3.1. Preprocessing 140 As the video clips are gathered from movies, dramas and TV reality shows, almost every video clip contains background information and other irrelevant objects which are disturbances for the task. Obviously facial image is the key part in emotion recognition. Therefore we conduct the following two preprocessing procedures for each frame. 145 Face detection. There are many mature face detection tools available nowadays. Here we use single shot detector (SSD) [26] in each frame of a video. Most of the faces are detected successfully while in a small number of frames SSD returns the wrong results. For those cases we discard the wrong detection part in the image and crop it into a smaller region. Then we run SSD again 150 until the correct facial images is obtained. After that we resize the cropped facial images to 224 * 224 for the following steps. Facial landmark annotation. As facial landmark action is one of the three cues to address the issue, we adopt our own facial landmark annotator [27] to A C C E P T E D M A N U S C R I P T locate 68 key facial points 2 in each facial image. Then the landmarks are aligned 155 using affine transformation based on two reference points. Here we choose the center of each eye as references which are estimated by taking average of the coordinates of six landmark points around the eye.3.2. CNN-BRNN for facial textureFacial texture is our first cue and we employ a cascaded CNN-RNN struc-160 ture to study it. CNN aims to extract discriminative facial features and RNN learns the temporal relationship in the feature sequence. Different from previous research like<ref type="bibr" target="#b23">[22]</ref>, where new CNN architectures are designed and trained with additional data, we adopt the already trained VGG-Face<ref type="bibr" target="#b5">[6]</ref> model 3 directly. VGG-Face network is a 16-layer CNN architecture which has achieved 165 great sucess in face recognition tasks. The model is trained based on 2.6 million images of celebrities collected from the web, which from a certain point of view, can be regarded as face images in the wild. Besides that, facial expression is actually one property of human face. Therefore the already trained VGG-Face model can be easily transferred from face recognition to facial expression recog-170 nition. Consequently we can deploy the transferred model to extract high-level facial texture features of each video frame which ought to be more discriminative than some widely adopted hand-crafted features like SIFT or HoG for the emotion recognition task. Instead of using additional facial expression images to train the VGG-Face 175 model, we take the facial images cropped from video clips as our samples. For simplicity, facial images from one video are regarded as samples whose labels are consistent with the video. Furthermore, in order to balance the training sample numbers for each category we adopt some data augment approaches, such as flipping and rotating, to generate more image samples. Validation set is used A C C E P T E D M A N U S C R I P T to evaluate the network performance during training. Based on these we finetune the VGG-Face model so as to transfer it to a emotion recognition model. Empirical results show that fine-tuning from the fifth convolutional layer, i.e., conv5, which is the last convolutional layer group before fully connected layers of VGG-Face network, can achieve relatively high accuracy while maintain fine 185 generalization ability on the validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: BRNN framework. Inside the red dashed box forward and backward hidden states are calculated. Then corresponding hidden states are added together.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>195Figure 3 :</head><label>3</label><figDesc>Figure 3: Illustration of five landmarks' trajectories sampled every 10 frames from a video clip of which the emotion state is happy. The vertical value is the distance from each landmark to the datum point. On the left corresponding landmark is marked with a circle of the same color. On the right based on trends of the trajectories emotion can be speculated successfully.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>…Figure 4 :</head><label>4</label><figDesc>Figure 4: Framework of CNN based facial landmark action model. N is the batchsize. 3 * 15, 3 * 11, 3 * 7 and 3 * 3 are kernel size of each convolutional layer. 64 and 128 are kernel numbers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>245 3 . 4 .</head><label>34</label><figDesc>CNN model for audio signalWe express our emotions and feelings not only through facial expressions, but also by what we say and the way we say it. It is the volume and pitch of sound that reflect our emotion. Just like a blind man can sense the emotion of other people by listening to their talk, audio signal is able to help recognize 250 A C C E P T E D M A N U S C R I P T the emotion state. Hence it is used as our third cue. As the audio signal in AFEW 6.0 and CHEAVD is not collected under lab controlled environment, a certain amount of background sound and other noises are contained in the data inevitably. A minority of the samples even do not comprise valuable sound.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>260ing a temporal-pyramid structure, so that different temporal information can be acquired. Unlike converting the audio signal to an image by fourier transform<ref type="bibr" target="#b31">[30]</ref>, we extract low-level acoustic features by openSMILE toolbox[31].After extracting features of each short clip, we stack the features of the same audio source together into an image-like matrix as shown in the bottom of Fig-265 ure 1. A 4-layer CNN is then employed to learn more discriminative high-level features for emotion classification. Both channels of the audio signal are used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>275</head><label></label><figDesc>For feature level fusion, we concatenate feature vectors output from three neural network models together and then train an SVM with radial basis function kernel to classify the samples. For decision level fusion, as we know, score matrices output from the proposed models indicate the probabilities of the corresponding sample belonging to different emotion types. Therefore during theA C C E P T E D M A N U S C R I P Ttesting phase, we can assign a proper weight to each score matrix and then add them up to generate the final score matrix S and classify the testing samples:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>4 i=1 ω i = 1 .</head><label>41</label><figDesc>Items such as S V ideo in the right of equation 7 are corresponding score matrix for each model. Here we employ grid search strategy to optimize the weights on the validation set. Note that there is only one score matrix for each model in the equation. However, multiple score matrices from one model can be used due to that variation in 280 parameters often has an effect on model performances and sometimes they are complementary, too.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>285</head><label></label><figDesc>training and validation sets and their corresponding labels. Testing samples without labels are released near the deadline of the competition. As we have participated in the video based emotion recognition challenge of ICMI 2016, the results on testing set are provided by the organizers. So we report our results on testing samples of AFEW 6.0. However we did not jion in the competition 290 of CCPR 2016, therefore only results on validation set are reported. During our experiments no additional data is used. Details of our experiments are presented below. 4.1. General settings Given a frame sequence in a video clip, there is only minor difference between 295 adjacent frames actually. Taking all frames into consideration and calculating through the networks seem unnecessary and demand a lot of computation. Thus in CNN-BRNN model, we fix the length of input video frames to 40 frames by randomly sampling from the original sequence. For videos whose length A C C E P T E D M A N U S C R I P T is shorter than 40 frames, we repeat the first frame in the beginning of the 300 sequence until we get one. As the competition databases are relatively small in scale, in order to enhance the generalization ability of the learnt models, we perform data augment by flipping video frames or rotating them with a small angles. Also we only employ one layer BRNN. And the learning rate is set to be 0.003. 305 In the landmark action model, based on the same concern above, we sample each video into a sequence of 30 frames randomly. Thus the input of CNN is an N * 30 * 136 matrix, where N is the batchsize. The convolutional parameters are shown in Figure 4. In the CNN based audio model, we segment the audio signal to 28 overlapping clips by using a temporal-pyramid structure, then the 310 input dimension is N * 28 * 1582, where 1582 is the feature dimension 5 extracted by openSMILE toolbox. The 4-layer convolutional parameters are respectively 32 * 3 * 15, 32 * 3 * 11, 32 * 3 * 7 and 32 * 3 * 3. The learning rate of both CNN is set to be 0.001. 4.2. Experiments on AFEW 6.0 315 AFEW 6.0 database consists of video clips collected from both hollywood movies and TV reality shows, including a total of 773 training samples, 383 validation samples and 593 testing samples. We need to assign each testing sample a label from the six basic emotions (angry, disgust, fear, happy, sad and surprise) and neutral. As the reality show clips only appear in testing set 320 and they are more spontaneous than movie characters' performance, it is quite difficult to recognize their emotions. Baseline method takes LBP-TOP feature as video descriptor and SVR as classifier and achieves 38.81% and 40.47% on validation and testing set respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Confusion matrices of our models on AFEW 6.0 testing set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Confusion matrices of our models on CHEAVD validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>395 texture, we take advantage of the transferred VGG-Face network to extract high-level feature of each frame and then deploy BRNN to model the dynamic change within the feature sequence. Facial landmark action is another intuitive cue as the trajectories of landmarks demonstrate the whole process of facial expressions vividly. We stretch the landmark coordinates into an image-like 400 matrix for each video clip and model them by SVM and CNN separately. Audio signal is the last but not least cue. Low-level acoustic features of the audio clips</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Performances of 3-fold cross validation of the proposed models and fusion approaches.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Method</cell><cell></cell><cell cols="3">Accuracy (%)</cell></row><row><cell></cell><cell></cell><cell cols="3">CNN-BRNN</cell><cell></cell><cell>44.46</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">Trajectory-SVM</cell><cell></cell><cell>37.37</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">Trajectory-CNN</cell><cell></cell><cell>35.73</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">Audio-CNN</cell><cell></cell><cell>30.88</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">Feature Fusion</cell><cell></cell><cell>46.57</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">Decision Fusion</cell><cell></cell><cell>49.22</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Angry</cell><cell>Disgust</cell><cell>Fear</cell><cell>Happy</cell><cell>Neutral</cell><cell>Sad</cell><cell>Surprise</cell></row><row><cell></cell><cell>Angry</cell><cell>73.60</cell><cell>2.03</cell><cell>2.54</cell><cell>4.57</cell><cell>10.66</cell><cell>3.55</cell><cell>3.05</cell></row><row><cell></cell><cell>Disgust</cell><cell>22.81</cell><cell>9.65</cell><cell>5.26</cell><cell>16.67</cell><cell>27.19</cell><cell>10.53</cell><cell>7.89</cell></row><row><cell></cell><cell>Fear</cell><cell>18.90</cell><cell>0.00</cell><cell>29.13</cell><cell>3.94</cell><cell>25.20</cell><cell>16.54</cell><cell>6.30</cell></row><row><cell></cell><cell>Happy</cell><cell>8.92</cell><cell>1.41</cell><cell>1.88</cell><cell>68.07</cell><cell>9.39</cell><cell>8.45</cell><cell>1.88</cell></row><row><cell></cell><cell>Neutral</cell><cell>13.04</cell><cell>0.97</cell><cell>5.31</cell><cell>5.80</cell><cell>58.94</cell><cell>11.11</cell><cell>4.83</cell></row><row><cell></cell><cell>Sad</cell><cell>14.04</cell><cell>1.12</cell><cell>7.30</cell><cell>11.24</cell><cell>16.85</cell><cell>48.32</cell><cell>1.12</cell></row><row><cell></cell><cell>Surprise</cell><cell>13.33</cell><cell>5.00</cell><cell>15.83</cell><cell>12.50</cell><cell>22.50</cell><cell>11.67</cell><cell>19.17</cell></row><row><cell></cell><cell cols="8">Figure 5: Confusion matrix of 3-fold cross validation.</cell></row><row><cell></cell><cell cols="8">trices at the fusion step. The performances of different models of 3-fold cross</cell></row><row><cell></cell><cell cols="8">validation are listed in Table 1. Compared to other models, the cascaded CNN-</cell></row><row><cell>330</cell><cell cols="8">BRNN performs better and achieves an accuracy of 44.46%. Model fusion im-</cell></row><row><cell></cell><cell cols="8">proves the overall performance furthermore. By fusing all the scores at decision</cell></row><row><cell></cell><cell cols="8">level, an accuracy of 49.22% is obtained, which outperforms other single model</cell></row><row><cell></cell><cell cols="8">alone and the feature fusion method. From this point of view, it proves that</cell></row><row><cell></cell><cell cols="8">facial textures, facial landmark action and audio signal are complementary to</cell></row><row><cell>335</cell><cell cols="8">some extent. The corresponding confusion matrix is shown in Figure 5.</cell></row><row><cell></cell><cell cols="8">During the test phase, each team has eight opportunities to submit their test-</cell></row><row><cell></cell><cell cols="8">ing sample labels. We test the performances of single CNN-BRNN model and</cell></row><row><cell></cell><cell cols="8">different combination of models. CNN-BRNN alone reaches an overall accuracy</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Top 3 performances on testing samples.</figDesc><table><row><cell></cell><cell>Method</cell><cell>Accuracy (%)</cell></row><row><cell></cell><cell>Baseline [4]</cell><cell>40.47</cell></row><row><cell></cell><cell>1 CNN-RNN + 3 C3Ds [16]</cell><cell>59.02</cell></row><row><cell></cell><cell>HoloNet [19]</cell><cell>57.84</cell></row><row><cell></cell><cell>CNN with extra images [22]</cell><cell>56.66</cell></row><row><cell></cell><cell>Decision Fusion</cell><cell>56.66</cell></row><row><cell></cell><cell cols="2">After decision fusion we achieve an encouraging accuracy of 56.66%, which</cell></row><row><cell>350</cell><cell cols="2">significantly surpasses the baseline by 16.19%. Confusion matrix is shown in</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Compared with other teams, our framework ranks third in all participants and is in first place among all teams from academia.The winner of the challenge deploys 3 C3D models and 1 CNN-RNN<ref type="bibr" target="#b16">[16]</ref> and obtain an accuracy of 59.02%.Bargal et al. [22]  also get the same result as we</figDesc><table /><note><p>360 do. However they use 148 thousand extra images which are annotated by 12-15 crowd workers when training three deep CNN models. 4.3. Experiments on CHEAVD CHEAVD is composed of video clips from Chinese movies and dramas. There are 1981 training samples and 243 validation samples with eight emotion classes, 365 i.e., angry, anxious, disgust, happy, neutral, sad, surprise and worried. As we have not participated in the competition, we can not evaluate our method on testing samples. We use the training set to tune our model parameters and test them on validation set. Results on validation set are reported and analyzed. The baseline method extracts LBT-TOP features for video and low-level energy 370 features for audio. Then random forest is employed as classifier. The results on validation set are listed in</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>We compare our method with Xia et al.[32] and Huang et al.<ref type="bibr" target="#b24">[23]</ref> as they achieved the best result on testing set in terms of video and audio modality separately. From Table3both our CNN-BRNN and trajectory based CNN model achieve better results</figDesc><table><row><cell>375</cell></row><row><cell>than [32]. CNN-BRNN alone surpasses the baseline by nearly 14%. Facial</cell></row><row><cell>landmark action based CNN model also achieves competitive performance which</cell></row><row><cell>indicates the effectiveness of this new modality. As for audio modality, Huang et</cell></row><row><cell>al. [23] obatain a surprisingly good result due to the outstanding performance</cell></row><row><cell>of the transferred speaker recognition model. In comparison with other fusion</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Experimental results on CHEAVD validation set.</figDesc><table><row><cell></cell><cell>Method</cell><cell>Accuracy (%)</cell></row><row><cell></cell><cell>Baseline on video [5]</cell><cell>37.04</cell></row><row><cell></cell><cell>Baseline on audio [5]</cell><cell>41.56</cell></row><row><cell></cell><cell>Feature Fusion [5]</cell><cell>41.98</cell></row><row><cell></cell><cell>Decision Fusion [5]</cell><cell>40.33</cell></row><row><cell></cell><cell>HoG+HMM on video [32]</cell><cell>47.00</cell></row><row><cell></cell><cell>Transfer learning on audio [23]</cell><cell>50.01</cell></row><row><cell></cell><cell>IS13+CNN+DSIFT+wordvec [24]</cell><cell>53.90</cell></row><row><cell></cell><cell>CNN-BRNN</cell><cell>51.03</cell></row><row><cell></cell><cell>Trajectory-SVM</cell><cell>44.86</cell></row><row><cell></cell><cell>Trajectory-CNN</cell><cell>48.97</cell></row><row><cell></cell><cell>Audio-CNN</cell><cell>43.21</cell></row><row><cell></cell><cell>Feature Fusion</cell><cell>52.26</cell></row><row><cell></cell><cell>Decision Fusion</cell><cell>55.14</cell></row><row><cell></cell><cell cols="2">models like Chen et al. [24] proposed, our decision level fusion method performs</cell></row><row><cell></cell><cell>best and achieves an overall accuracy of 55.14%.</cell></row><row><cell></cell><cell cols="2">Confusion matrices of each model and the decision fusion result are shown</cell></row><row><cell></cell><cell cols="2">in Figure 7. It is notable that the training and validation set is extremely</cell></row><row><cell>385</cell><cell cols="2">unbalanced. For example, there are only 45 training and 5 validation samples</cell></row><row><cell></cell><cell cols="2">of disgust compared to a total of 815 neutral samples in the database. Despite</cell></row><row><cell></cell><cell cols="2">the data augment procedure employed to balance the sample number of each</cell></row><row><cell></cell><cell cols="2">category, the models still can not learn sufficient knowledge from the limited</cell></row><row><cell></cell><cell cols="2">samples. This makes it quite challenging to recognize the emotion which has</cell></row><row><cell>390</cell><cell cols="2">small sample number. As a result, our models perform poorly on anxious and</cell></row><row><cell></cell><cell cols="2">worried and none of the disgust sample is correctly classified.</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>In the wild is contrary to under the lab-controlled environment. It means in the uncontrolled and real-world scenarios.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://ibug.doc.ic.ac.uk/resources/facial-point-annotations/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>VGG is the abbreviation of Visual Geometry Group of University of Oxford. More information can be referred to http://www.robots.ox.ac.uk/ ~vgg/software/vgg_face/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>AU6 means cheeks raised and eyes narrowed. AU12 means lip corners pulled up and laterally<ref type="bibr" target="#b29">[28]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>Here we use INTERSPEECH 2010 paralinguistics challenge configuration.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep learning for visual understanding: A review</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oerlemans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Lew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">187</biblScope>
			<biblScope unit="page" from="27" to="48" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ambadar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-pie</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="807" to="813" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Emotiw 2016: Video and group-level emotion recognition challenges</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gedeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICMI</title>
		<imprint>
			<biblScope unit="page" from="427" to="432" />
			<date type="published" when="2016">2016</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Mec 2016: the multimodal emotion recognition challenge of ccpr</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">420</biblScope>
			<biblScope unit="page" from="667" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TSP</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Recognizing action units for facial expression analysis</title>
		<author>
			<persName><forename type="first">Y.-I</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="97" to="115" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName><surname>A C C E P T E D M A N U S C R I P T</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-view facial expression recognition based on group sparse reduced-rank regression</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TAC</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="85" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Speech emotion recognition 430 using fourier parameters</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TAC</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="69" to="75" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural networks: An overview of early research, current frameworks and new challenges</title>
		<author>
			<persName><forename type="first">A</forename><surname>Prieto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Prieto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Ortigosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pelayo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Rojas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">214</biblScope>
			<biblScope unit="page" from="242" to="268" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">E</forename><surname>Alsaadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A survey of deep 435 neural network architectures and their applications</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">234</biblScope>
			<biblScope unit="page" from="11" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Multi-modal factorized bilinear pooling with co-attention learning for visual question answering</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<imprint>
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Beyond bilinear: Generalized multi-440 modal factorized high-order pooling for visual question answering</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.03619</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recurrent neural networks for emotion recognition in video</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Konda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICMI</title>
		<imprint>
			<biblScope unit="page" from="467" to="474" />
			<date type="published" when="2015">2015</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Video-based emotion recognition using cnnrnn and c3d hybrid networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICMI</title>
		<imprint>
			<biblScope unit="page" from="445" to="450" />
			<date type="published" when="2016">2016</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning spa-450 tiotemporal features with 3d convolutional networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>ICCV</publisher>
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Holonet: towards robust emotion recognition in the wild</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICMI</title>
		<imprint>
			<biblScope unit="page" from="472" to="478" />
			<date type="published" when="2016">2016</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName><surname>A C C E P T E D M A N U S C R I P T</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Emotion recognition in 460 the wild from videos using images</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Bargal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Barsoum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Ferrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICMI</title>
		<imprint>
			<biblScope unit="page" from="433" to="436" />
			<date type="published" when="2016">2016</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Transfer learning of deep neural network for speech emotion recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="721" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Emotion recognition 465 in videos via fusing multimodal features</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="632" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<title level="m">Efficient estimation of word representations in vector space</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<title level="m">470 Ssd: Single shot multibox detector</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Recurrent shape regression, IEEE TPAMI (under review</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Comprehensive database for facial expression analysis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<editor>FG, IEEE</editor>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="46" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Joint fine-tuning in deep neural networks for facial expression recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>ICCV</publisher>
			<biblScope unit="page" from="2983" to="2991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.08301</idno>
		<title level="m">Feedforward sequential memory networks: A new structure to learn long-term dependency</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">He is currently a Professor with the Key Laboratory of Child Development and Learning Science, Ministry of Education, Southeast University</title>
		<author>
			<persName><surname>A C C E P T E D M A N U S C R I P T Biography</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingwei</forename><surname>Of</surname></persName>
		</author>
		<author>
			<persName><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Electronic and Information Engineering from Northwest A&amp;F University in 2013, and M.S. degree in Signal and Information Processing from Beijing Normal University in 2016. Currently, he is a Ph</title>
		<title level="s">Southeast University in 2011. Currently, he is pursuing the Ph.D. degree in Key Laboratory of Child Development and Learning Science of Ministry of Education, School of Biological Sciences and Medical Engineering in Southeast University. His research interests include affective computing, deep learning and pattern recognition</title>
		<meeting><address><addrLine>Fuzhou, China; Quanzhou, China; Nanjing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004. 2006, and 2014. 2014 to 2015</date>
		</imprint>
		<respStmt>
			<orgName>Department of Instrument Science and Engineering ; Center for Learning Science, Southeast University ; Shandong Normal University, Sun Yat-sen University, and Institute of Computing Technology (ICT ; Department of Electrical and Computer Engineering at National University of Singapore (NUS ; Nanjing University of Science and Technology ; D. candidate of Key Laboratory of Child Development and Learning Science of Ministry of Education in Southeast University</orgName>
		</respStmt>
	</monogr>
	<note>His research interests include affective computing and pattern recognition. Biography of Tong Zhang: Tong Zhang received the B.S. degree in Department of Information Science and Technology. Southeast University in 2011, the M.S. degree in Research Center for Learning Science, Southeast University in 2014. Currently, he is pursuing the Ph.D. degree in information and communication engineering in Southeast University. His interests include pattern recognition, machine learning and computer vision. Biography of Yuan Zong: Yuan Zong received the B.S. and M.S. degrees in Electronics Engineering both from Nanjing Normal University in 2011 and 2014, respectively. Currently, he is pursuing the Ph.D. degree in</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
