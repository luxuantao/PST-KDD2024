<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Drones</forename><surname>Interac</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Real Reality Interfaces #chi4good</orgName>
								<address>
									<addrLine>CHI 2016</addrLine>
									<settlement>San Jose</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anton</forename><surname>{g</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Real Reality Interfaces #chi4good</orgName>
								<address>
									<addrLine>CHI 2016</addrLine>
									<settlement>San Jose</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C8CA6453B35DB711AF42B64DB58F89E2</idno>
					<idno type="DOI">10.1145/28580</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T17:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present BitD reality 3D dis evitating tangi step towards in n which the u structures. We d equipped with ShapeDrones, a 3D printed fr DisplayDrones, ouchscreen. W bimanual input resize of individ user interface e canvases and a and depict futu self-levitating p Author Keywo Organic User Claytronics; R Programmable M ACM Classifica H.5.m. Informa Miscellaneous. NTRODUCTIO The thought physically emb been around for he "Ultimate D controlled the e and Margolus [ of small, para geometrically s kind of materi Permission to mak personal or classro not made or distrib bear this notice an components of thi Abstracting with c post on servers or t and/or a fee. Reque CHI'16, May 07-12</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="bibr" target="#b17">[18]</ref><p>. A grammable mat for a full ms -somethin ces was unable ogress toward us forms of sel oms) <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b37">38]</ref>, m been theoretica system of Cato physical user e would promise A tegrated with re RI) that render matter. The pr toms need to upport by othe While there has ment of individu ted <ref type="bibr" target="#b30">[31]</ref>. We using nano-qu rations of s ications <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b25">26]</ref> real-time use </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contribution</head><p>In this paper, we present BitDrones, an interactive, glassesfree, 3D tangible display that uses nano-quadcopters as selflevitating Catoms that serve as real reality voxels (see Figure <ref type="figure">1</ref>). Our prototype is a first step towards interactive selflevitating tangible user interfaces with multiple building blocks that are capable of physically representing 3D data on the fly. We discuss three types of BitDrones, each representing Catoms of distinct resolutions: 1) PixelDrones, which are equipped with an RGB LED and an OLED display. PixelDrones represent a single illuminated voxel or, optionally, a text label; 2) ShapeDrones, which are BitDrones augmented with a lightweight acrylic mesh spun over a 3D printed frame in a larger geometric shape, such as a cube or a sphere. ShapeDrones only have one RGB LED, used to illuminate a lightweight mesh diffuser that covers the 3D frame. ShapeDrones allow users to build higher granularity 3D models without requiring drones to stack on top of each other in mid-flight; 3) DisplayDrones, which carry a flexible touchscreen display, molded in an arc, with an Android 5.1 smartphone board. DisplayDrones are able to render highresolution images, such as contextual menus, pictures and videos. We present both unimanual and bimanual input techniques, including touching, dragging, throwing and resizing of BitDrones and compound models, as well as user interface elements, including flying cone trees, video displays and alert boxes. We conclude with application scenarios in 3D editing, molecular modeling, real reality information visualization and telepresence. While the BitDrones system at present only allows for sparse models of up to 12 Catoms, unlike VR or AR, it promises to fully preserve natural depth and tactile-kinesthetic cues without requiring headsets or simulated haptics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BACKGROUND</head><p>We will first discuss prior work in the areas of Tangible User Interfaces, Augmented and Mixed Reality Interfaces, Actuated Shape Displays, Programmable Matter and Self-Levitating Interfaces, as they relate to the design of our system. Note that we purposefully omitted a discussion of VR interfaces as these systems are designed to be fully immersive, i.e., do not allow the user to interact with the real world. We also discuss ongoing work in the emerging field of Human-Drone Interaction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tangible User Interfaces</head><p>Tangible User Interfaces (TUIs) leverage our ability to sense and manipulate the material world by providing physical form to digital information, facilitating direct engagement with the physical world. TUIs were introduced by Ishii and Ullmer <ref type="bibr" target="#b16">[17]</ref> as an effort to augment the real world by coupling digital information to everyday physical objects and environments. Over the years, tangible interfaces have been extensively explored in electronic music platforms <ref type="bibr" target="#b19">[20]</ref>, urban planning <ref type="bibr" target="#b33">[34]</ref>, constructive assembly <ref type="bibr" target="#b34">[35]</ref>, as well as graspable interfaces <ref type="bibr" target="#b8">[9]</ref>. However, TUIs have limited ability to display changes applied to the physical objects by software processes or remote users. To overcome the limitations of tangibles, Radical Atoms <ref type="bibr" target="#b16">[17]</ref> described a hypothetical physical material that could be coupled with an underlying digital model, allowing for human interactions with computationally transformable physical materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Augmented Reality Interfaces</head><p>There is a large body of work aimed at interacting with virtual elements in 3D environments, which we cannot possibly do justice in this review. Robertson et al. <ref type="bibr" target="#b36">[37]</ref> designed cone trees as a means of visualizing file hierarchies in a 3D user interface. In cone trees, folders and files are arranged in a circular manner, with each level of the hierarchy containing a horizontal "wheel" of folder or file nodes. Robertson et al. also investigated other forms of navigating 3D Graphical User Interfaces (GUIs), but was limited to a 2D display. By contrast, Augmented Reality systems use partially transparent displays to superimpose graphics onto real objects or environments <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b23">24]</ref>. They can be combined with passive tangible input in the real world, for example, using AR Toolkit markers <ref type="bibr" target="#b15">[16]</ref>. HoloLens <ref type="bibr" target="#b14">[15]</ref> introduced a semi-transparent head-mounted display that maps the space around the user using a fully integrated depth camera, allowing the system to simultaneously model its environment as well as sense marker-less gestural input. Shader Lamps <ref type="bibr" target="#b35">[36]</ref> introduced a method for rendering AR directly onto physical objects, thus merging the display with the real world. Researchers have deployed such 3D projection-mapping techniques to create interactive 3D physical objects <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14]</ref> and to simulate textures and material properties of designs <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19]</ref>. 3D projection mapping typically requires expensive capturing of the 3D topography of the projection space. LightSpace <ref type="bibr" target="#b46">[47]</ref> showed how cheap arrays of Kinect depth cameras could be combined with multiple projectors to augment any surface in an entire room with rich interactive graphics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mixed Reality Interfaces</head><p>Mixed Reality <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref> is a special class of augmented and virtual reality technologies for creating environments wherein real and virtual world objects are presented together in a single display. Milgram and Colquhoun Jr. <ref type="bibr" target="#b27">[28]</ref> formulated a global taxonomy to describe how the virtual and real aspects of Mixed Reality environments are combined and experienced, with the ultimate objective of clarifying conceptual boundaries existing among noted research. Benford et al. <ref type="bibr" target="#b4">[5]</ref> experimented with projecting a virtual poet into a real theatre as part of a live performance. They suggested that their "early experience of social interaction between physical and synthetic spaces implies the need for a more systematic approach to joining them together". According to them, real-world interactions combine the local with the physical, VR combines the remote with the synthetic, while AR combines the local with the synthetic. They proposed mixed reality as a new form of shared space that integrates the local with the remote and the physical with the synthetic. An example mixed reality system, MirageTable <ref type="bibr" target="#b5">[6]</ref>, allowed users to mix real and virtually projected 3D objects on a curved tabletop. It used a depth camera to map objects and users around the tabletop, then project these back onto a remote table such that remote and local users could interact with one another. This allowed users to virtually share physical objects in a way that was fully spatially congruous. In this paper, we suggest working towards merging the local, remote, physical and synthetic in such a way that they become one and the same, at least when it comes to the sharing of objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Actuated Shape Displays</head><p>Actuated shape displays provide novel opportunities for experiencing and interacting with digital content in the physical world, by dimensioning the actual displays through actuation. Lumen <ref type="bibr" target="#b32">[33]</ref> explored the design of an actuated pixelated relief display. Lumen used light guides actuated through shape memory alloys to display low-resolution images with some z dimension. Leithinger and Ishii's Relief system <ref type="bibr" target="#b24">[25]</ref> featured an actuated tabletop display that rendered interactive 3D shapes with a wider range of z actuation. Their system used a series of motorized pins actuating a fabric display that was top projected with images. More recently, inFORM <ref type="bibr" target="#b10">[11]</ref> explored the use of highresolution dynamic relief to display digital information. inFORM's shape display provided for variable stiffness rendering and real-time user input through direct touch and tangible interaction. Although shape displays can provide a bi-directional interface between bits and matter, including visual, haptic and force feedback experiences, they are limited in the kind of shapes they can render. E.g., the aforementioned explorations cannot render shapes with holes inside their structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Programmable Matter</head><p>Recently, there has been a significant research effort towards developing a new generation of computer interface capable of displaying physical 3D structures via programmed movement of large quantities of self-assembling nanorobots <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b45">46]</ref>. Goldstein et al. <ref type="bibr" target="#b11">[12]</ref> alluded to the creation of physical artifacts using a collection of such Catoms <ref type="bibr" target="#b12">[13]</ref>, which would eventually be able to mimic an analog object's shape, movement, visual appearance, and tactile qualities. Alonso-Mora et al. <ref type="bibr" target="#b1">[2]</ref> presented a display in which each pixel is a mobile robot of controllable color. Their system took images or animations as input and produced a multirobot display as output. Similarly, Rubenstein et al. proposed a thousand-robot swarm capable of self-assembling into larger, potentially interactive, shapes <ref type="bibr" target="#b38">[39]</ref>. Practical attempts at creating claytronic atoms, however, have proven difficult.</p><p>While the above systems successfully demonstrated programmable self-assembly of complex shapes, thus mitigating some of the limitations associated with shape displays, they are limited to rendering shapes in two dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-Levitating Interfaces</head><p>An important problem in the creation of 3D programmable matter is that the structural integrity of the object needs to be preserved while it is changing shape <ref type="bibr" target="#b38">[39]</ref>. This can be difficult to achieve with robotic motes as they have to rest on top of one another to overcome gravity. To address this issue, researchers have investigated the use of magnetic <ref type="bibr" target="#b22">[23]</ref> and ultrasonic self-levitation <ref type="bibr" target="#b30">[31]</ref>. However, these methods pose distinct limitations to the independent motion of multiple Catoms. Karagozler et al. <ref type="bibr" target="#b20">[21]</ref> proposed the use of robotic helium balloons, or Giant Helium Catoms. The problem with these structures is that they are inherently difficult to miniaturize.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Drones and User Interaction</head><p>While there is a body of work that investigated noninteractive swarms of quadcopters acting together as a scalable production means <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b25">26]</ref>, flying displays <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41]</ref>, augmented sports <ref type="bibr" target="#b28">[29]</ref> and feedback mechanisms <ref type="bibr" target="#b41">[42]</ref>, further research is necessary to ascertain what are the most effective methods to facilitate human-drone interaction <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8]</ref>. Pfeil et al. <ref type="bibr" target="#b31">[32]</ref> explored 3D spatial interaction metaphors for interacting with drones. Their approach aimed at generalizing interaction metaphors that could potentially be applied to future user interfaces. This concept was further explored by Cauchard et al. <ref type="bibr" target="#b7">[8]</ref>, who conducted a Wizard-of-Oz elicitation study to evaluate how users naturally interact with drones. Their results suggested that users interact with drones in a similar way they would interact with a person or a pet. Nitta et al.'s <ref type="bibr" target="#b28">[29]</ref> Hoverball demonstrated a flying controllable ball to integrate imaginary dynamics into ball sports. The authors investigated new ball-playing vocabularies, such as hovering, anti-gravity, proximity, or remote manipulation, as a method to extend the way people experience ball-based sports. Researchers have also explored the concept of free-floating midair displays via flying robots equipped with projectors <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b39">40]</ref> and high-resolution displays <ref type="bibr" target="#b40">[41]</ref>. In contrast to traditional static displays, free-floating displays have the potential to change their position to appear at any given point in space and approach the user to communicate information. While these explorations facilitated human-drone interaction, they lacked support for multimodal I/O between the drone and the user. Additionally, they were limited to a single drone that primarily acted as a visual information display. Instead, we investigate how users can interact with swarms of nano-quadcopters via direct touch, unimanual and bimanual input techniques and gestural interactions. Our goal is to create the first system that uses multiple drones that act together to create interactive tangible interfaces, and generate real reality 3D displays using nanoquadcopters as self-levitating tangible building blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DESIGN RATIONALE</head><p>To inform the design of BitDrones, we considered the following design parameters:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modularity and Granularity</head><p>We designed the system in a modular fashion, like Lego ™ , allowing users to build structures out of individual, selflevitating building blocks. Structures can have varying granularity, i.e., consisting of a few to a dozen drones, depending on the complexity of the compound object. We compensated for a lack of density in building blocks by</p><p>Real Reality Interfaces #chi4good, CHI 2016, San Jose, CA, USA allowing the use of higher resolution, sub-voxel imaging via drone-mounted displays.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Drone-Mounted Displays</head><p>The original vision for single voxel Catoms would require thousands of microscopic drones flying in very close proximity. While this may be feasible in the future, we decided to use sub-voxel imaging techniques to enhance imaging resolution of a sparser drone cloud. We designed 3 types of drones, each with a different sub-voxel imaging resolution, and different form factors: PixelDrones that display a single color voxel; 2) ShapeDrones that display larger shapes and 3) DisplayDrones that carry a high resolution display.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Haptics: Tangible vs. Gestural Interaction</head><p>While BitDrones have the ability to follow gestures as well as user's body movements within a tracked space, an important design criterion was for each drone to respond to direct touch. This allows users to directly interact with the real reality interface, without any mapping or prior knowledge of predefined gestural interactions. BitDrones are simply grasped and moved around freespace at will. Groups of BitDrones are manipulated through the use of PixelDrones that serve as handles, allowing for intuitive bimanual 3D translation, rotation and resizing operations. The ability to directly touch and move drones provides haptic experiences not available in VR or AR systems. BitDrones do not require simulation of tactile kinesthetic feedback, as this is naturally provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multiple Form Factors</head><p>Due to their sparseness, we designed BitDrones in a multitude of form factors, allowing users to efficiently build larger objects. While BitDrones can carry any type of 3D printed structure, we limited ourselves to constructing basic 3D geometries using very lightweight, wire mesh materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Physically Immersive User Interface Elements</head><p>The design of our graphical user interface elements was inspired by prior work in the 3D interface space, with the distinction that users physically immerse themselves in the interface, without the aid of virtual reality headsets. Borrowing from Robertson et al. <ref type="bibr" target="#b36">[37]</ref>, e.g., users surround themselves with physical, circular displays showing filenames or content that rotate automatically to stay visible to users at all times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Physics Engine</head><p>We designed the BitDrones system with a built-in physics engine that allows drones to act under the constraint of realworld physics. Single drones, as well as groups of drones, can simulate physical momentum that corresponds to objects moving under friction, to allow greater predictability of their movements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Other Physical Constraints</head><p>Nanodrones are generally limited in payload. The use of specific, lightweight hardware was required to achieve our design goals. For this reason, we used thin-film flexible OLED (FOLED) displays in the design of DisplayDrones. Another physical constraint is the presence of turbulence, mainly caused by the rotors of the drones. This means BitDrones cannot fly directly above or below one another, and need some clearance in order to achieve stable flight. In general, the smaller the drones, the tighter formations they can form.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IMPLEMENTATION</head><p>In BitDrones, each nano-quadcopter represents a Catom that can hover anywhere inside a volume of 4m x 4m x 3m in size. Our system currently supports up to 12 drones in simultaneous flight. BitDrones can hover 15cm from one another and their individual accuracy is 5cm per axis. As an exception, however, when placed against one another, they can fly as a group in tight formations (see Fig. <ref type="figure">1</ref>). Users can walk around the interaction volume and interact with drones by touching and dragging them. BitDrones can be used for input, output, or for both at the same time. Simple atomic information can be displayed by a few drones, while more complex 3D data displays can be constructed using up to a dozen drones, providing basic elements for a voxel-based 3D modeling system capable of representing sparse 3D graphics in real reality.  design can only be used to represent relatively sparse 3D voxel models.</p><p>The lack of pixel density and resolution achievable with our system poses considerable challenges to support tasks that require a lot of detailed information. We considered combining floating voxels with a contextual large graphic display to complement the lack of resolution and density, however, we focused our efforts on developing a proof of principle for a tangible three-dimensional floating interface rather than detailed graphic renderings.</p><p>While we claim that BitDrones are immersive without requiring user augmentation, as evidenced by the DisplayDrone touchscreen, our current implementation does rely on the use of a Vicon to track user's interactions with ShapeDrones and PixelDrones. Another limitation of our system is that drones produce a clearly audible noise, a considerable challenge for video telepresence.</p><p>Finally, the system as presented lacks robustness for a large number of user trials. Specifically, the power requirements to run these trials as well more robust PID loops to correct for air draft and turbulence generated by BitDrones in close proximity are technical challenges that need to be addressed in order to perform a meaning and thorough evaluation of our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FUTURE DIRECTIONS</head><p>We aim to increase the number of simultaneously flying BitDrones by allowing drones to connect with one another magnetically, and spin down when stacking on top of one another to create vertical structures. Another solution is to use more robust motors to compensate for downdrafts caused by other drones.</p><p>That said, we do expect to be able to scale up our architecture to include hundreds of smaller drones. One of the benefits of smaller drones is that they generate significantly less turbulence, allowing them to fly in tighter formations.</p><p>To eliminate the need for user augmentation, future BitDrones prototypes will be outfitted with capacitive sensors for all touch and drag operations. Tracking of a remote user's head movements in the video conferencing scenario will be performed using a 3D depth camera, leading to a completely marker-less system that requires no user augmentation to experience immersive 3D tangible interactions.</p><p>While we used DisplayDrones to combat the lack of pixel density and resolution, we believe when miniaturized, and with more drones, our system will more fully embody the discussed interaction scenarios.</p><p>Currently, the battery life of BitDrones is limited to about 7 minutes. To address this limitation, we propose self-docking drones that use inductive recharge stations when not active. Furthermore, the audible noise produced by BitDrones can be improved by mechanical imbalances associated off-the-shelf hardware. Although ShapeDrones are currently safe to touch due to their protective mesh, PixelDrones and DisplayDrones have exposed propellers. We will include guards in future designs, thus ensuring user safety. Our next step is to devise and evaluate our interaction language for future self-levitating tangible user interfaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CONCLUSIONS</head><p>We presented BitDrones, a toolbox for building interactive real reality 3D displays with nano-quadcopters as selflevitating tangible building blocks. Our prototype is a first step towards interactive self-levitating programmable matter, where the user interface is represented using Catomic structures. We discussed 3 types of BitDrones: PixelDrones, with a single color RGB LED and a small OLED display; ShapeDrones, which are BitDrones that carry a pre-molded 3D printed frame in larger geometric shapes; and DisplayDrones, with a curved thin-film 720p FOLED touchscreen and Android functionality. We presented a number of unimanual and bimanual input techniques: touching, dragging, throwing and resizing of individual drones as well as compound models, and self-levitating user interface elements, including cone trees, 3D canvases and alert boxes. Finally, we discussed applications of BitDrones with 3D design, InfoVis and telepresence scenarios that exemplify the potential functionality of this new category of real reality interfaces.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 . 1 .</head><label>21</label><figDesc>Figure 2.1. PixelDrone with RGB LED and OLED Display.</figDesc><graphic coords="4,55.14,61.20,165.36,115.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Figure 1. BitDro opter D mmable</head><label></label><figDesc></figDesc><table><row><cell cols="5">s: Towa ards Us ing 3D</cell><cell>Nanoco</cell></row><row><cell cols="5">ctive Se elf-Levit tating P Program</cell></row><row><cell cols="5">nio Gomes, C Calvin Rub ens, Sean B</cell><cell>raley and R Roel Vertega</cell></row><row><cell></cell><cell cols="4">Hum man Media L Lab, Queen's s University, ,</cell></row><row><cell></cell><cell cols="4">Ki ingston, Onta ario, K7L 3N N6, Canada</cell></row><row><cell cols="5">gomes, braley ey, roel}@cs. .queensu.ca,</cell><cell>j.rubens@q queensu.ca</cell></row><row><cell cols="4">Drones, a toolb box for buildin ng interactive re eal</cell></row><row><cell cols="4">splays that us se nano-quadc copters as se lf-</cell></row><row><cell cols="4">ble building b blocks. Our pro ototype is a fir rst</cell></row><row><cell cols="4">teractive self-le evitating progr rammable matte er,</cell></row><row><cell>user interface</cell><cell cols="3">is represented d using Catom mic</cell></row><row><cell cols="4">discuss three ty ypes of BitDron nes: PixelDrone es,</cell></row><row><cell cols="2">an RGB LED D and a small</cell><cell cols="2">OLED displa ay;</cell></row><row><cell cols="4">augmented wit h an acrylic m mesh spun over r a</cell></row><row><cell cols="4">frame in a l larger geomet tric shape; an nd</cell></row><row><cell cols="2">, outfitted wi th a flexible</cell><cell cols="2">thin-film 720 0p</cell></row><row><cell cols="2">We present a a number of</cell><cell cols="2">unimanual an nd</cell></row><row><cell cols="2">techniques, in cluding touch,</cell><cell cols="2">drag, throw an nd</cell></row><row><cell cols="3">dual drones and d compound m models, as well</cell><cell>as</cell></row><row><cell cols="4">elements such a lert boxes. We as self-levitatin e describe appl ng cone trees, 3 ication scenari 3D ios</cell><cell>F</cell></row><row><cell cols="4">re directions to programmable m owards creating g high-resolutio on matter.</cell><cell>signific cant amount o of research con under various monik kers, such as C</cell></row><row><cell cols="4">rds r Interfaces; Radical Atom ms; Real Rea Tangible U User Interface ality Interface es; es; Matter. ation Keyword ation interfaces ds s and presenta ation (e.g., HC I):</cell><cell>User In nterfaces [48], , and Radical A seek, a at least in part, to utilize prog interfac ce purposes to allow synchro ronization of b bits with atom generat tion of Tangibl le User Interfac [17]. W While there has been pro hardwa are modules ca apable of variou</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(known n as Claytronic c atoms or Cato</cell></row><row><cell>ON</cell><cell></cell><cell></cell><cell></cell><cell>work o on programmab ble matter has b</cell></row><row><cell cols="2">that compute er interfaces</cell><cell cols="2">might somed ay</cell><cell>How to o create a mass sively parallel s</cell></row><row><cell cols="2">body user inte ractions with</cell><cell cols="2">digital data h has</cell><cell>of disp playing two-wa ay immersive p</cell></row><row><cell cols="4">r a long time. I In 1965, Suthe erland envision ed</cell><cell>is an en nduring researc ch goal. This w</cell></row><row><cell>Display" as a</cell><cell cols="3">room in whic ch the comput ter</cell><cell>Reality y (AR) systems s physically int</cell></row><row><cell cols="4">existence of ma atter [42]. Acco ording to Toffo oli</cell><cell>creatin ng Real Reality y Interfaces (RR</cell></row><row><cell cols="4">45], such progr rammable matt ter would cons ist</cell><cell>digital</cell><cell>experiences</cell><cell>using real m</cell></row><row><cell>allel, cellular</cell><cell cols="2">automata no odes capable</cell><cell>of</cell><cell>address s in this pape er is that Cat</cell></row><row><cell cols="4">shaping themse elves in 3D spa ace to create an ny</cell><cell>gravity y, typically via a structural su</cell></row><row><cell cols="3">ial structure. S Since then, th here has been</cell><cell>a</cell><cell>when b building larger r structures. W</cell></row><row><cell cols="4">ke digital or hard oom use is granted buted for profit or c nd the full citatio s work owned by credit is permitted to redistribute to li est permissions fro 2, 2016, San Jose, copies of all or p d without fee prov commercial advan on on the first pa y others than ACM . To copy otherw ists, requires prior om Permissions@a CA, USA. part of this work f vided that copies a ntage and that copi age. Copyrights f M must be honore wise, or republish, r specific permissio for are ies for ed. to on acm.org.</cell><cell>prior w in 3 d address While quadco been l work in this are ea, the movem dimensions is g generally limit s the levitati on problem there have been explor opters for visu ualization appli little work on n interactive, applica ations of 3D dro one displays.</cell></row><row><cell cols="3">BN 978-1-4503-33 362-7/16/05…$15. .00.</cell><cell></cell></row><row><cell cols="2">org/10.1145/28580 036.2858519</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>ones hovering in Displays e Matte aal</head><label></label><figDesc></figDesc><table><row><cell>nducted toward</cell></row><row><cell>Claytronics [13</cell></row><row><cell>Atoms</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>CHI 2016, San Jose, CA, USA</head><label></label><figDesc></figDesc><table><row><cell>Real Reality Interfaces</cell><cell>#chi4good,</cell></row><row><cell></cell><cell>ion.</cell></row><row><cell></cell><cell>ds this goal</cell></row><row><cell></cell><cell>3], Organic</cell></row><row><cell></cell><cell>All of these</cell></row><row><cell></cell><cell>tter for user</cell></row><row><cell></cell><cell>two-way</cell></row><row><cell></cell><cell>ng the first</cell></row><row><cell></cell><cell>e to achieve</cell></row><row><cell></cell><cell>ds building</cell></row><row><cell></cell><cell>lf-actuation</cell></row><row><cell></cell><cell>much of the</cell></row><row><cell></cell><cell>al in nature.</cell></row><row><cell></cell><cell>ms capable</cell></row><row><cell></cell><cell>experiences</cell></row><row><cell></cell><cell>Augmented</cell></row><row><cell></cell><cell>eal objects,</cell></row><row><cell></cell><cell>interactive</cell></row><row><cell></cell><cell>roblem we</cell></row><row><cell></cell><cell>overcome</cell></row><row><cell></cell><cell>er Catoms,</cell></row><row><cell></cell><cell>been some</cell></row><row><cell></cell><cell>ual Catoms</cell></row><row><cell></cell><cell>propose to</cell></row><row><cell></cell><cell>uadcopters.</cell></row><row><cell></cell><cell>warms of</cell></row><row><cell></cell><cell>, there has</cell></row><row><cell></cell><cell>r interface</cell></row></table><note><p>n a tight formati s r</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>2.2. ShapeDrone with RGB LED and Cube-shaped Diffuser. 2.3. DisplayDrone with curved 720p FOLED Android display. Real Reality Interfaces #chi4good, CHI 2016, San Jose, CA, USA</head><label></label><figDesc></figDesc><table><row><cell cols="2">Figure 3. Sphe rical, Cylindric cal and Rhombi cuboctahedral</cell><cell></cell></row><row><cell></cell><cell>ShapeD Drones.</cell><cell></cell></row><row><cell>H Hardware</cell><cell></cell><cell></cell></row><row><cell>W We built three</cell><cell>different types s of BitDrones, , each capable</cell><cell>of</cell></row><row><cell cols="3">d displaying infor rmation at diff ferent resolutio ons: PixelDrone es,</cell></row><row><cell cols="2">S ShapeDrones an nd DisplayDron nes (Figure 2).</cell><cell></cell><cell>F Figure 4.1. Con</cell></row><row><cell cols="2">P PixelDrone Hard rdware</cell><cell></cell><cell>(rotational w</cell></row><row><cell cols="3">F d a X M L o th T p P a to S S w a a d u b S s s to s Figure 2.1 show design. Each Pi a Micro MWC Xbee Wi-Fi rad MultiWii 2.3.3 LED provides optionally equip hat allows them This display is d programmed w PixelDrones ca approximately 7 o 3 min if an O ShapeDrone Ha ShapeDrones c without the OL acrylic mesh fa around their pr diffuser for the up in color to ac be printed in an ShapeDrones a shapes that wo sparsely spaced op and bottom shapes, but othe ws PixelDrone, ixelDrone is eq flight controll dio. PixelDrone as a flight con a colored voxe pped with an A m to display te driven by an A wirelessly throu arry a 300mAh 7 min of flight OLED display is , a nano-quadco quipped with 4 ler board with es run a custom ntrol operating s el to the user. Adafruit 128x6 ext labels and ATmega 328p ch ugh the flight c h battery, prov time. The fligh opter of our ow brushed motor an IMU, and mized version system. An RG PixelDrones a 64 OLED displ simple graphic hip which can b controller boar viding them wi ht time is reduc wn rs, an of GB are ay cs. be rd. ith ed s included. ardware consist of the LED display. In abric spun ove opellers (see F RGB LED, all ct as a larger pr ny shape. Simil allow for the uld normally b d set of drones m for airflow. W er shapes are po same hardwar nstead, they car er a 3D printed Figure 2.2). Th lowing the enti re-shaped voxe lar to pre-shape use of larger, be difficult to s. All shapes h We printed sph ossible (see Fig re as BitDron rry a lightweig d frame mount e mesh acts as ire shape to be el. This frame c ed Lego ™ brick , more compl produce using have holes in th herical and cub nes ght ed s a lit an ks, ex g a he ed gure 3).</cell><cell>and 15 5 cm in size d diagonally. The Androi id 5.1 operatin ng system, rend smartp phone functiona ality. A small mounte ed on the front t of the Display of the d display, allowi ing the drone to Each d drone is also o outfitted with a sound. DisplayDrone es carry a 1000 them w with approxima ately 7 minutes Flight Control Each B BitDrone has a a set of reflect configu uration, allowin ng its x,y,z posi individ dually tracked b by a Vicon MX running g Tracker 3.0 software [49]. the Vi icon system o over Ethernet flight control inform mation to a se Windo ows 8. The BitD Drones OS is on this s computer. It maintains eac object. Objects repre esenting all use applica ation functiona ality orchestrat system m's behavior. Bi itDrones OS al drones via the Vicon n, processing below. For each dron ne, the BitDro motor s signals to the M MultiWii softw to a pa articular locati ion. A set of drone's s movements towards end input a and interface or r application be</cell></row><row><cell cols="3">D F s D r m 6 li d b r a li m DisplayDrone H Figure 2.3 show size diagonally DisplayDrone u radio. MultiWi motor controlle 6A electronic ightweight 128 display, with a p below the pro resolution, flyin any Android ap ight and dynam mounted in a 3D Hardware ws a DisplayD y and capab uses the same fl ii firmware wa ers. Each brush speed contro 80 x 720 LG processor board opellers. Disp ng Graphical U pp. Embedded mically adjust s D printed fram Drone, a quadco le of lifting flight controller as modified to hless motor is oller. DisplayD Display FOL d running Andr layDrones allo User Interface (G light sensors m creen brightnes me, curved 90º a opter 17.5 cm heavier load r and Xbee Wi-o drive brushle outfitted with Drones carry LED touchscre roid 5.1 mount ow for a hig GUI) that can ru measure ambie ss. The display around the dron in ds. -Fi ess h a a en ed gh un ent y is ne,</cell><cell>PID Lo oops Each d drone object i in BitDrones Propor rtional, Integra al and Differen [44]. T These equation ns model each destina ation and velo city errors, ca affect t the drone's thru ust along the x,y z axis. Each PID loop p is tuned to th each in ndividual class s of drone. PID behavio or as it flies to a destination. O its de estination, the e MultiWii f embedd dded on the d drone controls firmwa are has its own n set of PID loo control ls the stability o of the drone. T on the e Vicon system m for this pu hardwa are in each dron ne.</cell></row></table><note><p>ne tree</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>node; 4.2 wheel); 4.3. Expa</head><label></label><figDesc></figDesc><table><row><cell cols="2">. Browsing of su anded 2 nd subm</cell></row><row><cell cols="2">e Android boar rd runs the</cell></row><row><cell cols="2">dering this dron ne with full</cell></row><row><cell cols="2">2.1MP, 30 fps s camera is</cell></row><row><cell>yDrone, to the</cell><cell>bottom left</cell></row><row><cell cols="2">o capture real-t time video.</cell></row><row><cell cols="2">a small stereo s speaker for</cell></row><row><cell cols="2">0 mAh battery , providing</cell></row><row><cell>of flight time.</cell><cell></cell></row><row><cell cols="2">tive markers in n a unique</cell></row><row><cell cols="2">ition and orient tation to be</cell></row><row><cell cols="2">X Motion Captu ure System</cell></row><row><cell cols="2">A Mac Pro co onnected to</cell></row><row><cell cols="2">provides loca ation-based</cell></row><row><cell cols="2">econd Mac Pr ro running</cell></row><row><cell cols="2">a C# applicati on running</cell></row><row><cell cols="2">ch drone's stat te in a C#</cell></row><row><cell cols="2">er interface ele ements and</cell></row><row><cell>te the overall</cell><cell>BitDrones</cell></row><row><cell cols="2">lso tracks user i input to the</cell></row><row><cell cols="2">input events a as outlined</cell></row><row><cell cols="2">ones OS wirele essly sends</cell></row><row><cell cols="2">ware over Wi-Fi i to direct it</cell></row><row><cell cols="2">PID loops con ntrols each</cell></row><row><cell cols="2">positions base ed on user</cell></row><row><cell>ehaviors.</cell><cell></cell></row><row><cell cols="2">OS has its o own set of</cell></row><row><cell cols="2">ntial (PID) con ntrol loops</cell></row><row><cell cols="2">drone's curren nt location,</cell></row><row><cell cols="2">alculating mult tipliers that</cell></row><row><cell cols="2">y and z axis an nd about the</cell></row><row><cell>he weight and</cell><cell>balance of</cell></row><row><cell cols="2">D loops govern n a drone's</cell></row><row><cell cols="2">Once a drone h has reached</cell></row><row><cell>flight control</cell><cell>firmware</cell></row><row><cell cols="2">s a hover-in-p place. This</cell></row><row><cell cols="2">ops, one per mo otor, which</cell></row><row><cell cols="2">The firmware do oes not rely</cell></row><row><cell cols="2">urpose, but on n the IMU</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>ubmenu menu. Real Reality Interfaces #chi4good, CHI 2016, San Jose, CA, USA</head><label></label><figDesc></figDesc><table><row><cell>U User Input</cell><cell></cell><cell></cell></row><row><cell cols="4">T The Vicon also o tracks small r reflective mark kers on the user r's</cell></row><row><cell cols="4">h hands, allowing g for detection n of touch and</cell><cell>move events b by</cell></row><row><cell cols="3">B BitDrone OS. B By estimating</cell><cell>the relative po ositions betwe en</cell></row><row><cell cols="2">m markers on the</cell><cell cols="2">user's hands a and the marker rs on the drone es,</cell></row><row><cell cols="3">th he system dete ects interaction</cell><cell>primitives suc ch as touching</cell><cell>or</cell></row><row><cell cols="3">d dragging of ind dividual drones</cell><cell>across the Vic con space. Wh en</cell></row><row><cell cols="2">a a user picks up</cell><cell cols="2">and moves a d drone, this allo ows the BitDron ne</cell></row><row><cell cols="4">O OS to respond d by telling a a drone to ho over in the ne ew</cell></row><row><cell cols="3">d destination. Inp put primitives</cell><cell>can be com mbined for mo ore</cell></row><row><cell cols="3">c complex interac ctions with 3D</cell><cell>compound obj jects, as outlin ed</cell></row><row><cell>b below.</cell><cell></cell><cell></cell></row><row><cell>IN</cell><cell></cell><cell cols="2">TECHNIQUES S</cell></row><row><cell>G</cell><cell cols="2">nes were used</cell><cell>to construct a a number of us ser</cell></row><row><cell>in</cell><cell cols="3">nts borrowed fr from 3D user in nterfaces:</cell></row><row><cell cols="2">U User Interface</cell><cell>Elements</cell><cell>Figure 5. Use er positioning Sh hapeDrones wit thin a compound ob bject and releas sing them in mid d-</cell></row><row><cell cols="4">3 F m w F P 3D Conetree Me Figure 4 shows menu structure with a top Pixe Figure 4.1). Wh PixelDrones fly enus, Folders a s 6 PixelDrones shaped as a c elDrone node re hen touched, th ying underneat and Files s flying in a gr cone tree. Our epresenting the his expands in th it in a circu roup that forms cone trees beg e root folder (s nto a submenu ular arrangeme s a gin see ent of</cell><cell>rendere ed shape, it par artly addresses require e many PixelDr rones to create Resulti ing designs are e automatically exporte ed, or 3D printe ed. 3D en ngine, from wh here they can</cell><cell>the concern th hat it would continuous larg ge shapes. communicated d to a Unity be texture m mapped and</cell></row><row><cell cols="4">( th b d w ( le d a (Figure 4.2). Th he correspondi browsed by gen drone inside th wheel to rotat (Figure 4.2). W evel of subme drones automat are automatical he name of the ing PixelDron ntle unimanual he submenu. T te with simul When a folder in enus can be tically make sp lly recruited to e folder or file e's display. Su l horizontal thr This causes the lated momentu n a submenu is displayed (Fig pace for this lay o display its co e is displayed o ubmenus can b rows of a sing e entire submen um and frictio s touched, a thi gure 4.3). Oth yer, while dron ontents. When on be gle nu on ird her n a nes</cell><cell>2D Con ntextual Flying Display yDrones are capable e of tracking distanc ce as they wal Menus, Alerts used to repr and Video Win resent contextu dows ual menus the user, stay ying within ar rms length lk around the Vicon space. Contextual menus can be used t to perform me enu commands , like copy and pa aste, on selecte ed drones. Dis splayDrones c can also be used a as a means o of alerting the e user through h a flying remote e users via a vid deo window. notifica ation dialog b box, to show images, or to o represent</cell></row><row><cell cols="4">f in m a to file node is tou nside the file. models. Due to are browsed in p o display the uched, drones a Currently, fil o the limited nu place, reusing d file. When a are recruited to le contents are umber of drone drones from th a file is open show the obje e limited to 3 es available, fil e cone tree men n, one BitDron ect 3D les nu ne</cell><cell>Unima anual Input Tec chniques We de signed a numb ber of unimanu ual, bimanual an and gestural input techniques th hat implement t tangible, di rect touch interac ctions with sing gle drones or co ompound 3D ob bjects.</cell></row><row><cell cols="4">r c m representing th contents of the menu by touchi he menu conti file. Users can inues to hover n save the file r above the 3 and return to th 3D he ing this drone.</cell><cell>Touch Given augmen the presence o of a Vicon sys stem, we sens e touch by nting the user r's hands with h small Vicon n markers.</cell></row><row><cell cols="4">3 W in c p p w H 3D Canvas and When a 3D m ndicated by 4 compound obje placed inside th part of a compo within the 3D Handles also p d Handles model file is o 4 PixelDrones, ect inside this he boundaries o ound object, ma canvas even w provide a conv opened, its 3D which act as 3D canvas. W of this 3D canv aintaining their when the 3D c enient way to D boundaries a s handles to th When drones a vas, they becom r relative locatio canvas is move resize, move are he are me on or ed.</cell><cell>Touch drone i is triggered wh hen the distance e of the user's is smaller than n 0.5 cm. Touc ch operates sim fingers to a milarly to a click in n a GUI. E.g., i if a drone repre esents a folder in a menu, touchin ng the drone r reveals its cont tent. Touching g the drone again c closes the cont tent. A touch o on a drone in a compound object selects the dron ne. A touch on n two different drones in a also sen nse capacitive touch within th heir display. compo ound object se elects the entir re object. Disp playDrones</cell></row><row><cell cols="2">r rotate an entire</cell><cell cols="2">3D Canvas usi ing bimanual in nput techniques s.</cell><cell>Drag</cell></row><row><cell cols="4">3 C P p S a th 3D Compound G Compound obj PixelDrones ac physical 3D d ShapeDrone is added to the ca hat shape. Wh Geometries jects typically cting as spar data structure. that larger, p anvas without r hile this requir y consist of S rse voxels rep The advanta pre-shaped 3D requiring PixelD res a priori kn ShapeDrones presenting som age of using content can b Drones to rend nowledge of th or me a be he der</cell><cell>Drones s can be drag gged by holdin ng them, then moving g them to ano other location within the Vi icon space. physically Upon detecting a d drag, drones w will hover at their new position on. When drone es are dragged d to within the boundaries of a 3 3D canvas, th hey automatic cally become part of a object. compo ound object, m maintaining rela ative position w within that</cell></row></table><note><p><p>NTERACTION</p>Groups of dron nterface elemen</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>air. Real Reality Interfaces #chi4good, CHI 2016, San Jose, CA, USA</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>orienta ation of the fi irst two drone es. 3D Canvas ses can be</cell></row><row><cell></cell><cell></cell><cell></cell><cell>rotated d by holding tw wo handles -o one with each h hand -and</cell></row><row><cell></cell><cell></cell><cell></cell><cell>perform ming a bimanua al rotation gest ture.</cell></row><row><cell></cell><cell></cell><cell></cell><cell>While</cell><cell>we discussed d dragging, resiz zing and rotatio on of entire</cell></row><row><cell></cell><cell></cell><cell></cell><cell>compo ound objects, o our system als o supports tra anslation of</cell></row><row><cell></cell><cell></cell><cell></cell><cell>both co ompound objec cts and single e</cell><cell>lements.</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Gestur ral Input Techn niques</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Follow</cell><cell>the User</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Any dr rone has the c capacity to foll low the user's</cell><cell>movement</cell></row><row><cell></cell><cell></cell><cell></cell><cell>within</cell><cell>the Vicon spa ace. Typically,</cell><cell>however, dron nes stay in</cell></row><row><cell></cell><cell></cell><cell></cell><cell>their lo ocation to pro ovide the user r with spatial</cell><cell>coherence.</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Display yDrones offer</cell><cell>a follow-me f function that al llows them</cell></row><row><cell></cell><cell></cell><cell></cell><cell>to stay y within arm's s reach as the e user moves</cell><cell>around the</cell></row><row><cell cols="3">F Figure 6. User r pinch gest resizing a compo ture, by moving ound object usin g 2 of the total 3 ng a bimanual drones.</cell><cell>space. around d the user. Alth This allows, e e.g., contextua al menu palette es to hover hough we could d have impleme ented other gestura al input techniq ques, such as su ummoning a dr rone from a</cell></row><row><cell cols="3">T D Drones can als so be moved t to out-of-reach h destinations b by Throw</cell><cell>focus o on the use of di irect touch inpu ut in this paper. distanc ce [7,32], we wanted to em mphasize tangi ibility, and</cell></row><row><cell cols="3">h o f a b th d holding them a of that destinati fly in the direc and friction tha by the user. Th he throw force desired destinat and physically ion. Upon detec ction of the thr at is based on th he simulated ph e required to throwing them cting a throw e row with simul heir acceleratio hysics allows u allow a drone m in the directio event, drones w lated momentu on when releas users to estima to move to th on will um ed ate he tion.</cell><cell>Remote te Interactions If two o users are wo orking on the e same object BitDro ones system in n two different t locations, dro one actions using the are auto tomatically syn nchronized betw ween sites. That t is, when a user ed dits the locati ion of a BitDr rone inside a compound object, its movemen nt is automat tically replicat ted in the corresp ponding remote e canvas. This allows users t to remotely</cell></row><row><cell cols="3">P W Pick Up/Remov When not in us se, drones are p placed on a tab ble in their hom me ve</cell><cell>Catom ms. collabo orate on 3D d designs with fu ully synchroniz zed remote</cell></row><row><cell cols="3">lo w th U p d th th a ocation. They when summone hem up and th Upon detectin propellers, hov drones can be r hem towards t hreshold dist automatically la ed to display c can be activ hen releasing th g a pick-up ering in positio removed from the home loca tance of the and and spin do vated either p content, or by hem in mid-ai event, drone on when relea a model by m ation. Once th eir home lo programmatical the user pickin r (see Figure 5 s activate the ased. Conversel moving or tossin ey arrive with ocation, dron lly ng 5). eir ly, ng hin nes own.</cell><cell>APPLIC CATION SCEN NARIOS While the BitDrone es OS can be e used for a applica ations, we deve eloped a numbe er of specific s scenarios of variety of use tha at demonstrate e the versatility y of the system m. Since at present t, BitDrones O OS can only dep ploy up to 12 drones at a time, m most application ns were design ed with sparse 3D models in mind d. 3D Des sign Figure 7 shows how w the BitDrone es OS can be used as a</cell></row><row><cell cols="3">B D H o c tw Bimanual Inpu t Techniques Drag Group Holding two dr object and mov compound obje wo of their han rones with sepa ving the hands ect. 3D Canvas arate hands ins allows users t ses can be dra side a compoun to drag an enti agged by movin nd ire ng ndles.</cell><cell>toolbox x for 3D real exampl le, users are b building an ar reality model ling applicatio ons. In this rchitectural mo odel out of cubed ShapeDrones. ShapeDrones can be added d, removed, and m moved around t the model fre ely. PixelDron nes form a boundi ing box that de efines a 3D can nvas. Users can n resize the model within the can nvas by holdin ng two ShapeD Drones and perform ming a bimanu ual pinch gest ture. They can n rotate the</cell></row><row><cell cols="3">R C in p r p R C in a Resize Group Compound obj nside the com pinch gesture resized by hold performing a bi ects can be re mpound object, (Figure 6). Si ding two handl imanual pinch g esized by hold , then perform imilarly, 3D C les-one with e ding two dron ming a bimanu Canvases can b each hand -an nes ual be nd gesture. Rotate Group Compound obj nside a compo arc, thus perform jects can be ro ound object an ming a bimanu otated by hold nd moving the ual rotation. Oth ding two dron hands in a joi her drones in th nes int he</cell><cell>model arc. W We chose to mo by holding tw wo ShapeDrone es and moving odel the "Cube house project" them in an " by Zafari archite ects [3] as it re epresents a spa arse architectur re in which drones need not stac ck or fly in ver ry close proxim mity. More traditio onal architectur ral models requ uiring stacking of building blocks are more cha allenging to re epresent with t the current system m. The 3D desig gn application i is synchronized d with a 3D visualiz zation in Unity y 3D, which a allows users to render the model with texture m maps and shadin ng, as well as 3 3D print the model after real realit ty edits are com mpleted (see Fig gure 8).</cell></row><row><cell>m model automat tically adjust</cell><cell>their relative</cell><cell>position to th he</cell></row></table><note><p>Real</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Reality Interfaces #chi4good, CHI 2016, San Jose, CA, USA</head><label></label><figDesc></figDesc><table><row><cell cols="2">Figure 7. 3D</cell></row><row><cell cols="2">Figu</cell></row><row><cell cols="2">M Molecular Mod deling</cell></row><row><cell>B BitDrones OS</cell><cell>allows the e</cell></row><row><cell cols="2">p pairings of atom ms in 3D. BitD</cell></row><row><cell>s structures in</cell><cell>3D in mid</cell></row><row><cell cols="2">S ShapeDrones, a allowing users</cell></row><row><cell cols="2">in n real reality. S Spherical Shap</cell></row><row><cell cols="2">to o simulate atom ms. Multiple S</cell></row><row><cell cols="2">s spatial arrangem ment that simul</cell></row><row><cell cols="2">T To simulate c chemical react</cell></row><row><cell cols="2">in ndividual atom ms by dragging</cell></row><row><cell>b bond. These</cell><cell>chemical reac</cell></row><row><cell>a automated use,</cell><cell>for example,</cell></row><row><cell cols="2">a about molecul ar bonds. Th</cell></row><row><cell cols="2">r repulsion and at ttraction forces</cell></row><row><cell cols="2">In nteractive Rea al Reality InfoV</cell></row><row><cell>T The BitDrones</cell><cell>system can als</cell></row><row><cell cols="2">in nteractive data a visualizations</cell></row><row><cell>d drones can be</cell><cell>determined b</cell></row><row><cell cols="2">c creating a ph hysical repres</cell></row><row><cell cols="2">M Manipulating o ne of the drone</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>D editor with ar out of 3 cubed ure 8. Unity 3D r "Cube hous</head><label></label><figDesc></figDesc><table><row><cell>exploration of</cell></row><row><cell>Drones can rep</cell></row><row><cell>d-air using</cell></row><row><cell>to interact with</cell></row><row><cell>peDrones are pa</cell></row><row><cell>ShapeDrones ca</cell></row><row><cell>lates a more co</cell></row><row><cell>tions, a user</cell></row><row><cell>drones in or ou</cell></row><row><cell>ctions can b</cell></row><row><cell>to instruct ch</cell></row><row><cell>he drones can</cell></row><row><cell>s present in mol</cell></row><row><cell>Vis</cell></row><row><cell>so be used to re</cell></row><row><cell>s. For instance</cell></row><row><cell>by a mathema</cell></row><row><cell>entation of t</cell></row><row><cell>es modifies som</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>rchitectural mod d ShapeDrones. rendering of res se" model [3].</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="3">Figure 9. User r inspecting a re emote facility us sing a</cell></row><row><cell></cell><cell></cell><cell cols="3">DisplayDron ne with telepres sence functional lity.</cell></row><row><cell>del composed</cell><cell cols="4">the exp pression, such a as the curvatur re of a paraboli ic function. Other d drones adjust th heir position ac ccordingly, pre eserving the</cell></row><row><cell></cell><cell>spatial</cell><cell cols="2">relations as d defined by the</cell><cell>mathematical</cell><cell>expression.</cell></row><row><cell></cell><cell cols="4">PixelD Drones can be e used to rep present live c cloud-based</cell></row><row><cell></cell><cell cols="2">inform mation, such as</cell><cell cols="2">stock market d data or twitter</cell><cell>feeds. The</cell></row><row><cell></cell><cell cols="4">location on of drones w would, in this e example, be tie ed to some</cell></row><row><cell></cell><cell cols="4">parame eter of the data a. The value o f stock or freq quency of a</cell></row><row><cell></cell><cell cols="4">retweet t can, e.g., be e represented b by the y coord dinate of a</cell></row><row><cell></cell><cell cols="2">PixelD Drone.</cell><cell></cell></row><row><cell></cell><cell cols="3">Remot te TelePresenc ce</cell></row><row><cell></cell><cell cols="2">We al lso developed</cell><cell cols="2">a remote tel epresence app plication in</cell></row><row><cell></cell><cell>which</cell><cell>users embody</cell><cell cols="2">themselves vi a a remote Dis splayDrone</cell></row><row><cell></cell><cell cols="4">that di isplays a Skyp pe videoconfe erence (Figure</cell><cell>9). When</cell></row><row><cell></cell><cell cols="4">recipro ocated, this give es users the abi ility to fly arou und a model</cell></row><row><cell></cell><cell cols="4">and in nteract with rem mote participa ants at eye lev vel without</cell></row><row><cell></cell><cell cols="4">requirin ng a robotic st tructure. The fr ront facing cam mera on the</cell></row><row><cell></cell><cell cols="2">Display yDrone shows</cell><cell cols="2">parallax-free i images of the r remote user</cell></row><row><cell></cell><cell cols="2">on the e local Disp</cell><cell cols="2">layDrone, and d vice versa . Multiple</cell></row><row><cell>sulting</cell><cell cols="4">Display yDrones can serving g as a self-levi itating Hydra s be used for system that pre multiparty co onferences, eserves eye</cell></row><row><cell></cell><cell cols="4">contact t and head orien ntation cues [7 ].</cell></row><row><cell>bonds betwe present molecul PixelDrones h these structur articularly usef an be placed in omplex molecul can manipula ut of a molecul e recorded f hemistry studen n also simula lecular bonding en lar or res ful n a le. ate lar for nts ate g.</cell><cell cols="4">5D Prin nting When polyhedral Sh hapeDrones ar re landed on t top of one another r, they can cre eate complex c compound stru uctures that resemb ble 3D prints. Here, ShapeD Drones are con nnected by magnet tic bonds, allow wing them to re emain in place even when their p propellers are spun down. While extrem mely low-resolut tion at present, this allows fo or 5D printing technology with fu full synchroniz zation between n hardware an nd software editing g of prints. We use the term 5D D to indicate th he real-time bi-direc ctional nature of such edits: : Future versio ons will be capable e of real-time 3 3D animation o of parts of the p print. LIMITA ATIONS Our cu urrent impleme entation is limi ited to 12 simu ultaneously flying BitDrones. On ne limiting fac ctor is downdr raft, which</cell></row><row><cell>epresent points e, the position atical expressio that expressio me parameters in of on, on. of</cell><cell cols="4">restrict ts the ability of f drones to fly other. A second lim miting factor is s drift due to directly over t top of each turbulence when m many drones ar re flying in a c confined space. BitDrones are, ho owever, sufficie ently stable to hover within 1 15 cm from one an nother. Because e of the above e limitations, o our current</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Real Reality Interfaces #chi4good, CHI 2016, San Jose, CA, USA</head><label></label><figDesc></figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>This work was funded by the generous support of NSERC of Canada and Immersion, Inc. We thank Jordan van der Kroon for his work on the first version of BitDrones OS.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">DisplayObjects: Prototyping functional physical interfaces on 3D styrofoam, paper or cardboard models</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Akaoka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Ginn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roel</forename><surname>Vertegaal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. TEI &apos;10</title>
		<meeting>TEI &apos;10</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="49" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Image and animation display with multiple mobile robots</title>
		<author>
			<persName><forename type="first">Javier</forename><surname>Alonso-Mora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Breitenmoser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Rufli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roland</forename><surname>Siegwart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Beardsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Rob. Res</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="753" to="773" />
			<date type="published" when="2012-05">2012. May 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Atelier</forename><surname>Zafari</surname></persName>
		</author>
		<ptr target="http://atelier-zafari.com/cube-house/" />
		<title level="m">Cube House Project</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Admittance control for physical human-quadrocopter interaction</title>
		<author>
			<persName><forename type="first">Federico</forename><surname>Augugliaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raffaello D'</forename><surname>Andrea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Control Conference (ECC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1805" to="1810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Understanding and constructing shared spaces with mixed reality boundaries</title>
		<author>
			<persName><forename type="first">Steve</forename><surname>Benford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Greenhalgh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gail</forename><surname>Reynard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boriana</forename><surname>Koleva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Comput.-Hum. Interact</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="185" to="223" />
			<date type="published" when="1998-09">1998. September 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">MirageTable: Freehand interaction on a projected augmented reality tabletop</title>
		<author>
			<persName><forename type="first">Hrvoje</forename><surname>Benko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Jota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Real Reality Interfaces #chi4good</title>
		<meeting><address><addrLine>San Jose, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012. 2016</date>
			<biblScope unit="page" from="199" to="208" />
		</imprint>
	</monogr>
	<note>Proc. CHI&apos;12</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Telepresence: Integrating shared task and person spaces</title>
		<author>
			<persName><forename type="first">William</forename><surname>Buxton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Graphics Interface&apos;92</title>
		<meeting>of Graphics Interface&apos;92</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="123" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Drone &amp; me: An exploration into natural human-drone interaction</title>
		<author>
			<persName><forename type="first">Jessica</forename><forename type="middle">R</forename><surname>Cauchard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jane</forename></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Kevin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">A</forename><surname>Landay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. UbiComp&apos;15</title>
		<meeting>UbiComp&apos;15</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="361" to="365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bricks: Laying the foundations for graspable user interfaces</title>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">W</forename><surname>Fitzmaurice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroshi</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Buxton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI&apos;95</title>
		<meeting>CHI&apos;95</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="442" to="449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title/>
		<ptr target="www.gramaziokohler.com/web/e/projekte/209.html" />
	</analytic>
	<monogr>
		<title level="j">Flight Assembled Architecture</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">inFORM: Dynamic physical affordances and constraints through shape and object actuation</title>
		<author>
			<persName><forename type="first">Sean</forename><surname>Follmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Leithinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Olwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akimitsu</forename><surname>Hogge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroshi</forename><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. UIST&apos;13</title>
		<meeting>UIST&apos;13</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="417" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Claytronics: A scalable basis for future robots</title>
		<author>
			<persName><forename type="first">Seth</forename><forename type="middle">C</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><forename type="middle">C</forename><surname>Mowry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>In RoboSphere&apos;04</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Programmable matter</title>
		<author>
			<persName><forename type="first">Seth</forename><forename type="middle">C</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><forename type="middle">C</forename><surname>Mowry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="99" to="101" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">PaperWindows: Interaction techniques for digital paper</title>
		<author>
			<persName><forename type="first">David</forename><surname>Holman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roel</forename><surname>Vertegaal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Altosaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nico</forename><surname>Troje</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Johns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI &apos;05</title>
		<meeting>CHI &apos;05</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="591" to="599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName><surname>Hololens</surname></persName>
		</author>
		<ptr target="www.microsoft.com/microsoft-hololens" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Using ARToolKit markers to build tangible prototypes and simulate other technologies</title>
		<author>
			<persName><forename type="first">Eva</forename><surname>Hornecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Psik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERACT&apos;05</title>
		<meeting>INTERACT&apos;05</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="30" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Tangible bits: Beyond pixels</title>
		<author>
			<persName><forename type="first">Hiroshi</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brygg</forename><surname>Ullmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI&apos;97</title>
		<meeting>CHI&apos;97</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Radical atoms: Beyond tangible bits, toward transformable materials</title>
		<author>
			<persName><forename type="first">Hiroshi</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dávid</forename><surname>Lakatos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonardo</forename><surname>Bonanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Baptiste</forename><surname>Labrune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Interactions</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="38" to="51" />
			<date type="published" when="2012-01">2012. January 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">DyRT: Dynamic response textures for real time deformation simulation with graphics hardware</title>
		<author>
			<persName><forename type="first">Doug</forename><forename type="middle">L</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinesh</forename><forename type="middle">K</forename><surname>Pai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="582" to="585" />
			<date type="published" when="2002-07">2002. July 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The reacTable: Exploring the synergy between live music performance and tabletop tangible interfaces</title>
		<author>
			<persName><forename type="first">Sergi</forename><surname>Jordà</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Günter</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcos</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Kaltenbrunner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. TEI &apos;07</title>
		<meeting>TEI &apos;07</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="139" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Ultralight modular robotic building blocks for the rapid deployment of planetary outposts</title>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Karagozler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Kirby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><forename type="middle">Jie</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Marinelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tze</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>RASC-AL Forum</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Brian</forename><surname>Kirby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Burak</forename><surname>Aksak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Padmanabhan</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hoburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><forename type="middle">C</forename><surname>Mowry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seth</forename><forename type="middle">C</forename><surname>Goldstein</surname></persName>
		</author>
		<title level="m">Catoms: Moving robots without moving parts. AAAI Robotics Exhibition&apos;05</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1730" to="1731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">ZeroN: Mid-air tangible interaction enabled by computer controlled magnetic levitation</title>
		<author>
			<persName><forename type="first">Jinha</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rehmi</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroshi</forename><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. UIST&apos;11</title>
		<meeting>UIST&apos;11</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="327" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Direct, spatial, and dexterous interaction with see-through 3D desktop</title>
		<author>
			<persName><forename type="first">Jinha</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cati</forename><surname>Boulanger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2012 Posters</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page">69</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Relief: A scalable actuated shape display</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Leithinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroshi</forename><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc TEI&apos;10</title>
		<meeting>TEI&apos;10</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="221" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName><surname>Mit Flyfire</surname></persName>
		</author>
		<ptr target="http://senseable.mit.edu/flyfire/" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A Taxonomy of Mixed Reality Visual Display</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Milgram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fumio</forename><surname>Kishino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IEICE Trans. Information and Systems</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1321" to="1329" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">A Taxonomy of Real and Virtual World Display Integration. Mixed Reality -Merging Real and Virtual Worlds</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Milgram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herman</forename><surname>Colquhoun</surname><genName>Jr</genName></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Springer Verlag</publisher>
			<biblScope unit="page" from="5" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">HoverBall: Augmented sports with a flying ball</title>
		<author>
			<persName><forename type="first">Kei</forename><surname>Nitta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keita</forename><surname>Higuchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Rekimoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AH&apos;14</title>
		<meeting>AH&apos;14</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Flying display: A movable display pairing projector and screen in the air</title>
		<author>
			<persName><forename type="first">Hiroki</forename><surname>Nozaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI EA&apos;14</title>
		<meeting>CHI EA&apos;14</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="909" to="914" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pixie dust: Graphics generated by levitated and animated objects in computational acoustic-potential field</title>
		<author>
			<persName><forename type="first">Yoichi</forename><surname>Ochiai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takayuki</forename><surname>Hoshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Rekimoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">85</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Exploring 3D gesture metaphors for interaction with unmanned aerial vehicles</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Pfeil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seng</forename><surname>Lee Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Laviola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IUI&apos;13</title>
		<meeting>IUI&apos;13</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="257" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Lumen: Interactive visual and shape display for calm computing</title>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Poupyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsushi</forename><surname>Nashida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shigeaki</forename><surname>Maruyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Rekimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasufumi</forename><surname>Yamaji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc SIGGRAPH &apos;04</title>
		<meeting>SIGGRAPH &apos;04</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Urp: A luminous-tangible workbench for urban planning and design</title>
		<author>
			<persName><forename type="first">John</forename><surname>Underkoffler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroshi</forename><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI &apos;99</title>
		<meeting>CHI &apos;99</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="386" to="393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Amanda</forename><forename type="middle">J</forename><surname>Hayes Solos Raffle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroshi</forename><surname>Parkes</surname></persName>
		</author>
		<author>
			<persName><surname>Ishii</surname></persName>
		</author>
		<title level="m">Proc. CHI &apos;04</title>
		<meeting>CHI &apos;04</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="647" to="654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Shader Lamps: Animating real objects with image-based illumination</title>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Welch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kok-Lim</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Bandyopadhyay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 12th Eurographics Workshop on Rendering Techniques</title>
		<meeting>of the 12th Eurographics Workshop on Rendering Techniques</meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="89" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">G</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jock</forename><forename type="middle">D</forename><surname>Mackinlay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stuart</forename><forename type="middle">K</forename><surname>Card</surname></persName>
		</author>
		<title level="m">Real Reality Interfaces #chi4good</title>
		<meeting><address><addrLine>San Jose, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991">1991. 2016</date>
			<biblScope unit="page" from="189" to="194" />
		</imprint>
	</monogr>
	<note>Proc. CHI &apos;91</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Kilobot: A low cost scalable robot system for collective behaviors</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Michael Rubenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radhika</forename><surname>Ahler</surname></persName>
		</author>
		<author>
			<persName><surname>Nagpal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICRA&apos;</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="3293" to="3298" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Programmable self-assembly in a thousand-robot swarm</title>
		<author>
			<persName><forename type="first">Alejandro</forename><surname>Michael Rubenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radhika</forename><surname>Cornejo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nagpal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">345</biblScope>
			<biblScope unit="issue">6198</biblScope>
			<biblScope unit="page" from="795" to="799" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Displaydrone: A flying robot based interactive display</title>
		<author>
			<persName><forename type="first">Juergen</forename><surname>Scheible</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Achim</forename><surname>Hoth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Saal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. PerDis&apos;13</title>
		<meeting>PerDis&apos;13</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="49" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Midair displays: Exploring the concept of free-floating public displays</title>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Schneegass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Alt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Scheible</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albrecht</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI EA&apos;14</title>
		<meeting>CHI EA&apos;14</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2035" to="2040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The Ultimate Display</title>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Sutherland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IFIP 65</title>
		<meeting>IFIP 65</meeting>
		<imprint>
			<date type="published" when="1965">1965</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="506" to="508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Communicating directionality in flying robots</title>
		<author>
			<persName><forename type="first">Bilge</forename><surname>Daniel Szafir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terrence</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><surname>Fong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. HRI&apos;15</title>
		<meeting>HRI&apos;15</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="19" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">Grzegorz</forename><surname>Szafranski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Czyba</surname></persName>
		</author>
		<title level="m">Different approaches of PID control UAV type quadrotor. IMAV&apos;11</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="70" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Programmable matter: Concepts and realization</title>
		<author>
			<persName><forename type="first">Tomasso</forename><surname>Toffoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Norman</forename><surname>Margolus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica D: Nonlinear Phenomena</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="263" to="272" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Selfassembly at all scales</title>
		<author>
			<persName><forename type="first">George</forename><surname>Whitesides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bartosz</forename><surname>Grzybowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">295</biblScope>
			<biblScope unit="issue">5564</biblScope>
			<biblScope unit="page" from="2418" to="2421" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Combining multiple depth cameras and projectors for interactions on, above and between surfaces</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">D</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hrovje</forename><surname>Benko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. UIST &apos;10</title>
		<meeting>UIST &apos;10</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="273" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Organic user interfaces</title>
		<author>
			<persName><forename type="first">Roel</forename><surname>Vertegaal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Poupyrev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="26" to="30" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><surname>Vicon</surname></persName>
		</author>
		<ptr target="http://www.vicon.com/" />
		<title level="m">Real Reality Interfaces #chi4good</title>
		<meeting><address><addrLine>San Jose, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
