<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Design Methodology for Synthesizing Parallel Algorithms and Architectures</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Marina</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Yale University</orgName>
								<address>
									<settlement>New Haven, Connecticut O&amp;?O</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Design Methodology for Synthesizing Parallel Algorithms and Architectures</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CBAE153CA3B75EEABE021FCF04C04388</idno>
					<note type="submission">Received November 14, 1985</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, a design methodology for synthesizing efficient parallel algorithms and VLSI architectures is presented. A design process starts with a problem definition specified in the parallel programming language Crystal and is followed by a series of program transformations in Crystal, each aiming at optimizing the target &amp;sign for a specific purpose. To illustrate the design methodology, a set of design methods for deriving systolic algorithms and architectures is given and the use of these methods in the &amp;sign of a dynamic programming solver is described. The design methodology, together with this particular set of design methods, can be viewed as a general theory of systolic designs (or multidimensional pipelines). The fact that Crystal is a general purpose language for parallel programming allows new design methods and synthesis techniques, properties and theorems about iroblems in specific application domains, and new insights into any given problem to be integrated readily within the existing design framework. 0 1986 Academic press, 1~.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>VLSI technology provides a natural medium for parallel processing, from the level of switching elements, to logic gates, to functional and control units, and to architectures and algorithms. To exploit its potential fully, parallelism must be used at increasingly higher levels. Design automation, consequently, must go beyond the level of taking as givens architectural level specifications, and should head toward compiling or synthesizing efficient parallel algorithms and architectures. The technology, on the other hand, constrains the way in which parallel computation can be organized. Dominant costs at the technological level often have profound implications in the designs at algorithmic and architectural levels. Clearly, to achieve an efficient design, one must take advantage of what the technology can offer, and minimize the costs associated with the constraints it imposes upon the design.</p><p>Motivated by the former, a design methodology for synthesizing efficient parallel algorithms and VLSI architectures is presented. A design process MARINA C. CHEN starts with a problem definition specified in the parallel programming language Crystal <ref type="bibr" target="#b3">[4]</ref> and is followed by a series of program transformations in Crystal, each aiming at optimizing the target design for a specific purpose. To illustrate the design methodology, a set of design methods for deriving systolic algorithms and VLSI architectures, motivated by the costs and constraints of the technology, is given. The use of these methods in the design of a dynamic programming solver is described. The design methodology, together with this particular set of design methods, can be viewed as a general theory of systolic designs (or multidimensional pipelines).</p><p>Each design method or synthesis technique takes into account the dominant costs in the underlying technology and minimizes such costs to yield efficient parallel algorithms and architectures. For instance, the design process starts with a problem definition specified in Crystal, which is interpreted as a parallel algorithm, however naive and inefficient it might be. From this naive algorithm, the dominant costs it might incur in the underlying parallel hardware, such as those of communications and fan-ins and fan-outs, are examined. The algorithm is then improved by a series of transformations that results in a bounded-order and bounded-degree program which has reduced hardware costs. Once such a program is obtained, it goes through another stage of transformation, called space-time mapping, by which pipelining is automatically incorporated into the algorithm and the hardware resources are fully utilized. Another stage of transformations then follows, which aims at reducing the amount of hardware necessary for achieving the timing control among the parallel processing elements.</p><p>The set of synthesis methods for the design of VLSI architecture described in this paper has a wide range of applications, but is by no means a complete set of general synthesis methods. In fact, it is hard to believe such a set may ever be attainable. However, the fact that Crystal is a general purpose parallel programming language allows new synthesis techniques, new theorems, and new insights for any given problem to be integrated readily within the existing design framework.</p><p>Throughout this paper, the dynamic programming problem is used for illustration. The well-known systolic algorithm of Guibas, Kung, and Thompson <ref type="bibr" target="#b6">[7]</ref> is one of the many possible designs generated. The rest of the paper is organized as follows: In Section 2, a mathematical definition of dynamic programming is given. In Section 3, its interpretation as a parallel algorithm is described. In Section 4, transformations for reducing the number of fan-ins and fan-outs are presented. In Section 5, transformations that reduce long-range communications are illustrated. In Section 6, algorithms are classified as either uniform or nonuniform because space-time mapping for uniform algorithms can be obtained by an expedient procedure, which is described here. A general inductive method for finding space-time mapping for nonuniform algorithms can be found in <ref type="bibr" target="#b4">[5]</ref>. In Section 7, optimization of control signals is illustrated. Some related work is discussed in Section 8. The PARALLEL ALGORITHMS AND ARCHITECTURES 463 Appendix contains the actual Crystal programs, starting with a simple mathematical definition in the first program to a complete architectural specification in the target program. An architectural level simulation of the design is generated as a by-product of executing the target program. Therefore the Crystal programming environment automatically provides a simulation environment for the design of VLSI architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">A GENERAL DEFINITION OFDYNAMIC PROGRAMMING</head><p>A large class of optimization problems can be solved by dynamic programming. The definition of such problems can be posed in a general form where C(i, j) is some cost function which is to be minimized, I s=j-i-l, and i and j are integers in the range 0 &lt; i &lt; j % n, for some constant n.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PARALLEL INTERPRETATION OF ALGORITHMS</head><p>The straightforward definition given in Eq. (2.1) is in the form of a Crystal recursion equation with indices i and j. It can be interpreted as a parallel algorithm as follows: Each pair (i, j) in Eq. (2.1) is interpreted as a process in an ensemble of parallel processes denoted by the set PI f%' {(i, j) : 0 &lt; i &lt; j I n}. The local processing at each process (i, j) is, in this example, the function min. ,&lt;k&lt;j{h (Xk, yk)} that takes j -i -1 pairs of arguments. The communication between processes is specified, for instance, by the pairs (i, k) and (k, j) appearing on the right-hand side of Eq. (2.1) to indicate that the computation of C(i, j) at process (i, j) needs the results from processes (i, k) and (k, j).</p><p>The ensemble of processes and its data flow can be depicted by a DAG (Directed Acyclic Graph) as shown in Fig. <ref type="figure" target="#fig_1">1</ref>. It consists of nodes, where each node corresponds to a process (i, j) in P, , and directed edges, each of which comes out of a node whose corresponding process appears on the right-hand side of Eq. (2. l), and goes into a node whose corresponding process appears on the left-hand side of the equation. The directed edges of the DAG define the data dependency relation of the algorithm. We say that a process u immediately precedes v (u 4 v), or v immediately depends on u (v + u), if there is a directed edge from u to v. The transitive closure "2" of this relation, called "precede ," is a partial order, and there is no infinite decreasing chain from any node v, nor is there an infinite. number of processes that immediately precede v. Those nodes that have no incoming edges are called Sources.</p><p>Processes are parallel in nature. A computation starts at the sources which are properly initialized, and is followed by other processes each of which starts execution when all of its required inputs, or dependent data, become available. The parallel execution of the naive dynamic programming definition starts with all processes such that s = 0, followed by processes with increasing s as illustrated in Fig. <ref type="figure" target="#fig_1">1</ref>.</p><p>An immediate, very naive implementation of the parallel interpretation uses one processor for each process, and altogether 0 (n ') parallel processors are needed. The number of time steps needed to compute the result is at best n -lsincenoC(i,j)canbecomputeduntilC(i,j -l)andC(i + 1,j)are computed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">PROBLEMS WITH LARGE NUMBERS OF FAN-INS AND FAN-OUTS</head><p>Due to the inherent physical constraints imposed by the driving capability of communication channels, power consumptions , heat dissipations, memory bandwidth, etc., it is reasonable to assume that data can only be sent or received simultaneously to or from a small number of destinations or sources. Putting such constraints in algorithmic terms: the number of "fan-ins" and "fan-outs" of a datum must be small, where fan-in degree of a datum is defined to be the number of data items on which it depends, and fun-out degree is the number of data items dependent upon it. If what we are interested in are practical and efficient algorithms, the fan-in and fan-out degrees should be taken into account in measuring the complexity.</p><p>It can be easily seen that the fan-in degree of C (i, j), which appears on the left-hand side of Eq. (2. l), is 2s, where s d"=' ji -1. Conversely, for any value appearing as C(i, k) on the right-hand side of Eq. (2. l), that value is needed by C (i, j) for j = k + 1, . . . , n (there are nk such terms). The same value also appears as C(k, j) in the equation and it is needed by processes (i, j) for i = 1, . . . , k -1 (there are k -1 such terms). Since both s and k grow with the problem size n, the number of fan-ins and fan-outs therefore grows linearly with the size of the problem.</p><p>To avoid the high cost incurred at the hardware level, the large fan-in and fan-out degrees must be reduced. In the following transformations, the original fan-in and fan-out degrees are reduced from 0 (n) to a constant, while the complexity of the original algorithm is maintained the same as before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Reducing Fan-Out Degrees</head><p>The idea used in reducing the fan-out degree is quite simple: PROPOSITION 4.1. Let z(l) for u I 1 I II, where u and v are integers, and u &lt; v be v -u + 1 variables to which a value x shall be assigned, i.e., z (1) = x for u I 1 5 v (where the fun-out degree of x is v -u + 1). Then the assignments of these variables can be performed instead by The two recursion equations above are only two out of many possible ways a value x can be concurrently assigned to many variables z(l), for all u 5 1 5 v. The first equation describes the situation where x is assigned first to the variable at the endpoint u of the interval between u and v . Alternatively, the second equation describes the assignment starting at the other endpoint O. Similarly, x could be first assigned to a z(w), where w is somewhere in the middle of the interval. To complete the concurrent assignments, all of the above schemes, called linear broadcasting arrays, require time steps linear to the number of variables to be assigned. A much different way of implementing the concurrent assignment of x to many variables may be via a broadcasting tree, in which only a logarithmic number of steps are needed to complete the assignments. Interested readers may try to describe the broadcasting tree by recursion equations. Such transformation is very useful in devising efficient parallel programs.</p><p>For the dynamic programming problem, in order to decrease the fan-out degree of C(i, k), the schemes of linear broadcasting array and the broadcast tree are first compared. The former is chosen since the time complexity of the target program employing each scheme turns out to be of the same order for both cases, and the linear broadcasting array is simpler and requires less hardware than a broadcasting tree. Within the class of linear broadcasting arrays, there are a few possible choices: initial assignments to variables at endpoints of an interval are in general preferred since they result in unidirectional data flow, which requires simpler control than bidirectional data flow.</p><p>Now let the variables at processes (i, j) for k I j 5 n to which C(i, k) will be assigned be denoted by a (i, j, k) so that Proposition 4.1 can be applied to C(i, k) (the value x) and a(i, j, k) for allj (the variables z(l)). Similarly, let the variables at process (i, j) for 1 I k &lt; i to which C(k, j) is to be assigned be denoted by b(i, j, k). Applying Eq. (4.1) of the proposition to a (i, j, k) and Eq. (4.2) to b (i, j, k), we obtain</p><formula xml:id="formula_0">a(&amp; j, k) = j i = k+ C(i, k) j &gt; k-, a(i,j -1, k) (4.3) b(i,j, k) = i = k+= C(k,j) i &lt; k+ b(i + l,j, k)</formula><p>for 1 5 i &lt; k &lt; j I n. Now the IZ -k fan-out degree of C(i, k) to all processes (i, j), where k &lt; j I: n, is reduced to 1, and so is the k -1 fan-out degree of C(k, j) to all processes (i, j) for 1 5 i &lt; k. We may now replace the values of high fan-out degree, C(i, k) and C(k, j), on the righthand side of Eq. (2.1) by the values of low fan-out degree, a(i, j, k) and b(i, j, k):</p><p>C(i, j) = s &gt; 0 * p&amp;{ML j, k), b(i, j, k)&gt;l </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Reducing the Fan-In Degrees</head><p>The reduction of fan-in degree of a value computed by a process relies on the associativity of the operation being computed. Similarly, the same operation can be achieved by a different sequence using both binary and ternary associative operations. PROPOSITION 4.2. Zf "@" is an associative operation on u -u -1 arguments x(l)for u &lt; 1 &lt; u, where z = $u&lt;l&lt;ox(l), then it can be replaced by a sequence of binary and ternary operations on variables z(l), for m &lt; 1 I u of the form I = m + x(m) G3 x(u + u -m) z(l) = m &lt; 1 &lt; u + z(l -1) @x(l) @ x(u + u -I) (4.6) I= u-+z(Z -l), where m = [(u + w)/2) and z = z(u).</p><p>Similar to the case of reducing fan-out degrees, there could be many ways in which fan-in degrees can reduced. Aside from the two linear schemes described by the recursion equations above, a merge tree which takes a logarithmic number of steps to complete an n-ary operation can be used. Again, the choice of these two very different schemes is problem dependent. Capturing these schemes by recursion equations in a formal programming language allows the chosen transformation to be performed mechanically, and the various choices to be made either manually by the users or, with the help of a set of ad hoc rules for making choices, automatically by a design tool.</p><p>For dynamic programming, note that the operation "min" for computing C(i, j) is associative. Therefore we may let the value of high fan-in degree, C(i, j), at process (i, j) be computed instead by the sequence c(i, j, k) for m &lt; k &lt; j, where m gf [(i + j)/21. We apply Proposition 4.2 to Eq. (4.4), and it is transformed to</p><formula xml:id="formula_1">'S=O+Ci s&gt;o+ r- k = m + min(h (a (i, j, k), b (i, j, k)), c(i, j, k) = * h(a(i, j, i + j -k), b(i, j, i + j -k))</formula><p>. m &lt; k &lt; j+ min(c(i, j, k -l), h(a(i, j, k), (4.7)</p><formula xml:id="formula_2">b(i, j, k)), h(a(i, j, i + j -k), b(i, j, i + j -k)) k = j+ c(i, j, k -l), \</formula><p>and C(i, j) = c(i, j, j) for all (i, j) E PI.</p><p>The application of Proposition 4.2 is motivated by the following: Due to the data dependency, the value h(u(i, j, k), b(i, j, k)) for i &lt; k &lt; j cannot be computed before at least max(ki, j -k) time steps. A pair with the least delay is the one with k = [(i + j)/2] and/or k = [(i + j)/2], where the delay is about half of the interval size. Consequently, the most efficient sequence of binary or ternary operations will start with this pair, and will be followed by other pairs in the order of increasing amount of delay.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">COMMUNICATIONS AND BOUNDED-ORDER RECURSION EQUATIONS</head><p>The two equations u(i, j, k) = j = k+ c(i, k, k)</p><formula xml:id="formula_3">1 j &gt; k-+ u(i, j -1, k) b(i, j, k) = i = k + c(k, j, j) i &lt; k+ b(i + 1, j, k),<label>(5.1)</label></formula><p>for 1 I i &lt; k &lt; j I n, are obtained from Eq. ( <ref type="formula">4</ref>.3) by substituting C(i, k) with c(i, k, k), and substituting C(k, j) with c(k, j, j). The system of recursion equations consisting of Eqs. (5.1) and (4.7) is a nzy algorithm for dynamic programming. The new ensemble of processes Pz = {(i, j, k) : 0 &lt; i C k &lt; j I n} is now three dimensional, as shown in Fig. <ref type="figure" target="#fig_5">2</ref>. The added one dimension and the increased number of processes are for the purpose of decreasing fan-in/fan-out degrees. A remaining criterion for a good parallel algorithm is that all communications between processes should be as local as possible, if such localization is not achieved at the expense of the total time complexity of the algorithm. To define locality, a few definitions are called for: DEFINITION 5.3. The order of a system of recursion equations is defined to be the maximum path length between any pair of processes u and v, where u + v.</p><p>Similar to the high fan-in and fan-out degrees, the extra delay introduced by the communications of the high-order terms in the equations can undermine the efficiency of an algorithm. Therefore the order of a system of equations should be either constant or grow very slowly with respect to the problem size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Localizing Communications by Domain Contraction</head><p>Note that Eq. (4.7) contains terms a(i, j, i + j -k) and a (i, j, k), and the order of the system of equations is therefore 2(1 k -(i + j)/2 I) for i &lt; k &lt; j, which grows with the problem size n. To eliminate the high-order terms, further transformations are required.</p><p>The path length 2(1 k -(i + j)/2 I) between two communicating processes is an expression whose value depends on those of i, j, and k. The goal is to transform this expression into one whose value is bounded. The most obvious choice is to set k = (i + j)/2, in which case the path length becomes 0, i.e., two processes are collapsed into a single one. Geometrically, the coordinates of these high-order terms are symmetric to those of the low-order terms a(i, j, k) and b(i, j, k), with respect to the plane k = (i + j)/2, which is the shaded area shown in Fig. <ref type="figure" target="#fig_5">2</ref>. The algebraic transformation now corresponds to the contraction of the original domain of processes by one-half along the plane, so that (i, j, k) and (i, j, i + jk) become a single process (i, j, k).</p><p>Let processes (i, j, k), such that m I k 5 j, be referred to as being in the upper half of the process structure P2. Similarly, let those processes, such that i I k I m ' , be referred to as being in the lower half of P2.</p><p>Let the new process structure be the upper half of P2, i.e., P3 gf {(i, j, k) : O&lt;i &lt;j ~nandm ok Ij},asshowninFig.3.Thedatastreamsaand b of those processes originally residing in the lower half must be given new names so that they can be differentiated from those of the processes originally in the upper half. To achieve this, let d and e be new data streams to replace a and b for the processes of the lower half as shown in Fig. <ref type="figure" target="#fig_6">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Algebraic Manipulation to Obtain Bounded-Order Equations</head><p>First, Eq. (5.1), defined for (i, j, k), i 5 k 5 j, is now split into two sets of equations, one for the case m gf [(i + j)/2] 5 k &lt; j, where (i, j, k) resides in the upper half of PZ, and the other for the case i &lt; k 5 m -, cle_f [(i + j)/2], where (i, j, k) resides in the lower half. Since, for the processes (i, j, k) that reside in the lower half, a and b will be renamed d and e, we must be aware of the case when a process (i, j, k) is in the lower half of fi while process (i, j -1, k) might be in the upper half, such as in the case when k = m ' and i + j is even (or s !Lf ji -1 is odd), Another case to watch for is that in which (i, j, k) is in the upper half while (i + 1, j, k) might be in the lower half: it occurs when k = m and i + j is even (or s is odd). Two equations in (5.1) are now split into four equations, where the above two cases are singled out:</p><formula xml:id="formula_4">Upper half: a(i, j, k) = = k--t c(i, k, k) m sk&lt;j-*a(i,j-- 1,k). lower half: f (k = m ') A (odd(s)) --, a (i, j -1, k) a(i' j' k, = (upper half) (k = m ') A (even(s)) -a(i, j -1, k) i &lt; k &lt; m' -+ a(i, j -1, k). upper half: (5.2) (k = m) A (odd(s)) + b(i + 1, j, k) b(i'J9 k, = . I (lower half) (k = m) A (even(s))-+ b(i + l,j, k) m &lt; k &lt;j+ b(i + l,j, k). lower half: b(i, j, k) = k = i+ c(k,j,j) i &lt; k I m' + b(i + l,j, k).</formula><p>Next, every a(i, j, k) in the lower half is replaced by d(i, j, i + j -k), where (i, j, i + j -k) is now the upper half. Similarly, every b(i, j, k) in the lower half is replaced by e(i, j, i + j -k):</p><p>(i, j, k) lower half:</p><formula xml:id="formula_5">f (k = m') A (odd(s)) + u(i, j -1, k) d(i, j, i + j -k) =</formula><p>(upper haNI (k = m ') A (even(s)) *d(i,j-l,i+(j-1)-k) ( i &lt; k &lt; m' + d(i, j -1, i + (j -1) -k).</p><p>(i, j, k) lower half:</p><p>(5.3) e(i,j, i +j -k) = k = i+= c(k,j,j) i &lt; k 5 m' + e(i + l,j, (i + 1) + j -k), Finally, a "substitution of variable" is performed on the above two equations, which replaces i + j -k by k ':</p><formula xml:id="formula_6">(k' = m) A (odd(s)) + u(i,j -1, i +j -k') d(i, j, k') = (k' = m) A (even(s)) + d(i, j -1, k' -1) m &lt; k' &lt;j-t d(i,j -1, k' -1).</formula><p>(5.4) e(i, j, k') = k' =j+ c(i,j,j) m I k' &lt;j+ e(i + l,j, k' + 1).</p><p>Since k' is a bound variable, it can be replaced by k throughout Eqs. (5.4) without any effect. Note also that when k ' = m, we have i + j -k ' = k ' .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Resulting System of Equations</head><p>The resulting algorithm is the following system of recursion equations:</p><formula xml:id="formula_7">a(i,j, k) = j = k+ c(i, k, k) m Ik&lt;j+-a(i,j- l,k) d(i, j, k) = b(i, j, k) = .(k = m) A (odd(s)) + a(i, j -1, k) (k = m) A (even(s)) + d(i, j -1, k -1) ,m &lt; k &lt; j + d(i, j -1, k -1) (k = m) A (odd(s)) + e(i + 1, j, k + 1) (k = m) A (even(s)) + b(i + 1, j, k) ,m &lt; k &lt;j+ b(i + l,j, k) e(i, j, k) = k = j + c(i, j, j) m 5 k &lt;j+ e(i + 1, j, k + 1) c(i, j, k) = (5.5) k = m + min(h(a(i, j, k), b(i, j, k)), h(d(i, j, k), eG. j, k))</formula><p>m &lt; k &lt; j+ min(c(i, j, k -l), h(a(i, j, k), bk j, k)), h(dG, j, k), eG, j, k)) k = j+ c(i, j, k -1).</p><p>The order of the system of equations is now 2, due to the path length between processes described by equations for streams b and d.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">SPACE-TIME MAPPING TO INCORPORATE PIPELINING</head><p>From the algorithm (5.5) derived above, we further improve the efficiency of the algorithm by incorporating pipelining automatically. From the standpoint of implementation, a process v in a system of recursion equations will be mapped to some physical processor s during execution, and once the process is terminated, another process can be mapped to the same processor. In fact, such reuse of the resource is the essence of pipefining. We call each execution of a process by a processor an invocation of the processor. Let t be an index for labeling the invocations so that the processes executed in the same processor can be differentiated, and let these invocations be labeled by strictly increasing nonnegative integers. Then fg; a given implementation of a program, each process v has an alias [s, t] = f (v), telling when (which invocation) and where (in which processor) it is executed. The key to an efficient parallel implementation of an algorithm is to find an appropriate one-to-one functionf that maps a process v to its alias [s, t] such that t will be nonnegative and t2 &gt; tl if vl -i v2, where [s,, t,] sff(vl) and [sz, t2] d"=' f(v2). In the foll owing, we call t the time index and each component of s a space index.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Data Dependency Vectors</head><p>To find such a mapping, we require that the domains of the indices of a Crystal program be vector spaces, and that the appropriate vector addition and scalar vector product be defined. Though each of the indices i and j in Eq. (2.1) assumes an integer value, its domain can be extended to the set of rationals. For instance, an m-dimensional process structure is now embedded in an m-dimensional vector space over the rationals. From now on, we may refer to the m -tuple of values of indices identified with a process as a vector. As suggested by [ 121, a data dependency vector plays an important role in mapping algorithms to parallel processors. Since the vector addition is defined and the data dependency of two vectors takes on an exact meaning, a data dependency vector can be formally defined: DEFINITION 6.1. A data dependency vector is the difference v -u of vector v and vector u, where u ( v (u immediately precedes v).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Basis Communication Vectors</head><p>As described above, each program defines a set of data dependency vectors. On the other hand, each network topology defines a set of linearly independent b&amp;is communication vectors. For instance, in an n-dimensional hypercube, a processor has n connections to its nearest neighboring processors. Each of the n communication vectors (one for each connection) has n + 1 components. The first n components indicate the movement in space and the (n + 1)th component indicates movement in time, which is always positive (counting invocations). These n communication vectors, together with the communication vector [0, 0, . . . , 0, l] representing the processor's communication of its current state to its next state, form the basis communication vectors. In an n-dimensional network, there can be more than one set of basis communication vectors. Taking a two-dimensional hexagonal network as an example, a diagonal connection has a communication vector [l, 1, 11. The set of vectors {[ 1, 0, 11, [0, 1, 11, [l, 1, 11) serves as the bases as well as the set {[l, 0, 11, [0, 1, 11, [0, 0, 11). For any ndimensional network which has nearest neighbor connections and is regularly connected and indefinitely extensible, all possible sets of basis communication vectors can be obtained by the enumeration of its symmetry groups (Lin and Mead <ref type="bibr" target="#b9">[ 10)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Uniformity of a Parallel Algorithm</head><p>The concept of uniformity is introduced to characterize parallel algorithms so that an expedient procedure can be applied to a uniform algorithm to find the space-time mapping from processes to processors. Let di, 1 5 i I k, denote the data dependency vectors appearing in a program. DEFINITION 6.2. A uniform algorithm is one in which a single set of basis vectors B = (b,, . , . , b,), m I k, where each bj is a column vector of m components, can be chosen so as to satisfy the mapping condition that every data dependency vector di appearing in the algorithm be expressed as a linear combination of the chosen basis vectors with nonnegative coordinates; in other words, there exists ai1 ai= 1</p><p>(i %m suchthatdi=Baiandq 'Ofor Sj Sm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Motivation for the Mapping Condition</head><p>The mapping condition is motivated by the possibility of using as the space-time mapping a linear transform from the basis data dependency vectors B to the basis communication vectors C = (ci, . . . , c,), m I k, where each Cj is a column vector of m components. Each basis communication vector Cj corresponds to a nearest neighbor communication on a network of processors. When the mapping between the two sets of basis vectors is determined, then each communication vector ei, 1 5 i 5 k, to which a data dependency vector di corresponds is also determined, i.e., if di = Bai then ei = Caj.</p><p>If a data dependency vector di has a term with a negative coordinate q in its linear combination, then vector d!uCj, which contributes in part to the communication vector ei, represents a communication that takes negative time steps, a situation that does not make sense in any physical implementation .</p><p>Examples of using such a simple procedure to find linear mappings of processes to parallel architectures for matrix products, LU decomposition, array multipliers, etc., can be found in [2, 31. Most of the systolic algorithms reported in the literature can be obtained in this way. New systolic algorithms are in fact discovered due to the ability to generate systematically all possible sets of basis communication vectors. In the case of a nonuniform program, more than one set of basis data dependency vectors must be chosen so as to satisfy the mapping condition, and the space-time mapping may become nonlinear. In this case, the inductive mapping procedure becomes necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.">A Uniform Dynamic Programming Algorithm</head><p>Letdi, i = 1, 2, . . . , 5, denote the data dependency vectors of System (5. The linear mapping T from the basis dependency vector to the basis communication vector is then PROFTXITION 6.1. A space-time mapping from each process (i, j, k) in P3 of System (5.5) to an invocation of a processor (x, y, t) is a linear mapping PARALLEL ALGORITHMS AND ARCHITECTURES 477 T, in the matrix notation where each process and invocation is written as a column vector, or written in a functional notation as (X, y, t) =f(i, j, k) = (-i, j, -2i + j + k)</p><p>where -n &lt;x I -1, -x &lt; y I n, 2 S t &lt; 2(n -1). The inverse mapping from the image off to the process structure, denoted by f -', is (i, j, k) = f-'(x, y, t) = (-x, y, -2x -y + t), specifying which process is being executed at a particular time step t of processor (x, y).</p><p>Similarly, other space-time mappings can be derived from other sets of basis communication vectors such as B,, BZ, and B3, given below in a matrix notation where each basis communication vector is written as a column vector: B,=(; ; i), Bz=(i ; ii), B3=(i ; ').</p><p>(6.</p><p>3)</p><p>The set of basis communication vectors in Eq. (6.2)) or in each of (6.3) above, results in a different systolic architecture, as demonstrated below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6.">Space-Time Recursion Equations (STREQ)</head><p>A system of space-time recursion equations (STREQ), which describes a target systolic architecture or algorithm, can be obtained from an original program and a space-time mapping by algebraic manipulation. The following STREQ, which describe the resulting design, are obtained by substituting the inverse mapping f -' of i, j, and k, the mappings f (dl), 1 = 1, 2, . . . , 5, of dependency vectors dl, into the original system of recursion equations, and by the renaming B(x, y, t) = a(i, j, k), 6(x, y, t) = b(i, j, k), e(x, y, t) = c(i, j, k), 2(x, y, t) = d(i, j, k), and C(x, y, t) = e(i, j, k). Note that as a result, the substitution of the predicates s = 0 becomes z = 1, where z ef x + y, k = m becomes t = [3(x + y)/21 = [3z/21, k = j becomes t = 22, m &lt; k &lt; j becomes ([3z/21 &lt; t &lt; 2z), etc. (1 32 -i-t=2</p><p>1 + min@@(x, Y, t), ~(-G Y, t)), h@(x, Y, t), e^(x, Y, t)) 1 5 t &lt; 22 + min(e(x, y, t -l), h(ci(x, y, t), 1 6(x, Y, t)), mx, Y, t), ~b, Y, 6) Z -+ i2(x, y, t -1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">TARGET SYSTOLIC ARCHITECTURES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Processor and Time Requirements</head><p>The design consists of a triangular array of (n -l)(n -2)/2 processors which complete the computation in 2(n -2) time steps, as indicated by the range of x, y, and t in Proposition 6.1. Note that the space-time mapping has achieved the goal of decreasing the complexity of processor requirement from 0 (n') of the naive interpretation of System (5 S), to 0 (n') in the new algorithm, by only increasing the time complexity with a constant factor, in this case, 2. Thus we see pipelining in the resulting algorithm and the successful sharing of a single processor among O(n) processes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">II0 and Storage Requirements</head><p>In the resulting algorithm, if any of the space components of a process on the right-hand side of an equation differs from that on its left-hand side, then the data associated with the right-hand-side process are an input to the lefthand-side process, such as &amp;(x, y -1, t -1) in Eq. <ref type="bibr">(6.4)</ref>. In this case, the input is from processor (x, y -l), which is, say, south of processor (x, y), and it takes one time step to arrive. If space components of two processes on both sides of an equation are the same, then the data associated with the right-hand-side process constitute a stored value such as C(x, y, t -1) in Eq. <ref type="bibr">(6.4)</ref>.</p><p>The space-time mapping in Proposition 6.1 yields the well-known systolic architecture for dynamic programming <ref type="bibr" target="#b6">[7]</ref>. It can be seen in Fig. <ref type="figure" target="#fig_3">4</ref> that a process (i, j, k) is mapped to processor (-i, j) at time step t = -2i + j + k. Such an invocation may have inputs from invocations both at time t -1 and at time t -2 as shown, corresponding to the fast registers and the slow registers described in <ref type="bibr" target="#b6">[7]</ref>. Ail signals controlling the loading and unloading between registers of different speeds can be systematically derived by techniques of program optimization as described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Control Requirements</head><p>Predicates appearing in a given system of STREQ indicate the control of the functionality and timing of the processors. Their implementations might be different depending on different target implementation media. In a multiprocessor machine, the "processor id" (x, y) can be stored and an "invocation counter" keeps track of t until conditions such as t = 22 are satisfied. In a VLSI implementation, however, it is too costly to store these integers and dedicate hardware to perform the tests. Program optimization of STREQ to eliminate such tests therefore becomes necessary. A better design can be obtained by replacing the expensive computations of a predicate by transferring a one-bit control signal, as illustrated by <ref type="bibr" target="#b6">[7]</ref>. For a predicate that is independent of t, there is no concern, since it can be hardwired into the design. For any predicate that is dependent on t, it must be substituted by one that is independent of t. Since a communication always both moves in space and takes time to complete, it can be used to "compute" expressions of the space-time indices in a time-variant predicate.  <ref type="figure">,</ref><ref type="figure">x,</ref><ref type="figure">,</ref>) of the space indices x, , 1 5 i I n, which is nonnegative and rational valued and consists of only rational coefficients and first-power terms. However, the expression may consist of nonlinear operations, e.g., absolute value, ceiling, floor, conditional, etc. Processes on the same plane are mapped to the same invocation number t. (The processor number of a process in this case is its projection onto the bottom plane with the sign of x changed.) SttiCtly monotonic in some space index Xi, i.e., if X: -C xi then p(Xly . . . 9 Xi), e * * 9 Xn) C p(Xl, . . * 9 Xi, f . . 7 x,). Then for each xi, the time dependency dt(Xi) = p(Xl, X7.9 . . . f X, . . . 9 Xn)p(Xl, X2, . . . , Xi -1, . . . 7 Xn) of the control expression has a fixed value which is positive. Certainly, a similar theorem holds for a control expression that is strictly monotonic decreasing. For an application of the above theorem, take, for instance, predicates t = x -1 and t &gt; x -1. The control expression x -1 is monotonic in x, and its time dependency is d(x) = 1 for all x. By the theorem, it can be implemented by predicates q(x, t) = 1 and q(x, t) = 2, respectively, where the control stream is defined as</p><formula xml:id="formula_8">x--1=0+ 1 { t=O*l 4(x, 4 = t&gt;0+2</formula><p>x -1 &gt; o+ q(x -1, t -1).</p><p>. Note that since predicate t &lt; x -1 is not needed, q(x, t) needs to assume only two values, 1 and 2. Thus it can be reduced to a single-bit signal. This control stream can be easily implemented by a one-bit signal initially fed to processor x = 1, which changes its value at t = 1, and shifts subsequently to processors with increasing x values one at a time.</p><p>Readers might be curious about how, in general, the initialization of control stream 4(x,, x2, . . , x,, t) for the switch-on predicates t &lt; 0 is carried out in practice. The implementation is just the familiar "system reset" which puts a given system into a desired initial state.</p><p>In general, a control expression might be strictly monotonic (decreasing) in more than one space index. The choice of an index xi should be such that dt(x,) is minimized over all other choices of space indices because dt(Xi) can determine the rate at which data streams are transferred. In the case when some choices of space indices are equally good as far as timing is concerned, the topology of the chip layout and the input pin arrangement may determine which space index should be chosen since control signals must be fed into the chip from outside. For the dynamic programming example, one possible implementation of all of the time-variant predicates is illustrated in BOX 5 of the Appendix. It is worth noting, in particular, the implementation of the predicate t = [3z/2]. The predicate contains a piecewise linear control expression, and the resulting signal travels in those processors with odd z at half the speed at which it travels in those processors with even z. This particular type of signal is described incorrectly as a linear signal that moves "two cells every three time units" in <ref type="bibr" target="#b6">[7]</ref> as opposed to the above-mentioned piecewise linear signal, Central to an optimizing compiler for parallel systems is the making of trade-offs between communications and computations, i.e., trading local computation with global control signals or vice versa. Such trade-offs can be systematically devised and carried out by symbolic transformations in quite an elegant fashion, as illustrated here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5.">Transformations for Lower-Dimensional Networks</head><p>Another stage of transformations is required if a problem must be solved on a fixed-dimensional network. Taking the dynamic programming problem, for instance, suppose one is interested in solving it by a one-dimensional network. At any given time step of the target program above, there are 0 (n2) number of processors in execution, and the number must be reduced to 0 (n) for a one-dimensional network. For any processor (x, y) in this target program, one can simply choose to interpret any of the space components, say, x as a loop index within a processor y (described by the rest of the space components). It is clear then that any linear combination of the space components can serve as a loop index. In general, one can perform a linear transformation on the original space coordinates and choose any one of the components of the new coordinates as a loop index; the remaining components then become the new "processor id." The transformations on the program are quite similar to those described in Section 6.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">RELATED WORK</head><p>Many attempts in pursuit of systematic methods for synthesizing systolic computations have appeared in the literature. In the classification of systolic designs from the geometric point of view, Lin and Mead [lo] classify systolic arrays using symmetry groups, but there is no attempt made to obtain new designs. Cappello and Steiglitz [ 11 view systolic designs as affine transforms from combinational designs; however, their method does not give clues to finding the appropriate transforms.</p><p>Moldovan <ref type="bibr">[ 12,</ref> 131 notes that data dependency vectors are what suggest clues to potential linear transforms. Miranker and Winkler [ 1 l] describe a similar method, and they have observed that all planar two-dimensional regularly connected arrays can be embedded in a hexagonal array, called a universal array. Quinton [ 141 starts with a specification called a system of uniform recurrence equations and applies affine and linear transforms to obtain a systolic array. The class of systolic computations that can be described by uniform recurrence equations is limited to those that have only one of their data streams dependent upon other data streams. Li and Wah <ref type="bibr" target="#b7">[8]</ref> describe a method that first finds an appropriate placement of one of the input streams and then performs the linear transforms. Delosme and Ipsen <ref type="bibr" target="#b5">[6]</ref> describe how to implement a hyperbolic Cholesky solver by a linear transformation from a system of recurrences.</p><p>All of the above four methods focus on (quasi-)linear or affine mappings from a restricted class of algorithms to a new design. The class of designs that can be obtained by these methods is subject to restrictions such as starting with uniform recurrence equations (a much more restrictive definition than the uniformity defined in this paper), yielding only designs that are synchronous and regular, and having uniform data flow. The capabilities of these methods in various restricted situations may, at best, be equal to the general mapping procedure for the uniform algorithms defined in this paper. However, instead of using a simple, constant time procedure consisting of enumerating the basis communication vectors by subgroup enumerations and finding mappings between the two sets of basis vectors, they find linear mappings by searching through the space of possible linear transforms or input placements. The search requires a time that is at least polynomial of the size of the problem i31.</p><p>Some attempts, such as those described in <ref type="bibr" target="#b11">[ 12,</ref><ref type="bibr">111</ref>, are made to transform an initial mathematical definition to a description that is suitable for space-time mapping. The method used in these works applies to certain specific cases, and is somewhat ad hoc and not systematic or formal enough for potentially automating the process of transformations.</p><p>Li and Wah <ref type="bibr" target="#b8">[9]</ref> have classified the problems that can be solved by dynamic programming into several classes and have proposed systolic processors for solving them. The formulation given in this paper corresponds to the most general class-in their terminology, the "polyadic-nonserial" class. In their treatment, the issue of synchronization and timing is not addressed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">CONCLUDING REMARKS</head><p>The objective of this work is the devising of efficient parallel programs and VLSI architectures. Due to the complexity that might arise in dealing with hundreds of thousands of autonomous parallel processing elements, we find design methodology for this process necessary in order to take the enormous burden off the designers.</p><p>Program synthesis, at the very basic level, relies on a formal notation for describing the problem, and henceforth on manipulating the descriptions to yield efficient parallel programs. This paper describes, along with a set of program synthesis methods, a design methodology and program transformation environment in which these methods are applied at the various stages of transformations, such as the stage that proceeds from a problem definition to a program that has bounded fan-in and fan-out degrees, the stage that reduces long-range communications to local ones, the stage that incorporates pipelining into programs by space-time mapping, and the stage that derives optimized control signals.</p><p>It is worth mentioning that the transformation rules for deriving bounded fan-in/fan-out degree and bounded-order recursion equations are automatable and apply to problem definitions in general. The procedure of space-time mapping for uniform algorithms and the test for the existence of linear mappings are computationally attractive and their complexities are independent of the problem sizes. The theorems that allow the conversion of timevariant predicates to time-invariant ones for generating optimized control signals have a wide range of applications.</p><p>It is essential that the language Crystal be amenable to algebraic manipulation so that techniques developed for algorithm transformation can be car-tied out formally by an automated process. More importantly, Crystal is a general purpose programming language which allows new design methods and synthesis techniques, properties and theorems about problems in specific application domains, and new insights into any given problem to be integrated within the existing program transformation and architecure simulation environment . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="01.">01</head><p>! the identity element for the binary operator mins I FUNCTIONF~CDlBININGCC6TS u and" froasub~oblmres: t as-the cost of mltiplying a @ x q) nmtrix by a (q x r) matrix I is p*q*r , cl and 02 are costs l ceumAataJfrcmprmiou6stagas.md I Cp, r] is the pair of dimmsions of tba new matrix. </p><formula xml:id="formula_9">I+/&amp;=([ Cp, r], cl + c2 + p ' q * r] P = ref(u, [l/1.1]) r = ref(v, [1,1,2]) q = &lt;&lt; ref(u. [1.1,2]) = ref(v, [l,l,l]) -&gt; ref(v, [<label>l</label></formula><formula xml:id="formula_10">!I!!!!!!!!I!!I!!!!!!!!!I!!!!I!1!!!!!I!!I!!!!!!!!!!!!!!!I!!!!!!!!!!!!!!!!!! I lW.s systsmof recursion equations has constant fan-out degree ' t =(i,j,k) = &lt;&lt; j = k -&gt; C&amp;k). ' j &gt; k -&gt; a(i. j-l. k) ' &gt;&gt; I b(i,j,k) = &lt;&lt; i = k -&gt; C(k,j). i &lt; k -&gt; b(i+l. j, k) &gt;&gt; C(i.j) = 1 &lt;&lt; (j-i)-1 = 0 -&gt; co (i), I (j-1)-1 &gt; 0 -&gt; , \mins { b(a(i. j,k), b(i,j.k)), kl I k in (i+l) :(j-l) 1 ' &gt;&gt; !!!!!I!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!I!!!!!!!I!II!!!!!!! leoX2 !!!!!!!!!!!!I!!!!!!!!!!!!!II!!!!!!I!!!!!!!!!!!1!!!!I!!!!!!!!!!!!!lI!!!!!!!l ! This systemof recurElon equations has constant fan-in/fan-out degrees ! I C&amp;j) = =(L 1, j) ! I ! substituting cc into a and b a(i,j,k) = &lt;&lt; j = k -&gt; cc(i,k,k), j &gt; k -&gt; a(i, j-l, k) &gt;&gt; b(i,j,k) = &lt;&lt; i = k -&gt; cC(k.j.j). i &lt; k -&gt; b(i+l, j<label>.</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>k) &gt;&gt;</head><p>! fan-in degree as been reduced cc(i,j,k) = &lt;&lt; (j-i)-l= 0 -&gt; CO (i), </p><formula xml:id="formula_11">(j-1)-1 &gt; 0 -&gt; ! &lt;&lt; k = m(i,j) -&gt; mi~c( p(a(i.j,k). b(i.j.k)). kl. 8 Ch(a(i,j, (i+j)-k), b&amp;j. (i+j)-k)). F+j)-kl 1. i m(i,j) &lt; k C j -&gt; ! \mi~c { cc(i,j,k-1), p(a(i,j,k). b(i,j.k)). kl, Cb(a(i.j, (i+j)-k). b(i.j. (i+j)-k)), @+I)-kl 1, j k = j -&gt; cc(i,j, k-l) ' ZL-e ( m(i,j) = &lt;&lt; evw!i;;) -&gt; (i+j)/2, I ' '+' -&gt; (i+j+1)/2 9 &gt;&gt; ' 1 1 &gt;&gt; I 1 !I!!!!!!!!!!!!!!!!!!!! !!!!!!!!!!!I!!!!!!!!!. I!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!</formula><formula xml:id="formula_12">k = j -&gt; cc(i,k,k), m&amp;j) &lt;= k &lt; j -&gt; a(i, j-l, k) &gt;&gt; ! ! ! ! ! ! I ! ! 1 ! ! ! ! d(i,j,k) = &lt;&lt; (k = m&amp;j)) and (odd((j-Q-1)) -' a(L j-l. k). ! (k = m&amp;j))</formula><p>and (even(( </p><formula xml:id="formula_13">j-i)-1)) -&gt; d(L j-l. k-l), ! m(i,j) &lt; k &lt; j -&gt; d(i. j-l, k-l) ! &gt;&gt; I ! b(i,j,k) = &lt;&lt; (k = m(i,</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>s ' 0 * ~!#W(i, a COG j&gt;)l, where h is some C(i, j) = function on the costs s = 0 + Ci for some individual cost, where (2.1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FIG. 1 .</head><label>1</label><figDesc>FIG. 1. The DAG describing the data dependency of dynamic programming.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>v, where the fan-out degrees of x and z(l) for u I 1 I v are I.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>( 4</head><label>4</label><figDesc>.4) s = o-, c;.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>DEFINITION 4 . 1 .</head><label>41</label><figDesc>An operation "$,', where z = &amp;K~x(I), on uu -1 arguments, is associative if it can be replaced by the composition of the following sequence of binary associative operations "GY on variables z(l), u. &lt; 1 &lt; u;z(Z) = { 1= u + l-+x(u + 1) u + 1 &lt; 1 &lt; u + z(Z -1) $x(l),and z = z(u -1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>FIG. 2 .</head><label>2</label><figDesc>FIG. 2. Process structure &amp; in which the fan-in and fan-out degrees are constant, but in which there are still long-range communications.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>FIG. 3 .</head><label>3</label><figDesc>FIG. 3. Process structure 5, in which all data dependencies are local.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>b"&amp;p chosen basis data dependency vectors and let matrix B be defined as B = (b,, bz, b3); then di = Bai, where Since every component o+ of ai is nonnegative, System (5.5) of dynamic programming is a uniform algorithm. Choose the set of basis communication vectors</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>f-t = 22 -j 2(x, y, t) a(.% y, 4 = Ir 1; 5t&lt;2z+B(x,y-l,t-1)A (even(z)) + ci(x, y -1, t -1) A (odd(z)) + ri(x, y -1, t -</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>7. 4 .</head><label>4</label><figDesc>Control Signal Optimization DEFINITION 7.1. A control expression is an expressionp (x1, x2, . . .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>PROPOSITION 7 . 1 .</head><label>71</label><figDesc>FIG. 4. A series of parallel planes indicates the time steps of the space-time mapping.Processes on the same plane are mapped to the same invocation number t. (The processor number of a process in this case is its projection onto the bottom plane with the sign of x changed.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>2 .</head><label>2</label><figDesc>Suppose a control expression p (x1, x2, . . . , x,) is strictly monotonic decreasing in some space index xi, i.e., tf x/ &lt; xi then p (~1, . . . , xi), . . . , xn) &gt; p (~1, . . . , xi, . . . , xn). Then for each xi 9 the time dependency dt(xi) = p(Xl, X2, . . . 7 Xi, . . . 7 Xn)p(Xly X27 e . . 7 Xi + 1, . . . 3 Xn) of the control expression has a fixed value which is positive. THEOREM 7.1. Time-variant predicates of the form t &lt; Ph, x2, . . . , &amp;A t = Pbl, x2, * * * , XJ, and t ' Ph, x2, . . . , x,), wherep(x,, x2, . . . , x,) is a strictly monotonic control expression, can be implemented by time-invariant predicates 4, x2, . * . , xnr 4 = 2, respectively, where q(x,, x2, . . . , x,, t) is a control stream q(x,, x2, . . . , &amp;I, 4 = p(x1, x2, . . . ) xn) = o-, t = o+ 1 t&gt;O-,2 (establish the truth initially for the test) p(x*,x2, . . . ,x,) &gt; o-, q(X1, ~2, . . . 9 Xi -1, . . , ) Xnr t -dt(xi)) in which allpredicates, exceptfor the switch-onpredicates t &lt; 0, t = 0, and t &gt; 0, are time invariant, and dt(xi) is the time dependency of control expression p (x1, x2, . . . , xn).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>APPENDIX</head><label></label><figDesc>FIG. 5. Dynamic programming in Crystal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>FIG. L-Continued.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>FIG</head><label></label><figDesc>FIG. S-Continued.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>FIG</head><label></label><figDesc>FIG. S-Continued.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unifying VLSI array designs with geometric transformations</title>
		<author>
			<persName><forename type="first">F</forename><surname>Cappello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Steiglitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Parallel Processing</title>
		<meeting>IEEE International Conference on Parallel essing</meeting>
		<imprint>
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The generation of a class of multipliers: A synthesis approach to the design of highly parallel algorithms in VLSI</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Design: VLSI in Computers</title>
		<meeting>IEEE International Conference on Computer Design: VLSI in Computers</meeting>
		<imprint>
			<date type="published" when="1985-10">Oct. 1985</date>
			<biblScope unit="page" from="116" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Synthesizing systolic designs</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Second International Symposium on VLSI Technology, Systems, and Applications</title>
		<meeting>Second International Symposium on VLSI Technology, Systems, and Applications</meeting>
		<imprint>
			<date type="published" when="1985-05">May 1985</date>
			<biblScope unit="page" from="209" to="215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A parallel language and its compilation to multiprocessor machines</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 13th Annual Symposium on POPL</title>
		<meeting>13th Annual Symposium on POPL</meeting>
		<imprint>
			<date type="published" when="1986-01">Jan. 1986</date>
			<biblScope unit="page" from="131" to="139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Transformations of parallel programs in Crystal</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IFIP</title>
		<meeting>IFIP<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1986-09">Sept. 1986</date>
			<biblScope unit="volume">86</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An illustration of a methodology for the construction of efficient systolic architecture in VLSI</title>
		<author>
			<persName><forename type="first">J-M</forename><surname>Delosme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ipsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Second International Symposium on VLSI Technology, Systems, and Applications</title>
		<meeting>Second International Symposium on VLSI Technology, Systems, and Applications</meeting>
		<imprint>
			<date type="published" when="1985-05">May 1985</date>
			<biblScope unit="page" from="268" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Direct VLSI implementation of combinatorial algorithms</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Kung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Cultech Conf. VLSI</title>
		<meeting>Cultech Conf. VLSI</meeting>
		<imprint>
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The design of optimal systolic arrays</title>
		<author>
			<persName><forename type="first">G.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Wah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput. C</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="66" to="77" />
			<date type="published" when="1985-01">Jan. 1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Systolic processing for dynamic programming problems</title>
		<author>
			<persName><forename type="first">G.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1985 International Conference on Parallel Processing</title>
		<meeting>1985 International Conference on Parallel essing</meeting>
		<imprint>
			<date type="published" when="1985">1985</date>
			<biblScope unit="page" from="434" to="441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The application of group theory in classifying systolic arrays</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Mead</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Display File</title>
		<imprint>
			<biblScope unit="volume">5006</biblScope>
			<date type="published" when="1982-03">Mar. 1982</date>
		</imprint>
	</monogr>
	<note>Caltech</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Spacetime representations of computational structures</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Miranker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing</title>
		<imprint>
			<biblScope unit="page" from="93" to="107" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On the design of algorithms for VLSI systolic arrays</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">I</forename><surname>Moldovan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput</title>
		<imprint>
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">ADVIS: A software package for the design of systolic arrays</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">I</forename><surname>Moldovan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCD</title>
		<meeting>ICCD</meeting>
		<imprint>
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automatic synthesis of systolic arrays from uniform recurrent equations</title>
		<author>
			<persName><forename type="first">Quinton</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th Annual Symposium on Computer Architecture</title>
		<meeting>11th Annual Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1984">1984</date>
			<biblScope unit="page" from="208" to="214" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
