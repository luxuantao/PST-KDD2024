<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GPT-based Generation for Classical Chinese Poetry *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-09-05">5 Sep 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yi</forename><surname>Liao</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yasheng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">GPT-based Generation for Classical Chinese Poetry *</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-09-05">5 Sep 2019</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1907.00151v5[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a simple yet effective method for generating high quality classical Chinese poetry with Generative Pre-trained Language Model (GPT) <ref type="bibr" target="#b4">[5]</ref>. The method adopts a simple GPT model, without using any human crafted rules or features, or designing any additional neural components. While the proposed model learns to generate various forms of classical Chinese poems, including Jueju(绝句), Lüshi(律诗), various Cipai(词 牌) and Couples(对联), the generated poems are of very high quality. We also propose and implement a method to fine-tune the model to generate acrostic poetry. To the best of our knowledge, this is the first to employ GPT in developing a poetry generation system. We have released an online mini demonstration program on Wechat 1 to show the generation capability of the proposed method for classical Chinese poetry. * A side product by Huawei Noah's Ark Lab when investigating the capability of Generative Pretrained Language Model (GPT).</p><p>1 User may have to register a Wechat account and add "EI体验空间" or "诺亚实验室" 2 Couplets are not regarded as poems normally but here we do so just for convinience.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Classical Chinese poetry generation is an interesting challenge of natural language generation. Unlike free text generation, a classical Chinese poem should normally meet both form and content requirements <ref type="bibr" target="#b0">[1]</ref>. The form requirements includes the regulations on the number of words (字数), rhyming (押韵), tone patterns (平仄), pairing (对仗), etc . The other requirement is regarding content, which requires that the theme of a poem is consistent and coherent throughout the poem.</p><p>There are many different forms of classical Chinese poetry. Appendix A gives a brief introduction of these forms. Our system mainly focus those forms which have strict rules, which include couplets(对联) 2 , Wujue(五绝), Qijue(七 绝), Wulü(五律), Qilü(七律), and vaious Cipai(词牌) including Xijiangyue(西江 月), Manjianghong(满江红), Shuidiaogetou(水调歌头), etc .</p><p>Various methods e.g., <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> have been proposed to generate classical Chinese poetry. However, these methods are somewhat complicated so as to satisfy the aforementioned requirements in both form and content. For example, templatebased or constraint checking method is employed to guarantee the correctness of the form of the generated poetry. Key-words based mechanism is proposed to guarantee the consistency and coherency of a poem.</p><p>In this paper, we study the problem of poetry generation given a specific type of form requirement and a specific theme. In contrast with the existing methods, we propose a poetry generation method based on the pre-trained model GPT. The underlying model of our proposed method is simply a GPT language model fine-tuned with a classical Chinese poetry corpus, without any additional modifications. All we need to do is to serialize the training poems into formatted text sequences as training data. Poems are generated by sampling from the language model token by token without any constraint to meet the form and content requirements.</p><p>In addition, we proposed a fine-tune method to train a model to generate acrostic poetry (藏头诗), where some characters in given positions are predefined. The proposed method can guarantee that specific tokens can be generated by the language model in the corresponding positions.</p><p>Compared with existing methods, our propose method has below characteristics:</p><p>1. Model Conciseness. The proposed method is a simple Transformer model without additional variables. However, it is powerful enough to guarantee the form and content requirements. Neither we use any human-defined rules or features, nor we define any specific designed neural networks rather the standard GPT.</p><p>2. Well-formedness. We surprisingly observe that although we did not explicitly feed the model with any rules or features about classic Chinese poetry, such as the number of characters, rythming, tune patterns and coupling, the model is able to generate poems that automatically meet these rules very well for the tens of forms, even for some fairly complicated "Cipai" like "Shuidiaogetou" which contain around 100 characters. Actually, even for ordinary Chinese people,it is quite hard to master the skills to write well formed classical poems.</p><p>3. Poetry Diversity. We employ truncated top-k sampling strategy during the generation process. Hence the generated poems are highly diverse in different runs given the same form and theme.</p><p>4. Poetry Artistry. We observe that the model have a fair chance to generate high quality poems that express the poetry themes artistically, which is close to one written by specialized poets. Table <ref type="table">1</ref> shows four poems, among which only one was written by a Chinese poet more than one thousand years ago, while the remaining three poems are generated by our system.</p><formula xml:id="formula_0">江上田家 村南喧鸟雀，江北梦悠悠。 桑熟蚕三眠，人家半依楼。 一身千万里，何处得穷愁。 日暮歌明月，长河满斛秋。 江上田家 江边田舍好，茅屋远相迎。 竹里开门入，芦中引水行。 犬来沙上吠，鸥去岸间鸣。 不是无吟兴，谁知乐此生。 江上田家 近海川原薄，人家本自稀。 黍苗期腊酒，霜叶是寒衣。 市井谁相识，渔樵夜始归。 不须骑马问，恐畏狎鸥飞。 江上田家 野水通渔路，江村带夕阳。 数家深竹里，一树隔芦塘。 牧去牛羊下，人行果橘旁。 相逢皆贺岁，还有醉眠乡。</formula><p>Table <ref type="table">1</ref>: A real poem and three poems generated by our system 2 Our Method</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Model Details</head><p>We refer the readers to the blog<ref type="foot" target="#foot_0">3</ref> or the papers <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b3">4]</ref> to get a basic understanding of Transformer, which is the underlying model of the proposed method. Figure <ref type="figure">1</ref> depicts the process of training the poetry generation model.</p><p>We implement our own GPT model based on the source code of BERT<ref type="foot" target="#foot_1">4</ref> . The configuration of the size of the transformer is identical to the BERT-Base. We also adopt the tokenization script and Chinese vocab released in BERT. For text generation, we implement truncated top-k sampling instead of beam-search to generate diverse text <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Data processing</head><p>The training process includes two phases: pre-training and fine-tuning. Our GPT model is pre-trained with a Chinese news corpus.</p><p>For fine-tuning, we collect publicly available classical Chinese poems. As shown in Figure <ref type="figure">1</ref>, a sample poem is first transformed into a formatted sequence. A special case is couplets. In most case couplets do not have a theme, we use the first line as the theme and the second line as the body. The form is automatically filled with "对联" (couplets). So the generation of a couplet becomes to generate the second line given the first line, which exactly mimics the activity of Duiduizi (对对子).</p><p>The statistics of the pre-training data and fine-tuning data of our model are given in Table <ref type="table">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Model Training</head><p>Pre-training: We pre-trained our GPT model on Huawei Cloud Service with a news corpus which is detailed in Table <ref type="table">2</ref>. We trained it in 8 Nvidia V100 (16 GB) GPUs for 4 epochs. The pre-training takes totally 90 hours <ref type="foot" target="#foot_2">5</ref> . Chinese Wikipedia can be an alternative training corpus.</p><p>Fine-tuning: We feed the all the training sequences of poems into the transformer and train a auto-regressive language model. The objective is to maximize the probability of observing any sequence X = {x 1 , x 2 , ..., x |X| }:</p><formula xml:id="formula_1">P (X) = 1≤i≤|X| log p(x i |x 1 , ..., x i−1 )<label>(1)</label></formula><p>where p(x i |x 1 , ..., x i−1 ) is the probability that the token x i will be generated given all the historical tokens. The fine-tune process takes much less time as the model gets overfitted if trained too long. When the model is overfitted, it tends to retrieve raw sentences from the corpus during the generation process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Poetry Generation</head><p>Once the training is completed, we apply the model to generate poems given a form requirement and a theme in the following process. We first transform the form and theme into an initial sequence as [form, identifier 1, theme, identifier 2 ], then the initial sequence is fed into the model and the remaining field of body is decoded token-by-token. Note that we do not apply hard constraint during the decoding process to guarantee the correctness of the form. Instead, the model is able to automatically assign high probabilities to commas and periods in certain positions when decoding. The decoding process is end when we reach the end of the body, which is recognized by an "EOS" token. Truncated top-k sampling: Instead of beam-search during the decoding process, we apply truncated top-k sampling strategy to obtain diverse poems. Each time to sample a token, tokens with top-k largest probabilities are first selected and then a specific token is sampled from the top-k tokens. We observe that the generated poems are in correct form even though truncated top-k sampling strategy is applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Train a model for acrostic poetry generation</head><p>We employ the same method for train a model for generating acrostic poetry. While the training and generation processes are exactly the same, we format the sequence in a slightly different way. Specifically, we replace the original theme of a poem with the combinations of the first character in each line. In the example, the first characters are "床", "疑", "举", and "低". Then the original them "静夜思" is replaced with "床疑举低". With the new data processing method, this training sample becomes " 五言绝句(格式)床疑举低(藏头诗)床前 明月光，疑. . . 月，低头思故乡。"</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Generated examples and observations</head><p>A selected set of generated examples by our model is given in Table <ref type="table" target="#tab_2">3 -6</ref>.</p><p>We observed that:</p><p>• The method performs consistently well in generating couplets(对联), jueju(绝 句) and lüshi(律诗). For couplets shown in Table <ref type="table" target="#tab_0">3</ref>, almost all the generated second line (下联) are paired with their corresponding first line (上 联) in terms of characters in the same position. For Lüshi as shown in the third and forth poetry in Table <ref type="table">4</ref>, it pairs the third sentences and the forth sentences, and pairs the fifth and sixth sentences, while the remaining sentences are not paired. The observation is quite surprising as the model learns the much complicated pairing rules for Lüshi, which are hard to grasp even for normal educated Chinese native speakers. Pairing sentences in certain places in a poem greatly improves its the quality and beauty.</p><p>• Well-formedness. More than 95% of the generated Jueju and Lüshi are well-formed as the form requirement of these poetry categories are relatively simple compared with Cipai. In terms of Cipai, the method does not performs as good as Jueju and Lüshi regarding well-formedness. Possible reasons may include the complexity of the forms and the lack of sufficient data for each type of Cipai. There are tens of thousands of training samples for each category of Jueju/Lüshi, while for Cipai, there are totally 882 types of Cipai in the training corpus but only 104 of them have more than one hundred training samples each and the largest type contains only 816 training samples. The possibilities to generate correct Cis also vary for different types of Cipai, which could be attributed to the differences in complexity as well as the numbers of training samples of that type in the training corpus. The Cipai Shuidiaogetou, which contains 744 training samples, is relatively difficult as the length requirement for each line varies. Roughly 70% of the generated Cis for Shuidiaogetou are correct in form. One of the example is shown in Table <ref type="table" target="#tab_1">5</ref>.</p><p>• Although not explicitly modelled, the rhyming (平仄) and tone patterns (押韵) of the generated poems are also fairly good.</p><p>• Diversity. As we adopt sampling strategy while decoding, our method can generate highly diverse output in different runs. As shown in Table <ref type="table" target="#tab_0">3</ref> and Table <ref type="table">4</ref>, the model generate totally different second lines (下联) for the same first line (上联) and different poems for the same theme in multiple runs. We also notice that when the given first line is in the training corpus, it is possible that the model retrieves the whole original second line from the training corpus. However, for poetry generation, it generates new sentences even though the given theme is in the training corpus.</p><p>• Artistry. We observe that the model can sometimes generate high quality poems that express the poetry themes artistically. However, while the generation quality for some given themes are constantly good in multiple runs, some themes, such as "机器翻译", which appear rarely in the training corpus are less likely to generate good poems. "秋思" is a good theme to generate high quality poems. The examples in Table <ref type="table">4</ref> are generated in one run without manual selection. </p><formula xml:id="formula_2">上联(First line) 下联(Second line) 一句相思吟岁月 几分寂寞醉诗词 三杯落泪诉年华 三分眷恋到天涯 风弦未拨心先乱 诗卷未题笔已枯 月盏虽清梦已酣 草色初青眼欲穿 海上飞燕飞上海 城外环山环外城 江中落叶落中江 山前雾气雾前山 水墨丹青，烟雨江南春纵笔 花红柳绿，桃源粤地燕裁云 诗词歌赋，风花塞外夏抒怀 天光风韵，云霞岭上日倾杯</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Early works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref> on Chinese poetry generation have been mostly rulebased or template-based. Recurrent Neural Network (RNN) <ref type="bibr" target="#b10">[11]</ref> was recently introduced as it has been proved to be effective in generation tasks such as machine translation and dialog generation. However, few researchers have adopted the latest self-attention models for generating poems. As far as we know, we are the first to employ GPT in developing a poetry generation system. GPT has been famous for having the capability to generate text that can hardly be distinguished even by human beings. As a natural consequence, a GPT-based method could potentially write high quality poems.</p><p>Existing methods have been focusing on improve the well-formedness and content coherence of generated poems. To make sure the rhyming, the tone patterns and the pairing of a generation are correct, various strategies have been adopted. For example, Yan (2016) <ref type="bibr" target="#b11">[12]</ref> proposes an iterative polishing schema, which refines the generated poem until a well-formed one is obtained. In the mean time, some other works have been investigating the coherence of content throughout a poem. For example, <ref type="bibr" target="#b0">Yi et al. (2018)</ref>  <ref type="bibr" target="#b0">[1]</ref> propose a salientclue mechanism which automatically selects the most salient characters from the so-far generated lines as a theme clue for generating the next line. <ref type="bibr" target="#b13">Yang et al. (2017)</ref>  <ref type="bibr" target="#b13">[14]</ref> and <ref type="bibr" target="#b12">Wang et al. (2016)</ref>  <ref type="bibr" target="#b12">[13]</ref> employ a two-stage approach,</p><formula xml:id="formula_3">五绝(Wujue)•秋思 暮燕翻惊户， 飞鸿却唤人。 西风卷梧叶， 触落一庭秋。 七绝(Qijue)•秋思 年华冉冉飞无翼， 风物萧萧滞故乡。 万里重云正愁绝， 洞庭湖外见清霜。 五律(Wulü)•秋思 秋窗堕清影，默坐不复收。 风来户外闹，月过竹边幽。 得失已搔首，是非空置忧。 南鸿若忘机，直飞念其幽。 七律(Qilü)•秋思 南浦西山雨送秋，病留白发日添愁。 风尘苒苒侵儒素，霜露番番到客裘。 客子渐衰空感慨，古人难得始淹留。 欲谈往事浑无可，万里波涛一叶舟。 Table 4:</formula><p>Theme "秋思" with different form requirements. Generated in one run without manual selection where a set of keywords are planned first and are then fed into the generation of different lines sequentially. Besides above works, some researchers investigate other interesting topics on poetry generation. For example, <ref type="bibr">Yang et al. (2018)</ref> [15] propose a model for stylistic Chinese poetry generation .</p><p>Compared with the existing methods, the major advantage of our proposed approach is the conciseness and simplicity of the model. Meanwhile, it still exhibits strong, or sometimes even better ability in generating well-formed and coherent poems. For example, it is easy for our proposed method to generate well-paired sentences at once, especially for Lüshi, which, however, is relatively difficult for the existing method unless multiple times of polishing are adopted. Regarding the content coherence, in rare cases, our proposed method is even beyond relying on keywords to make the poetry coherent. Rather, it expresses either the story, the scene, or the emotion to describe a deeply coherent poetry comprehensively and artistically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Works</head><p>We present a classical Chinese poetry generation method based on pre-trained language model. The proposed method is far simpler than existing method based on recurrent neural networks and can generate better poems in some perspectives. Though the generated poems are not perfect all the time, our preliminary experiments have shown that GPT provides a good start to promote the overall quality of generated poems. That is, how to express the scene, the story, the emotions, and so on in a natural and artistry way. We present this report in the hope of helping researchers in understanding the capability of GPT as well as developing better poetry generation systems. </p><formula xml:id="formula_4">水调歌头(Shuidiaogetou)•游园 天上摘星汉，谪下笔端严。 云台望断，千古遗恨沓难兼。 谁信而今老态，犹恋吾庐清磐，乘兴下江南。 便好徜徉去，好处却容瞻。 风流事，人未老，气方炎。 平生出处何似，此乐几年淹。 闻道玉霄仙驾，犹记横空丹鹤，飘下五云顶。 不是出世相，来到玉皇边。</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Forms of Classical Chinese Poetry</head><p>According to the form requirements, there are different categories of classical Chinese poetry which could be summarized as follows:</p><p>• Couplets (对联): A couplet is a pair of sentences in classical Chinese poems which follow strict rules on lengths, rhyming, tone patterns and paring. Couplets are normally not regarded as poems, however they can also be used independently. Here we treat couplets as a category of poetry just for convinience.</p><p>• Old Style Poetry (Gutishi,古体诗): mainly includes two forms:</p><p>-Five-character Gushi (五言古诗), contain variable number of sentence pairs where each sentence has 5 characters.</p><p>-Seven-character Gushi (七言古诗), contains variable number of sentence pairs where each sentence has 7 characters.</p><p>Its forms are relatively flexible without strict regulations on lengths, rhyming, tone patterns and pairing.</p><p>• New Stype Poetry (Jintishi,近体诗): mainly includes four forms:</p><p>-Five-character Jueju (or Wujue, 五言绝句，五绝)，contains 4 sentences where each sentence has 5 characters, totally 20 characters.</p><p>-Seven-character Jueju (or Qijue, 七言绝句，七绝), contains 4 sentences where each sentence has 7 characters, totally 28 characters.</p><p>-Five-character Lüshi (or Qilü, 五言律诗, 五律), contains 8 sentences where each sentence has 5 characters, totally 40 characters.</p><p>-Seven-character Lüshi (or Qilü, 七言律诗, 七律), contains 8 sentences where each sentence has 7 characters, totally 56 characters.</p><p>There are strict rules on these forms regarding lengths, rhyming, tone patterns and paring.</p><p>• Lyric Poetry (Ci,词): Unlike Gutishi or Jintishi, the sentences in a piece of Ci have different number of characters. Depending on the combinations of different length sentences, there are about 100 different forms of Ci (Cipai), where some of the most frequently used forms are:</p><p>-Shuidiaogetou (水调歌头) </p><formula xml:id="formula_5">-Manjianghong (满江红) -Langtaosha (浪淘沙) 五绝(Wujue)•瑞雪兆丰年 瑞雪中山酒， 农祥吉岁功。 从今三白兆， 大府似新丰。 七绝(Qijue)•瑞雪兆丰年 十分晴色弄春寒， 雪压云低晓更看。 农事未明天意好， 一蓑带月下长滩。 五律(Wulü)•瑞雪兆丰年 雪兆开丰岁，农祥积庆家。 三登禾稼熟，万顷玉粒加。 晓色晴方好，风光日欲斜。 愿流鸿满野，飞洒到天涯。 七律(Qilü)•瑞雪兆丰年 雪后乾坤万象开，老天还有喜春回。 三农已报丰年瑞，百兽应知温室梅。 千里欢声连绿野，几家高卧卷青苔。 麦田多庆须吾辈，闻说江东亦好栽。 Table 9: Theme "瑞雪兆丰年" with different form requirements. 五绝(Wujue)•溪流背坡村 溪流向人急， 山路背城斜。 不识春林处， 蜂喧蝶趁衙。 七绝(Qijue)•溪流背坡村 一湾窈窕竹婵娟， 十里平波镜样天。 白鹭飞来无个事， 淡烟和雨暝苍然。 五律(Wulü)•溪流背坡村 溪流疑有路，日暮缓跻攀。 白石前村近，青山远寺间。 雨余林翠湿，风软鸟声闲。 幽趣无人会，扁舟又欲还。 七律(Qilü)•溪流背坡村 十里荒坡路不通，一溪流过碧玲珑。 树头鸦舅遥相应，篱下鸡豚晚自丰。 岁旱得粮多乐业，村深无酒易浇胸。 老翁已作扶犁叟，儿女逢迎笑语同。</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 : 2 :</head><label>12</label><figDesc>Figure 1: The process for training the poetry generation model Training Phases Corpus type Corpus size Pre-training Chinese news 235M sentences</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>[ 15 ]</head><label>15</label><figDesc>Yang C, Sun M, Yi X, Li W. Stylistic Chinese Poetry Generation via Unsupervised Style Disentanglement. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing 2018.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Example of Couplets. We present three generated second lines (下联) for each given first line (上联)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 5 :</head><label>5</label><figDesc>Examples of Ci。</figDesc><table><row><cell>七律(Qilü)•一路平安</cell></row><row><cell>一声天际雁横秋，忽梦青城旧友游。</cell></row><row><cell>路入青林无去马，手携黄牒有归舟。</cell></row><row><cell>平生志业商山老，何日公卿汉署留。</cell></row><row><cell>安得相从话畴昔，一樽同醉万山头。</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 6 :</head><label>6</label><figDesc>Examples for Acrostic Poetry</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 10 :</head><label>10</label><figDesc>Theme "溪流背坡村" with different form requirements.</figDesc><table><row><cell>五绝(Wujue)•沧海横流方显英雄本色</cell></row><row><cell>沧海横丈夫，</cell></row><row><cell>慷慨平生气。</cell></row><row><cell>胸中一寸铁，</cell></row><row><cell>霹雳惊鱼艇。</cell></row><row><cell>七绝(Qijue)•沧海横流方显英雄本色</cell></row><row><cell>一叶飘然万里秋，</cell></row><row><cell>独凭江槛思悠悠。</cell></row><row><cell>英雄只有心相许，</cell></row><row><cell>到处须横反不流。</cell></row><row><cell>五律(Wulü)•沧海横流方显英雄本色</cell></row><row><cell>赤子观星陨，沧波起风涛。</cell></row><row><cell>怒号天失夜，冻缆海变潮。</cell></row><row><cell>神力与柔颉，雄心生鼓刀。</cell></row><row><cell>一身千亿骨，敢以不羁高。</cell></row><row><cell>七律(Qilü)•沧海横流方显英雄本色</cell></row><row><cell>一线春蚕起两眠，赤鳞红尾血流年。</cell></row><row><cell>长江风浪惊天地，老海波涛入圣贤。</cell></row><row><cell>此日英英呈伟画，他时咄咄逼人肩。</cell></row><row><cell>凭谁为唤真头角，直上冲云跨九天。</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 11 :</head><label>11</label><figDesc>Theme "沧海横流方显英雄本色" with different form requirements.</figDesc><table><row><cell>五绝(Wujue)•端午</cell></row><row><cell>节遇端阳日，</cell></row><row><cell>蒲觞满自倾。</cell></row><row><cell>儿童采菖屑，</cell></row><row><cell>白首忆家庭。</cell></row><row><cell>七绝(Qijue)•端午</cell></row><row><cell>榴花角黍泛菖蒲，</cell></row><row><cell>想见端居昼景舒。</cell></row><row><cell>欲赋前贤无好语，</cell></row><row><cell>愿将此意答天衢。</cell></row><row><cell>五律(Wulü)•端午</cell></row><row><cell>节序重重过，乡邻草草同。</cell></row><row><cell>不堪时服彩，空把角巾东。</cell></row><row><cell>江月随人好，荆花笑己穷。</cell></row><row><cell>今朝千里至，共贺两年丰。</cell></row><row><cell>七律(Qilü)•端午</cell></row><row><cell>彩丝百缕纫为佩，艾叶千窠结作人。</cell></row><row><cell>台架周遭珠竞出，宫庭左右玉争新。</cell></row><row><cell>香浓蜜渍蜂房细，味硬金蒸蚁杓醇。</cell></row><row><cell>欲学灵均吊湘客，不知谁是楚骚臣。</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 12 :</head><label>12</label><figDesc>Theme "端午" with different form requirements.</figDesc><table><row><cell>五绝(Wujue)•春草</cell></row><row><cell>日暖春草长，</cell></row><row><cell>游子念归程。</cell></row><row><cell>此日草堂下，</cell></row><row><cell>思君肠断声。</cell></row><row><cell>七绝(Qijue)•春草</cell></row><row><cell>绿到江干草又青，</cell></row><row><cell>可怜春色太分明。</cell></row><row><cell>无情最是黄莺语，</cell></row><row><cell>犹恋斜阳犹恋名。</cell></row><row><cell>五律(Wulü)•春草</cell></row><row><cell>草色连朝尽，花枝一夜稀。</cell></row><row><cell>春光已无限，客行何必归。</cell></row><row><cell>野兴寻溪宿，乡情在鸟飞。</cell></row><row><cell>山中有羁客，应识异乡衣。</cell></row><row><cell>七律(Qilü)•春草</cell></row><row><cell>几年曾记翠离宫，草木萋萋色更浓。</cell></row><row><cell>画出晓霞明涧户，笔成春雨落池峰。</cell></row><row><cell>池边好鸟冲花出，亭下闲人卷箔从。</cell></row><row><cell>闻道斜阳已惆怅，不堪空听子规钟。</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 13 :</head><label>13</label><figDesc>Theme "春草" with different form requirements.</figDesc><table><row><cell>七绝(Qijue)•机器翻译</cell></row><row><cell>机器中藏千里波，</cell></row><row><cell>无风自动亦长讹。</cell></row><row><cell>不知谁是翻译手，</cell></row><row><cell>妙用方圆宛转多。</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 14 :</head><label>14</label><figDesc>More examples of Qijue</figDesc><table><row><cell>七绝(Qijue)•阅读理解</cell></row><row><cell>理随心解妙难知，</cell></row><row><cell>识尽偏圆始析离。</cell></row><row><cell>若谓二三皆正用，</cell></row><row><cell>不应礼义总成违。</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 15 :</head><label>15</label><figDesc>More examples of Qijue</figDesc><table><row><cell>五绝(Wujue)•华为雄起</cell></row><row><cell>华 华 华月移樽侧</cell></row><row><cell>为 为 为农古汉家</cell></row><row><cell>雄 雄 雄风生两腋</cell></row><row><cell>起 起 起舞纵天沙</cell></row><row><cell>Note: 两腋风生is an idiom.</cell></row><row><cell>七律(Qilü)•小城故事</cell></row><row><cell>小 小 小园东第起征轩，水柳低垂几万千。</cell></row><row><cell>城 城 城里东西新宰相，楼中左右旧王孙。</cell></row><row><cell>故 故 故人不见乌衣巷，晚节谁如白乐园。</cell></row><row><cell>事 事 事去节残人自老，花残叶落独魂销。</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 16 :</head><label>16</label><figDesc>More examples for Acrostic Poetry</figDesc><table><row><cell>满江红(Manjianghong)•游园</cell></row><row><cell>一雨连朝，春事少、游园病酒。</cell></row><row><cell>还记取、少年游乐，踏青斗草。</cell></row><row><cell>红日三竿花似洗，玉肌十二香如扫。</cell></row><row><cell>任醉扶、金盏倒垂莲，都忘了。</cell></row><row><cell>狂态度，无拘检。</cell></row><row><cell>娇姿媚，浑如削。</cell></row><row><cell>任宝钗零落，翠鬟偷帖。</cell></row><row><cell>帘外卖花枝上燕，尊前醉玉枝头小。</cell></row><row><cell>问此花、不肯转诗肠，须频绕。</cell></row><row><cell>武陵春(词牌名)游园(标题)</cell></row><row><cell>长忆西湖湖上宴，一笑倒琼彝。</cell></row><row><cell>红翠照人如有思，相见又相疑。</cell></row><row><cell>别岸潮生苹末涨，催发浪花飞。</cell></row><row><cell>照酒红妆间舞衣，天远水长迟。</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 17 :</head><label>17</label><figDesc>More examples for Ci</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0">http://jalammar.github.io/illustrated-transformer/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1">https://github.com/google-research/bert</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2">The model underfitted the data when we employed it for fine-tuning</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements The authors would like to thank Yuanzhen Liu and Xiaoya Wei for building the demonstration system on Wechat; to thank Wenyong Huang, Xiaozhe Ren, and Weiguo Li for building the training platform on Huawei Cloud Service; to thank Junqiu Wei, Xiaoguang Li, Liangyou Li, Yun Chen, Meng Zhang, Yinpeng Guo and Xiao Chen for providing insightful suggestions.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>-Xijiangyue (西江月) -... Ci also has very strict rules on lengths, rhyming, tone patterns and paring. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B More Examples</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Chinese Poetry Generation with a Salient-Clue Mechanism</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Generating Classical Chinese Poems via Conditional Variational Autoencoder and Adversarial Training</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
				<imprint>
			<date type="published" when="2018-10-11">2018 Oct 11</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://s3-us-west-2.amazon-aws.com/openai-assets/research-covers/languageunsupervised/languageunderstandingpaper.pdf" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The curious case of neural text degeneration</title>
		<author>
			<persName><forename type="first">A</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09751</idno>
		<imprint>
			<date type="published" when="2019-04-22">2019 Apr 22</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">New hitch haiku: An interactive renku poem composition supporting tool applied for sightseeing navigation system</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nakatsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Entertainment Computing</title>
				<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Genetic algorithm and its implementation of automatic generation of chinese songci</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Software</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="427" to="437" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">poet: automatic chinese poetry composition through a generative summarization framework under constrained optimization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><forename type="middle">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Third International Joint Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generating Chinese classical poems with statistical machine translation models</title>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Sixth AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Chinese poetry generation with recurrent neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Poet: Automatic Poetry Composition through Recurrent Neural Networks with Iterative Polishing Schema</title>
		<author>
			<persName><forename type="first">Yan</forename><forename type="middle">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Twenty-Fifth International Joint Conference on Artificial Intelligence</title>
				<meeting>Twenty-Fifth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Chinese poetry generation with planning based neural network</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Computational Linguistics: Technical Papers</title>
				<meeting>the 26th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generating thematic chinese poetry with conditional variational autoencoder</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence</title>
				<meeting>the Twenty-Seventh International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
