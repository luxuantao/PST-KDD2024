<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Human Pose Estimation from Video and IMUs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Timo</forename><surname>Von Marcard</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Leibniz-University of Hannover</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Gerard Pons-Moll is with the Perceiving Systems Department of the Max Planck for Intelligent Systems</orgName>
								<address>
									<settlement>Tuebingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Human Pose Estimation from Video and IMUs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BD84A28C9F7CD9D2CE85788971F6DA51</idno>
					<idno type="DOI">10.1109/TPAMI.2016.2522398</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2016.2522398, IEEE Transactions on Pattern Analysis and Machine Intelligence</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Human Pose Estimation</term>
					<term>Motion Capture</term>
					<term>Multisensor Fusion</term>
					<term>Inertial Sensors</term>
					<term>IMU</term>
					<term>Animation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we present an approach to fuse video with sparse orientation data obtained from inertial sensors to improve and stabilize full-body human motion capture. Even though video data is a strong cue for motion analysis, tracking artifacts occur frequently due to ambiguities in the images, rapid motions, occlusions or noise. As a complementary data source, inertial sensors allow for accurate estimation of limb orientations even under fast motions. However, accurate position information cannot be obtained in continuous operation. Therefore, we propose a hybrid tracker that combines video with a small number of inertial units to compensate for the drawbacks of each sensor type: on the one hand, we obtain drift-free and accurate position information from video data and, on the other hand, we obtain accurate limb orientations and good performance under fast motions from inertial sensors. In several experiments we demonstrate the increased performance and stability of our human motion tracker.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>I N this paper we deal with the task of human pose tracking, also known as motion capturing (MoCap) <ref type="bibr" target="#b0">[1]</ref>. Compared to commercial marker-based systems, video-based marker-less motion capture systems are very appealing because they are inexpensive and non-intrusive. Unfortunately, occlusions, partial observations and image ambiguities make the problem very hard. Hence, there is still a gap between the accuracy and reliability of marker-less systems compared to marker-based solutions. To this end, we propose a hybrid tracker that combines information coming from video cameras with information coming from a small number of inertial sensors. In particular, we use only five inertial sensors attached at the body extremities of the subject. By combining both sensor types the tracking performance increases in both accuracy and stability. The proposed tracking solution is an inexpensive alternative to commercial marker-based systems to perform motion capture. Although it is more intrusive than pure marker-less systems, five miniature IMU sensors do not hamper the range of motions a subject can perform. This makes it a very appealing and practical solution for applications where high accuracy and realism is required, e.g., for movie production and medical analysis. Stabilizing pure video-driven MoCap with learned priors is very common in the literature to compensate for inherent ambiguities. Using additional a priori knowledge such as familiar pose configurations learned from motion capture data helps considerably to handle more difficult scenarios like partial occlusions, background clutter, or corrupted image data. There are several ways to employ such a priori knowledge to human tracking. One option is to learn the space of plausible human poses and motions <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. Another option is to learn a direct mapping from image features to the pose space <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref> or to mid-level representations of pose <ref type="bibr" target="#b13">[14]</ref> through posebits.</p><p>Manuscript received August 9th, 2015. To constrain the high dimensional space of kinematic models, a major theme of recent research on human tracking has been dealing with dimensionality reduction <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>. Here, the idea is that a typical motion pattern like walking should be a rather simple trajectory in a lower dimensional manifold. Therefore, prior distributions are learned in this lower dimensional space. Such methods are believed to generalize well with only little training data.</p><p>Inspired by the same ideas of dimensionality reduction, physical and illumination models have been recently proposed to constrain and to represent human motion in a more realistic way <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. A current trend of research tries to estimate shape deformations from images besides the body pose by either directly deforming the mesh geometry <ref type="bibr" target="#b19">[20]</ref> or by a combination of skeleton-based pose estimation with surface deformation <ref type="bibr" target="#b20">[21]</ref>.</p><p>Recently, inertial sensors (e.g. gyroscopes and accelerometers) have become popular for human motion analysis. <ref type="bibr" target="#b21">[22]</ref> presents a system to capture full-body motion using only inertial and magnetic sensors. While the system in <ref type="bibr" target="#b21">[22]</ref> is very appealing because it does not require cameras for tracking, the subject has to wear a suit with at least 17 inertial sensors, which might hamper the movement of the subject. In addition, long preparation time before recording is needed. Moreover, inertial sensors suffer from severe drift problems and cannot provide accurate position information in continuous operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Contributions</head><p>Even using learned priors from MoCap data, obtaining limb orientations from video is a difficult problem. Intuitively, because of the cylindrical shape of human limbs different limb orientations project to very similar appearances in the images. These orientation ambiguities can be easily captured by the inertial sensors but accurate joint positions in the world space cannot be obtained. Therefore, we propose to use a small number of sensors (we use only five) fixed at the body extremities (lower arms, shanks and waist) as a complementary data source to visual information. On the one hand, we obtain stable and drift-free accurate position information from video data and, on the other hand, we obtain accurate limb orientations from the inertial sensors. The present work is an extension of our preliminary conference paper <ref type="bibr" target="#b22">[23]</ref> and improves it in several ways:</p><p>•</p><p>We provide additional details on integrating orientation data to a contour-based video tracker. In Sec. 6.1 we minimize the squared geodesic distance of estimated and measured sensor orientations. This leads to three independent constraint equations per sensor instead of nine. We also show how to minimize the squared Frobenius norm of orientation differences (chordal distance) in Sec. 6.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>In order to provide a more thorough evaluation of the hybrid approach, we recorded a new data set TNT15. We will make it available for research at <ref type="bibr" target="#b23">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>We present a totally new experimental evaluation. In Sec. 7.1 we describe the experimental setup and introduce two error metrics, which measure the tracker performance in complementary ways. A comparison of the video and hybrid approach is given in Sec. 7.2.</p><p>• Additionally, we investigated different settings of the hybrid tracker. In Sec. 7.3 we evaluate the influence of the hybrid tracker's weighting parameter λ that balances the IMUs and video energy terms. In Sec. 7.4 we inspect the tracking error vs. the number of camera views. We also evaluate the sensitivity of the tracker to sensor lag in Sec. 7.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>In Sec. 7.6 we evaluate the hybrid tracker approach on the HumanEva <ref type="bibr" target="#b24">[25]</ref> dataset. Ground-truth body poses are used to synthesize the missing IMU data and the average joint position error is reported and compared to state-of-the-art approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Most works in 3D human pose estimation in computer vision have focused on obtaining a rough estimate of the skeletal configuration by lifting 2D body part detections <ref type="bibr" target="#b25">[26]</ref>. Multiple views and temporal consistency have also been exploited. For example, in <ref type="bibr" target="#b26">[27]</ref>, the authors propose a so-called temporally consistent 3D Pictorial Structures model (3DPS) for multiple human pose estimation from multiple cameras views. The model extends multiview 3D pictorial structures with a temporal consistency between the inferred poses. The focus of these works is to estimate the pose for tasks such as human action recognition or scene understanding. Hence, realism is not a requirement and pose estimates often do not include 3D limb orientations and suffer from errors such as motion jitter, foot skating and unbalance. Higher fidelity body models with an underlying skeleton have also been used for pose estimation <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b27">[28]</ref>. The use of models with higher degrees of freedom also comes with more ambiguities which researchers compensate by using action priors <ref type="bibr" target="#b7">[8]</ref> or robust likelihood functions and global optimization schemes <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>. On the other end of the spectrum, performance capture approaches use a large number of cameras to capture the full surface geometry of the body, potentially including clothing <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>. These approaches are very appealing and produce very realistic results. However, it is not trivial to transfer the captured surface motion to new avatars. Furthermore, such approaches could benefit from our proposed hybrid tracker since limb orientations are very hard to estimate when they are occluded by clothing, e.g., legs occluded by a skirt. The field of human pose estimation has experienced significant advances with the availability of the inexpensive depth sensor kinect. A depth sensor significantly simplifies the problem since many depth ambiguities can be resolved. In the influential paper of <ref type="bibr" target="#b32">[33]</ref> the pose estimation problem is turned into a body part classification problem. In <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref> they extended the approach of <ref type="bibr" target="#b32">[33]</ref> to directly regress correspondences to a model to improve the accuracy of the predictions. Several other approaches have been published to tackle the problem of pose and shape estimation from depth sensors <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>. A survey on pose estimation using depth images has been presented in <ref type="bibr" target="#b40">[41]</ref>. Most existing works focus on body part detection and pose estimation. Although depth sensors are very appealing for applications such as gaming, they do not work very well outdoors and the recording volume is limited. Furthermore, for both RGB and depth data, orientation ambiguities are still an issue.</p><p>Inertial sensors (IMU) do not suffer from such limitations but they are intrusive by nature: at least 17 units must be attached to the body which poses a problem for bio-mechanical studies and sports sciences. Additionally, IMUs alone fail to measure accurately translational motion and suffer from drift. Perhaps surprisingly, not many works can be found that combine inertial sensors with visual cues. This is maybe due to the fact that IMUs have been less available in the past. However, inertial sensors are becoming affordable. In fact, most cellphones come with integrated IMUs. IMUs alone have been often used for medical applications, see, e. g., <ref type="bibr" target="#b41">[42]</ref> where accelerometer and gyroscope data is fused. However, their application concentrates on the estimation of the lower limb orientation in the sagittal plane. An exception that combines visual and orientation cues is <ref type="bibr" target="#b42">[43]</ref>, but it is restricted to the tracking of a single limb (the arm). Moreover, only a simple red arm band is used as image feature. In <ref type="bibr" target="#b43">[44]</ref>, data obtained from few accelerometers is used to retrieve and play back human motions from a database. In <ref type="bibr" target="#b44">[45]</ref>, the authors fuse information from densely placed inertial sensors with a global position estimate by using a laser range scanner equipped robot accompanying the tracked person. In terms of full-body motion tracking with visual and inertial cues, the most similar approaches to the current work are probably <ref type="bibr" target="#b38">[39]</ref> and <ref type="bibr" target="#b45">[46]</ref>. In <ref type="bibr" target="#b38">[39]</ref>, the authors combine a generative tracker and a discriminative tracker by retrieving closest poses in a database. A visibility model based on depth images (kinect) as well as an inertial database lookup is used. In <ref type="bibr" target="#b45">[46]</ref> IMUs are used to derive a manifold of poses that satisfy the sensor orientation constraints. A particle-based optimization scheme is then applied to find the pose in the manifold, which best matches the image information obtained from video cameras. The proposed hybrid tracker in this work is based on fast local optimization, while the approach in <ref type="bibr" target="#b45">[46]</ref> relies on global optimization which is computationally too expensive for many applications. The authors of <ref type="bibr" target="#b27">[28]</ref> have demonstrated, that fusing global and local optimization methods can lead to systems which combine the best of both worlds; similarly <ref type="bibr" target="#b45">[46]</ref> could be combined with the hybrid tracker to recover from tracking failures. In contrast to <ref type="bibr" target="#b38">[39]</ref>, our proposed approach directly optimizes consistency of model and sensor measurements and is not restricted to motions in a prerecorded inertial database.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPONENTIAL MAPS FOR RIGID BODY MOTION</head><p>To model human joint motion, it is often needed to specify the axis of rotation of the joint. For example we might want to specify the motion of the knee joint as a rotation about an axis perpendicular to the leg and parallel to the hips. Therefore, for our purpose the axis-angle representation is optimal because rotations are described as an angle θ and an axis in space ω ∈ R 3 where θ determines the amount of rotation about ω. Unlike quaternions the axis-angle, requires only 3 parameters θω to describe a rotation. The axis angle representation does not suffer from gimbal lock and their singularities occur in a region of parameter space that can be easily avoided. For a more detailed description we refer the reader to <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Exponential Formula</head><p>Every rotation R can be written in exponential form in terms of the axis of rotation ω ∈ R 3 , s.t. ω = 1 and the angle of rotation θ as R = exp(θ ω)</p><p>where ω ∈ so(3) is the skew symmetric matrix constructed from ω. The elements of so(3) are skew symmetric matrices i.e., matrices that verify {A ∈ R 3×3 |A = -A T }. Given the vector θω = θ[ω 1 , ω 2 , ω 3 ] T the skew symmetric matrix is constructed with the wedge operator ∧ as follows:</p><formula xml:id="formula_1">θ ω = θ   0 -ω 3 ω 2 ω 3 0 -ω 1 -ω 2 ω 1 0  <label>(2)</label></formula><p>By definition, the multiplication of the matrix ω with a point p is equivalent to the cross-product of the vector ω with the point. Hence, the tangential direction of a rotating point is obtained as ṗ(t) = ω × p(t) = ωp(t), which is a differential equation that can be integrated to obtain the the exponential formula in Eq. (1).</p><p>The exponential map of a matrix A ∈ R 3×3 is analogous to the exponential used for real numbers a ∈ R. In particular the Taylor expansion of the exponential has the same form:</p><formula xml:id="formula_2">exp (θ ω) = e (θ ω) = I + θ ω + θ 2 2! ω 2 + θ 3 3! ω 3 + . . .<label>(3)</label></formula><p>Exploiting the fact that (θ ω) is screw symmetric, we can easily compute the exponential of the matrix ω in closed form using the Rodriguez formula:</p><formula xml:id="formula_3">exp(θ ω) = I + ω sin(θ) + ω 2 (1 -cos(θ))<label>(4)</label></formula><p>where only the square of the matrix ω and sine and cosine of real numbers have to be computed. Note that this formula allows us to reconstruct the rotation matrix from the angle θ and the axis of rotation ω by simple operations and this is probably the main justification of using the axis-angle representation at all. The exponential map formulation can be extended to represent rigid body motions, namely any motion composed by a rotation R and a translation t. This is done by extending the parameters θω with θv ∈ R 3 which is related to the translation along the axis of rotation and the location of the axis. These six parameters form the twist coordinates θξ = θ(v 1 , v 2 , v 3 , ω 1 , ω 2 , ω 3 ) of a twist. Analogous to Eq. ( <ref type="formula" target="#formula_0">1</ref>), any rigid motion G ∈ R 4×4 can be written in exponential form as:</p><formula xml:id="formula_4">G(θ, ω) = R 3×3 t 3×1 0 1×3 1 = exp(θ ξ)<label>(5)</label></formula><p>where the 4 × 4 matrix θ ξ ∈ se(3) is the twist action and is a generalization of the screw symmetric matrix θ ω of Eq. ( <ref type="formula" target="#formula_1">2</ref>). The twist action is constructed from the twist coordinates θξ ∈ R 6 using the wedge operator</p><formula xml:id="formula_5">∧ [θξ] ∧ = θ ξ = θ     0 -ω 3 ω 2 v 1 ω 3 0 -ω 1 v 2 -ω 2 ω 1 0 v 3 0 0 0 0    <label>(6)</label></formula><p>and its exponential can be computed using the following formula</p><formula xml:id="formula_6">exp(θ ξ) = exp(θ ω) (I -exp(θ ω))(ω × v + ωω T vθ) 0 1×3 1<label>(7)</label></formula><p>with exp(θ ω) computed by using the Rodriguez formula Eq. ( <ref type="formula" target="#formula_3">4</ref>) as explained before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pose Parameterization</head><p>The dynamics of the subject are modelled by a kinematic chain F, which describes the motion constraints of an articulated rigid body such as the human skeleton. A kinematic chain models the motion of a body segment as the motion of the previous body segment in the chain and an angular rotation around a joint axis. Specifically, the kinematic chain is defined with a 6 DoF (degree of freedom) root joint representing the global rigid body motion and a set of 1 DoF revolute joints describing the angular motion of the limbs. Higher DoF joints like hips or shoulders are represented by concatenating two or three 1 DoF revolute joints; for a comparison of balljoint parameterizations see <ref type="bibr" target="#b48">[49]</ref>. The root joint is expressed as a twist of the form θξ with the rotation axis orientation, location, and angle as free parameters. Revolute joints are expressed as special twists with no pitch of the from θ j ξ j with known ξ j (the location and orientation of the rotation axis as part of the model representation). Therefore, the full configuration of the kinematic chain is completely defined by a (6 + n) vector of free parameters</p><formula xml:id="formula_7">x := (θξ, θ 1 , . . . , θ n )<label>(8)</label></formula><p>similar to <ref type="bibr" target="#b49">[50]</ref>. Now, for a given point p ∈ R 3 on the kinematic chain, we define J (p) ⊆ {1, . . . , n} to be the ordered set that encodes the joint transformations influencing p. Let ps = p 1 be the homogeneous coordinate of p and denote P c () as the associated projection with P c (p) = p. Then, the transformation Fig. <ref type="figure">2</ref>: General tracking procedure of the video tracker, using silhouettes as image features. Multi-view silhouettes are obtained by background subtraction. In parallel, the mesh model is adapted to the current pose and projected to the respective camera views. One seeks for the pose parameters that best explain the image evidence.</p><p>of a point p using the kinematic chain F(x; p) and a parameter vector x is defined by</p><formula xml:id="formula_8">F(x; p) = P c G T B (x)p s (0) = P c     exp(θ ξ) j∈J (x) exp(θ j ξj )   ps (0)   .<label>(9)</label></formula><p>Here, F(x; p) : R 3 → R 3 is the function representing the total rigid body motion G T B (x) of the segment in the chain where p belongs. Eq. ( <ref type="formula" target="#formula_8">9</ref>) is commonly known as the product of exponentials formula <ref type="bibr" target="#b46">[47]</ref>, denoted as F(x; p).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">VIDEO-BASED TRACKER</head><p>The input of our video tracker consists of a rigid surface mesh of the actor obtained from a laser scanner and multi-view images obtained by a set of calibrated and sychronized RGB-cameras.</p><p>In order to relate the surface model to the human's images we find correspondences between the 3D surface vertices and the 2D image contours obtained with background subtraction, see Fig. <ref type="figure">2</ref>. We first collect 2D-2D correspondences by matching the projected surface silhouette with the background subtracted image contour. Thereby, we obtain a collection of 2D-3D correspondences since we know the 3D counterparts of the projected 2D points of the silhouette. In the experiments we only use the silhouettes as image features. We then minimize the distance e i between the transformed 3D points F(x; p i (0)) of the model and the projection rays defined by the 2D contour points p i and the respective camera center. This gives us a point-to-line constraint for each correspondence. Defining L i = (n i , m i ) as the 3D Plücker line with unit direction n i and moment m i of the corresponding 2D point r i = [x i , y i ], the point to line distance residual e i ∈ R 3 can be expressed as</p><formula xml:id="formula_9">e i = F(x; p i ) × n i -m i . (<label>10</label></formula><formula xml:id="formula_10">)</formula><p>Similar to Bregler et al. <ref type="bibr" target="#b50">[51]</ref> we now linearize the equation by</p><formula xml:id="formula_11">using exp(θ ξ) = ∞ k=0 (θ ξ) k k! .</formula><p>With I as identity matrix, this results in</p><formula xml:id="formula_12">(I + ∆ξ + j∈J (x) ∆θ j ξ j )) p i (x)) × n i -m i = 0 . (11)</formula><p>where ξ j is the j-th twist in the chain transformed to the current pose configuration. Having N correspondences, the energy we minimize E video is the sum of squared point-to-line distances e i arg min</p><formula xml:id="formula_13">x E video (x) = N i=1 e i 2 (12) = N i=1 F(x; p i ) × n i -m i 2<label>(13)</label></formula><p>which can be locally optimized. After linearization, Eq. ( <ref type="formula" target="#formula_13">13</ref>) can be re-ordered into an equation of the form J video (x)∆x = e video . Collecting a set of such equations leads to an over-determined system of equations, which can be solved using numerical methods like the Householder algorithm. The pose parameters are then updated as x k+1 = x k + ∆x. The Rodriguez formula can be applied to reconstruct the group action g from the estimated twists θ j ξ j . Then, the 3D points can be transformed and the process is iterated until convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">IMUS</head><p>An Inertial Measurement Unit or IMU is an electronic device that measures orientation and acceleration, using a combination of accelerometers and gyroscopes, sometimes also magnetometers. IMUs are very appealing because they provide a direct 3D measurement in contrast to images where the 3D information needs to be hallucinated. Furthermore, IMUs do not suffer from occlusions/self-occlusions and are not restricted to a designated recording volume. However, they have some limitations: • Wearing many IMU is intrusive for the subject.</p><p>• IMUs suffer from drift in continuous operation. Sensor biases are constantly estimated and this is difficult, especially if the local magnetic field is distorted by ferromagnetic material in the surrounding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>No positional measurement is directly available from the IMU. One could in principle derive position from the acceleration measurements but we have found this to be numerically unstable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>The orientation data provided by IMUs suffers from lag.</p><p>That is due to the fact that orientation data is obtained as the output of a Kalman filter that integrates acceleration, magnetometer and gyroscope information together. This is lag is specially problematic during fast motions.</p><p>Hence, we introduce a hybrid tracker that fuses information coming from a small set of IMUs (we use 5) and information coming from video cameras to compensate for the drawbacks of each sensor type.</p><p>The IMU measurements are taken with respect to a global inertial coordinate frame F I , which is commonly defined by gravity and magnetic north direction. The video tracking coordinate frame F T is defined by a calibration cube placed in the recording volume and usually differs from the inertial frame. Therefore, in order to be able to integrate the orientation data from the inertial sensors into our tracking system, we must determine the rotational offset R T I : F I → F T between both coordinate systems, see Fig. <ref type="figure" target="#fig_1">3</ref>. Then, we can easily transform the IMU data according to</p><formula xml:id="formula_14">R T S (t) = R T I R IS (t) ,<label>(14)</label></formula><p>such that they define a map from the local sensor frame F S to the tracking frame F T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">HYBRID TRACKER</head><p>The input of our hybrid tracker is identical to the video tracker, but extended with global orientation measurements of the IMUs.</p><p>We define a joint energy E hybrid that measures the consistency between pose estimates with measurements coming from video and inertial sensors:</p><formula xml:id="formula_15">arg min x E hybrid (x) = E video (x) + λE sens (x)<label>(15)</label></formula><p>where E video (x) is the energy cost corresponding to the video measurements defined in Eq. 13 and λE sens (x) is the cost associated with the IMU orientation measurements. To have a balanced Eq. ( <ref type="formula">16</ref>) is then iteratively linearized and the step ∆x is found by solving the following linear system</p><formula xml:id="formula_16">J video (x) √ λ J sens (x) ∆x = e video (x) √ λ e sens (x) . (<label>17</label></formula><formula xml:id="formula_17">)</formula><p>The pose parameters are then updated as x (k) = x (k-1) + ∆x.</p><p>The term corresponding to the video data is explained in the previous section. The term for the inertial sensors is explained in Sec. 6.1. In Fig. <ref type="figure" target="#fig_2">4</ref> we summarize the main ingredients of the hybrid tracker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Geodesic Distance Minimization</head><p>In this section we explain how to integrate the orientation data into the video-based tracker described earlier. In particular, we derive the linearization of a cost function that accounts for orientation consistency. After linearization this can be integrated into a big linear system according to Eq. <ref type="bibr" target="#b16">(17)</ref>.</p><p>In order to relate the orientation data to the differential twist parameters x t of our model, we will compare the ground-truth orientations R T S (t) of each of the sensors with the estimated sensor orientations from the tracking procedure RT S (x t ), which we will denote as tracking orientation. For the sake of clarity we will drop the time subindex x t and just write RT S (x), and will consider an energy for a single sensor. We define the estimation error e sens in terms of the screw coordinates ω rel (x) ∈ R 3 of the relative rotation between tracking and ground-truth orientation </p><formula xml:id="formula_18">e sens (x) = ω rel (x) = log(R T S (t) RT S (x) -1 ),<label>(18)</label></formula><p>Note that E sens corresponds to the squared geodesic distance between R T S (t) and RT S (x).</p><p>We can linearize Eq. ( <ref type="formula" target="#formula_19">19</ref>) and reformulate our objective function in terms of an optimal pose variation ∆x arg min</p><formula xml:id="formula_20">∆x ω rel (x) + ∆ω rel (x) ∆x ∆x 2 . (<label>20</label></formula><formula xml:id="formula_21">)</formula><p>The expression ∆ωrel(x) ∆x maps an increment in parameter space to the equivalent screw of the associated rigid motion. It corresponds to the Jacobian J ori : R D → so(3) of the orientation forward kinematics map F : R D → SO(3). Since Eq. ( <ref type="formula" target="#formula_20">20</ref>) is essentially a least squares problem, the optimal step can be found by solving the following linear equations</p><formula xml:id="formula_22">J ori ∆x = -ω rel (x).<label>(21)</label></formula><p>Thus, our sensor estimation error Jacobian J sens is simply the Jacobian of the forward kinematic map J ori . We can now setup those equations for all orientation sensors and plug them into the linear system defined in Eq. ( <ref type="formula" target="#formula_16">17</ref>). However, we need to define the ground-truth orientations R T S (t) and the estimated sensor orientations RT S (x). Recall from Sec. 5 that the sensor orientation data is given as a rotation matrix R T S (t) : F S → F T defining the transformation from the local sensor frame F S to the global tracking frame F T . In order to derive an expression for RT S (x), we introduce the body frame F B (the local frame of a segment in the chain, e.g. the leg). As depicted in Fig. <ref type="figure" target="#fig_3">5</ref>, the tracking orientation can be constructed by the concatenated transformation R BS (t) and R T B (x), i.e. RT S (x) = R T B (x)R BS . The first transformation defines the mapping from the sensor frame to the body frame R BS (t) :</p><formula xml:id="formula_23">F S → F B .</formula><p>The second term describes the total accumulated motion of a body segment at time t, i.e., R(x) : F B → F T . As the sensor is rigidly attached to the body segment, this mapping remains constant during tracking and we can compute it in the first frame</p><formula xml:id="formula_24">R BS = R T B (0) -1 R T S (0),<label>(22)</label></formula><p>where R T B (0) is the configuration of the body part B in the first frame where the sensor is attached.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Chordal Distance Minimization</head><p>The previous derivation allows us to integrate linearized equations associated to orientation consistency to the equations corresponding to the cost associated with the image evidence. It is however interesting to derive alternative equations from a more geometric point of view. Consider the local infinitesimal rotation R B (∆x) of frame F B , see Fig. <ref type="figure" target="#fig_4">6</ref>. R B (∆x) is defined in the body frame and represents the transformation from F B (x + ∆x) to F B (x). The tracking orientation RT S is now given by the longer path in Fig. <ref type="figure" target="#fig_4">6</ref>,</p><formula xml:id="formula_25">F S R BS =⇒ F B (x + ∆x) R B =⇒ F B x+∆x R T B (x)</formula><p>=⇒ F T . In We can make this transformation matrix match the ground-truth orientation R T S by minimizing the geodesic distance between them. This leads to exactly the same set of equations as in Eq. 21. However, in <ref type="bibr" target="#b22">[23]</ref> the squared chordal distance was minimised:</p><formula xml:id="formula_26">arg min ∆x R T B (x)R B (∆x)R BS -R T S (t) 2 F . (<label>23</label></formula><formula xml:id="formula_27">)</formula><p>The rotation R B (x) defined in the body frame is related to the rotation R(x) defined in the tracking frame by the adjoint transformation Ad R -1 (x) ,</p><formula xml:id="formula_28">R B (∆x) = R T B (x) -1 R(∆x)R T B (x) .<label>(24)</label></formula><p>Substituting R B (x) by its expression in <ref type="bibr" target="#b23">(24)</ref> it simplifies to</p><formula xml:id="formula_29">arg min ∆x R(∆x)R T B (x)R BS -R T S (t) 2 F .<label>(25)</label></formula><p>In <ref type="bibr" target="#b22">[23]</ref>, we have shown how to linearize Eq. ( <ref type="formula" target="#formula_29">25</ref>) and integrated it into the linear system defined in Eq. ( <ref type="formula" target="#formula_16">17</ref>). Nonetheless, it is interesting to take a closer look at the left term of Eq. ( <ref type="formula" target="#formula_29">25</ref>). Substituting the rotational displacement R BS in Eq. ( <ref type="formula" target="#formula_29">25</ref>) by its expression in Eq. ( <ref type="formula" target="#formula_24">22</ref>) R T B (0) -1 R T S (0), and writing</p><formula xml:id="formula_30">R T B (x) = 1 j=t-1 R(j)R T B (0) in terms of instantaneous ro- tations we obtain R(∆x)( 1 j=t-1 R(j))R T S (0) .<label>(26)</label></formula><p>This last equation has a very nice interpretation because the columns of the matrix</p><formula xml:id="formula_31">1 j=t-1</formula><p>R(j))R T S (0) are simply the coordinates of the sensor axis in the first frame (columns of R T S (0)), rotated by the accumulated tracking motion from the first frame forward (i.e. not including the initialization motion in frame 0). This last result was very much expected and the interpretation is the following: if we have our rotation matrices defined in a reference frame F T , we can just take the sensor axes in global coordinates in the first frame (columns of R T S (0)) and rotate them at every frame by the instantaneous rotational motions of the tracking. This will result in the estimated sensor axes in world coordinates, which is exactly the tracking orientation defined earlier in this section. Therefore, the problem can be simplified to additional 3D-vector to 3D-vector equations which can be very conveniently integrated in our twist formulation. Being rT S 1 (x), rT S 2 (x), rT S 3 (x) the tracking orientation basis axes at configuration x, and x(t), y(t), z(t) ground-truth orientation basis axes in the current frame t, the constraint equations are</p><formula xml:id="formula_32">R(∆x)r T S i (x) = r T S i , i = 1 . . . 3<label>(27)</label></formula><p>which can be linearized similarly as we did in the video-based tracker with image points to mesh points correspondences (2Dpoint to 3D-point). The difference now is that since we rotate vectors, only the rotational component of the twists is needed. Each additional sensor results in an additional nine equations in the linear system</p><formula xml:id="formula_33">  I + ∆ ω + j∈J (x) ∆θ j ω j   rT S i (x) = r T S i (t), i = 1 . . . 3<label>(28</label></formula><p>) which depends only on θ j ω j . The last equations can more conveniently be expressed in matrix form as J vec (x; rT S )∆x = e vec,i for i = {1 . . . 3} . Here J vec : R D → R 3 has almost the same structure as the positional pose Jacobian J video of the video tracker except that it does not depend on the translational motion nor the location of the joints. This implies that we can integrate the sensor information into the tracking system independently of the initial sensor orientation and location at the body limb. Note that J vec takes a vector r as input as opposed to a point in J video . Also, note the difference between J vec which are the derivatives of a rotating vector r and is therefore local, and J ori which maps to the tangential space so(3). Minimizing the chordal distance Eq. ( <ref type="formula" target="#formula_33">28</ref>) leads to nine equations for each sensor. In the previous section we minimized the geodesic distance, where the rotational error is defined by screw coordinates producing only three equations. Hence, there is a discrepancy of six equations. According to Euler's rotation theorem an orientation R ∈ SO(3) can be expressed by a minimum of three real parameters. Thus, the geodesic error term operates on the minimal representation, while minimizing the Frobenius norm produces some additional dependent equations. Indeed, the three 3D-vector to 3D-vector correspondences are related to the respective coordinate axes, meaning they have to be orthogonal. This relationship is covered by the six dependent equations. In the end, both methods minimize a distance metric of a relative rotation and lead to equivalent results but we minimize geodesic distance because it is more compact and efficient.</p><p>To conclude, we have derived the linearized equations for orientation consistency in terms of the geodesic distance (Sec. 6.1) and motivated the sensor integration from a more geometrical point of view within this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">EXPERIMENTS</head><p>In this section we evaluate our sensor fusion approach by comparing the video-based tracker with our proposed hybrid tracker. Learning-based stabilization methods or joint angle limits can also be integrated into the video-based tracker. However, we did not include further constraints to clearly demonstrate the influence of incorporating inertial data. For our experimental evaluation we need inertial sensor data, which is missing in publicly available benchmarks for videobased trackers (e.g. HumanEva <ref type="bibr" target="#b24">[25]</ref>, Human3.6M <ref type="bibr" target="#b51">[52]</ref>). In the preliminary work of this paper <ref type="bibr" target="#b22">[23]</ref>, the MPI08 dataset <ref type="bibr" target="#b52">[53]</ref> was used to evaluate the hybrid trackers performance. This dataset provides inertial data of 5 IMUs along with video data. In order to expand our experimental evaluation and provide enhanced error metrics, we have recorded a new dataset, TNT15 <ref type="bibr" target="#b23">[24]</ref>, which includes data of 10 IMUs and 8 synchronized RGB-cameras. Similar to <ref type="bibr" target="#b45">[46]</ref>, 5 IMU sensors were used for tracking and the residual 5 sensors were utilized for an independent validation measure. Additionally, we refrained from using a monochrome background cover as in <ref type="bibr" target="#b52">[53]</ref> and recorded in a normal office room situation. This generates noisier silhouettes, as it becomes more difficult to clearly separate foreground from background. For the video-based tracker, noisy silhouettes are very demanding, since the local optimization scheme gets stuck in local minima more often. However, this semi-controlled scenarios where we think incorporating sparse inertial sensor data is ideal to enable highquality, marker-less motion tracking with a fast, local optimization scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.1">TNT15 Dataset</head><p>The TNT15 dataset consists of synchronized data streams from 8 RGB-cameras and 10 IMUs. Four subjects perform five activities, namely walking, running on the spot, rotating arms, jumping and punching. The walking sequence consists of simple locomotion along a path with a 180 • turn on the spot. In running on the spot the actors were asked to run on the spot at three different velocities. More complex motions are executed in the residual sequences. The rotating arms sequence contains forward, backward, synchronized and unsynchronized arm rotations, while jumping covers jumping jacks and skiing exercises. The punching sequence includes some dynamic boxing motions. In total, the dataset contains more than 4:30 minutes of video data, which amounts to almost 13 thousand frames at a frame rate of 50 Hz. Multi-view video data was captured by a set of 8 synchronized RGB-video cameras at a resolution of 800×600 px. In order to generate silhouettes we used a background subtraction method based on a pixel-wise Gaussian model, similar to <ref type="bibr" target="#b53">[54]</ref>. The orientation data was recorded by 10 IMUs, which have been strapped to shanks, thighs, upper arms, lower arms, waist and Fig. <ref type="figure">7</ref>: Sensor placement: 10 sensors are strapped to body extremities (shank, thigh, forearm, upper arm), chest and waist. chest. Fig. <ref type="figure">7</ref> illustrates the sensor placement. As stated above, the set of sensors is divided into tracking and validation sensors. Sensors at shanks, lower arms and waist have been selected for tracking, while the orientation measurements at thigh, upper arms and chest are utilized for validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.2">Technical and Recording Setup</head><p>We have used the wireless MTw system provided by XSens <ref type="bibr" target="#b54">[55]</ref> to capture inertial sensor data. The MTw system consists of a receiver and multiple MTw motion trackers. Each MTw motion tracker contains a three-axis gyroscope, accelerometer and magnetometer and has the dimensions 34.5x57.8x14.5mm at a weight of 27g. The sensory output is transmitted to the receiver and then fused using a proprietary algorithm to provide a 3D orientation. Orientation accuracy is specified to be smaller than 1 • with an angular resolution of 0.05 •1 . In our experiments we are using 10 MTw units and record at a frame-rate of 50 Hz. The MTw units provide orientation data relative to a static global inertial frame F I , which is computed internally in each of the sensor units at the initial static position. It is defined as follows: the Z-axis is the negative direction of gravity measured by the internal accelerometer. The X-axis is the direction of the magnetic north pole measured by the magnetometer. Finally, the Y -axis is defined by the cross product Z × X. For each sensor the absolute orientation data is provided by a stream of quaternions that define, at every frame, the map or coordinate transformation from the local sensor coordinate system to the global one R IS (t) : F S ⇒ F I , see Sec. 5. In order to integrate the sensor orientation measurements in our tracking system, we have to determine the mapping between the inertial and tracking coordinate systems. Since the Y -axis of the calibration cube for the tracking frame is perpendicular to the ground, the Yaxis of the tracking frame and the Z-axis of the inertial frame are aligned. Therefore, R T I is a one parametric planar rotation that can be estimated beforehand using a calibration sequence <ref type="bibr" target="#b55">[56]</ref>. This calibration step can be avoided if the tracking frame coincides with the inertial frame, which is easily achieved by aligning the sensors with the tracking frame and performing a heading reset. This action basically rotates the inertial frame such that its X-axis is adjusted to the MTw units X-axis. To synchronize the cameras with the IMU measurements, the actors were asked to perform 1. Specifications provided by the manufacturer a foot stamp at the beginning of every sequence which is easily detected in the camera images and IMU acceleration data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.3">Methodology</head><p>In order to evaluate our hybrid tracker performance we consider two frame-wise error metrics. First we investigate the angular error d ang of our five validation IMUs w.r.t the corresponding bone orientations. We define d ang as the geodesic distance between the ground-truth and tracking orientations, R T S and RT S , according to</p><formula xml:id="formula_34">d ang (R T S , RT S ) = log(R T S ( RT S ) -1 ) . (<label>29</label></formula><formula xml:id="formula_35">)</formula><p>However, as the validation sensor error only measures orientation consistency, it is not sensitive to erroneous limb positions. Thus, we consider a second error metric and based on silhouette overlap between our projected model estimate and the image silhouette. It measures how well the estimate explains the video observations. Specifically, we define d xor as the ratio of pixels in the XORed image to the number of pixels in the disjunct image</p><formula xml:id="formula_36">d xor (S video , S model ) = 1 K K j=1 S video j ⊕ S model j S video j ∨ S model j ,<label>(30)</label></formula><p>where S video j and S model j are the binary silhouette images for every camera view j. It is defined in the range of [0, 1], i.e. a XOR error of d xor = 0 means the silhouettes are identical and d xor = 1 indicates no overlap at all. Our error metrics are different from the commonly used joint position error in motion capture experiments. If MoCap data is available one typically evaluates the euclidean distances of virtual markers corresponding to joint positions and the estimated joint positions of the motion tracker. However, this defines the state of a bone by two points in space, thus a rotational degree of freedom is not captured. Ideally, a metric for evaluating the human body pose should consider both joint position and joint orientation, i.e. the rigid motion of each bone of the human skeleton. MoCap data is not available for our experiments, thus we alternatively evaluate the estimated pose by measuring the bone orientations corresponding to the validation IMUs and the silhouette overlap error. Several experiments were carried out to investigate the tracker's performance. For every experiment we carefully analyse both error terms, the angular error d ang and XOR error d xor , and define the total tracking error as their concurrent combination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Tracking Error Analysis</head><p>In this section we investigate the tracking error of the video and hybrid tracker for a fixed parameter setting. The hybrid tracker weighting parameter λ was set to 1.0, which implies equal weighting of silhouette and sensor terms of the objective function, see Eq. <ref type="bibr" target="#b14">(15)</ref>. First, we present the outcome of two exemplary sequences and then show results for the complete database.</p><p>Fig. <ref type="figure" target="#fig_5">8</ref> shows the frame-wise tracking error for a walking sequence. The lower graph shows the orientation error curve, containing the average d ang for all validation sensors. For the hybrid tracker, the orientation error stays below 20 • for the whole sequence. In contrast, the video tracker shows some large deviations from ground-truth between frames 160 and 370. A manual inspection revealed that the right upper arm was partially flipped about 180 degrees, see Fig. <ref type="figure" target="#fig_0">1</ref> Fig. <ref type="figure">9</ref>: Frame-wise XOR and orientation error for a dynamic punching sequence. The video tracker (red) struggles to track the complex motion and cannot recover from frame 210 on. The hybrid tracker (blue) performs better with respect to both error metrics.</p><p>of Fig. <ref type="figure" target="#fig_5">8</ref>. This gross error would also be almost imperceptible in standard joint error metrics used for example in HumanEva. This further demonstrates that the error metrics used in human pose estimation are incomplete and that orientation flips result in similar visual cues. Nevertheless, the hybrid tracker performs better even on this video based metric. Another example sequence is shown in Fig. <ref type="figure">9</ref>. It depicts the tracking error for a dynamic punching sequence. The angular error curve indicates that the video tracker struggles to track the complex motion which starts at frame 80.</p><p>From frame 210 on, the average orientation error increases to approximately 60 • and remains in this region for the rest of the sequence. The tracking failure is also visible in the XOR error  curve. The hybrid tracker in contrast performs better with respect to both error metrics.</p><p>In both example sequences the hybrid approach performed better and successfully resolved visual ambiguities, which caused the video tracker to get stuck in undesired local minima or loose track completely. To evaluate the performance of our hybrid tracker on more sequences, we computed the tracking error for all sequences of the data set. We denote the mean and standard deviation of the XOR error as µ xor and σ xor and the angular error µ ang and σ ang , respectively. As depicted in Tab. 1, the mean angular error got almost halved to 15.71 • . Additionally, the XOR error has been reduced from 0.209 to 0.192, which shows that the orientation cues at the extremities propagate up the skeleton resulting in better pose estimates. The improved tracking results are also supported by the respective standard deviations. We conclude this section with the remark, that the mean tracking errors might be higher as one expects. For the XOR error this is due to the imperfect silhouettes.</p><p>As we have recorded the data in a normal office room situation, background subtraction generates artifacts in the silhouettes, i.e holes in the foreground regions and background pixels, which have been incorrectly labeled as foreground. Such an artifact is visible in the left silhouette in Fig. <ref type="figure">2</ref>. In order to explore the main components of the mean angular error, we depict the respective terms for each validation sensor in Tab. 2. We immediately see that the validation sensors placed at the upper arms obtain a much higher error than the ones placed at thighs and chest. The reason for this imbalance is twofold. First, the data set contains very difficult arm motions, which are difficult to track. Second, given the tracking orientation at the lower arms, some joint angles are simply not observable. If we assume an extended elbow joint, a rotation of the lower arm can be caused by a rotation in the elbow or in the shoulder. Besides, the skeletal structure of the shoulder and arms is very complex and thus difficult to model accurately. However, in comparison to the video tracker, the hybrid tracker reduced the angular error of the upper arm validation sensors by approximately one half.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Tracking Error vs. Feature Weighting</head><p>In this section we investigate how the weighting parameter λ influences the tracking error. Several weighting factors have been tested and their tracking error statistics are summarized in Tab. 3. Additionally, the respective mean XOR errors and mean angular errors are visualized in Fig. <ref type="figure" target="#fig_6">10</ref>.</p><p>In comparison to the video tracker, even a small weighting parameter of λ = 0.1 improves the tracking and reduces the average XOR error by 0.04 and mean angular error by 7.32 • . In  terms of the XOR error, the best values are obtained for moderate weighting values between 0.5 and 2.0, reaching its minimum of 0.192 at λ = 1.0. For larger weighting factors the XOR error increases almost proportionally. Thus, forcing higher penalties on orientation deviations does not necessarily help to resolve visual ambiguities. The objective is altered in a way that the tracker finds a local minimum which might not be in accordance to the visual cues. This illustrates why it is important to consider both error metrics to really judge the tracker's performance.</p><p>The angular error shows a different behavior. For small weighting parameters, it drops from 30.17 • for the video tracker (λ = 0.0) to 16.57 • for the hybrid tracker with a weighting parameter λ = 0.5.</p><p>Then the angular error decreases at a slower rate to its minimum of 15.41 • at λ = 3.0. A further increase of λ results in slightly growing angular errors. At first this might be counter-intuitive, but we only penalize orientation deviations of the tracking sensors. A too large weight makes the tracker ignore the video cues producing un-plausible poses to match the sensor orientations. We defined the optimal weighting parameter to be the one that minimizes the XOR error, which happens for a weighting factor of λ = 1.0. This value has a slightly higher angular error compared to higher weighting factors, but we think the difference is reasonably small. In general, the value of the weighting parameter depends heavily on how well the IMU readings represent the truth limb orientations. Sensor readings are corrupted by noise and rigidly attaching them to the bones is simply not possible. Thus, the ideal weighting parameter varies depending on the motion to be tracked.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Tracking Error vs. Number of Views</head><p>So far we investigated how the trackers perform using 8 cameras and 5 IMUs. In this section, we compare tracking results for reduced sets of camera views. Using only a subset of camera views has the effect that ambiguities in the silhouettes increase and local TABLE 4: Tracking camera list for three experiments with a designated scenario. Cameras that have been used for tracking during the scenarios are marked with a X.</p><formula xml:id="formula_37">Camera ID 0 1 2 3 4 5 6 7 6 cameras X X X X X X 4 cameras X X X X 2 cameras X X</formula><p>minima in the video-based energy term become more prevalent.</p><p>Especially within those scenarios we expect adding inertial cues improves the tracking.</p><p>In order to compare the results to the previous experiments we compute the XOR error on the full set of available camera views and use only a subset for tracking. The weighting parameter λ of the hybrid tracker is fixed to 1.0 for all experiments in this section. In a first experiment we evaluate the tracking performance for three distinct camera setups with 6, 4 and 2 cameras. To distinguish the different setups, we denote the video and hybrid tracker as video xc and hybrid xc , where x will be replaced by the quantity of cameras used for tracking. Fig. <ref type="figure" target="#fig_7">11</ref> shows the rough camera placements around the recording volume, where each camera is denoted with a circle, filled with its associated ID. For the 6 camera setup we removed cameras with ID 0 and 4 from the set of available camera views. Excluding those cameras removes an entire viewing direction on the scene, as they are arranged at opposite positions. For the four camera setup we remove half of the cameras and used only cameras with ID 0, 1, 2 and 3 for tracking. This setup would have the advantage of requiring less space than the eight camera setup. We also evaluated the tracker performance using only two cameras for tracking. We have chosen camera 0 and 2 for this scenario, as they have orthogonal viewing directions to the scene, which provides most information for a stereo setup. See Tab. 4, for a summary of which cameras are used for tracking in the respective setups. In Tab. 5 the mean tracking errors and associated standard deviations are summarized for the respective camera setups. The tracking error of the video tracker deteriorates by reducing the number of available camera views.</p><p>In contrast, the hybrid tracker is more robust to escalating visual ambiguities and the tracking error increases at a far slower rate. Especially video 6c shows inferior tracking results, though only two cameras have been removed from the tracking subset. Compared to the full camera setup, the mean XOR error rises by 0.099 and the mean angular error almost doubles. Obviously, the missing camera views provide vital information to the video tracker. In fact, inferring limb positions and orientations orthogonal to the viewing direction of the missing cameras is hampered. As a result it gets more difficult to determine, whether limbs are oriented forward or away from the residual camera views. However, the  The previous camera setups were designed to investigate a certain scenario, such as a missing viewing direction or spatial restrictions for camera positioning. In order to investigate how the trackers perform with respect to the number of tracking cameras in general we now vary the tracking camera count. Thus, we incrementally reduce the number of tracking cameras and run the trackers for all camera permutations. We computed the tracking errors of this experiment for a single actor of the database, which adds up to evaluating 2460 recording sequences, each of it containing 621 frames on average. As can be seen in Fig. <ref type="figure" target="#fig_8">12</ref> the hybrid tracker clearly outperforms the video tracker. For both trackers the average XOR error and mean angular error increase by reducing the number of tracking cameras, but the average error terms of the hybrid tracker increase at a slower rate. The mean tracking error and standard deviations are summarized in Tab. 6. So far, the mean XOR error has been averaged over all camera views, independent of which cameras have been selected for tracking. As the XOR evaluation metric and objective functions of the trackers both operate on silhouette data, we carried out a leave-one-camera-out experiment to prove validity of the XOR metric. For this experiment we only consider the XOR error of the camera that has not been used for tracking and reevaluate the tracking results of the previous 7 tracking camera setup. We denote the leave-one-out XOR error as XOR loo in the following.  For the hybrid tracker the average XOR loo results in 0.207 and 0.222 for the video tracker. In comparison, the XOR error µ xor computed over all camera views is 0.197 and 0.210 respectively, see Tab. 6. Thus there is a minor offset in the values, but the relative difference for both trackers is almost identical. In fact, with respect to XOR loo the hybrid tracker performs even better than for µ xor , as the difference of mean values is slightly higher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Tracking Error vs. Sensor Lag</head><p>In this section we evaluate the robustness of the hybrid tracker to sensor lag. As we fuse sensor information of two independent measurement systems, namely video and IMUs, we desire and assume perfect synchronization of the measurement data. However, imperfect manual synchronization during post-processing, sampling rate jitter or time delays due to filtering might lead to asynchronous data streams. The latter refers to the Kalman-filtered orientation estimates of the IMUs, which might lag during high dynamical motions, see Sec. 5.</p><p>In order to evaluate how the hybrid tracker responds to asynchronous measurement data, we added constant time offsets to the orientation data streams of the tracking IMUs. This does not model possibly time-varying characteristics of measurement lag, but applies a constant worst-case delay on each frame. Our experiments comprise the tracking error, where IMU data is artificially delayed by 1, 2, 3, 5, 10 and 25 successive frames. At maximum this corresponds to a delay of 0.5s at 50 Hz sampling rate. For all experiments in this section we have used the full set of available camera views and a weighting parameter of λ = 1.0. As can be seen in Tab. 7 the mean XOR error and mean angular error increase if the orientation measurements of the tracking sensors are artificially delayed. However, up to a sensor lag of approximately 7 frames, the hybrid tracker performs better in both error metrics compared to the video tracker. Thus, even though every orientation measurement is delayed by 0.14s, the hybrid tracker is able to provide more accurate results than the video tracker.</p><p>For small delays such as 3 frames the hybrid tracker mean XOR error slightly increase by 0.006 and the mean angular error by 0.89 • . In general, time delays due to jitter, manual synchronization and filtering do not exceed multiple frames. Thus, the preceding experiment shows that the hybrid tracker is robust to moderate asynchronicity in the measurements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.6">Evaluation on HumanEva Dataset</head><p>In order to evaluate the hybrid tracker with respect to groundtruth motion capture data we use the HumanEva <ref type="bibr" target="#b24">[25]</ref> dataset. This dataset consists of multi-view video and ground-truth 3D body poses obtained from a commercial marker-based motion capture system. As IMU data are not available in HumanEva we use ground-truth body part orientations to generate virtual IMU data. The HumanEva dataset consists of four actors performing several activities. Every activity is recorded three times and the data is then divided into disjoint subsets for validation, train and test purposes. For many sequences, including all test subsets, MoCap data is withheld for evaluation purposes. We evaluate the hybrid and video tracker on the first trial of validation sequences of actors 1-3. Because no surface models are provided for those actors we have used the method of <ref type="bibr" target="#b56">[57]</ref> to estimate the subject shape from marker data alone. In particular, we used SMPL <ref type="bibr" target="#b57">[58]</ref> as our body model which is publicly available. For efficiency, tracking was performed with a skeleton with fewer DoF and pose blend-shapes set to zero. For all experiments on the dataset we proceed as follows. In order to initialize the surface model and align the virtual IMU readings, we use ground-truth body poses of the first frame. For every subsequent frame we estimate the body pose using silhouettes and virtual IMU readings only. Silhouettes are generated by a background subtraction procedure provided along the dataset. All 7 cameras are considered for tracking except for Subject 1, where the four monochrome cameras are excluded due to very poor segmentation results. For the hybrid tracker we use ground-truth orientations of lower arms, shanks and torso for tracking and equal weighting of orientation and visual cues. Different to the previous experiments we now evaluate the tracking performance with respect to the average joint position error, see Sec. 7.1.3. We use the method proposed within the HumanEva dataset and compute the sum of Euclidean distances of 15 virtual markers. In Fig. <ref type="figure" target="#fig_9">14</ref>, we show the joint position error for the jogging sequence of subject 2 obtained by the hybrid and video tracker. It clearly demonstrates the superior performance of the hybrid tracker. In Tab. 8 we show the average tracking results for all sequences that have been evaluated. For the hybrid tracker the mean joint position error is between 3.8 -5.2cm. The video tracker is not capable to track the motions properly as the silhouettes are too ambiguous and achieves a mean joint position error of 7 -13.8cm. Due to corrupted MoCap data, we excluded the Jog sequence of subject 3.</p><p>Plenty approaches have been evaluated on the HumanEva dataset in the literature. The majority report tracking errors with respect to the test sequences, which we could not use due to missing ground-truth motion capture data. Similar to our experiments, <ref type="bibr" target="#b58">[59]</ref> and <ref type="bibr" target="#b11">[12]</ref> also evaluated on the validation subsets and report an average joint position error of 5.9 -7.7cm and 1.9 -4.8cm for walking and jogging activities, respectively. <ref type="bibr" target="#b58">[59]</ref> applies a loosely-connected-parts body model and combines an image likelihood function based on silhouette and edge features with body part detectors and uses non-parametric belief propagation for inference. Within the same publication a tracking error of 6.6 to 7.0cm is reported for an algorithm based on an Annealed Particle Filter, using silhouette and edge features. <ref type="bibr" target="#b11">[12]</ref> achieves state-of-the-art results with a discriminative approach based on Twin Gaussian Processes with HoG features generated from the three color camera views only.</p><p>To conclude, we have shown that a simple approach based on local optimization and silhouette features achieves competitive tracking errors, when inertial orientation information is incorporated. Thus without using more image features such as edge and color information or adding physical constraints, a sparse set of IMUs can improve the tracking significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSIONS</head><p>In this paper, we presented an approach for stabilizing full-body marker-less human motion capturing using a small number of additional inertial sensors. Reconstructing a 3D pose from 2D video data suffers from inherent ambiguities. We showed that a hybrid approach combining information of multiple sensor types can resolve such ambiguities, significantly improving the tracking quality. In particular, our orientation-based approach could correct tracking errors arising from rotationally symmetric limbs and noisy visual cues. Using only a small number of inertial sensors fixed at outer extremities stabilized the tracking for the entire underlying kinematic chain. In contrast to the preliminary work <ref type="bibr" target="#b22">[23]</ref>, we provide additional derivations and details to integrate orientation data and present an extended evaluation. A thorough evaluation on both orientation and video error metrics have proven the superior performance of the hybrid approach. We have shown that we require less cameras compared to a pure video-based tracker and have evaluated the robustness against sensor lag. Experiments on HumanEva dataset show that even using very basic image features we achieve competitive results compared to approaches which rely on learning or expensive inference methods. Another conclusion from our experiments is that commonly used error metrics based only on joint errors are incomplete to asses human pose estimation accuracy. To that end we make the TNT15 dataset including the 10 IMUs publicly available at <ref type="bibr" target="#b23">[24]</ref> so that other researchers can use it to validate their human pose estimation methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Tracking result for two selected frames. (a) Video-based tracker. (b) Our proposed hybrid tracker.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Global frames: tracking frame F T and inertial frame F I . Local frame: sensor frame F S .</figDesc><graphic coords="5,73.20,43.70,201.60,146.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Sketch of the hybrid tracker pipeline. Silhouette and orientation features are obtained from the inputs and their consistencies are combined in a hybrid energy term. We search for the model pose, which results in the minimal hybrid energy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Integration of orientation data into the video-based tracker. Ground-truth orientation: clockwise down path from F S at time t to F T . Tracking orientation: anti-clockwise upper path from F S at time t to F T .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Alternative interpretation of integrating orientation data into the video-based tracker. In comparison to Fig. 5 the tracking orientation path is extended by a local infinitesimal rotation R B (∆x) of the body frame F B .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 :</head><label>8</label><figDesc>Fig.8: Frame-wise XOR and orientation error for a walking sequence. The hybrid tracker (blue) performs well for the entire sequence. The video tracker (red) shows some large orientation errors between frames 160 and 370. Interestingly, this is almost invisible in the XOR error curve.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 10 :</head><label>10</label><figDesc>Fig. 10: Mean XOR error (red) and mean angular error (blue) for different weighting parameter λ of the hybrid tracker. The dashed lines show the respective error levels of the video tracker.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 11 :</head><label>11</label><figDesc>Fig. 11: Sketch of the camera setup. Eight cameras (IDs 0 to 7) are positioned around the recording volume.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 12 :</head><label>12</label><figDesc>Fig. 12: Average tracking error of the video tracker (red) and hybrid tracker (blue) for varying tracking camera counts. The mean values are marked with a x and the respective bars show one standard deviation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 14 :</head><label>14</label><figDesc>Fig.14: Joint position error for the Jog sequence of subject 2 in the HumanEva dataset. The hybrid tracker (blue) outperforms the video tracker (red). A manual inspection revealed that twisted legs causes the peak of the hybrid tracker around frame 220. The video tracker has problems tracking the legs for the entire sequence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 13 :</head><label>13</label><figDesc>Fig. 13: We show three exemplary frames of the TNT15 dataset and one of HumanEva (right most example). Each frame is is illustrated by two images. The images on the left depict the estimated model pose; ground-truth orientations are shown in solid lines and estimated orientations in dashed lines. The right images show the mesh projected to the respective RGB-image. For the TNT15 sequences we have used a lower resolution mesh for tracking, which is actually visible in the respective RGB-images.</figDesc><graphic coords="13,141.54,43.70,67.25,107.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1 :</head><label>1</label><figDesc>Mean tracking error values µ and standard deviations σ for video-based and hybrid tracker for all sequences of the database.</figDesc><table><row><cell>approach</cell><cell>µxor</cell><cell>σxor</cell><cell>µang[deg]</cell><cell>σang[deg]</cell></row><row><cell>video</cell><cell>0.209</cell><cell>0.056</cell><cell>30.17</cell><cell>42.38</cell></row><row><cell>hybrid</cell><cell>0.192</cell><cell>0.044</cell><cell>15.71</cell><cell>19.19</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2 :</head><label>2</label><figDesc>Mean angular error µ ang [deg] of the validation sensors attached to thighs, chest and upper arms for the video-based and hybrid tracker for all sequences of the database.</figDesc><table><row><cell></cell><cell cols="2">lThigh rThigh</cell><cell>chest</cell><cell cols="2">lUArm rUArm</cell></row><row><cell>video tracker</cell><cell>19.12</cell><cell>12.36</cell><cell>11.97</cell><cell>61.03</cell><cell>46.28</cell></row><row><cell>hybrid tracker</cell><cell>8.64</cell><cell>6.75</cell><cell>6.88</cell><cell>27.30</cell><cell>28.96</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 3 :</head><label>3</label><figDesc>Mean tracking error for varying weighting of sensor cues, computed over all sequences of the database.</figDesc><table><row><cell></cell><cell cols="2">approach</cell><cell></cell><cell>µxor</cell><cell>σxor</cell><cell>µang[deg]</cell><cell cols="2">σang[deg]</cell></row><row><cell></cell><cell cols="3">video (λ = 0.0)</cell><cell>0.209</cell><cell>0.056</cell><cell>30.17</cell><cell cols="2">42.38</cell></row><row><cell></cell><cell cols="3">hybrid (λ = 0.1)</cell><cell>0.205</cell><cell>0.052</cell><cell>22.85</cell><cell cols="2">32.54</cell></row><row><cell></cell><cell cols="3">hybrid (λ = 0.5)</cell><cell>0.197</cell><cell>0.047</cell><cell>16.57</cell><cell cols="2">21.23</cell></row><row><cell></cell><cell cols="3">hybrid (λ = 1.0)</cell><cell>0.192</cell><cell>0.044</cell><cell>15.71</cell><cell cols="2">19.19</cell></row><row><cell></cell><cell cols="3">hybrid (λ = 2.0)</cell><cell>0.196</cell><cell>0.046</cell><cell>15.46</cell><cell cols="2">19.53</cell></row><row><cell></cell><cell cols="3">hybrid (λ = 3.0)</cell><cell>0.202</cell><cell>0.047</cell><cell>15.41</cell><cell cols="2">19.54</cell></row><row><cell></cell><cell cols="3">hybrid (λ = 5.0)</cell><cell>0.211</cell><cell>0.051</cell><cell>15.98</cell><cell cols="2">21.50</cell></row><row><cell></cell><cell cols="4">hybrid (λ = 10.0) 0.223</cell><cell>0.057</cell><cell>16.42</cell><cell cols="2">22.15</cell></row><row><cell></cell><cell>0.23</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>30</cell></row><row><cell>d xor</cell><cell>0.2 0.21 0.22</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>20 25</cell><cell>d ang [deg]</cell></row><row><cell></cell><cell>0.19</cell><cell>0 0</cell><cell>2 2</cell><cell>4 4</cell><cell>6 6</cell><cell>8 8</cell><cell>10 10</cell><cell>15</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>λ</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 5 :</head><label>5</label><figDesc>Tracking error statistics for camera setups shown in Tab. 4.</figDesc><table><row><cell></cell><cell cols="2">approach</cell><cell>µxor</cell><cell>σxor</cell><cell>µang[deg]</cell><cell cols="2">σang[deg]</cell></row><row><cell></cell><cell cols="2">video 8c</cell><cell>0.209</cell><cell>0.056</cell><cell>30.17</cell><cell></cell><cell>42.38</cell></row><row><cell></cell><cell cols="2">hybrid 8c</cell><cell>0.192</cell><cell>0.044</cell><cell>15.71</cell><cell></cell><cell>19.19</cell></row><row><cell></cell><cell cols="2">video 6c</cell><cell>0.308</cell><cell>0.139</cell><cell>54.64</cell><cell></cell><cell>56.16</cell></row><row><cell></cell><cell cols="2">hybrid 6c</cell><cell>0.214</cell><cell>0.059</cell><cell>18.62</cell><cell></cell><cell>26.31</cell></row><row><cell></cell><cell cols="2">video 4c</cell><cell>0.269</cell><cell>0.103</cell><cell>41.58</cell><cell></cell><cell>49.14</cell></row><row><cell></cell><cell cols="2">hybrid 4c</cell><cell>0.219</cell><cell>0.057</cell><cell>17.56</cell><cell></cell><cell>22.70</cell></row><row><cell></cell><cell cols="2">video 2c</cell><cell>0.360</cell><cell>0.138</cell><cell>58.68</cell><cell></cell><cell>54.76</cell></row><row><cell></cell><cell cols="2">hybrid 2c</cell><cell>0.258</cell><cell>0.079</cell><cell>20.63</cell><cell></cell><cell>26.44</cell></row><row><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>xor</cell><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>d</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>7</cell><cell></cell><cell>6</cell><cell>5</cell><cell>4</cell><cell>3</cell><cell>2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2"># tracking cameras</cell><cell></cell></row><row><cell></cell><cell>135</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>d ang [deg]</cell><cell>45 90</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>7</cell><cell>6</cell><cell>5</cell><cell>4</cell><cell>3</cell><cell>2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2"># tracking cameras</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 6 :</head><label>6</label><figDesc>Average tracking error for varying number of camera views. The number of tracking cameras is varied from 7 to 2 and all N possible permutations have been considered.</figDesc><table><row><cell>approach</cell><cell>µxor</cell><cell>σxor</cell><cell>µang[deg]</cell><cell>σang[deg]</cell><cell>N</cell></row><row><cell>video 7c</cell><cell>0.210</cell><cell>0.045</cell><cell>33.12</cell><cell>45.10</cell><cell>8</cell></row><row><cell>hybrid 7c</cell><cell>0.197</cell><cell>0.040</cell><cell>16.41</cell><cell>17.99</cell><cell>8</cell></row><row><cell>video 6c</cell><cell>0.222</cell><cell>0.051</cell><cell>35.86</cell><cell cols="2">46.91 28</cell></row><row><cell>hybrid 6c</cell><cell>0.201</cell><cell>0.042</cell><cell>16.89</cell><cell cols="2">19.35 28</cell></row><row><cell>video 5c</cell><cell>0.239</cell><cell>0.061</cell><cell>40.26</cell><cell cols="2">49.46 56</cell></row><row><cell>hybrid 5c</cell><cell>0.209</cell><cell>0.046</cell><cell>17.57</cell><cell cols="2">21.06 56</cell></row><row><cell>video 4c</cell><cell>0.265</cell><cell>0.079</cell><cell>44.65</cell><cell cols="2">50.21 70</cell></row><row><cell>hybrid 4c</cell><cell>0.221</cell><cell>0.052</cell><cell>18.65</cell><cell cols="2">22.95 70</cell></row><row><cell>video 3c</cell><cell>0.318</cell><cell>0.111</cell><cell>55.68</cell><cell cols="2">53.91 56</cell></row><row><cell>hybrid 3c</cell><cell>0.244</cell><cell>0.066</cell><cell>20.95</cell><cell cols="2">27.00 56</cell></row><row><cell>video 2c</cell><cell>0.431</cell><cell>0.152</cell><cell>73.83</cell><cell cols="2">56.45 28</cell></row><row><cell>hybrid 2c</cell><cell>0.299</cell><cell>0.094</cell><cell>25.48</cell><cell cols="2">31.71 28</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 7 :</head><label>7</label><figDesc>Tracking error statistics for different levels of sensor lag. The orientation data of the IMUs used for tracking have been artificially delayed by constant time offsets of multiple frames.</figDesc><table><row><cell>offset [frames]</cell><cell>µxor</cell><cell>σxor</cell><cell>µang[deg]</cell><cell>σang[deg]</cell></row><row><cell>0</cell><cell>0.192</cell><cell>0.044</cell><cell>15.71</cell><cell>19.19</cell></row><row><cell>1</cell><cell>0.196</cell><cell>0.046</cell><cell>16.43</cell><cell>21.40</cell></row><row><cell>2</cell><cell>0.197</cell><cell>0.046</cell><cell>16.57</cell><cell>21.72</cell></row><row><cell>3</cell><cell>0.198</cell><cell>0.046</cell><cell>16.60</cell><cell>21.42</cell></row><row><cell>5</cell><cell>0.203</cell><cell>0.049</cell><cell>18.31</cell><cell>25.89</cell></row><row><cell>10</cell><cell>0.213</cell><cell>0.055</cell><cell>20.22</cell><cell>28.39</cell></row><row><cell>25</cell><cell>0.232</cell><cell>0.056</cell><cell>27.34</cell><cell>37.48</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 8 :</head><label>8</label><figDesc>Mean and standard deviations of the joint position error for validation subsets of the HumanEva dataset. .81 ± 6.41 4.19 ± 1.31 S1 Jog 11.97 ± 4.64 3.76 ± 1.57 S2 Walking 13.39 ± 2.88 4.88 ± 1.26 S2 Jog 9.21 ± 2.30 4.46 ± 1.66 S3 Walking 7.03 ± 2.56 5.15 ± 2.87</figDesc><table><row><cell>Actor</cell><cell>Action</cell><cell cols="2">3D error [cm]</cell></row><row><cell></cell><cell></cell><cell>Video</cell><cell>Hybrid</cell></row><row><cell>S1</cell><cell>Walking 13</cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The work is funded by the ERC-Starting Grant (DYNAMIC MINVIP). The authors gratefully acknowledge the support.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A survey of advances in visionbased human motion capture and analysis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Moeslund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Krüger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding (CVIU)</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="90" to="126" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Stabilizing motion tracking using retrieved motion priors</title>
		<author>
			<persName><forename type="first">A</forename><surname>Baak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Seidel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1428" to="1435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Detailed human shape and pose from images</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">O</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Haussecker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Implicit surface joint limits to constrain video-based motion capture</title>
		<author>
			<persName><forename type="first">L</forename><surname>Herda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Euroepan Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">3022</biblScope>
			<biblScope unit="page" from="405" to="418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Human tracking with mixtures of trees</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="690" to="695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Stochastic tracking of 3d human figures using 2d image motion</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sidenbladh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fleet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV), ser</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Berlin / Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">1843</biblScope>
			<biblScope unit="page" from="702" to="718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast pose estimation with parameter-sensitive hashing</title>
		<author>
			<persName><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="750" to="757" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">2D action recognition serves 3D human pose estimation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="425" to="438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Recovering 3D human pose from monocular images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="44" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Inferring 3D body pose from silhouettes using activity manifoldlearning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Elgammal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Discriminative density propagation for 3D human motion estimation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kanaujia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">390</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Twin Gaussian Processes for Structured Prediction</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="28" to="52" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012-09">September 2012</date>
		</imprint>
		<respStmt>
			<orgName>Romanian Academy and University of Bonn, Tech. Rep.</orgName>
		</respStmt>
	</monogr>
	<note>Institute of Mathematics of the</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Posebits for monocular human pose estimation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Columbus, Ohio, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06">Jun. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">3D people tracking with gaussian process dynamical models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Gaussian process dynamical models for human motion</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="283" to="298" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Physics-based person tracking using the anthropomorphic walker</title>
		<author>
			<persName><forename type="first">D</forename><surname>Brubaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="140" to="155" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Estimating human shape and pose from a single image</title>
		<author>
			<persName><forename type="first">P</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1381" to="1388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Physical simulation for probabilistic motion tracking</title>
		<author>
			<persName><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vondrak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Jenkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Performance capture from sparse multi-view video</title>
		<author>
			<persName><forename type="first">E</forename><surname>De Aguiar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stoll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2008">2008</date>
			<publisher>ACM</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Motion capture using joint skeleton tracking and surface estimation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stoll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>De Aguiar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Seidel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Inertial and magnetic sensing of human motion</title>
		<author>
			<persName><forename type="first">D</forename><surname>Roetenberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note type="report_type">These de doctorat</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multisensor-fusion for 3D full-body human motion capture</title>
		<author>
			<persName><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Baak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Helten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="663" to="670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Multimodal human motion database TNT15</title>
		<ptr target="http://www.tnt.uni-hannover.de/project/TNT15/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Humaneva: Synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion</title>
		<author>
			<persName><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="27" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Monocular 3d pose estimation and tracking by detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="623" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multiple Human Pose Estimation with Temporally Consistent 3D Pictorial Structures</title>
		<author>
			<persName><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision, ChaLearn Looking at People Workshop</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Optimization and filtering for human motion capture</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Seidel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="75" to="92" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A robust likelihood function for 3d human pose tracking</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Marker-less deformable mesh tracking for human shape and motion capture</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">De</forename><surname>Aguiar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stoll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">High-quality shape from multi-view stereo and shading under general illumination</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wilburn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Matsushita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="969" to="976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Human shape and pose tracking using keyframes</title>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Boyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3446" to="3453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Real-time human pose recognition in parts from single depth images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Finocchio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kipman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1297" to="1304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The vitruvian manifold: Inferring dense correspondences for one-shot human pose estimation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="103" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Metric regression forests for human pose estimation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Real time motion capture using a time-of-flight camera</title>
		<author>
			<persName><forename type="first">V</forename><surname>Ganapath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Plagemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><surname>Ganapathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Plagemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<title level="m">Real-time human pose tracking from range data</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A datadriven approach for real-time full body pose reconstruction from a depth camera</title>
		<author>
			<persName><forename type="first">A</forename><surname>Baak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bharaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Consumer Depth Cameras for Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="71" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Real-time body tracking with one depth camera and inertial sensors</title>
		<author>
			<persName><forename type="first">T</forename><surname>Helten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 IEEE International Conference on Computer Vision, ser. ICCV &apos;13</title>
		<meeting>the 2013 IEEE International Conference on Computer Vision, ser. ICCV &apos;13<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1105" to="1112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Detailed full-body reconstructions of moving people from monocular RGB-D sequences</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015-12">Dec. 2015</date>
			<biblScope unit="page" from="2300" to="2308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A survey of human motion analysis using depth imagery</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ferryman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn. Lett</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">15</biblScope>
			<date type="published" when="1995">1995-2006, Nov. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Estimation and visualization of sagittal kinematics of lower limbsorientation using body-fixed sensors</title>
		<author>
			<persName><forename type="first">H</forename><surname>Dejnabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Aminian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TBME</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1382" to="1393" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Integration of vision and inertial sensors for 3D arm motion tracking in home-based rehabilitation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Robotics Research (IJRR)</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">607</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Action capture with accelerometers</title>
		<author>
			<persName><forename type="first">R</forename><surname>Slyper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hodgins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH/Eurographics, SCA</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Accurate human motion capture in large areas by combining IMUand laser-based people tracking</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kretzschmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stachniss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Grisetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<meeting>of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="86" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Outdoor human motion capture using inverse kinematics and von mises-fisher sampling</title>
		<author>
			<persName><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Baak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2011-11">nov 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">A Mathematical Introduction to Robotic Manipulation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sastry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>CRC Press</publisher>
			<pubPlace>Baton Rouge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Model-based pose estimation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="139" to="170" />
		</imprint>
	</monogr>
	<note>in Visual Analysis of Humans: Looking at People</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Ball joints for marker-less human motion capture</title>
		<author>
			<persName><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on applications on Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Scaled motion dynamics for markerless motion capture</title>
		<author>
			<persName><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Twist based acquisition and tracking of animal and human kinematics</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Pullen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="179" to="194" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2014-07">jul 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Multimodal human motion database MPI08</title>
		<ptr target="http://www.tnt.uni-hannover.de/project/MPI08Database/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Pfinder: Real-time tracking of the human body</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Wren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Azarbayejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="780" to="785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Xsens Motion Technologies</title>
		<ptr target="http://www.xsens.com/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Analyzing and evaluating markerless motion tracking using inertial sensors</title>
		<author>
			<persName><forename type="first">A</forename><surname>Baak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Helten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Seidel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conjunction with ECCV, ser. Lecture Notes of Computer Science (LNCS)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">6553</biblScope>
			<biblScope unit="page" from="137" to="150" />
		</imprint>
	</monogr>
	<note>Proceedings of the 3rd International Workshop on Human Motion</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">MoSh: Motion and shape capture from sparse markers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGGRAPH Asia)</title>
		<meeting>SIGGRAPH Asia)</meeting>
		<imprint>
			<date type="published" when="2014-11">Nov. 2014</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">SMPL: A skinned multi-person linear model</title>
		<author>
			<persName><forename type="first">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graphics (Proc. SIGGRAPH Asia)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="248" />
			<date type="published" when="2015-10">Oct. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Loose-limbed people: Estimating 3d human pose and motion using non-parametric belief propagation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Haussecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="15" to="48" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
