<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Unknown from Correlations: Graph Neural Network for Inter-novel-protein Interaction Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-05-14">14 May 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Guofeng</forename><surname>Lv</surname></persName>
							<email>lvguofeng@sensetime.com</email>
							<affiliation key="aff0">
								<orgName type="department">Sensetim Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhiqiang</forename><surname>Hu</surname></persName>
							<email>huzhiqiang@sensetime.com</email>
							<affiliation key="aff0">
								<orgName type="department">Sensetim Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yanguang</forename><surname>Bi</surname></persName>
							<email>biyanguang@sensetime.com</email>
							<affiliation key="aff0">
								<orgName type="department">Sensetim Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
							<email>zhangshaoting@sensetime.com</email>
							<affiliation key="aff0">
								<orgName type="department">Sensetim Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Unknown from Correlations: Graph Neural Network for Inter-novel-protein Interaction Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-05-14">14 May 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2105.06709v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The study of multi-type Protein-Protein Interaction (PPI) is fundamental for understanding biological processes from a systematic perspective and revealing disease mechanisms. Existing methods suffer from significant performance degradation when tested in unseen dataset. In this paper, we investigate the problem and find that it is mainly attributed to the poor performance for inter-novel-protein interaction prediction. However, current evaluations overlook the inter-novel-protein interactions, and thus fail to give an instructive assessment. As a result, we propose to address the problem from both the evaluation and the methodology. Firstly, we design a new evaluation framework that fully respects the inter-novel-protein interactions and gives consistent assessment across datasets. Secondly, we argue that correlations between proteins must provide useful information for analysis of novel proteins, and based on this, we propose a graph neural network based method (GNN-PPI) for better inter-novel-protein interaction prediction. Experimental results on real-world datasets of different scales demonstrate that GNN-PPI significantly outperforms state-of-the-art PPI prediction methods, especially for the inter-novel-protein interaction prediction. (Code have been release. 1 </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Protein-protein Interactions (PPIs) play an important role in most biological processes. In addition to direct physical binding, PPI also has many other, indirect ways of cooperation and mutual regulation, such as exchange reaction products, participate in signal relay mechanisms, or jointly contribute toward specific organismal functions <ref type="bibr" target="#b7">[Szklarczyk et al., 2016]</ref>. It can be said that the study of PPIs and their interaction types are essential toward understanding cellular biological processes in normal and disease states, which in turn facilitate the therapeutic target identification and novel drug design <ref type="bibr" target="#b6">[Skrabanek et al., 2008]</ref>. There are many experimental methods to detect PPI, where the most conventional and widely used high-throughput methods are yeast two-hybrid screening <ref type="bibr">[Fields and Song, 1989</ref>]. However, the experimentbased methods are expensive and time-consuming, but more importantly, even if a single experiment has detected PPI, it cannot fully interpret its types <ref type="bibr" target="#b2">[De Las Rivas and Fontanillo, 2010]</ref>. Evidently, we urgently need reliable computational methods that are learned from the accumulated PPI data to predict the unknown PPIs accurately.</p><p>Despite long-term research works <ref type="bibr" target="#b3">[Guo et al., 2008;</ref><ref type="bibr" target="#b3">Hashemifar et al., 2018;</ref><ref type="bibr" target="#b1">Chen et al., 2019]</ref> make noticeable progress, existing methods suffer from significant performance degradation when tested on unseen dataset. Take the state-of-the-art model PIPR <ref type="bibr" target="#b1">[Chen et al., 2019]</ref> as an example, compared tested on trainset-homologous SHS148k testset with on a larger STRING testset, micro F1 score drops from 92.42 to 53.85. For further investigation, we divide the STRING testset into BS, ES and NS subsets, where BS denotes Both of the pair proteins in interaction were Seen during training, ES denotes Either (but not both) of the pair proteins was Seen, and NS denotes Neither proteins were Seen dur-ing training. As clearly shown in Figure <ref type="figure" target="#fig_0">1</ref>, poor performance in the ES and NS subsets (collectively termed as inter-novelprotein interactions in this paper) is the main reason for the performance degradation.</p><p>On the other hand, current evaluations on the trainsethomologous SHS148k testset apply a protein-irrepective perinteraction randomized strategy to divide the trainset and testset, and consequently, BS comprises over 92% of the whole testset and dominates the overall performance (see Supplementary Section 1 for more discussions). The evaluations overlook the inter-novel-protein interactions, and are thus not instructive for the performance when tested on other datasets. As a result, in this paper we firstly design a new evaluation framework with two per-protein randomized data partition strategies. Instead of simple protein-independent randomization, we take also into consideration the distance between proteins and utilize Breadth-First and Depth-First Search methods to construct the testset. Comparison experiments between the trainset-homologous testset and the unseen STRING testset demonstrates the proposed evaluation can give consistent assessment across datasets.</p><p>Besides the evaluation, for the methodology existing works take PPIs as independent instances. Correlations between proteins have long been ignored. Intuitively, for predicting the type of interaction between protein A and B, the interaction between protein A and C, as well as B and C must provide useful information. The correlations can be naturally modeled and excavated with a graph, where proteins serve as the nodes and interactions as the edges. In this paper, the graph is processed with a graph neural network based model (GNN-PPI). As demonstrated in Figure <ref type="figure" target="#fig_0">1</ref>, the introduction of correlations and the proposed GNN-PPI model have largely narrow the performance gap between BS, ES and NS subsets.</p><p>In summary, the contribution of this paper is three-fold:</p><p>1. We design a new evaluation framework that fully respects the inter-novel-protein interactions and give consistent assessment across datasets.</p><p>2. We propose to incorporate correlation between proteins into the PPI prediction problem. A graph neural network based method is presented to model the correlations.</p><p>3. The proposed GNN-PPI model achieves state-of-the-art performance in real-world datasets of different scales, especially for the inter-novel-protein interaction prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The primary amino acid sequences are confirmed to contain all the protein information <ref type="bibr" target="#b0">[Anfinsen, 1972]</ref> and are extremely easy to obtain. Thus, there is a longstanding interest in using sequence-based methods to model proteinrelated tasks. The research work of PPI prediction and classification can be summarized into two stages. The early research is based on Machine Learning (ML) <ref type="bibr" target="#b3">[Guo et al., 2008;</ref><ref type="bibr">Wong et al., 2015;</ref><ref type="bibr" target="#b6">Silberberg et al., 2014;</ref><ref type="bibr" target="#b6">Shen et al., 2007]</ref>. These methods provide feasible solutions, but their performance is limited by the PPI feature representation and model expressiveness. Deep Learning (DL) has recently been widely used in bioinformatics problems due to its powerful expressive ability, including PPI prediction and classification. These works <ref type="bibr" target="#b4">[Li et al., 2018;</ref><ref type="bibr" target="#b3">Hashemifar et al., 2018;</ref><ref type="bibr" target="#b1">Chen et al., 2019;</ref><ref type="bibr" target="#b3">Sun et al., 2017]</ref> typically use Convolution Neural Networks or Recurrent Neural Networks to extract features from the amino acid sequence of the protein.</p><p>More recent work has focused on the feature representation of proteins. <ref type="bibr" target="#b5">[Saha and others, 2020]</ref> proposes a novel deep multi-modal architecture, which extracts multi-modal information from protein structure and existing text information in biomedical literature. <ref type="bibr">[Nambiar et al., 2020]</ref> proposes a Transformer based neural network to generate proteins pretrained embedding. In the latest research, <ref type="bibr" target="#b2">[Yang et al., 2020]</ref> considers the correlation of PPIs and first proposed to use <ref type="bibr">GCN[Kipf and Welling, 2016]</ref> to learn protein features in the PPI network automatically. However, their work cannot be extended to the multi-label PPI classification.</p><p>To the best of our knowledge, the current work of PPI has not been concerned with the problems of inter-novel-protein interactions. However, In the field of Drug-drug Interaction (DDI), <ref type="bibr" target="#b2">[Deng et al., 2020]</ref> mentioned that the testset is divided according to whether the drug was seen during training, and the results show that the performance for the inter-noveldrug interactions is extremely degraded, but the original paper does not propose a solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>Suppose we have protein set P = {p 0 , p 1 , ..., p n } and PPI set X = {x ij = {p i , p j }|i = j, p i , p j ∈ P, I(x ij ) ∈ {0, 1}}. I is the PPI indicator function, if I(x ij ) = 1, then it means that protein p i interacts with protein p j . Note that when I(x ij ) = 0, it may mean that protein p i and p j will not interact, or they have a potential interaction while it has not been discovered so far. In order to avoid unnecessary errors, we will not do any operation on unknown protein pairs (default ∀x ij ∈ X , I(x ij ) = 1). We define PPI label space as L = {l 0 , l 1 , ..., l n } with n possible interaction types. For each PPI x ij , its labels is represented as y ij ⊆ L. In summary, the multi-type PPI dataset is defined as D = {(x ij , y ij )|x ij ∈ X }. Considering the correlation of PPIs, we use protein as nodes and PPIs as edges to build the PPI graph G = (P, X ).</p><p>The task of multi-type PPI learning is to learn a model F : x → ŷ from the training set X train . For any protein pair x ij ∈ X test , the model F predict ŷij as the set of proper labels for x ij . The above-mentioned X train and X test are obtained from X based on the evaluation, where X train + X test = X . Further, according to whether protein p was seen during training, the protein set P is divided into known P v = X train and unknown P u = P − P train . Moreover, as mentioned in section 1, X test can be divided into X BS , X ES , and X NS , which defined as follows:</p><formula xml:id="formula_0">X BS = {x ij |x ij ∈ X test , p i , p j ∈ P v } X ES = {x ij |x ij ∈ X test , p i ∈ P u , p j ∈ P v or p j ∈ P u , p i ∈ P v } X NS = {x ij |x ij ∈ X test , p i , p j ∈ P u } Labels PPI ….</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Build</head><p>The Graph Since inter-novel-protein interactions are the main bottlenecks, we require the testset X test of the evaluation framework to meet condition</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PPI Graph</head><formula xml:id="formula_1">|X BS | |X ES | + |X NS |.</formula><p>Our goal is that under this evaluation, the model F learned from X train can accurately predict the multi-label label of PPI in X test .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Overview</head><p>The GNN-PPI framework and evaluation are shown in Figure 2. We will introduce GNN-PPI from the following three aspects. First is Evaluation Framework. We propose two sets of heuristic data partition schemes based on the PPI network, and the generated testset meets the conditions</p><formula xml:id="formula_2">|X BS | |X ES | + |X NS |.</formula><p>Secondly, Protein feature encoding. We design Protein-Independent Encoding (PIE) and Protein-Graph Encoding (PGE) modules to encode protein features. The last is Multi-label PPI prediction. For unknown PPIs, we combine their protein feature encoded by the previous process, calculate their scores in different PPI types, and output its multi-label prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evaluation Framework</head><p>Generally, existing machine learning algorithms usually randomly divide part of the dataset as a testset to evaluate the performance of the model. However, in the PPI-related tasks, we have the following Corollary, derived from Erdős-Rényi(ER) random graph model. <ref type="bibr" target="#b3">[ERDdS and R&amp;wi, 1959;</ref><ref type="bibr">Erdős and Rényi, 1960]</ref>: Corollary 1. Randomly divide the PPI dataset, select t ≤ 0.2 as the testset, then most of the proteins in the testset were seen in training. The detailed proof of the corollary is elaborated in the Supplementary Section 1. It can be inferred from this corollary that the performance of the testset obtained by random division only reflects the predictive ability of the PPI between known P v . In the real world, there are still many proteins and their PPIs that have not been discovered. We perform empirical studies by comparison of two different time points, 2021/01/25 and 2020/04/11, of the Homo sapiens subset of BioGRID<ref type="foot" target="#foot_1">2</ref>  <ref type="bibr">[Stark et al., 2006]</ref> database. We found that the newly discovered proteins exhibit some BFS-like or DFS-like local patterns (More details in the Supplementary Section 5). Even if the PPIs have been discovered, most of their types remain relatively unexplored. Therefore, we need a brand-new evaluation that can reflect the model's predictive performance </p><formula xml:id="formula_3">X cur = {{p cur , p k }|p k ∈ N (p cur )} 8: X test = X test ∪ X cur 9: p cur = Search Next(G, p cur , S) 10: until |X test | ≥ N Trainset construction 11: X train = X − X test 12: return X train , X test</formula><p>on the inter-novel-protein interactions. The next content will introduce the evaluation framework we design.</p><p>We design two heuristic evaluation schemes based on the PPI network, namely BFS and DFS. They simulated two scenarios of unknown proteins P u in reality:</p><p>1. P u interact tightly with each other, and they exist in the form of clusters in the PPI network (See Figure <ref type="figure" target="#fig_1">3</ref> (b)).</p><p>2. P u are sparsely distributed in the PPI network and have little interaction with each other (See Figure <ref type="figure" target="#fig_1">3</ref> (c)).</p><p>We select a root node p root , fix the size of the testset N , and then use the Breadth-First Search (BFS) algorithm in the PPI network to obtain the proteins P u that meet the scenario 1.</p><p>All PPIs related to these proteins are the generated testset.</p><p>For scenario 2, we just need to simply randomly select proteins to form P u . However, in order to maintain </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Protein feature encoding</head><p>Previous work <ref type="bibr" target="#b1">[Chen et al., 2019]</ref> has proved that protein features based on the amino acid sequence are beneficial to the performance improvement of PPI-related tasks. Therefore, we design a Protein-Independent Encoding (PIE) module, which contains Conv1d with pooling, BiGRU, and fully connected (FC) layer, to generate protein feature representations as input to the PPI network.</p><p>The subsequent Protein-Graph Encoding (PGE) module is the core of GNN-PPI. Inspired by PPI network being widely used in bioinformatics computing, we construct PPI network G = (P, X ), and convert the original independent learning tasks F(x ij |p i , p j , θ) → ŷij into graph-related learning tasks F(x ij |G, θ) → ŷij . Recently, GNN is the most effective graph representation learning method, its main idea is the recursive neighborhood aggregation scheme, where each node computes a new feature by aggregating the previous features of its neighbor nodes. After k iterations, a node is represented by its transformed feature, which captures the structural information within the node's k-hop neighborhood. More specifically, the GNN of the k-th iteration is</p><formula xml:id="formula_4">a k p = Agg {g k−1 p |p ∈ N (p)} , g k p = Update {g k−1 p , a k p }</formula><p>where g k p is the feature of node p at the k-th iteration. The design of Agg(•) and Update(•) are the keys to different GNN architectures. In this paper we use Graph Isomorphism Network (GIN) <ref type="bibr" target="#b3">[Xu et al., 2018]</ref>, where the sum of the neighbor node features is used as the aggregation function, and multilayer perceptrons (MLPs) is used to update the aggregated features. Then, GIN updates node features as</p><formula xml:id="formula_5">g k p = MLP k (1 + k ) • g k−1 p + p ∈N (p) g k−1 p</formula><p>where can be a learnable parameter or a fixed scalar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Multi-label PPI Prediction</head><p>With the feature of protein learned from the previous stages for the PPI x ij , we use the dot product operation to combine the features of p i and p j , and then use a fully connected layer (FC) as classifier for multi-label PPI prediction, expressed as ŷij = FC(g pi • g pj ). The PIE and PGE modules are jointly training in an end-to-end way. Given a training set X train and its ground-truth multi-label interaction Y train , we can use the multi-task binary cross-entropy as the loss function:</p><formula xml:id="formula_6">L = n k=0 xij ∈Xtrain −y k ij log ŷk ij − (1 − y k ij ) log(1 − ŷk ij ) .</formula><p>Different from the algorithm that considers PPI independently, GNN-PPI learns to combine protein neighbors to generate feature representations. Therefore, for the X test constructed by our proposed BFS or DFS, GNN-PPI can also be based on its neighbors to generate suitable feature representations for multi-type PPI prediction. On the other hand, even if the PPI network G = (P v , X train ) used in the training process is constructed with only X train , it can perform well for unknown PPI x ij ∈ X test . (See details in Table <ref type="table" target="#tab_6">4</ref>)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>We use multi-type PPI data from the STRING database<ref type="foot" target="#foot_2">3</ref>  <ref type="bibr">[Szklarczyk et al., 2019]</ref> to evaluate our proposed GNN-PPI. The STRING database collected, scored, and integrated most publicly available sources of protein-protein interaction information and built a comprehensive and objective global PPI network, including direct (physical) and indirect (functional) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Details Experimental Settings and Metrics</head><p>We select 20% of PPIs for testing, using our proposed BFS, DFS, and original evaluation (Random). The BFS or DFS partition algorithm has completely different results for different root nodes. To simulate the realistic scene mentioned in Section 3.3, the root node's degree should not be too large. We set the root node degree threshold t = 5. To eliminate the influence of the randomness of data partitioning on the performance of PPI methods, we repeat experimental results under 3 different random seeds. We use the protein features based on amino acid sequence, refer to [Chen et al., 2019] using embedding method to represent each amino acid (Details in the Supplementary Section 3). We adopt Adam algorithm <ref type="bibr" target="#b3">[Kingma and Ba, 2014]</ref> to optimize all trainable parameters. The other hyper-parameters settings are shown in Supplementary Table <ref type="table" target="#tab_8">5</ref>. We evaluate the multi-label PPI prediction performance using micro-F1. This is because micro-averaging will emphasize the common labels in the dataset, which gives each sample the same importance. Since the different PPI types in the datasets we used are very imbalanced, micro-F1 may be preferred. Even so, we still evaluate the F1 performance of each PPI type, and the results are shown in Supplementary Table <ref type="table" target="#tab_9">6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baselines</head><p>We compare GNN-PPI against a variety of baselines, which can be categorized as follows:</p><p>1. Machine Learning based: We choose three representative machine learning (ML) algorithms, SVM <ref type="bibr" target="#b3">[Guo et al., 2008]</ref>, RF <ref type="bibr">[Wong et al., 2015]</ref>, and LR <ref type="bibr" target="#b6">[Silberberg et al., 2014]</ref>. The input feature of the algorithms uniformly selects common handcrafted protein features, AC <ref type="bibr" target="#b3">[Guo et al., 2008]</ref> and CTD <ref type="bibr" target="#b3">[Du et al., 2017]</ref>, of which CTD use seven attributes for the division (See in Supplementary Section 2).</p><p>2. Deep Learning based: We choose three representative deep learning (DL) algorithms in PPI prediction, PIPR <ref type="bibr" target="#b1">[Chen et al., 2019]</ref>, DNN-PPI <ref type="bibr" target="#b4">[Li et al., 2018]</ref> and DPPI <ref type="bibr" target="#b3">[Hashemifar et al., 2018]</ref>. We construct the same architecture as the original papers and modify the output of the original implementation from a binary class to multi-label. The protein input feature based on the amino acid sequence is consistent with GNN-PPI. The other settings are the same as the original papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results and Analysis</head><p>Table <ref type="table" target="#tab_3">1</ref> compares the performance of different methods under different evaluations and different datasets. Firstly, consider the impact of different evaluations, we can see that any method in Table <ref type="table" target="#tab_3">1</ref> perform well under Random partition. However, under BFS or DFS partition, except for GNN-PPI, the performance of other methods declines clearly. Moreover, the performance under the DFS is generally higher than that of the BFS, which means that the clustered distribution of unknown proteins in the PPI network is harder to learn than discrete distribution. Next, observe the performance on different datasets. Regardless of the evaluations, the performance of any method will improve as the data size increases. However, the problems mentioned above will not be trivially solved by increasing the amount of data. Finally, comparing different methods, we can see that DL-based methods are generally better than ML-based, and GNN-PPI can achieve state-ofthe-art performance. However, under the Random partition, the advantage of GNN-PPI over DL-based methods will be smaller as the dataset size increases. The most prominent advantage of GNN-PPI is that under the BFS or DFS partition, and for the inter-novel-protein interactions, it can still learn useful feature representations from protein neighbors so as to obtain good performance in multi-label PPI prediction. In summary, the experimental results show that GNN-PPI can effectively improve the prediction accuracy of internovel-protein interactions. However, how to further push the performance to be comparable as Random partition is still a problem worthy of further discussion, and it is also our future work.</p><p>We make a more in-depth analysis of performance between PIPR and GNN-PPI on X test , as shown in Table <ref type="table" target="#tab_4">2</ref>. Observing the proportions of different subsets of the testset, we can find that under Random partition, more than 92% test samples belong to X BS , which is consistent with our corollary 1. PIPR performs well on the randomly divided testset <ref type="bibr">(81.58 in SHS27k,</ref><ref type="bibr">92.42 in SHS148k,</ref><ref type="bibr">and 94.23 in STRING)</ref>, but if we further investigate the testset, we will find that PIPR performs very poorly for inter-novel-protein interactions (x ij ∈ X ES or X NS ), but it is dominated by X BS , which has accurate performance and a high proportion. According to the results of Table <ref type="table" target="#tab_4">1 and Table 2</ref>, with sufficient X ES and X NS data, we can assert that the methods which treats PPI as an independent sample (represented by PIPR), cannot accurately predict inter-novel-protein interactions. On the contrary, our proposed GNN-PPI can still perform well under BFS and DFS. Moreover, as the data size increases, the performance of .</p><p>Next, we study the ability of different evaluations to assess the model's generalization. We take the trained model's test performance on the larger dataset STRING as the model's true generalization ability. If the gap between the trainset-homologous test performance and the generalization is smaller, then the evaluation can better reflect the model's generalization. The experimental results are shown in Table 3. It can be seen that the previous evaluation (Random), whether it is for PIPR or GNN-PPI, the test performance on the STRING dataset has severely dropped. Like our speculation, it cannot reflect the generalization of the model. On the contrary, under the evaluation of BFS or DFS, its test performance can truly reflect the performance of the model, no matter it is good or bad (e.g. 66.13 vs. 63.74 in PIPR-SHS148k-BFS and 71.19 vs. 67.43 in GNN-PPI-SHS27k-DFS). In fact, the testset obtained by BFS or DFS is theoretically the same as the sample tested on STRING. The only difference is the proportion of different types of PPI (BS, ES and NS). Testing On the STRING, the proportion of NS is higher. Finally, we study the impact of the PPI network graph construction method (mentioned in 3.5) in the GNN-PPI. There are two graph construction methods, graph construct by all data (GCA, G = (P, X )) and graph construct by trainset (GCT, G = (P v , X train )). The experimental results are shown in Table <ref type="table" target="#tab_6">4</ref>. It can be seen that the performance of GCA all exceeds that of GCT, which is reasonable because the graph construction of GCA accesses more complete information than GCT. Compared with BFS, in the case of DFS, the performance of GCT is closer to GCA, which seems to indicate that the protein neighbors are more complete, the performance will be better. What is more noteworthy is that GCT is still much higher than non-graph algorithms, which shows the superiority of GNN in processing the few-shot learning for multi-label PPI prediction task. Moreover, for unknown proteins, we often cannot know their neighbors in advance. The effectiveness of GCT shows that the trained model is robust to newly discovered proteins and their interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we study the significant performance degradation of existing PPI methods when tested in unseen dataset. Experimental results show that this problem is due to the poor performance of the model for inter-novel-protein interactions.</p><p>However, current evaluation overlook the inter-novel-protein interactions, and are thus not instructive for the performance when tested on unseen datasets. Therefore, we design a new evaluation framework with two per-protein randomized data partition startegies, namely BFS and DFS, and propose a GNN based method GNN-PPI to model the correlations between PPIs. Our experimental results show that GNN-PPI outperforms state-of-the-art PPI prediction methods regardless of the evaluation is original or our proposed, especially for the inter-novel-protein interactions prediction. <ref type="bibr">[Szklarczyk et al., 2019]</ref>  1. In the G(n, M ) model, a graph is chosen uniformly at random from the collection of all graphs which have n nodes and M edges.</p><p>2. In the G(n, p) model, a graph is constructed by connecting nodes randomly. Each edge is included in the graph with probability p independent from every other edge.</p><p>The behavior of random graphs is often studied in the case where n, the number of nodes, tends to infinity. Although p and M can be fixed in this case, they can also be functions depending on n.</p><p>[Erdős and <ref type="bibr">Rényi, 1960]</ref> described the behavior of G(n, p) very precisely for various values of p when n tends to infinity. Their results include the following lemma:</p><formula xml:id="formula_7">Lemma 1. If p &gt; ln n</formula><p>n , then a graph in G(n, p) will almost surely be connected.</p><p>The expected number of edges in G(n, p) is C 2 n p, and by the law of large numbers any graph in G(n, p) will almost surely have approximately this many edges (provided the expected number of edges tends to infinity). Therefore, a rough heuristic is that if pn 2 → ∞ then G(n, p) should behave similarly to G(n, M ) with M = C 2 n p as n increases <ref type="bibr">[Erdős and Rényi, 1960]</ref>. Therefore, we can get the following lemma based on Lemma 1: Lemma 2. If M &gt; (n−1) ln n 2 , then a graph in G(n, M ) will almost surely be connected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Random Partition Strategy in the PPI Network</head><p>As mentioned in section 3.3 of the original paper, we propose a corollary as follows:</p><p>Corollary 2. Randomly divide the PPI dataset, select t ≤ 0.2 as the test set, then most of the proteins in the test set were seen in training.</p><p>The above corollary is equivalent to whether the training set protein includes most of the protein in the dataset. Review our problem formulation in the original paper: Given the Protein set P and PPI set X , where |P| = N, |X | = M . The PPI network is denoted as G = (P, X ) = (N, M ), and assume G is connected (The G used in the original paper are all connected). After using the random data partition strategy, if the connectivity of the training PPI network G train = (P, X train ) is large, then the corollary 2 will be proved. In the real-world PPI dataset, the number of proteins is not infinity. Therefore, we can roughly judge whether our Corollary 2 is correct based on Lemma 2. The experimental results are shown in  <ref type="bibr">et al., 1995]</ref> proposes to use these attributes to describe amino acids. The amino acids are divided into three classes according to attribute, and each amino acid is encoded by one of the indices 1,2, 3 according to which class it belongs. Table <ref type="table" target="#tab_9">6</ref> shows that amino acid attributes and corresponding division.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Pre-train amino acid embeddings</head><p>We use the embedding method to represent each amino acid a ∈ A as a vector. Each embedding vector is a concatenation of two sub-embedding, i. The skip-gram model is trained using negative sampling, where the vocabulary samples are overlapping 3-mer amino acids, and the word vector size is 5. The second part E 2 is a one-hot encoding based on the classification defined by the similarity of electrostaticity and hydrophobicity among amino acids, where 20 natural amino acids can be clustered into 7 classes <ref type="bibr" target="#b6">[Shen et al., 2007]</ref>, shown in Table <ref type="table">7</ref>. For the 21 st amino acid U(Selenocysteine), the 22 nd amino acid O(Pyrrolysine) and the unknown amino acids X are included in the eighth category. In summary, each amino acid is expressed as E(a) ∈ R 5+(7+1)=13 . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Real-world PPI network</head><p>We perform empirical studies by comparison of two different time points, 2021/01/25 and 2020/04/11, of the Homo sapiens subset of BioGRID database. Some qualitative results are shown in Figure <ref type="figure" target="#fig_3">4</ref>, where green and red nodes denote already and newly discovered proteins, respectively. It can be seen that proteins are not discovered randomly. Instead, the newly discovered proteins exhibit some BFS-like or DFS-like local patterns. This may justify that the proposed partitions are more realistic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Performance of different PPI types</head><p>We show separate results for the 7 labels in Table <ref type="table" target="#tab_12">10</ref>. Performance in "expression" is worse due to the limited samples (positive samples lower than 10%). Our model demonstrates consistent advantage over PIPR among all the 7 labels.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Proportions of Protein</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Results of PIPR (baseline) and GNN-PPI (ours) when trained on the smaller dataset SHS148k and tested on the larger STRING dataset. The metric is micro F1 score for multi-label PPI type prediction. Avg is the overall result of the testset. For further investigation, we divide the testset into BS, ES and NS subsets, where BS denotes Both of the pair proteins in interaction were Seen during training, ES denotes Either (but not both) of the pair proteins was Seen, and NS denotes Neither proteins were Seen during training. We regard ES and NS as inter-novel-protein interactions.</figDesc><graphic url="image-1.png" coords="1,337.29,216.00,198.43,144.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Examples of different testset construction strategies. Random is the current scheme, while Breath-First Search (BFS) and Depth-First Search (DFS) are the proposed schemes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>e. E(a) = [E 1 (a), E 2 (a)]. The first part E 1 measures the co-occurrence similarity of the amino acids, obtained by pre-training the Skip-Gram[Mikolov et al., 2013] model protein sequences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Comparison of two different time points of the Homo sapiens of BioGRID database, where green and red nodes denote already and newly discovered proteins, respectively.</figDesc><graphic url="image-2.png" coords="10,121.75,54.00,368.50,163.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Development and evaluation of the GNN-PPI framework. Pairwise interaction data are firstly assembled to build the graph, where proteins serve as the nodes and interactions as the edges. The testset is constructed by firstly selecting the root node and then performing the proposed BFS or DFS strategy. The model is developed by firstly performing embedding for each protein to obtain predefined features, then processed by Convolution, Pooling, BiGRU and FC modules to extract protein-independent encoding (PIE) features, which are finally aggregated by graph convolutions and arrive at protein-graph encoding (PGE) features. Features of the pair proteins in interaction are multiplied and classified, supervised by the trainset labels.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Root node</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Test</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Train</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Construct</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Trainset / Testset</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Trainset</cell></row><row><cell>Embedding</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Protein Neighbors Graph Convolution</cell><cell>Supervise</cell></row><row><cell></cell><cell>Conv1D</cell><cell>Pooling</cell><cell>BiGRU</cell><cell>FC</cell><cell></cell><cell>MUL</cell><cell>Classifier</cell></row><row><cell>….</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>….</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Agg Update</cell></row><row><cell>Predefined Feature</cell><cell cols="4">Protein-Independent Encoding</cell><cell>PIE Feature</cell><cell>Protein-Graph Encoding</cell><cell>PGE Feature</cell><cell>Prediction</cell></row><row><cell>Figure 2:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Algorithm 1 Data Partition Algorithm Input: Protein set P; PPI set X ; Testset size N ; Root node selection threshold t; Search order S ∈ {BFS, DFS}; Output: X train ; X test ; Randomly select a protein as root node p root . 4: until |N (p root )| &lt; t N returns the neighbors Testset construction 5: X test = ∅, p cur = p root 6: repeat</figDesc><table><row><cell>Root node selection</cell></row><row><cell>2: repeat</cell></row><row><cell>3:</cell></row><row><cell>7:</cell></row></table><note>1: Build PPI graph G = (P, X )</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Performance of GNN-PPI against comparative methods over different datasets and data partition schemes. The reported results are mean±std micro-averaged F1 score over three repeated experiments. Results of SVM on STRING is omitted for unafforable running time.interactions. In this paper, we focus on the multi-type classification of PPI by STRING. It divides PPI into 7 types, namely reaction, binding, post-translational modifications (ptmod), activation, inhibition, catalysis, and expression. Each pair of interacting proteins contains at least one of them.<ref type="bibr" target="#b1">[Chen et al., 2019]</ref> randomly select 1,690 and 5,189 proteins from the Homo sapiens subset of STRING that shares &lt; 40% of sequence identity to generate two subsets, namely SHS27k and SHS148k, which contain 7,624 and 44,488 multi-label PPIs. At the same time, we use all PPIs of Homo sapiens as our third dataset, namely STRING, which contains 15,335 proteins and 593,397 PPIs. We will use these three PPI datasets of different sizes to evaluate GNN-PPI and other PPI methods in the following content.</figDesc><table><row><cell>Dataset</cell><cell>Partition Scheme</cell><cell>SVM</cell><cell>RF</cell><cell>LR</cell><cell>Methods DPPI</cell><cell>DNN-PPI</cell><cell>PIPR</cell><cell>GNN-PPI</cell></row><row><cell></cell><cell cols="6">Random 75.35±1.05 78.45±0.88 71.55±0.93 73.99±5.04 77.89±4.97</cell><cell>83.31±0.75</cell><cell>87.91±0.39</cell></row><row><cell>SHS27k</cell><cell>BFS</cell><cell cols="5">42.98±6.15 37.67±1.57 43.06±5.05 41.43±0.56 48.90±7.24</cell><cell>44.48±4.44</cell><cell>63.81±1.79</cell></row><row><cell></cell><cell>DFS</cell><cell cols="5">53.07±5.16 35.55±2.22 48.51±1.87 46.12±3.02 54.34±1.30</cell><cell>57.80±3.24</cell><cell>74.72±5.26</cell></row><row><cell></cell><cell cols="6">Random 80.55±0.23 82.10±0.20 67.00±0.07 77.48±1.39 88.49±0.48</cell><cell>90.05±2.59</cell><cell>92.26±0.10</cell></row><row><cell>SHS148k</cell><cell>BFS</cell><cell cols="7">49.14±5.30 38.96±1.94 47.45±1.42 52.12±8.70 57.40±9.10 61.83±10.23 71.37±5.33</cell></row><row><cell></cell><cell>DFS</cell><cell cols="5">58.59±0.07 43.26±3.43 51.09±2.09 52.03±1.18 58.42±2.05</cell><cell>63.98±0.76</cell><cell>82.67±0.85</cell></row><row><cell></cell><cell>Random</cell><cell>-</cell><cell cols="4">88.91±0.08 67.74±0.16 94.85±0.13 83.08±0.11</cell><cell>94.43±0.10</cell><cell>95.43±0.10</cell></row><row><cell>STRING</cell><cell>BFS</cell><cell>-</cell><cell cols="4">55.31±1.02 50.54±2.00 56.68±1.04 53.05±0.82</cell><cell>55.65±1.60</cell><cell>78.37±5.40</cell></row><row><cell></cell><cell>DFS</cell><cell>-</cell><cell cols="4">70.80±0.45 61.28±0.53 66.82±0.29 64.94±0.93</cell><cell>67.45±0.34</cell><cell>91.07±0.58</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>In-depth analysis between PIPR and GNN-PPI over BS, ES and NS subsets.</figDesc><table><row><cell></cell><cell>Partition</cell><cell cols="2">XBS</cell><cell></cell><cell></cell><cell>XES</cell><cell></cell><cell>XNS</cell><cell></cell><cell></cell><cell>XAvg</cell></row><row><cell>Dataset</cell><cell>Scheme</cell><cell cols="11">PIPR GNN-PPI PIPR GNN-PPI PIPR GNN-PPI Proportion(BS/ES/NS) PIPR GNN-PPI</cell></row><row><cell></cell><cell cols="2">Random 83.12</cell><cell>88.31</cell><cell cols="2">64.48</cell><cell>74.28</cell><cell>35.29</cell><cell>33.33</cell><cell>92.2</cell><cell>7.5</cell><cell>0.3 81.58</cell><cell>87.11</cell></row><row><cell>SHS27k</cell><cell>BFS</cell><cell>-</cell><cell>-</cell><cell cols="2">44.92</cell><cell>68.08</cell><cell>30.34</cell><cell>46.25</cell><cell>0.0</cell><cell>72.6</cell><cell>27.4 40.92</cell><cell>62.10</cell></row><row><cell></cell><cell>DFS</cell><cell>-</cell><cell>-</cell><cell cols="2">58.25</cell><cell>72.22</cell><cell>48.77</cell><cell>63.22</cell><cell>0.0</cell><cell>88.6</cell><cell>11.4 57.17</cell><cell>71.19</cell></row><row><cell></cell><cell cols="2">Random 92.82</cell><cell>92.24</cell><cell cols="2">78.80</cell><cell>73.09</cell><cell>40.72</cell><cell>36.36</cell><cell>97.2</cell><cell>2.7</cell><cell>0.1 92.42</cell><cell>91.68</cell></row><row><cell>SHS148k</cell><cell>BFS</cell><cell>-</cell><cell>-</cell><cell cols="2">62.80</cell><cell>72.51</cell><cell>73.82</cell><cell>77.02</cell><cell>0.0</cell><cell>69.7</cell><cell>30.3 66.13</cell><cell>73.88</cell></row><row><cell></cell><cell>DFS</cell><cell>-</cell><cell>-</cell><cell cols="2">64.17</cell><cell>83.37</cell><cell>55.51</cell><cell>73.08</cell><cell>0.0</cell><cell>91.9</cell><cell>8.1 63.47</cell><cell>82.54</cell></row><row><cell></cell><cell cols="2">Random 94.32</cell><cell>95.42</cell><cell cols="2">61.65</cell><cell>77.68</cell><cell>33.33</cell><cell>57.14</cell><cell>99.7</cell><cell>0.3</cell><cell>0 94.23</cell><cell>95.37</cell></row><row><cell>STRING</cell><cell>BFS</cell><cell>-</cell><cell>-</cell><cell cols="2">56.71</cell><cell>83.99</cell><cell>39.87</cell><cell>72.83</cell><cell>0.0</cell><cell>85.8</cell><cell>14.2 54.31</cell><cell>82.41</cell></row><row><cell></cell><cell>DFS</cell><cell>-</cell><cell>-</cell><cell cols="2">68.61</cell><cell>90.38</cell><cell>55.22</cell><cell>87.07</cell><cell>0.0</cell><cell>94.3</cell><cell>5.7 67.84</cell><cell>90.19</cell></row><row><cell>Methods</cell><cell>Trainset</cell><cell>Testset</cell><cell cols="4">Partition Scheme Random BFS DFS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PIPR</cell><cell>SHS27k-Train SHS148k-Train</cell><cell cols="2">SHS27k-Test STRING SHS148k-Test STRING</cell><cell>81.58 42.79 92.42 53.85</cell><cell cols="2">40.92 57.17 48.55 57.44 66.13 63.47 63.74 62.46</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GNN-PPI</cell><cell>SHS27k-Train SHS148k-Train</cell><cell cols="2">SHS27k-Test STRING SHS148k-Test STRING</cell><cell>87.11 66.85 91.68 73.12</cell><cell cols="2">62.10 71.19 66.39 67.43 73.88 82.54 67.43 70.64</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Performance comparison of tested on trainset-homologous testset vs. unseen testset, under different evaluations (partition schemes).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Performance of GNN-PPI with different PPI Graph construction method.</figDesc><table><row><cell cols="2">Partition Graph Scheme</cell><cell>SHS27k</cell><cell>Dataset SHS148k</cell><cell>STRING</cell></row><row><cell>BFS</cell><cell>GCA GCT</cell><cell cols="3">63.81±1.79 71.37±5.33 78.37±5.40 60.61±5.32 69.56±6.89 73.23±3.93</cell></row><row><cell>DFS</cell><cell>GCA GCT</cell><cell cols="3">74.72±5.26 82.67±0.85 91.07±0.58 73.42±5.50 80.35±2.20 89.04±1.06</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 .</head><label>5</label><figDesc>No matter theoretical deductions(|X train | &gt; M ) or real test results(Proportion of X BS ), it shows our Corollary 2 is right. It is worth mentioning that regarding dataset SHS148k and STRING, why |X train |M but the proportion of X BS still does not reach the proportion of connected graphs, which is equal to 1. This is because there are many proteins in the PPI network, and they only interact with one protein.(Shown in the column deg(p) = 1 of Table5)</figDesc><table><row><cell>B Composition(C), Transition(T) and</cell></row><row><cell>Distribution(D)</cell></row></table><note>[Dubchak</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Seven attributes and the division of the amino acids.</figDesc><table><row><cell cols="3">No. Dipole scale Volume scale</cell><cell cols="2">Class</cell></row><row><cell>1</cell><cell>-</cell><cell>-</cell><cell cols="2">A, G, V</cell></row><row><cell>2</cell><cell>-</cell><cell>+</cell><cell cols="2">I, L, F, P</cell></row><row><cell>3</cell><cell>+</cell><cell>+</cell><cell cols="2">Y, M, T, S</cell></row><row><cell>4</cell><cell>++</cell><cell>+</cell><cell cols="2">H, N, E, W</cell></row><row><cell>5</cell><cell>+++</cell><cell>+</cell><cell cols="2">R, K</cell></row><row><cell>6</cell><cell>+'+'+'</cell><cell>+</cell><cell cols="2">D, E</cell></row><row><cell>7</cell><cell>+"</cell><cell>+</cell><cell>C</cell></row><row><cell cols="5">Table 7: Classification of amino acids. Dipole scale: -, Dipole&lt;1.0;</cell></row><row><cell cols="5">+, 1.0&lt;Dipole&lt;2.0; ++, 2.0&lt;Dipole&lt;3.0; +++, Dipole&gt;3.0;</cell></row><row><cell cols="5">+'+'+', Dipole&gt;3.0 with opposite orientation; +", Cys is separated</cell></row><row><cell cols="5">from class 3 because of its ability to form disulfide bonds. Volume</cell></row><row><cell cols="3">scale: -, Volume&lt;50; +, Volume&gt;50.</cell><cell></cell></row><row><cell cols="4">D Ablation Study of PIE and PGE</cell></row><row><cell cols="5">We perform ablation studies on the PIE and PGE components.</cell></row><row><cell cols="5">As shown in Table 8, both components are beneficial to the</cell></row><row><cell cols="2">overall performance.</cell><cell></cell><cell></cell></row><row><cell>PIE PGE</cell><cell>Random</cell><cell cols="2">Partition Schemes BFS</cell><cell>DFS</cell></row><row><cell></cell><cell cols="4">69.88±0.04 50.03±2.08 61.86±1.04</cell></row><row><cell></cell><cell cols="4">94.30±0.52 73.81±6.82 88.03±0.59</cell></row><row><cell></cell><cell cols="4">95.38±0.12 78.37±5.40 91.07±0.58</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Results of ablation studies on the PIE and PGE components.</figDesc><table><row><cell></cell><cell>Hyper-Parameters</cell><cell>Values</cell></row><row><cell></cell><cell>Fixed amino acid length</cell><cell>2000</cell></row><row><cell>Model</cell><cell>Protein-I Feature</cell><cell>256</cell></row><row><cell cols="2">Architecture Protein-G Feature</cell><cell>50</cell></row><row><cell></cell><cell>Graph layers</cell><cell>1</cell></row><row><cell></cell><cell>learning rate(lr)</cell><cell>0.001</cell></row><row><cell></cell><cell>lr reduce rate</cell><cell>0.5</cell></row><row><cell>Model</cell><cell>lr reduce patience</cell><cell>20</cell></row><row><cell>Training</cell><cell>l2 weight decay</cell><cell>5e-4</cell></row><row><cell></cell><cell>batch size</cell><cell>1024</cell></row><row><cell></cell><cell>epochs</cell><cell>300</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>The hyper-parameter settings for GNN-PPI.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 :</head><label>10</label><figDesc>39±0.12 97.62±0.07 55.96±7.59 83.19±4.01 68.09±1.19 93.28±1.44 Binding 67.87 95.63±0.23 96.43±0.07 71.34±0.36 83.80±3.70 81.79±1.90 94.06±0.51 Ptmod 6.92 86.94±0.26 87.28±0.27 25.91±13.8 70.48±7.44 17.09±5.31 82.12±1.40 Activation 17.53 86.31±0.31 87.96±0.37 39.17±13.3 66.20±15.5 27.28±10.0 81.58±0.94 Inhibition 6.58 90.36±0.34 91.49±0.14 12.08±7.55 65.58±12.4 19.16±0.94 82.62±2.80 Catalysis 48.59 96.28±0.19 97.58±0.08 58.84±6.39 83.94±4.53 66.24±4.32 92.79±0.51 Expression 2.07 39.06±1.42 32.55±1.53 0.81±1.27 15.67±10.8 1.04±1.80 23.22±9.21 Micro-Avg -94.43±0.10 95.43±0.10 55.65±1.60 78.37±5.40 67.45±0.34 91.07±0.58 Separate results in STRING dataset for the multi labels between PIPR and GNN-PPI over Random, BFS and DFS partition schemes.</figDesc><table><row><cell cols="2">Multi Labels Type Ratio (%)</cell><cell cols="2">Random Partition</cell><cell cols="2">BFS Partition</cell><cell>DFS Partition</cell></row><row><cell></cell><cell></cell><cell>PIPR</cell><cell>GNN-PPI</cell><cell>PIPR</cell><cell>GNN-PPI</cell><cell>PIPR</cell><cell>GNN-PPI</cell></row><row><cell>Reaction</cell><cell>51.08</cell><cell>96.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 11 :</head><label>11</label><figDesc>Table2in main text shows some quantitative analysis of the BFS and DFS partitions in terms of the proportions of BS, ES and NS edges. We also calculate the proportions of nodes in testset-only, both-sets and trainset-only as shown in Table11. Compared with conventional random partition, BFS and DFS partitions lead to more ES and NS edges, and less nodes in Both-sets, and thus better evaluate the performance of models for unseen proteins. Proportion of proteins in trainset and testset over different datasets and data partition schemes.</figDesc><table><row><cell>Datasets</cell><cell cols="4">Partition Schemes Trainset-only Testset-only Both-sets Protein Proportions (%)</cell></row><row><cell></cell><cell>Random</cell><cell>41.64</cell><cell>6.06</cell><cell>52.31</cell></row><row><cell>SHS27k</cell><cell>BFS</cell><cell>60.16</cell><cell>9.63</cell><cell>30.22</cell></row><row><cell></cell><cell>DFS</cell><cell>63.69</cell><cell>5.60</cell><cell>30.71</cell></row><row><cell></cell><cell>Random</cell><cell>34.79</cell><cell>4.20</cell><cell>61.01</cell></row><row><cell>SHS148k</cell><cell>BFS</cell><cell>52.54</cell><cell>9.62</cell><cell>37.84</cell></row><row><cell></cell><cell>DFS</cell><cell>51.51</cell><cell>5.72</cell><cell>42.77</cell></row><row><cell></cell><cell>Random</cell><cell>13.96</cell><cell>1.65</cell><cell>84.39</cell></row><row><cell>STRING</cell><cell>BFS</cell><cell>31.72</cell><cell>5.03</cell><cell>63.25</cell></row><row><cell></cell><cell>DFS</cell><cell>26.94</cell><cell>4.75</cell><cell>68.31</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://github.com/lvguofeng/GNN PPI.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">https://thebiogrid.org/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">https://string-db.org/</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The formation and stabilization of protein structure</title>
		<author>
			<persName><surname>Anfinsen ; Christian B Anfinsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biochemical Journal</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="737" to="749" />
			<date type="published" when="1972">1972. 1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multifaceted protein-protein interaction prediction based on siamese residual rcnn</title>
		<author>
			<persName><forename type="first">Chen</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="305" to="314" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Protein-protein interactions essentials: key concepts to building and analyzing interactome networks</title>
		<author>
			<persName><forename type="first">Las</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><surname>Rivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Javier</forename><surname>Fontanillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Las</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Celia</forename><surname>Rivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Fontanillo</surname></persName>
		</author>
		<author>
			<persName><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Comput Biol</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">e1000807</biblScope>
			<date type="published" when="2010">2010. 2020. 2020</date>
		</imprint>
	</monogr>
	<note>Bioinformatics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Using support vector machine combined with auto covariance to predict protein-protein interactions from protein sequences</title>
		<author>
			<persName><surname>Du</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<idno>arXiv:1609.02907</idno>
	</analytic>
	<monogr>
		<title level="m">Erdős and Rényi, 1960] Paul Erdős and Alfréd Rényi</title>
				<imprint>
			<publisher>Kingma and Ba</publisher>
			<date type="published" when="1959">2017. 2017. 1995. 1995. 1959. 1960. 1989. 1989. 2008. 2008. 2018. 2018. 2014. 2014. 2016</date>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="802" to="810" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Adam: A method for stochastic optimization. Kipf and Welling, 2016] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Transforming the language of life: Transformer neural networks for protein prediction tasks</title>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th ACM International Conference on Bioinformatics</title>
				<editor>
			<persName><forename type="first">Maeve</forename><surname>Heflin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Simon</forename><surname>Liu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sergei</forename><surname>Maslov</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mark</forename><surname>Hopkins</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Anna</forename><surname>Ritz</surname></persName>
		</editor>
		<meeting>the 11th ACM International Conference on Bioinformatics</meeting>
		<imprint>
			<publisher>Ananthan Nambiar</publisher>
			<date type="published" when="1923">2018. 1923. 2018. 2013. 2013. 2020</date>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>Molecules</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Amalgamation of protein sequence, structure and textual information for improving protein-protein interaction identification</title>
		<author>
			<persName><forename type="first">Others ; Sriparna</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><surname>Saha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="6396" to="6407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Predicting protein-protein interactions based only on sequences information</title>
		<author>
			<persName><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Molecular biotechnology</title>
				<editor>
			<persName><forename type="first">Joe</forename><surname>Breitkreutz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Teresa</forename><surname>Reguly</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Lorrie</forename><surname>Boucher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ashton</forename><surname>Breitkreutz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mike</forename><surname>Tyers</surname></persName>
		</editor>
		<meeting><address><addrLine>Bo</addrLine></address></meeting>
		<imprint>
			<publisher>Tanlin Sun</publisher>
			<date type="published" when="2006">2007. 2007. 2014. 2014. 2008. 2008. 2006. 2017. 2017</date>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>Nucleic acids research. suppl. Sequence-based prediction of protein protein interaction using a deep-learning algorithm. BMC bioinformatics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The string database in 2017: quality-controlled protein-protein association networks, made broadly accessible</title>
		<author>
			<persName><surname>Szklarczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="page">937</biblScope>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
