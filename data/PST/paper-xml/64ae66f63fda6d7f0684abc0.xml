<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Do PLMs Know and Understand Ontological Knowledge?</title>
				<funder ref="#_dR27eSM">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-09-12">12 Sep 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Weiqi</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chengyue</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Shanghai Engineering Research Center of Intelligent Vision and Imaging</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Yong</forename><surname>Jiang</surname></persName>
							<email>jiangchy@shanghaitech.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<settlement>Alibaba Group</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pengjun</forename><surname>Xie</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<settlement>Alibaba Group</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kewei</forename><surname>Tu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Shanghai Engineering Research Center of Intelligent Vision and Imaging</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Do PLMs Know and Understand Ontological Knowledge?</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-09-12">12 Sep 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2309.05936v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Ontological knowledge, which comprises classes and properties and their relationships, is integral to world knowledge. It is significant to explore whether Pretrained Language Models (PLMs) know and understand such knowledge. However, existing PLM-probing studies focus mainly on factual knowledge, lacking a systematic probing of ontological knowledge. In this paper, we focus on probing whether PLMs store ontological knowledge and have a semantic understanding of the knowledge rather than rote memorization of the surface form. To probe whether PLMs know ontological knowledge, we investigate how well PLMs memorize: (1) types of entities; (2) hierarchical relationships among classes and properties, e.g., Person is a subclass of Animal and Member of Sports Team is a subproperty of Member of ; (3) domain and range constraints of properties, e.g., the subject of Member of Sports Team should be a Person and the object should be a Sports Team. To further probe whether PLMs truly understand ontological knowledge beyond memorization, we comprehensively study whether they can reliably perform logical reasoning with given knowledge according to ontological entailment rules. Our probing results show that PLMs can memorize certain ontological knowledge and utilize implicit knowledge in reasoning. However, both the memorizing and reasoning performances are less than perfect, indicating incomplete knowledge and understanding.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Pretrained Language Models (PLMs) have orchestrated impressive progress in NLP across a wide variety of downstream tasks, including knowledge-intensive tasks. Previous works propose that PLMs are capable of encoding a significant amount of knowledge from the pretraining corpora <ref type="bibr" target="#b0">(AlKhamissi et al., 2022)</ref>, and determine to explore the kinds of knowledge within PLMs. (P2) Lionel Messi is a player at Argentina National Football Team.</p><p>(H) Therefore, Lionel Messi is a [MASK] . Existing probing works mainly focus on factual knowledge associated with instances <ref type="bibr" target="#b31">(Petroni et al., 2019;</ref><ref type="bibr" target="#b17">Jiang et al., 2020;</ref><ref type="bibr" target="#b33">Safavi and Koutra, 2021)</ref>. Meanwhile, although classes (concepts) have raised some research interest <ref type="bibr" target="#b3">(Bhatia and Richie, 2020;</ref><ref type="bibr" target="#b29">Peng et al., 2022;</ref><ref type="bibr" target="#b23">Lin and Ng, 2022)</ref>, there is no systematic study of ontological knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Member of Sports Team Person</head><p>Ontological knowledge models the world with a set of classes and properties and the relationships that hold between them <ref type="bibr" target="#b28">(Nilsson, 2006;</ref><ref type="bibr" target="#b21">Kumar et al., 2019)</ref>. It plays a vital role in many NLP tasks such as question answering by being injected into <ref type="bibr" target="#b11">(Goodwin and Demner-Fushman, 2020)</ref> or embedded outside deep neural networks <ref type="bibr" target="#b40">(Wang et al., 2017)</ref>. Therefore, it is essential to explore whether PLMs can encode ontological knowledge and have a semantic understanding of the knowledge rather than rote memorizing its surface form.</p><p>In this paper, we first probe PLM's memorization of ontological knowledge. Specifically, as shown in Figure <ref type="figure">1</ref>(a), we construct memorization tests about (1) Types of entities. Entities can be categorized into classes, as Lionel Messi is a Person and Argentina National Football Team is a Sports Team.</p><p>(2) Hierarchical relationships between classes, e.g., Person is a subclass of Animal. (3) Hierarchical relationships between properties, e.g., Member of Sports Team is a subproperty of Member of. (4) Domain constraints of properties. It specifies information about the subjects to which a property applies. For example, the subject of Member of Sports Team should be an instance of Person. (5) Range constraints of properties. Similar to domain, range specifies information about the object of a property, such as the object of Member of Sports Team should be an instance of Sports Team. Experiments prove that PLMs store a certain amount of ontological knowledge.</p><p>To further examine whether PLMs understand ontological knowledge, we investigate if PLMs can correctly perform logical reasoning that requires ontological knowledge. Illustrated in Figure <ref type="figure">1(b)</ref>, given the fact triple (Lionel Messi, Member of Sports Team, Argentina National Football Team) along with property constraints, we can perform type inferences to conclude that Lionel Messi is a Person, and Argentina National Football Team is a Sports Team. We comprehensively investigate the reasoning capability of PLMs over ontological knowledge following six entailment rules. Experiments show that PLMs can apply implicit ontological knowledge to draw conclusions through reasoning, but the accuracy of their reasoning falls short of perfection. This observation suggests that PLMs possess a limited understanding of ontological knowledge.</p><p>In summary, we systematically probe whether PLMs know and understand ontological knowledge. Our main contributions can be summarized as follows: (1) We construct a dataset that evaluates the ability of PLMs to memorize ontological knowledge and their capacity to draw inferences based on ontological entailment rules. (2) We comprehensively probe the reasoning ability of PLMs by carefully classifying how ontological knowledge is given as a premise. (3) We find that PLMs can memorize certain ontological knowledge but have a limited understanding. We anticipate that our work will facilitate more in-depth research on ontological knowledge probing with PLMs. The code and dataset are released at https://github.com/ vickywu1022/OntoProbe-PLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Benchmark Construction</head><p>In this section, we present our methodology for ontology construction and the process of generating memorizing and reasoning tasks based on the ontology for our probing analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Ontology Building</head><p>Class We use DBpedia <ref type="bibr" target="#b1">(Auer et al., 2007)</ref> to obtain classes and their instances. Specifically, we first retrieve all 783 classes in DBpedia, then use SPARQL <ref type="bibr">(hommeaux, 2011)</ref> to query their instances using the type relation and superclasses using the subclass-of relation. We sample 20 instances for each class.</p><p>Property Properties are collected based on DBpedia and Wikidata <ref type="bibr" target="#b38">(Vrande?i? and Kr?tzsch, 2014)</ref> using the following pipeline: (1) Obtain properties from Wikidata and use subproperty of (P1647) in Wikidata to find their superproperties. (2) Query the domain and range constraints of the properties using property constraint (P2302) in Wikidata. (3) Align the Wikidata properties with DBpedia properties by equivalent property (P1628). (4) Query the domain and range constraints of the properties in DBpedia. (5) Cleanse the collected constraints using the above-collected class set as vocabulary. We choose 50 properties with sensible domain, range and superproperties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Construction of Memorizing Task</head><p>The memorizing task consists of five subtasks, each probing the memorization of an ontological relationship: (1) TP: types of a given instance, (2) SCO: superclasses of a given class, (3) SPO: superproperties of a given property, (4) DM: domain constraint on a given property, and (5) RG: range constraint on a given property. Every subtask is formulated as a cloze-completion problem, as shown in Figure <ref type="figure">1</ref>(b). Multiple correct answers exist for TP, SCO, and SPO, which form a chain of classes or properties. There is only one correct answer for DM and RG, as it is not sound to declare an expanded restriction on a property. For instance, Animal is too broad as the domain constraint of the property Member of Sports Team (P54), hence applying Person as the domain.</p><p>We construct the dataset for each subtask using the ontology built in Sec. 2.1 and reserve 10 samples for training and 10 for validation to facilitate few-shot knowledge probing. The statistics of the dataset for each subtask are shown in Table <ref type="table" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Construction of Reasoning Task</head><p>We construct the reasoning task based on the entailment rules specified in the Resource Description Framework Schema (RDFS) 1 . We propose six subtasks, each probing the reasoning ability following a rule listed in Table <ref type="table" target="#tab_2">2</ref>. For rule rdfs2/3/7, we design a pattern for each property to be used between a pair of instances, e.g., "[X] is a player at <ref type="bibr">[Y]</ref> ." for Member of Sports Team, where [X] and [Y] are the subject and object, respectively.</p><p>Each entailment rule describes a reasoning process: P 1 ? P 2 |= H, where P 1 , P 2 are the premises 1 RDFS is an extension of RDF <ref type="bibr" target="#b4">(Brickley and Guha, 2002;</ref><ref type="bibr" target="#b9">Gibbins and Shadbolt, 2009)</ref>, a widely used and recognized data model. See https://www.w3.org/TR/rdf11-mt/ #rdfs-entailment for all the entailment rules. and H is the hypothesis. Similar to the memorizing task, we formulate the reasoning task as cloze-completion by masking the hypothesis (see Figure <ref type="figure">1(b)</ref>). Premises are also essential to the reasoning process and can be:</p><p>? Explicitly Given: The premise is explicitly included in the input of the model, and inferences are made with natural language statements.</p><p>? Implicitly Given: The premise is not explicitly given but memorized by the model as implicit knowledge. The model needs to utilize implicit knowledge to perform inferences, which relieves the effect of context and requires understanding the knowledge.</p><p>? Not Given: The premise is neither explicitly given nor memorized by the model. It serves as a baseline where the model makes no inference.</p><p>Hence, there exist 3 ? 3 different setups for two premises. It is a refinement of the experimental setup used by <ref type="bibr" target="#b37">Talmor et al. (2020)</ref>, which only distinguishes whether a premise is explicitly included in the input. We determine the memorization of a premise by the probing results of the memorizing task, which will be elaborated in Sec. 3.2.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Probing Methods</head><p>We investigate encoder-based PLMs (BERT <ref type="bibr" target="#b6">(Devlin et al., 2019)</ref> and RoBERTa <ref type="bibr" target="#b26">(Liu et al., 2019)</ref>) that can be utilized as input encoders for various NLP tasks. Prompt is an intuitive method of our probing task as it matches the mask-filling nature   of BERT. We use OpenPrompt <ref type="bibr" target="#b7">(Ding et al., 2022)</ref>, an open-source framework for prompt learning that includes the mainstream prompt methods, to facilitate the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Probing Methods for Memorization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Prompt Templates</head><p>Manual Templates Manual prompts with human-designed templates written in discrete language phrases are widely used in zero-shot probing <ref type="bibr" target="#b34">(Schick and Sch?tze, 2021)</ref> as PLMs can perform tasks without any training. Manual templates are designed for all the ontological relationships in our task, as shown in Table <ref type="table" target="#tab_3">3</ref>.</p><p>Soft Templates One of the disadvantages of manual prompts is that the performance can be significantly affected by perturbation to the prompt templates <ref type="bibr" target="#b17">(Jiang et al., 2020)</ref>. A common alternative is to use soft prompts that consist of learnable soft tokens <ref type="bibr" target="#b25">(Liu et al., 2021;</ref><ref type="bibr" target="#b22">Li and Liang, 2021)</ref>  </p><formula xml:id="formula_0">?i = log (p([M ASK] i = c i )) s = Pooling(? 1 , ?2 , . . . , ?n )</formula><p>Single Mask We use one single <ref type="bibr">[MASK]</ref> token to obtain an independent prediction of each token. The log probability of each composing token c i equals the log probability of recovering c i in the same <ref type="bibr">[MASK]</ref>, and the candidate is scored with the proposed pooling methods.</p><formula xml:id="formula_1">?i = log (p([M ASK] = c i ))</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Metrics</head><p>We rank the candidates by their log probability scores and use the top K Recall (R@K) and Mean Reciprocal Rank (MRR) as our evaluation metrics.</p><p>Since MRR only evaluates the ability to retrieve the first ground truth, we additionally take the average rank of all gold labels as the final rank when computing mean reciprocal rank to evaluate models' ability to retrieve all the ground truths and denote it as MRR a . Formally, MRR a is defined as:</p><formula xml:id="formula_2">MRR a = 1 n n i=1 1/( 1 |G i | g?G i rank(g))</formula><p>where n is the number of samples in the dataset and G i is the gold label set of the ith sample.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Probing Methods for Reasoning</head><p>We explain how we concatenate the premises and hypothesis in the textual input, exclude the models' memory of hypotheses and split a set of premises based on how well the knowledge they represent is memorized by the model. We follow the candidate scoring methods proposed in Sec. 3.1.2 and evaluation metrics in Sec. 3.1.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Prompt Templates</head><p>Apart from the prompt templates for our concerned ontological relationships introduced in Sec. 3.1.1, we further add conjunction tokens between the premises and hypothesis, which can be either manually designed or automatically tuned.</p><p>Manual Conj. As in Figure <ref type="figure">1</ref>(b), we use a conjunctive adverb therefore between the premises and hypothesis. It is kept when there is no premise explicitly given in the input to exclude the effect of the template on probing results under different premise settings.</p><p>Soft Conj. We can also use soft conjunctions by adding a soft token between premises explicitly given in the input and a soft token between the premises and the hypothesis. Therefore, the input would be "P 1 &lt;s4&gt; P 2 &lt;s5&gt; H". The soft templates used in P 1 , P 2 and H are loaded from the learned soft prompts in memorizing tasks and finetuned together with soft conjunctions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Reasoning with Pseudowords</head><p>When testing the reasoning ability of PLMs, we replace the specific instances, classes, and properties in the hypothesis prompt with pseudowords to prevent probing the memorization of hypotheses. Pseudowords <ref type="bibr" target="#b35">(Sch?tze, 1998;</ref><ref type="bibr" target="#b43">Zhang and Pei, 2022;</ref><ref type="bibr">Goodwin et al., 2020)</ref> are artificially constructed words without any specific lexical meaning. For example, the reasoning prompt for the transitivity of subclass (i.e., rule rdfs9) is "[X] is a person. Person is an animal. Therefore, [X] is a particular <ref type="bibr">[MASK]</ref> .", where [X] is a pseudoword.</p><p>Inspired by <ref type="bibr" target="#b18">(Karidi et al., 2021)</ref>, we obtain pseudowords for PLMs by creating embeddings without special semantics. Specifically, we sample embeddings at a given distance from the [MASK] token, as the <ref type="bibr">[MASK]</ref> token can be used to predict all the words in the vocabulary and appear anywhere in the sentence. The sampling distance d is set to be smaller than the minimum L2 distance between [MASK] and any other tokens in the static embedding space. Formally:</p><formula xml:id="formula_3">d = ? ? min t?V ?z t -z [M ASK] ? 2</formula><p>where z t is the static embedding of token t and ? ? (0, 1) is a coefficient. Moreover, we require that the distance between two pseudowords is at least the sampling distance d to ensure they can be distinguished from each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Classifying Premises: Memorized or not</head><p>To determine whether a premise is memorized by the model when it is not explicitly given in the input, we employ a classifying method based on the rank of the correct answer in the memorizing task to sort and divide the premise set. The first half of the premise set is regarded as memorized, and the second half is not.</p><p>Each rule consists of two premises and we classify them separately. For P 1 , which involves knowledge of subclass, subproperty, domain or range tested in the memorizing task, we can leverage previously calculated reciprocal rank during the evaluation. Premises are then sorted in descending order by the reciprocal rank. We conduct the same tests on P 2 , which involves knowledge of pseudowords, to examine model predispositions towards specific predictions and classify whether P 2 is memorized or not. Finally, we form our test set by combining premises according to the entailment rule and how each premise is given.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Findings</head><p>In this section, we introduce the performance of PLMs<ref type="foot" target="#foot_0">2</ref> on the test sets of memorizing and reasoning tasks, and analyze the results to posit a series of findings. We then analyze the effectiveness of different prompts. Detailed experimental results can be found in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Memorizing Task</head><p>The baseline model used for the memorizing task is a frequency-based model which predicts a list of gold labels in the training set based on the frequency at which they appear, followed by a random list of candidates that are not gold labels in the training set. It combines prior knowledge and random guesses and is stronger than a random baseline.</p><p>The experimental results of the memorizing task are summarized in Table <ref type="table" target="#tab_5">4</ref>, from which we can observe that: (1) The best performance of PLMs is better than the baseline on every task except for DM. On DM, the baseline achieves higher MRR. If taking all three metrics into account, the best performance of PLMs still surpasses the performance of the baseline. (2) Except for DM, BERT models achieve much better performance than the baseline in all subtasks and all metrics. Taking an average of the increase in each metric, they outperform the baseline by 43-198%. Only BERTbase-uncased and BERT-large-cased outperform the baseline in DM by a small margin of 1% and 7%. (3) RoBERTa models generally fall behind BERT, showing a 38-134% improvement compared with the baseline except for DM. (4) Despite a significant improvement from the baseline, the results are still not perfect in all subtasks.</p><p>PLMs can memorize certain ontological knowledge but not perfectly. Based on the above observation, we can conclude that PLMs have a certain memory of the concerned ontological rela-tionships and the knowledge can be accessed via prompt, allowing them to outperform a strong baseline. It proves that during pretraining, language models learn not only facts about entities but also their ontological relationships, which is essential for a better organization of world knowledge. However, the memorization is not perfect, urging further efforts on ontology-aware pretraining.</p><p>Large models are not necessarily better at memorizing ontological knowledge. According to <ref type="bibr" target="#b31">Petroni et al. (2019)</ref>, models with larger sizes appear to store more knowledge and achieve better performance in both knowledge probing tasks and downstream NLP tasks. However, as shown in Table 4, BERT-large-uncased is worse than its smaller variant under most circumstances, and RoBERTalarge is worse than RoBERTa-base in TP and DM. It demonstrates that the scale of model parameters does not necessarily determine the storage of ontological knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Reasoning Task</head><p>We fix the usage of multiple masks and meanpooling in the reasoning experiments as they generally outperform other settings in the memorizing task (see Appendix B). We take an average of the MRR metrics using different templates and illustrate the results of BERT-base-cased and RoBERTa- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BERT-BASE-CASED ROBERTA-BASE</head><p>Figure <ref type="figure">2</ref>: The MRR by BERT-base-cased and RoBERTa-base using different combinations of premises. EX stands for explicitly given, IM stands for implicitly given and NO stands for not given. Other metrics show similar trends.</p><p>base in Figure <ref type="figure">2</ref>. With neither premise given, the rank of the ground truth is usually low. It shows that models have little idea of the hypothesis, which is reasonable because the information of pseudowords is probed. With premises implicitly or explicitly given, especially P 1 , the MRR metrics improve in varying degrees. Moreover, results show that BERT-base-cased has better reasoning ability with our concerned ontological entailment rules than RoBERTa-base.</p><p>PLMs have a limited understanding of the semantics behind ontological knowledge. To reach a more general conclusion, we illustrate the overall reasoning performance in Figure <ref type="figure" target="#fig_2">3</ref> by averaging over all the entailment rules and PLMs, and find that: (1) When P 1 is explicitly given in the input text, models are able to significantly improve the rank of gold labels. As P 1 contains the ground truth in its context, it raises doubt about whether the improvement is obtained through logical reasoning or just priming <ref type="bibr" target="#b27">(Misra et al., 2020)</ref>.</p><p>(2) Explicitly giving P 2 introduces additional tokens that may not be present in gold labels, making P 1 /P 2 = EX/EX worse than P 1 /P 2 = EX/IM.</p><p>(3) When premises are implicitly given, the MRR metrics are higher than when they are not given. It implies that, to some extent, PLMs can utilize the implicit ontological knowledge and select the correct entailment rule to make inferences. (4) However, none of the premises combinations can give near-perfect reasoning performance (MRR metrics close to 1), suggesting that PLMs only have a weak understanding of ontological knowledge.</p><p>Paraphrased properties are a challenge for language models. In Figure <ref type="figure">2</ref>(d), the premise P 1 of rule rdfs7 contains a paraphrased version of the ground truth, which is the manually-designed pattern of a particular property. Compared with rule rdfs5 shown in Figure <ref type="figure">2(c)</ref>, where P 1 contains the surface form of the correct property, the MRR of BERT-base-cased of rdfs7 decreases by 23%, 49% and 29% when P 1 is explicitly given and P 2 is not, implicitly and explicitly given, respectively. Though the MRR of RoBERTa-base of rdfs7 increases when P 2 is not given, it decreases by 40% and 15% when P 2 is implicitly and explicitly given. This suggests that PLMs fail to understand the semantics of some properties, thus demonstrating a limited understanding of ontological knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Effectiveness of Prompts</head><p>In this section, we discuss how prompt templates affect performance. In the memorizing task, Table <ref type="table" target="#tab_5">4</ref> shows that using soft templates generally improves the performance of memorizing tasks, in particular TP, SCO and SPO. It suggests that it is non-trivial to extract knowledge from PLMs.</p><p>Meanwhile, only a few models perform better with soft templates on DM and RG with a relatively marginal improvement. This could be explained by the fact that both the manual templates and semantics of domain and range constraints are more complex than those of other relationships. Therefore, it is difficult for models to capture with only three soft tokens. We also note that RoBERTa models appear to benefit more from soft templates than BERT models, probably due to their poor performance with manual templates.</p><p>Trained soft templates for each relation barely help with reasoning, though. In Figure <ref type="figure">4</ref>, we summarize the performance by averaging across different models and reasoning tasks and find that it is the trained conjunction token which improves the performance of reasoning rather than the soft templates that describe ontological relationships. It might be inspiring that natural language inference with PLMs can be improved by adding trainable tokens as conjunctions instead of simply concatenating all the premises.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Preliminary Evaluation of ChatGPT</head><p>After we finished the majority of our probing experiments, ChatGPT, a decoder-only model, was publicly released and demonstrated remarkable capabilities in commonsense knowledge and reasoning. Therefore, we additionally perform a preliminary probe of the ability of ChatGPT to memorize and 0.225 R@1 0.4171 R@1 0.4394 R@1 0.4112 R@5 0.4405 R@5 0.6419 R@5 0.6612 understand ontological knowledge.</p><p>Since ChatGPT is a decoder-only model, we employ a distinct probing method from what is expounded in Sec. 3. Instead of filling masks, we directly ask ChatGPT to answer multiple-choice questions with 20 candidate choices and evaluate the accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Probing for Memorization Ability</head><p>For memorization probing, we use the finestgrained gold label as the correct answer and randomly sample 19 negative candidates to form the choice set. Take the TP task as an example, we query the GPT-3.5-turbo API with the prompt "What is the type of Lionel Messi? (a) soccer player, (b) work, (c) ..." followed by remaining candidates. We sample 500 test cases for the TP and SCO tasks and use the complete test sets for the other tasks.</p><p>For comparison, we also conduct the experiments using BERT-base-uncased, a generally competitive PLM in memorizing and understanding ontological knowledge, with manual prompts and the identical candidate subset. The results presented in base-uncased significantly in most of the memorizing tasks associated with ontological knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Probing for Reasoning Ability</head><p>Since we cannot input embeddings in the GPT-3.5-turbo API, we use X and Y to represent pseudowords as they are single letters that do not convey meanings. However, ChatGPT cannot generate any valid prediction without sufficient context regarding these pseudowords. Therefore, P 2 needs to be explicitly provided to describe the characteristics or relations of the pseudowords. We then explore the ability of ChatGPT to select the correct answer from 20 candidates with different forms of P 1 . In this task, P 1 is regarded as memorized if the model can correctly choose the gold answer from the given 20 candidates in the memorizing task.</p><p>Based on the results presented in Table <ref type="table" target="#tab_8">6</ref>, Chat-GPT demonstrates high accuracy when P 1 is either implicitly or explicitly given, suggesting its strong capacity to reason and understand ontological knowledge. Due to a substantial disparity in the knowledge memorized by ChatGPT compared to other models (as shown in section 5.1), their performance is not directly comparable when P 1 is not given or implicitly given. Therefore, we only compare ChatGPT and BERT-base-uncased when P 1 is explicitly given. Results show that ChatGPT significantly outperforms BERT-base-uncased in explicit reasoning (97.1% vs. 88.2%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Knowledge Probing Language models are shown to encode a wide variety of knowledge after being pretrained on a large-scale corpus. Recent studies probe PLMs for linguistic knowledge <ref type="bibr" target="#b39">(Vuli? et al., 2020;</ref><ref type="bibr" target="#b14">Hewitt and Manning, 2019)</ref>, world knowledge <ref type="bibr" target="#b31">(Petroni et al., 2019;</ref><ref type="bibr" target="#b17">Jiang et al., 2020;</ref><ref type="bibr" target="#b33">Safavi and Koutra, 2021)</ref>, actionable knowledge <ref type="bibr" target="#b16">(Huang et al., 2022)</ref>, etc. via methods such as cloze prompts <ref type="bibr" target="#b2">(Beloucif and Biemann, 2021;</ref><ref type="bibr" target="#b30">Petroni et al., 2020)</ref> and linear classifiers <ref type="bibr" target="#b13">(Hewitt and Liang, 2019;</ref><ref type="bibr" target="#b32">Pimentel et al., 2020)</ref>. Although having explored extensive knowledge within PLMs, previous knowledge probing works have not studied ontological knowledge systematically. We cut through this gap to investigate how well PLMs know about ontological knowledge and the meaning behind the surface form.</p><p>Knowledge Reasoning Reasoning is the process of drawing new conclusions through the use of existing knowledge and rules. Progress has been reported in using PLMs to perform reasoning tasks, including arithmetic <ref type="bibr" target="#b41">(Wang et al., 2022;</ref><ref type="bibr" target="#b42">Wei et al., 2022)</ref>, commonsense <ref type="bibr" target="#b36">(Talmor et al., 2019</ref><ref type="bibr" target="#b37">(Talmor et al., , 2020;;</ref><ref type="bibr" target="#b42">Wei et al., 2022)</ref>, logical <ref type="bibr" target="#b5">(Creswell et al., 2022)</ref> and symbolic reasoning <ref type="bibr" target="#b42">(Wei et al., 2022)</ref>. These abilities can be unlocked by finetuning a classifier on downstream datasets <ref type="bibr" target="#b37">(Talmor et al., 2020)</ref> or using proper prompting strategies (e.g., chain of thought (CoT) prompting <ref type="bibr" target="#b42">(Wei et al., 2022)</ref> and generated knowledge prompting <ref type="bibr" target="#b24">(Liu et al., 2022)</ref>). This suggests that despite their insensitivity to negation <ref type="bibr" target="#b8">(Ettinger, 2020;</ref><ref type="bibr" target="#b19">Kassner and Sch?tze, 2020)</ref> and over-sensitivity to lexicon cues like priming words <ref type="bibr" target="#b12">(Helwe et al., 2021;</ref><ref type="bibr" target="#b27">Misra et al., 2020)</ref>, PLMs have the potential to make inferences over implicit knowledge and explicit natural language statements. In this work, we investigate the ability of PLMs to perform logical reasoning with implicit ontological knowledge to examine whether they understand the semantics beyond memorization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this work, we systematically probe whether PLMs encode ontological knowledge and understand its semantics beyond the surface form. Experiments show that PLMs can memorize some ontological knowledge and make inferences based on implicit knowledge following ontological entailment rules, suggesting that PLMs possess a certain level of awareness and understanding of ontological knowledge. However, it is important to note that both the accuracy of memorizing and reasoning is less than perfect, and the difficulty encountered by PLMs when processing paraphrased knowledge is confirmed. These observations indicate that their knowledge and understanding of ontology are limited. Therefore, enhancing the knowledge and understanding of ontology would be a worthy future research goal for language models. Our exploration into ChatGPT shows an improved performance in both memorizing and reasoning tasks, signifying the potential for further advancements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>The purpose of our work is to evaluate the ontological knowledge of PLMs. However, a sea of classes and properties exist in the real world and we only cover a selective part of them. Consequently, the scope of our dataset for the experimental analysis is limited. The findings from our experiments demonstrate an imperfect knowledge and understanding obtained by the models, indicating a tangible room for enhancement in both ontological knowledge memorization and understanding and a need for a better ability to address paraphrasing. These observations lead us to contemplate refining the existing pretraining methods to help language models achieve better performance in related tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethics Statement</head><p>We propose our ethics statement of the work in this section: (1) Dataset. Our data is obtained from DBpedia and Wikidata, two publicly available linked open data projects related to Wikipedia. Wikidata is under the Creative Commons CC0 License, and DBpedia is licensed under the terms of the Creative Commons Attribution-ShareAlike 3.0 license and the GNU Free Documentation License. We believe the privacy policies of DBpedia<ref type="foot" target="#foot_1">3</ref> and Wikidata<ref type="foot" target="#foot_2">4</ref> are well carried out. We inspect whether our dataset, especially instances collected, contains any unethical content. No private information or offensive topics are found during human inspection.</p><p>(2) Labor considerations. During dataset construction, the authors voluntarily undertake works requiring human efforts, including data collection, cleansing, revision and design of property patterns. All the participants are well informed about how the dataset will be processed, used and released.</p><p>(3) Probing results. As PLMs are pretrained on large corpora, they may give biased results when being probed. We randomly check some probing results and find no unethical content in these samples. Therefore, we believe that our study does not introduce additional risks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Experimental Setup</head><p>We train soft tokens for 100 epochs with AdamW optimizer. The learning rate is set to 0.5 and a linear warmup scheduler is used. Since both the memorizing and reasoning task can be formulated as a multi-label classification problem, we use BCE-WithLogitsLoss or NLLLoss as our loss function in the memorizing task to report the better results given by one of these two and select a better training objective. Therefore, we fix the loss function to BCEWithLogitsLoss in the reasoning task.</p><p>For pseudowords, we set the coefficient ? to 0.5 and sample 10 pairs of pseudowords for each entailment rule as we at most need two pseudowords to substitute the subject and object instances respectively, and report the averaged performance as the final result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Multi-token Prompting Methods</head><p>In the main body of the paper, we discuss the impact of different prompts on the performance of knowledge probing and reasoning. In this section, we continuously discuss the impact of other prompt settings by comparing the averaged performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Number of [MASK] Tokens</head><p>To support multi-token candidate scoring, we use multiple <ref type="bibr">[MASK]</ref> tokens or one single <ref type="bibr">[MASK]</ref> token to predict with masked language models. The comparison between the two methods is shown in Figure <ref type="figure" target="#fig_4">5</ref>, by averaging the performance of all the memorizing tasks and models. We can observe that single [MASK] prediction achieves better accuracy (R@1) with a negligible tiny margin but worse performance in other metrics. Therefore, using multiple <ref type="bibr">[MASK]</ref> tokens to obtain prediction by forward pass inference is more sensible and achieves better results.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Pooling Methods</head><p>Three pooling methods are proposed when computing the probability of a candidate that can be tokenized into multiple subtokens. The mean-pooling method is usually used in multi-token probing. Furthermore, we introduce max-pooling and firstpooling, which retain the score of only one important token. They can exclude the influence of prepositions, e.g., by attending to mean or transportation when scoring the candidate mean of transportation, but at the cost of other useful information. We are interested in whether it is better to consider the whole word or focus on the important part.</p><p>Figure <ref type="figure">6</ref> shows that mean-pooling, as a classical method, is much better than the other two pooling methods. Besides, first-pooling gives clearly better results than max-pooling, which is possibly caused by the unique information contained in the headword (usually the first token). Consider candidates volleyball player, squash player and golf player, the conditional log probability of token player might be higher, but the candidates are distinguished by their headwords. In summary, mean-pooling obtains the best results with the most comprehensive information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Loss Functions</head><p>As mentioned in Appendix A, we try two loss functions in the memorizing task. ( <ref type="formula">1</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Task Examples</head><p>In order to enhance the clarity of the experiments, we have compiled a list in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Memorizing Results</head><p>The complete results of the memorizing task are reported in Table <ref type="table" target="#tab_13">8</ref>, 9, 10, 11 and 12.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Reasoning Results</head><p>We report the MRR Metric of BERT-baseuncased, BERT-large-cased, BERT-large-uncased and RoBERTa-large in Figure <ref type="figure" target="#fig_6">8</ref>. It is generally consistent with the two models reported in the main body of the paper and the macro-averaged performance across different PLMs, so consistent conclusions can be drawn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BERT-BASE-CASED</head><p>BERT-BASE-UNCASED RoBERTa-BASE Template Masks Pooling Loss R@1 R@5 MRRa MRR R@1 R@5 MRRa MRR R@1 R@5 MRRa MRR     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>One has to be a particular[MASK]  to be a player at a sports team. ? ? z Member of sports team [s][s][s] [MASK] . Soft: domain Pseudoword (P1) One has to be a person to be a player at a sports team.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 1: (a) An example of an ontological knowledge graph. (b) Potential manual and soft prompts to probe the knowledge and corresponding semantics. Instances are replaced by pseudowords in reasoning experiments to mitigate potential interference from model memory.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The macro-averaged MRR across different entailment rules and language models with different combinations of premises.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Comparison between multiple [MASK] tokens and a single [MASK] token in the memorizing task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Comparison between two different training objectives in the memorizing task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Figure8: The MRR by BERT-base-uncased, BERT-large-(un)cased and RoBERTa-large using different combinations of premises. EX stands for explicitly given, IM stands for implicitly given and NO stands for not given. The other metrics show similar trends.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Ontological relationship, type of candidate, and dataset size for each memorizing subtask.</figDesc><table><row><cell cols="6">Task Ontological Rel. Candidate Train Dev Test</cell></row><row><cell>TP</cell><cell>type</cell><cell>class</cell><cell>10</cell><cell cols="2">10 8789</cell></row><row><cell>SCO</cell><cell>subclass of</cell><cell>class</cell><cell>10</cell><cell cols="2">10 701</cell></row><row><cell cols="2">SPO subproperty of</cell><cell>property</cell><cell>10</cell><cell>10</cell><cell>39</cell></row><row><cell>DM</cell><cell>domain</cell><cell>class</cell><cell>10</cell><cell>10</cell><cell>30</cell></row><row><cell>RG</cell><cell>range</cell><cell>class</cell><cell>10</cell><cell>10</cell><cell>28</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>Ontological Rel.</cell><cell>Manual Template</cell><cell>Soft Template</cell></row><row><cell></cell><cell>Lionel Messi is a [MASK] .</cell><cell></cell></row><row><cell>type</cell><cell>Lionel Messi has class [MASK] .</cell><cell>Lionel Messi &lt;s1&gt; &lt;s2&gt; &lt;s3&gt; [MASK] .</cell></row><row><cell></cell><cell>Lionel Messi is a particular [MASK].</cell><cell></cell></row><row><cell></cell><cell>Person is a [MASK] .</cell><cell></cell></row><row><cell>subclass of</cell><cell>Person has superclass [MASK] .</cell><cell>Person &lt;s1&gt; &lt;s2&gt; &lt;s3&gt; [MASK] .</cell></row><row><cell></cell><cell>Person is a particular [MASK].</cell><cell></cell></row><row><cell>subproperty of</cell><cell cols="2">Member of sports team implies [MASK] . Member of sports team &lt;s1&gt; &lt;s2&gt; &lt;s3&gt; [MASK] .</cell></row><row><cell>domain</cell><cell>One has to be a particular [MASK] to be a player at a sports team .</cell><cell>Member of sports team &lt;s1&gt; &lt;s2&gt; &lt;s3&gt; [MASK] .</cell></row><row><cell>range</cell><cell>One has to be a particular [MASK] to have a player at that .</cell><cell>Member of sports team &lt;s1&gt; &lt;s2&gt; &lt;s3&gt; [MASK] .</cell></row></table><note><p>Entailment rules for the reasoning task. Symbol aaa and bbb represent any random property. Symbols xxx, yyy and zzz represent some classes, and uuu and vvv represent some instances. Constituents of the conclusion highlighted in orange are to be masked in the input, and P 1 is the premise that contains the same constituents.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Manual and soft templates used in prompt-based probing. In soft templates, &lt;s1&gt; &lt;s2&gt; and &lt;s3&gt; correspond to soft tokens.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Model</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Task Metric</cell><cell>Frequency</cell><cell cols="2">BERT-B-C</cell><cell cols="2">BERT-B-U</cell><cell cols="2">BERT-L-C</cell><cell cols="2">BERT-L-U</cell><cell cols="2">RoBERTa-B</cell><cell cols="2">RoBERTa-L</cell></row><row><cell></cell><cell></cell><cell>Baseline</cell><cell cols="12">manT softT manT softT manT softT manT softT manT softT manT softT</cell></row><row><cell></cell><cell>R@1</cell><cell>15.4</cell><cell>18.9</cell><cell>20.1</cell><cell>21.2</cell><cell>24.8</cell><cell>15.7</cell><cell>22.9</cell><cell>22.3</cell><cell>13.1</cell><cell>6.6</cell><cell>15.9</cell><cell>9.0</cell><cell>8.7</cell></row><row><cell>TP</cell><cell>R@5 MRR a</cell><cell>15.6 1.3</cell><cell>41.0 2.0</cell><cell>46.4 1.9</cell><cell>48.8 3.1</cell><cell>49.3 2.7</cell><cell>46.3 2.4</cell><cell>50.6 2.0</cell><cell>42.1 1.8</cell><cell>43.9 2.0</cell><cell>18.3 0.9</cell><cell>41.1 1.9</cell><cell>39.1 1.6</cell><cell>22.4 0.9</cell></row><row><cell></cell><cell>MRR</cell><cell>19.6</cell><cell>28.4</cell><cell>31.2</cell><cell>33.2</cell><cell>35.1</cell><cell>25.0</cell><cell>36.0</cell><cell>32.1</cell><cell>23.9</cell><cell>11.9</cell><cell>28.1</cell><cell>23.7</cell><cell>14.9</cell></row><row><cell></cell><cell>R@1</cell><cell>8.1</cell><cell>11.0</cell><cell>29.7</cell><cell>15.1</cell><cell>37.9</cell><cell>14.0</cell><cell>35.0</cell><cell>11.6</cell><cell>31.0</cell><cell>9.8</cell><cell>24.5</cell><cell>9.0</cell><cell>22.8</cell></row><row><cell>SCO</cell><cell>R@5 MRR a</cell><cell>38.9 7.4</cell><cell>38.1 5.3</cell><cell>47.9 11.8</cell><cell>43.5 6.6</cell><cell>55.9 13.3</cell><cell>43.8 6.7</cell><cell>54.6 9.7</cell><cell>35.4 3.7</cell><cell>53.5 8.9</cell><cell>22.1 4.2</cell><cell>41.4 8.5</cell><cell>39.1 4.5</cell><cell>42.8 5.5</cell></row><row><cell></cell><cell>MRR</cell><cell>23.7</cell><cell>22.7</cell><cell>39.2</cell><cell>29.0</cell><cell>46.4</cell><cell>25.8</cell><cell>41.2</cell><cell>21.9</cell><cell>41.9</cell><cell>16.7</cell><cell>29.7</cell><cell>24.6</cell><cell>32.9</cell></row><row><cell></cell><cell>R@1</cell><cell>25.6</cell><cell>23.1</cell><cell>38.5</cell><cell>20.5</cell><cell>38.5</cell><cell>18.0</cell><cell>38.5</cell><cell>23.1</cell><cell>41.0</cell><cell>10.3</cell><cell>35.9</cell><cell>10.3</cell><cell>41.0</cell></row><row><cell>SPO</cell><cell>R@5 MRR a</cell><cell>28.2 15.8</cell><cell>64.1 15.8</cell><cell>64.1 23.8</cell><cell>69.2 19.5</cell><cell>74.4 29.3</cell><cell>59.0 19.5</cell><cell>76.9 29.8</cell><cell>69.2 19.0</cell><cell>64.1 28.8</cell><cell>33.3 8.8</cell><cell>61.5 25.1</cell><cell>30.8 10.0</cell><cell>69.2 29.6</cell></row><row><cell></cell><cell>MRR</cell><cell>31.2</cell><cell>39.2</cell><cell>43.7</cell><cell>38.3</cell><cell>53.5</cell><cell>34.5</cell><cell>49.8</cell><cell>39.3</cell><cell>52.9</cell><cell>20.6</cell><cell>47.4</cell><cell>21.9</cell><cell>53.8</cell></row><row><cell></cell><cell>R@1</cell><cell>43.3</cell><cell>43.3</cell><cell>30.0</cell><cell>43.3</cell><cell>40.0</cell><cell>50.0</cell><cell>40.0</cell><cell>33.3</cell><cell>26.7</cell><cell>6.7</cell><cell>43.3</cell><cell>13.3</cell><cell>16.7</cell></row><row><cell>DM</cell><cell>R@5</cell><cell>60.0</cell><cell>53.3</cell><cell>60.0</cell><cell>53.3</cell><cell>63.3</cell><cell>60.0</cell><cell>63.3</cell><cell>53.3</cell><cell>50.0</cell><cell>20.0</cell><cell>63.3</cell><cell>46.7</cell><cell>50.0</cell></row><row><cell></cell><cell>MRR</cell><cell>50.9</cell><cell>47.6</cell><cell>40.7</cell><cell>49.3</cell><cell>50.0</cell><cell>50.3</cell><cell>48.7</cell><cell>43.2</cell><cell>33.5</cell><cell>15.3</cell><cell>49.0</cell><cell>27.4</cell><cell>25.5</cell></row><row><cell></cell><cell>R@1</cell><cell>10.7</cell><cell>46.4</cell><cell>57.1</cell><cell>42.9</cell><cell>57.1</cell><cell>57.1</cell><cell>57.1</cell><cell>46.4</cell><cell>53.6</cell><cell>32.1</cell><cell>46.4</cell><cell>17.9</cell><cell>42.9</cell></row><row><cell>RG</cell><cell>R@5</cell><cell>53.6</cell><cell>67.9</cell><cell>67.9</cell><cell>75.0</cell><cell>75.0</cell><cell>78.6</cell><cell>75.0</cell><cell>78.6</cell><cell>75.0</cell><cell>57.1</cell><cell>53.6</cell><cell>53.6</cell><cell>71.4</cell></row><row><cell></cell><cell>MRR</cell><cell>31.2</cell><cell>59.1</cell><cell>62.7</cell><cell>56.0</cell><cell>63.9</cell><cell>66.8</cell><cell>66.2</cell><cell>61.1</cell><cell>59.5</cell><cell>44.0</cell><cell>50.3</cell><cell>33.2</cell><cell>48.5</cell></row></table><note><p>Performance (%) of the memorizing task. B/L stands for base/large and C/U stands for cased/uncased. The distinction between the prompt templates (manT for manual template and softT for soft template) is preserved, and for the other settings, such as the number of [MASK] tokens and pooling methods, we use the ones that give the best results and discuss their impacts in Appendix B.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Accuracy (%) achieved by ChatGPT and BERTbase-uncased on the multiple-choice memorizing task with 20 candidates.</figDesc><table><row><cell>R@5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Table 5 indicate that ChatGPT outperforms BERT-Accuracy (%) achieved by ChatGPT on each reasoning subtask with P 2 explicitly given.</figDesc><table><row><cell>P 1 AVG</cell><cell></cell><cell cols="2">RDFS Rule</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="5">rdfs2 rdfs3 rdfs5 rdfs7 rdfs9 rdfs11</cell></row><row><cell>NO 13.5</cell><cell>25.0 16.7</cell><cell>0.0</cell><cell>0.0</cell><cell>19.0</cell><cell>20.8</cell></row><row><cell>IM 82.8</cell><cell cols="4">76.9 86.4 71.5 77.7 91.9</cell><cell>92.4</cell></row><row><cell cols="5">EX 97.1 100.0 96.4 94.9 96.9 97.4</cell><cell>97.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>) The Binary Cross Entropy With Logits Loss (BCEWithLogitsLoss) is a common loss function for multi-label classification which numerically stably combines a Sigmoid layer and the Binary Cross Entropy Loss into one layer. All examples are given the same weight</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Mean</cell><cell>First</cell><cell></cell><cell>Max</cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.1</cell><cell>0.1993 Mean</cell><cell>0.1719 First</cell><cell>0.1466 Max</cell><cell>0.4101 Mean</cell><cell>0.3511 First</cell><cell>0.2684 Max</cell><cell>0.1799 Mean</cell><cell>0.1459 First</cell><cell>0.1067 Max</cell><cell>0.3031 Mean</cell><cell>0.2487 First</cell><cell>0.1863 Max</cell></row><row><cell>0</cell><cell cols="2">R@1</cell><cell></cell><cell></cell><cell>R@5</cell><cell></cell><cell cols="2">MRRa</cell><cell></cell><cell></cell><cell>MRR</cell><cell></cell></row><row><cell cols="13">Figure 6: Effectiveness of different pooling methods in</cell></row><row><cell cols="5">the memorizing task.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="13">when calculating the loss. (2) The Negative Log</cell></row><row><cell cols="13">Likelihood Loss (NLLLoss) is a loss function for</cell></row><row><cell cols="13">multi-class classification. However, we can convert</cell></row><row><cell cols="13">the original multi-label problem to a multi-class</cell></row><row><cell cols="13">one by sampling one ground truth at a time to gen-</cell></row><row><cell cols="13">erate multiple single-label multi-class classification</cell></row><row><cell cols="13">cases. As can be seen from Figure 7, using BCE-</cell></row><row><cell cols="13">WithLogitsLoss as the loss function achieves better</cell></row><row><cell cols="13">results than using NLLLoss. Hence, in subsequent</cell></row><row><cell cols="13">reasoning experiments, we stick to the classical</cell></row><row><cell cols="10">loss for multi-label classification.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">BCEWithLogitsLoss</cell><cell cols="2">NLLLoss</cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.1</cell><cell>0.2282 BCEWithLogitsLoss</cell><cell cols="2">0.2062 NLLLoss</cell><cell>0.4275 BCEWithLogitsLoss</cell><cell cols="2">0.3935 NLLLoss</cell><cell>0.1895 BCEWithLogitsLoss</cell><cell cols="2">0.18 NLLLoss</cell><cell>0.3095 BCEWithLogitsLoss</cell><cell></cell><cell>0.2849 NLLLoss</cell></row><row><cell>0</cell><cell cols="2">R@1</cell><cell></cell><cell></cell><cell>R@5</cell><cell></cell><cell cols="2">MRRa</cell><cell></cell><cell></cell><cell>MRR</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Table 7 that includes task Example manual prompt and predictions by BERT-base-cased for each memorizing task. Correct predictions and golds predicted among the top-5 are marked with a and highlighted in green.</figDesc><table><row><cell></cell><cell>Memorizing Task</cell><cell></cell><cell></cell></row><row><cell>Task</cell><cell>Prompt</cell><cell>Top-5 Predictions</cell><cell>Golds</cell></row><row><cell></cell><cell></cell><cell>disease</cell><cell></cell></row><row><cell>TP</cell><cell>Salininema is a particular [MASK] .</cell><cell>medical specialty case drug</cell><cell>bacteria species</cell></row><row><cell></cell><cell></cell><cell>species</cell><cell></cell></row><row><cell>SCO</cell><cell>Motor race is a particular [MASK] .</cell><cell>sport sports event genre event</cell><cell>tournament sports event societal event event</cell></row><row><cell></cell><cell></cell><cell>team sport</cell><cell></cell></row><row><cell></cell><cell></cell><cell>corporate officer</cell><cell></cell></row><row><cell>SPO</cell><cell>Chief executive officer implies [MASK] .</cell><cell>director / manager significant person head of government</cell><cell>corporate officer director / manager</cell></row><row><cell></cell><cell></cell><cell>rector</cell><cell></cell></row><row><cell></cell><cell></cell><cell>music composer</cell><cell></cell></row><row><cell></cell><cell></cell><cell>person</cell><cell></cell></row><row><cell cols="2">DM One has to be a particular [MASK] to have composer.</cell><cell>musical artist</cell><cell>work</cell></row><row><cell></cell><cell></cell><cell>place</cell><cell></cell></row><row><cell></cell><cell></cell><cell>case</cell><cell></cell></row><row><cell></cell><cell></cell><cell>person</cell><cell></cell></row><row><cell></cell><cell></cell><cell>woman</cell><cell></cell></row><row><cell>RG</cell><cell>One has to be a particular [MASK] to be mother.</cell><cell>family</cell><cell>woman</cell></row><row><cell></cell><cell></cell><cell>name</cell><cell></cell></row><row><cell></cell><cell></cell><cell>case</cell><cell></cell></row><row><cell cols="2">prompts as well as the top five predicted candidate</cell><cell></cell><cell></cell></row><row><cell cols="2">words generated by BERT-base-cased. The table</cell><cell></cell><cell></cell></row><row><cell cols="2">consists of examples with successful predictions</cell><cell></cell><cell></cell></row><row><cell cols="2">for all correct answers (SPO, RG), examples with</cell><cell></cell><cell></cell></row><row><cell cols="2">partial correct answers predicted (TP, SCO), and</cell><cell></cell><cell></cell></row><row><cell cols="2">examples where the correct answer is not predicted</cell><cell></cell><cell></cell></row><row><cell cols="2">within the top five candidates (DM).</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>TP results.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">BERT-BASE-CASED</cell><cell cols="4">BERT-BASE-UNCASED</cell><cell></cell><cell cols="2">RoBERTa-BASE</cell></row><row><cell cols="15">Template Masks Pooling Loss R@1 R@5 MRRa MRR R@1 R@5 MRRa MRR R@1 R@5 MRRa MRR</cell></row><row><cell>soft</cell><cell></cell><cell></cell><cell cols="3">log 10.27 38.09</cell><cell>4.62</cell><cell cols="7">21.50 34.81 48.79 10.49 42.26 22.25 37.95</cell><cell>6.14</cell><cell>29.75</cell></row><row><cell>soft</cell><cell></cell><cell></cell><cell cols="3">NLL 7.70 30.24</cell><cell>4.66</cell><cell cols="7">17.00 32.52 49.36 11.10 41.13 10.41 33.81</cell><cell>4.19</cell><cell>21.20</cell></row><row><cell>manual1</cell><cell></cell><cell>first</cell><cell></cell><cell>1.14</cell><cell>5.42</cell><cell>1.21</cell><cell>4.55</cell><cell cols="2">1.43 10.70</cell><cell>1.57</cell><cell>6.51</cell><cell>0.71</cell><cell>3.99</cell><cell>0.75</cell><cell>3.67</cell></row><row><cell>manual2</cell><cell></cell><cell></cell><cell>-</cell><cell cols="2">8.84 25.82</cell><cell>2.02</cell><cell cols="3">16.82 6.85 21.26</cell><cell>2.18</cell><cell cols="3">15.15 9.84 22.11</cell><cell>2.57</cell><cell>16.72</cell></row><row><cell>manual3</cell><cell></cell><cell></cell><cell></cell><cell cols="2">9.99 30.39</cell><cell>4.58</cell><cell cols="3">19.21 14.84 38.80</cell><cell>5.30</cell><cell cols="3">25.99 0.14 14.12</cell><cell>1.34</cell><cell>7.80</cell></row><row><cell>soft</cell><cell></cell><cell></cell><cell cols="3">log 29.10 45.51</cell><cell>7.74</cell><cell cols="3">35.85 24.25 39.37</cell><cell>4.74</cell><cell cols="3">31.07 15.55 32.24</cell><cell>5.09</cell><cell>23.50</cell></row><row><cell>soft manual1</cell><cell>m</cell><cell>max</cell><cell cols="3">NLL 5.14 25.39 0.43 2.00</cell><cell>3.85 1.25</cell><cell cols="3">12.32 11.84 33.52 2.45 0.43 1.14</cell><cell>4.45 1.25</cell><cell cols="2">19.75 5.56 2.51 0.43</cell><cell>9.99 3.57</cell><cell>2.30 0.85</cell><cell>8.63 2.26</cell></row><row><cell>manual2</cell><cell></cell><cell></cell><cell>-</cell><cell cols="2">9.42 23.97</cell><cell>2.25</cell><cell cols="3">13.44 7.28 21.11</cell><cell>2.66</cell><cell cols="2">11.32 1.28</cell><cell>6.13</cell><cell>1.19</cell><cell>3.96</cell></row><row><cell>manual3</cell><cell></cell><cell></cell><cell></cell><cell cols="2">4.99 11.84</cell><cell>2.59</cell><cell>8.32</cell><cell cols="2">5.42 18.54</cell><cell>3.04</cell><cell cols="2">12.03 1.14</cell><cell>4.99</cell><cell>1.21</cell><cell>2.95</cell></row><row><cell>soft</cell><cell></cell><cell></cell><cell cols="7">log 29.67 47.93 11.76 39.16 34.52 45.22</cell><cell>9.65</cell><cell cols="3">40.16 16.12 36.95</cell><cell>7.37</cell><cell>25.84</cell></row><row><cell>soft</cell><cell></cell><cell></cell><cell cols="3">NLL 12.55 38.37</cell><cell>8.25</cell><cell cols="3">25.17 34.66 48.07</cell><cell>9.58</cell><cell cols="3">41.44 17.83 30.53</cell><cell>6.36</cell><cell>24.80</cell></row><row><cell>manual1</cell><cell></cell><cell>mean</cell><cell></cell><cell cols="2">7.13 19.12</cell><cell>3.25</cell><cell cols="3">14.18 2.57 13.69</cell><cell>2.13</cell><cell>9.01</cell><cell>1.00</cell><cell>3.00</cell><cell>1.30</cell><cell>3.98</cell></row><row><cell>manual2</cell><cell></cell><cell></cell><cell>-</cell><cell cols="2">10.98 33.10</cell><cell>3.02</cell><cell cols="3">22.06 8.56 31.95</cell><cell>3.04</cell><cell cols="2">19.86 2.28</cell><cell>7.70</cell><cell>2.21</cell><cell>7.15</cell></row><row><cell>manual3</cell><cell></cell><cell></cell><cell></cell><cell cols="2">9.56 38.09</cell><cell>5.27</cell><cell cols="3">22.68 15.12 43.51</cell><cell>6.55</cell><cell cols="2">28.98 2.14</cell><cell>7.70</cell><cell>2.35</cell><cell>6.94</cell></row><row><cell>soft</cell><cell></cell><cell></cell><cell cols="3">log 22.40 40.66</cell><cell>7.84</cell><cell cols="3">29.02 29.53 43.79</cell><cell>9.88</cell><cell cols="3">36.09 16.12 37.80</cell><cell>5.52</cell><cell>27.61</cell></row><row><cell>soft</cell><cell></cell><cell></cell><cell cols="3">NLL 14.12 33.38</cell><cell>7.47</cell><cell cols="3">22.71 30.96 41.80</cell><cell>9.26</cell><cell cols="3">35.75 6.42 33.81</cell><cell>4.15</cell><cell>21.48</cell></row><row><cell>manual1</cell><cell></cell><cell>first</cell><cell></cell><cell>2.43</cell><cell>7.70</cell><cell>1.86</cell><cell>6.54</cell><cell>0.29</cell><cell>6.85</cell><cell>1.44</cell><cell>4.92</cell><cell>0.71</cell><cell>1.57</cell><cell>0.95</cell><cell>2.64</cell></row><row><cell>manual2</cell><cell></cell><cell></cell><cell>-</cell><cell cols="2">8.84 21.83</cell><cell>2.54</cell><cell cols="3">16.34 7.56 20.54</cell><cell>2.29</cell><cell cols="3">14.89 4.28 12.98</cell><cell>1.43</cell><cell>9.88</cell></row><row><cell>manual3</cell><cell></cell><cell></cell><cell></cell><cell cols="2">7.28 26.25</cell><cell>4.61</cell><cell cols="3">17.40 13.98 33.52</cell><cell>3.80</cell><cell cols="3">21.68 7.28 13.98</cell><cell>2.10</cell><cell>12.37</cell></row><row><cell>soft</cell><cell></cell><cell></cell><cell cols="3">log 16.12 28.82</cell><cell>5.46</cell><cell cols="3">20.46 27.25 41.80</cell><cell>5.67</cell><cell cols="3">28.69 24.54 34.38</cell><cell>4.38</cell><cell>26.72</cell></row><row><cell>soft manual1</cell><cell>s</cell><cell>max</cell><cell cols="3">NLL 22.40 32.10 0.86 3.00</cell><cell>5.80 1.94</cell><cell cols="3">20.81 32.24 44.22 3.53 0.86 2.28</cell><cell>7.38 1.65</cell><cell cols="3">29.42 10.13 23.25 2.79 0.14 4.71</cell><cell>3.09 1.13</cell><cell>16.90 2.16</cell></row><row><cell>manual2</cell><cell></cell><cell></cell><cell>-</cell><cell cols="2">9.70 20.40</cell><cell>3.19</cell><cell cols="3">12.90 9.27 20.54</cell><cell>3.83</cell><cell cols="3">13.09 3.14 12.98</cell><cell>1.81</cell><cell>7.30</cell></row><row><cell>manual3</cell><cell></cell><cell></cell><cell></cell><cell cols="2">6.99 14.69</cell><cell>3.95</cell><cell cols="3">10.95 13.98 21.83</cell><cell>4.76</cell><cell cols="3">14.10 1.57 12.27</cell><cell>1.35</cell><cell>6.09</cell></row><row><cell>soft</cell><cell></cell><cell></cell><cell cols="3">log 23.11 42.51</cell><cell>8.80</cell><cell cols="7">32.21 37.95 55.49 13.29 46.45 19.83 41.37</cell><cell>8.50</cell><cell>29.48</cell></row><row><cell>soft</cell><cell></cell><cell></cell><cell cols="3">NLL 8.13 25.25</cell><cell>5.52</cell><cell cols="3">17.82 36.09 55.92</cell><cell>9.90</cell><cell cols="3">45.37 17.55 36.09</cell><cell>7.25</cell><cell>26.81</cell></row><row><cell>manual1</cell><cell></cell><cell>mean</cell><cell></cell><cell cols="2">7.56 18.97</cell><cell>3.44</cell><cell cols="3">14.54 2.71 15.55</cell><cell>2.44</cell><cell>9.92</cell><cell>1.14</cell><cell>4.42</cell><cell>1.85</cell><cell>4.40</cell></row><row><cell>manual2</cell><cell></cell><cell></cell><cell>-</cell><cell cols="2">9.56 28.67</cell><cell>3.36</cell><cell cols="3">19.58 8.84 25.39</cell><cell>3.09</cell><cell cols="3">18.75 2.00 10.27</cell><cell>2.60</cell><cell>8.06</cell></row><row><cell>manual3</cell><cell></cell><cell></cell><cell></cell><cell cols="2">8.42 34.52</cell><cell>4.80</cell><cell cols="3">21.78 15.12 41.80</cell><cell>5.24</cell><cell cols="3">27.74 6.70 17.12</cell><cell>4.24</cell><cell>13.44</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">BERT-LARGE-CASED</cell><cell cols="4">BERT-LARGE-UNCASED</cell><cell></cell><cell cols="2">RoBERTa-LARGE</cell></row><row><cell>soft</cell><cell></cell><cell></cell><cell cols="3">log 15.41 41.94</cell><cell>6.05</cell><cell cols="3">26.93 28.82 44.79</cell><cell>5.28</cell><cell cols="3">36.93 16.69 26.82</cell><cell>3.69</cell><cell>21.80</cell></row><row><cell>soft</cell><cell></cell><cell></cell><cell cols="3">NLL 20.40 43.94</cell><cell>6.80</cell><cell cols="3">32.20 25.68 43.22</cell><cell>6.14</cell><cell cols="3">34.56 10.56 21.40</cell><cell>3.42</cell><cell>16.51</cell></row><row><cell>manual1</cell><cell></cell><cell>first</cell><cell></cell><cell cols="2">4.14 11.70</cell><cell>1.73</cell><cell>9.27</cell><cell>2.43</cell><cell>9.70</cell><cell>1.66</cell><cell>7.29</cell><cell>0.71</cell><cell>3.99</cell><cell>1.00</cell><cell>3.53</cell></row><row><cell>manual2</cell><cell></cell><cell></cell><cell>-</cell><cell cols="2">5.71 23.97</cell><cell>2.60</cell><cell cols="3">14.90 6.99 22.97</cell><cell>2.25</cell><cell cols="3">15.16 5.99 19.97</cell><cell>2.60</cell><cell>13.88</cell></row><row><cell>manual3</cell><cell></cell><cell></cell><cell></cell><cell cols="2">13.98 37.80</cell><cell>5.56</cell><cell cols="3">24.04 5.42 18.54</cell><cell>2.46</cell><cell cols="3">11.78 9.13 26.11</cell><cell>3.02</cell><cell>17.96</cell></row><row><cell>soft</cell><cell></cell><cell></cell><cell cols="3">log 21.68 36.38</cell><cell>5.03</cell><cell cols="3">28.70 28.82 40.37</cell><cell>6.04</cell><cell cols="3">34.92 11.41 24.82</cell><cell>3.16</cell><cell>17.69</cell></row><row><cell>soft manual1</cell><cell>m</cell><cell>max</cell><cell cols="3">NLL 5.71 20.11 1.85 8.70</cell><cell>4.21 1.75</cell><cell cols="3">14.18 12.55 22.97 6.51 1.43 4.99</cell><cell>4.69 2.11</cell><cell cols="3">18.47 6.99 13.98 3.99 0.14 2.43</cell><cell>3.80 0.86</cell><cell>11.41 2.13</cell></row><row><cell>manual2</cell><cell></cell><cell></cell><cell>-</cell><cell cols="2">5.85 12.98</cell><cell>2.11</cell><cell cols="3">8.79 11.55 25.53</cell><cell>2.73</cell><cell cols="2">13.79 0.86</cell><cell>4.99</cell><cell>1.00</cell><cell>3.36</cell></row><row><cell>manual3</cell><cell></cell><cell></cell><cell></cell><cell cols="2">7.13 21.54</cell><cell>3.78</cell><cell cols="2">15.57 2.57</cell><cell>8.27</cell><cell>2.57</cell><cell>5.92</cell><cell>1.14</cell><cell>5.14</cell><cell>1.16</cell><cell>3.40</cell></row><row><cell>soft</cell><cell></cell><cell></cell><cell cols="3">log 24.25 38.66</cell><cell>4.99</cell><cell cols="3">31.50 21.40 42.80</cell><cell>5.04</cell><cell cols="3">32.11 22.68 42.51</cell><cell>5.54</cell><cell>32.74</cell></row><row><cell>soft</cell><cell></cell><cell></cell><cell cols="3">NLL 22.40 42.65</cell><cell>6.04</cell><cell cols="3">32.95 30.96 53.50</cell><cell>7.99</cell><cell cols="3">41.91 22.82 42.80</cell><cell>4.77</cell><cell>32.89</cell></row><row><cell>manual1</cell><cell></cell><cell>mean</cell><cell></cell><cell cols="2">5.14 21.68</cell><cell>2.50</cell><cell cols="3">13.72 3.57 17.40</cell><cell>2.56</cell><cell cols="2">10.83 2.00</cell><cell>7.42</cell><cell>1.81</cell><cell>5.88</cell></row><row><cell>manual2</cell><cell></cell><cell></cell><cell>-</cell><cell cols="2">8.27 31.38</cell><cell>3.69</cell><cell cols="3">19.77 9.27 35.38</cell><cell>3.15</cell><cell cols="3">21.89 2.43 12.55</cell><cell>2.36</cell><cell>8.65</cell></row><row><cell>manual3</cell><cell></cell><cell></cell><cell></cell><cell cols="2">10.27 43.79</cell><cell>6.71</cell><cell cols="3">25.80 3.99 18.83</cell><cell>3.39</cell><cell cols="3">12.79 6.13 16.83</cell><cell>3.50</cell><cell>13.53</cell></row><row><cell>soft</cell><cell></cell><cell></cell><cell cols="3">log 30.96 47.65</cell><cell>7.24</cell><cell cols="3">35.95 24.82 46.08</cell><cell>8.83</cell><cell cols="3">33.06 6.56 12.70</cell><cell>2.33</cell><cell>11.33</cell></row><row><cell>soft</cell><cell></cell><cell></cell><cell cols="3">NLL 34.95 49.22</cell><cell>8.96</cell><cell cols="3">37.90 25.25 44.08</cell><cell>8.90</cell><cell cols="3">32.90 9.70 26.25</cell><cell>2.92</cell><cell>19.40</cell></row><row><cell>manual1</cell><cell></cell><cell>first</cell><cell></cell><cell>1.57</cell><cell>4.56</cell><cell>1.80</cell><cell>5.56</cell><cell>1.85</cell><cell>8.42</cell><cell>1.98</cell><cell>6.92</cell><cell>1.43</cell><cell>7.28</cell><cell>1.26</cell><cell>5.85</cell></row><row><cell>manual2</cell><cell></cell><cell></cell><cell>-</cell><cell cols="2">7.56 20.83</cell><cell>3.15</cell><cell cols="3">16.15 7.42 23.40</cell><cell>2.61</cell><cell cols="2">15.53 2.43</cell><cell>7.99</cell><cell>1.41</cell><cell>7.77</cell></row><row><cell>manual3</cell><cell></cell><cell></cell><cell></cell><cell cols="2">9.70 31.38</cell><cell>4.94</cell><cell cols="3">20.07 3.71 16.12</cell><cell>2.47</cell><cell cols="3">11.34 12.27 39.80</cell><cell>3.21</cell><cell>24.56</cell></row><row><cell>soft</cell><cell></cell><cell></cell><cell cols="3">log 32.38 46.65</cell><cell>8.60</cell><cell cols="3">30.89 20.54 33.10</cell><cell>5.41</cell><cell cols="3">25.27 12.13 22.82</cell><cell>3.20</cell><cell>16.22</cell></row><row><cell>soft manual1</cell><cell>s</cell><cell>max</cell><cell cols="3">NLL 32.38 44.22 0.29 2.28</cell><cell>7.81 1.61</cell><cell cols="2">26.46 0.00 3.19 2.28</cell><cell>1.43 5.28</cell><cell>0.78 2.65</cell><cell>1.63 4.60</cell><cell cols="2">8.56 17.69 0.14 4.85</cell><cell>2.14 0.98</cell><cell>13.28 2.25</cell></row><row><cell>manual2</cell><cell></cell><cell></cell><cell>-</cell><cell cols="2">7.99 18.83</cell><cell>3.97</cell><cell cols="3">12.91 11.13 24.11</cell><cell>3.72</cell><cell cols="3">13.35 4.14 12.70</cell><cell>1.96</cell><cell>6.97</cell></row><row><cell>manual3</cell><cell></cell><cell></cell><cell></cell><cell cols="2">7.85 16.83</cell><cell>4.81</cell><cell cols="2">11.86 3.71</cell><cell>5.85</cell><cell>2.64</cell><cell>5.59</cell><cell cols="2">5.14 20.11</cell><cell>2.20</cell><cell>9.66</cell></row><row><cell>soft</cell><cell></cell><cell></cell><cell cols="3">log 28.96 50.50</cell><cell>9.16</cell><cell cols="3">39.13 19.26 44.79</cell><cell>6.02</cell><cell cols="3">30.95 14.27 36.95</cell><cell>5.14</cell><cell>25.52</cell></row><row><cell>soft</cell><cell></cell><cell></cell><cell cols="3">NLL 29.53 54.64</cell><cell>9.70</cell><cell cols="3">41.18 20.11 31.38</cell><cell>5.34</cell><cell cols="3">26.79 8.27 22.97</cell><cell>4.71</cell><cell>16.72</cell></row><row><cell>manual1</cell><cell></cell><cell>mean</cell><cell></cell><cell cols="2">5.99 21.54</cell><cell>2.36</cell><cell cols="3">14.33 3.85 19.83</cell><cell>3.05</cell><cell cols="2">11.94 1.57</cell><cell>8.27</cell><cell>2.79</cell><cell>6.34</cell></row><row><cell>manual2</cell><cell></cell><cell></cell><cell>-</cell><cell cols="2">9.27 29.81</cell><cell>3.77</cell><cell cols="3">19.62 8.42 33.24</cell><cell>3.45</cell><cell cols="3">19.95 1.85 12.41</cell><cell>2.90</cell><cell>8.24</cell></row><row><cell>manual3</cell><cell></cell><cell></cell><cell></cell><cell cols="2">11.55 37.95</cell><cell>5.68</cell><cell cols="3">24.57 3.71 24.96</cell><cell>3.42</cell><cell cols="3">14.69 4.56 21.40</cell><cell>4.45</cell><cell>13.97</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9 :</head><label>9</label><figDesc>SCO results.    </figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">BERT-BASE-CASED</cell><cell cols="3">BERT-BASE-UNCASED</cell><cell>RoBERTa-BASE</cell></row><row><cell cols="11">Template Masks Pooling Loss R@1 R@5 MRRa MRR R@1 R@5 MRRa MRR R@1 R@5 MRRa MRR</cell></row><row><cell>soft</cell><cell></cell><cell></cell><cell cols="4">log 20.51 43.59 15.37</cell><cell>32</cell><cell cols="3">20.51 61.54 19.41 36.06 7.69 43.59 11.31 20.65</cell></row><row><cell>soft</cell><cell></cell><cell>first</cell><cell cols="8">NLL 23.08 38.46 15.44 33.36 20.51 58.97 18.61 37.51 2.56 43.59 11.09 21.63</cell></row><row><cell>manual</cell><cell></cell><cell></cell><cell>-</cell><cell cols="2">20.51 58.97</cell><cell>13.5</cell><cell cols="4">34.67 17.95 48.72 16.15 32.42 10.26 25.64</cell><cell>8.77</cell><cell>20.34</cell></row><row><cell>soft</cell><cell></cell><cell></cell><cell cols="3">log 23.08 64.1</cell><cell cols="5">20.93 43.68 28.21 58.97 25.12 44.43 12.82 28.21 12.02 18.46</cell></row><row><cell>soft manual</cell><cell>m</cell><cell>max</cell><cell cols="3">NLL 20.51 64.1 -7.69 25.64</cell><cell cols="3">21.13 39.21 38.46 58.97 9.47 19.56 7.69 35.9</cell><cell>22.5 9.26</cell><cell>45.98 15.38 35.9 21.12 0 10.26</cell><cell>15.03 27.21 4.51 7.27</cell></row><row><cell>soft</cell><cell></cell><cell></cell><cell cols="3">log 17.95 64.1</cell><cell cols="5">20.97 35.62 38.46 71.79 29.32 53.51 35.9 61.54 22.23 47.35</cell></row><row><cell>soft</cell><cell></cell><cell>mean</cell><cell cols="8">NLL 25.64 51.28 21.33 38.26 28.21 74.36 25.87 47.12 33.33 61.54 25.12 46.47</cell></row><row><cell>manual</cell><cell></cell><cell></cell><cell>-</cell><cell cols="2">23.08 64.1</cell><cell cols="5">15.81 39.17 17.95 69.23 19.48 38.11 10.26 25.64</cell><cell>7.91</cell><cell>18.72</cell></row><row><cell>soft</cell><cell></cell><cell></cell><cell cols="3">log 15.38 35.9</cell><cell cols="5">18.38 29.45 28.21 58.97 20.01 34.91 20.51 35.9</cell><cell>14.42 28.34</cell></row><row><cell>soft</cell><cell></cell><cell>first</cell><cell cols="3">NLL 25.64 41.03</cell><cell>15.8</cell><cell cols="3">31.25 25.64 51.28 17.82</cell><cell>33.3 20.51 43.59 15.67 26.17</cell></row><row><cell>manual</cell><cell></cell><cell></cell><cell>-</cell><cell cols="7">20.51 53.85 13.12 33.99 17.95 61.54 16.75 35.37 10.26 33.33</cell><cell>8.35</cell><cell>20.6</cell></row><row><cell>soft</cell><cell></cell><cell></cell><cell cols="3">log 30.77 64.1</cell><cell cols="3">22.04 42.89 20.51 35.9</cell><cell cols="2">12.16 27.34 20.51 33.33 12.14 24.43</cell></row><row><cell>soft manual</cell><cell>s</cell><cell>max</cell><cell cols="8">NLL 38.46 48.72 21.48 43.04 17.95 33.33 12.48 27.17 20.51 28.21 11.93 27.11 -20.51 43.59 10.66 27.05 15.38 51.28 16.53 33.61 0 7.69 3.28 8.36</cell></row><row><cell>soft</cell><cell></cell><cell></cell><cell cols="3">log 30.77 64.1</cell><cell cols="3">23.81 42.82 23.08 56.41</cell><cell>21.8</cell><cell>39.4 33.33 61.54 23.05 46.92</cell></row><row><cell>soft</cell><cell></cell><cell>mean</cell><cell cols="3">NLL 20.51 48.72</cell><cell>17</cell><cell cols="4">33.75 20.51 56.41 23.15 37.13 30.77 61.54 22.35 44.84</cell></row><row><cell>manual</cell><cell></cell><cell></cell><cell>-</cell><cell cols="2">20.51 53.85</cell><cell>13.3</cell><cell cols="4">34.31 20.51 61.54 17.85 38.29 7.69 20.51</cell><cell>7.21</cell><cell>16.25</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">BERT-LARGE-CASED</cell><cell cols="3">BERT-LARGE-UNCASED</cell><cell>RoBERTa-LARGE</cell></row><row><cell>soft</cell><cell></cell><cell></cell><cell cols="8">log 30.77 61.54 20.95 41.45 15.38 53.85 18.47 30.22 17.95 43.59 16.03 26.64</cell></row><row><cell>soft</cell><cell></cell><cell>first</cell><cell cols="8">NLL 38.46 56.41 16.24 32.73 30.77 56.41 20.75 34.34 15.38 33.33 13.59 25.34</cell></row><row><cell>manual</cell><cell></cell><cell></cell><cell>-</cell><cell cols="7">10.26 51.28 15.19 28.84 15.38 43.59 14.99 28.95 7.69 30.77</cell><cell>7.99</cell><cell>21.87</cell></row><row><cell>soft</cell><cell></cell><cell></cell><cell cols="8">log 28.21 58.97 23.35 42.49 25.64 46.15 17.68 37.26 17.95 46.15 11.04</cell><cell>27.7</cell></row><row><cell>soft manual</cell><cell>m</cell><cell>max</cell><cell cols="8">NLL 10.26 43.59 13.35 26.44 23.08 56.41 17.96 39.64 17.95 51.28 -7.69 41.03 11.76 23.03 12.82 35.9 11.38 26.06 0 2.56</cell><cell>9.54 3.99</cell><cell>27.72 7.27</cell></row><row><cell>soft</cell><cell></cell><cell></cell><cell cols="6">log 28.21 76.92 28.31 49.83 41.03 64.1</cell><cell cols="2">28.83 52.91 23.08 51.28 17.42 34.65</cell></row><row><cell>soft</cell><cell></cell><cell>mean</cell><cell cols="2">NLL 35.9</cell><cell>64.1</cell><cell>29.8</cell><cell cols="2">48.47 38.46 64.1</cell><cell>26.7</cell><cell>49.25 25.64 35.9</cell><cell>13.9</cell><cell>33.24</cell></row><row><cell>manual</cell><cell></cell><cell></cell><cell>-</cell><cell cols="7">10.26 58.97 19.52 33.82 20.51 69.23 18.97 39.31 5.13 23.08</cell><cell>7.45</cell><cell>17.07</cell></row><row><cell>soft</cell><cell></cell><cell></cell><cell cols="3">log 30.77 64.1</cell><cell cols="5">17.89 32.48 20.51 61.54 16.96 32.09 15.38 51.28 16.87 30.19</cell></row><row><cell>soft</cell><cell></cell><cell>first</cell><cell cols="6">NLL 30.77 53.85 13.78 25.24 5.13 12.82</cell><cell>3.87</cell><cell>13.48 17.95 43.59 14.73 24.36</cell></row><row><cell>manual</cell><cell></cell><cell></cell><cell>-</cell><cell cols="3">15.38 53.85 15.59</cell><cell cols="4">33.3 23.08 48.72 14.41 33.99 10.26 30.77 10.02 21.01</cell></row><row><cell>soft</cell><cell></cell><cell></cell><cell cols="7">log 25.64 43.59 17.92 33.47 20.51 46.15 16.76</cell><cell>31.9 15.38 43.59</cell><cell>9.44</cell><cell>25.14</cell></row><row><cell>soft manual</cell><cell>s</cell><cell>max</cell><cell cols="8">NLL 25.64 23.08 11.77 -17.95 53.85 14.78 29.56 20.51 51.28 14.68 33.25 2.56 10.26 29.8 33.33 56.41 22.49 44.09 15.38 38.46</cell><cell>9.82 4.28</cell><cell>26.53 11.03</cell></row><row><cell>soft</cell><cell></cell><cell></cell><cell cols="8">log 33.33 58.97 24.04 44.57 17.95 56.41 18.76 36.52 33.33 69.23 22.07 47.02</cell></row><row><cell>soft</cell><cell></cell><cell>mean</cell><cell cols="6">NLL 23.08 58.97 20.72 40.35 23.08 64.1</cell><cell cols="2">21.55 39.82 41.03 69.23 29.61 53.77</cell></row><row><cell>manual</cell><cell></cell><cell></cell><cell>-</cell><cell cols="7">15.38 56.41 17.15 34.53 17.95 53.85 15.81 34.43 10.26 20.51</cell><cell>7.43</cell><cell>19.49</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 10 :</head><label>10</label><figDesc>SPO results.   .00 60.00 39.13 39.13 10.00 56.67 29.65 29.65 43.33 53.33 48.18  48.18 soft NLL 20.00 56.67 39.01 39.01 6.67 50.00 25.47 25.47 20.00 56.67 36.26 36.26 manual s mean -43.33 53.33 47.63 47.63 43.33 53.33 49.34 49.34 6.67 20.00 15.31 15.31 .00 50.18 50.18 33.33 53.33 43.17 43.17 3.33 16.67 11.58 11.58</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">BERT-BASE-CASED</cell><cell cols="3">BERT-BASE-UNCASED</cell><cell>RoBERTa-BASE</cell></row><row><cell cols="10">Template Masks Pooling Loss R@1 R@5 MRRa MRR R@1 R@5 MRRa MRR R@1 R@5 MRRa MRR</cell></row><row><cell>soft</cell><cell></cell><cell></cell><cell cols="7">log 30.00 56.67 39.28 39.28 10.00 36.67 23.14 23.14 20.00 63.33 39.59 39.59</cell></row><row><cell>soft</cell><cell></cell><cell>first</cell><cell cols="7">NLL 3.33 13.33 11.03 11.03 6.67 20.00 10.17 10.17 20.00 63.33 38.52 38.52</cell></row><row><cell>manual</cell><cell></cell><cell></cell><cell>-</cell><cell cols="6">40.00 46.67 44.06 44.06 43.33 46.67 46.65 46.65 0.00</cell><cell>3.33</cell><cell>3.26</cell><cell>3.26</cell></row><row><cell>soft</cell><cell></cell><cell></cell><cell>log</cell><cell>3.33 10.00</cell><cell>8.38</cell><cell cols="4">8.38 30.00 43.33 36.96 36.96 30.00 43.33 37.14 37.14</cell></row><row><cell>soft manual</cell><cell>m</cell><cell>max</cell><cell cols="7">NLL 0.00 -33.33 46.67 39.34 39.34 40.00 46.67 43.32 43.32 0.00 0.00 2.45 2.45 20.00 26.67 23.66 23.66 13.33 16.67 16.62 16.62 0.00 0.46 0.46</cell></row><row><cell>soft</cell><cell></cell><cell></cell><cell cols="7">log 23.33 60.00 40.66 40.66 40.00 63.33 50.02 50.02 40.00 60.00 49.00 49.00</cell></row><row><cell>soft</cell><cell></cell><cell>mean</cell><cell cols="7">NLL 13.33 46.67 29.67 29.67 30.00 43.33 38.77 38.77 13.33 53.33 32.36 32.36</cell></row><row><cell>manual</cell><cell></cell><cell></cell><cell>-</cell><cell cols="6">43.33 50.00 46.91 46.91 43.33 53.33 48.65 48.65 0.00</cell><cell>3.33</cell><cell>4.00</cell><cell>4.00</cell></row><row><cell>soft</cell><cell></cell><cell></cell><cell cols="7">log 16.67 53.33 32.13 32.13 20.00 50.00 27.56 27.56 10.00 30.00 18.79 18.79</cell></row><row><cell>soft</cell><cell></cell><cell>first</cell><cell cols="7">NLL 13.33 43.33 27.50 27.50 13.33 36.67 25.53 25.53 10.00 26.67 18.12 18.12</cell></row><row><cell>manual</cell><cell></cell><cell></cell><cell>-</cell><cell cols="6">43.33 50.00 36.40 36.40 40.00 53.33 39.41 39.41 3.33</cell><cell>6.67</cell><cell>7.56</cell><cell>7.56</cell></row><row><cell>soft</cell><cell></cell><cell></cell><cell cols="7">log 13.33 16.67 10.49 10.49 30.00 40.00 15.58 15.58 3.33</cell><cell>3.33</cell><cell>3.22</cell><cell>3.22</cell></row><row><cell>soft</cell><cell></cell><cell>max</cell><cell cols="6">NLL 10.00 16.67 11.80 11.80 3.33 10.00</cell><cell>6.84</cell><cell>6.84</cell><cell>3.33</cell><cell>3.33</cell><cell>4.01</cell><cell>4.01</cell></row><row><cell>manual</cell><cell></cell><cell></cell><cell>-</cell><cell cols="6">40.00 46.67 20.30 20.30 43.33 50.00 19.99 19.99 0.00</cell><cell>0.00</cell><cell>0.81</cell><cell>0.81</cell></row><row><cell>soft</cell><cell></cell><cell></cell><cell cols="4">log 20BERT-LARGE-CASED</cell><cell cols="3">BERT-LARGE-UNCASED</cell><cell>RoBERTa-LARGE</cell></row><row><cell>soft</cell><cell></cell><cell></cell><cell cols="5">log 40.00 60.00 48.67 48.67 0.00</cell><cell>6.67</cell><cell>3.77</cell><cell>3.77</cell><cell>6.67 13.33 10.23 10.23</cell></row><row><cell>soft</cell><cell></cell><cell>first</cell><cell cols="7">NLL 16.67 30.00 24.71 24.71 26.67 40.00 33.48 33.48 13.33 16.67 15.05 15.05</cell></row><row><cell>manual</cell><cell></cell><cell></cell><cell>-</cell><cell cols="6">33.33 50.00 42.28 42.28 30.00 46.67 39.19 39.19 0.00</cell><cell>0.00</cell><cell>3.75</cell><cell>3.75</cell></row><row><cell>soft</cell><cell></cell><cell></cell><cell cols="7">log 23.33 33.33 29.60 29.60 13.33 26.67 19.89 19.89 0.00 13.33</cell><cell>5.41</cell><cell>5.41</cell></row><row><cell>soft manual</cell><cell>m</cell><cell>max</cell><cell cols="7">NLL 20.00 43.33 29.44 29.44 6.67 13.33 10.59 10.59 0.00 -33.33 43.33 38.52 38.52 23.33 36.67 30.94 30.94 0.00</cell><cell>0.00 0.00</cell><cell>0.60 0.36</cell><cell>0.60 0.36</cell></row><row><cell>soft</cell><cell></cell><cell></cell><cell cols="7">log 26.67 56.67 39.09 39.09 6.67 26.67 15.42 15.42 13.33 30.00 23.08 23.08</cell></row><row><cell>soft</cell><cell></cell><cell>mean</cell><cell cols="7">NLL 36.67 63.33 48.11 48.11 6.67 13.33 10.15 10.15 10.00 50.00 25.53 25.53</cell></row><row><cell>manual</cell><cell></cell><cell></cell><cell>-</cell><cell cols="6">46.67 50.00 50.33 50.33 30.00 53.33 41.43 41.43 0.00</cell><cell>0.00</cell><cell>1.73</cell><cell>1.73</cell></row><row><cell>soft</cell><cell></cell><cell></cell><cell cols="7">log 30.00 56.67 34.30 34.30 6.67 16.67 13.42 13.42 10.00 13.33 14.15 14.15</cell></row><row><cell>soft</cell><cell></cell><cell>first</cell><cell cols="7">NLL 16.67 46.67 30.00 30.00 26.67 50.00 32.20 32.20 6.67</cell><cell>6.67</cell><cell>8.50</cell><cell>8.50</cell></row><row><cell>manual</cell><cell></cell><cell></cell><cell>-</cell><cell cols="6">40.00 50.00 30.83 30.83 33.33 50.00 32.08 32.08 13.33 46.67 27.36 27.36</cell></row><row><cell>soft</cell><cell></cell><cell></cell><cell cols="5">log 10.00 23.33 11.55 11.55 6.67</cell><cell>6.67</cell><cell>8.69</cell><cell>8.69</cell><cell>0.00</cell><cell>0.00</cell><cell>0.34</cell><cell>0.34</cell></row><row><cell>soft manual</cell><cell>s</cell><cell>max</cell><cell cols="7">NLL 23.33 36.67 16.67 16.67 6.67 10.00 -40.00 50.00 18.87 18.87 30.00 43.33 16.04 16.04 0.00 6.69 6.69 16.67 20.00 17.79 17.79 3.33 1.31 1.31</cell></row><row><cell>soft</cell><cell></cell><cell></cell><cell cols="7">log 30.00 53.33 40.98 40.98 20.00 46.67 32.22 32.22 0.00 10.00</cell><cell>5.54</cell><cell>5.54</cell></row><row><cell>soft</cell><cell></cell><cell>mean</cell><cell cols="6">NLL 26.67 53.33 38.80 38.80 6.67 10.00</cell><cell>9.26</cell><cell>9.26 10.00 23.33 18.06 18.06</cell></row><row><cell>manual</cell><cell></cell><cell></cell><cell>-</cell><cell>46.67 50</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 11 :</head><label>11</label><figDesc>DM results.   .14 64.29 62.72 62.72 57.14 71.43 63.93 63.93  39.29 53.57 46.41 46.41 soft NLL 46.43 67.86 56.19 56.19 53.57 71.43 62.39 62.39 32.14 53.57 42.03 42.03 manual</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">BERT-BASE-CASED</cell><cell cols="2">BERT-BASE-UNCASED</cell><cell>RoBERTa-BASE</cell></row><row><cell cols="10">Template Masks Pooling Loss R@1 R@5 MRRa MRR R@1 R@5 MRRa MRR R@1 R@5 MRRa MRR</cell></row><row><cell>soft</cell><cell></cell><cell></cell><cell cols="7">log 42.86 53.57 51.49 51.49 46.43 67.86 55.89 55.89 39.29 53.57 44.34 44.34</cell></row><row><cell>soft</cell><cell></cell><cell>first</cell><cell cols="7">NLL 53.57 60.71 58.18 58.18 46.43 64.29 55.87 55.87 32.14</cell><cell>50</cell><cell>40.67 40.67</cell></row><row><cell>manual</cell><cell></cell><cell></cell><cell>-</cell><cell cols="6">39.29 60.71 48.89 48.89 28.57 67.86 44.74 44.74 10.71 39.29</cell><cell>22.7</cell><cell>22.7</cell></row><row><cell>soft</cell><cell></cell><cell></cell><cell cols="3">log 35.71 57.14</cell><cell>45.6</cell><cell cols="3">45.6 39.29 71.43</cell><cell>53.1</cell><cell>53.1 17.86 46.43 30.01 30.01</cell></row><row><cell>soft manual</cell><cell>m</cell><cell>max</cell><cell cols="7">NLL 17.86 -46.43 57.14 48.64 48.64 32.14 57.14 39.43 39.43 50 34.58 34.58 42.86 50 47.38 47.38 39.29 46.43 43.27 43.27 0 0 1.23 1.23</cell></row><row><cell>soft</cell><cell></cell><cell></cell><cell cols="6">log 42.86 60.71 50.87 50.87 46.43</cell><cell>75</cell><cell>58.74 58.74 46.43</cell><cell>50</cell><cell>50.32 50.32</cell></row><row><cell>soft</cell><cell></cell><cell>mean</cell><cell>NLL</cell><cell>50</cell><cell cols="5">67.86 57.81 57.81 46.43 67.86 57.86 57.86 39.29 53.57 47.69 47.69</cell></row><row><cell>manual</cell><cell></cell><cell></cell><cell>-</cell><cell cols="5">46.43 67.86 59.08 59.08 42.86</cell><cell>75</cell><cell>55.97 55.97 14.29 32.14 23.52 23.52</cell></row><row><cell>soft</cell><cell></cell><cell></cell><cell cols="7">log 46.43 60.71 53.94 53.94 42.86 60.71</cell><cell>41.7</cell><cell>41.7 35.71 46.43</cell><cell>38.4</cell><cell>38.4</cell></row><row><cell>soft</cell><cell></cell><cell>first</cell><cell>NLL</cell><cell>50</cell><cell cols="5">64.29 52.59 52.59 42.86 67.86 39.37 39.37 28.57 53.57 38.78 38.78</cell></row><row><cell>manual</cell><cell></cell><cell></cell><cell>-</cell><cell cols="6">42.86 60.71 42.73 42.73 39.29 64.29 35.56 35.56 14.29 35.71 25.13 25.13</cell></row><row><cell>soft</cell><cell></cell><cell></cell><cell cols="7">log 39.29 53.57 26.05 26.05 35.71 57.14</cell><cell>23.6</cell><cell>23.6 32.14 35.71 34.04 34.04</cell></row><row><cell>soft manual soft</cell><cell cols="9">NLL 39.29 -42.86 60.71 30.76 30.76 42.86 53.57 24.73 24.73 3.57 50 27.73 27.73 42.86 46.43 26.98 26.98 32.14 42.86 3.57 log 57s max</cell><cell>36.1 4.78</cell><cell>36.1 4.78</cell></row><row><cell></cell><cell></cell><cell>mean</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>-</cell><cell cols="5">42.86 67.86 56.61 56.61 39.29</cell><cell>75</cell><cell>53.48 53.48 32.14 57.14 43.97 43.97</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">BERT-LARGE-CASED</cell><cell cols="2">BERT-LARGE-UNCASED</cell><cell>RoBERTa-LARGE</cell></row><row><cell>soft</cell><cell></cell><cell></cell><cell cols="2">log 57.14</cell><cell>75</cell><cell cols="2">66.24 66.24</cell><cell>0</cell><cell>0</cell><cell>1.53</cell><cell>1.53 35.71 60.71 46.25 46.25</cell></row><row><cell>soft</cell><cell></cell><cell>first</cell><cell cols="7">NLL 53.57 67.86 61.64 61.64 35.71 67.86 49.25 49.25 28.57 67.86 44.01 44.01</cell></row><row><cell>manual</cell><cell></cell><cell></cell><cell>-</cell><cell cols="6">46.43 67.86 56.92 56.92 39.29 71.43 51.73 51.73</cell><cell>0</cell><cell>14.29</cell><cell>8.29</cell><cell>8.29</cell></row><row><cell>soft</cell><cell></cell><cell></cell><cell>log</cell><cell>50</cell><cell>75</cell><cell cols="4">60.48 60.48 35.71 57.14 44.92 44.92</cell><cell>25</cell><cell>32.14 28.94 28.94</cell></row><row><cell>soft manual</cell><cell>m</cell><cell>max</cell><cell cols="7">NLL 17.86 60.71 37.55 37.55 28.57 67.86 41.38 41.38 -46.43 57.14 52.08 52.08 39.29 60.71 46.45 46.45</cell><cell>0 0</cell><cell>3.57 0</cell><cell>1.51 1.01</cell><cell>1.51 1.01</cell></row><row><cell>soft</cell><cell></cell><cell></cell><cell cols="7">log 53.57 67.86 60.88 60.88 28.57 46.43 39.13 39.13 35.71 57.14</cell><cell>45.3</cell><cell>45.3</cell></row><row><cell>soft</cell><cell></cell><cell>mean</cell><cell>NLL</cell><cell>50</cell><cell cols="4">71.43 60.42 60.42 46.43</cell><cell>75</cell><cell>59.46 59.46 32.14 67.86 46.06 46.06</cell></row><row><cell>manual</cell><cell></cell><cell></cell><cell>-</cell><cell cols="6">57.14 78.57 66.82 66.82 46.43 78.57 61.06 61.06 7.14 17.86 14.74 14.74</cell></row><row><cell>soft</cell><cell></cell><cell></cell><cell cols="3">log 46.43 60.71</cell><cell>45.3</cell><cell cols="3">45.3 32.14 60.71 38.88 38.88 42.86 53.57 44.55 44.55</cell></row><row><cell>soft</cell><cell></cell><cell>first</cell><cell cols="7">NLL 39.29 71.43 51.73 51.73 53.57 67.86 44.62 44.62</cell><cell>25</cell><cell>28.57</cell><cell>26.3</cell><cell>26.3</cell></row><row><cell>manual</cell><cell></cell><cell></cell><cell>-</cell><cell>50</cell><cell cols="5">67.86 53.54 53.54 42.86 71.43 41.39 41.39 10.71 53.57 29.46 29.46</cell></row><row><cell>soft</cell><cell></cell><cell></cell><cell cols="3">log 42.86 64.29</cell><cell>31.5</cell><cell>31.5</cell><cell>25</cell><cell>42.86 18.64 18.64 17.86 42.86 27.04 27.04</cell></row><row><cell>soft manual</cell><cell>s</cell><cell>max</cell><cell cols="7">NLL 42.86 57.14 -42.86 67.86 32.89 32.89 46.43 53.57 31.91 31.91 3.57 28.5 28.5 0 7.14 2.26 2.26 25</cell><cell>50 7.14</cell><cell>32.22 32.22 2.35 2.35</cell></row><row><cell>soft</cell><cell></cell><cell></cell><cell>log</cell><cell>0</cell><cell>0</cell><cell>1.71</cell><cell cols="3">1.71 46.43 64.29 55.81 55.81 32.14 60.71 43.23 43.23</cell></row><row><cell>soft</cell><cell></cell><cell>mean</cell><cell cols="7">NLL 42.86 67.86 56.91 56.91 35.71 53.57 45.75 45.75 35.71 71.43 48.53 48.53</cell></row><row><cell>manual</cell><cell></cell><cell></cell><cell>-</cell><cell>57.14</cell><cell>75</cell><cell cols="4">66.62 66.62 42.86 78.57</cell><cell>58.3</cell><cell>58.3 17.86</cell><cell>50</cell><cell>33.22 33.22</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 12 :</head><label>12</label><figDesc>RG results.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>We use variants of BERT and RoBERTa models from https://huggingface.co.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>https://www.dbpedia.org/privacy/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>https://foundation.wikimedia.org/wiki/ Privacy_policy</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>This work was supported by the <rs type="funder">National Natural Science Foundation of China</rs> (<rs type="grantNumber">61976139</rs>) and by <rs type="person">Alibaba Group</rs> through <rs type="programName">Alibaba Innovative Research Program</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_dR27eSM">
					<idno type="grant-number">61976139</idno>
					<orgName type="program" subtype="full">Alibaba Innovative Research Program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Badr</forename><surname>Alkhamissi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Millicent</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.06031</idno>
		<title level="m">A review on language models as knowledge bases</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Dbpedia: A nucleus for a web of open data</title>
		<author>
			<persName><forename type="first">S?ren</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgi</forename><surname>Kobilarov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Cyganiak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Ives</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-540-76298-0_52</idno>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="722" to="735" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Probing pre-trained language models for semantic attributes and their values</title>
		<author>
			<persName><forename type="first">Meriem</forename><surname>Beloucif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Biemann</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-emnlp.218</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2021</title>
		<meeting><address><addrLine>Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2554" to="2559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Transformer networks of human conceptual knowledge. Psychological review</title>
		<author>
			<persName><forename type="first">Sudeep</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russell</forename><surname>Richie</surname></persName>
		</author>
		<idno type="DOI">10.1037/rev0000319</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Resource description framework (rdf) model and syntax specification</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Brickley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName><surname>Guha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Selection-inference: Exploiting large language models for interpretable logical reasoning</title>
		<author>
			<persName><forename type="first">Antonia</forename><surname>Creswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murray</forename><surname>Shanahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2205.09712</idno>
		<idno>ArXiv, abs/2205.09712</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">OpenPrompt: An open-source framework for promptlearning</title>
		<author>
			<persName><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengding</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haitao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-demo.10</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="105" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models</title>
		<author>
			<persName><forename type="first">Allyson</forename><surname>Ettinger</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00298</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="34" to="48" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Gibbins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nigel</forename><surname>Shadbolt</surname></persName>
		</author>
		<title level="m">Resource description framework (rdf)</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Probing linguistic systematicity</title>
		<author>
			<persName><forename type="first">Emily</forename><surname>Goodwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koustuv</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">J</forename><surname>O'donnell</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.177</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1958" to="1969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures</title>
		<author>
			<persName><forename type="first">Travis</forename><surname>Goodwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dina</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.deelio-1.7</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Deep Learning Inside Out</title>
		<meeting>Deep Learning Inside Out</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="56" to="63" />
		</imprint>
	</monogr>
	<note>Enhancing question answering by injecting ontological knowledge through regularization</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Reasoning with transformer-based models: Deep learning, but shallow reasoning</title>
		<author>
			<persName><forename type="first">Chadi</forename><surname>Helwe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chlo?</forename><surname>Clavel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabian</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
		<idno type="DOI">10.24432/C5W300</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note>rd Conference on Automated Knowledge Base Construction</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Designing and interpreting probes with control tasks</title>
		<author>
			<persName><forename type="first">John</forename><surname>Hewitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1275</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2733" to="2743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A structural probe for finding syntax in word representations</title>
		<author>
			<persName><forename type="first">John</forename><surname>Hewitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1419</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4129" to="4138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Prud</surname></persName>
		</author>
		<title level="m">Sparql query language for rdf</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Language models as zero-shot planners: Extracting actionable knowledge for embodied agents</title>
		<author>
			<persName><forename type="first">Wenlong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Mordatch</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="9118" to="9147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">How can we know what language models know?</title>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><forename type="middle">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Araki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00324</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="423" to="438" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Putting words in BERT&apos;s mouth: Navigating contextualized vector spaces with pseudowords</title>
		<author>
			<persName><forename type="first">Taelin</forename><surname>Karidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omri</forename><surname>Abend</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Srikumar</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.806</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Dominican Republic. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10300" to="10313" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly</title>
		<author>
			<persName><forename type="first">Nora</forename><surname>Kassner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.698</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7811" to="7818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Contrastive selfsupervised learning for commonsense reasoning</title>
		<author>
			<persName><forename type="first">Tassilo</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moin</forename><surname>Nabi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.671</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7517" to="7523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Dikshit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agam</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Man</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Archana</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarika</forename><surname>Jain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Prefix-tuning: Optimizing continuous prompts for generation</title>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.353</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4582" to="4597" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Does BERT know that the IS-a relation is transitive?</title>
		<author>
			<persName><forename type="first">Ruixi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-short.11</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="94" to="99" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Generated knowledge prompting for commonsense reasoning</title>
		<author>
			<persName><forename type="first">Jiacheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alisa</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ximing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Welleck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.225</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3154" to="3169" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Gpt understands, too</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujie</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno>ArXiv, abs/2103.10385</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Roberta: A robustly optimized bert pretraining approach</title>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1907.11692</idno>
		<idno>ArXiv, abs/1907.11692</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Exploring BERT&apos;s sensitivity to lexical cues using tests from semantic priming</title>
		<author>
			<persName><forename type="first">Kanishka</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allyson</forename><surname>Ettinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julia</forename><surname>Rayz</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.415</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4625" to="4635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ontological constitutions for classes and properties</title>
		<author>
			<persName><forename type="first">J?rgen</forename><surname>Fischer Nilsson</surname></persName>
		</author>
		<idno type="DOI">10.1007/11787181_4</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Conceptual Structures</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Copen: Probing conceptual knowledge in pre-trained language models</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaozhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengding</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hailong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">How context affects language models&apos; factual predictions</title>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rockt?schel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">H</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2005.04611</idno>
		<idno type="arXiv">arXiv:2005.04611</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Language models as knowledge bases?</title>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rockt?schel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1250</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2463" to="2473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Information-theoretic probing for linguistic structure</title>
		<author>
			<persName><forename type="first">Tiago</forename><surname>Pimentel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josef</forename><surname>Valvoda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Hall Maudslay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ran</forename><surname>Zmigrod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.420</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4609" to="4622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Relational World Knowledge Representation in Contextual Language Models: A Review</title>
		<author>
			<persName><forename type="first">Tara</forename><surname>Safavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.81</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Dominican Republic. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1053" to="1067" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Exploiting cloze-questions for few-shot text classification and natural language inference</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.eacl-main.20</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</title>
		<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="255" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Automatic word sense discrimination</title>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="97" to="123" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">CommonsenseQA: A question answering challenge targeting commonsense knowledge</title>
		<author>
			<persName><forename type="first">Alon</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Lourie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1421</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4149" to="4158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Leap-of-thought: Teaching pre-trained models to systematically reason over implicit knowledge</title>
		<author>
			<persName><forename type="first">Alon</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="20227" to="20237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Wikidata: a free collaborative knowledgebase</title>
		<author>
			<persName><forename type="first">Denny</forename><surname>Vrande?i?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Kr?tzsch</surname></persName>
		</author>
		<idno type="DOI">10.1145/2629489</idno>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="78" to="85" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Probing pretrained language models for lexical semantics</title>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Vuli?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Edoardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Ponti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Goran</forename><surname>Litschko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Glava?</surname></persName>
		</author>
		<author>
			<persName><surname>Korhonen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.586</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7222" to="7240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding: A survey of approaches and applications</title>
		<author>
			<persName><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhendong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
		<idno type="DOI">10.1109/TKDE.2017.2754499</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2724" to="2743" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Self-consistency improves chain of thought reasoning in language models</title>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2203.11171</idno>
		<idno>ArXiv, abs/2203.11171</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Chain of thought prompting elicits reasoning in large language models</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2201.11903</idno>
		<idno>ArXiv, abs/2201.11903</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Word knowledge dimensions in l2 lexical inference: Testing vocabulary knowledge and partial word knowledge</title>
		<author>
			<persName><forename type="first">(</forename><surname>Haomin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">)</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenxia</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Pei</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10936-021-09831-x</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Psycholinguistic Research</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="151" to="168" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
