<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MagNet: a Two-Pronged Defense against Adversarial Examples</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Dongyu</forename><surname>Meng</surname></persName>
							<email>mengdy@shanghaitech.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">ShanghaiTech University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hao</forename><surname>Chen</surname></persName>
							<email>chen@ucdavis.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Davis</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MagNet: a Two-Pronged Defense against Adversarial Examples</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3133956.3134057</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>adversarial example</term>
					<term>neural network</term>
					<term>autoencoder</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning has shown impressive performance on hard perceptual problems. However, researchers found deep learning systems to be vulnerable to small, specially crafted perturbations that are imperceptible to humans. Such perturbations cause deep learning systems to mis-classify adversarial examples, with potentially disastrous consequences where safety or security is crucial. Prior defenses against adversarial examples either targeted specific attacks or were shown to be ineffective. We propose MagNet, a framework for defending neural network classifiers against adversarial examples. MagNet neither modifies the protected classifier nor requires knowledge of the process for generating adversarial examples. MagNet includes one or more separate detector networks and a reformer network. The detector networks learn to differentiate between normal and adversarial examples by approximating the manifold of normal examples. Since they assume no specific process for generating adversarial examples, they generalize well. The reformer network moves adversarial examples towards the manifold of normal examples, which is effective for correctly classifying adversarial examples with small perturbation. We discuss the intrinsic difficulties in defending against whitebox attack and propose a mechanism to defend against graybox attack. Inspired by the use of randomness in cryptography, we use diversity to strengthen MagNet. We show empirically that Mag-Net is effective against the most advanced state-of-the-art attacks in blackbox and graybox scenarios without sacrificing false positive rate on normal examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>• Security and privacy → Domain-specific security and privacy architectures; • Computing methodologies → Neural networks;</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In recent years, deep learning demonstrated impressive performance on many tasks, such as image classification <ref type="bibr" target="#b8">[9]</ref> and natural language processing <ref type="bibr" target="#b15">[16]</ref>. However, recent research showed that an attacker could generate adversarial examples to fool classifiers <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b18">19]</ref>. Their algorithms perturbed benign examples, which were correctly classified, by a small amount that did not affect human recognition but that caused neural networks to mis-classify. We call theses neural networks target classifiers.</p><p>Current defenses against adversarial examples follow three approaches: <ref type="bibr" target="#b0">(1)</ref> Training the target classifier with adversarial examples, called adversarial training <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b4">5]</ref>; <ref type="bibr" target="#b1">(2)</ref> Training a classifier to distinguish between normal and adversarial examples <ref type="bibr" target="#b19">[20]</ref>; and (3) Making target classifiers hard to attack by blocking gradient pathway, e.g., defensive distillation <ref type="bibr" target="#b24">[25]</ref>.</p><p>However, all these approaches have limitations. Both (1) and (2) require adversarial examples to train the defense, so the defense is specific to the process for generating those adversarial examples. For (3), <ref type="bibr">Carlini et al.</ref> showed that defensive distillation did not significantly increase the robustness of neural networks <ref type="bibr" target="#b1">[2]</ref>. Moreover, this approach requires changing and retraining the target classifier, which adds engineering complexities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arXiv:1705.09064v2 [cs.CR] 11 Sep 2017</head><p>We propose MagNet<ref type="foot" target="#foot_0">1</ref> , a defense against adversarial examples with two novel properties. First, it neither modifies the target classifier nor relies on specific properties of the classifier, so it can be used to protect a wide range of neural networks. MagNet uses the target classifier as a blackbox: MagNet reads the output of the classifier's last layer, but neither reads data on any internal layer nor modifies the classifier. Second, MagNet is independent of the process for generating adversarial examples, as it requires only normal examples for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Adversarial examples</head><p>A normal example x for a classification task is an example that occurs naturally. In other words, the physical process for this classification task generates x with non-negligible probability. For example, if the task is classifying handwritten digits, then the data generation process rarely generates an image of a tiger. An adversarial example y for a classifier is not a normal example and the classifier's decision on y disagrees with human's prevailing judgment. See Section 3.1 for a more detailed discussion.</p><p>Researchers speculate that for many AI tasks, their relevant data lie on a manifold that is of much lower dimension than the full sample space <ref type="bibr" target="#b22">[23]</ref>. This suggests that the normal examples for a classification task are on a manifold, and adversarial examples are off the manifold with high probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Causes of mis-classification and solutions</head><p>A classifier mis-classifies an adversarial example for two reasons.</p><p>(1) The adversarial example is far from the boundary of the manifold of the task. For example, the task is handwritten digit classification, and the adversarial example is an image containing no digit, but the classifier has no option to reject this example and is forced to output a class label. (2) The adversarial example is close to the boundary of the manifold. If the classifier generalizes poorly off the manifold in the vicinity of the adversarial example, then mis-classification occurs.</p><p>We propose MagNet to mitigate these problems. To deal with the first problem, MagNet uses detectors to detect how different a test example is from normal examples. A detector learns a function f : X → {0, 1}, where X is the set of all examples. f (x) tries to measure the distance between the example x and the manifold. If this distance is greater than a threshold, then the detector rejects x.</p><p>To deal with the second problem, MagNet uses a reformer to reform adversarial examples. For this we use autoencoders, which are neural networks trained to attempt to copy its input to its output. Autoencoders leverage simpler hidden representation to introduce regularization to uncover useful properties of the data <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref>. We train an autoencoder with adequate normal examples for it to learn an approximate manifold of the data. Given an adversarial example x close to the boundary of the manifold, we expect the autoencoder to output an example y on the manifold where y is close to x. This way, the autoencoder reforms the adversarial example x to a similar normal example y. Figure <ref type="figure" target="#fig_0">1</ref> shows the effect of the reformer.</p><p>Since MagNet is independent of the target classifier, we assume that the attacker always knows the target classifier and its parameters. In the case of blackbox attack on MagNet, the attacker does not know the defense parameters. In this setting, we evaluated MagNet on popular attacks <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b1">2]</ref>. On the MNIST dataset, Mag-Net achieved more than 99% classification accuracy on adversarial examples generated by nine out of ten attacks considered. On the CIFAR-10 dataset, the classification accuracy improvement was also significant. Particularly, MagNet achieved high accuracy on adversarial examples generated by Carlini's attack, the most powerful attack known to us, across a wide range of confidence levels of the attack on both datasets. Note that we trained our defense without using any adversarial examples generated by the attack. In the case of whitebox attack, the attacker knows the parameters of MagNet. In this case, the attacker could view MagNet and the target classifier as a new composite classifier, and then generate adversarial examples against this composite classifier. Not surprisingly, we found that the performance of MagNet on whitebox attack degraded sharply. When we trained Carlini's attack on our reformer, the attack was able to generate adversarial examples that all fooled our reformer. In fact, we can view any defense against adversarial examples as enhancing the target classifier. As long as the enhanced classifier is imperfect (i.e., unable to match human decisions), adversarial examples are guaranteed to exist. One could make it difficult to find these examples, e.g., by hiding the defense mechanism or its parameters, but these are precluded in whitebox attack.</p><p>We advocate defense via diversity and draw inspiration from cryptography. The security of a good cipher relies on the diversity of its keys, as long as there is no better attack than searching the key space by brute force and this search is computationally infeasible. Adopting a similar approach, we create a number of different defenses and randomly pick one at run time. This way, we defend against graybox attack (Section 3.3). In our implementation, we trained a number of different autoencoders as described above. If the attacker cannot predict which of these autoencoders is used at run time, then he has to generate adversarial examples that can fool all of them. As the diversity of these autoencoders grows, it becomes more difficult for the attacker to find adversarial examples. Section 5.4 will show that this technique raises the classification accuracy on Carlini's adversarial examples from 0 (whitebox attack) to 80% (graybox attack).</p><p>We may also take advantage of these diverse autoencoders to build another detector, which distinguishes between normal and adversarial examples. The insight is that since normal examples are on the manifold, their classification decisions change little after being transformed by an autoencoder. By contrast, since adversarial examples are not on the manifold, their classification results change more significantly after being transformed by the autoencoder. We use the similarity between an example and its output from an autoencoder as a metric. But in contrast to the previous detector, which computes the distance between a test example and the manifold without consulting the target classifer, here we enlist the help from the target classifier. We assume that the classifier outputs the probability distribution of the test example on each label. Let this distribution be p(y; x) for the original test example x, and q(y; ae(x)) for the output of the autoencoder ae on x, where y is the random variable for class labels. We use the Jensen-Shannon divergence between p and q as the similarity measure. Note that although this approach uses the target classifier, during training it does not depend on any specific classifier. It uses the classifier to compute the similarity measure only during testing. We found this detector more sensitive than the previous detector on powerful attacks (Section 5.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Contributions</head><p>We make the following contributions.</p><p>• We formally define adversarial example and metrics for evaluating defense against adversarial examples (Section 3.1). • We argue that it would be very difficult to defend against whitebox attacks. Therefore, we propose the graybox threat model and advocate defending against such attacks using diversity. We demonstrate our approach using diversity (Section 4.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND AND RELATED WORK 2.1 Deep learning systems in adversarial environments</head><p>Deep learning systems play an increasingly important role in modern world. They are used in autonomous control for robots and vehicles <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>, financial systems <ref type="bibr" target="#b31">[32]</ref>, medical treatments <ref type="bibr" target="#b30">[31]</ref>, information security <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b28">29]</ref>, and human-computer interaction <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13]</ref>. These security-critical domains require better understanding of neural networks from the security perspective.</p><p>Recent work has demonstrated the feasibility of attacking such systems with carefully crafted input for real-world systems <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b7">8]</ref>. More specifically, researchers showed that it was possible to generate adversarial examples to fool classifiers <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b18">19]</ref>. Their algorithms perturbed normal examples by a small volume that did not affect human recognition but that caused mis-classification by the learning system. Therefore, how to protect such classifiers from adversarial examples is a real concern.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Distance metrics</head><p>By definition, adversarial examples and their normal counterparts should be visually indistinguishable by humans. Since it is hard to model human perception, researchers proposed three popular metrics to approximate human's perception of visual difference, namely L 0 , L 2 , and L ∞ <ref type="bibr" target="#b1">[2]</ref>. These metrics are special cases of the L p norm:</p><formula xml:id="formula_0">∥x ∥ p = n i=1 |x i | p 1 p</formula><p>These three metrics focus on different aspects of visual significance. L 0 counts the number of pixels with different values at corresponding positions in the two images. It answers the question of how many pixels are changed. L 2 measures the Euclidean distance between the two images. L ∞ measures the maximum difference for all pixels at corresponding positions in the two images.</p><p>Since there is no consensus on which metric is the best, we evaluated our defense on all these three metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Existing attacks</head><p>Since the discovery of adversarial examples for neural networks in <ref type="bibr" target="#b33">[34]</ref>, researchers have found adversarial examples on various network architectures. For example, feedforward convolutional classification networks <ref type="bibr" target="#b1">[2]</ref>, generative networks <ref type="bibr" target="#b13">[14]</ref>, and recurrent networks <ref type="bibr" target="#b26">[27]</ref>. These adversarial examples threaten a wide range of applications, e.g., classification <ref type="bibr" target="#b21">[22]</ref> and semantic segmentation <ref type="bibr" target="#b36">[37]</ref>. Researchers developed several methods for generating adversarial examples, most of which leveraged gradient based optimization from normal examples <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b4">5]</ref>. Moosavi et al. showed that it was even possible to find one effective universal adversarial perturbation that, when applied, turned many images adversarial <ref type="bibr" target="#b20">[21]</ref>.</p><p>To simplify the discussion, we only focus on attacks targeting neural network classifiers. We evaluated our defense against four popular, and arguably most advanced, attacks. We now explain these attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Fast gradient sign method(FGSM). Given a normal image</head><p>x, fast gradient sign method <ref type="bibr" target="#b4">[5]</ref> looks for a similar image x ′ in the L ∞ neighborhood of x that fools the classifier. It defines a loss function Loss(x, l) that describes the cost of classifying x as label l. Then, it transforms the problem to maximizing Loss(x ′ , l x ) which is the cost of classifying image x ′ as its ground truth label l x while keeping the perturbation small. Fast gradient sign method solves this optimization problem by performing one step gradient update from x in the image space with volume ϵ. The update step-width ϵ is identical for each pixel, and the update direction is determined by the sign of gradient at this pixel. Formally, the adversarial example x ′ is calculated as:</p><formula xml:id="formula_1">x ′ = x + ϵ • siдn(∇ x Loss(x, l x ))</formula><p>Although this attack is simple, it is fast and can be quite powerful. Normally, ϵ is set to be small. Increasing ϵ usually leads to higher attack success rate. For this paper, we use FGSM to refer to this attack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.3.2</head><p>Iterative gradient sign Method. <ref type="bibr" target="#b16">[17]</ref> proposed to improve FGSM by using a finer iterative optimization strategy. For each iteration, the attack performs FGSM with a smaller step-width α, and clips the updated result so that the updated image stays in the ϵ neighborhood of x. Such iteration is then repeated for several times. For the ith iteration, the update process is:</p><formula xml:id="formula_2">x ′ i+1 = clip ϵ,x (x ′ i + α • siдn(∇ x Loss(x, l x )</formula><p>)) This update strategy can be used for both L ∞ and L 2 metrics and greatly improves the success rate of FGSM attack. We refer to this attack as the iterative method for the rest of the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3">DeepFool.</head><p>DeepFool is also an iterative attack but formalizes the problem in a different way <ref type="bibr" target="#b21">[22]</ref>. The basic idea is to find the closest decision boundary from a normal image x in the image space, and then to cross that boundary to fool the classifier. It is hard to solve this problem directly in the high-dimensional and highly non-linear space in neural networks. So instead, it iteratively solves this problem with a linearized approximation. More specifically, for each iteration, it linearizes the classifier around the intermediate x ′ and derives an optimal update direction on this linearized model. It then updates x ′ towards this direction by a small step α. By repeating the linearize-update process until x ′ crosses the decision boundary, the attack finds an adversarial example with small perturbation. We use the L ∞ version of the DeepFool attack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.4">Carlini attack.</head><p>Carlini recently introduced a powerful attack that generates adversarial examples with small perturbation <ref type="bibr" target="#b1">[2]</ref>. The attack can be targeted or untargeted for all three metrics L 0 , L 2 , and L ∞ . We take the untargeted L 2 version as an example here to introduce its main idea.</p><p>We may formalize the attack as the following optimization problem:</p><formula xml:id="formula_3">minimize δ ∥δ ∥ 2 + c • f (x + δ ) such that x + δ ∈ [0, 1] n</formula><p>For a fixed input image x, the attack looks for a perturbation δ that is small in length(∥ • ∥ term in objective) and fools the classifier(the f (•) term in objective) at the same time. c is a hyperparameter that balances the two. Also, the optimization has to satisfy the box constraints to be a valid image.</p><p>f (•) is designed in such a way that f (x ′ ) ⩽ 0 if and only if the classifier classifies x ′ incorrectly, which indicates that the attack succeeds. f (x ′ ) has hinge loss form and is defined as</p><formula xml:id="formula_4">f (x ′ ) = max(Z (x ′ ) l x − max{Z (x ′ ) i : i l x }, −κ)</formula><p>where Z (x ′ ) is the pre-softmax classification result vector (called logits) and l x is the ground truth label. κ is a hyper-parameter called confidence. Higher confidence encourages the attack to search for adversarial examples that are stronger in classification confidence. High-confidence attacks often have larger perturbation and better transferability.</p><p>In this paper, we show that our defense is effective against Carlini's attack across a wide range of confidence levels (Section 5.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Existing defense</head><p>Defense on neural networks is much harder compared with attacks. We summarize some ideas of current approaches to defense and compare them to our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1">Adversarial training. One idea of defending against adver-</head><p>sarial examples is to train a better classifier <ref type="bibr" target="#b29">[30]</ref>. An intuitive way to build a robust classifier is to include adversarial information in the training process, which we refer to as adversarial training. For example, one may use a mixture of normal and adversarial examples in the training set for data augmentation <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b21">22]</ref>, or mix the adversarial objective with the classification objective as regularizer <ref type="bibr" target="#b4">[5]</ref>. Though this idea is promising, it is hard to reason about what attacks to train on and how important the adversarial component should be. Currently, these questions are still unanswered.</p><p>Meanwhile, our approach is orthogonal to this branch of work. MagNet is an additional defense framework that does not require modification to the target classifier in any sense. The design and training of MagNet is independent from the target classifier, and is therefore faster and more flexible. MagNet may benefit from a robust target classifier (section 5). <ref type="bibr" target="#b24">[25]</ref> trains the classifier in a certain way such that it is nearly impossible for gradient based attacks to generate adversarial examples directly on the network. Defensive distillation leverages distillation training techniques <ref type="bibr" target="#b9">[10]</ref> and hides the gradient between the pre-softmax layer (logits) and softmax outputs. However, <ref type="bibr" target="#b1">[2]</ref> showed that it is easy to bypass the defense by adopting one of the three following strategies: (1) choose a more proper loss function (2) calculate gradient directly from pre-softmax layer instead of from post-softmax layer (3) attack an easy-to-attack network first and then transfer to the distilled network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2">Defensive distillation. Defensive distillation</head><p>We argue that in whitebox attack where the attacker knows the parameters of the defense network, it is very difficult to prevent adversaries from generating adversarial examples that defeat the defense. Instead, we propose to study defense in the graybox model (Section 3.3), where we introduce a randomization strategy to make it hard for the attacker to generate adversarial examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.3">Detecting adversarial examples.</head><p>Another idea of defense is to detect adversarial examples with hand-crafted statistical features <ref type="bibr" target="#b6">[7]</ref> or separate classification networks <ref type="bibr" target="#b19">[20]</ref>. An representative work of this idea is <ref type="bibr" target="#b19">[20]</ref>. For each attack generating method considered, it constructed a deep neural network classifier (detector) to tell whether an input is normal or adversarial. The detector was directly trained on both normal and adversarial examples. The detector showed good performance when the training and testing attack examples were generated from the same process and the perturbation was large enough, but it did not generalize well across different attack parameters and attack generation processes.</p><p>MagNet also employs one more more detectors. Contrary to previous work, however, we do not train our detectors on any adversarial examples. Instead, MagNet tries to learn the manifold of normal data and makes decision based on the relationship between a test example and the manifold. Further, MagNet includes a reformer that pushes hard-to-detect adversarial examples (with small perturbation) towards the manifold. Since MagNet is independent of any process for generating adversarial examples, it generalizes well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROBLEM DEFINITION 3.1 Adversarial examples</head><p>We define the following sets:</p><p>• S: the set of all examples in the sample space (e.g., all images). The ground-truth classifier for a task t represents human's prevailing judgment. We represent it by a function д t : S → C t ∪ {⊥} where ⊥ represents the judgment that the input x is unlikely from t's data generation process. Definition 3.3. An adversarial example x for a task t and a classifier f t is one where:</p><formula xml:id="formula_5">• f t (x) д t (x), and • x ∈ S \ N t</formula><p>The first condition indicates that the classifier makes a mistake, but this in itself is not adequate for making the example adversarial. Since no classifier is perfect, there must exist natural examples that a classifier mis-classifies, so an attacker could try to find these examples. But these are not interesting adversarial examples for two reasons. First, traditionally they are considered as testing errors as they reflect poor generalization of the classifier. Second, finding these examples by brute force in large collections of natural examples is inefficient and laborious, because it would require humans to collect and label all the natural examples. Therefore, we add the second condition above to limit adversarial examples to only examples generated artificially by the attacker to fool the classifier. 2    The defense d f t extends the classifier f t to make it robust. The defense algorithm in d f t may use f t in three different ways:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Defense and evaluation</head><p>• The defense algorithm does not read data in f t or modify parameters in f t . • The defense algorithm reads data in f t but does not modify parameters in f t . • The defense algorithm modifies parameters in f t .</p><p>When evaluating the effectiveness of a defense d f t , we cannot merely evaluate whether it classifies each example correctly, i.e., whether its decision agrees with that of the ground truth classifier д t . After all, the goal of the defense is to improve the accuracy of the classifier on adversarial examples rather than on normal examples. Definition 3.5. The defense d f t makes a correct decision on an example x if either of the following applies:</p><p>• x is a normal example, and d f t and the ground-truth classifier д t agree on x's class, i.e., x ∈ N t and d f t (x) = д t (x). • x is an adversarial example, and either d f t decides that x is adversarial or that d f t and the ground-truth classifier д t agree on x's class, i.e., x ∈ S \ N t and (d</p><formula xml:id="formula_6">f t (x) = ⊥ or d f t (x) = д t (x)).</formula><p>2 Kurakin et al. showed that many adversarial images generated artificially remain adversarial after being printed and then captured by a camera <ref type="bibr" target="#b16">[17]</ref>. We still consider these as adversarial examples because although they occurred in physical forms, they were not generated by the natural process for generating normal examples. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Threat model</head><p>We assume that the attacker knows everything about the classifier f t that she wishes to attack, called target classifier, such as its structure, parameters, and training procedure. Depending on whether the attacker knows the defense d f t , there are two scenarios:</p><p>• Blackbox attack: the attacker does not know the parameters of d f t . • Whitebox attack: the attacker knows the parameters of d f t .</p><p>• Graybox attack: except for the parameters, the attacker knows everything else about d f t , such as its structure, hyper-parameters, training set, training epochs. If we train a neural network multiple times while fixing these variables, we often get different model parameters each time because of random initialization. We can view that we get a different network each time. To push this one step further, we can train these different networks at the same time and force them to be sufficiently different by penalizing their resemblance. Section 4.3 for an example. The defense can be trained with different structures and hyper-parameters for even greater diversity.</p><p>We assume that the defense knows nothing about how the attacker generates adversarial examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DESIGN</head><p>MagNet is a framework for defending against adversarial examples (Figure <ref type="figure" target="#fig_2">2</ref>). In Section 1.2 we provided two reasons why a classifier mis-classifies an adversarial example: (1) The example is far from the boundary of the manifold of normal examples, but the classifier has no option to reject it; (2) The example is close to the boundary of the manifold, but the classifier generalizes poorly off the manifold in the vicinity of the example. Motivated by these observations, MagNet consists of two components: (1) a detector that rejects examples that are far from the manifold boundary, and (2) a reformer that, given an example x, strives to find an example x ′ on or close to the manifold where x ′ is a close approximation to x, and then gives x ′ to the target classifier. Figure <ref type="figure" target="#fig_3">3</ref> illustrates the effect of the detector and reformer in a 2-D sample space. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Detector</head><p>The detector is a function d : S → {0, 1} that decides whether the input is adversarial. As an example of this approach, a recent work trained a classifier to distinguish between normal and adversarial examples <ref type="bibr" target="#b19">[20]</ref>. However, it has the fundamental limitation that it requires the defender to model the attacker, by either acquiring adversarial examples or knowing the process for generating adversarial examples. Therefore, it unlikely generalizes to other processes for generating adversarial examples. For example, <ref type="bibr" target="#b19">[20]</ref> used a basic iterative attack based on the L 2 norm. Its results showed that if its detector was trained with slightly perturbed adversarial samples, the detector had high false positive rates because it decided many normal examples as adversarial. On the other hand, if the detector was trained with significantly perturbed examples, it would not be able to detect slightly perturbed adversarial examples. </p><formula xml:id="formula_7">L(X train ) = 1 |X train | x ∈X train ∥x − ae(x))∥ 2</formula><p>The reconstruction error on a test example x is</p><formula xml:id="formula_8">E(x) = ∥x − ae(x))∥ p</formula><p>An autoencoder learns the features of the training set so that the encoder can encode the input with hidden representation of certain properties, and the decoder tries to reconstruct the input from the hidden representation. If an input is drawn from the same data generation process as the training set, then we expect a small reconstruction error. Otherwise, we expect a larger reconstruction error. Hence, we use reconstruction error to estimate how far a test example is from the manifold of normal examples. Since reconstruction error is a continuous value, we must set a threshold t re for deciding whether the input is normal. This threshold is a hyperparameter of an instance of detector. It should be as low as possible to detect slightly perturbed adversarial examples, but not too low to falsely flag normal examples. We decide t re by a validation set containing normal examples, where we select the highest t re such that the detector's false positive rate on the validation set is below a threshold t fp . This threshold t fp should be decided catering for the requirement of the system.</p><p>When calculating reconstruction errors, it is important to choose suitable norms. Though reconstruction error based detectors are attack-independent, the norm choosen for detection do influence the sharpness of detection results. Intuitively, p-norm with larger p is more sensitive to the maximum difference among all pixels, while smaller p averages its concentration to each pixel. Empirically, we found it sufficient to use two reconstruction error based detectors with L 1 and L 2 norms respectively to cover both ends. Most neural network classifiers implement the softmax function at the last layer softmax(l</p><formula xml:id="formula_9">) i = exp(l i ) n j=1 exp(l j )</formula><p>The output of softmax is a probability mass function over the classes. The input to softmax is a vector l called logit. Let rank(l, i) be the index of the element that is ranked the ith largest among all the elements in l. Given a normal example whose logit is l, the goal of the attacker is to perturb the example to get a new logit l ′ such that rank(l, 1) rank(l ′ , 1).</p><p>Let f (x) be the output of the last layer (softmax) of the neural network f on the input x. Let ae(x) be the output of the autoencoder ae that was trained on normal examples. If x is a normal example, since ae(x) is very close to x, the probability mass functions f (x) and f (ae(x)) are similar. By contrast, if x ′ is an adversarial example, ae(x ′ ) is significantly different from x ′ . We observed that even when the reconstruction error on x ′ is small, f (x ′ ) and f (ae(x ′ )) can be significantly different. This indicates that the divergence between f (x) and f (ae(x)) reflects how likely x is from the same data generation process as normal examples. We use Jensen-Shannon divergence:</p><formula xml:id="formula_10">JSD(P ∥ Q) = 1 2 D KL (P ∥ M) + 1 2 D KL (Q ∥ M)</formula><p>where</p><formula xml:id="formula_11">D KL (P ∥ Q) = i P(i) log P(i) Q(i) and M = 1 2 (P + Q)</formula><p>When we implemented this, we encountered a numerical problem. Let l(x) be the logit of the input x. When the largest element in l(x) is much larger than its second largest element, softmax(l(x)) saturates, i.e., the largest element in softmax(l(x)) is very close to 1. When this happens, we observed that softmax(l(ae(x))) also saturates on the same element. This will make the Jensen-Shannon divergence between softmax(l(x)) and softmax(l(ae(x))) very small. To overcome this numerical problem, we add a temperature T &gt; 1 when calculating softmax:</p><formula xml:id="formula_12">softmax(l) i = exp(l i /T ) n j=1 exp(l j /T )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Reformer</head><p>The reformer is a function r : S → N t that tries to reconstruct the test input. The output of the reformer is then fed to the target classifier. Note that we do not use the reformer when training the target classifier, but use the reformer only when deploying the target classifier. An ideal reformer: where y~N (y; 0, I) is the normal distribution with zero mean and identity covariance matrix, ϵ scales the noise, and clip is a function that clips each element of its input vector to be in the valid range.</p><p>A shortcoming of this noise-based reformer is that it fails to take advantage of the distribution of normal examples. Therefore, it changes both normal and adversarial examples randomly and blindly, but our ideal reformer should barely change normal examples but should move adversarial examples towards normal examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Autoencoder-based reformer.</head><p>We propose to use autoencoders as the reformer. We train the autoencoder to minimize the reconstruction error on the training set and ensures that it generalizes well on the validation set. Afterwards, when given a normal example, which is from the same data generating process as the training examples, the autoencoder is expected to output a very similar example. But when given an adversarial example, the autoencoder is expected to output an example that approximates the adversarial example and that is closer to the manifold of the normal examples. In this way, MagNet improves the classification accuracy of adversarial examples while keeping the classification accuracy of normal examples unchanged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Use diversity to mitigate graybox attacks</head><p>In blackbox attacks, the attacker knows the parameters of the target classifier but not those of the detector or reformer. Our evaluation showed that MagNet was highly effective in defending against blackbox attacks (Section 5.2).</p><p>However, in whitebox attacks, where the attacker also knows the parameters of the detector and reformer, our evaluation showed that MagNet became less accurate. This is not surprising because we can view that MagNet transforms the target classifier f t into a new classifier f ′ t . In whitebox attacks, the attacker knows all the parameters of f ′ t , so he can use the same method that he used on f t to find adversarial examples for f ′ t . If such adversarial examples did not exist or were negligible, then it would mean that f ′ t agrees with the ground-truth classifier on almost all the examples off the manifold of normal example. Since there is no evidence that we could find this perfect classifier anytime soon, non-negligibly number of adversarial examples exist for any classifier, including f ′ t .</p><p>Although we cannot eliminate adversarial examples, we could make it difficult for attackers to find them. One approach would be to create a robust classifier such that even if the attacker knows all the parameters of the classifier, it would be difficult for her to find adversarial example <ref type="bibr" target="#b24">[25]</ref>. However, <ref type="bibr" target="#b1">[2]</ref> showed that it was actually easy to find adversarial examples for the classifier hardened in <ref type="bibr" target="#b24">[25]</ref>. We do not know how to find such robust classifiers, or even if they exist.</p><p>We take a different approach. We draw inspirations from cryptography, which uses randomness to make it computationally difficult for the attacker to find secrets, such as secret keys. We use the same idea to diversify our defense. In our implementation, we create a large number of autoencoders as candidate detectors and reformers. MagNet randomly picks one of these autoencoders for each defensive device for every session, every test set, or even every test example. Assume that the attacker cannot predict which autoencoder we pick for her adversarial example and that successful adversarial examples trained on one autoencoder succeed on another autoencoders with low probability, then the attacker would have to train her adversarial examples to work on all the autoencoders in our collection. We can increase the size and diversity of this collection to make the attack harder to perform. This way, we defend against graybox attack as defined in Section 3.3.</p><p>A key question is how to find large number of diverse autoencoders such that transfer attacks on target classifiers succeed with low probability. Rigorous theoretical analysis of the question is beyond the scope of this paper. Instead, we show a method for constructing these autoencoders and empirical evidence of its effectiveness.</p><p>We train n autoencoders of the same or different architectures at the same time with random initialization. During training, in the cost function we add a regularization term to penalize the resemblance of these autoencoders</p><formula xml:id="formula_14">L(x) = n i=1 MSE(x, ae i (x)) − α n i=1 MSE(ae i (x), 1 n n j=1 ae j (x)) (1)</formula><p>where ae i is the ith autoencoder, MSE is the mean squared error function, and α &gt; 0 is a hyper-parameter that reflects the tradeoff between reconstruction error and autoencoder diversity. When α becomes larger, it encourages autoencoder diversity but also increases reconstruction error. We will evaluate this approach in Section 5.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">IMPLEMENTATION AND EVALUATION</head><p>We evaluated the accuracy and properties of our defense described in section 4 on two standard dataset: MNIST <ref type="bibr" target="#b17">[18]</ref> and CIFAR-10 [15].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Setup</head><p>On MNIST, we selected 55 000 examples for the training set, 5 000 for the validation set, and 1 000 for the test set. We trained a classifier using the setting in <ref type="bibr" target="#b1">[2]</ref> and got an accuracy of 99.4%. On CIFAR-10, we selected 45 000 examples for training set, 5 000 for the validation set, and 10 000 for the test set. We used the architecture in <ref type="bibr" target="#b32">[33]</ref> and got an accuracy of 90.6%. The accuracy of both these classifiers is near the state of the art on these datasets. Table <ref type="table" target="#tab_2">1</ref> and Table <ref type="table" target="#tab_3">2</ref> show the architecture and training parameters of these classifiers. We used a scaled range of [0, 1] instead of [0, 255] for simplicity.</p><p>In the rest of this section, first we evaluate the robustness of MagNet in blackbox attack, where the attacker does not know the parameters used in MagNet. To understand why MagNet works and when it works well, we analyze the impact of the detector and the reformer, respectively, on the accuracy of MagNet against Carlini's attack. Finally, we evaluate the use of diversity to mitigate graybox attack, where we use the same classifier architecture but train it to get many classifiers of different parameters.</p><p>Table <ref type="table">3</ref>: Defensive devices architectures used for MNIST, including both encoders and decoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Detector I &amp; Reformer</head><p>Detector II</p><formula xml:id="formula_15">Conv.Sigmoid 3 × 3 × 3 Conv.Sigmoid 3 × 3 × 3 AveragePooling 2 × 2 Conv.Sigmoid 3 × 3 × 3 Conv.Sigmoid 3 × 3 × 3 Conv.Sigmoid 3 × 3 × 1 Conv.Sigmoid 3 × 3 × 3 Upsampling 2 × 2 Conv.Sigmoid 3 × 3 × 3 Conv.Sigmoid 3 × 3 × 1</formula><p>Table <ref type="table">4</ref>: Defensive devices architecture used for CIFAR-10, including both encoders and decoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Detectors &amp; Reformer</head><p>Conv.Sigmoid 3</p><formula xml:id="formula_16">× 3 × 3 Conv.Sigmoid 3 × 3 × 3 Conv.Sigmoid 3 × 3 × 1</formula><p>We may divide attacks using adversarial examples into two types. In targeted attack, the attacker chooses a particular class and then creates adversarial examples that the victim classifier mis-classifies into that class. In untargeted attack, the attacker does not care which class the victim classifier outputs as long as it is different from the ground truth. Previous work showed that untargeted attack is easier to succeed, results in smaller perturbations, and transfers better to different models <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b1">2]</ref>. Since untargeted attack is more difficult to defend against, we evaluate MagNet on untargeted attack to show its worst case performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Overall performance against blackbox attacks</head><p>We tested MagNet against attacks using fast gradient sign method, iterative gradient sign method, DeepFool, and Carlini's method. For fast gradient sign method and iterative gradient sign method, we used the implementation of Cleverhans <ref type="bibr" target="#b25">[26]</ref>. For DeepFool and Carlini's attack, we used their authors' open source implementations <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b1">2]</ref>. In principle, MagNet works better when we deploy several instances of both reconstruction error based detectors and probability divergence based detectors. Diversified autoencoder architecture also boosts defense performance. In our implementation, we try to simplify the setup by limiting our detector usage and sharing architectures among autoencoders. This is for convenience rather than mandatory. More specifically, for MNIST dataset, we only use two reconstruction error based detectors of two unique architectures. For CIFAR-10 dataset, we share the same structure among all autoencoders. Table <ref type="table">3</ref>, Table <ref type="table">4</ref>, and Table <ref type="table" target="#tab_4">5</ref> show the architectures and training hyper-parameters of the autoencoder for MNIST and CIFAR-10. We tune the network to make sure it works, but make no further effort to optimize these settings. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">MNIST.</head><p>Compared to CIFAR-10, MNIST is an easier dataset for classifiers. We trained a classifier to achieve an accuracy of 99.4%, which is close to the state of the art. We found that we needed only the reconstruction error-based detector and reformer to become highly accurate against adversarial examples generated from MNIST. Therefore, we did not include the probability divergencebased detector in MagNet in this evaluation. Detector II and detector I(see Table <ref type="table">3</ref>) used the L 2 and L 1 norm to measure reconstruction error, respectively. We selected the threshold of reconstruction error such that the false positive rate of the detector on the validation set is at most 0.001, i.e., each detector mistakenly rejects no more than 0.1% examples in the validation set.</p><p>Effect on normal examples. On the test set, without MagNet, the accuracy of the classifier is 99.4%; with MagNet, the accuracy is reduced to 99.1%. This small reduction is negligible.</p><p>Effect on adversarial examples. Table <ref type="table" target="#tab_5">6a</ref> shows that the accuracy of MagNet is above 99% on all the attacks considered except Carlini attack with L 0 norm(92.0%). Note that we achieved such high accuracy without training MagNet on any of these attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">CIFAR-10.</head><p>Compared to MNIST, CIFAR-10 is more challenging for classifiers. We trained a classifier to achieve an accuracy of 90.6%, which is close to the state of the art. For CIFAR-10, MagNet consists of a reformer, a reconstruction error-based detector, and two probability divergence-based detectors with temperature T of 10 and 40, respectively. We trained the autoencoder as a denoising autoencoder with unit Gaussian noise with volume 0.025. Error based detector uses the L 1 norm to measure reconstruction error. Again, we set a threshold of false positive rate t fp on the validation set to find the threshold of reconstruction error. We set t fp to 0.005 for the reconstruction error-based detector, and 0.01 for the probability divergence-based detector.</p><p>Effect on normal examples. On the test set, without MagNet, the accuracy of the classifier is 90.6%; with MagNet, the accuracy is reduced to 86.8%. The reduction in accuracy is small. Effect on adversarial examples. Table <ref type="table" target="#tab_5">6b</ref> shows that the accuracy of MagNet on 10 different attacks. MagNet is not as accurate on CIFAR-10 as on MNIST, because the target classifier is not as strong on CIFAR-10 and leaves less space for MagNet to take effect. MagNet achieved an accuracy above 75% on all the attacks, and above 90% on more than half attacks. This provides empirical evidence that </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Case study on Carlini attack, why does MagNet work?</head><p>Carlini showed that it was viable to mount transfer attack with higher confidence on MNIST <ref type="bibr" target="#b1">[2]</ref>. Among the attacks that we evaluated, Carlini's attack is the most interesting because it is the most effective on the distillation defense <ref type="bibr" target="#b24">[25]</ref> and there is no known effective defense prior to our work. This attack is also interesting because the attacker can change the attack strength by adjusting the confidence level when generating adversarial examples. The higher confidence is, the stronger classification confidence is, and the larger distortion gets. At a confidence level of 40, the attack achieved a success rate of close to 100% on classifier with distillation defense even by conducting transfer attack.</p><p>We evaluated the impact of different confidence levels in Carlini's attack on MagNet. For MNIST, we used the same classifier as in Carlini's paper <ref type="bibr" target="#b1">[2]</ref> for generating adversarial examples and as the target classifier in our evaluation. We generated adversarial examples with confidence levels in the range of <ref type="bibr">[0,</ref><ref type="bibr">40]</ref>  [2] did not evaluate the impact of confidence level, but we picked confidence levels in the range of [0, 100]. We use the classifier in Section 5.2 for CIFAR-10 as target classifier. We keep the defense setting in Section 5.2 unchanged for both datasets.</p><p>Figure <ref type="figure" target="#fig_7">4</ref> shows the performance of the detector and reformer on MNIST. Without MagNet, the attack succeeded almost 100%, i.e., the classification accuracy rate is close to 0. With MagNet, the classification accuracy rate is above 99% on adversarial examples generated at all confidence levels tested. This indicates that MagNet blocks Carlini attack completely in blackbox scenario.</p><p>Figure <ref type="figure">5</ref> shows the classification accuracy of MagNet on CIFAR-10. The attack also gets near 100% success rate for all confidences. A striking revelation in Figure <ref type="figure">5</ref> is that the detector and reformer compensate each other to achieve an overall high accuracy at all confidence levels. At high confidence level, the adversarial example is far from the manifold of normal examples, so it likely has a high reconstruction error, and therefore will be rejected by the detector. At low confidence level, the adversarial example is close to the manifold of normal examples, so the reconstructed example by the reformer is more likely to lie on the manifold and therefore to be classified correctly. In other words, as the confidence level of the adversarial example goes up, the reformer becomes less effective but the detector becomes more effective, so there is a dip in the mid range on the curve of the overall classification accuracy as shown in Figure <ref type="figure">5</ref>. This dip is an window of opportunity for the attacker, as it is where the effectiveness of the reformer begins to wane but the power of detectors have not started. In Figure <ref type="figure">5</ref>, even though this window of opportunity exists, MagNet still achieves classification accuracy above 80% at all confidence levels.</p><p>Same dip should have appeared in Figure <ref type="figure" target="#fig_7">4</ref>, but the classifier and MagNet is strong enough to fill the dip.</p><p>Figure <ref type="figure">6</ref> shows the effect of the temperature T on the accuracy of the probability divergence-based detector. Low temperature makes  Note again that we did not train MagNet with Carlini's attack or any other attacks, so we conjecture that the results likely generalize to other attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Defend against graybox attacks</head><p>In graybox attack, except for the parameters, the attacker knows everything else about the defense, such as network structure, training set, and training procedure. If we assume that (1) the attacker cannot predict the parameters that the defender uses when classifying her adversarial examples; and (2) the attacker cannot feasibly mislead all possible defense when generating her adversarial examples, then we can defend against attackers by diversifying our defensive network.</p><p>We show an example defense against graybox attack. In this example, we provide diversity by training n different autoencoders for the reformer in MagNet. In our proof-of-concept implementation, we used the same architecture, a convolutional autoencoder with 3 × 3 × 8 hidden layers and ReLU activation, to obtain eight autoencoders of different parameters. During training, we used the same hyper-parameters as in Section 5.2 except that we first trained the eight autoencoders independently for 3 epochs using the standard mean squared error loss. Then, we continued training these autoencoders using the loss in Equation 1 for another 10 epochs, where we chose α = 0.2 empirically. At test time, we randomly picked one of the eight autoencoders as the reformer.</p><p>We chose Carlini's attack to evaluate this defense. However, Carlini's attack models only one network and uses the decision of the network to decide how to perturb the candidate adversarial example. But MagNet contains at least two networks, a reformer and one (or more) detector, that make independent decisions. Therefore, the attack as described in <ref type="bibr" target="#b1">[2]</ref> cannot handle MagNet. To overcome this obstacle, we removed the detectors from MagNet and kept only the reformer to allow Carlini's attack to generate adversarial examples. But in this case, it would not fair to test MagNet with adversarial examples at high confidence level, because MagNet relies on the detector to reject adversarial examples at high confidence level (Figure <ref type="figure">5</ref>). Therefore, we ran Carlini attack to generate adversarial examples at confidence level 0. We chose only CIFAR-10 because Carlini's attack is more effective on it than on MNIST.</p><p>Table <ref type="table" target="#tab_8">7</ref> shows the classification accuracy of MagNet on adversarial examples generated by Carlini's attack. We name each autoencoder A through H. Each column corresponds to an autoencoder that the attack is generated on, and each row corresponds to an autoencoder that is used during testing. The last row, random, means that MagNet picks a random one from its eight autoencoders. The diagonal shows that MagNet's classification accuracy drops to mostly 0 when the autoencoder on which Carlini's attack was trained is also the one that MagNet used during testing. However, when these two autoencoders differ, the classification accuracy jumps to above 90%. The last row shows a more realistic scenario when the attacker chooses a random autoencoder during training and MagNet also chooses a random autoencoder during testing from the eight candidate autoencoders. In this case, MagNet maintains classification accuracy above 80%.</p><p>Table <ref type="table" target="#tab_9">8</ref> shows the classifier accuracy of these autoencoders on the test set for CIFAR-10. Compared to the accuracy of the target classifier, 90.6%, these autoencoders barely reduce the accuracy of the target classifier. There is much room for improvement on how to diversify Mag-Net. We could use autoencoders of different architectures, tune autoencoders with different training parameters, increase the amount of autoencoders, and encourage the difference between these autoencoders. We leave these for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The effectiveness of MagNet against adversarial examples depends on the following assumptions:</head><p>• There exist detector functions that measure the distance between its input and the manifold of normal examples. • There exist reformer functions that output an example x ′ that is perceptibly close to the input x, and x ′ is closer to the manifold than x. We chose autoencoder for both the reformer and the two types of detectors in MagNet. MagNet's high accuracy against the stateof-the-art attacks provides empirical evidence that our assumptions are likely correct. However, before we find stronger justification or proof, we cannot dismiss the possibility that our good results occurred because the state-of-the-art attacks are not powerful enough. We hope that our results would motivate further research on finding more powerful attacks or more powerful detectors and reformers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>We proposed MagNet, a framework for defending against adversarial perturbation of examples for neural networks. MagNet handles untrusted input using two methods. It detects adversarial examples with large perturbation using detector networks, and pushes examples with small perturbation towards the manifold of normal examples. These two methods work jointly to enhance the classification accuracy. Moreover, by using autoencoder as detector networks, MagNet learns to detect adversarial examples without requiring either adversarial examples or the knowledge of the process for generating them, which leads to better generalization. Experiments show that MagNet defended against the state-of-art attacks effectively. In case that the attacker knows the training examples of MagNet, we described a new graybox threat model and used diversity to defend against this attack effectively.</p><p>We advocate that defense against adversarial examples should be attack-independent. Instead of finding properties of adversarial examples from specific generation processes, a defense would be more transferable by finding intrinsic common properties among all adversarial generation processes. MagNet is a first step towards this end and demonstrated good performance empirically.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An illustration of the reformer's effect on adversarial perturbations. The second row displays adversarial examples generated from the original normal examples in the first row by Carlini's L ∞ attack. The third row shows their perturbations against the original examples, and these perturbations lack prominent patterns. The fourth row displays the adversarial examples after being reformed by MagNet. The fifth row displays the remaining perturbations in the reformed examples against their original examples in the first row, and these perturbations have the shapes of their original examples.</figDesc><graphic url="image-1.png" coords="1,317.96,156.14,244.58,142.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Definition 3 . 4 .</head><label>34</label><figDesc>A defense against adversarial examples for a classifier f t is a function d f t : S → C t ∪ {⊥}</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: MagNet workflow in test phase. MagNet includes one or more detectors. It considers a test example x adversarial if any detector considers x adversarial. If x is not considered adversarial, MagNet reforms it before feeding it to the target classifier.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Illustration of how detector and reformer work in a 2-D sample space. We represent the manifold of normal examples by a curve, and depict normal and adversarial examples by green dots and red crosses, respectively. We depict the transformation by autoencoder using arrows. The detector measures reconstruction error and rejects examples with large reconstruction errors (e.g. cross (3) in the figure), and the reformer finds an example near the manifold that approximates the original example (e.g. cross (1) in the figure).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>4. 1 . 1</head><label>11</label><figDesc>Detector based on reconstruction error. To avoid the problem of requiring adversarial examples, MagNet's detector models only normal examples, and estimates the distance between the test example and boundary of the manifold of normal examples. Our implementation uses an autoencoder as the detector and uses the reconstruction error to approximate the distance between the input and the manifold of normal examples. An autoencoder ae = d • e contains two components: an encoder e : S → H and a decoder d : H → S, where S is the input space and H is the space of hidden representation. We train the autoencoder to minimize a loss function over the training set, where the loss function commonly is mean squared error:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>4. 1 . 2</head><label>12</label><figDesc>Detector based on probability divergence. The detector described in Section 4.1.1 is effective in detecting adversarial examples whose reconstruction errors are large. However, it becomes less effective on adversarial examples whose reconstruction errors are small. To overcome this problem, we take advantage of the target classifier.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>) should not change the classification results of normal examples. (2) should change adversarial examples adequately so that the reconstructed examples are close to normal examples. In other words, it should reform adversarial examples. 4.2.1 Noise-based reformer. A naive reformer is a function that adds random noise to the input. If we use Gaussian noise, we get the following reformer r (x) = clip(x + ϵ • y)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Defense performance with different confidence of Carlini's L 2 attack on MNIST dataset. The performance is measured as the percentage of adversarial examples that are either detected by the detector, or classified correctly by the classifier.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Figure 5: Defense performance on different confidence of Carlini's L 2 attack on CIFAR-10 dataset. The performance is measured as the percentage of adversarial examples that are either detected by the detector, or classified correctly by the classifier.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>• We propose a defense against adversarial examples. The defense is independent of either the target classifier or the process for generating adversarial examples (Section 4.1, Section 4.2).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>• C t : the set of mutually exclusive classes for the classification task t. E.g., if t is handwritten digit classification, then C = {0, 1, . . . , 9}.• N t = {x |x ∈ S and x occurs naturally with regard to the classification task t }. Each classification task t assumes a data generation process that generates each example x ∈ S with</figDesc><table /><note>probability p(x). x occurs naturally if p(x) is non-negligible. Researchers believe that N t constitute a manifold that is of much lower dimension than S<ref type="bibr" target="#b22">[23]</ref>. Since we do not know the data generation process, we approximate N t by the union of natural datasets for t, such as CIFAR and MNIST for image recognition. Definition 3.1. A classifier for a task t is a function f t : S → C t Definition 3.2.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Architecture of the classifiers to be protected</figDesc><table><row><cell>MNIST</cell><cell></cell><cell>CIFAR</cell><cell></cell></row><row><cell>Conv.ReLU</cell><cell>3 × 3 × 32</cell><cell>Conv.ReLU</cell><cell>3 × 3 × 96</cell></row><row><cell>Conv.ReLU</cell><cell>3 × 3 × 32</cell><cell>Conv.ReLU</cell><cell>3 × 3 × 96</cell></row><row><cell cols="2">Max Pooling 2 × 2</cell><cell>Conv.ReLU</cell><cell>3 × 3 × 96</cell></row><row><cell>Conv.ReLU</cell><cell>3 × 3 × 64</cell><cell cols="2">Max Pooling 2 × 2</cell></row><row><cell>Conv.ReLU</cell><cell>3 × 3 × 64</cell><cell>Conv.ReLU</cell><cell>3 × 3 × 192</cell></row><row><cell cols="2">Max Pooling 2 × 2</cell><cell>Conv.ReLU</cell><cell>3 × 3 × 192</cell></row><row><cell cols="2">Dense.ReLU 200</cell><cell>Conv.ReLU</cell><cell>3 × 3 × 192</cell></row><row><cell cols="2">Dense.ReLU 200</cell><cell cols="2">Max Pooling 2 × 2</cell></row><row><cell>Softmax</cell><cell>10</cell><cell>Conv.ReLU</cell><cell>3 × 3 × 192</cell></row><row><cell></cell><cell></cell><cell>Conv.ReLU</cell><cell>1 × 1 × 192</cell></row><row><cell></cell><cell></cell><cell>Conv.ReLU</cell><cell>1 × 1 × 10</cell></row><row><cell></cell><cell></cell><cell cols="2">Global Average Pooling</cell></row><row><cell></cell><cell></cell><cell>Softmax</cell><cell>10</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Training parameters of classifiers to be protected</figDesc><table><row><cell>Parameters</cell><cell cols="2">MNIST CIFAR</cell></row><row><cell cols="2">Optimization Method SGD</cell><cell>SGD</cell></row><row><cell>Learning Rate</cell><cell>0.01</cell><cell>0.01</cell></row><row><cell>Batch Size</cell><cell>128</cell><cell>32</cell></row><row><cell>Epochs</cell><cell>50</cell><cell>350</cell></row><row><cell>Data Augmentation</cell><cell>-</cell><cell>Shifting + Horizontal Flip</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Training parameters for defensive devices.</figDesc><table><row><cell>Parameters</cell><cell>MNIST</cell><cell>CIFAR</cell></row><row><cell cols="2">Optimization Method Adam</cell><cell>Adam</cell></row><row><cell>Learning Rate</cell><cell>0.001</cell><cell>0.001</cell></row><row><cell>Batch Size</cell><cell>256</cell><cell>256</cell></row><row><cell>Epochs</cell><cell>100</cell><cell>400</cell></row><row><cell>Regularization</cell><cell cols="2">L 2 (10 −9 ) Noise</cell></row><row><cell cols="3">Below we use the criteria described and justified in Section 3.2</cell></row><row><cell cols="3">to evaluate the accuracy of MagNet on normal and adversarial</cell></row><row><cell>examples.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Classification accuracy of MagNet on adversarial examples generated by different attack methods. Some of these attacks have different parameters on MNIST and CIFAR-10 because they need to adjust their parameters according to datasets.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">(a) MNIST</cell><cell></cell></row><row><cell>Attack</cell><cell cols="4">Norm Parameter No Defense With Defense</cell></row><row><cell>FGSM</cell><cell>L ∞</cell><cell>ϵ = 0.005</cell><cell>96.8%</cell><cell>100.0%</cell></row><row><cell>FGSM</cell><cell>L ∞</cell><cell>ϵ = 0.010</cell><cell>91.1%</cell><cell>100.0%</cell></row><row><cell cols="2">Iterative L ∞</cell><cell>ϵ = 0.005</cell><cell>95.2%</cell><cell>100.0%</cell></row><row><cell cols="2">Iterative L ∞</cell><cell>ϵ = 0.010</cell><cell>72.0%</cell><cell>100.0%</cell></row><row><cell cols="2">Iterative L 2</cell><cell>ϵ = 0.5</cell><cell>86.7%</cell><cell>99.2%</cell></row><row><cell cols="2">Iterative L 2</cell><cell>ϵ = 1.0</cell><cell>76.6%</cell><cell>100.0%</cell></row><row><cell cols="2">Deepfool L ∞</cell><cell></cell><cell>19.1%</cell><cell>99.4%</cell></row><row><cell>Carlini</cell><cell>L 2</cell><cell></cell><cell>0.0%</cell><cell>99.5%</cell></row><row><cell>Carlini</cell><cell>L ∞</cell><cell></cell><cell>0.0%</cell><cell>99.8%</cell></row><row><cell>Carlini</cell><cell>L 0</cell><cell></cell><cell>0.0%</cell><cell>92.0%</cell></row><row><cell></cell><cell></cell><cell cols="2">(b) CIFAR</cell><cell></cell></row><row><cell>Attack</cell><cell cols="4">Norm Parameter No Defense With Defense</cell></row><row><cell>FGSM</cell><cell>L ∞</cell><cell>ϵ = 0.025</cell><cell>46.0%</cell><cell>99.9%</cell></row><row><cell>FGSM</cell><cell>L ∞</cell><cell>ϵ = 0.050</cell><cell>40.5%</cell><cell>100.0%</cell></row><row><cell cols="2">Iterative L ∞</cell><cell>ϵ = 0.010</cell><cell>28.6%</cell><cell>96.0%</cell></row><row><cell cols="2">Iterative L ∞</cell><cell>ϵ = 0.025</cell><cell>11.1%</cell><cell>99.9%</cell></row><row><cell cols="2">Iterative L 2</cell><cell>ϵ = 0.25</cell><cell>18.4%</cell><cell>76.3%</cell></row><row><cell cols="2">Iterative L 2</cell><cell>ϵ = 0.50</cell><cell>6.6%</cell><cell>83.3%</cell></row><row><cell cols="2">Deepfool L ∞</cell><cell></cell><cell>4.5%</cell><cell>93.4%</cell></row><row><cell>Carlini</cell><cell>L 2</cell><cell></cell><cell>0.0%</cell><cell>93.7%</cell></row><row><cell>Carlini</cell><cell>L ∞</cell><cell></cell><cell>0.0%</cell><cell>83.0%</cell></row><row><cell>Carlini</cell><cell>L 0</cell><cell></cell><cell>0.0%</cell><cell>77.5%</cell></row><row><cell cols="5">MagNet is effective and generalizes well to different attacks and</cell></row><row><cell cols="4">different parameters of the same attack.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>. For CIFAR-10,</figDesc><table><row><cell></cell><cell>100%</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Classification accuracy</cell><cell>20% 40% 60% 80%</cell><cell cols="2">No fefense With detector With reformer With detector &amp; reformer</cell><cell></cell></row><row><cell></cell><cell>0%</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>5</cell><cell>10 Confidence in Carlini L 2 attack 15 20 25 30</cell><cell>35</cell><cell>40</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Classification accuracy in percentage on adversarial examples generated by graybox attack on CIFAR-10. We name each autoencoder A through H. Each column corresponds to an autoencoder that the attack is trained on, and each row corresponds to an autoencoder that is used during testing. The last row, random, means that MagNet picks a random one from its eight autoencoders. .0 91.8 92.6 91.4 92.3 92.4 0.0 Random 81.1 81.4 80.8 81.3 80.3 81.3 80.5 81.7</figDesc><table><row><cell></cell><cell>A</cell><cell>B</cell><cell>C</cell><cell>D</cell><cell>E</cell><cell>F</cell><cell>G</cell><cell>H</cell></row><row><cell>A</cell><cell>0.0</cell><cell cols="7">92.8 92.5 93.1 91.8 91.8 92.5 93.6</cell></row><row><cell>B</cell><cell cols="2">92.1 0.0</cell><cell cols="6">92.0 92.5 91.4 92.5 91.3 92.5</cell></row><row><cell>C</cell><cell cols="3">93.2 93.8 0.0</cell><cell cols="5">92.8 93.3 94.1 92.7 93.6</cell></row><row><cell>D</cell><cell cols="4">92.8 92.2 91.3 0.0</cell><cell cols="4">91.7 92.8 91.2 93.9</cell></row><row><cell>E</cell><cell cols="5">93.3 94.0 93.4 93.2 0.0</cell><cell cols="3">93.4 91.0 92.8</cell></row><row><cell>F</cell><cell cols="6">92.8 93.1 93.2 93.6 92.2 0.0</cell><cell cols="2">92.8 93.8</cell></row><row><cell>G</cell><cell cols="7">92.5 93.1 92.0 92.2 90.5 93.5 0.1</cell><cell>93.4</cell></row><row><cell>H</cell><cell cols="2">92.3 92</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Classification accuracy in percentage on the test set for CIFAR-10. Each column corresponds to a different autoencoder chosen during testing. "Rand" means that Mag-Net randomly chooses an autoencoder during testing.</figDesc><table><row><cell>AE A</cell><cell>B</cell><cell>C</cell><cell>D</cell><cell>E</cell><cell>F</cell><cell>G</cell><cell>H</cell><cell>Rand</cell></row><row><cell cols="9">Acc 89.2 88.7 89.0 89.0 88.7 89.3 89.2 89.1 89.0</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Imagine the manifold of normal examples as a magnet and test examples as iron particles in a high-dimensional space. The magnet is able to attract and move nearby particles (illustrating the effect of the reformer) but is unable to move distant particles (illustrating the effect of the detectors).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank Dr. Xuming He and anonymous reviewers for their valuable feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">End to end learning for self-driving cars</title>
		<author>
			<persName><forename type="first">Mariusz</forename><surname>Bojarski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><forename type="middle">Del</forename><surname>Testa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Dworakowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Firner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beat</forename><surname>Flepp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prasoon</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathew</forename><surname>Monfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Urs</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiakai</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.07316</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards evaluating the robustness of neural networks</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Security and Privacy</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning transferable policies for monocular reactive mav control</title>
		<author>
			<persName><forename type="first">Shreyansh</forename><surname>Daftry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.00627</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deep visual foresight for planning robot motion</title>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.00696</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deep Learning</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">On the (statistical) detection of adversarial examples</title>
		<author>
			<persName><forename type="first">Kathrin</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Praveen</forename><surname>Manoharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Backes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.06280</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Adversarial perturbations against deep neural networks for malware classification</title>
		<author>
			<persName><forename type="first">Kathrin</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Praveen</forename><surname>Manoharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Backes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04435</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition: the shared views of four research groups</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Poster: deep learning for zero-day flash malware detection</title>
		<author>
			<persName><forename type="first">Wookhyun</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sangwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sangyong</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">36th IEEE Symposium on Security and Privacy</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Uncertainty-aware reinforcement learning for collision avoidance</title>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Villaflor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitchyr</forename><surname>Pong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.01182</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adversarial examples for generative models</title>
		<author>
			<persName><forename type="first">Jernej</forename><surname>Kos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.06832</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Ask me anything: dynamic memory networks for natural language processing</title>
		<author>
			<persName><forename type="first">Ankit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Ondruska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1378" to="1387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adversarial examples in the physical world</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<idno>CoRR, abs/1607.02533</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">The mnist database of handwritten digits</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><surname>Burges</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Delving into transferable adversarial examples and black-box attacks</title>
		<author>
			<persName><forename type="first">Yanpei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On detecting adversarial perturbations</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Hendrik Metzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Genewein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Volker</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bastian</forename><surname>Bischoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2017">April 24-26, 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Universal adversarial perturbations</title>
		<author>
			<persName><forename type="first">Seyed-Mohsen</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alhussein</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omar</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Frossard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.08401</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Deepfool: a simple and accurate method to fool deep neural networks</title>
		<author>
			<persName><forename type="first">Seyed-Mohsen</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alhussein</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Frossard</surname></persName>
		</author>
		<idno>CoRR, abs/1511.04599</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sample complexity of testing the manifold hypothesis</title>
		<author>
			<persName><forename type="first">H</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mitter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The limitations of deep learning in adversarial settings</title>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fredrikson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">B</forename><surname>Celik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE European Symposium on Security and Privacy</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Distillation as a defense to adversarial perturbations against deep neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Security and Privacy</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Sheatsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reuben</forename><surname>Feinman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.00768</idno>
		<title level="m">Cleverhans v1.0.0: an adversarial machine learning library</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Crafting adversarial input sequences for recurrent neural networks</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">D</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Harang</surname></persName>
		</author>
		<idno>CoRR, abs/1604.08275</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Practical blackbox attacks against machine learning</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Berkay Celik</surname></persName>
		</author>
		<author>
			<persName><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security</title>
				<meeting>the 2017 ACM on Asia Conference on Computer and Communications Security</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="506" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Malware classification with recurrent networks</title>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><forename type="middle">W</forename><surname>Stokes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hermineh</forename><surname>Sanossian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mady</forename><surname>Marinescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anil</forename><surname>Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1916" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Understanding adversarial training: increasing local stability of neural nets through robust optimization</title>
		<author>
			<persName><forename type="first">Uri</forename><surname>Shaham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutaro</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sahand</forename><surname>Negahban</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05432</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep learning in medical image analysis</title>
		<author>
			<persName><forename type="first">Dinggang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guorong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heung-Il</forename><surname>Suk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Biomedical Engineering</title>
		<imprint>
			<biblScope unit="issue">0</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Deep learning for mortgage risk</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Sirignano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apaar</forename><surname>Sadhwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kay</forename><surname>Giesecke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Striving for simplicity: the all convolutional net</title>
		<author>
			<persName><forename type="first">Jost</forename><surname>Tobias Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6806</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
				<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1096" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabelle</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3371" to="3408" />
			<date type="published" when="2010-12">Dec. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Adversarial examples for semantic segmentation and object detection</title>
		<author>
			<persName><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuyin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.08603</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
