<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Speech 2 : End-to-End Speech Recognition in English and Mandarin</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sundaram</forename><surname>Ananthanarayanan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Rishita</forename><surname>Anubhai</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jingliang</forename><surname>Bai</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Eric</forename><surname>Battenberg</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Carl</forename><surname>Case</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jared</forename><surname>Casper</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Qiang</forename><surname>Cheng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Guoliang</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jie</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jingdong</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhijie</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mike</forename><surname>Chrzanowski</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Adam</forename><surname>Coates</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Greg</forename><surname>Diamos</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ke</forename><surname>Ding</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Niandong</forename><surname>Du</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Erich</forename><surname>Elsen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jesse</forename><surname>Engel</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Weiwei</forename><surname>Fang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Linxi</forename><surname>Fan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Christopher</forename><surname>Fougner</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Liang</forename><surname>Gao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Caixia</forename><surname>Gong</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Awni</forename><surname>Hannun</surname></persName>
						</author>
						<author>
							<persName><roleName>Lappi</roleName><forename type="first">Tony</forename><surname>Han</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Vaino</forename><surname>Johannes</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Bing</forename><surname>Jiang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Cai</forename><surname>Ju</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Billy</forename><surname>Jun</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Libby</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Junjie</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Weigao</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xiangang</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dongpeng</forename><surname>Ma</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Andrew</forename><surname>Ng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yiping</forename><surname>Peng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ryan</forename><surname>Prenger</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sheng</forename><surname>Qian</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zongfeng</forename><surname>Quan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jonathan</forename><surname>Raiman</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Vinay</forename><surname>Rao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Seetapun</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shubho</forename><surname>Sengupta</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Likai</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Bo</forename><surname>Wei</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Wen</forename><surname>Xiao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yan</forename><surname>Xie</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dani</forename><surname>Xie</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Bin</forename><surname>Yogatama</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jun</forename><surname>Yuan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhenyao</forename><surname>Zhan</surname></persName>
						</author>
						<author>
							<persName><surname>Zhu</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Kavya Srinet</orgName>
								<address>
									<addrLine>Anuroop Sriram, Liliang Tang, Chong Wang, Yi Wang, Zhijian Wang</addrLine>
									<settlement>Haiyuan Tang, Jidong Wang, Kaifu Wang, Zhiqian Wang, Shuang</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">Lab 1</orgName>
								<orgName type="institution">Baidu Silicon Valley AI</orgName>
								<address>
									<addrLine>1195 Bordeaux Avenue</addrLine>
									<postCode>94086</postCode>
									<settlement>Sunnyvale</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Baidu Speech Technology Group</orgName>
								<address>
									<addrLine>No. 10 Xibeiwang East Street, Ke Ji Yuan</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Haidian District</orgName>
								<address>
									<postCode>100193</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">CHINA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Speech 2 : End-to-End Speech Recognition in English and Mandarin</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech-two vastly different languages. Because it replaces entire pipelines of hand-engineered components with neural networks, end-to-end learning allows us to handle a diverse variety of speech including noisy environments, accents and different languages. Key to our approach is our application of HPC techniques, enabling experiments that previously took weeks to now run in days. This allows us to iterate more quickly to identify superior architectures and algorithms. As a result, in several cases, our system is competitive with the transcription of human workers when benchmarked on standard datasets. Finally, using a technique called Batch Dispatch with GPUs in the data center, we show that our system can be inexpensively deployed in an online setting, delivering low latency when serving users at scale.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Decades worth of hand-engineered domain knowledge has gone into current state-of-the-art automatic speech recognition (ASR) pipelines. A simple but powerful alternative solution is to train such ASR models end-to-end, using deep 1 Contact author: sanjeevsatheesh@baidu.com Proceedings of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W&amp;CP volume 48. Copyright 2016 by the author(s).</p><p>learning to replace most modules with a single model as in <ref type="bibr" target="#b19">(Hannun et al., 2014a)</ref> and <ref type="bibr" target="#b16">(Graves &amp; Jaitly, 2014b)</ref>. This "end to end" vision of training simplyfies the training process by removing the engineering required for the bootstrapping/alignment/clustering/HMM machinery often used to build state-of-the-art ASR models. On such a system, built on end-to-end deep learning, we can employ a range of deep learning techniques: capturing large training sets, training larger models with high performance computing, and methodically exploring the space of neural network architectures.</p><p>This paper details our contribution to the model architecture, large labeled training datasets, and computational scale for speech recognition. This includes an extensive investigation of model architectures, and our data capturing pipeline that has enabled us to create larger datasets than what is typically used to train speech recognition systems. We benchmark our system on several publicly available test sets with a goal of eventually attaining human-level performance. To that end, we have also measured the performance of crowd workers on each benchmark for comparison. We find that our best Mandarin Chinese speech system transcribes short voice-query like utterances better than a typical Mandarin Chinese speaker.</p><p>The remainder of the paper is as follows. We begin with a review of related work in deep learning, end-to-end speech recognition, and scalability in Section 2. Section 3 describes the architectural and algorithmic improvements to the model and Section 4 explains how to efficiently compute them. We discuss the training data and steps taken to further augment the training set in Section 5. An analysis of results for our system in English and Mandarin is presented in Section 6. We end with a description of the steps needed to deploy our system to real users in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>This work is inspired by previous work in both deep learning and speech recognition. Feed-forward neural network acoustic models were explored more than 20 years ago <ref type="bibr" target="#b4">(Bourlard &amp; Morgan, 1993;</ref><ref type="bibr" target="#b38">Renals et al., 1994)</ref>. Recurrent neural networks and networks with convolution were also used in speech recognition around the same time <ref type="bibr" target="#b39">(Robinson et al., 1996;</ref><ref type="bibr" target="#b48">Waibel et al., 1989)</ref>. More recently DNNs have become a fixture in the ASR pipeline with almost all state of the art speech work containing some form of deep neural network <ref type="bibr">(Mohamed et al., 2011;</ref><ref type="bibr" target="#b21">Hinton et al., 2012;</ref><ref type="bibr" target="#b11">Dahl et al., 2011;</ref><ref type="bibr">N. Jaitly &amp; Vanhoucke, 2012;</ref><ref type="bibr" target="#b44">Seide et al., 2011)</ref>. Convolutional networks have also been found beneficial for acoustic models <ref type="bibr" target="#b0">(Abdel-Hamid et al., 2012;</ref><ref type="bibr" target="#b41">Sainath et al., 2013)</ref>. Recurrent neural networks are beginning to be deployed in state-of-the art recognizers <ref type="bibr" target="#b17">(Graves et al., 2013;</ref><ref type="bibr" target="#b18">H. Sak et al., 2014)</ref> and work well with convolutional layers for the feature extraction <ref type="bibr" target="#b40">(Sainath et al., 2015)</ref>.</p><p>End-to-end speech recognition is an active area of research, showing compelling results when used to rescore the outputs of a DNN-HMM <ref type="bibr" target="#b14">(Graves &amp; Jaitly, 2014a)</ref> and standalone <ref type="bibr" target="#b19">(Hannun et al., 2014a)</ref>. The RNN encoder-decoder with attention performs well in predicting phonemes <ref type="bibr" target="#b8">(Chorowski et al., 2015)</ref> or graphemes <ref type="bibr" target="#b1">(Bahdanau et al., 2015;</ref><ref type="bibr" target="#b5">Chan et al., 2015)</ref>. The CTC loss function <ref type="bibr" target="#b15">(Graves et al., 2006)</ref> coupled with an RNN to model temporal information also performs well in endto-end speech recognition with character outputs <ref type="bibr" target="#b14">(Graves &amp; Jaitly, 2014a;</ref><ref type="bibr" target="#b20">Hannun et al., 2014b;</ref><ref type="bibr">a;</ref><ref type="bibr" target="#b29">Maas et al., 2015)</ref>. The CTC-RNN model also works well in predicting phonemes <ref type="bibr" target="#b30">(Miao et al., 2015;</ref><ref type="bibr" target="#b42">Sak et al., 2015)</ref>, though a lexicon is still needed in this case.</p><p>Exploiting scale in deep learning has been central to the success of the field thus far <ref type="bibr" target="#b24">(Krizhevsky et al., 2012;</ref><ref type="bibr" target="#b26">Le et al., 2012)</ref>. Training on a single GPU resulted in substantial performance gains <ref type="bibr" target="#b36">(Raina et al., 2009)</ref>, which were subsequently scaled linearly to two <ref type="bibr" target="#b24">(Krizhevsky et al., 2012)</ref> or more GPUs <ref type="bibr" target="#b10">(Coates et al., 2013)</ref>. We take advantage of work in increasing individual GPU efficiency for low-level deep learning primitives <ref type="bibr">(Chetlur et al.)</ref>. We built on the past work in using model-parallelism <ref type="bibr" target="#b10">(Coates et al., 2013)</ref>, data-parallelism <ref type="bibr" target="#b12">(Dean et al., 2012)</ref> or a combination of the two <ref type="bibr" target="#b47">(Szegedy et al., 2014;</ref><ref type="bibr" target="#b19">Hannun et al., 2014a)</ref> to create a fast and highly scalable system for training deep RNNs in speech recognition.</p><p>Data has also been central to the success of end-to-end speech recognition, with over 7000 hours of labeled speech used in <ref type="bibr" target="#b19">(Hannun et al., 2014a)</ref>. Data augmentation has been highly effective in improving the performance of deep learning in computer vision <ref type="bibr" target="#b27">(LeCun et al., 2004;</ref><ref type="bibr" target="#b43">Sapp et al., 2008;</ref><ref type="bibr" target="#b9">Coates et al., 2011)</ref> and speech recognition (Gales  <ref type="bibr">et al., 2009;</ref><ref type="bibr" target="#b19">Hannun et al., 2014a)</ref>. Existing speech systems can also be used to bootstrap new data collection. For example, an existing speech engine can be used to align and filter thousands of hours of audiobooks <ref type="bibr" target="#b34">(Panayotov et al., 2015)</ref>. We draw inspiration from these past approaches in bootstrapping larger datasets and data augmentation to increase the effective amount of labeled data for our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Model Architecture</head><p>Figure <ref type="figure">1</ref> shows the wireframe of our architecture, and lays out the swappable components which we explore in detail in this paper. Our system (similar at its core to the one in <ref type="bibr" target="#b19">(Hannun et al., 2014a)</ref>), is a recurrent neural network (RNN) with one or more convolutional input layers, followed by multiple recurrent (uni or bidirectional) layers and one fully connected layer before a softmax layer.</p><p>The network is trained end-to-end using the CTC loss function <ref type="bibr" target="#b15">(Graves et al., 2006)</ref>, which allows us to directly predict the sequences of characters from input audio.<ref type="foot" target="#foot_0">2</ref> </p><p>The inputs to the network are a sequence of logspectrograms of power normalized audio clips, calculated on 20ms windows. The outputs are the alphabet of each language. At each output time-step t, the RNN makes a prediction, p( t |x), where t is either a character in the alphabet or the blank symbol. In English we have t ∈ {a, b, c, . . . , z, space, apostrophe, blank}, where we have added the space symbol to denote word boundaries. For the Mandarin system the network outputs simplified Chi- At inference time, CTC models are paired a with language model trained on a bigger corpus of text. We use a specialized beam search <ref type="bibr" target="#b20">(Hannun et al., 2014b)</ref> to find the transcription y that maximizes</p><formula xml:id="formula_0">Q(y) = log(p RNN (y|x)) + α log(p LM (y)) + βwc(y) (1)</formula><p>where wc(y) is the number of words (English) or characters (Chinese) in the transcription y. The weight α controls the relative contributions of the language model and the CTC network. The weight β encourages more words in the transcription. These parameters are tuned on a held out development set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Batch Normalization for Deep RNNs</head><p>To efficiently absorb data as we scale the training set, we increase the depth of the networks by adding more recurrent layers. However, it becomes more challenging to train networks using gradient descent as the size and depth increases. We have experimented with the Batch Normalization (BatchNorm) method to train deeper nets faster <ref type="bibr" target="#b23">(Ioffe &amp; Szegedy, 2015)</ref>. Recent research has shown that Batch-Norm can speed convergence of RNNs training, though not always improving generalization error <ref type="bibr" target="#b25">(Laurent et al., 2015)</ref>. In contrast, we find that when applied to very deep networks of RNNs on large data sets, the variant of Batch-Norm we use substantially improves final generalization error in addition to accelerating training.</p><p>A recurrent layer is implemented as</p><formula xml:id="formula_1">h l t = f (W l h l−1 t + U l h l t−1 + b).<label>(2)</label></formula><p>where the activations of layer l at time step t are computed by combining the activations from the previous layer h l−1 t at the same time step t and the activations from the current layer at the previous time step h l t−1 . We see a wider gap in performance on the deeper 9-7 network (which has 9 layers in total, 7 of which are vanilla bidirectional RNNs) than the shallower 5-1 network (in which only 1 of the 5 layers is a bidirectional RNN). We start the plot after the first epoch of training as the curve is more difficult to interpret due to the SortaGrad curriculum method mentioned in Section 3.2</p><p>As in <ref type="bibr" target="#b25">(Laurent et al., 2015)</ref>, there are two ways of applying BatchNorm to the recurrent operation. A natural extension is to insert a BatchNorm transformation, B(•), immediately before every non-linearity as follows:</p><formula xml:id="formula_2">h l t = f (B(W l h l−1 t + U l h l t−1 )).<label>(3)</label></formula><p>In this case the mean and variance statistics are accumulated over a single time-step of the minibatch. We did not find this to be effective.</p><p>An alternative (sequence-wise normalization) is to batch normalize only the vertical connections. The recurrent computation is given by</p><formula xml:id="formula_3">h l t = f (B(W l h l−1 t ) + U l h l t−1 ).<label>(4)</label></formula><p>For each hidden unit we compute the mean and variance statistics over all items in the minibatch over the length of the sequence. Figure <ref type="figure" target="#fig_0">2</ref> shows that deep networks converge faster with sequence-wise normalization. Table <ref type="table">1</ref> shows that the performance improvement from sequencewise normalization increases with the depth of the network, with a 12% performance difference for the deepest network. We store a running average of the mean and variance for the neuron collected during training, and use these for evaluation <ref type="bibr" target="#b23">(Ioffe &amp; Szegedy, 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">SortaGrad</head><p>Even with Batch Normalization, we find training with CTC to be occasionally unstable, particularly in the early stages.</p><p>In order to make training more stable, we experiment with a training curriculum <ref type="bibr">(Bengio et al., 2009;</ref><ref type="bibr" target="#b50">Zaremba &amp; Sutskever, 2014)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Comparison of vanilla RNNs and GRUs</head><p>The models we have shown so far are vanilla RNNs which are modeled by Equation 3 with ReLU activations. More sophisticated hidden units such as the Long Short-Term Memory (LSTM) units <ref type="bibr" target="#b22">(Hochreiter &amp; Schmidhuber, 1997)</ref> and the Gated Recurrent Units (GRU) <ref type="bibr" target="#b7">(Cho et al., 2014)</ref>, have been shown to be very effective on similar tasks <ref type="bibr" target="#b1">(Bahdanau et al., 2015)</ref>. We examine GRUs because experiments on smaller data sets show the GRU and LSTM reach similar accuracy for the same number of parameters, but the GRUs are faster to train and less likely to diverge.</p><p>Both GRU and vanilla RNN architectures benefit from BatchNorm and show strong results with deep networks. The last two columns in table <ref type="table">1</ref> show that for a fixed number of parameters the GRU architecture achieves better WER for all network depths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Frequency Convolutions</head><p>Temporal convolution is commonly used in speech recognition to efficiently model temporal translation invariance for variable length utterances. Convolution in frequency attempts to model spectral variance due to speaker variability more concisely than what is possible with large fully connected networks.</p><p>We experiment with adding between one and three layers of convolution. These are both in the time-and-frequency domain (2D) and in the time-only domain (1D). In all cases we use a "same" convolution. In some cases we specify a stride (subsampling) across either dimension which reduces the size of the output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recurrent layer</head><p>Row conv layer We report results on two datasets-a development set of 2048 utterances ("Regular Dev") and a much noisier dataset of 2048 utterances ("Noisy Dev") randomly sampled from the CHiME 2015 development datasets <ref type="bibr" target="#b2">(Barker et al., 2015)</ref>. We find that multiple layers of 1D convolution provides a very small benefit. The 2D convolutions improve results substantially on noisy data, while providing a small benefit on clean data. The change from one layer of 1D convolution to three layers of 2D convolution improves WER by 23.9% on the noisy development set.</p><formula xml:id="formula_4">h t h t+1 h t+2 h t+3 r t+3 r t+2 r t+1 r t</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Lookahead Convolution and Unidirectional Models</head><p>Bidirectional RNN models are challenging to deploy in an online, low-latency setting because they cannot stream the transcription process as the utterance arrives from the user. However, models with only forward recurrences routinely perform worse than similar bidirectional models, implying some amount of future context is vital to good performance. One possible solution is to delay the system from emitting a label until it has more context as in <ref type="bibr" target="#b42">(Sak et al., 2015)</ref>, but we found it difficult to induce this behavior in our models. In order to build a unidirectional model without any loss in accuracy, we develop a special layer that we call lookahead convolution, shown in Figure <ref type="figure" target="#fig_1">3</ref>. The layer learns weights to linearly combine each neuron's activations τ timesteps into the future, and thus allows us to control the amount of future context needed. The lookahead layer is defined by a parameter matrix W ∈ R (d,τ ) , where d matches the number of neurons in the previous layer. The activations r t for the new layer at time-step t are</p><formula xml:id="formula_5">r t,i = τ +1 j=1 W i,j h t+j−1,i , for 1 ≤ i ≤ d.<label>(5)</label></formula><p>We place the lookahead convolution above all recurrent layers. This allows us to stream all computation below the lookahead convolution on a finer granularity. In all cases, the convolutions are followed by 7 recurrent layers and 1 fully connected layer. For 2D convolutions the first dimension is frequency and the second dimension is time. Each model is trained with BatchNorm, SortaGrad, and has 35M parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Adaptation to Mandarin</head><p>To port a traditional speech recognition pipeline to another language typically requires a significant amount of new language-specific development. For example, one often needs to hand-engineer a pronunciation model <ref type="bibr" target="#b45">(Shan et al., 2010)</ref>. We may also need to explicitly model languagespecific pronunciation features, such as tones in Mandarin <ref type="bibr" target="#b45">(Shan et al., 2010;</ref><ref type="bibr" target="#b33">Niu et al., 2013)</ref>. Since our endto-end system directly predicts characters, these time consuming efforts are no longer needed. This has enabled us to quickly create an end-to-end Mandarin speech recognition system (that outputs Chinese characters) using the approach described above with only a few changes.</p><p>The only architectural changes we make to our networks are due to the characteristics of the Chinese character set. The network outputs probabilities for about 6000 characters, which includes the Roman alphabet, since hybrid Chinese-English transcripts are common. We incur an out of vocabulary error at evaluation time if a character is not contained in this set. This is not a major concern, as our test set has only 0.74% out of vocab characters.</p><p>We use a character level language model in Mandarin as words are not usually segmented in text. In Section 6.2 we show that our Mandarin speech models show roughly the same improvements to architectural changes as our English speech models, suggesting that modeling knowledge from development in one language transfers well to others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">System Optimizations</head><p>Our networks have tens of millions of parameters, and a training experiment involves tens of single-precision ex-aFLOPs. Since our ability to evaluate hypotheses about our data and models depends on training speed, we created a highly optimized training system based on high performance computing (HPC) infrastructure. 3 Although many frameworks exist for training deep networks on parallel 3 Our software runs on dense compute nodes with 8 NVIDIA Titan X GPUs per node with a theoretical peak throughput of 48 single-precision TFLOP/s. machines, we have found that our ability to scale well is often bottlenecked by unoptimized routines that are taken for granted. Therefore, we focus on careful optimization of the most important routines used for training. Specifically, we created customized All-Reduce code for OpenMPI to sum gradients across GPUs on multiple nodes, developed a fast implementation of CTC for GPUs, and use custom memory allocators. Taken together, these techniques enable us to sustain overall 45% of theoretical peak performance on each node.</p><p>Our training distributes work over multiple GPUs in a dataparallel fashion with synchronous SGD, where each GPU uses a local copy of the model to work on a portion of the current minibatch and then exchanges computed gradients with all other GPUs. We prefer synchronous SGD because it is reproducible, which facilitates discovering and fixing regressions. In this setup, however, the GPUs must communicate quickly (using an "All-Reduce" operation) at each iteration in order to avoid wasting computational cycles. Prior work has used asynchronous updates to mitigate this issue <ref type="bibr" target="#b12">(Dean et al., 2012;</ref><ref type="bibr" target="#b37">Recht et al., 2011)</ref>. We instead focused on optimizing the All-Reduce operation itself, achieving a 4x-21x speedup using techniques to reduce CPU-GPU communication for our specific workloads. Similarly, to enhance overall computation, we have used highly-optimized kernels from Nervana Systems and NVIDIA that are tuned for our deep learning applications. We similarly discovered that custom memory allocation routines were crucial to maximizing performance as they reduce the number of synchronizations between GPU and CPU.</p><p>We also found that the CTC cost computation accounted for a significant fraction of running time. Since no public well-optimized code for CTC existed, we developed a fast GPU implementation that reduced overall training time by 10-20%.<ref type="foot" target="#foot_1">4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Training Data</head><p>Large-scale deep learning systems require an abundance of labeled training data. For training our English model, we use 11,940 hours of labeled speech containing 8 million utterances, and the Mandarin system uses 9,400 hours of labeled speech containing 11 million utterances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Dataset Construction</head><p>Parts of the English and Mandarin datasets were created from raw data captured as long audio clips with noisy transcriptions. In order to segment the audio into several second long clips, we align the speech with the transcript. For a given audio-transcript pair (x, y), the most likely alignment is calculated as * = arg max ∈Align(x,y)</p><formula xml:id="formula_6">T t p ctc ( t |x; θ). (<label>6</label></formula><formula xml:id="formula_7">)</formula><p>This is essentially a Viterbi alignment found using a RNN model trained with CTC. Since the CTC loss function integrates over all alignments, this is not guaranteed to produce an accurate alignment. However, we found that this approach produces an accurate alignment when using a bidirectional RNN.</p><p>In order to filter out clips with poor transcriptions, we build a simple classifier with the following features: the raw CTC cost, the CTC cost normalized by the sequence length, the CTC cost normalized by the transcript length, the ratio of the sequence length to the transcript length, the number of words in the transcription and the number of characters in the transcription. We crowd source the labels for building this dataset. For the English dataset, we find that the filtering pipeline reduces the WER from 17% to 5% while retaining more than 50% of the examples.</p><p>Additionally, we dynamically augment the dataset by adding unique noise every epoch with an SNR between 0dB and 30dB, just as in <ref type="bibr" target="#b19">(Hannun et al., 2014a;</ref><ref type="bibr" target="#b40">Sainath et al., 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Scaling Data</head><p>We show the effect of increasing the amount of labeled training data on WER in Table <ref type="table">3</ref>. This is done by randomly sampling the full dataset before training. For each dataset, the model was trained for up to 20 epochs with early-stopping based on the error on a held out development set to prevent overfitting. The WER decreases by ∼40% relative for each factor of 10 increase in training set size. We also observe a consistent gap in WER (∼60% relative) between the regular and noisy datasets, implying that more data benefits both cases equally. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Results</head><p>To better assess the real-world applicability of our speech system, we evaluate on a wide range of test sets. We use several publicly available benchmarks and several test sets collected internally. All models are trained for 20 epochs on either the full English dataset, or the full Mandarin dataset described in Section 5. We use stochastic gradient descent with Nesterov momentum <ref type="bibr" target="#b46">(Sutskever et al., 2013)</ref> along with a minibatch of 512 utterances. If the norm of the gradient exceeds a threshold of 400, it is rescaled to 400 <ref type="bibr" target="#b35">(Pascanu et al., 2012)</ref>. The model which performs the best on a held-out development set during training is chosen for evaluation. The learning rate is chosen from [1 × 10 −4 , 6 × 10 −4 ] to yield fastest convergence and annealed by a constant factor of 1.2 after each epoch. We use a momentum of 0.99 for all models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">English</head><p>The best English model has 2 layers of 2D convolution, followed by 3 layers of unidirectional recurrent layers with 2560 GRU cells each, followed by a lookahead convolution layer with τ = 80, trained with BatchNorm and SortaGrad. We do not adapt the model to any of the speech conditions in the test sets. Language model decoding parameters are set once on a held-out development set.</p><p>We report results on several test sets for both our system and an estimate of human accuracy. We obtain a measure of human level performance by asking workers from Amazon Mechanical Turk to hand-transcribe all of our test sets. Crowdsourced workers are not as accurate as dedicated, trained transcriptionists. For example, <ref type="bibr" target="#b28">(Lippmann, 1997)</ref> find that human transcribers achieve close to 1% WER on the WSJ-Eval92 set, when they are motivated with extra reward for getting a lower WER, and automatic typo and spell corrections, and further reductions in error rates by using a committee of transcribers.</p><p>We employ the following mechanism without rewards and auto correct as a valid competing "ASR wizard-of-Oz" that we strive to outperform. every audio clip, on average about 5 seconds long each.</p><p>We then take the better of the two transcriptions for the final WER calculation. Most workers are based in the United States, are allowed to listen to the audio clip multiple times and on average spend 27 seconds per transcription. The hand-transcribed results are compared to the existing ground truth to produce a WER estimate. While the existing ground truth transcriptions do have some label error, on most sets it is less than 1%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1.">BENCHMARK RESULTS</head><p>Read speech with high signal-to-noise ratio is arguably the easiest task in large vocabulary continuous speech recognition. We benchmark our system on two test sets from the Wall Street Journal (WSJ) corpus of read news articles and the LibriSpeech corpus constructed from audio books <ref type="bibr" target="#b34">(Panayotov et al., 2015)</ref>. Table <ref type="table" target="#tab_5">4</ref> shows that our system outperforms crowd-sourced human workers on 3 out of 4 test sets.</p><p>We also tested our system for robustness to common accents using the VoxForge (http://www.voxforge.org) dataset. The set contains speech read by speakers with many different accents. We group these accents into four categories: American-Canadian, Indian, Commonwealth<ref type="foot" target="#foot_2">5</ref> and European<ref type="foot" target="#foot_3">6</ref> . We construct a test set from the VoxForge data with 1024 examples from each accent group for a total of 4096 examples. Human level performance is still notably better than that of our system for all but the Indian accent.</p><p>Finally, we tested our performance on noisy speech using the test sets from the recently completed third CHiME challenge <ref type="bibr" target="#b2">(Barker et al., 2015)</ref>. This dataset has utterances from the WSJ test set collected in real noisy environments and with artificially added noise. Using all 6 channels of the CHiME audio can provide substantial performance improvements <ref type="bibr" target="#b49">(Yoshioka et al., 2015)</ref>. We use a single channel for all our models, since access to multi-channel audio</p><p>is not yet pervasive. The gap between our system and human level performance is larger when the data comes from a real noisy environment instead of synthetically adding noise to clean speech.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Mandarin</head><p>In Table <ref type="table" target="#tab_6">5</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Deployment</head><p>Bidirectional models are not well-designed for real time transcription: since the RNN has several bidirectional layers, transcribing an utterance requires the entire utterance to be presented to the RNN; and since we use a wide beam search for decoding, beam search can be expensive.</p><p>To increase deployment scalability, while still providing low latency transcription, we built a batching scheduler called Batch Dispatch that assembles streams of data from user requests into batches before performing RNN forward propagation on these batches. With this scheduler, we can trade increased batch size, and consequently improved efficiency, with increased latency.</p><p>We use an eager batching scheme that processes each batch as soon as the previous batch is completed, regardless of how much work is ready by that point. This scheduling algorithm balances efficiency and latency, achieving relatively small dynamic batch sizes up to 10 samples per batch, with median batch size proportional to server load. We see in Table <ref type="table" target="#tab_9">7</ref> that our system achieves a median latency of 44 ms, and a 98th percentile latency of 70 ms when loaded with 10 concurrent streams. This server uses one NVIDIA Quadro K1200 GPU for RNN evaluation. As designed, Batch Dispatch shifts work to larger batches as server load grows, keeping latency low.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Load</head><p>Our deployment system evaluates RNNs in half-precision arithmetic, which has no measurable accuracy impact, but significantly improves efficiency. We wrote our own 16-bit matrix-matrix multiply routines for this task, substantially improving throughput for our relatively small batches.</p><p>Performing the beam search involves repeated lookups in the n-gram language model, most of which translate to uncached reads from memory. To reduce the cost of these lookups, we employ a heuristic: only consider the fewest number of characters whose cumulative probability is at least p. In practice, we find that p = 0.99 works well, and additionally we limit the search to 40 characters. This speeds up the cumulative Mandarin language model lookup time by a factor of 150x, and has a negligible effect on CER (0.1-0.3% relative).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Deep Speech in production environment</head><p>Deep Speech has been integrated with a state-of-the-art production speech pipeline for user applications. We have found several key challenges that affect the deployment of end-to-end deep learning methods like ours. First, we have found that even modest amounts of application-specific training data is invaluable despite the large quantities of general speech data used for training. For example, while we are able to train on more than 10,000 hours of Mandarin speech, we find that the addition of just 500 hours of application-specific data can significantly enhance performance for the application. Similarly, application-specific language models are important for achieving top accuracy and we leverage strong existing n-gram models with our Deep Speech system. Finally, we note that since our system is trained from a wide range of labeled training data to output characters directly, there are idiosyncratic conventions for transcriptions in each application that must be handled in post-processing (such as the formatting of digits). Thus, while our model has removed many complexities, more flexibility and application-awareness for end-toend deep learning methods are open areas for further research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>End-to-end deep learning presents the exciting opportunity to improve speech recognition systems continually with increases in data and computation. Since the approach is highly generic, we have shown that it can quickly be applied to new languages. Creating high-performing recognizers for two very different languages, English and Mandarin, required essentially no expert knowledge of the languages. Finally, we have also shown that this approach can be efficiently deployed by batching user requests together on a GPU server, paving the way to deliver end-toend Deep Learning technologies to users.</p><p>To achieve these results, we have explored various network architectures, finding several effective techniques: enhancements to numerical optimization through Sorta-Grad and Batch Normalization, and lookahead convolution for unidirectional models. This exploration was powered by a well optimized, high performance computing inspired training system that allows us to train full-scale models on our large datasets in just a few days.</p><p>Overall, we believe our results confirm and exemplify the value of end-to-end deep learning methods for speech recognition in several settings. We believe these techniques will continue to scale.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Training curves of two models trained with and without BatchNorm (BN). We see a wider gap in performance on the deeper 9-7 network (which has 9 layers in total, 7 of which are vanilla bidirectional RNNs) than the shallower 5-1 network (in which only 1 of the 5 layers is a bidirectional RNN). We start the plot after the first epoch of training as the curve is more difficult to interpret due to the SortaGrad curriculum method mentioned in Section 3.2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Lookahead convolution architecture with future context size of 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Training very deep networks (or RNNs with many steps) from scratch can fail early in training since outputs and gradients must be propagated through many poorly tuned layers of weights. In addition to exploding gradients<ref type="bibr" target="#b35">(Pascanu et al., 2012)</ref>, CTC often ends up assigning near-zero probability to very long transcriptions making gradient descent quite volatile. This observation motivates a curriculum learning strategy we title SortaGrad: we use the length of the utterance as a heuristic for difficulty and train on the shorter (easier) utterances first. Specifically, in the first training epoch we iterate through minibatches in the training set in increasing order of the length of the longest utterance in the minibatch. After the first epoch training reverts back to a random order over minibatches. Table1shows a comparison of training cost with and without SortaGrad on the 9 layer model with 7 recurrent layers. SortaGrad improves the stability of training, and this effect is particularly pronounced in networks without BatchNorm, since these are even less numerically stable.</figDesc><table /><note>, which accelerates training and results in better generalization as well.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison of WER for different configurations of convolutional layers.</figDesc><table><row><cell cols="2">Architecture Channels</cell><cell>Filter dimension</cell><cell>Stride</cell><cell cols="2">Regular Dev Noisy Dev</cell></row><row><cell>1-layer 1D</cell><cell>1280</cell><cell>11</cell><cell>2</cell><cell>9.52</cell><cell>19.36</cell></row><row><cell>2-layer 1D</cell><cell>640, 640</cell><cell>5, 5</cell><cell>1, 2</cell><cell>9.67</cell><cell>19.21</cell></row><row><cell>3-layer 1D</cell><cell cols="2">512, 512, 512 5, 5, 5</cell><cell>1, 1, 2</cell><cell>9.20</cell><cell>20.22</cell></row><row><cell>1-layer 2D</cell><cell>32</cell><cell>41x11</cell><cell>2x2</cell><cell>8.94</cell><cell>16.22</cell></row><row><cell>2-layer 2D</cell><cell>32, 32</cell><cell>41x11, 21x11</cell><cell>2x2, 2x1</cell><cell>9.06</cell><cell>15.71</cell></row><row><cell>3-layer 2D</cell><cell>32, 32, 96</cell><cell cols="2">41x11, 21x11, 21x11 2x2, 2x1, 2x1</cell><cell>8.61</cell><cell>14.74</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Two random workers transcribe Comparison of WER for our speech system and crowd-sourced human level performance.</figDesc><table><row><cell></cell><cell>Test set</cell><cell cols="2">Ours Human</cell></row><row><cell></cell><cell>WSJ eval'92</cell><cell>3.10</cell><cell>5.03</cell></row><row><cell>Read</cell><cell>WSJ eval'93 LibriSpeech test-clean</cell><cell>4.42 5.15</cell><cell>8.08 5.83</cell></row><row><cell></cell><cell>LibriSpeech test-other</cell><cell>12.73</cell><cell>12.69</cell></row><row><cell>Accented</cell><cell>VoxForge American-Canadian VoxForge Commonwealth VoxForge European VoxForge Indian</cell><cell>7.94 14.85 18.44 22.89</cell><cell>4.85 8.15 12.76 22.15</cell></row><row><cell>Noisy</cell><cell>CHiME eval real CHiME eval sim</cell><cell>21.59 42.55</cell><cell>11.84 31.33</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>we compare several architectures trained on Mandarin Chinese speech on a development set of 2000 utterances as well as a test set of 1882 examples of noisy speech. This development set was also used to tune the decoding parameters. We see that the deepest model with 2D convolution and BatchNorm outperforms the shallow RNN by 48% relative. Comparison of the different RNN architectures. The development and test sets are internal corpora. Each model in the table has about 80 million parameters.</figDesc><table><row><cell>Architecture</cell><cell>Dev</cell><cell>Test</cell></row><row><cell>5-layer, 1 RNN</cell><cell cols="2">7.13 15.41</cell></row><row><cell>5-layer, 3 RNN</cell><cell cols="2">6.49 11.85</cell></row><row><cell cols="2">5-layer, 3 RNN + BatchNorm 6.22</cell><cell>9.39</cell></row><row><cell>9-layer, 7 RNN + BatchNorm</cell><cell></cell><cell></cell></row><row><cell>+ frequency Convolution</cell><cell>5.81</cell><cell>7.93</cell></row><row><cell>Test</cell><cell cols="2">Human RNN</cell></row><row><cell>100 utterances / committee</cell><cell>4.0</cell><cell>3.7</cell></row><row><cell>250 utterances / individual</cell><cell>9.7</cell><cell>5.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>We benchmark the best Mandarin system against humans on two randomly selected test sets. The first set has 100 examples and is labelled by a committee of 5 Chinese speakers. The second has 250 examples and is labelled by a single human transcriber.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell>shows that our best Mandarin Chinese speech sys-</cell></row><row><cell>tem transcribes short voice-query like utterances better than</cell></row><row><cell>a typical Mandarin Chinese speaker and a committee of 5</cell></row><row><cell>Chinese speakers working together.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Latency distribution (ms) versus load</figDesc><table><row><cell></cell><cell cols="2">Median 98%ile</cell></row><row><cell>10 streams</cell><cell>44</cell><cell>70</cell></row><row><cell>20 streams</cell><cell>48</cell><cell>86</cell></row><row><cell>30 streams</cell><cell>67</cell><cell>114</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">Most of our experiments use bidirectional recurrent layers with clipped rectified-linear units (ReLU) σ(x) = min{max{x, 0}, 20} as the activation function.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1">Details of our CTC implementation will be made available along with open source code.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2">"Commonwealth" includes British, Irish, South African, Australian and New Zealand accents.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3">"European" includes countries in Europe without English as a first language.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Applying convolutional neural networks concepts to hybrid nn-hmm model for speech recognition</title>
		<author>
			<persName><surname>Abdel-Hamid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Ossama</surname></persName>
		</author>
		<author>
			<persName><surname>Abdel-Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerald</forename><surname>Penn</surname></persName>
		</author>
		<editor>ICASSP</editor>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">End-to-end attention-based large vocabulary speech recognition</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName><surname>Dmitriy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philemon</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1508.04395</idno>
		<ptr target="http://arxiv.org/abs/1508.04395" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The third &apos;CHiME&apos; speech separation and recognition challenge: Dataset, task and baselines</title>
		<author>
			<persName><forename type="first">Jon</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricard</forename><surname>Marxer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Submitted to IEEE 2015 Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Weston, Jason. Curriculum learning</title>
		<author>
			<persName><forename type="first">Louradour</forename><surname>Yoshua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jér</forename><surname>Ęome</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Speech 2 : End-to-End Speech Recognition in English and Mandarin Bengio</title>
				<meeting><address><addrLine>Collobert, Ronan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>International Conference on Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Connectionist Speech Recognition: A Hybrid Approach</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bourlard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Morgan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Kluwer Academic Publishers</publisher>
			<pubPlace>Norwell, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Navdeep</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Vinyals</surname></persName>
		</author>
		<idno>spell. abs/1508.01211</idno>
		<ptr target="http://arxiv.org/abs/1508.01211" />
	</analytic>
	<monogr>
		<title level="j">Oriol. Listen</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Chetlur</surname></persName>
		</author>
		<author>
			<persName><surname>Woolley</surname></persName>
		</author>
		<author>
			<persName><surname>Cliff</surname></persName>
		</author>
		<author>
			<persName><surname>Vandermersch</surname></persName>
		</author>
		<author>
			<persName><surname>Philippe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1410.0759" />
		<title level="m">Efficient primitives for deep learning</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Van</forename><surname>Merrienboer</surname></persName>
		</author>
		<author>
			<persName><surname>Bart</surname></persName>
		</author>
		<author>
			<persName><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><surname>Caglar</surname></persName>
		</author>
		<author>
			<persName><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><surname>Dzmitry</surname></persName>
		</author>
		<author>
			<persName><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><surname>Fethi</surname></persName>
		</author>
		<author>
			<persName><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><surname>Holger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">End-to-end continuous speech recognition using attention-based recurrent nn: First</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><surname>Dzmitry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>results. abs/1412.1602</idno>
		<ptr target="http://arxiv.org/abs/1412.1602" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Text detection and character recognition in scene images with unsupervised feature learning</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><surname>Carpenter</surname></persName>
		</author>
		<author>
			<persName><surname>Blake</surname></persName>
		</author>
		<author>
			<persName><surname>Case</surname></persName>
		</author>
		<author>
			<persName><surname>Carl</surname></persName>
		</author>
		<author>
			<persName><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><surname>Sanjeev</surname></persName>
		</author>
		<author>
			<persName><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName><surname>Bipin</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Document Analysis and Recognition</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep learning with COTS HPC</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><surname>Huval</surname></persName>
		</author>
		<author>
			<persName><surname>Brody</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Context-dependent pre-trained deep neural networks for large vocabulary speech recognition</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Audio, Speech, and Language Processing</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Large scale distributed deep networks</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><surname>Rajat</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Kai</surname></persName>
		</author>
		<author>
			<persName><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><surname>Matthieu</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><surname>Mark</surname></persName>
		</author>
		<author>
			<persName><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><surname>Marcâ Ȃ Źaurelio</surname></persName>
		</author>
		<author>
			<persName><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 25</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Support vector machines for noise robust ASR</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J F</forename><surname>Gales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ragni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Aldamarki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gautier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASRU</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="205" to="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Towards end-to-end speech recognition with recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2014">2014a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Towards end-to-end speech recognition with recurrent neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning (ICML-14)</title>
				<meeting>the 31st International Conference on Machine Learning (ICML-14)</meeting>
		<imprint>
			<date type="published" when="2014">2014b</date>
			<biblScope unit="page" from="1764" to="1772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Abdel-Rahman</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICASSP</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Long short-term memory recurrent neural network architectures for large scale acoustic modeling</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName><surname>Hasim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francoise</forename><surname>Beaufays</surname></persName>
		</author>
		<editor>Interspeech</editor>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Deep speech: Scaling up end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">Awni</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName><surname>Case</surname></persName>
		</author>
		<author>
			<persName><surname>Carl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName><surname>Greg</surname></persName>
		</author>
		<author>
			<persName><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><surname>Erich</surname></persName>
		</author>
		<author>
			<persName><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><surname>Sanjeev</surname></persName>
		</author>
		<author>
			<persName><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName><surname>Shubho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.5567" />
		<imprint>
			<date type="published" when="1412">1412. 2014a</date>
			<biblScope unit="volume">5567</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">First-pass large vocabulary continuous speech recognition using bi-directional recurrent DNNs</title>
		<author>
			<persName><forename type="first">Awni</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<idno>abs/1408.2873</idno>
		<ptr target="http://arxiv.org/abs/1408.2873" />
		<imprint>
			<date type="published" when="2014">2014b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012-11">November. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>abs/1502.03167</idno>
		<ptr target="http://arxiv.org/abs/1502.03167" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><surname>Ilya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoff</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Batch normalized recurrent neural</title>
		<author>
			<persName><forename type="first">Cesar</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><surname>Pereyra</surname></persName>
		</author>
		<author>
			<persName><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName><surname>Philemon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>networks. abs/1510.01378</idno>
		<ptr target="http://arxiv.org/abs/1510.01378" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Building high-level features using large scale unsupervised learning</title>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning methods for generic object recognition with invariance to pose and lighting</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><surname>Jie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="97" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Speech recognition by machines and humans</title>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">P</forename><surname>Lippmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech communication</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Lexicon-free conversational speech recognition with neural networks</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><surname>Ziang</surname></persName>
		</author>
		<author>
			<persName><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">EESEN: End-to-end speech recognition using deep rnn models and wfst-based decoding</title>
		<author>
			<persName><forename type="first">Yajie</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Gowayyed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Metz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
		<respStmt>
			<orgName>ASRU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Acoustic modeling using deep belief networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Audio, Speech, and Language Processing</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Application of pretrained deep neural networks to large vocabulary speech recognition</title>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<editor>Interspeech</editor>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Context-dependent deep neural networks for commercial mandarin speech recognition applications</title>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Na</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">APSIPA</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Librispeech: an asr corpus based on public domain audio books</title>
		<author>
			<persName><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName><surname>Vassil</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Guoguo</surname></persName>
		</author>
		<author>
			<persName><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><surname>Khudanpur</surname></persName>
		</author>
		<author>
			<persName><surname>Sanjeev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">On the difficulty of training recurrent neural</title>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>networks. abs/1211.5063</idno>
		<ptr target="http://arxiv.org/abs/1211.5063" />
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Large-scale deep unsupervised learning using graphics processors</title>
		<author>
			<persName><forename type="first">R</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madhavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">26th International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Hogwild: A lock-free approach to parallelizing stochastic gradient descent</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><surname>Re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wright</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="693" to="701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Connectionist probability estimators in HMM speech recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Renals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bourlard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Franco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Speech and Audio Processing</title>
				<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="161" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">The use of recurrent neural networks in continuous speech recognition</title>
		<author>
			<persName><forename type="first">Tony</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><surname>Hochberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Renals</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="253" to="258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Convolutional, long short-term memory, fully connected deep neural networks</title>
		<author>
			<persName><forename type="first">Tara</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Oriol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><surname>Sak</surname></persName>
		</author>
		<author>
			<persName><surname>Hasim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICASSP</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for LVCSR</title>
		<author>
			<persName><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><surname>Rahman Mohamed</surname></persName>
		</author>
		<author>
			<persName><surname>Abdel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhuvana</forename><surname>Ramabhadran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Francoise. Fast and accurate recurrent neural network acoustic models for speech recognition</title>
		<author>
			<persName><surname>Sak</surname></persName>
		</author>
		<author>
			<persName><surname>Hasim</surname></persName>
		</author>
		<author>
			<persName><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kanishka</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><surname>Beaufays</surname></persName>
		</author>
		<idno>abs/1507.06947</idno>
		<ptr target="http://arxiv.org/abs/1507.06947" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A fast data collection and augmentation procedure for object recognition</title>
		<author>
			<persName><forename type="first">Benjaminn</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashutosh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Twenty-Third Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Conversational speech transcription using context-dependent deep neural networks</title>
		<author>
			<persName><forename type="first">Frank</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Gang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="437" to="440" />
		</imprint>
	</monogr>
	<note type="report_type">In Interspeech</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Search by voice in mandarin chinese</title>
		<author>
			<persName><forename type="first">Jiulong</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>Genqing</surname></persName>
		</author>
		<author>
			<persName><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><surname>Zhihong</surname></persName>
		</author>
		<author>
			<persName><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><surname>Xiliu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Jansche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Moreno</surname></persName>
		</author>
		<editor>Interspeech</editor>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">On the importance of momentum and initialization in deep learning</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">30th International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><surname>Yangqing</surname></persName>
		</author>
		<author>
			<persName><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><surname>Pierre</surname></persName>
		</author>
		<author>
			<persName><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><surname>Dragomir</surname></persName>
		</author>
		<author>
			<persName><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><surname>Dumitru</surname></persName>
		</author>
		<author>
			<persName><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>Going deeper with convolutions</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Phoneme recognition using time-delay neural networks,â Ȃİ acoustics speech and signal processing</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Waibel</surname></persName>
		</author>
		<author>
			<persName><surname>Hanazawa</surname></persName>
		</author>
		<author>
			<persName><surname>Toshiyuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><surname>Shikano</surname></persName>
		</author>
		<author>
			<persName><surname>Kiyohiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="328" to="339" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">The ntt chime-3 system: Advances in speech enhancement and recognition for mobile multi-microphone devices</title>
		<author>
			<persName><forename type="first">T</forename><surname>Yoshioka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Delcroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kinoshita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Fabian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Espi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Higuchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Araki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nakatani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ASRU</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1410.4615" />
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">4615</biblScope>
		</imprint>
	</monogr>
	<note>Learning to execute. abs/1410</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
