<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Max-Margin Majority Voting for Learning from Crowds</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tian</forename><surname>Tian</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science &amp; Technology</orgName>
								<orgName type="department" key="dep2">Center for Bio-Inspired Computing Research Tsinghua National Lab for Information Science &amp; Technology State Key Lab of Intelligent Technology &amp; Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science &amp; Technology</orgName>
								<orgName type="department" key="dep2">Center for Bio-Inspired Computing Research Tsinghua National Lab for Information Science &amp; Technology State Key Lab of Intelligent Technology &amp; Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Max-Margin Majority Voting for Learning from Crowds</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning-from-crowds aims to design proper aggregation strategies to infer the unknown true labels from the noisy labels provided by ordinary web workers. This paper presents max-margin majority voting (M 3 V) to improve the discriminative ability of majority voting and further presents a Bayesian generalization to incorporate the flexibility of generative methods on modeling noisy observations with worker confusion matrices. We formulate the joint learning as a regularized Bayesian inference problem, where the posterior regularization is derived by maximizing the margin between the aggregated score of a potential true label and that of any alternative label. Our Bayesian model naturally covers the Dawid-Skene estimator and M 3 V. Empirical results demonstrate that our methods are competitive, often achieving better results than state-of-the-art estimators.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Many learning tasks require labeling large datasets. Though reliable, it is often too expensive and time-consuming to collect labels from domain experts or well-trained workers. Recently, online crowdsourcing platforms have dramatically decreased the labeling cost by dividing the workload into small parts, then distributing micro-tasks to a crowd of ordinary web workers <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b19">20]</ref>. However, the labeling accuracy of web workers could be lower than expected due to their various backgrounds or lack of knowledge. To improve the accuracy, it is usually suggested to label every task multiple times by different workers, then the redundant labels can provide hints on resolving the true labels.</p><p>Much progress has been made in designing effective aggregation mechanisms to infer the true labels from noisy observations. From a modeling perspective, existing work includes both generative approaches and discriminative approaches. A generative method builds a flexible probabilistic model for generating the noisy observations conditioned on the unknown true labels and some behavior assumptions, with examples of the Dawid-Skene (DS) estimator <ref type="bibr" target="#b4">[5]</ref>, the minimax entropy (Entropy) estimator<ref type="foot" target="#foot_0">1</ref>  <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>, and their variants. In contrast, a discriminative approach does not model the observations; it directly identifies the true labels via some aggregation rules. Examples include majority voting and the weighted majority voting that takes worker reliability into consideration <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>.</p><p>In this paper, we present a max-margin formulation of the most popular majority voting estimator to improve its discriminative ability, and further present a Bayesian generalization that conjoins the advantages of both generative and discriminative approaches. The max-margin majority voting (M 3 V) directly maximizes the margin between the aggregated score of a potential true label and that of any alternative label, and the Bayesian model consists of a flexible probabilistic model to generate the noisy observations by conditioning on the unknown true labels. We adopt the same approach as the classical Dawid-Skene estimator to build the probabilistic model by considering worker confusion matrices, though many other generative models are also possible. Then, we strongly couple the generative model and M 3 V by formulating a joint learning problem under the regularized Bayesian inference (RegBayes) <ref type="bibr" target="#b26">[27]</ref> framework, where the posterior regularization <ref type="bibr" target="#b6">[7]</ref> enforces a large margin between the potential true label and any alternative label. Naturally, our Bayesian model covers both the David-Skene estimator and M 3 V as special cases by setting the regularization parameter to its extreme values (i.e., 0 or ∞). We investigate two choices on defining the max-margin posterior regularization: <ref type="bibr" target="#b0">(1)</ref> an averaging model with a variational inference algorithm; and (2) a Gibbs model with a Gibbs sampler under a data augmentation formulation. The averaging version can be seen as an extension to the MLE learner of Dawid-Skene model. Experiments on real datasets suggest that max-margin learning can significantly improve the accuracy of majority voting, and that our Bayesian estimators are competitive, often achieving better results than state-of-the-art estimators on true label estimation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminary</head><p>We consider the label aggregation problem with a dataset consisting of M items (e.g., pictures or paragraphs). Each item i has an unknown true label y i ∈ [D], where [D] := {1, . . . , D}. The task t i is to label item i. In crowdsourcing, we have N workers assigning labels to these items. Each worker may only label a part of the dataset. Let I i ⊆ [N ] denote the workers who have done task t i . We use x ij to denote the label of t i provided by worker j, x i to denote the labels provided to task t i , and X is the collection of these worker labels, which is an incomplete matrix. The goal of learning-from-crowds is to estimate the true labels of items from the noisy observations X.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Majority Voting Estimator</head><p>Majority voting (MV) is arguably the simplest method. It posits that for every task the true label is always most commonly given. Thus, it selects the most frequent label for each task as its true label, by solving the problem:</p><formula xml:id="formula_0">ŷi = argmax d∈[D] N j=1 I(x ij = d), ∀i ∈ [M ],<label>(1)</label></formula><p>where I(•) is an indicator function. It equals to 1 whenever the predicate is true, otherwise it equals to 0. Previous work has extended this method to weighted majority voting (WMV) by putting different weights on workers to measure worker reliability <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Dawid-Skene Estimator</head><p>The method of Dawid and Skene <ref type="bibr" target="#b4">[5]</ref> is a generative approach by considering worker confusability. It posits that the performance of a worker is consistent across different tasks, as measured by a confusion matrix whose diagonal entries denote the probability of assigning correct labels while offdiagonal entries denote the probability of making specific mistakes to label items in one category as another. Formally, let φ j be the confusion matrix of worker j. Then, φ jkd denotes the probability that worker j assigns label d to an item whose true label is k. Under the basic assumption that workers finish each task independently, the likelihood of observed labels can be expressed as</p><formula xml:id="formula_1">p(X|Φ, y) = M i=1 N j=1 D d,k=1 φ jkd n i jkd = N j=1 D d,k=1 φ jkd n jkd ,<label>(2)</label></formula><p>where n i jkd = I(x ij = d, y i = k), and n jkd = M i=1 n i jkd is the number of tasks with true label k but being labeled to d by worker j.</p><p>The unknown labels and parameters can be estimated by maximum-likelihood estimation (MLE), { ŷ, Φ} = argmax y,Φ log p(X|Φ, y), via an expectation-maximization (EM) algorithm that iteratively updates the true labels y and the parameters Φ. The learning procedure is often initialized by majority voting to avoid bad local optima. If we assume some structure of the confusion matrix, various variants of the DS estimator have been studied, including the homogenous DS model <ref type="bibr" target="#b14">[15]</ref> and the class-conditional DS model <ref type="bibr" target="#b10">[11]</ref>. We can also put a prior over worker confusion matrices and transform the inference into a standard inference problem in graphical models <ref type="bibr" target="#b11">[12]</ref>. Recently, spectral methods have also been applied to better initialize the DS model <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Max-Margin Majority Voting</head><p>Majority voting is a discriminative model that directly finds the most likely label for each item. In this section, we present max-margin majority voting (M 3 V), a novel extension of (weighted) majority voting with a new notion of margin (named crowdsourcing margin).  Let g(x i , d) be a N -dimensional vector, with the element j equaling to I(j ∈ I i , x ij = d).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Geometric Interpretation of Crowdsourcing Margin</head><p>Then, the estimation of the vanilla majority voting in Eq. ( <ref type="formula" target="#formula_0">1</ref>) can be formulated as finding solutions {y i } i∈[M ] that satisfy the following constraints:</p><formula xml:id="formula_2">1 N g(x i , y i ) − 1 N g(x i , d) ≥ 0, ∀i, d,<label>(3</label></formula><p>) where 1 N is the N -dimensional all-one vector and 1 N g(x i , k) is the aggregated score of the potential true label k for task t i . By using the all-one vector, the aggregated score has an intuitive interpretation -it denotes the number of workers who have labeled t i as class k.</p><p>Apparently, the all-one vector treats all workers equally, which may be unrealistic in practice due to the various backgrounds of the workers. By simply choosing what the majority of workers agree on, the vanilla MV is prone to errors when many workers give low quality labels. One way to tackle this problem is to take worker reliability into consideration. Let η denote the worker weights. When these values are known, we can get the aggregated score η g(x i , k) of a weighted majority voting (WMV), and estimate the true labels by the rule: ŷi = argmax d∈[D] η g(x i , d). Thus, reliable workers contribute more to the decisions. Geometrically, g(x i , d) is a point in the N -dimensional space for each task t i . The aggregated score 1 N g(x i , d) measures the distance (up to a constant scaling) from this point to the hyperplane 1 N x = 0. So the MV estimator actually finds a point that has the largest distance to that hyperplane for each task, and the decision boundary of majority voting is another hyperplane 1 N x−b = 0 which separates the point g(x i , ŷi ) from the other points g(x i , k), k = ŷi . By introducing the worker weights η, we relax the constraint of the all-one vector to allow for more flexible decision boundaries η x−b = 0. All the possible decision boundaries with the same orientation are equivalent. Inspired by the generalized notion of margin in multi-class SVM <ref type="bibr" target="#b3">[4]</ref>, we define the crowdsourcing margin as the minimal difference between the aggregated score of the potential true label and the aggregated scores of other alternative labels. Then, one reasonable choice of the best hyperplane (i.e. η) is the one that represents the largest margin between the potential true label and other alternatives. where each axis represents the label of a worker. Assume that both workers provide labels 3 and 1 to item i. Then, the vectors g(x i , y), y ∈ [3] are three points in the 2D plane. Given the worker weights η, the estimated label should be 1, since g(x i , 1) has the largest distance to line P 0 . Line P 1 and line P 2 are two boundaries that separate g(x i , 1) and other points. The margin is the distance between them. In this case, g(x i , 1) and g(x i , 3) are support vectors that decide the margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Max-Margin Majority Voting Estimator</head><p>Let be the minimum margin between the potential true label and all other alternatives. We define the max-margin majority voting (M 3 V) as solving the constrained optimization problem to estimate the true labels y and weights η:</p><formula xml:id="formula_3">inf η,y 1 2 η 2 2 (4) s. t. : η g ∆ i (d) ≥ ∆ i (d), ∀i ∈ [M ], d ∈ [D], where g ∆ i (d) := g(x i , y i ) − g(x i , d) 2 and ∆ i (d) = I(y i = d).</formula><p>And in practice, the worker labels are often linearly inseparable by a single hyperplane. Therefore, we relax the hard constraints by introducing non-negative slack variables {ξ i } M i=1 , one for each task, and define the soft-margin max-margin majority voting as inf ξi≥0,η,y</p><formula xml:id="formula_4">1 2 η 2 2 + c i ξ i (5) s. t. : η g ∆ i (d) ≥ ∆ i (d) − ξ i , ∀i ∈ [M ], d ∈ [D]</formula><p>, where c is a positive regularization parameter and − ξ i is the soft-margin for task t i . The value of ξ i reflects the difficulty of task t i -a small ξ i suggests a large discriminant margin, indicating that the task is easy with a rare chance to make mistakes; while a large ξ i suggests that the task is hard with a higher chance to make mistakes. Note that our max-margin majority voting is significantly different from the unsupervised SVMs (or max-margin clustering) <ref type="bibr" target="#b20">[21]</ref>, which aims to assign cluster labels to the data points by maximizing some different notion of margin with balance constraints to avoid trivial solutions. Our M 3 V does not need such balance constraints.</p><p>Albeit not jointly convex, problem (5) can be solved by iteratively updating η and y to find a local optimum. For η, the solution can be derived as η</p><formula xml:id="formula_5">= M i=1 D d=1 ω d i g ∆ i (d)</formula><p>by the fact that the subproblem is convex. The parameters ω are obtained by solving the dual problem sup</p><formula xml:id="formula_6">0≤ω d i ≤c − 1 2 η η + i d ω d i ∆ i (d),<label>(6)</label></formula><p>which is exactly the QP dual problem in standard SVM <ref type="bibr" target="#b3">[4]</ref>. So it can be efficiently solved by welldeveloped SVM solvers like LIBSVM <ref type="bibr" target="#b1">[2]</ref>. For updating y, we define (x) + := max(0, x), and then it is a weighted majority voting with a margin gap constraint:</p><formula xml:id="formula_7">ŷi = argmax yi∈[D] −c max d∈[D] ∆ i (d) − η g ∆ i (d) + ,<label>(7)</label></formula><p>Overall, the algorithm is a max-margin iterative weighted majority voting (MM-IWMV). Comparing with the iterative weighted majority voting (IWMV) <ref type="bibr" target="#b10">[11]</ref>, which tends to maximize the expected gap of the aggregated scores under the Homogenous DS model, our M 3 V directly maximizes the data specified margin without further assumption on data model. Empirically, as we shall see, our M 3 V could have more powerful discriminative ability with better accuracy than IWMV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Bayesian Max-Margin Estimator</head><p>With the intuitive and simple max-margin principle, we now present a more sophisticated Bayesian max-margin estimator, which conjoins the discriminative ability of M 3 V and the flexibility of the generative DS estimator. Though slightly more complicated in learning and inference, the Bayesian models retain the intuitive simplicity of M 3 V and the flexibility of DS, as explained below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Model Definition</head><p>We adopt the same DS model to generate observations conditioned on confusion matrices, with the full likelihood in Eq. ( <ref type="formula" target="#formula_1">2</ref>). We further impose a prior p 0 (Φ, η) for Bayesian inference. Assuming that the true labels y are given, we aim to get the target posterior p(Φ, η|X, y), which can be obtained by solving an optimization problem: inf</p><formula xml:id="formula_8">q(Φ,η) L (q(Φ, η); y) ,<label>(8)</label></formula><p>where L(q; y) := KL(q p 0 (Φ, η)) − E q [log p(X|Φ, y)] measures the Kullback-Leibler (KL) divergence between a desired post-data posterior q and the original Bayesian posterior, and p 0 (Φ, η) is the prior, often factorized as p 0 (Φ)p 0 (η). As we shall see, this Bayesian DS estimator often leads to better performance than the vanilla DS.</p><p>Then, we explore the ideas of regularized Bayesian inference (RegBayes) <ref type="bibr" target="#b26">[27]</ref> to incorporate max-margin majority voting constraints as posterior regularization on problem <ref type="bibr" target="#b7">(8)</ref>, and define the Bayesian max-margin estimator (denoted by CrowdSVM) as solving:</p><formula xml:id="formula_9">inf ξi≥0,q∈P,y L(q(Φ, η); y) + c • i ξ i<label>(9)</label></formula><formula xml:id="formula_10">s. t. : E q [η g ∆ i (d)] ≥ ∆ i (d) − ξ i , ∀i ∈ [M ], d ∈ [D],</formula><p>where P is the probabilistic simplex, and we take expectation over q to define the margin constraints. Such posterior constraints will influence the estimates of y and Φ to get better aggregation, as we shall see. We use a Dirichlet prior on worker confusion matrices, φ mk |α ∼ Dir(α), and a spherical Gaussian prior on η, η ∼ N (0, vI). By absorbing the slack variables, CrowdSVM solves the equivalent unconstrained problem:</p><formula xml:id="formula_11">inf q∈P,y L(q(Φ, η); y) + c • R m (q(Φ, η); y),<label>(10)</label></formula><p>where R m (q; y)</p><formula xml:id="formula_12">= M i=1 max D d=1 ∆ i (d)−E q [η g ∆ i (d)]</formula><p>+ is the posterior regularization. Remark 1. From the above definition, we can see that both the Bayesian DS estimator and the maxmargin majority voting are special cases of CrowdSVM. Specifically, when c → 0, it is equivalent to the DS model. If we set v = v /c for some positive parameter v , then when c → ∞ CrowdSVM reduces to the max-margin majority voting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Variational Inference</head><p>Algorithm 1: The CrowdSVM algorithm 1. Initialize y by majority voting. while Not converge do 2. For each worker j and category k: q(φ jk ) ← Dir(n jk + α).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Solve the dual problem (11). 4. For each item</head><formula xml:id="formula_13">i: ŷi ← argmax yi∈[D] f (y i , x i ; q). end</formula><p>Since it is intractable to directly solve problem ( <ref type="formula" target="#formula_9">9</ref>) or <ref type="bibr" target="#b9">(10)</ref>, we introduce the structured mean-field assumption on the post-data posterior, q(Φ, η) = q(Φ)q(η), and solve the problem by alternating minimization as outlined in Alg. 1. The algorithm iteratively performs the following steps until a local optimum is reached:</p><p>Infer q(Φ): Fixing the distribution q(η) and the true labels y, the problem in Eq. ( <ref type="formula" target="#formula_9">9</ref>) turns to a standard Bayesian inference problem with the closed-form solution: q * (Φ) ∝ p 0 (Φ)p(X|Φ, y). Since the prior is a Dirichlet distribution, the inferred distribution is also Dirichlet, q * (φ jk ) = Dir(n jk + α), where n jk is a D-dimensional vector with element d being n jkd .</p><p>Infer q(η) and solve for ω:</p><p>Fixing the distribution q(Φ) and the true labels y, we optimize Eq. ( <ref type="formula" target="#formula_9">9</ref>) over q(η), which is also convex. We can derive the optimal solution: q * (η) ∝ p 0 (η) exp</p><formula xml:id="formula_14">η i d ω d i g ∆ i (d)</formula><p>, where ω = {ω d i } are Lagrange multipliers. With the normal prior, p 0 (η) = N (0, vI), the posterior is a normal distribution: q * (η) = N (µ, vI) , whose mean</p><formula xml:id="formula_15">is µ = v M i=1 D d=1 ω d i g ∆ i (d).</formula><p>Then the parameters ω are obtained by solving the dual problem sup</p><formula xml:id="formula_16">0≤ω d i ≤c − 1 2v µ µ + i d ω d i ∆ i (d),<label>(11)</label></formula><p>which is same as the problem (6) in max-margin majority voting.</p><p>Infer y: Fixing the distributions of Φ and η at their optimum q * , we find y by solving problem <ref type="bibr" target="#b9">(10)</ref>. To make the prediction more efficient, we approximate the distribution q * (Φ) by a Dirac delta mass δ(Φ − Φ), where Φ is the mean of q * (Φ). Then since all tasks are independent, we can derive the discriminant function of y i as</p><formula xml:id="formula_17">f (y i , x i ; q * ) = log p(x i | Φ, y i ) − c max d∈[D] ( ∆ i (d) − μ g ∆ i (d)) + , (<label>12</label></formula><formula xml:id="formula_18">)</formula><p>where μ is the mean of q * (η). Then we can make predictions by maximize this function.</p><p>Apparently, the discriminant function <ref type="bibr" target="#b11">(12)</ref> represents a strong coupling between the generative model and the discriminative margin constraints. Therefore, CrowdSVM jointly considers these two factors when estimating true labels. We also note that the estimation rule used here reduces to the rule (7) of MM-IWMV by simply setting c = ∞.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Gibbs CrowdSVM Estimator</head><p>CrowdSVM adopts an averaging model to define the posterior constraints in problem <ref type="bibr" target="#b8">(9)</ref>. Here, we further provide an alternative strategy which leads to a full Bayesian model with a Gibbs sampler.</p><p>The resulting Gibbs-CrowdSVM does not need to make the mean-field assumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Model Definition</head><p>Suppose the target posterior q(Φ, η) is given, we perform the max-margin majority voting by drawing a random sample η. This leads to the crowdsourcing hinge-loss</p><formula xml:id="formula_19">R(η, y) = M i=1 max d∈[D] ∆ i (d) − η g ∆ i (d) + ,<label>(13)</label></formula><p>which is a function of η. Since η are random, we define the overall hinge-loss as the expectation over q(η), that is, R m (q(Φ, η); y) = E q [R(η, y)]. Due to the convexity of max function, the expected loss is in fact an upper bound of the average loss, i.e., R m (q(Φ, η); y) ≥ R m (q(Φ, η); y). Differing from CrowdSVM, we also treat the hidden true labels y as random variables with a uniform prior. Then we define Gibbs-CrowdSVM as solving the problem:</p><formula xml:id="formula_20">inf q∈P L q(Φ, η, y) + E q M i=1 2c(ζ isi ) + ,<label>(14)</label></formula><p>where</p><formula xml:id="formula_21">ζ id = ∆ i (d) − η g ∆ i (d), s i = argmax d =yi ζ id ,</formula><p>and the factor 2 is introduced for simplicity. Data Augmentation In order to build an efficient Gibbs sampler for this problem, we derive the posterior distribution with the data augmentation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b25">26]</ref> for the max-margin regularization term. We let ψ(y i |x i , η) = exp(−2c(ζ isi ) + ) to represent the regularizer. According to the equality:</p><formula xml:id="formula_22">ψ(y i |x i , η) = ∞ 0 ψ(y i , λ i |x i , η)dλ i , where ψ(y i , λ i |x i , η) = (2πλ i ) − 1 2 exp( −1 2λi (λ i + cζ isi ) 2</formula><p>) is a (unnormalized) joint distribution of y i and the augmented variable λ i <ref type="bibr" target="#b13">[14]</ref>, the posterior of Gibbs-CrowdSVM can be expressed as the marginal of a higher dimensional distribution, i.e., q(Φ, η, y) = q(Φ, η, y, λ)dλ, where q(Φ, η, y, λ) ∝ p 0 (Φ, η, y)</p><formula xml:id="formula_23">M i=1 p(x i |Φ, y i )ψ(y i , λ i |x i , η).<label>(15)</label></formula><p>Putting the last two terms together, we can view q(Φ, η, y, λ) as a standard Bayesian posterior, but with the unnormalized likelihood p(x i , λ i |Φ, η, y i ) ∝ p(x i |Φ, y i )ψ(y i , λ i |x i , η), which jointly considers the noisy observations and the large margin discrimination between the potential true labels and alternatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Posterior Inference</head><p>With the augmented representation, we can do Gibbs sampling to infer the posterior distribution q(Φ, η, y, λ) and thus q(Φ, η, y) by discarding λ. The conditional distributions for {Φ, η, λ, y} are derived in Appendix A. Note that when sample λ from the inverse Gaussian distribution, a fast sampling algorithm <ref type="bibr" target="#b12">[13]</ref> can be applied with O(1) time complexity. And for the hidden variables y, we initially set them as the results of majority voting. After removing burn-in samples, we use their most frequent values of as the final outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>We now present experimental results to demonstrate the strong discriminative ability of max-margin majority voting and the promise of our Bayesian models, by comparing with various strong competitors on multiple real datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Datasets and Setups</head><p>We use four real world crowd labeling datasets as summarized in Table <ref type="table" target="#tab_0">1</ref>. Web Search <ref type="bibr" target="#b23">[24]</ref>: 177 workers are asked to rate a set of 2,665 query-URL pairs on a relevance rating scale from 1 to 5. Each task is labeled by 6 workers on average. In total 15,567 labels are collected. Age <ref type="bibr" target="#b7">[8]</ref>: It consists of 10,020 labels of age estimations for 1,002 face images. Each image was labeled by 10 workers. And there are 165 workers involved in these tasks. The final estimations are discretized into 7 bins. Bluebirds <ref type="bibr" target="#b18">[19]</ref>: It consists of 108 bluebird pictures. There are 2 breeds among all the images, and each image is labeled by all 39 workers. 4,214 labels in total. Flowers <ref type="bibr" target="#b17">[18]</ref>: It contains 2,366 binary labels for a dataset with 200 flower pictures. Each worker is asked to answer whether the flower in picture is peach flower. 36 workers participate in these tasks. We compare M 3 V, as well as its Bayesian extensions CrowdSVM and Gibbs-CrowdSVM, with various baselines, including majority voting (MV), iterative weighted majority voting (IWMV) <ref type="bibr" target="#b10">[11]</ref>, the Dawid-Skene (DS) estimator <ref type="bibr" target="#b4">[5]</ref>, and the minimax entropy (Entropy) estimator <ref type="bibr" target="#b24">[25]</ref>. For Entropy estimator, we use the implementation provided by the authors, and show both the performances of its multiclass version (Entropy (M)) and the ordinal version (Entropy (O)).</p><p>All the estimators that require an iterative updating are initialized by majority voting to avoid bad local minima. All experiments were conducted on a PC with Intel Core i5 3.00GHz CPU and 12.00GB RAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Model Selection</head><p>Due to the special property of crowdsourcing, we cannot simply split the training data into multiple folds to cross-validate the hyperparameters by using accuracy as the selection criterion, which may bias to over-optimistic models. Instead, we adopt the likelihood p(X| Φ, ŷ) as the criterion to select parameters, which is indirectly related to our evaluation criterion (i.e., accuracy). Specifically, we test multiple values of c and , and select the value that produces a model with the maximal likelihood on the given dataset. This method ensures us to select model without any prior knowledge on the true labels. For the special case of M 3 V, we fix the learned true labels y after training the model with certain parameters, and learn confusion matrices that optimize the full likelihood in Eq. ( <ref type="formula" target="#formula_1">2</ref>).</p><p>Note that the likelihood-based cross-validation strategy <ref type="bibr" target="#b24">[25]</ref> is not suitable for CrowdSVM, because this strategy uses marginal likelihood p(X|Φ) to select model and ignores the label information of y, through which the effect of constraints is passed for CrowdSVM. If we use this strategy on CrowdSVM, it will tend to optimize the generative component without considering the discriminant constraints, thus resulting in c → 0, which is a trivial solution for model selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Experimental Results</head><p>We first test our estimators on the task of estimating true labels. For CrowdSVM, we set α = 1 and v = 1 for all experiments, since we find that the results are insensitive to them. For M 3 V, CrowdSVM and Gibbs-CrowdSVM, the regularization parameters (c, ) are selected from c = 2ˆ[−8 : 0] and = <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5]</ref> by the method in Sec. 6.2. As for Gibbs-CrowdSVM, we generate 50 samples in each run and discard the first 10 samples as burn-in steps, which are sufficiently large to reach convergence of the likelihood. The reported error rate is the average over 5 runs.</p><p>Table <ref type="table" target="#tab_1">2</ref> presents the error rates of various estimators. We group the comparisons into three parts:</p><p>I. All the MV, IWMV and M 3 V are purely discriminative estimators. We can see that our M 3 V produces consistently lower error rates on all the four datasets compared with the vanilla MV and IWMV, which show the effectiveness of max-margin principle for crowdsourcing;</p><p>II. This part analyzes the effects of prior and max-margin regularization on improving the DS model. We can see that DS+Prior is better than the vanilla DS model on the two larger datasets by using a Dirichlet prior. Furthermore, CrowdSVM consistently improves the performance of DS+Prior by considering the max-margin constraints, again demonstrating the effectiveness of max-margin learning;</p><p>III. This part compares our Gibbs-CrowdSVM estimator to the state-of-the-art minimax entropy estimators. We can see that Gibbs-CrowdSVM performs better than CrowdSVM on Web-Search, Age and Flowers datasets, while worse on the small Bluebuirds dataset. And it is comparable to the minimax entropy estimators, sometimes better with faster running speed as shown in Fig. <ref type="figure" target="#fig_3">2</ref> and explained below. Note that we only test Entropy (O) on two ordinal datasets, since this method is specifically designed for ordinal labels, while not always effective.</p><p>Fig. <ref type="figure" target="#fig_3">2</ref> summarizes the training time and error rates after each iteration for all estimators on the largest Web-Search dataset. It shows that the discriminative methods (e.g., IWMV and M 3 V) run fast but converge to high error rates. Compared to the minimax entropy estimator, CrowdSVM is  computationally more efficient and also converges to a lower error rate. Gibbs-CrowdSVM runs slower than CrowdSVM since it needs to compute the inversion of matrices. The performance of the DS estimator seems mediocre -its estimation error rate is large and slowly increases when it runs longer. Perhaps this is partly because the DS estimator cannot make good use of the initial knowledge provided by majority voting. Web-Search dataset. For the generative part, we compared CrowdSVM (c = 0.125, = 3) with DS and M 3 V (c = 0.125, = 3). Fig. <ref type="figure" target="#fig_4">3</ref>(a) compares the negative log likelihoods (NLL) of these models, computed with Eq. ( <ref type="formula" target="#formula_1">2</ref>). For M 3 V, we fix its estimated true labels and find the confusion matrices to optimize the likelihood. The results show that CrowdSVM achieves a lower NLL than DS; this suggests that by incorporating M 3 V constraints, CrowdSVM finds a better solution of the true labels as well as the confusion matrices than that found by the original EM algorithm. For the discriminative part, we use the mean of worker weights μ to estimate the true labels as y i = argmax d∈[D] μ g(x i , d), and show the error rates in Fig. <ref type="figure" target="#fig_4">3</ref>(b). Apparently, the weights learned by CrowdSVM are also better than those learned by the other MV estimators. Overall, these results suggest that CrowdSVM can achieve a good balance between the generative modeling and the discriminative prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions and Future Work</head><p>We present a simple and intuitive max-margin majority voting estimator for learning-from-crowds as well as its Bayesian extension that conjoins the generative modeling and discriminative prediction. By formulating as a regularized Bayesian inference problem, our methods naturally cover the classical Dawid-Skene estimator. Empirical results demonstrate the effectiveness of our methods.</p><p>Our model is flexible to fit specific complicated application scenarios <ref type="bibr" target="#b21">[22]</ref>. One seminal feature of Bayesian methods is their sequential updating. We can extend our Bayesian estimators to the online setting where the crowdsourcing labels are collected in a stream and more tasks are distributed. We have some preliminary results as shown in Appendix B. It would also be interesting to investigate more on active learning, such as selecting reliable workers to reduce costs <ref type="bibr" target="#b8">[9]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>, 1 : (𝟎, 𝟏) 𝑻 𝒈 𝒙 𝑖 , 2 : (𝟎, 𝟎) 𝑻 𝒈 𝒙 𝑖 , 3 : (𝟏, 𝟎) 𝑻</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A geometric interpretation of the crowdsourcing margin.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 1</head><label>1</label><figDesc>Fig.1provides an illustration of the crowdsourcing margin for WMV with D = 3 and N = 2, where each axis represents the label of a worker. Assume that both workers provide labels 3 and 1 to item i. Then, the vectors g(x i , y), y ∈ [3] are three points in the 2D plane. Given the worker weights η, the estimated label should be 1, since g(x i , 1) has the largest distance to line P 0 . Line P 1 and line P 2 are two boundaries that separate g(x i , 1) and other points. The margin is the distance between them. In this case, g(x i , 1) and g(x i , 3) are support vectors that decide the margin.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Error rates per iteration of various estimators on the web search dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: NLLs and ERs when separately test the generative and discriminative components.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Datasets Overview.</figDesc><table><row><cell>DATASET</cell><cell cols="3">LABELS ITEMS WORKERS</cell></row><row><cell>WEB SEARCH</cell><cell>15,567</cell><cell>2,665</cell><cell>177</cell></row><row><cell>AGE</cell><cell>10,020</cell><cell>1,002</cell><cell>165</cell></row><row><cell>BLUEBIRDS</cell><cell>4,214</cell><cell>108</cell><cell>39</cell></row><row><cell>FLOWERS</cell><cell>2,366</cell><cell>200</cell><cell>36</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Error-rates (%) of different estimators on four datasets.</figDesc><table><row><cell></cell><cell>METHODS</cell><cell>WEB SEARCH</cell><cell>AGE</cell><cell>BLUEBIRDS</cell><cell>FLOWERS</cell></row><row><cell></cell><cell>MV</cell><cell>26.90</cell><cell>34.88</cell><cell>24.07</cell><cell>22.00</cell></row><row><cell>I</cell><cell>IWMV</cell><cell>15.04</cell><cell>34.53</cell><cell>27.78</cell><cell>19.00</cell></row><row><cell></cell><cell>M 3 V</cell><cell>12.74</cell><cell>33.33</cell><cell>20.37</cell><cell>13.50</cell></row><row><cell></cell><cell>DS</cell><cell>16.92</cell><cell>39.62</cell><cell>10.19</cell><cell>13.00</cell></row><row><cell>II</cell><cell>DS+PRIOR</cell><cell>13.26</cell><cell>34.53</cell><cell>10.19</cell><cell>13.50</cell></row><row><cell></cell><cell>CROWDSVM</cell><cell>9.42</cell><cell>33.33</cell><cell>10.19</cell><cell>13.50</cell></row><row><cell></cell><cell>ENTROPY (M)</cell><cell>11.10</cell><cell>31.14</cell><cell>8.33</cell><cell>13.00</cell></row><row><cell>III</cell><cell>ENTROPY (O)</cell><cell>10.40</cell><cell>37.32</cell><cell>−</cell><cell>−</cell></row><row><cell></cell><cell>G-CROWDSVM</cell><cell>7.99 ± 0.26</cell><cell cols="3">32.98 ± 0.36 10.37±0.41 12.10 ± 1.07</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.18</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.14</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Error rate</cell><cell>IWMV</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>M 3 V</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.10</cell><cell>Dawid−Skene</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Entropy (M)</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Entropy (O)</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>CrowdSVM</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Gibbs−CrowdSVM</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>10 0</cell><cell>10 1</cell><cell>10 2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Time (Seconds)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">A maximum entropy estimator can be understood as a dual of the MLE of a probabilistic model<ref type="bibr" target="#b5">[6]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">The offset b is canceled out in the margin constraints.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The work was supported by the National Basic Research Program (973 Program) of China (Nos. 2013CB329403, 2012CB316301), National NSF of China (Nos. 61322308, 61332007), Tsinghua National Laboratory for Information Science and Technology Big Data Initiative, and Tsinghua Initiative Scientific Research Program (Nos. 20121088071, 20141080934).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Toward an architecture for never-ending language learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Betteridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kisiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Settles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Hruschka</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">LIBSVM: A library for support vector machines</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Robust Bayesian max-margin clustering</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On the algorithmic implementation of multiclass kernel-based vector machines</title>
		<author>
			<persName><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="265" to="292" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Maximum likelihood estimation of observer error-rates using the EM algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dawid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Skene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Statistics</title>
		<imprint>
			<biblScope unit="page" from="20" to="28" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Maximum entropy density estimation with generalized regularization and an application to species distribution modeling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dudík</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Posterior regularization for structured latent variable models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Grac ¸a</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gillenwater</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<date type="published" when="2001">2001-2049, 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Demographic estimation from face images: Human vs. machine performance</title>
		<author>
			<persName><forename type="first">C</forename><surname>Otto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on PAMI</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Reputation-based worker filtering in crowdsourcing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jagabathula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Venkataraman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Iterative learning for reliable crowdsourcing systems</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Karger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.4086</idno>
		<title level="m">Error rate bounds and iterative weighted majority voting for crowdsourcing</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Variational inference for crowdsourcing</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ihler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generating random variates using transformations with multiple roots</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Schucany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Haas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American Statistician</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="88" to="90" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Data augmentation for support vector machines</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">G</forename><surname>Polson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bayesian Analysis</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">C</forename><surname>Raykar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Valadez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Florin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bogoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Moy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Learning from crowds. JMLR</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1297" to="1322" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Online Bayesian passive-aggressive learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cheap and fast-but is it good?: evaluating non-expert annotations for natural language tasks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>O'connor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Uncovering the latent structures of crowd labeling</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PAKDD</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The multidimensional wisdom of crowds</title>
		<author>
			<persName><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Whose vote should count more: Optimal integration of labels from labelers of unknown expertise</title>
		<author>
			<persName><forename type="first">J</forename><surname>Whitehill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bergsma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Movellan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Ruvolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unsupervised and semi-supervised multi-class support vector machines</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Crowdsourcing translation: Professional quality from non-professionals</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">F</forename><surname>Zaidan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Spectral methods meet EM: A provably optimal algorithm for crowdsourcing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning from the wisdom of crowds by minimax entropy</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Aggregating ordinal labels from crowds by minimax conditional entropy</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Gibbs max-margin topic models with data augmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Perkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1073" to="1110" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bayesian inference with posterior regularization and applications to infinite latent svms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1799" to="1847" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
