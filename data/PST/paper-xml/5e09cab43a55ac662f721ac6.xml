<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Category-Level Articulated Object Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiaolong</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">He</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Li</forename><surname>Yi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Leonidas</forename><surname>Guibas</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">A</forename><forename type="middle">Lynn</forename><surname>Abbott</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shuran</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Virginia</forename><surname>Tech</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Stanford</forename><surname>University</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Google</forename><surname>Research</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Category-Level Articulated Object Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper addresses the task of category-level pose estimation for articulated objects from a single depth image. We present a novel category-level approach that correctly accommodates object instances previously unseen during training. We introduce Articulation-aware Normalized Coordinate Space Hierarchy (ANCSH) -a canonical representation for different articulated objects in a given category. As the key to achieve intra-category generalization, the representation constructs a canonical object space as well as a set of canonical part spaces. The canonical object space normalizes the object orientation, scales and articulations (e.g. joint parameters and states) while each canonical part space further normalizes its part pose and scale. We develop a deep network based on PointNet++ that predicts ANCSH from a single depth point cloud, including part segmentation, normalized coordinates, and joint parameters in the canonical object space. By leveraging the canonicalized joints, we demonstrate: 1) improved performance in part pose and scale estimations using the induced kinematic constraints from joints; 2) high accuracy for joint parameter estimation in camera space.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Our environment is populated with articulated objects, ranging from furniture such as cabinets and ovens to small tabletop objects such as laptops and eyeglasses. Effectively interacting with these objects requires a detailed understanding of their articulation states and part-level poses. Such understanding is beyond the scope of typical 6D pose estimation algorithms, which have been designed for rigid objects <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b27">28]</ref>. Algorithms that do consider object articulations <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16]</ref> often require the exact object CAD model and the associated joint parameters at test time, preventing them from generalizing to new object instances.</p><p>In this paper, we adopt a learning-based approach to perform category-level pose estimation for articulated objects. Specifically, we consider the task of estimating perpart 6D poses and 3D scales, joint parameters (i.e. type, position, axis orientation), and joint states (i.e. joint angle) * indicates equal contributions. of a novel articulated object instance in a known category from a single depth image. Here object instances from one category will share a known kinematic chain composing of a fixed number of rigid parts connected by certain types of joints. We are particularly interested in the two most common joint types, revolute joints that cause 1D rotational motion (e.g., door hinges), and prismatic joints that allow 1D translational movement (e.g., drawers in a cabinet). An overview is shown in Figure <ref type="figure" target="#fig_0">1</ref>. To achieve this goal, several major challenges need to be addressed:</p><p>First, to handle novel articulated objects without knowing exact 3D CAD models, we need to find a shared representation for different instances within a given category. The representation needs to accommodate the large variations in part geometry, joint parameters, joint states, and self-occlusion patterns. More importantly, for learning on such diverse data, the representation needs to facilitate intra-category generalization.</p><p>Second, in contrast to a rigid object, an articulated object is composed of multiple rigid parts leading to a higher degree of freedom in its pose. Moreover, the parts are connected and constrained by certain joints and hence their 1 arXiv:1912.11913v2 [cs.CV] 8 Apr 2020 poses are not independent. It is challenging to accurately estimate poses in such a high-dimensional space while complying with physical constraints.</p><p>Third, various types of joints provide different physical constraints and priors for part articulations. Designing a framework that can accurately predict the parameters and effectively leverage the constraints for both revolute and prismatic joints is yet an open research problem.</p><p>To address the representation challenge, we propose a shared category-level representation for different articulated object instances, namely, Articulation-aware Normalized Coordinate Space Hierarchy (ANCSH). Concretely, ANCSH is a two-level hierarchy of canonical space, composed of Normalized Articulated Object Coordinate Space (NAOCS) at the root level and a set of Normalized Part Coordiante Spaces (NPCSs) at the leaf level. In the NAOCS, object scales, orientations, and joint states are normalized. In the NPCS of each rigid part, the part pose and scale are further normalized . We note that NAOCS and NPCS are complimentary to each other: NAOCS provides a canonical reference on the object level while NPCSs provide canonical part references. The two-level reference frames from ANCSH allow us to define per-part pose as well as joint attributes for previously unseen articulated object instances on the category-level.</p><p>To address the pose estimation challenge, we segment objects into multiple rigid parts and predict the normalized coordinates in ANCSH. However, separate per-part pose estimation could easily lead to physically impossible solutions since joint constraints are not considered. To conform with the kinematic constraints introduced by joints, we estimate joint parameters in the NAOCS from the observation, mathematically model the constraints based upon the joint type, and then leverage the kinematic priors to regularize the part poses. We formulate articulated pose fitting from the ANCSH to the depth observation as a combined optimization problem, taking both part pose fitting and joint constraints into consideration. In this work we mainly focus on 1D revolute joints and 1D prismatic joints, while the above formulation can be extended to model and support other types of joints.</p><p>Our experiments demonstrate that leveraging the joint constraints in the combined optimization leads to improved performance in part pose and scale prediction. Noting that leveraging joint constraints for regularizing part poses requires high-accuracy joint parameter predictions, which itself is very challenging. Instead of directly predicting joint parameters in the camera space, we consider and leverage predictions in NAOCS, where joints are posed in a canonical orientation, e.g. the revolute joints always point upward for eyeglasses.</p><p>By transforming joint parameter predictions from NAOCS back to camera space, we further demonstrate supreme accuracy on camera-space joint parameter predictions. In summary, the primary contribution of our paper is a unified framework for category-level articulated pose estimation. In support of this framework, we design:</p><p>• A novel category-level representation for articulated objects -Articulation-aware Normalized Coordinate Space Hierarchy (ANCSH).</p><p>• A PointNet++ based neural network that is capable of predicting ANCSH for previously unseen articulated object instances from a single depth input.</p><p>• A combined optimization scheme that leverages ANCSH predictions along with induced joint constraints for part pose and scale estimation.</p><p>• A two-step approach for high-accuracy joint parameter estimation that first predicts joint parameters in the NAOCS and then transforms them into camera space using part poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>This section summarizes related work on pose estimation for rigid and articulated objects.</p><p>Rigid object pose estimation. Classically, the goal of pose estimation is to infer an object's 6D pose (3D rotation and 3D location) relative to a given reference frame. Most previous work has focused on estimating instance-level pose by assuming that exact 3D CAD models are available. For example, traditional algorithms such as iterative closest point (ICP) <ref type="bibr" target="#b3">[4]</ref> perform template matching by aligning the CAD model with an observed 3D point cloud. Another family of approaches aim to regress the object coordinates onto its CAD model for each observed object pixel, and then use voting to solve for object pose <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. These approaches are limited by the need to have exact CAD models for particular object instances.</p><p>Category-level pose estimation aims to infer an object's pose and scale relative to a category-specific canonical representation. Recently, Wang et al. <ref type="bibr" target="#b27">[28]</ref> extended the object coordinate based approach to perform categorylevel pose estimation. The key idea behind the intracategory generalization is to regress the coordinates within a Normalized Object Coordinate Space (NOCS), where the sizes are normalized and the orientations are aligned for objects in a given category. Whereas the work by <ref type="bibr" target="#b27">[28]</ref> focuses on pose and size estimation for rigid objects, the work presented here extends the NOCS concept to accommodate articulated objects at both part and object level. In addition to pose, our work also infers joint information and addresses particular problems related to occlusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Articulated object pose estimation.</head><p>Most algorithms that attempt pose estimation for articulated objects assume that instance-level information is available. The approaches often use CAD models for particular instances along with known kinematic parameters to constrain the search space and to recover the pose separately for different parts <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b8">9]</ref>. Michel et al. <ref type="bibr" target="#b17">[18]</ref> used a random forest to vote for pose parameters on canonical body parts for each point in a depth image, followed by a variant of the Kabsch algorithm to estimate joint parameters using RANSACbased energy minimization. Desingh et al. <ref type="bibr" target="#b8">[9]</ref> adopted a generative approach using a Markov Random Field formulation, factoring the state as individual parts constrained by their articulation parameters. However, these approaches only consider known object instances and cannot handle different part and kinematic variations. A recent work <ref type="bibr" target="#b0">[1]</ref> also tries to handle novel objects within the same category by training a mixed density model <ref type="bibr" target="#b4">[5]</ref> on depth images, their method could infer kinematic model using probabilities predictions of a mixtures of Gaussians. However they don't explicitly estimate pose on part-level, the simplified geometry predictions like length, width are for the whole object with scale variation only.</p><p>Another line of work relies on active manipulation of an object to infer its articulation pattern <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b31">32]</ref>. For example, Katz et al. <ref type="bibr" target="#b13">[14]</ref>, uses a robot manipulator to interact with articulated objects as RGB-D videos are recorded. Then the 3D points are clustered into rigid parts according to their motion. Although these approaches could perform pose estimation for unknown objects, they require the input to be a sequence of images that observe an object's different articulation states, whereas our approach is able to perform the task using a single depth observation.</p><p>Human body and hand pose estimation. Two specific articulated classes have gained considerable attention recently: the human body and the human hand. For human pose estimation, approaches have been developed using end-to-end networks to predict 3D joint locations directly <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b18">19]</ref>, using dense correspondence maps between 2D images and 3D surface models <ref type="bibr" target="#b2">[3]</ref>, or estimating full 3D shape through 2D supervision <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b19">20]</ref>. Similarly, techniques for hand pose estimation (e.g., <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b10">11]</ref>) leverages dense coordinate regression, which is then used for voting 3D joint locations. Approaches for both body and hand pose estimation are often specifically customized for those object types, relying on a fixed skeletal model with class-dependent variability (e.g., expected joint lengths) and strong shape priors (e.g., using parametric body shape model for low-dimensional parameterization). Also, such hand/body approaches accommodate only revolute joints. In contrast, our algorithm is designed to handle generic articulated objects with varying kinematic chain, allowing both revolute joints and prismatic joints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Problem Statement</head><p>The input to the system is a 3D point cloud P = {p i ∈ R 3 | i = 1, ..., N } backprojected from a single depth image representing an unknown object instance from a known category, where N denotes the number of points. We know that all objects from this category share the same kinematic chain composed of M rigid parts {S (j) | j = 1, ..., M } and K joints with known types {J k | k = 1, ..., K}. The goal is to segment the point cloud into rigid parts {S (j) }, recover the 3D rotations {R (j) }, 3D translations {t (j) }, and sizes {s (j) } for the parts in {S (j) }, and predict the joint parameter {φ k } and state {θ k } for the joints in {J k }. In this work, we consider 1D revolute joints and 1D prismatic joints. We parameterize the two types of joints as following. For a revolute joint, its joint parameters include the direction of the rotation axis u (r) k as well as a pivot point q k on the rotation axis; its joint state is defined as the relative rotation angle along u (r) k between the two connected parts compared with a pre-defined rest state. For a prismatic joint, its joint parameter is the direction of the translation axis u (t) k , and its joint state is defined as the relative translation distance along u (t) k between the two connected parts compared with a pre-defined rest state.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Method</head><p>ANCSH provides a category-specific reference frame defining per-part poses as well as joint attributes for previously unseen articulated object instances. In Sec. 4.1, we first explain ANCSH in detail. In Sec. 4.2, we then present a deep neural network capable of predicting the ANCSH representation. Sec. 4.3 describes how the ANCSH representation is used to jointly optimize part poses with explicit joint constraints. Last, we describe how we compute joint states and deduce camera-space joint parameters in Sec. 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">ANCSH Representation</head><p>Our ANCSH representation is inspired by and closely related to Normalized Object Coordinate Space (NOCS) <ref type="bibr" target="#b27">[28]</ref>, which we briefly review here. NOCS is defined as a 3D space contained within a unit cube and was introduced in <ref type="bibr" target="#b27">[28]</ref> to estimate the category-level 6D pose and size of rigid objects. For a given category, the objects are consistently aligned by their orientations in the NOCS. Furthermore, these objects are zero-centered and uniformly scaled so that their tight bounding boxes are all centered at the origin of the NOCS with a diagonal length of 1. NOCS provides a reference frame for rigid objects in a given category so that the object pose and size can then be defined using the similarity transformation from the NOCS to the camera space. However, NOCS is limited for representing articulated objects. Instead of the object pose and size, we care more about the poses and the states for each individual parts and joints, which isn't addressed in NOCS.</p><p>To define category-level per-part poses and joint attributes, we present ANCSH, a two-level hierarchy of normalized coordinate spaces, as shown in Figure <ref type="figure" target="#fig_1">2</ref>. At the root level, NAOCS provides an object-level reference frame with normalized pose, scale, and articulation; at the leaf level, NPCS provides a reference frame for each individual part. We explain both NPCS and NAOCS in detail below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NAOCS.</head><p>To construct a category-level object reference frame for the collection of objects, we first bring all the object articulations into a set of pre-defined rest states. Basically, for each joint J k , we manually define its rest state θ k0 and then set the joint into this state. For example, we define the rest states of the two revolute joints in the eyeglasses category to be in right angles; we define the rest states of all drawers to be closed. In addition to normalizing the articulations, NAOCS applies the same normalization used in <ref type="bibr" target="#b27">[28]</ref> to the objects, including zero-centering, aligning orientations, and uniformly scaling.</p><p>As a canonical object representation, NAOCS has the following advantages: 1) the joints are set to predefined states so that accurately estimating joint parameters in NAOCS, e.g. the direction of rotation/translation axis, becomes an easy task; 2) with the canonical joints, we can build simple mathematical models to describe the kinematic constraints regarding each individual joint in NAOCS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NPCS.</head><p>For each part, NPCS further zero-centers its position and uniformly scales it as is done in <ref type="bibr" target="#b27">[28]</ref>, while at the same time keeps its orientation unchanged as in NAOCS. In this respect, NPCS is defined similarly to NOCS <ref type="bibr" target="#b27">[28]</ref> but for individual parts instead of whole objects. NPCS provides a part reference frame and we can define the part pose and scale as the transformation from NPCS to the camera space. Note that corresponding parts of different object instances are aligned in NPCS, which facilitates intra-category generalization and enables predictions for unseen instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relationship between NPCS, NAOCS and NOCS.</head><p>Both NPCS and NAOCS are inspired by the NOCS representation and designed for handling a collection of articulated objects from a given category. Therefore, similar to NOCS, both representations encode canonical information and enable generalization to new object instances. However, each of the two representations has its own advantages in modeling articulated objects and hence provides complementary information. Thus, our ANCSH leverages both NPCS and NAOCS to form a comprehensive representation of both parts and articulations.</p><p>On the one hand, NPCSs normalize the position, orientation, and size for each part. Therefore, transformation between NPCSs and camera space can naturally be used to compute per-part 3D amodal bounding boxes, which is not well-presented in NAOCS representation. On the other hand, NAOCS looks at the parts from a holistic view, encoding the canonical relationship of different parts in the object space. NAOCS provides a parent reference frame to those in NPCSs and allows a consistent definition of the joint parameters across different parts. We hence model joints and predict joint parameters in the NAOCS instead of NPCSs. The joint parameters can be used to deduce joint constraints, which can regularize the poses between connected parts. Note that the information defined in NPCS and NAOCS is not mutually exclusive -each NPCS can transform into its counterpart in NAOCS by a uniform scaling and translation. Therefore, instead of independently predicting the full NAOCS representation, our network predicts the scaling and translation parameters for each object part and directly applies it on the corresponding NPCS to obtain the NAOCS estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">ANCSH Network</head><p>We devise a deep neural network capable of predicting the ANCSH representation for unseen articulated object instances. As shown in Figure <ref type="figure" target="#fig_2">3</ref>, the network takes a depth point cloud P as input and its four heads output rigid part segmentation, dense coordinate predictions in each NPCS, transformations from each NPCS to NAOCS, and joint parameters in NAOCS, correspondingly. The network is based on two modules adapted from the PointNet++ <ref type="bibr" target="#b20">[21]</ref> segmentation architecture. The part segmentation head predicts a per-point probability distribution among the M rigid parts. The NPCS head predicts M coordinates {c (j) i ∈ R 3 | j = 1, ..., M } for each point p i . We use the predicted part label to select the corresponding NPCS. This design helps to inject the geometry prior of each part into the network and hence specializes the networks on part-specific predictions. We design the segmentation network and the NPCS regression network to share the same PointNet++ backbone and only branch at the last fully-connected layers.</p><p>The NAOCS head predicts the transformations {G (j) } from each NPCS to the NAOCS, and computes the coordinates in NAOCS using the predicted transformations. Since part orientations are the same between NPCS and NAOCS, the network only needs to estimate a 3D translation G (j) t and a 1D scaling G (j) s for the NPCS of the part S (j) . Similar to NPCS head, the head here predicts for each point p i dense transformations with G (j) t,i and G (j) s,i for each NPCS of the parts S (j) . We use the predicted segmentation label to select per-point translation G t,i and scaling G s,i . Then the NAOCS coordinates can be represented as</p><formula xml:id="formula_0">{g i | g i = G s,i c i + G t,i }. Finally, we compute G (j) s and G (j)</formula><p>t by averaging over points {p i ∈ S (j) }. The last head infers joint parameters {φ ′ k } for each joint J k in the NAOCS space (we use " ′ " here to distinguish the NAOCS parameters from camera-space parameters.) We consider the following two types of joints: 1D revolute joint whose parameters include the rotation axis direction and the pivot point position, namely φ ′ k = (u (r)′ k , q ′ k ); 1D prismatic joint whose parameters is the translation axis direction</p><formula xml:id="formula_1">φ ′ k = (u (t)′ k ).</formula><p>We adopt a voting scheme to accurately predict joint parameters, in which we first associate points to each joint via a labeling scheme and then let the points vote for the parameters of its associated joint.</p><p>We define a per-point joint association {a i | a i ∈ {0, 1, ..., K}}, where label k means the point p i is associated to the joint J k and label 0 means no association to any joint. We use the following heuristics to provide the ground truth joint association: for a revolute joint J k , if a point p i belongs to its two connecting parts and is within a distance σ from its rotation axis, then we set a i = k; for a prismatic joint, we associate it with all the points on its corresponding moving part. We empirically find σ = 0.2 leads to a non-overlapping joint association on our data.</p><p>In addition to predicting joint association, the joint parameter head performs dense regression on the associated joint parameters. To be more specific, for each point p i , the head regresses a 7D vector v i ∈ R 7 . The first three dimensions of v i is a unit vector, which either represents u (r)′ for a revolute joint or u (t)′ for a prismatic joint. The rest four dimensions are dedicated to the pivot point q ′ in case the point is associated to a revolute joint. Since the pivot point of a 1D revolute joint is not uniquely defined (it can move arbitrarily along the rotation axis), we instead predict the projection of p i to the rotation axis of its associated revolute joint by regressing a 3D unit vector for the projection direction and a scalar for the projection distance. For training, we only supervise the matched dimensions of v i for points p i with a i = 0. We use the ground truth joint parameters φ ′ ai associated with joint J ai as the supervision. During inference, we use the predicted joint association to interpret v i . We perform a voting step to get the final joint parameter prediction φ ′ k , where we simply average the predictions from points associated with each joint J k . Note that the NAOCS head and the joint parameter head share the second PointNet++ as their backbone since they all predict attributes in the NAOCS.</p><p>Loss functions: We use relaxed IoU loss <ref type="bibr" target="#b31">[32]</ref> L seg for part segmentation as well as for joint association L association . We use mean-square loss L NPCS for NPCS coordinate regression. We use mean-square loss L NAOCS for NAOCS to supervise per-point translation {G (j) t,i } i,j and scaling {G (j) s,i } i,j . We again use mean-square loss L joint for joint parameter predictions. Our total loss is given by L = λ 1 L seg + λ 2 L NPCS + λ 3 L NAOCS + λ 4 L association + λ 5 L joint , where the loss weights are set to [1, 10, 1, 1, 1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Pose Optimization with Kinematic Constraints</head><p>Given the output of our ANCSH network, including part segmentation, {c i } for each point p i , {G (j) t , G (j) s } for each part S (j) , and {φ ′ k } for each joint J k , we now estimate the 6D poses and sizes {R (j) , t (j) , s (j) } for each part S (j) .</p><p>Considering a part S (j) , for the points {p i ∈ S (j) }, we have their corresponding NPCS predictions {c i |p i ∈ S (j) }. We could follow <ref type="bibr" target="#b27">[28]</ref> to perform pose fitting, where the Umeyama algorithm <ref type="bibr" target="#b25">[26]</ref> is adopted within a RANSAC <ref type="bibr" target="#b9">[10]</ref> framework to robustly estimate the 6D pose and size of a single rigid object. However, without leveraging joint constraints, naively applying this approach to each individual part in our setting would easily lead to physically impossible part poses. To cope with this issue, we propose the following optimization scheme leveraging kinematic constraints for estimating the part poses. Without the kinematic constraints, the energy function E vanilla regarding all part poses can be written as E vanilla = j e j , where</p><formula xml:id="formula_2">e j = 1 |S (j) | p i ∈S (j) ||p i − (s (j) R (j) c i + t (j) )|| 2</formula><p>We then introduce the kinematic constraints by adding an energy term e k for each joint to the energy function. In concrete terms, our modified energy function is E constrained = j e j + λ k e k , where e k is defined differently for each type of joint. For a revolute joint J k with parameters φ ′ k = (u (r)′ k , q ′ k ) in the NAOCS, assuming it connects part S (j1) and part S (j2) , we define e k as:</p><formula xml:id="formula_3">e k = ||R (j1) u (r)′ k − R (j2) u (r)′ k || 2</formula><p>For a prismatic joint J k with parameters φ ′ k = (u</p><formula xml:id="formula_4">(t)′</formula><p>k ) in the NAOCS, again assuming it connects part S (j1) and part S (j2) , we define e k as:</p><formula xml:id="formula_5">e k = µ||R (j1) R (j2) T − I|| 2 + j=j1,j2 ||[R (j) u (t)′ k ] × δ j1,j2 || 2</formula><p>where [•] × converts a vector into a matrix for conducting cross product with other vectors, and δ j1,j2 is defined as:</p><formula xml:id="formula_6">δ j1,j2 = t (j2) − t (j1) + s (j1) R (j1) G (j1) t − s (j2) R (j2) G (j2) t</formula><p>To minimize our energy function E constrained , we can no longer separately solve different part poses using the Umeyama algorithm. Instead, we first minimize E vanilla using the Umeyama algorithm to initialize our estimation of the part poses. Then we fix {s (j) } and adopt a non-linear least-squares solver to further optimize {R (j) , t (j) }, as is commonly done for bundle adjustment <ref type="bibr" target="#b1">[2]</ref>. Similar to <ref type="bibr" target="#b27">[28]</ref>, we also use RANSAC for outlier removal.</p><p>Finally, for each part S (j) , we use the fitted R (j) , t (j) , s (j) and the NPCS {c i |p i ∈ S (j) } to compute an amodal bounding box, the same as in <ref type="bibr" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Camera-Space Joint Parameters and Joint States Estimation</head><p>Knowing {R (j) , t (j) , s (j) , G</p><formula xml:id="formula_7">(j) t , G<label>(j)</label></formula><p>s } of each part, we can compute the joint states {θ k } and deduce joint parameters {φ k } in the camera space from NAOCS joint parameters {φ ′ k }. For a revolute joint J k connecting parts S (j1) and S (j2) , we compute its parameters φ k = (u (r) k , q k ) in the camera space as:</p><formula xml:id="formula_8">u (r) k = (R (j1) + R (j2) )u (r)′ k ||(R (j1) + R (j2) )u (r)′ k || q k = 1 2 j=j1,j2 R (j) s (j) G (j) s q ′ k − G (j) t + t (j)</formula><p>The joint state θ k can be computed as:</p><formula xml:id="formula_9">θ k = arccos((trace(R (j2) (R (j1) ) T ) − 1)/2)</formula><p>For a prismatic joint J k connecting parts S (k1) and S (k2) , we compute its parameters φ k = (u</p><formula xml:id="formula_10">(t)</formula><p>k ) in the camera space similar to computing u (r) k for revolute joints and and its state θ k is simply ||δ k1,k2 ||.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental Setup</head><p>Evaluation Metrics. We use the following metrics to evaluate our method.</p><p>• Part-based metrics. For each part, we evaluate rotation error measured in degrees, translation error, and 3D intersection over union (IoU) <ref type="bibr" target="#b21">[22]</ref> of the predicted amodal bounding box.</p><p>• Joint states. For each revolute joint, we evaluate joint angle error in degrees. For each prismatic joint, we evaluate the error of relative translation amounts.</p><p>• Joint parameters. For each revolute joint, we evaluate the orientation error of the rotation axis in degrees, and the position error using the minimum line-to-line distance. For each prismatic joint, we compute the orientation error of the translation axis.</p><p>Datasets. We have evaluated our algorithm using both synthetic and real-word datasets. To generate the synthetic data, we mainly use object CAD models from <ref type="bibr" target="#b28">[29]</ref> along with drawer models from <ref type="bibr" target="#b29">[30]</ref>. Following the same rendering pipeline with random camera viewpoints, we use PyBullet <ref type="bibr" target="#b7">[8]</ref> to generate on average 3000 testing images of unseen object instances for each object category that do not overlap with our training data. For the real data, we evaluated our algorithm on the dataset provided by Michel et al. <ref type="bibr" target="#b17">[18]</ref>, which contains depth images for 4 different objects captured using the Kinect. Baselines. There are no existing methods for categorylevel articulated object pose estimation. We therefore use ablated versions of our system for baseline comparison.</p><p>• NPCS. This algorithm predicts part segmentation and NPCS for each part (without the joint parameters).</p><p>The prediction allows the algorithm to infer part pose, amodal bounding box for each part, and joint state for revolute joint by treating each part as an independent rigid body. However, it is not able to perform a combined optimization with the kinematic constraints.</p><p>• NAOCS. This algorithm predicts part segmentation and NAOCS representation for the whole object instance. The prediction allows the algorithm to infer part pose and joint state, but not the amodal bounding boxes for each part since the amodal bounding boxes are not defined in the NAOCS alone. Note the part pose here is defined from the NAOCS to the camera space, different from the one we defined based upon NPCS. We measure the error in the observed object scale so that it is comparable with our method.</p><p>• Direct joint voting. This algorithm directly votes for joint-associated parameters in camera space, including offset vectors and orientation for each joint from the point cloud using PointNet++ segmentation network.</p><p>Our final algorithm predicts the full ANCSH representation that includes NPCS, joint parameters, and per-point global scaling and translation value that can be used together with the NPCS prediction for computing NAOCS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Experimental Results</head><p>Figure <ref type="figure" target="#fig_3">4</ref> presents some qualitative results. Tables 1 summarizes the quantitative results. Following paragraphs provide our analysis and discussion of the results.</p><p>Effect of combined optimization. First, we want to examine how combined optimization would influence the accuracy of articulated object pose estimation, using both predicted joint parameters and predicted part poses. To see this, we compare the algorithm performance between NPCS and ANCSH, where NPCS performs a per-part pose estimation and ANCSH performs a combined optimization using the full kinematic chain to constrain the result. The results in Table <ref type="table" target="#tab_0">1</ref> show that the combined optimization of joint parameters and part pose consistently improves the predict results for almost all object categories and on almost all evaluation metrics. The improvement is particularly salient for thin object parts such as the two temples of eyeglasses (the parts that extend over the ears), where the per-part based method produces large pose errors due to limited number of visible points and shape ambiguity. This result demonstrates that the joint parameters predicted in the NAOCS can regularize the part poses based on kinematic chain constraints during the combined pose optimization step and improve the pose estimation accuracy.</p><p>Joint parameters estimation. Predicting the location and the orientation of joints in camera space directly with all degrees of freedom is challenging. Our approach predicts the joint parameters in NAOCS since it provides a canonical representation where the joint axes usually have a strong orientation prior. We further use a voting-based scheme to reduce the prediction noise. Given joint axis predictions in NAOCS, we leverage the transformation between NAOCS and NPCS to compute corresponding joint parameters in NPCS. Based on the high-quality prediction of part poses, we will transform the joint parameters into the camera coordinate. Comparing to a direct voting baseline using PointNet++, our approach significantly improves the joint axis prediction for unseen instances (Table <ref type="table" target="#tab_1">2</ref>).  <ref type="bibr" target="#b28">[29]</ref> and SAPIEN dataset <ref type="bibr" target="#b29">[30]</ref> (for only drawer category). Bottom two rows show test result on seen instances in the real-world dataset <ref type="bibr" target="#b17">[18]</ref>. Here we visualize the predicted amodal bounding box for each parts. Color images are for visualization only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Category Methods Angle error Distance error</head><p>Eye-PointNet++ 2.9 Generalization to real depth images. We have also tested our algorithm's ability to generalize to real-world depth images on the dataset provided in <ref type="bibr" target="#b17">[18]</ref>. The dataset contains video sequences captured with Kinect for four different object instances. Following the same training protocol, we train the algorithm with synthetically rendered depth images of the provided object instances. Then we test the pose estimation accuracy on the real world depth images. We adopt the same evaluation metric in <ref type="bibr" target="#b17">[18]</ref>, which uses 10% of the object part diameter as the threshold to compute Averaged Distance (AD) accuracy, and test the performance on each sequence separately. Although our algorithm is not specifically designed for instancelevel pose estimation and the network has never been trained using any real-world depth images, our algorithm achieves strong performance on par with or even better than state-of-the-art. On average our algorithm achieves 96.25%, 92.3%, 96.9%, 79.8% AD accuracy on the whole kinematic chain of object instance laptop, cabinet, cupboard and toy train. For detailed results on each part in all the test sequences, as well as more visualizations, please refer to the supplementary material.</p><p>Limitation and failure cases. Figure <ref type="figure" target="#fig_4">5</ref> shows typical failure cases of our algorithm. A typical failure mode of our algorithm is the inaccurate prediction under heavy occlusion where one of the object parts is almost not observed. Figure <ref type="figure" target="#fig_4">5</ref> shows one of such cases where one of the eye-glasses temples is almost completely occluded. Also, under the situation of heavy occlusion for prismatic joints, there is considerate ambiguity for ANCSH prediction on the size of the heavily occluded parts, as shown in Figure <ref type="figure" target="#fig_4">5</ref>. However, NAOCS representation does not suffer from the size ambiguity, thus leading to a more reliable estimation of the joint state (relative translation distance compare to the rest state) and joint parameters (translation axis).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>This paper has presented an approach for categorylevel pose estimation of articulated objects from a single depth image. To accommodate unseen object instances with large intra-category variations, we introduce a novel object representation, namely Articulation-aware Normalized Coordinate Space Hierarchy (ANCSH). We further devise a deep neural network capable of predicting ANCSH from a single depth point cloud. We then formulate articulated pose fitting from the ANCSH predictions as  While not designed for instance-level articulated object pose estimation, our algorithm is able to achieve comparable performance compare to the state-of-the-art approach and improves the performance for challenging cases such as laptops. AD accuracy is evaluated for both the whole kinematic chain(all) and different parts(parts). a combined optimization problem, taking both part pose errors and joint constraints into consideration. Our experiments demonstrate that the ANCSH representation and the combined optimization scheme significantly improve the accuracy for both part pose prediction and joint parameters estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details</head><p>We use Tensorflow 1.10 to build our models and run the experiments for all categories. The input is uniformly sampled points from the whole back-projected depth point cloud, with points number N set to 1024. We train our model on a single Nvidia V100 GPU with batch size of 16 across the experiments. The initial learning rate is set to 0.001, with a decay factor of 0.7 every 200k steps. From the observations of our experiments, the loss will usually converge well after &gt; 150k steps in less than one day.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Data generation and statistics</head><p>We render synthetic depth images using the object 3D model provided in the Shape2Motion dataset <ref type="bibr" target="#b28">[29]</ref> and SAPIEN dataset <ref type="bibr" target="#b29">[30]</ref>. Both datasets provide the descriptions of the object geometry and articulation information, which we leverage for generating ground truths. During rendering, the program automatically generates random * indicates equal contributions. joint states for each object instance, according to its joint motion ranges. Then the depth images and corresponding ground truth masks are rendered from a set of random camera viewpoints. We also filter out camera poses where some parts of the object are completely occluded. Figure <ref type="figure" target="#fig_5">6</ref> shows the index definitions of parts for each object category used in the main paper, together with number of object instances splitted for training and testing.</p><p>We use real data from ICCV2015 Articulated Object Challenge <ref type="bibr" target="#b17">[18]</ref>, which contains RGB-D data with 4 articulated objects: laptop, cabinet, cupboard and toy train. This dataset provides 2 testing sequences for each object. Each sequence contains around 1000 images captured by having a RGB-D camera slowly moving around the object. Objects maintain the same articulation state within each sequence. Each part of the articulated object is annotated with its 6D pose with respect to the known CAD model. Since no training data is provided, we use the provided CAD models to render synthetic depth data, with 10 groups of random articulation status considered. We render object masks for the testing sequences with Pybullet <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Laptop</head><p>Oven Washing Machine Eyeglasses Drawer </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Handling severe occlusion cases</head><p>We carefully examine how ANCSH performs under different levels of occlusion. Compared to our NPCS baseline, our proposed method is still capable of improving the pose estimation under severe occlusion, as shown in Figure <ref type="figure" target="#fig_6">7</ref>. The occlusion level is defined according to the ratio of visible area with respect to the total mesh surface per part.    <ref type="figure">9</ref>. Additional results on real-world instance-level depth dataset. More qualitative results on all 4 objects from ICCV2015 Articulated Object Challenge <ref type="bibr" target="#b17">[18]</ref> are shown here, with toy train, cupboard, laptop, cabinet from up-pest row to lowest row in order. Only depth images are used for pose estimation, RGB images are shown here for better reference. For each object, we estimate 3D tight bounding boxes to all parts on the kinematic chain, and project the predicted bounding boxes back to the depth image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Additional results</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Category-level articulated object pose estimation. Given a depth point cloud of a novel articulated object from a known category, our algorithm estimates: part attributes, including part segmentation, poses, scales and amodal bounding boxes; joint attributes, including joint parameters and joint states.</figDesc><graphic url="image-2.png" coords="1,309.78,328.46,50.28,50.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Articulation-aware Normalized Coordinate Space Hierarchy (ANCSH) is a category-level object representation composed of a Normalized Articulated Object Coordinate Space (NAOCS) on top of a set of Normalized Part Coordinate Spaces (NPCSs) per part. Here we show two examples of ANCSH representation (points are colored according to its corresponding coordinates in the NAOCS/NPCS). Note that NAOCS sets the object articulations to pre-defined states, all the joints in the NAOCS are hence canonicalized, e.g. the axes of the revolute joints in the eyeglasses example all point upwards and the joint angles are right angles. For each individual part, NPCS maintains the part orientation as in the NAOCS but zero-centers its position and normalizes its scales.</figDesc><graphic url="image-21.png" coords="3,210.03,12.71,246.79,161.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. ANCSH network leverages two PointNet++ [21] modules to predict the ANCSH representation, including part segmentation, NPCS coordinates, transformations (1D scaling and 3D translation) from each NPCS to the NAOCS, and joint parameters in the NAOCS. This figure illustrates the eyeglasses case with only revolute joints, but the network structure also applies to objects with revolute and prismatic joints.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Qualitative Results. Top tow rows show test results on unseen object instances from the Shape2Motion dataset<ref type="bibr" target="#b28">[29]</ref> and SAPIEN dataset<ref type="bibr" target="#b29">[30]</ref> (for only drawer category). Bottom two rows show test result on seen instances in the real-world dataset<ref type="bibr" target="#b17">[18]</ref>. Here we visualize the predicted amodal bounding box for each parts. Color images are for visualization only.</figDesc><graphic url="image-45.png" coords="8,65.75,214.76,95.35,69.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Failure cases.Left column shows failure cases on unseen eyeglasses instances, when a part is under heavy occlusion and barely visible. Right column shows the failure case on unseen drawers, when there are shape variations on parts and only the front area of the drawer is visible. The predicted drawer size is bigger than the real size. Although the box prediction is wrong, our method can reliably predict the joint state and joint parameters by leveraging the NAOCS representation.</figDesc><graphic url="image-64.png" coords="9,50.11,362.95,236.23,118.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Synthetic data statistics. We list part definitions for each object category tested in our experiments on synthetic data, together with the numbers of object instances used for training and testing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Performance under different occlusion levels. Data is collected from part 2 of unseen eyeglasses.</figDesc><graphic url="image-70.png" coords="12,50.11,66.33,236.25,150.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 Figure 8 .</head><label>88</label><figDesc>Figure8shows additional qualitative results on the synthetic dataset. More qualitative results on real-world dataset are visualized in Figure9.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure</head><label></label><figDesc>Figure 9. Additional results on real-world instance-level depth dataset. More qualitative results on all 4 objects from ICCV2015 Articulated Object Challenge<ref type="bibr" target="#b17">[18]</ref> are shown here, with toy train, cupboard, laptop, cabinet from up-pest row to lowest row in order. Only depth images are used for pose estimation, RGB images are shown here for better reference. For each object, we estimate 3D tight bounding boxes to all parts on the kinematic chain, and project the predicted bounding boxes back to the depth image.</figDesc><graphic url="image-131.png" coords="14,62.49,71.99,470.24,636.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>• , 1.1 • , 1.2 • , 1.5 • 0.024, 0.021, 0.021, 0.033 84.0,72.1, 71.7, 78.6 0.011, 0.020, 0.030 0.8 • , 0.8 • , 0.8 • -Performance comparison on unseen object instances. The categories eyeglasses, oven, washing machine, and laptop contain only revolute joints and the drawer category contains three prismatic joints.</figDesc><table><row><cell cols="2">Category Method</cell><cell></cell><cell>Part-based Metrics</cell><cell></cell><cell>Joint States</cell><cell cols="2">Joint Parameters</cell></row><row><cell></cell><cell></cell><cell>Rotation Error ↓</cell><cell>Translation Error ↓</cell><cell>3D IoU % ↑</cell><cell>Error ↓</cell><cell>Angle error ↓</cell><cell>Distance error ↓</cell></row><row><cell>Eye-</cell><cell>NPCS NAOCS</cell><cell>4.0 • , 7.7 • , 7.2 • 4.2 • , 12.1 • , 13.5 •</cell><cell>0.044, 0.080, 0.071 0.157, 0.252, 0.168</cell><cell>86.9, 40.5, 41.4 -</cell><cell>8.8 • , 8.4 • 13.7 • , 15.1 •</cell><cell>--</cell><cell>--</cell></row><row><cell cols="2">glasses ANCSH</cell><cell>3.7 • , 5.1 • , 3.7 •</cell><cell>0.035, 0.051, 0.057</cell><cell>87.4, 43.6, 44.5</cell><cell>4.3 • , 4.5 •</cell><cell>2.2 • , 2.3 •</cell><cell>0.019 , 0.014</cell></row><row><cell></cell><cell>NPCS</cell><cell>1.3 • , 3.5 •</cell><cell>0.032, 0.049</cell><cell>75.8 , 88.5</cell><cell>4.0 •</cell><cell>-</cell><cell>-</cell></row><row><cell>Oven</cell><cell>NAOCS</cell><cell>1.7 • , 4.7 •</cell><cell>0.036 , 0.090</cell><cell>-</cell><cell>5.1 •</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>ANCSH</cell><cell>1.1 • , 2.2 •</cell><cell>0.030, 0.046</cell><cell>75.9 , 89.0</cell><cell>2.1 •</cell><cell>0.8 •</cell><cell>0.024</cell></row><row><cell>Washing</cell><cell>NPCS NAOCS</cell><cell>1.1 • , 2.0 • 1.1 • , 3.3 •</cell><cell>0.043 , 0.056 0.072 , 0.119</cell><cell>86.9 , 88.0 -</cell><cell>2.3 • 3.1 •</cell><cell>--</cell><cell>--</cell></row><row><cell cols="2">Machine ANCSH</cell><cell>1.0 • , 1.4 •</cell><cell>0.042, 0.053</cell><cell>87.0 , 88.3</cell><cell>1.00 •</cell><cell>0.7 •</cell><cell>0.008</cell></row><row><cell></cell><cell>NPCS</cell><cell>11.6 • , 4.4 •</cell><cell>0.098, 0.044</cell><cell>35.7, 93.6</cell><cell>14.4 •</cell><cell>-</cell><cell>-</cell></row><row><cell>Laptop</cell><cell>NAOCS</cell><cell>12.4 • , 4.9 •</cell><cell>0.110, 0.049</cell><cell>-</cell><cell>15.2 •</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>ANCSH</cell><cell>6.7 • , 4.3 •</cell><cell>0.062, 0.044</cell><cell>41.1, 93.0</cell><cell>9.7 •</cell><cell>0.5 •</cell><cell>0.017</cell></row><row><cell></cell><cell cols="5">NPCS 1.9 • , 3.5 • , 2.4 • , 1.8 • 0.032, 0.038, 0.024, 0.025 82.8, 71.2, 71.5, 79.3 0.026, 0.031, 0.046</cell><cell>-</cell><cell>-</cell></row><row><cell>Drawer</cell><cell cols="3">NAOCS 1.5 • , 2.5 • , 2.5 • , 2.0 • 0.044, 0.045, 0.073, 0.054</cell><cell>-</cell><cell>0.043, 0.066, 0.048</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="2">ANCSH 1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>A comparison of joint parameters estimation. Here PointNet++ denotes the direct joint voting baseline.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>0.140, 0.197</cell></row><row><cell>glass</cell><cell>ANCSH</cell><cell>2.2 • , 2.3 •</cell><cell>0.019, 0.014</cell></row><row><cell>Oven</cell><cell>PointNet++ ANCSH</cell><cell>27.0 • 0.8 •</cell><cell>0.024 0.024</cell></row><row><cell cols="2">Washing PointNet++</cell><cell>8.7 •</cell><cell>0.010</cell></row><row><cell cols="2">Machine ANCSH</cell><cell>0.7 •</cell><cell>0.008</cell></row><row><cell>Laptop</cell><cell>PointNet++ ANCSH</cell><cell>29.5 • 0.5 •</cell><cell>0.007 0.017</cell></row><row><cell>Drawer</cell><cell cols="2">PointNet++ 4.9 • ,5.0 • ,5.1 • ANCSH 0.8 • ,0.8 • ,0.8 •</cell><cell>--</cell></row></table><note>• , 15.7 •</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>1% 17.8% 81.1% 52.5% 99.2% 99.9% 99.9% 99.1% 92.0% 68.5% 99.3% 99.2%</figDesc><table><row><cell>Object</cell><cell cols="2">Sequence</cell><cell>Brachmann et al.[6]</cell><cell>Frank et al.[18]</cell><cell>ANCSH (Ours)</cell></row><row><cell>Laptop</cell><cell>1</cell><cell>all parts</cell><cell>8.9% 29.8% 25.1%</cell><cell>64.8% 65.5% 66.9%</cell><cell>94.1% 97.5% 94.7%</cell></row><row><cell></cell><cell>2</cell><cell>all parts</cell><cell>1% 1.1% 63.9%</cell><cell>65.7% 66.3% 66.6%</cell><cell>98.4% 98.9% 99.0%</cell></row><row><cell>Cabinet</cell><cell>3</cell><cell>all parts</cell><cell>0.5% 86% 46.7% 2.6%</cell><cell>95.8% 98.2% 97.2% 96.1%</cell><cell>90.0% 98.9% 97.8% 91.9%</cell></row><row><cell></cell><cell>4</cell><cell>all parts</cell><cell>49.8% 76.8% 85% 74%</cell><cell>98.3% 98.3% 98.7% 98.7%</cell><cell>94.5% 99.5% 99.5% 94.9%</cell></row><row><cell>Cupboard</cell><cell>5</cell><cell>all parts</cell><cell>90% 91.5% 94.3%</cell><cell>95.8% 95.9% 95.8%</cell><cell>93.9% 99.9% 93.9%</cell></row><row><cell></cell><cell>6</cell><cell>all parts</cell><cell>71.1% 76.1% 81.4%</cell><cell>99.2% 99.9% 99.2%</cell><cell>99.9% 100% 99.9%</cell></row><row><cell>Toy train</cell><cell cols="4">all all parts 90.8 7 parts 74.8% 20.3% 78.2% 51.2% 100% 100% 97% 94.3% 7.8% 98.1% 5.7% 94.3%</cell><cell>68.4% 91.1% 100% 100% 100% 91.1%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Instance-level real-world depth benchmark.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement: This research is supported by a grant from Toyota-Stanford Center for AI Research, resources provided by Advanced Research Computing in the Division of Information Technology at Virginia Tech. We thank Vision and Learning Lab at Virginia Tech for help on visualization tool.</p><p>We are also grateful for the financial and hardware support from Google.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to generalize kinematic models to novel objects</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Abbatematteo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Tellex</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Konidaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Robot Learning</title>
				<meeting>the Third Conference on Robot Learning</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bundle adjustment in the large</title>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="29" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Densepose: Dense human pose estimation in the wild</title>
		<author>
			<persName><forename type="first">Rıza</forename><surname>Alp Güler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7297" to="7306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A method for registration of 3-d shapes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><forename type="middle">D</forename><surname>Besl</surname></persName>
		</author>
		<author>
			<persName><surname>Mckay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PAMI</title>
				<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Mixture density networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning 6d object pose estimation using 3d object coordinates</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Gumhold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Uncertainty-driven 6d pose estimation of objects and scenes from a single rgb image</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">Ying</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Gumhold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3364" to="3372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Pybullet, a python module for physics simulation for games, robotics and machine learning</title>
		<author>
			<persName><forename type="first">Erwin</forename><surname>Coumans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfei</forename><surname>Bai</surname></persName>
		</author>
		<ptr target="http://pybullet.org" />
		<imprint>
			<date type="published" when="2016">2016. 2018</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Factored pose estimation of articulated objects using efficient nonparametric belief propagation</title>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Desingh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Opipari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Odest Chadwicke</forename><surname>Jenkins</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.03647</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName><forename type="first">A</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">C</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="381" to="395" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Point-topoint regression pointnet for 3d hand pose estimation</title>
		<author>
			<persName><forename type="first">Liuhao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junsong</forename><surname>Zhou Ren</surname></persName>
		</author>
		<author>
			<persName><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
				<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="475" to="491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Active articulation model estimation through interactive perception</title>
		<author>
			<persName><forename type="first">Karol</forename><surname>Hausman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Niekum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Osentoski</surname></persName>
		</author>
		<author>
			<persName><surname>Gaurav S Sukhatme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Robotics and Automation (ICRA)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Manipulating articulated objects with interactive perception</title>
		<author>
			<persName><forename type="first">Dov</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Brock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 IEEE International Conference on Robotics and Automation</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Interactive segmentation, tracking, and kinematic modeling of unknown 3d articulated objects</title>
		<author>
			<persName><forename type="first">Dov</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moslem</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Stentz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2013">2013. 2013</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unite the people: Closing the loop between 3d and 2d human representations</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6050" to="6059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An integrated approach to visual perception of articulated objects</title>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Martín-Martín</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Höfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Brock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Robotics and Automation (ICRA)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Vnect: Real-time 3d human pose estimation with a single rgb camera</title>
		<author>
			<persName><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">44</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pose estimation of kinematic chain instances via object coordinate regression</title>
		<author>
			<persName><forename type="first">Frank</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carsten</forename><surname>Gumhold</surname></persName>
		</author>
		<author>
			<persName><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
				<imprint>
			<date type="published" when="2009">2015. 3, 6, 8, 9</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3d human pose</title>
		<author>
			<persName><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantinos</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="7025" to="7034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning to estimate 3d human pose and shape from a single color image</title>
		<author>
			<persName><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luyang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="459" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName><forename type="first">Charles</forename><surname>Ruizhongtai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep sliding shapes for amodal 3d object detection in rgb-d images</title>
		<author>
			<persName><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="808" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Compositional human pose regression</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxiang</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2602" to="2611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Implicit 3d orientation learning for 6d object detection from rgb images</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoltan-Csaba</forename><surname>Marton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Durner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Brucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rudolph</forename><surname>Triebel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
				<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="699" to="715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Deep object pose estimation for semantic robotic grasping of household objects</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Tremblay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thang</forename><surname>To</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balakumar</forename><surname>Sundaralingam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stan</forename><surname>Birchfield</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.10790</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Least-squares estimation of transformation parameters between two point patterns</title>
		<author>
			<persName><forename type="first">Shinji</forename><surname>Umeyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="376" to="380" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dense 3d regression for hand pose estimation</title>
		<author>
			<persName><forename type="first">Chengde</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Probst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5147" to="5156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Normalized object coordinate space for category-level 6d object pose and size estimation</title>
		<author>
			<persName><forename type="first">He</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingwei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006">2019. 1, 2, 4, 6</date>
			<biblScope unit="page" from="2642" to="2651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Shape2motion: Joint analysis of motion parts and attributes from 3d shapes</title>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yahao</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinping</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Sapien: A simulated part-based interactive environment</title>
		<author>
			<persName><forename type="first">Fanbo</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuzhe</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaichun</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yikuan</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minghua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">He</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonidas</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Posecnn: A convolutional neural network for 6d object pose estimation in cluttered scenes</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tanner</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Venkatraman</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00199</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep part induction from articulated object pairs</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Difan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evangelos</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonidas</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH Asia</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Technical Papers</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
