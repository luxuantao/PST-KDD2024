<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Knowledge-Based Systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yiling</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">Southwest Jiaotong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Yan</forename><surname>Yang</surname></persName>
							<email>yyang@swjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">Southwest Jiaotong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tianrui</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hamido</forename><surname>Fujita</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">Southwest Jiaotong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Iwate Prefectural University</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Knowledge-Based Systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">50ADA44AEFC2D7B10A8774F30875AF92</idno>
					<idno type="DOI">10.1016/j.knosys.2018.10.001</idno>
					<note type="submission">Received 17 May 2018 Received in revised form 25 August 2018 Accepted 1 October 2018</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T05:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Data mining Clustering Multi-task multi-view</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-view clustering and multi-task clustering attract much attention in recent years. With the development of data mining, a new learning scenario containing the properties of multi-task and multi-view together appears, which called multi-task multi-view learning. Existing multi-task multi-view learning usually applies for classification and considers that all tasks share the same class label sets. Nevertheless, there is not much information about label sets in real world applications and it is difficult for all learning tasks to contain the same label sets. Hence, in order to overcome the two limitations, we propose a multitask multi-view clustering algorithm in heterogeneous situations based on Locally Linear Embedding (LLE) and Laplacian Eigenmaps (LE) methods (L3E-M2VC). It maps the samples of multiple views from each task to a common view space firstly, then transforms the samples to a discriminative task space secondly, and finally exploits K-Means for clustering. Experiments on several multi-task multi-view data sets are evaluated by RI and CA and the results show that our L3E-M2VC outperforms the other 11 methods, including single-task single-view, multi-view, multi-task, multi-view multi-task algorithms and the varieties of our method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>It is interesting for many real world applications. People are not able to distinguish the birds and hens by appearance accurately, but the feature of flying from another view assists distinction. It is usually called multi-view learning. Besides, a task of clustering fruits from the mixture of vegetables and fruits relates to another clustering task about vegetables. That is multi-task learning. The two phenomena even exist in a same application sometimes. With the development of data mining, there is no doubt for that multiview learning is a hot topic in research. Among it, multi-view clustering has attracted increasing attention in recent years. It exploits the shared and complementary features among different views to improve the performance of single-view clustering <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref>. On the other hand, multi-task clustering as the one of the most representative multi-task learning branches improves the performance of individual clustering task by extracting the relationship among related tasks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. However, some problems exhibit dualdiversity as previously mentioned. For example, the clustering task corresponding to English and Chinese songs is a problem of multitask clustering with two tasks, and the songs consist of two views, including lyric and audio features. Such type of problems is widely known as multi-task multi-view clustering.</p><p>Multi-view clustering is a hot topic recently years because of its universality for applications. Most of multi-view approaches are motivated by single view methods. A novel clustering model for single view was proposed firstly to learn the data similarity matrix and clustering structure simultaneously <ref type="bibr" target="#b6">[7]</ref>, which assigns the adaptive and optimal neighbors for each data point based on the local distance and imposes a new rank constraint such that the connected components in the resulted similarity matrix are exactly equal to the cluster number. Graph-based methods make a big contribution to clustering. <ref type="bibr" target="#b7">[8]</ref> allowed the data graph itself to be adjusted as part of the clustering procedure and utilized Constrained Laplacian Rank (CLR) method to learn a graph with exactly k connected components. To address the limitation of sensitivity for outlier and noise, a robust graph regularized NMF model (RGNMF) was presented <ref type="bibr" target="#b8">[9]</ref> to approximate the data matrix for clustering and an error matrix was introduced to capture the sparse corruption. Further, an unified learning framework of co-clustering with adaptive local structure learning was proposed <ref type="bibr" target="#b9">[10]</ref>, which performs intrinsic structure learning and tri-factorization simultaneously. Motivated by various single view clustering approaches, multiview clustering is developed rapidly. A framework of NMF based clustering was presented <ref type="bibr" target="#b10">[11]</ref> to tackle multi-view unmapped data and define the disagreement between each pair of views to assist clustering. It is popular to extend graph-based methods from single view to multi-view clustering. In <ref type="bibr" target="#b11">[12]</ref>, individual similarity matrix was reconstructed for each view and a unified similarity matrix was obtained consisting of the similarity matrices among various views. In addition, a novel framework via the reformulation of the standard spectral learning model for multi-view clustering was proposed <ref type="bibr" target="#b12">[13]</ref>, which integrates the different representations and obtains the most consistent manifold with the real data distributions. Considering assigning a reasonable weight to each view according to the view importance, a Laplacian rank constrained graph was explored <ref type="bibr" target="#b13">[14]</ref>. Similarly, a novel multi-view learning model was presented <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>, which performs clustering and local structure learning simultaneously, and the obtained optimal graph could be partitioned into specific clusters directly. Besides, a multiview clustering model without parameters was proposed <ref type="bibr" target="#b16">[17]</ref>, and the soft capped norm was introduced to cap the residual of outliers as a constant value and provide a probability for certain data point being an outlier. Nonnegative matrix factorization based multi-view clustering algorithms have also shown their competitiveness. A multi-manifold regularized nonnegative matrix factorization framework (MMNMF) was putted forward <ref type="bibr" target="#b17">[18]</ref>, which preserves locally geometrical structure of the manifolds for multiview clustering. Furthermore, a novel robust multi-view clustering method via capped-norm K-means was presented <ref type="bibr" target="#b18">[19]</ref> to integrate heterogeneous representations of data. To satisfy the consistency across various views and weight the different features in each view simultaneously, a novel multi-view clustering approach termed Two-level Weighted Collaborative k-means (TW-Co-k-means) was designed <ref type="bibr" target="#b19">[20]</ref>, which exploits the distinctive information in each view while taking advantages of the complementary and consistency across all views in a collaborative manner to assist multiview clustering.</p><p>With the rapid evolution of complex tasks for data processing, more and more emerging challenges have been posed and it is significant to develop multi-task clustering. A smart multi-task Bregman clustering (S-MBC) was presented <ref type="bibr" target="#b20">[21]</ref>, which identifies the negative effects of the boosting and avoids them if they occur. Simultaneously, a smart multi-task kernel clustering framework for nonlinear separable data was extended <ref type="bibr" target="#b20">[21]</ref> by using a similar framework like multi-task Bregman clustering in the kernel space. In order to capture the relationship among tasks, a multi-task spectral clustering (MTSC) algorithm was proposed <ref type="bibr" target="#b21">[22]</ref> to learn intertask clustering correlation and intra-task learning correlation. Further, to solve generative clustering, two convex Discriminative Multi-task Clustering (DMTC) objectives <ref type="bibr" target="#b22">[23]</ref> were presented to learn a shared feature representation and the task relationship respectively. Based on the assumption that the tasks are usually partially related, and brute-force transfer may cause negative effect, a self-adapted multi-task clustering (SAMTC) method was putted forward <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>, which automatically identifies and transfers reusable instances among tasks, and avoids negative transfer. Similarly, a manifold regularized coding multi-task clustering (MRCMTC) method was proposed <ref type="bibr" target="#b24">[25]</ref>, which constructs the similarity matrix for each target task by exploiting useful information from the source tasks through related instances transfer. Considering different knowledge transfer ways, a general multi-task clustering algorithm by transferring knowledge of instances was presented <ref type="bibr" target="#b25">[26]</ref>, which reweights the distance between samples in various tasks by learning a shared subspace, and selects the nearest neighbors for each sample from the other tasks in the shared subspace as the auxiliary data to assist clustering.</p><p>The conventional multi-view clustering and multi-task clustering are the specific situations in multi-task multi-view clustering. Thus, it is not suitable for applying their approaches into the problems of multi-task multi-view clustering directly <ref type="bibr" target="#b26">[27]</ref>. In order to tackle the limitations, some algorithms for multi-task multi-view clustering are proposed. Multi-task clustering via SymNMF <ref type="bibr" target="#b27">[28]</ref> for multi-view data had been presented <ref type="bibr" target="#b28">[29]</ref>, which exploits a geometric affine transformation to perform several tasks simultaneously and achieves the knowledge shared between the intra-task and inter-task, respectively. Besides, a multi-view spectral clustering method <ref type="bibr" target="#b29">[30]</ref> was involved in a multi-objective formulation to explore the Pareto optimization and solve the problems of multitask multi-view clustering. In order to improve the performance of multi-task multi-view clustering methods, the ensemble learning was introduced to form a multi-objective multi-view ensemble framework based on evolutionary approach <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>. What is more, to take use of sufficient information between tasks and views, a framework integrating within-view-task clustering, multi-view relationship learning, and multi-task relationship learning <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref> was studied, where it proposes two multi-task multi-view clustering algorithms, including the bipartite graph based multi-task multi-view algorithm and the semi-NMF based multi-task multiview algorithm.</p><p>However, the existing algorithms for multi-task multi-view clustering do not consider kinds of situations in clustering simultaneously. Firstly, there are complementary features in individual task besides the shared features among several tasks <ref type="bibr" target="#b34">[35]</ref>, but the existing methods usually focus on the shared features between tasks; Secondly multiple clustering tasks do not share the same set of labels <ref type="bibr" target="#b35">[36]</ref>, such as the example of clustering songs, Chinese and English songs consist of different sets of labels because of the difference of western and eastern culture, that are called heterogeneous tasks. However, most multi-task multi-view clustering methods assume that all clustering tasks have the same class label set <ref type="bibr" target="#b36">[37]</ref>. Therefore, in this paper, we propose a multitask multi-view clustering algorithm in heterogeneous situations based on Laplacian Eigenmaps and Locally Linear Embedding (L3E-M2VC). Some multi-task multi-view learning algorithms consider the property of heterogeneous, such as <ref type="bibr" target="#b37">[38]</ref>, exploiting Linear Discriminant Analysis (LDA) to solve the multi-task multi-view learning. Nevertheless, they are only for classification and need the label information. For most of small-scale data sets, it is difficult to explore sufficient labels to train models and extract relationship. Our method utilizes the advantages of Laplacian Eigenmaps (LE) and Locally Linear Embedding (LLE), which do not need class label information and preserve the locally manifold structure of data in mapping, to learn the feature transformation matrices for all views from all tasks and consider the shared and complementary feature matrices in tasks. Hence, our proposed method facilitates the knowledge sharing, assists to solve the problems in heterogeneous situation and is suitable for the problems of multi-task multi-view clustering.</p><p>The rest of this paper is structured as follows. In Section 2, some related works about the principles of LE and LLE are reviewed. Section 3 is the detailed description of proposed L3E-M2VC. Section 4 is the time complexity analysis. Experiments on several multi-task multi-view data sets are shown in Section 5. Section 6 discusses the conclusion and future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>In this section, we present some preliminaries of multi-task multi-view clustering problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Locally Linear Embedding (LLE)</head><p>Locally Linear Embedding (LLE) belongs to locally manifold learning and it is a nonlinear dimension reduction algorithm <ref type="bibr" target="#b38">[39]</ref>. It is significant to describe the geometry by using the locally neighborhood linear reconstruction and preserve the locally structure of data. LLE assumes that each small neighborhood of data is linear, that is, each object in the neighborhood may be represented by some other objects with weight. For example, there is an object features. Suppose X 2 , X 3 , X 4 , X 5 are four samples from the locally neighborhood of X 1 , then X 1 can be represented as follows:</p><formula xml:id="formula_0">matrix X = {X 1 , X 2 , . . . , X n } ∈ R n *</formula><formula xml:id="formula_1">X 1 = W 2 X 2 + W 3 X 3 + W 4 X 4 + W 5 X 5<label>(1)</label></formula><p>where W = {W 1 , W 2 , . . . , W n } is the weight matrix and the dimension of the matrix is same as that of object matrix X . The ideology of LLE is that this locally linear structure in the locally neighborhood does not change after dimension reduction. Therefore, we obtain the relationship after data mapping and dimension reduction as follows:</p><formula xml:id="formula_2">X ′ 1 = W 2 X ′ 2 + W 3 X ′ 3 + W 4 X ′ 4 + W 5 X ′ 5 (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>where</p><formula xml:id="formula_4">X ′ = {X ′ 1 , X ′ 2 , .</formula><p>. . , X ′ n } is the object matrix after mapping. As we know, the linear structure only affects the relationship nearby the current sample and it does not have an influence on the objects staying away from the sample. The objective function of LLE is partitioned to two parts:</p><formula xml:id="formula_5">J(W ) = n ∑ i=1 ∥X i - k ∑ j=1 W j X j ∥ 2 2 (3) J(X ′ ) = n ∑ i=1 ∥X ′ i - k ∑ j=1 W j X ′ j ∥ 2 2 (4)</formula><p>Eq. ( <ref type="formula">3</ref>) is to obtain the weight matrix W of each object to learn the relationship between the objects and their neighborhoods. And Eq. ( <ref type="formula">4</ref>) exploits the matrix from Eq. ( <ref type="formula">3</ref>) to get the results of dimension reduction, a new object matrix X ′ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Laplacian Eigenmaps (LE)</head><p>Laplacian Eigenmaps (LE) is similar to LLE, and it also utilizes the locality to construct the relationship between objects and preserve the manifold structure of data <ref type="bibr" target="#b39">[40]</ref>. The main idea of LE is that if object i and object j are similar in original space, then they will be very similar after dimension reduction. Assume that there are n objects with m 1 attributes, X = {X 1 , X 2 , . . . , X n } ∈ R n * m 1 is the object matrix, and</p><formula xml:id="formula_6">Y = {Y 1 , Y 2 , . . . , Y i , . . . , Y n } ∈ R n * m 2 is the matrix with m 2 attributes after dimension reduction, i = 1, 2, . . . , n, m 1 &gt; m 2 , W i = {W i,1 , W i,2 , .</formula><p>. . , W i,j , . . . , W i,k } represents the adjacent matrix of the ith object with k samples, W i,j is the jth entry of W i . LE consists of three steps, including graph construction, weight decision and eigenmapping.</p><p>The first step, graph construction, is achieved by some simple methods, such as k-nearest-neighbors (KNN) algorithms. Secondly, LE decides the weight between objects by exploiting the constructed relationship from the first step and using fundamental functions, like thermonuclear function, to get the adjacent matrices. Finally, it computes the eigenvalues and eigenvectors of Laplacian matrix L by utilizing the adjacent matrices from the second step to achieve eigenmapping, where Laplacian matrix</p><formula xml:id="formula_7">L i = D i -W i and D i = ∑ j W ij is the diagonal matrix, W = {W 1 , W 2 , . . . , W n }</formula><p>contains the adjacent matrix of each object. In summary, the objective function of LE is shown as follows:</p><formula xml:id="formula_8">J(Y ) = min ∑ i,j ∥Y i -Y j ∥ 2 W i,j<label>(5)</label></formula><p>The difference between LE and LLE is that LLE uses the neighbors to reconstruct a test sample and it transforms original data from high dimensional space into separable low dimensional space, while LE considers keeping the similar distance from origin space to a new feature space and it makes related samples to be as close as possible in reduced space. They both preserve the locality and achieve dimension reduction effectively, and play a vital role on clustering or classifying high-dimension data. However, neither of them is able to achieve two times of dimension reduction, because the weight matrix or adjacent matrix is not changed and it makes no sense on the second time of dimension reduction. Therefore, we combine the two algorithms to solve multi-task multi-view clustering.</p><p>It is necessary for the two dimension because of the complexity of original data. For multi-task multi-view problems, especially in heterogeneous situation, various tasks and views may have different structures. We explore the first dimension reduction to transform the original data from multiple views into the common intermediate space (view space) and obtain the more separable data structures, and LLE is suitable for this step because it keeps the locally linear structures and makes the data be separable simultaneously; The second dimension reduction is to transform the data from the intermediate space (view space) into task space and extract the shared and complementary features among various tasks. We want to obtain the samples that are relate to each other to be close such that sufficient shared and complementary features are learned, and LE is more appropriate for this step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>For multi-task multi-view clustering, we usually consider that there is similar relationship between several views of each task. However, it is difficult to directly learn knowledge through the original data. In order to facilitate information extracting, we collectively learn the feature transformation matrices for all views from each task by LLE. Besides, some similar features exist in multiple tasks, we want to keep the distances between tasks in the process of mapping and solve the multi-task problems by LE. In this section, we introduce the detail of our proposed method, L3E-M2VC.</p><p>As previous mentioned, LLE and LE are for learning the structures in views of each task and multiple tasks, respectively. Therefore, the process of multi-task multi-view clustering for knowledge sharing is partitioned to two steps. First step is mapping the views of each task to a common space called view space. Second is extracting the features in multiple tasks and mapping them to a discriminant space called task space, which is shown in Fig. <ref type="figure" target="#fig_0">1</ref>.</p><p>Firstly, through the first transformation step, the data samples of the vth view from the tth task in original space is transformed into the view space, which just makes samples be more separable and each transformation matrix is dependent on various tasks and views. The view space can be seemed as a common intermediate latent space that is shared by all the views from each task <ref type="bibr" target="#b37">[38]</ref>.</p><p>Then through the transformation in the second step, the sample is mapped from view space to task space. Assume the second transformation matrix is R t , which consists of two parts, including the shared feature R which is common in multiple tasks, and the complementary feature Rc t that belongs to only one task. Of course, the second transformation step for the different views in the same task is identical, because they are mapped to the same space in the first step. In other words, the view space offers a common plane for various views in each task which uniforms the features in different views; then the task space considers the shared and complementary features in multiple tasks in order to facilitate knowledge sharing in multi-task multi-view clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem definition</head><p>Assume that there are T clustering tasks with V t views, and  mapped samples in task space Z v t are similar to X v t . Besides, some notations is shown as follows.</p><formula xml:id="formula_9">X v t = {x v t,1 , x v t,2 , . . . , x v t,i , . . . , x v t,nt } ∈ R nt * d v t , i = 1,</formula><p>Notations. Let tr(X ) be the trace of matrix X and the inverse of X is X -1 , and I l1 is the l 1 * l 1 identity matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Knowledge transformation in multi-task multi-view clustering</head><p>In order to achieve that mapping the original data into the view space by LLE, at first we need to define a weight matrix W v t to reconstruct the samples. Hence, the relationship matrix W v t is formulated as follows:</p><formula xml:id="formula_10">ξ (W v * t ) = min ∑ i ∥X v t,i - ∑ j W v t,ij X v t,j ∥ 2 = min ∑ i W v t,i T E v t,i W v t,i s.t. K ∑ j=1 W v t,ij = 1.<label>(6)</label></formula><p>where,</p><formula xml:id="formula_11">E v t,i = (X v t,i -X v t,j ) T (X v t,i -X v t,j ), W v t,i represents the weight between X v</formula><p>t,i and the samples in its neighborhood, K is the number of neighbors, and</p><formula xml:id="formula_12">∑ K j=1 W v t,ij = 1 represents the normalization of the weight matrix.</formula><p>After obtaining the relationship matrix W v t for each view of each task, we map the sample data from origin space to the view space. It can be explicitly described as follows:</p><formula xml:id="formula_13">ξ (Y v * t ) = min ∑ i ∥Y v t,i - ∑ j W v t,i Y v t,j ∥ 2 = min tr(Y v t T M v t Y v t ) s.t. nt ∑ i=1 Y v t,i = 0, 1 n t nt ∑ i=1 Y i Y i T = I nt .<label>(7)</label></formula><p>where,</p><formula xml:id="formula_14">M v t = (I v t -W v t ) T (I v t -W v t ),<label>and</label></formula><formula xml:id="formula_15">I v t is the unit matrix. Similarly, the ∑ nt i=1 Y v t,i = 0 and 1 nt ∑ nt i=1 Y i Y i T = I nt is for data standardization. It is noted that the dimension of Y t v is changed from d v t to d 1 ,</formula><p>which represents eigenvectors corresponding to the top d 1 minimum eigenvalues. Now we obtain the result of transformation from origin space to view space Y v t and different views in the same task are in a common plain. Finally, we map the data from the view space to the task space by learning the shared and complementary features in different tasks to cluster samples. We assume that</p><formula xml:id="formula_16">Z v t = Y v t * R t , it is significant to get the transformation matrix R t = [Rs, Rc t ],</formula><p>where Rs is the shared transformation matrix among tasks and Rc t is the complementary matrix depending on various views. The formula is as follows:</p><formula xml:id="formula_17">ξ (Rs, Rc t ) = min ∑ t ∑ v ∑ i ∑ j ∥Z v t,i -Z v t,j ∥ 2 W v t,ij = min ∑ t ∑ v 2tr([Rs, Rc t ] T Y v t T L(Y v t [Rs, Rc t ])) s.t.Z v t T DZ v t = I nt , R t T R t = I d 2 , Rs T Rs = I d 2 ′ , Rc t T Rc t = I d 2 -d 2 ′ . (<label>8</label></formula><formula xml:id="formula_18">)</formula><p>where, L = D -W represents the Laplacian matrix, and D is the</p><formula xml:id="formula_19">degree matrix of W . Z v t T DZ v t = I nt is to ensure that there is a solution, and R t T R t = I d 2 , Rs T Rs = I d 2 ′ , Rc t T Rc t = I d 2 -d 2 ′ illustrate</formula><p>that the final dimension of sample data is changed and the mapped features consist of shared and complementary parts. The detailed derivation of Eqs. ( <ref type="formula" target="#formula_10">6</ref>)-( <ref type="formula" target="#formula_17">8</ref>) has been putted into supplementary material.</p><p>It is interesting to discover that the adjacent matrix in second step is not computed. Actually, the adjacent matrix is same as the weight matrix in the first transformation step. This is because we consider that utilizing the relationship of neighborhood as the graph matrix constructed by LE and preserving the stable distance and local dependency in the processes of view mapping and task mapping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Multi-task multi-view clustering in tasks space</head><p>Through the transformations of view space and task space, a low-dimensional representation of samples is obtained. It is worth noting that we get the representations of each view from each task. Indeed, samples in each view from the same task are transformed by the same matrix R t = [Rs, Rc t ], because of the independence of the task space for different views. Therefore, in order to eliminate the noise and combine the information in different views, we average these representation on different views related to the same task. That is, the final representation for a specific task is </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Optimization procedure</head><p>In this section, we present the solving procedure related to W v * t in Eq. ( <ref type="formula" target="#formula_10">6</ref>) and Y v t in Eq. ( <ref type="formula" target="#formula_13">7</ref>), and explore a iterative solution method to optimization Rs and Rc t in Eq. ( <ref type="formula" target="#formula_17">8</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Computation of the weight matrix</head><formula xml:id="formula_20">W v t ξ (W v * t ) = min ∑ i W v t,i T E v t,i W v t,i s.t. K ∑ j=1 W v t,ij = 1.<label>(9)</label></formula><p>Firstly, we transform the constraint requirement to a form of matrix as follows:</p><formula xml:id="formula_21">K ∑ j=1 W v t,ij = W v t,i I K = I K (<label>10</label></formula><formula xml:id="formula_22">)</formula><p>Then we apply the lagrangian multiplier method to construct an optimization objective for W v t , compute the derivation of W v t , and obtain the W v t . The procedure is shown as follows:</p><formula xml:id="formula_23">L(W v t ) = ∑ i W v t,i T E v t,i W v t,i + λ 1 (W v t,i I K -I K ) ∂L ∂W v t,i = 2E v t,i W v t,i + λ 1 I K 2E v t,i W v t,i + λ 1 I K = 0 W v t,i = λ 1 ′ E v t,i -1 I K (11)</formula><p>where, λ 1 ′ = -1 2 λ 1 . The normalization result is as follows:</p><formula xml:id="formula_24">W v t,i = E v t,i -1 I K I K T E v t,i -1 I K (<label>12</label></formula><formula xml:id="formula_25">)</formula><p>• Computation of the result in view space</p><formula xml:id="formula_26">Y v t ξ (Y v * t ) = min tr(Y v t T M v t Y v t ) s.t. nt ∑ i=1 Y v t,i = 0, 1 n t nt ∑ i=1 Y i Y i T = I nt .<label>(13)</label></formula><p>According to the Eq. ( <ref type="formula" target="#formula_26">13</ref>), we construct the optimization objective as the steps of computing the W v t . The detailed process is described as follows:</p><formula xml:id="formula_27">L(Y v t ) = tr(Y v t T M v t Y v t ) + λ 2 (Y v t T Y v t -n t I) 2M v t Y v t + 2λ 2 Y v t = 0 M v t Y v t = λ ′ 2 Y v t (<label>14</label></formula><formula xml:id="formula_28">)</formula><p>where, λ ′ 2 = -λ 2 . Now we choose the first d 1 eigenvectors corresponding to the top d 1 minimum eigenvalues directly as the mapped result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Computation of Rs with fixed Rc t</head><p>As for matrices relating to the shared and complementary features of tasks, Rs and Rc t , we exploit alternately iteration methods. That is, we initial the Rs and Rc t , then fix one to compute another, and do not stop until the two matrices are stable, which means they both are not changed. Hence, we firstly optimize the shared feature matrix Rs with the fixed Rc t .</p><formula xml:id="formula_29">J(Rs) = min ∑ t ∑ v 2tr([Rs, Rc t ] T Y v t T L(Y v t [Rs, Rc t ])) = min tr( ∑ t ∑ v Rs T Y v t T L(Y v t Rs)) + a * d 2 ′ s.t.Rs T Rs = I d 2 ′ (<label>15</label></formula><formula xml:id="formula_30">)</formula><p>where a is written as follows:</p><formula xml:id="formula_31">a = 1 d 2 ′ tr( ∑ t ∑ v Rc t T Y v t T LY v t Rc t )<label>(16)</label></formula><p>Therefore, the objective function is described as:</p><formula xml:id="formula_32">J(Rs) = min tr(Rs T VRs)<label>(17)</label></formula><p>where,</p><formula xml:id="formula_33">V = ∑ t ∑ v Y v t T LY v t + aI nt (18)</formula><p>Similarly, we take use of lagrangian multiplier method to solve Rs:</p><formula xml:id="formula_34">∂J(Rs) ∂Rs = ∂tr(Rs T VRs) + λ 3 (Rs T Rs -I d 2 ′ ) ∂Rs = 2VRs + 2λ 3 Rs (19) Then, 2VRs + 2λ 3 Rs = 0 VRs = -λ 3 Rs (20)</formula><p>Now the Rs is obtained by outputting the eigenvectors corresponding to the smallest d 2 ′ eigenvalues. However, the Rs may not be the final result of the shared features in multiple tasks, we need to solve the Rc t to compute the Rs iteratively till the results of them get stable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Computation of Rc t with fixed Rs</head><p>The Computation of the complementary matrix, Rc t is similar to Rs. It is noted that Rs is same in all tasks but Rc t is different among different tasks. When the Rs is fixed, the optimization problem of Eq. ( <ref type="formula" target="#formula_17">8</ref>) becomes:</p><formula xml:id="formula_35">J(Rc t ) = min ∑ t ∑ v 2tr([Rs, Rc t ] T Y v t T L(Y v t [Rs, Rc t ])) = min tr(Rc t T ( ∑ v Y v t T LY v t Rc t ) + b t (d 2 -d 2 ′ ))<label>(21)</label></formula><p>where b t is written as:</p><formula xml:id="formula_36">b t = 1 d 2 -d 2 ′ (tr(Rs T ∑ t ∑ v (Y v t T LY v t )Rs) +tr( T ∑ i=1,i̸ =t Rc i T ( ∑ v Y v i T LY v i )Rc i ))<label>(22)</label></formula><p>The objective function becomes:</p><formula xml:id="formula_37">J(Rc t ) = min tr(Rc T t P t Rc t )<label>(23)</label></formula><p>where,</p><formula xml:id="formula_38">P t = ∑ v Y v t T LY v t + b t I nt (24)</formula><p>Finally, computing the Rc t by lagrangian multiplier algorithm as follows:</p><p>∂J(Rc t ) ∂Rc t = ∂tr(Rc In summary, the optimization procedure computes the mapping result of view space, and iteratively solves the shared and complementary features matrix of multiple tasks until convergence. It is significantly could produce the mapping of view space and task space. Also, decreases the computation complexity of multi-view multi-task clustering problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Time complexity analysis</head><p>The solving process of Rc t and Rs is time-consuming. To achieve better performances in these processes, we design to decrease overlapping steps when solving the Rc t and Rs as much as possible.</p><p>Therefore, the time complexity of optimization is O(iter * (T * (T + V ))). iter is the number of iteration, T is the number of tasks and V is the number that of views. The global time-consuming will be discussed in the following Experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we evaluate our proposed method L3E-M2VC on several multi-task multi-view data sets and compare it with 11 related methods. All experiments are compiled and tested on a Windows server of following specification: (CPU: Intel(R) Xeon(R) CPU E5-2620 v4 @ 2.10 GHz).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Data sets</head><p>We use 5 multi-task multi-view data sets on our experiments.</p><p>Leaves 1 : The Leaves data set consists of 100 species of leaves and each species has 16 different samples. There are 3 views in each sample, including margin, shape and texture. We construct 3 heterogeneous tasks by choosing 18 different species of leaves.</p><p>WebKB 2 : The WebKB data set is a collection of web pages in 4 universities: Cornell, Texas, Washington, Wisconsin. There are 3 views including contents, the hyper-links pointing to the web pages, and the words in the titles. Obviously, the data set forms 4 tasks to cluster web pages in each university, respectively. Mfeat 3 : The Mfeat data set consists of features of handwritten numerals ('0'-'9') extracted from a collection of Dutch utility maps. 200 patterns per class (for a total of 2000 patterns) have been digitized in binary images. We separate evens and odds to form 2 task and choose 5 attributes to construct 5 views, including Fourier coefficients of the character shapes, profile correlations, Karhunen-Love coefficients, pixel and morphological features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>COIL-ORL:</head><p>The COIL-ORL data set contains 3 tasks with 4 views. It comprises of COIL20 4 and ORL 5 data sets. The COIL20 contains 20 objects with 72 images respectively, and it has 1024 attributes. Therefore, we cut it off to form 2 tasks with 10 objects and obtain 4 views with 256 attributes. The third task is the ORL data set that includes 10 different images of each of 40 distinct subjects. Similarly, we separate its 1024 attributes to form 4 views. NUS-WIDE 6 : The NUS-WIDE data set contains 269648 images that belong to 81 concepts. We select 20 concepts to form 4 tasks. Each task includes 7 views: color histogram, color correlogram, edge direction histogram, wavelet texture, block-wise color moments, SIFT descriptions based words, and tags (see Table <ref type="table" target="#tab_4">1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Baseline algorithms</head><p>In order to show the performance of our proposed L3E-M2VC method, we compare it with 11 algorithms including:</p><p>(1) single-task single-view clustering algorithms: K-Means <ref type="bibr" target="#b40">[41]</ref> and kernel K-Means (KK-Means) <ref type="bibr" target="#b41">[42]</ref>;</p><p>(2) multi-view clustering algorithms: Co-Regularized multiview spectral clustering algorithm (CoRe) <ref type="bibr" target="#b42">[43]</ref> and our method without the consideration of multi-task, L3E-M2VC-MV;</p><p>(3) multi-task clustering algorithms: the shared subspace learning multi-task clustering algorithm (LSSMTC) <ref type="bibr" target="#b43">[44]</ref> and our method without the consideration of multi-view, L3E-M2VC-MT;</p><p>(4) multi-task multi-view clustering methods: the Bipartite graph based multi-task multi-view clustering algorithm (BMT-MVC) and the semi-nonnegative matrix tri-factorization based multi-task multi-view clustering algorithm (SMTMVC) <ref type="bibr" target="#b33">[34]</ref>.</p><p>(5) variants of our method: We design 3 variant methods from our proposed method. Firstly, we transform the dimension reduction order of LLE and LE to form the variant of our method (LE-LLE-M2VC), which obtains the view space by LE and task space by LLE; secondly, we only use the first dimension reduction by LLE and merge all views to form the second variant of our method (LLE-M2VC); thirdly, we directly use the second dimension reduction by LE to form the third variant of our method (LE-M2VC).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Experiment evaluation and parameter setting</head><p>In our experiments, we select the Rand Index (RI) <ref type="bibr" target="#b44">[45]</ref> and Cluster Accuracy (CA) <ref type="bibr" target="#b45">[46]</ref> as the evaluation indicators of clustering performance. RI evaluates the purity of each cluster and the larger value corresponds to the better clustering accuracy. CA measures the performance of clustering algorithms directly and the larger value of CA relates to the better method.</p><p>The parameters of our L3E-M2VC contain the reduced dimension of the first transition from original space to view space r1, two dimension parameters of the second transition from view space to task space, r2 and r3, and the number of neighborhoods. In general, the large number of neighbors obtains more information but increases time-consuming. For the data sets with small number of samples, we can choose more neighbors; While for big data sets, it is not available. Thus, we test our method in three different scale    From Fig. <ref type="figure" target="#fig_1">3</ref>, the insensitivity of our method about r1 is demonstrated. Thus, we choose a moderate value as r1. It is noted that too small value will impact the performance of following dimension reduction and too large value will cause high time-consuming. Figs. <ref type="figure">4</ref><ref type="figure">5</ref><ref type="figure" target="#fig_3">6</ref>show that the change of r2 and r3 has little effect on results in most cases. However, when the sum of r2 and r3 is equal to 1, the result is not stable. Therefore, we also set r2 and r3 as moderate values, such as r2 = 0.3 and r3 = 0.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Experimental results and analysis</head><p>Experimental results about RI and CA are shown in Tables 2, 3, 4, 5, and 6. Due to the baseline algorithm, BMTMVC, is only for nonnegative data, the data set NUS-WIDE is not ran on this method.</p><p>(1) L3E-M2VC, BMTMVC and SMTMVC outperform other algorithms in most situations. It is significant to guarantee that multi-task multi-view methods deal with the problems containing related tasks and views better than single-task single-view methods, multi-task methods or multi-view methods. Also, it illustrates that multi-task multi-view clustering methods exploit more useful information in samples and extract more complete structure of data to achieve more excellent performance.</p><p>(2) L3E-M2VC is better than the other multi-task multi-view algorithms, BMTMVC and SMTMVC, in an average level. Due to L3E-M2VC directly utilizes the multi-view feature in each of tasks and the shared and complementary features of multiple tasks, while BMTMVC and SMTMVC consider the individual multi-view relationship, individual multi-task relationship and task-view relationship, it ignores that the features in specific task or view should not be as that of other tasks or views and disturbs the performance of multi-task multi-view clustering.</p><p>(3) It is worth noting that L3E-M2VC outperforms the L3E-M2VC-MV and L3E-M2VC-MT. It shows that extracting the features in multiple tasks and view simultaneously assists to improve the performance of algorithms for multi-task multi-view problems and the structures from tasks and view both are important in data mining.</p><p>(4) In most cases, the performance of L3E-M2VC-MV is better than that of L3E-M2VC-MT. It is noted that the 5 data sets are in heterogeneous task situations, therefore, there is a stronger relationship between multiple views in the same tasks. Simultaneously, the better performance of L3E-M2VC-MV than that of single-task multi-view clustering algorithm CoRe, shows that the good ability to process multi-view problems, and illustrates the effectiveness in other hand.</p><p>(5) Comparing with the variants of our method, the performance of L3E-M2VC is superior to that of them. LLE-M2VC just uses the first dimension reduction while it does not consider the shared and complementary structure in original data. It is worth noting that the second dimension reduction is significant for multitask multi-view clustering. As for LE-M2VC, it directly learns the shared and complementary knowledge from original data, but neglects the complex structures and inseparability of original data. For LE-LLE-M2VC, the proposed method has different orders of dimension reduction. It is noted that we want to obtain the more separable data structures in the first dimension reduction and LLE is more suitable due to it keeps the locally linear structures and makes the data be separable simultaneously. And in the second dimension reduction, LE is more appropriate to make samples that are relate to each other to be close such that sufficient shared and complementary features are learned. That is, the first dimension is the foundation of the second dimension reduction, and the second dimension reduction is to extract significant knowledge.</p><p>Besides, we compare the running time with other two multitask multi-view clustering algorithms as shown in Table <ref type="table" target="#tab_13">7</ref>. It is noted that our proposed method is faster than the two existing algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions and future work</head><p>In this paper, we have proposed a multi-task multi-view clustering algorithm which is suitable for heterogeneous situations. It integrates the advantages of LLE and LE and considers that different the consistency and individual divergency. Finally, experimental results on 5 data sets prove the superiority of the proposed method over 11 baseline algorithms.</p><p>In the future work, we consider to extract the shared and complementary features from multiple views to assist clustering. Also, because some tasks may miss data in some views while others not, we want to exploit the multi-task multi-view clustering problems to process the missing data. Further more, semi-supervised multitask multi-view clustering should be extended.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The feature transformation process of L3E-M2VC.</figDesc><graphic coords="4,124.43,55.32,336.64,195.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The performance of the first dimension parameter r1 about RI on 5 data sets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>Fig. 4. Visualization result of the effect of two reduced dimensions r2 and r3 from task space about CA on Leaves. data sets (WebKB, Mfeat and NUS-WIDE) about various number, neighbor number = {0.1, 0.2, . . . , 1} * neighbor rate as Fig. 2. The results from Fig. 2 demonstrate the insensitivity of neighbor number in our method. Hence, we set the neighbor number as 15 for all data sets;As for the reduced dimensions of two transformations, we observe that the different dimensions affect the performance of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Visualization result of the effect of two reduced dimensions r2 and r3 from task space about CA on NUS-WIDE. results of the effect of two reduced dimensions r2 and r3 from task space about CA on 3 different scale data sets (Leaves, COIL-ORL and NUS-WIDE) are shown as Figs. 4-6.From Fig.3, the insensitivity of our method about r1 is demonstrated. Thus, we choose a moderate value as r1. It is noted that too small value will impact the performance of following dimension reduction and too large value will cause high time-consuming. Figs.4-6show that the change of r2 and r3 has little effect on results in most cases. However, when the sum of r2 and r3 is equal to 1, the result is not stable. Therefore, we also set r2 and r3 as</figDesc><graphic coords="8,76.24,229.44,438.80,73.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>d with n objects and each has d Please cite this article in press as: Y. Zhang, et al., A multitask multiview clustering algorithm in heterogeneous situations based on LLE and LE, Knowledge-Based Systems (2018), https://doi.org/10.1016/j.knosys.2018.10.001.</figDesc><table><row><cell>Y. Zhang et al. / Knowledge-Based Systems (</cell><cell>)</cell><cell>-</cell><cell>3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>2, . . . , n t , t = 1, 2, . . . , T , and v = 1, 2, . . . , V t , where n t represents the number of objects in tth task, and d v t is the dimension of feature of the vth view in the tth task. Obviously, each task is clustered to c t classes, and it is different for various tasks about the label sets because of the heterogeneity. The structures of data in view space Y v</figDesc><table /><note><p>t and</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>2P t Rc t + 2λ 4 Rc t Rc t + 2λ 4 Rc t = 0 P t Rc t = -λ 4 Rc t</figDesc><table><row><cell>Then,</cell></row><row><cell>2P t (26)</cell></row><row><cell>(25)</cell></row></table><note><p>T t P t Rc t ) + λ 4 (Rc T t Rc t -I l 2) ∂Rc t =</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1</head><label>1</label><figDesc>Constitutions of data sets in experiments.</figDesc><table><row><cell></cell><cell></cell><cell>Leaves</cell><cell>WebKB</cell><cell>Mfeat</cell><cell>COIL-ORL</cell><cell>NUS-WIDE</cell></row><row><cell></cell><cell>Views</cell><cell>3</cell><cell>3</cell><cell>5</cell><cell>4</cell><cell>7</cell></row><row><cell>Task 1</cell><cell>Classes</cell><cell>6</cell><cell>4</cell><cell>5</cell><cell>10</cell><cell>5</cell></row><row><cell></cell><cell>Samples</cell><cell>96</cell><cell>226</cell><cell>1000</cell><cell>720</cell><cell>4163</cell></row><row><cell></cell><cell>Views</cell><cell>3</cell><cell>3</cell><cell>5</cell><cell>4</cell><cell>7</cell></row><row><cell>Task 2</cell><cell>Classes</cell><cell>6</cell><cell>4</cell><cell>5</cell><cell>10</cell><cell>5</cell></row><row><cell></cell><cell>Samples</cell><cell>96</cell><cell>252</cell><cell>1000</cell><cell>720</cell><cell>3914</cell></row><row><cell></cell><cell>Views</cell><cell>3</cell><cell>3</cell><cell>-</cell><cell>4</cell><cell>7</cell></row><row><cell>Task 3</cell><cell>Classes</cell><cell>6</cell><cell>4</cell><cell>-</cell><cell>40</cell><cell>5</cell></row><row><cell></cell><cell>Samples</cell><cell>96</cell><cell>255</cell><cell>-</cell><cell>400</cell><cell>3689</cell></row><row><cell></cell><cell>Views</cell><cell>-</cell><cell>3</cell><cell>-</cell><cell>-</cell><cell>7</cell></row><row><cell>Task 4</cell><cell>Classes</cell><cell>-</cell><cell>4</cell><cell>-</cell><cell>-</cell><cell>5</cell></row><row><cell></cell><cell>Samples</cell><cell>-</cell><cell>307</cell><cell>-</cell><cell>-</cell><cell>3615</cell></row></table><note><p>1 https://archive.ics.uci.edu/ml/datasets/One-hundred+plant+species+leaves+ data+set. 2 http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/. 3 https://archive.ics.uci.edu/ml/datasets/Multiple+Features. 4 http://www.cad.zju.edu.cn/home/dengcai/Data/MLData.html. 5 http://www.cad.zju.edu.cn/home/dengcai/Data/FaceData.html.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2</head><label>2</label><figDesc>The RI and CA (avg ± std%) of each task on Leaves data sets. ± 7.38 67.39 ± 11.79 58.47 ± 10.29 66.88 ± 13.68 59.35 ± 4.49 71.31 ± 3.78 59.74 ± 7.39 68.53 ± 9.75 KK-Means 60.13 ± 4.32 74.18 ± 4.89 61.56 ± 5.76 75.99 ± 6.78 58.27 ± 7.17 72.62 ± 8.95 59.99 ± 5.75 74.26 ± 6.87 CoRe 69.11 ± 7.88 85.98 ± 9.05 70.13 ± 7.92 87.67 ± 11.42 69.92 ± 7.62 85.98 ± 11.23 69.72 ± 7.81 86.54 ± 10.57</figDesc><table><row><cell>Methods</cell><cell>Taks1</cell><cell></cell><cell>Taks2</cell><cell></cell><cell>Taks3</cell><cell></cell><cell>Average</cell></row><row><cell></cell><cell>RI</cell><cell>CA</cell><cell>RI</cell><cell>CA</cell><cell>RI</cell><cell>CA</cell><cell>RI</cell><cell>CA</cell></row><row><cell cols="3">K-Means 61.39 L3E-M2VC-MV 94.23 ± 1.91 92.54 ± 1.28</cell><cell>94.27 ± 2.47</cell><cell>93.24 ± 1.77</cell><cell cols="2">93.88 ± 2.32 94.66 ± 1.82</cell><cell cols="2">94.13 ± 2.23 93.48 ± 1.62</cell></row><row><cell>LSSMTC</cell><cell cols="2">65.98 ± 6.90 72.56 ± 7.12</cell><cell>60.12 ± 5.55</cell><cell>67.56 ± 4.51</cell><cell cols="4">68.32 ± 8.11 76.45 ± 10.79 64.81 ± 6.85 72.19 ± 7.47</cell></row><row><cell>L3E-M2VC-MT</cell><cell cols="2">93.67 ± 2.79 91.49 ± 2.01</cell><cell>93.58 ± 3.21</cell><cell>93.01 ± 2.35</cell><cell cols="2">92.79 ± 2.39 93.37 ± 1.68</cell><cell cols="2">93.35 ± 2.80 92.62 ± 2.01</cell></row><row><cell>BMTMVC</cell><cell cols="2">92.54 ± 2.59 96.45 ± 0.15</cell><cell>94.14 ± 3.67</cell><cell>93.23 ± 1.78</cell><cell cols="2">93.98 ± 2.89 98.45 ± 0.24</cell><cell cols="2">93.55 ± 3.05 96.04 ± 1.45</cell></row><row><cell>SMTMVC</cell><cell cols="2">91.01 ± 3.66 93.33 ± 2.17</cell><cell>94.23 ± 3.46</cell><cell>97.89 ± 1.99</cell><cell cols="2">94.12 ± 3.77 97.23 ± 0.98</cell><cell cols="2">93.12 ± 3.63 96.15 ± 1.72</cell></row><row><cell>LLE-M2VC</cell><cell cols="2">90.19 ± 4.54 83.21 ± 6.77</cell><cell>88.64 ± 4.62</cell><cell>68.48 ± 7.98</cell><cell cols="2">91.74 ± 5.02 85.45 ± 7.99</cell><cell cols="2">90.19 ± 4.73 79.05 ± 7.58</cell></row><row><cell>LE-M2VC</cell><cell cols="3">84.65 ± 6.77 68.54 ± 10.13 90.11 ± 6.89</cell><cell cols="3">62.71 ± 11.23 85.39 ± 6.73 78.98 ± 9.10</cell><cell cols="2">86.72 ± 6.80 70.08 ± 10.15</cell></row><row><cell>LE-LLE-M2VC</cell><cell cols="2">90.33 ± 4.78 91.69 ± 5.69</cell><cell>88.98 ± 4.29</cell><cell>90.01 ± 5.44</cell><cell cols="2">87.43 ± 6.55 90.67 ± 4.93</cell><cell cols="2">88.91 ± 5.21 90.79 ± 5.35</cell></row><row><cell>L3E-M2VC</cell><cell cols="2">96.25 ± 2.68 97.98 ± 1.02</cell><cell>95.04 ± 3.01</cell><cell>98.24 ± 1.45</cell><cell cols="2">95.15 ± 2.11 98.76 ± 1.02</cell><cell cols="2">95.48 ± 2.60 98.33 ± 1.17</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3</head><label>3</label><figDesc>The RI and CA (avg ± std%) of each task on WebKB data sets. ± 3.44 61.39 ± 1.98 44.12 ± 6.54 52.08 ± 8.97 41.23 ± 6.07 52.<ref type="bibr" target="#b21">22</ref> ± 3.44 50.22 ± 6.31 61.18 ± 5.49 46.99 ± 5.59 56.72 ± 4.97 KK-Means 54.14 ± 3.47 48.28 ± 2.36 40.76 ± 5.24 44.78 ± 3.57 41.36 ± 2.11 46.12 ± 1.33 48.29 ± 1.23 56.12 ± 0.78 46.14 ± 3.01 48.83 ± 2.01 CoRe 60.04 ± 1.31 75.46 ± 0.35 59.34 ± 1.45 68.55 ± 0.13 58.99 ± 1.02 67.12 ± 0.22 60.47 ± 1.26 69.98 ± 0.25 59.51 ± 1.26 70.28 ± 0.24 L3E-M2VC-MV 91.33 ± 1.48 81.63 ± 2.52 84.35 ± 2.08 78.42 ± 2.18 89.74 ± 2.17 81.65 ± 2.01 90.62 ± 3.21 79.49 ± 3.12 89.01 ± 2.24 80.30 ± 2.46 LSSMTC 83.47 ± 7.46 62.79 ± 8.33 81.23 ± 7.54 61.58 ± 5.67 79.77 ± 5.38 58.45 ± 6.33 82.31 ± 7.03 66.24 ± 9.82 81.70 ± 6.85 62.27 ± 7.54 L3E-M2VC-MT 90.27 ± 3.26 79.32 ± 2.34 82.11 ± 2.67 73.23 ± 2.55 84.36 ± 3.44 80.24 ± 2.98 85.78 ± 4.22 77.12 ± 3.41 85.63 ± 3.40 74.48 ± 2.82 BMTMVC 90.25 ± 4.57 83.21 ± 3.54 83.28 ± 4.15 79.12 ± 3.45 85.37 ± 4.25 83.39 ± 1.45 87.35 ± 3.09 85.53 ± 1.16 86.56 ± 4.02 82.81 ± 2.40 SMTMVC 91.74 ± 3.88 80.59 ± 2.12 82.11 ± 3.24 73.52 ± 1.37 86.34 ± 3.45 85.24 ± 2.87 87.89 ± 4.38 76.89 ± 5.22 87.02 ± 3.74 79.06 ± 2.90 LLE-M2VC 75.92 ± 5.49 63.33 ± 6.76 68.48 ± 6.67 69.56 ± 7.68 63.94 ± 7.43 57.39 ± 6.75 64.08 ± 7.89 60.85 ± 7.89 68.11 ± 6.87 62.78 ± 7.27 LE-M2VC 72.86 ± 4.64 60.26 ± 6.73 67.28 ± 5.15 65.61 ± 6.22 64.24 ± 5.07 56.96 ± 5.98 61.11 ± 4.78 63.58 ± 5.87 66.37 ± 4.91 61.60 ± 6.20 LE-LLE-M2VC 90.22 ± 1.79 81.96 ± 2.33 83.11 ± 1.23 78.04 ± 1.89 90.67 ± 1.87 86.32 ± 2.04 92.13 ± 2.46 84.44 ± 2.65 89.03 ± 1.84 82.69 ± 2.23</figDesc><table><row><cell>Methods</cell><cell>Taks1</cell><cell></cell><cell>Taks2</cell><cell></cell><cell>Taks3</cell><cell></cell><cell>Taks4</cell><cell></cell><cell>Average</cell></row><row><cell></cell><cell>RI</cell><cell>CA</cell><cell>RI</cell><cell>CA</cell><cell>RI</cell><cell>CA</cell><cell>RI</cell><cell>CA</cell><cell>RI</cell><cell>CA</cell></row><row><cell cols="2">K-Means 52.40 L3E-M2VC 93.59 ± 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>.36 85.47 ± 2.01 87.19 ± 1.45 82.88 ± 1.23 92.79 ± 1.13 88.41 ± 1.68 94.46 ± 2.10 87.36 ± 1.28 92.01 ± 1.51 86.03 ± 1.55Table 4</head><label>4</label><figDesc>The RI and CA (avg ± std%) of each task on Mfeat data sets.</figDesc><table><row><cell>Methods</cell><cell>Taks1</cell><cell></cell><cell>Taks2</cell><cell></cell><cell>Average</cell></row><row><cell></cell><cell>RI</cell><cell>CA</cell><cell>RI</cell><cell>CA</cell><cell>RI</cell><cell>CA</cell></row><row><cell>K-Means</cell><cell cols="6">57.26 ± 11.28 69.54 ± 12.52 48.67 ± 9.06 69.55 ± 10.63 52.97 ± 10.17 69.55 ± 11.58</cell></row><row><cell>KK-Means</cell><cell>56.78 ± 9.27</cell><cell>66.43 ± 7.79</cell><cell cols="2">49.38 ± 9.02 68.46 ± 8.78</cell><cell>53.08 ± 9.15</cell><cell>67.50 ± 8.29</cell></row><row><cell>CoRe</cell><cell>86.47 ± 3.53</cell><cell>89.75 ± 4.32</cell><cell cols="2">79.38 ± 3.66 90.22 ± 4.28</cell><cell>82.93 ± 3.60</cell><cell>89.99 ± 4.30</cell></row><row><cell cols="2">L3E-M2VC-MV 91.03 ± 6.21</cell><cell>92.45 ± 3.21</cell><cell cols="2">83.53 ± 5.22 91.72 ± 4.27</cell><cell>87.28 ± 5.72</cell><cell>90.09 ± 3.74</cell></row><row><cell>LSSMTC</cell><cell>88.23 ± 7.77</cell><cell>90.01 ± 6.48</cell><cell cols="2">85.12 ± 6.37 87.26 ± 7.68</cell><cell>86.68 ± 7.07</cell><cell>88.64 ± 7.08</cell></row><row><cell cols="2">L3E-M2VC-MT 89.67 ± 4.77</cell><cell>92.11 ± 5.62</cell><cell cols="2">81.75 ± 4.35 90.37 ± 4.97</cell><cell>85.71 ± 4.56</cell><cell>91.24 ± 5.30</cell></row><row><cell>BMTMVC</cell><cell>91.86 ± 4.76</cell><cell>93.22 ± 5.79</cell><cell cols="2">86.33 ± 5.89 94.08 ± 4.97</cell><cell>89.10 ± 5.33</cell><cell>93.65 ± 5.38</cell></row><row><cell>SMTMVC</cell><cell>92.11 ± 5.12</cell><cell>91.07 ± 3.63</cell><cell cols="2">84.37 ± 4.23 90.08 ± 5.46</cell><cell>88.24 ± 4.68</cell><cell>90.58 ± 4.55</cell></row><row><cell>LLE-M2VC</cell><cell>83.69 ± 6.53</cell><cell>74.30 ± 6.55</cell><cell cols="2">79.78 ± 7.11 72.70 ± 5.63</cell><cell>81.74 ± 6.82</cell><cell>73.50 ± 6.09</cell></row><row><cell>LE-M2VC</cell><cell>80.44 ± 4.25</cell><cell>68.20 ± 4.32</cell><cell cols="2">76.09 ± 4.74 68.50 ± 5.92</cell><cell>78.27 ± 4.50</cell><cell>68.35 ± 5.12</cell></row><row><cell>LE-LLE-M2VC</cell><cell>91.76 ± 3.98</cell><cell>91.55 ± 5.43</cell><cell cols="2">86.24 ± 3.98 92.14 ± 3.99</cell><cell>89.00 ± 3.98</cell><cell>91.85 ± 4.71</cell></row><row><cell>L3E-M2VC</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>94.68 ± 4.05 94.90 ± 5.23 88.98 ± 3.60 93</head><label></label><figDesc>.40 ± 4.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>05 91.83 ± 3.83 94.15 ± 4.64Table 5</head><label>5</label><figDesc>The RI and CA (avg ± std%) of each task on COIL-ORL data sets. 11.<ref type="bibr" target="#b22">23</ref> 72.57 ± 9.67 69.37 ± 10.33 74.85 ± 9.78 67.23 ± 9.18 61.23 ± 10.25 67.65 ± 10.25 69.55 ± 9.9 ± 3.25 93.23 ± 2.31 93.48 ± 3.54 94.27 ± 2.38 84.19 ± 3.33 91.58 ± 2.28 88.78 ± 3.37 LSSMTC 81.34 ± 8.37 83.47 ± 6.45 83.29 ± 7.34 85.22 ± 6.68 85.57 ± 7.38 80.17 ± 6.42 83.4 ± 7.70 82.95 ± 6.52 L3E-M2VC-MT 82.17 ± 2.49 88.43 ± 6.36 91.37 ± 3.52 93.21 ± 3.47 92.41 ± 2.73 84.11 ± 4.03 88.65 ± 2.91 88.58 ± 4.62 BMTMVC 87.67 ± 1.76 92.18 ± 4.21 94.23 ± 2.33 93.65 ± 2.77 96.99 ± 2.13 86.21 ± 3.65 92.96 ± 2.07 90.68 ± 3.54 SMTMVC 88.95 ± 1.33 93.98 ± 4.69 94.26 ± 1.67 95.89 ± 3.54 96.77 ± 1.55 88.39 ± 3.97 93.33 ± 1.52 92.75 ± 4.07 LLE-M2VC 82.35 ± 6.29 75.00 ± 6.21 77.98 ± 7.10 74.21 ± 5.55 71.04 ± 6.37 69.74 ± 6.08 77.12 ± 6.59 72.98 ± 5.95 LE-M2VC 83.64 ± 7.12 79.67 ± 5.09 80.45 ± 7.32 79.29 ± 5.68 73.67 ± 7.61 75.48 ± 6.88 79.25 ± 7.35 78.15 ± 5.88 LE-LLE-M2VC 87.33 ± 2.53 91.45 ± 4.89 90.22 ± 2.09 93.72 ± 3.88 93.10 ± 2.16 85.68 ± 3.80 90.22 ± 2.26 90.28 ± 4.19 It extracts the structure of multiple views in tasks. Then we transform the samples from view space to task space. It learns the shared and complementary features of multiple tasks to keep Y. Zhang et al. / Knowledge-Based Systems ( ) -</figDesc><table><row><cell>Methods</cell><cell>Taks1</cell><cell></cell><cell>Taks2</cell><cell></cell><cell>Taks3</cell><cell></cell><cell>Average</cell></row><row><cell></cell><cell>RI</cell><cell>CA</cell><cell>RI</cell><cell>CA</cell><cell>RI</cell><cell>CA</cell><cell>RI</cell><cell>CA</cell></row><row><cell cols="2">K-Means 66.34 ± KK-Means 68.79 ± 6.37</cell><cell cols="2">71.24 ± 4.28 71.26 ± 7.58</cell><cell cols="3">76.35 ± 4.32 68.13 ± 6.63 63.03 ± 4.11</cell><cell>69.39 ± 6.86</cell><cell>70.21 ± 4.24</cell></row><row><cell>CoRe</cell><cell>63.37 ± 4.35</cell><cell cols="2">73.53 ± 4.12 65.59 ± 3.16</cell><cell cols="3">79.25 ± 5.87 67.46 ± 5.26 66.17 ± 3.25</cell><cell>65.47 ± 4.26</cell><cell>72.98 ± 4.41</cell></row><row><cell>L3E-M2VC-MV</cell><cell>87.23 ± 2.14</cell><cell>88.66</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p><p><p><p><p>L3E-M2VC</p>90.</p><ref type="bibr" target="#b41">42</ref> </p>± 0.95 93.75 ± 3.49 97.51 ± 0.88 96.67 ± 2.26 98.72 ± 0.54 89.25 ± 3.11 95.55 ± 0.79 93.22 ± 2.95</p>tasks may have different class labels to deal with multi-task multiview problems. We firstly map the samples from original space to view space to preserve the relationship of views in the same task.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6</head><label>6</label><figDesc>The RI and CA (avg ± std%) of each task on NUS-WIDE data sets. ± 7.87 59.88 ± 2.<ref type="bibr" target="#b17">18</ref> 40.22 ± 7.89 44.67 ± 3.22 42.28 ± 7.59 44.13 ± 2.56 43.58 ± 7.05 59.71 ± 5.32 43.11 ± 7.60 52.10 ± 3.32 kk-means 45.36 ± 6.36 60.23 ± 2.31 41.34 ± 6.25 45.88 ± 3.12 43.66 ± 6.49 43.85 ± 2.78 45.60 ± 6.09 59.44 ± 5.44 43.99 ± 6.30 52.35 ± 3.41 CoRe 58.37±3.24 63.11 ± 1.02 58.47 ± 3.18 52.53 ± 2.47 59.87 ± 3.42 52.12 ± 0.98 57.43 ± 2.97 62.96 ± 1.55 58.54 ± 3.20 57.68 ± 1.51 L3E-M2VC-MV 60.21 ± 2.12 68.12 ± 2.33 59.78 ± 2.34 53.21 ± 2.34 61.17 ± 2.45 55.17 ± 2.04 56.88 ± 2.79 68.76 ± 2.53 59.51 ± 2.43 61.32 ± 2.31 LSSMTC 60.32 ± 2.66 69.77 ± 7.03 62.43 ± 2.89 52.10 ± 5.22 63.57 ± 2.41 49.26 ± 2.32 59.26 ± 3.20 65.53 ± 7.68 61.40 ± 2.79 59.17 ± 5.56 L3E-M2VC-MT 59.34 ± 2.37 67.45 ± 2.56 59.23 ± 2.22 51.94 ± 2.77 60.14 ± 1.98 52.21 ± 2.43 54.13 ± 2.31 66.22 ± 2.17 58.21 ± 2.22 59.46 ± 2.48 SMTMVC 68.35 ± 1.66 79.23 ± 2.01 67.56 ± 2.12 58.97 ± 3.20 71.48 ± 1.39 58.32 ± 2.64 64.12 ± 2.11 73.65 ± 3.03 67.88 ± 1.82 67.54 ± 2.72 LLE-M2VC 60.14 ± 5.11 65.78 ± 6.77 60.82 ± 5.42 53.85 ± 6.77 57.51 ± 6.18 55.52 ± 5.36 59.62 ± 6.33 55.91 ± 6.71 59.52 ± 5.76 57.77 ± 6.40 LE-M2VC 61.87 ± 4.76 54.66 ± 5.21 57.42 ± 6.01 54.80 ± 6.34 60.76 ± 6.12 49.52 ± 5.54 60.30 ± 6.66 54.23 ± 4.21 60.09 ± 5.89 53.30 ± 5.33 LE-LLE-M2VC 63.23 ± 1.43 74.64 ± 1.98 60.21 ± 1.79 56.98 ± 2.11 64.24 ± 1.99 55.77 ± 2.03 59.76 ± 2.03 62.31 ± 1.32 61.86 ± 1.81 62.43 ± 1.86</figDesc><table><row><cell>Methods</cell><cell>Taks1</cell><cell></cell><cell>Taks2</cell><cell></cell><cell>Taks3</cell><cell></cell><cell>Taks4</cell><cell></cell><cell>Average</cell></row><row><cell></cell><cell>RI</cell><cell>CA</cell><cell>RI</cell><cell>CA</cell><cell>RI</cell><cell>CA</cell><cell>RI</cell><cell>CA</cell><cell>RI</cell><cell>CA</cell></row><row><cell cols="2">k-means 46.37 L3E-M2VC 69.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>45 ± 1.13 80.74 ± 1.05 68.24 ± 1.09 61.27 ± 2.25 71</head><label></label><figDesc>.22 ± 1.55 60.32 ± 2.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>12 65.23 ± 1.33 74.63 ± 2.57 68.54 ± 1.28 69.24 ± 2.00Table 7</head><label>7</label><figDesc>Comparison of running time (seconds).</figDesc><table><row><cell></cell><cell>Leaves</cell><cell>WebKB</cell><cell>Mfeat</cell><cell>COIL-ORL</cell><cell>NUS-WIDE</cell></row><row><cell>Total samples</cell><cell>288</cell><cell>1040</cell><cell>2000</cell><cell>1840</cell><cell>15381</cell></row><row><cell>BMTMVC</cell><cell>7.78</cell><cell>71.20</cell><cell>292.34</cell><cell>262.90</cell><cell>-</cell></row><row><cell>SMTMVC</cell><cell>3.32</cell><cell>35.36</cell><cell>144.76</cell><cell>121.23</cell><cell>8188</cell></row><row><cell>L3E-M2VC</cell><cell>2.41</cell><cell>38.75</cell><cell>115.94</cell><cell>98.38</cell><cell>7072</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is supported by the National Natural Science Foundation of China (Nos. 61572407 and 61773324).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Supplementary data</head><p>Supplementary material related to this article can be found online at https://doi.org/10.1016/j.knosys.2018.10.001.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning latent features for multi-view clustering based on nmf</title>
		<author>
			<persName><forename type="first">M</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-47160-0_42</idno>
		<ptr target="http://dx.doi.org/10.1007/978-3-319-47160-0_42" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 International Joint Conference on Rough Set</title>
		<meeting>the 2016 International Joint Conference on Rough Set</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="459" to="469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi-view clustering: a survey</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Big Data Min. Analytics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="83" to="107" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Weighted multi-view spectral clustering based on spectral perturbation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)</title>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Tw-co-k-means: two-level weighted collaborative k-means for multi-view clustering</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.knosys.2018.03.009</idno>
		<ptr target="http://dx.doi.org/10.1016/j.knosys.2018.03.009" />
	</analytic>
	<monogr>
		<title level="j">Knowl. Based Syst</title>
		<imprint>
			<biblScope unit="volume">150</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="127" to="138" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hierarchical clustering multi-task learning for joint human action grouping and recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kankanhalli</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2016.2537337</idno>
		<ptr target="http://dx.doi.org/10.1109/TPAMI.2016.2537337" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="102" to="114" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Algorithm-dependent generalization bounds for multi-task learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Maybank</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2016.2544314</idno>
		<ptr target="http://dx.doi.org/10.1109/TPAMI.2016.2544314" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="227" to="241" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Clustering and projected clustering with adaptive neighbors</title>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1145/2623330.2623726</idno>
		<ptr target="http://dx.doi.org/10.1145/2623330.2623726" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI-15)</title>
		<meeting>the Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI-15)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="977" to="986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The constrained laplacian rank algorithm for graph-based clustering</title>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty AAAI Conference on Artificial Intelligence (AAAI-16</title>
		<meeting>the Thirty AAAI Conference on Artificial Intelligence (AAAI-16</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1969" to="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Robust multi-view data clustering with multi-view capped-norm k-means</title>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Min. Knowl. Discovery</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="483" to="503" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adaptive local structure learning for document coclustering</title>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lv</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.knosys.2018.02.020</idno>
		<ptr target="http://dx.doi.org/10.1016/j.knosys.2018.02.020" />
	</analytic>
	<monogr>
		<title level="j">Knowl. Based Syst</title>
		<imprint>
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="74" to="84" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Constrained nmf-based multi-view clustering on unmapped data</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI-15</title>
		<meeting>the Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI-15</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3174" to="3180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Local linear neighbor reconstruction for multi-view data</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ding</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patrec.2016.08.002</idno>
		<ptr target="http://dx.doi.org/10.1016/j.patrec.2016.08.002" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="56" to="62" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Parameter-free auto-weighted multiple graph learning: a framework for multiview clustering and semi-supervised classification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Fifth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1881" to="1887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Self-weighted multiview clustering with multiple graphs</title>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2017/357</idno>
		<ptr target="http://dx.doi.org/10.24963/ijcai.2017/357" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Sixth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="2564" to="2570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Self-weighted multiview clustering with multiple graphs</title>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17</title>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2408" to="2414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Auto-weighted multi-view learning for image clustering and semi-supervised classification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIP.2017.2754939</idno>
		<ptr target="http://dx.doi.org/10.1109/TIP.2017.2754939" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1501" to="1511" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Self-weighted multi-view clustering with soft capped norm</title>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.knosys.2018.05.017</idno>
		<ptr target="http://dx.doi.org/10.1016/j.knosys.2018.05.017" />
	</analytic>
	<monogr>
		<title level="j">Knowl. Based Syst</title>
		<imprint>
			<biblScope unit="volume">158</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-view clustering via multimanifold regularized non-negative matrix factorization</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neunet.2017.02.003</idno>
		<ptr target="http://dx.doi.org/10.1016/j.neunet.2017.02.003" />
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="74" to="89" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Robust multi-view data clustering with multi-view capped-norm k-means</title>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2018.05.072</idno>
		<ptr target="http://dx.doi.org/10.1016/j.neucom.2018.05.072" />
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">311</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="197" to="208" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Reliable multi-view clustering</title>
		<author>
			<persName><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)</title>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Smart multitask bregman clustering and multitask kernel clustering</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1145/2747879</idno>
		<ptr target="http://dx.doi.org/10.1145/2747879" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Knowl. Discovery Data (TKDD)</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="29" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multitask spectral clustering by exploring intertask correlation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<idno type="DOI">10.1109/TCYB.2014.2344015</idno>
		<ptr target="http://dx.doi.org/10.1109/TCYB.2014.2344015" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1083" to="1094" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Convex discriminative multitask clustering</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2014.2343221</idno>
		<ptr target="http://dx.doi.org/10.1109/TPAMI.2014.2343221" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="28" to="40" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Self-Adapted multi-task clustering</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Fifth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="2357" to="2363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Partially related multi-task clustering</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TKDE.2018.2818705</idno>
		<ptr target="http://dx.doi.org/10.1109/TKDE.2018.2818705" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng. (TKDE)</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multi-task clustering through instances transfer</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2017.04.029</idno>
		<ptr target="http://dx.doi.org/10.1016/j.neucom.2017.04.029" />
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">251</biblScope>
			<biblScope unit="page" from="145" to="155" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Systematic identification of feature combinations for predicting drug response with bayesian multi-view multi-task linear regression</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ammaduddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wennerberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aittokallio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.knosys.2018.10.001</idno>
		<ptr target="https://doi.org/10.1016/j.knosys.2018.10.001" />
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2017">2017. 2018</date>
		</imprint>
	</monogr>
	<note>A multitask multiview clustering algorithm in heterogeneous situations based on LLE and LE, Knowledge-Based Systems</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Symmetric nonnegative matrix factorization for graph clustering</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<idno type="DOI">10.1137/1.9781611972825.10</idno>
		<ptr target="http://dx.doi.org/10.1137/1.9781611972825.10" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 SIAM International Conference on Data Mining</title>
		<meeting>the 2012 SIAM International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="106" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multi-task clustering using constrained symmetric non-negative matrix factorization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Al-Stouhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Reddy</surname></persName>
		</author>
		<idno type="DOI">10.1137/1.9781611973440.90</idno>
		<ptr target="http://dx.doi.org/10.1137/1.9781611973440.90" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 SIAM International Conference on Data Mining (ICDM 2014)</title>
		<meeting>the 2014 SIAM International Conference on Data Mining (ICDM 2014)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="785" to="793" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multi-objective multi-view spectral clustering via pareto optimization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Davidson</surname></persName>
		</author>
		<idno type="DOI">10.1137/1.9781611972832.26</idno>
		<ptr target="http://dx.doi.org/10.1137/1.9781611972832.26" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 SIAM International Conference on Data Mining</title>
		<meeting>the 2013 SIAM International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="234" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multi-view clustering of web documents ssing multi-objective genetic algorithm</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wahid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Andreae</surname></persName>
		</author>
		<idno type="DOI">10.1109/CEC.2014.6900586</idno>
		<ptr target="http://dx.doi.org/10.1109/CEC.2014.6900586" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 IEEE Congress on Evolutionary Computation (CEC 2014)</title>
		<meeting>the 2014 IEEE Congress on Evolutionary Computation (CEC 2014)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2625" to="2632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-objective multi-view clustering ensemble based on evolutionary approach</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wahid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Andreae</surname></persName>
		</author>
		<idno type="DOI">10.1109/CEC.2015.7257091</idno>
		<ptr target="http://dx.doi.org/10.1109/CEC.2015.7257091" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2015 IEEE Congress on Evolutionary Computation (CEC 2015</title>
		<meeting>2015 IEEE Congress on Evolutionary Computation (CEC 2015</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1696" to="1703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multi-Task multi-view clustering for non-negative data</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Joint Conference on Artificial Intelligence (IJCAI</title>
		<meeting>the 24th International Joint Conference on Artificial Intelligence (IJCAI</meeting>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="4055" to="4061" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multi-Task multi-view clustering</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TKDE.2016.2603983</idno>
		<ptr target="http://dx.doi.org/10.1109/TKDE.2016.2603983" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3324" to="3338" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Inductive multi-task learning with multiple view data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huan</surname></persName>
		</author>
		<idno type="DOI">10.1145/2339530.2339617</idno>
		<ptr target="http://dx.doi.org/10.1145/2339530.2339617" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="543" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Graph-based framework for multi-task multi-view learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lawrence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML 2011</title>
		<meeting>the 28th International Conference on Machine Learning (ICML 2011</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Shared structure learning for multiple tasks with multiple views</title>
		<author>
			<persName><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-40991-2_23</idno>
		<ptr target="http://dx.doi.org/10.1007/978-3-642-40991-2_23" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2013 Joint European Conference on Machine Learning and Knowledge Discovery in Databases (ECML PKDD)</title>
		<meeting>2013 Joint European Conference on Machine Learning and Knowledge Discovery in Databases (ECML PKDD)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="353" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multi-task multi-view learning for heterogeneous tasks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1145/2661829.2662054</idno>
		<ptr target="http://dx.doi.org/10.1145/2661829.2662054" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Information and Knowledge Management (CIKM 2014)</title>
		<meeting>the ACM International Conference on Information and Knowledge Management (CIKM 2014)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="441" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Nonlinear dimensionality reduction by locally linear embedding</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page" from="2323" to="2326" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps and spectral techniques for embedding and clustering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Neural Information Processing Systems: Natural and Synthetic</title>
		<meeting>International Conference on Neural Information Processing Systems: Natural and Synthetic</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="585" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">An entropy weighting k-means algorithm for subspace clustering of high-dimensional sparse data</title>
		<author>
			<persName><forename type="first">L</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TKDE.2007.1048</idno>
		<ptr target="http://dx.doi.org/10.1109/TKDE.2007.1048" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng. (TKDE)</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1026" to="1041" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A fast and effective kernel-based k-means clustering algorithm</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kong</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISDEA.2012.21</idno>
		<ptr target="http://dx.doi.org/10.1109/ISDEA.2012.21" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Third International Conference on Intelligent System Design and Engineering Applications</title>
		<meeting>the 2013 Third International Conference on Intelligent System Design and Engineering Applications</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="58" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Co-regularized multi-view spectral clustering</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Daume</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Neural Information Processing Systems (NIPS 2011)</title>
		<meeting>International Conference on Neural Information Processing Systems (NIPS 2011)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1413" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning the shared subspace for multi-task clustering and transductive transfer classification</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICDM.2009.32</idno>
		<ptr target="http://dx.doi.org/10.1109/ICDM.2009.32" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of 9th IEEE International Conference on Data Mining (ICDM 2009</title>
		<meeting>9th IEEE International Conference on Data Mining (ICDM 2009</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="159" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A survey of clustering algorithms for big data: taxonomy and empirical analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fahad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Alshatri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alamri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Khalil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Zomaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Foufou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bouras</surname></persName>
		</author>
		<idno type="DOI">10.1109/TETC.2014.2330519</idno>
		<ptr target="http://dx.doi.org/10.1109/TETC.2014.2330519" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Emerg. Top. Comput</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="267" to="279" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A unified framework for model-based clustering</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
		<idno type="DOI">10.1162/1532443041827943</idno>
		<ptr target="http://dx.doi.org/10.1162/1532443041827943" />
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1001" to="1037" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
