<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SSMix: Saliency-Based Span Mixup for Text Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Soyoung</forename><surname>Yoon</surname></persName>
							<email>soyoungyoon@kaist.ac.kr</email>
						</author>
						<author>
							<persName><forename type="first">Gyuwan</forename><surname>Kim</surname></persName>
							<email>gyuwan.kim@navercorp.com</email>
						</author>
						<author>
							<persName><forename type="first">Kyumin</forename><surname>Park</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Clova</forename><surname>Ai</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Naver</forename><surname>Corp</surname></persName>
						</author>
						<author>
							<persName><surname>Kaist</surname></persName>
						</author>
						<title level="a" type="main">SSMix: Saliency-Based Span Mixup for Text Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Data augmentation with mixup has shown to be effective on various computer vision tasks. Despite its great success, there has been a hurdle to apply mixup to NLP tasks since text consists of discrete tokens with variable length. In this work, we propose SSMix, a novel mixup method where the operation is performed on input text rather than on hidden vectors like previous approaches. SSMix synthesizes a sentence while preserving the locality of two original texts by span-based mixing and keeping more tokens related to the prediction relying on saliency information. With extensive experiments, we empirically validate that our method outperforms hidden-level mixup methods on a wide range of text classification benchmarks, including textual entailment, sentiment classification, and questiontype classification. Our code is available at https://github.com/clovaai/ssmix.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Data augmentation gains popularity in natural language processing (NLP) <ref type="bibr" target="#b4">(Feng et al., 2021)</ref> due to the expensive cost of data collection. Some of them are based on simple rules <ref type="bibr" target="#b24">(Wei and Zou, 2019)</ref> and models <ref type="bibr" target="#b3">(Edunov et al., 2018;</ref><ref type="bibr" target="#b12">Ng et al., 2020)</ref> to generate similar text. Augmented samples are trained jointly with original samples by a standard way or advanced training methods <ref type="bibr" target="#b28">(Zhu et al., 2019;</ref><ref type="bibr" target="#b14">Park et al., 2021)</ref>. On the other hand, mixup <ref type="bibr" target="#b27">(Zhang et al., 2018)</ref> interpolates input texts and labels for the augmentation.</p><p>Training with mixup and its variants become a popular regularization method in computer vision to improve the generalization of neural networks. Mixup approaches are categorized into input-level mixup <ref type="bibr" target="#b26">(Yun et al., 2019;</ref><ref type="bibr">Kim et al.,</ref> Figure <ref type="figure">1</ref>: Illustration of SSMix. Two data samples x A and x B are labeled negative and positive respectively for sentiment classification task. For each token, saliency maps are visualized where darker concentration of colors mean higher contribution to corresponding label. We select the least salient span from x A and replace it with the most salient span from x B . The output results in x = mixup(x A , x B ). We also assign ỹ by the mixup ratio λ. In this example, λ is set to 0.2 as the span length is 2 out of 10. 2020; <ref type="bibr" target="#b22">Walawalkar et al., 2020;</ref><ref type="bibr" target="#b20">Uddin et al., 2021)</ref> and hidden-level mixup <ref type="bibr" target="#b21">(Verma et al., 2019)</ref> depending on the location of the mix operation. Inputlevel mixup is a more prevalent approach than hidden-level mixup because of its simplicity and the ability to capture locality, leading to better accuracy.</p><p>Applying mixup in NLP is more challenging than in computer vision because of the discrete nature of text data and variable sequence lengths. Therefore, most previous attempts on mixup for texts <ref type="bibr" target="#b5">(Guo et al., 2019;</ref><ref type="bibr" target="#b1">Chen et al., 2020)</ref> apply mixup on hidden vectors like embeddings or intermediate representations. However, input-level mixup might have an advantage over hidden-level mixup with a similar intuition from computer vision. This motivation encourages us to examine input-level mixup approaches for text data.</p><p>In this work, we propose SSMix (Fig 1 <ref type="figure">)</ref>, a novel input-level spanwise mixup method considering the saliency of spans. First, we conduct a mixup by replacing a span of contiguous tokens with a span in another text, which is inspired from CutMix arXiv:2106.08062v1 [cs.CL] 15 Jun 2021 <ref type="bibr" target="#b26">(Yun et al., 2019)</ref>, to preserves the locality of two source texts in the mixed text. Second, we select a span to be replaced and to replace based on saliency information to make the mixed text contain tokens more related to output prediction, which may be semantically important. Our input-level method is different from hidden level mixup methods in that while current hidden level mixup methods linear interpolate original hidden vectors, our method mix tokens on the input level, resulting in a nonlinear output. Also, we utilize saliency values to select span from each sentence and discretely define the length of span and mixup ratio, which is outside the hidden level.</p><p>SSMix has empirically proven effective through extensive experiments on a wide range of text classification benchmarks. Especially, we prove that input-level mixup methods generally outperform hidden-level methods. We also show the importance of using saliency information and restricting token selection in span-level when conducting our method via ablation study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">SSMix</head><p>We propose SSMix to synthesize a new text x by replacing a span x A S from one text x A into another span x B S from another text x B based on saliency information. Also, we have to set a new label ỹ for x using y A and y B which are one-hot labels corresponding to x A and x B , respectively. Consequently, we can additionally use this generated virtual sample (x, ỹ) for training.</p><p>Saliency Saliency measures how each portion of data (in this case, tokens) affects the final prediction. Gradient-based methods <ref type="bibr" target="#b16">(Simonyan et al., 2013;</ref><ref type="bibr" target="#b10">Li et al., 2016)</ref> are widely used for the saliency computation. We compute the gradient of classification loss L with respect to input embedding e, and use its magnitude as the saliency: i.e., s = ∂L/∂e 2 . We apply the L2 norm to obtain the magnitude of a gradient vector, which becomes a saliency of each token similar to PuzzleMix <ref type="bibr" target="#b9">(Kim et al., 2020)</ref>.</p><p>Mixing text Text data x A and x B are discrete token sequences. Using saliency scores as explained earlier, we can find the least salient span in x A with a length l A as x A S and the most salient span in x A with a length l B as x B S . We set l A = l B = max(min([λ 0 |x A |], |x B |), 1) given a prior mixup ratio λ 0 . Then, final x becomes the concatena-</p><formula xml:id="formula_0">Algorithm 1 Mixup loss calculation procedure SSMIX_LOSS(x A , x B , y A , y B , λ) x ← SSM ix(x A , x B ) logit ← model(x) loss A ← CrossEntropy(logit, y A ) loss B ← CrossEntropy(logit, y B ) total_loss ← loss A * λ + loss B * (1 − λ) return total_loss end procedure tion of (x A L ; x B S ; x A R )</formula><p>where x A L and x A R are tokens located to the left and the right side of x A S respectively in the original text x A .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Same span length</head><p>We set the length of the original (l A ) and replaced (l B ) span to be the same, since allowing different length of spans would result in redundant and ambiguous mixup variations. Also, calculating the mixup ratio between different span length would be too complex. This same-size replacement strategy is also adopted in <ref type="bibr" target="#b26">Yun et al. (2019)</ref> and <ref type="bibr" target="#b20">Uddin et al. (2021)</ref>. In situations where span length is the same, our method maximizes the effect of saliency. Since SSMix doesn't restrict the position of tokens, we can pick the most salient span and replace it with least salient span on the other text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mixing label</head><p>We set mixup ratio λ for label as λ = |x B S |/|x|. Since λ is recalculated by counting the number of tokens in the span, it may differ from λ 0 . We set the label of x to ỹ = (1 − λ)y A + λy B . Algorithm 1 shows how we utilize the original sample pairs to compute the mixup loss for augmented samples. We calculate the cross-entropy loss of the augmented output logit with respect to the original target label of each sample and combine them by weighted sum, which is similar to the original implementation of <ref type="bibr" target="#b27">Zhang et al. (2018)</ref>.<ref type="foot" target="#foot_0">1</ref> Therefore, applying SSMix is independent of the total number of labels of the classification dataset. On any dataset, output label ratio is calculated by linear combination of two original labels.</p><p>Paired sentence tasks For tasks requiring a pair of texts as an input such as textual entailment and similarity classification, we conduct mixup in a pairwise manner and calculate the mixup ratio by aggregating token counts in each mixup result. Denoting x = (p, q), we define mixup of paired sentence data as x = (mixup(p A , p B ), mixup(q A , q B )).</p><formula xml:id="formula_1">x A = (p A , q A ), x B = (p B , q B ),</formula><p>Here, we set the mixup ratio on paired sentence tasks as λ = (|p S | + |q S |)/(|p| + |q|), where p S and q S are replacing spans of independent mixup operations. Illustration is available in Appendix B.3.</p><p>3 Experimental Setup</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset</head><p>As listed in table 1, to evaluate the effectiveness of SSMix, we perform experiments on various text classfication benchmarks: six datasets in GLUE benchmark <ref type="bibr" target="#b23">(Wang et al., 2018)</ref>, TREC <ref type="bibr" target="#b11">(Li and Roth, 2002;</ref><ref type="bibr" target="#b7">Hovy et al., 2001)</ref>, and ANLI <ref type="bibr" target="#b13">(Nie et al., 2020)</ref>. Two of them are single sentence classification tasks, and six of them are sentence pair classification tasks. All datasets are extracted from HuggingFace datasets library. <ref type="foot" target="#foot_1">2</ref>For GLUE, we use SST-2 <ref type="bibr" target="#b17">(Socher et al., 2013)</ref>, MNLI <ref type="bibr" target="#b25">(Williams et al., 2018)</ref>, QNLI <ref type="bibr" target="#b15">(Rajpurkar et al., 2016)</ref>, RTE <ref type="bibr" target="#b0">(Bentivogli et al., 2009)</ref>, MRPC <ref type="bibr" target="#b2">(Dolan and Brockett, 2005)</ref>, and QQP<ref type="foot" target="#foot_2">3</ref> . Among GLUE, we leave out datasets that were not evaluated by accuracy, along with WNLI, because the size is too small to show any general trend of effectiveness.</p><p>TREC is a commonly used dataset to evaluate mixup methods in sentence classification <ref type="bibr" target="#b5">(Guo et al., 2019;</ref><ref type="bibr" target="#b19">Thulasidasan et al., 2019)</ref>. We use two different versions of TREC (coarse, fine) that have different levels of label number to test the dependency of mixup effectiveness on the number of class labels. In addition, we use ANLI to see how mixup can help to improve model robustness.</p><p>For training ANLI, we concatenate all training data from different rounds and use them to train the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Baseline</head><p>We compare SSMix with three baselines: (1) standard training without mixup, (2) EmbedMix, and (3) TMix. EmbedMix apply mixup on the embedding layer, which is similar to the wordMixup in <ref type="bibr" target="#b5">Guo et al. (2019)</ref> except their experiments are performed with LSTM or CNN architecture. TMix, borrowed from Chen et al. ( <ref type="formula">2020</ref>), interpolates hidden states of two different inputs at a particular encoder layer and forward the combined hidden states to the remaining layers. For EmbedMix and TMix, we follow the best settings stated in the original papers: mixup ratio is set by λ ∼ Beta(α, α), λ = max(λ , 1 − λ ) with α = 0.2. During the training with TMix, we randomly sample the mixup layer from <ref type="bibr">[7,</ref><ref type="bibr">9,</ref><ref type="bibr">12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ablation study</head><p>To investigate how much (1) considering saliency and (2) restricting mixup operation on the spanlevel individually benefit our proposed method, we conduct an ablation study. We implement SSMix without considering saliency information (SSMix -saliency) where the spans are randomly selected, and additionally without the span-level restriction (SSMix -saliency -span). For SSMix -saliencyspan, we randomly sample tokens from x B , which need not be a contiguous span and are conducted on a per-token basis. Then, we replace tokens accordingly with the position of the token be preserved, meaning that the second token from x A is replaced with the second token from x B , and so on. For all ablation studies, the lambda values were set to 0.1 to compare methods with the same setting as SSMix. Detailed implementation and illustration of ablation methods and comparison with simple word dropout methods are described in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training Details</head><p>Among the entire experiment, we use sequence classification task with the pre-trained BERT-base model having 110M parameters from Hugging-Face Transformers library. <ref type="foot" target="#foot_3">4</ref> We perform all experiments with five different seeds (0 to 4) on a single NVIDIA P40 GPU and report the average score. We set a maximum sequence length of 128, batch size of 32, with AdamW optimizer with eps of 1e-8 and weight decay of 1e-4. We use a linear scheduler with a warmup for 10% of the total training step. We update the best checkpoint by measuring validation accuracy on every 500 steps. For datasets that have less than 500 steps per epoch, we update and validate every epoch.</p><p>Considering our objective of enhancing performance through mixup, we conduct training in two steps. We first train without mixup with a learning rate of 5e-5 for three epochs, and then train with mixup starting from previous training's best checkpoint, with a learning rate of 1e-5 for five epochs. This two-step training, which also utilized by <ref type="bibr" target="#b27">Zhang et al. (2018)</ref>, speeds up the model convergence. We report the best accuracy among both training with and without mixup. For the ANLI task, we select the best checkpoint for training without mixup separately for each round, then conduct training with mixup and report the best accuracy of each round's evaluation dataset.</p><p>For each iteration, we split the batch into two smaller batches with the same size, A and B. Since mixup operation in SSMix is not symmetric, we conduct mixup back-and-forth so that mixup performance is evaluated regardless of the data position in batch. To prevent the training data distribution getting too far from the original data distribution, we train with and without mixup together as <ref type="bibr" target="#b6">He et al. (2019)</ref>. As a result, we forward each step with average loss from A, B, mixup(A, B), and mixup(B, A).</p><p>We leave out tokens specific to transformer architecture (e.g., [CLS], [SEP ]) when conducting a mixup to preserve special signs. As stated by <ref type="bibr" target="#b27">Zhang et al. (2018)</ref>, giving too high values for mixup ratio may lead to underfitting, while giving λ close to 0 leads to the same effect of giving nonaugmented original data. From our experiments, we found out that augmentation with prior ratio λ 0 = 0.1 is the optimal hyperparameter.</p><p>In terms of computation time, SSMix takes about twice the training time compared with other mixup methods since we need an additional forward and backward step to compute the saliency of tokens. Among hidden-level mixup methods, TMix takes a slightly longer time to train than EmbedMix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Discussion</head><p>Table <ref type="table" target="#tab_1">2</ref> illustrates our results. We investigate the effectiveness of SSMix compared with hidden layer mixup methods on the aspect of dataset size, number of class labels, and paired sentence tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset size</head><p>Compared with hidden-level mixup methods, SSMix fully demonstrate its effectiveness on datasets having a sufficient amount of data. Since SSMix is a discrete combination rather than a linear combination of two data samples, it creates data samples on a synthetic space in a larger range than hidden-level mixup (Fig. <ref type="figure" target="#fig_0">2</ref>). We hypothesize that a large amount of data help better representation in synthetic space.</p><p>The number of class labels SSMix is especially effective for multiple class label datasets (TREC, ANLI, MNLI, QNLI). Accordingly, the accuracy gain of SSMix from the training without mixup is much higher on TREC-fine (47 labels) than TRECcoarse (6 labels), with +3.56 and +0.52, respectively. We hypothesize that this result originates from the mixup characteristic that benefits more from cross-label mixup than mixup with the same label, as stated at <ref type="bibr" target="#b27">Zhang et al. (2018)</ref>. <ref type="foot" target="#foot_4">5</ref>  datasets with multiple total class labels increase the possibility of being selected cross-label in a random sampling of mixup sources, we assert mixup performance increases in such datasets.</p><p>Paired sentence tasks SSMix have a competitive advantage on paired sentence tasks, such as textual entailment or similarity classification. We suspect this accuracy gain originates from consideration of individual tokens. Existing methods (hidden-level mixup) apply mixup on the hidden layer, without consideration of special tokens, i.e.,</p><p>[SEP ], <ref type="bibr">[CLS]</ref>. These methods may lose information about the start of the sentence or appropriate separation of pair of sentences. In contrast, SSMix can consider the individual token property when applying mixup. Here, our mixup strategy on paired data (Section 2) preserves the property of [SEP ], which is not guaranteed by hidden mixup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head><p>The results of SSMix and its variants demonstrate that the performance improves as we add span constraint and saliency information. Adding span constraint in the mixup operation benefit from better localizable ability, and most salient spans have more relationship to corresponding labels while discarding least salient spans have a higher probability that those spans are not semantically important with respect to the original labels. Among those two, introducing saliency information contributes to accuracy relatively more than the span constraint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We present SSMix, a novel and simple input-level mixup method for text data that improves regularization ability leading to better performance in text classification. SSMix preserves the locality of mixing texts by replacing in span-level and keep most discriminative tokens in the mixed text using saliency score. Throughout the experiment, we show that our method improves performance in various types of text classification tasks. For future work, we plan to apply SSMix on a broader range of tasks, including generation or different scenarios like semi-supervised learning.</p><p>A Accuracy Variance We also report accuracy variance among the five seeds for each experiment (Table <ref type="table">.</ref> A.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Ablation</head><p>Fig. <ref type="figure" target="#fig_1">3</ref> and Fig. <ref type="figure" target="#fig_3">4</ref> shows the illustration of different variants of SSMix and random UNK replacement with λ = 0.2. Fig. <ref type="figure" target="#fig_4">5</ref> shows the illustration of getting the augmented output with lambda calculation by SSMix for paired sentence tasks. The saliency maps are visualized where darker concentration of colors mean higher contribution to corresponding label.  At normal training, only two real data samples (x A and x B ) are used to train the model. For Figure <ref type="figure">.</ref> 3 (b), we randomly select each span from x A and x B . Then, we replace x B to x A to make a new data x. For Figure <ref type="figure">.</ref> 3 (c), input level mixup is conducted on a per-token basis. After calculaton of l given the prior mixup ratio, we randomly sample tokens from x A . The tokens need not be a contiguous span. Then, we replace tokens accordingly with the position of the token be preserved, meaning that the second token from x A is replaced with second token from x B , the sixth token from x A is replaced with sixth token from x B (by the illustration example), and so on.  We also compare SSMix with simple word dropout methods, which may seem similar in the perspective that they create noisy sentences. The difference is whether label mixup is performed. Illustration of the implementation of random [UNK] replacement is available at Fig. <ref type="figure" target="#fig_3">4</ref>. Random UNK replacement is similar to word dropout. We don't use x B when making synthetic samples (l = 0). Instead, we randomly sample a set of tokens from x A and replace each token in that span with [UNK]. The process is similar to <ref type="bibr">Figure. 3 (c)</ref>, except that the selected tokens at x A are replaced into [UNK]. Another difference is that the output label (ỹ) completely follow the origin (y A ) and no label mixup is performed. The illustration is available at 3. We evaluate the random [UNK] replacement method on all dataset with SSMix and variants of SSMix at ablation study. By the experiment results at Table <ref type="table" target="#tab_3">B</ref>.1, we show that input level mixup methods generally outperform simple regularization methods. This means that datasets synthesized from SSMix and the according target vectors have more gain on the generalization ability than word dropout. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Variants of SSMix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Comparison with other simple augmentation methods</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Visualization of original data and synthesized data by hidden-level mixup (EmbedMix or TMix) and SSMix in the hidden space. Black dots indicate the original data, x A and x B. For hidden-level mixup, synthetic data (x) are created only along the line (blue) connecting two points, since it is a linear combination within the hidden space. However, SSMix explore larger synthetic sample space for x, since it consists of a discrete combination within the input space. Synthetic data for SSMix are illustrated in pink dots.</figDesc><graphic url="image-2.png" coords="4,314.37,62.81,204.09,130.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Illustration of normal training and variants of</figDesc><graphic url="image-4.png" coords="8,86.23,597.27,204.10,87.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) Random [UNK] replacement (b) SSMix</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Comparison of our methods with word dropout</figDesc><graphic url="image-6.png" coords="9,86.23,453.00,204.10,87.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Illustration of applying SSMix to make x for paired sentence, in particular NLI tasks, which classifies whether the relation of sentence pairs is entailment, neutral, or contradiction. Mixup is conducted individually, sentence by sentence.</figDesc><graphic url="image-8.png" coords="10,185.39,62.81,226.77,96.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>and Dataset name, task, number of total labels, and dataset size of datasets we used as benchmark. Task column describes the objective of each dataset. ANLI dataset shows aggregated dataset statistics among different rounds. GLUE tasks report the size as (train / validation) format, TREC reports (train / test) and ANLI reports (train / validation / test).</figDesc><table><row><cell>Dataset</cell><cell>Task</cell><cell># Label</cell><cell>Size</cell></row><row><cell>SST-2</cell><cell>Sentiment</cell><cell>2</cell><cell>67k / 1.8k</cell></row><row><cell>QQP</cell><cell>Paraphrase</cell><cell>2</cell><cell>364k / 391k</cell></row><row><cell>MNLI</cell><cell>NLI</cell><cell>3</cell><cell>393k / 20k</cell></row><row><cell>QNLI</cell><cell>QA/NLI</cell><cell>3</cell><cell>105k / 5.4k</cell></row><row><cell>RTE</cell><cell>NLI</cell><cell>2</cell><cell>2.5k / 3k</cell></row><row><cell>MRPC</cell><cell>Paraphrase</cell><cell>2</cell><cell>3.7k / 1.7k</cell></row><row><cell cols="2">TREC-coarse Classification</cell><cell>6</cell><cell>5.5k / 500</cell></row><row><cell>TREC-fine</cell><cell>Classification</cell><cell>47</cell><cell>5.5k / 500</cell></row><row><cell>ANLI</cell><cell>NLI</cell><cell>3</cell><cell>162.8k / 3.2k / 3.2k</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Since Experimental results of comparison with baselines and ablation study. All values are average accuracy (%) of five runs with different seeds. MNLI indicates MNLI-mismatched dev set accuracy. We report validation accuracy for GLUE, test accuracy for TREC, and valid (upper) / test (lower) accuracy for ANLI. We report variance on Appendix. A.</figDesc><table><row><cell>Model</cell><cell>GLUE</cell><cell>TREC</cell><cell></cell><cell>ANLI</cell></row><row><cell></cell><cell cols="2">SST-2 QQP MNLI QNLI RTE MRPC coarse fine</cell><cell>R1</cell><cell>R2</cell><cell>R3</cell></row><row><cell>No mixup</cell><cell>92.96 91.32 84.27 91.28 65.56 86.37</cell><cell>97.08 86.68</cell><cell cols="2">56.40 47.10 47.62 57.16 47.36 48.00</cell></row><row><cell>EmbedMix</cell><cell>93.03 91.36 84.35 91.43 67.73 86.72</cell><cell>97.44 90.04</cell><cell cols="2">56.78 47.84 47.67 57.16 47.42 48.00</cell></row><row><cell>TMix</cell><cell>93.03 91.34 84.33 91.40 66.86 86.42</cell><cell>97.52 90.16</cell><cell cols="2">56.68 47.58 47.78 57.28 47.90 48.42</cell></row><row><cell>SSMix</cell><cell>93.10 91.43 84.54 91.54 67.22 86.57</cell><cell>97.60 90.24</cell><cell cols="2">57.26 48.36 47.78 57.34 48.06 48.00</cell></row><row><cell>SSMix -saliency</cell><cell>93.12 91.32 84.48 91.29 67.00 86.42</cell><cell>97.44 89.56</cell><cell cols="2">57.04 48.22 47.95 57.16 47.94 48.07</cell></row><row><cell cols="2">SSMix -saliency -span 93.14 91.32 84.54 91.45 66.93 86.37</cell><cell>97.40 89.20</cell><cell cols="2">56.74 47.52 47.77 57.20 47.90 48.00</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table A .</head><label>A</label><figDesc>1: Standard deviation results, corresponding with the average of our experiments. The deviation is conducted by 5 runs with different seeds.</figDesc><table><row><cell>Model</cell><cell></cell><cell></cell><cell cols="2">GLUE</cell><cell></cell><cell cols="2">TREC</cell><cell></cell><cell>ANLI</cell></row><row><cell></cell><cell cols="8">SST-2 QQP MNLI QNLI RTE MRPC Coarse Fine R1</cell><cell>R2</cell><cell>R3</cell></row><row><cell>No mixup</cell><cell>0.04</cell><cell>0.04</cell><cell>0.12</cell><cell>0.05 3.89</cell><cell>1.73</cell><cell>0.17</cell><cell>2.21</cell><cell cols="2">1.21 0.16 0.73 0.24 1.26 0.84</cell></row><row><cell>EmbedMix</cell><cell>0.02</cell><cell>0.03</cell><cell>0.14</cell><cell>0.04 3.89</cell><cell>1.39</cell><cell>0.09</cell><cell>0.31</cell><cell cols="2">1.38 0.46 0.75 0.24 1.18 0.84</cell></row><row><cell>TMix</cell><cell>0.04</cell><cell>0.04</cell><cell>0.09</cell><cell>0.03 1.85</cell><cell>1.55</cell><cell>0.05</cell><cell>0.63</cell><cell cols="2">1.44 0.33 0.73 0.25 0.75 1.28</cell></row><row><cell>SSMix</cell><cell>0.03</cell><cell>0.07</cell><cell>0.07</cell><cell>0.03 2.57</cell><cell>1.15</cell><cell>0.03</cell><cell>0.49</cell><cell cols="2">1.56 0.27 0.73 0.25 0.46 0.84</cell></row><row><cell>SSMix -saliency</cell><cell>0.02</cell><cell>0.04</cell><cell>0.11</cell><cell>0.04 2.06</cell><cell>1.55</cell><cell>0.09</cell><cell>0.69</cell><cell cols="2">1.33 0.18 0.62 0.24 1.99 0.80</cell></row><row><cell cols="2">SSMix -saliency -span 0.00</cell><cell>0.04</cell><cell>0.09</cell><cell>0.03 1.86</cell><cell>1.73</cell><cell>0.08</cell><cell>0.14</cell><cell cols="2">2.01 0.11 0.68 0.28 0.45 0.84</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table B .</head><label>B</label><figDesc>1: Accuracy (%) comparison with simple data augmentation method(random UNK replacement) and input mixup methods. The results are average of five runs with different seeds. Results show that our input level mixup methods are generally competitive with simple word dropout methods.</figDesc><table><row><cell>Model</cell><cell>GLUE</cell><cell>TREC</cell><cell></cell><cell>ANLI</cell></row><row><cell></cell><cell cols="2">SST-2 QQP MNLI QNLI RTE MRPC coarse fine</cell><cell>R1</cell><cell>R2</cell><cell>R3</cell></row><row><cell>No mixup</cell><cell>92.96 91.32 84.27 91.28 65.56 86.37</cell><cell>97.08 86.68</cell><cell cols="2">56.40 47.10 47.62 57.16 47.36 48.00</cell></row><row><cell cols="2">Random UNK replacement 93.10 91.33 84.46 91.45 66.86 86.62</cell><cell>97.44 89.24</cell><cell cols="2">56.98 47.86 47.98 57.26 48.36 48.32</cell></row><row><cell>SSMix</cell><cell>93.10 91.43 84.54 91.54 67.22 86.57</cell><cell>97.60 90.24</cell><cell cols="2">57.26 48.36 47.78 57.34 48.06 48.00</cell></row><row><cell>SSMix -saliency</cell><cell>93.12 91.32 84.48 91.29 67.00 86.42</cell><cell>97.44 89.56</cell><cell cols="2">57.04 48.22 47.95 57.16 47.94 48.07</cell></row><row><cell>SSMix -saliency -span</cell><cell>93.14 91.32 84.54 91.45 66.93 86.37</cell><cell>97.40 89.20</cell><cell cols="2">56.74 47.52 47.77 57.20 47.90 48.00</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://github.com/hongyizhang/mixup/blob/master/cifar/utils.py#L34</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">https://github.com/huggingface/datasets</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">https://www.quora.com/First-Quora-Dataset-Release-Question-Pairs</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">https://github.com/huggingface/transformers</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><ref type="bibr" target="#b27">Zhang et al. (2018)</ref> states that mixing random pairs from all classes (per-batch basis) has the strongest regularization effect compared with mixup by per-class (same class) basis.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank Clova AI members for proofreading this manuscript and the anonymous reviewers for their constructive feedback. We use Naver Smart Machine Learning <ref type="bibr" target="#b18">(Sung et al., 2017;</ref><ref type="bibr" target="#b8">Kim et al., 2018)</ref> platform for the experiments. This work was supported by Institute of Information &amp; communications Technology Planning &amp; Evaluation (IITP) grant funded by the Korea government (MSIT) (No.2019-0-00075, Artificial Intelligence Graduate School Program (KAIST)).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The fifth pascal recognizing textual entailment challenge</title>
		<author>
			<persName><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<editor>TAC</editor>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mix-Text: Linguistically-informed interpolation of hidden space for semi-supervised text classification</title>
		<author>
			<persName><forename type="first">Jiaao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.194</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2147" to="2157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automatically constructing a corpus of sentential paraphrases</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">B</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Workshop on Paraphrasing (IWP2005)</title>
				<meeting>the Third International Workshop on Paraphrasing (IWP2005)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Understanding back-translation at scale</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.09381</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Varun</forename><surname>Steven Y Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Gangal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarath</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soroush</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teruko</forename><surname>Vosoughi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Mitamura</surname></persName>
		</author>
		<author>
			<persName><surname>Hovy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.03075</idno>
		<title level="m">A survey of data augmentation approaches for nlp</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Augmenting data with mixup for sentence classification: An empirical study</title>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongyi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richong</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.08941</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Data augmentation revisited: Rethinking the distribution gap between clean and augmented data</title>
		<author>
			<persName><forename type="first">Zhuoxun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Toward semantics-based answer pinpointing</title>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurie</forename><surname>Gerber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Ravichandran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First International Conference on Human Language Technology Research</title>
				<meeting>the First International Conference on Human Language Technology Research</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Hanjoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minkyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongjoo</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinwoong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heungseok</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soeun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyunwoo</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngil</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngkwan</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.09957</idno>
		<title level="m">Nsml: Meet the mlaas platform with a real-world case study</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Puzzle mix: Exploiting saliency and local statistics for optimal mixup</title>
		<author>
			<persName><forename type="first">Jang-Hyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wonho</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyun</forename><forename type="middle">Oh</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5275" to="5285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Visualizing and understanding neural models in NLP</title>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1082</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="681" to="691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning question classifiers</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING 2002: The 19th International Conference on Computational Linguistics</title>
				<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Ssmba: Self-supervised manifold based data augmentation for improving out-of-domain robustness</title>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marzyeh</forename><surname>Ghassemi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.10195</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adversarial nli: A new benchmark for natural language understanding</title>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohitand</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Consistency training with virtual adversarial discrete perturbation</title>
		<author>
			<persName><forename type="first">Jungsoo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gyuwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.07284</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Squad: 100, 000+ questions for machine comprehension of text</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno>CoRR, abs/1606.05250</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6034</idno>
		<title level="m">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Nako</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minkyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyunwoo</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngil</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingwoong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonard</forename><surname>Lausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngkwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gayoung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghyun</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.05902</idno>
		<title level="m">Nsml: A machine learning platform that enables you to focus on your models</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On mixup training: Improved calibration and predictive uncertainty for deep neural networks</title>
		<author>
			<persName><forename type="first">Gopinath</forename><surname>Sunil Thulasidasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><forename type="middle">A</forename><surname>Chennupati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tanmoy</forename><surname>Bilmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><surname>Michalak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="13888" to="13899" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Saliencymix: A saliency guided data augmentation strategy for better regularization</title>
		<author>
			<persName><forename type="first">A F M</forename><surname>Shahab Uddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mst</forename><forename type="middle">Sirazam</forename><surname>Monira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wheemyung</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taechoong</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sung-Ho</forename><surname>Bae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Manifold mixup: Better representations by interpolating hidden states</title>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Beckham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Najafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6438" to="6447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Attentive cutmix: An enhanced data augmentation approach for deep learning based image classification</title>
		<author>
			<persName><forename type="first">Devesh</forename><surname>Walawalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zechun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marios</forename><surname>Savvides</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-5446</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 EMNLP Workshop Black-boxNLP: Analyzing and Interpreting Neural Networks for NLP</title>
				<meeting>the 2018 EMNLP Workshop Black-boxNLP: Analyzing and Interpreting Neural Networks for NLP<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="353" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.11196</idno>
		<title level="m">Eda: Easy data augmentation techniques for boosting performance on text classification tasks</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1101</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1112" to="1122" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seong</forename><surname>Joon Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junsuk</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngjoon</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11764</idno>
		<title level="m">Freelb: Enhanced adversarial training for natural language understanding</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
