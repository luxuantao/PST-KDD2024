<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LMC: FAST TRAINING OF GNNS VIA SUBGRAPH-WISE SAMPLING WITH PROVABLE CONVERGENCE</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-02-02">2 Feb 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhihao</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xize</forename><surname>Liang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Jie</forename><surname>Wang</surname></persName>
							<email>jiewangx@ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">LMC: FAST TRAINING OF GNNS VIA SUBGRAPH-WISE SAMPLING WITH PROVABLE CONVERGENCE</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-02-02">2 Feb 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2302.00924v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The message passing-based graph neural networks (GNNs) have achieved great success in many real-world applications. However, training GNNs on large-scale graphs suffers from the well-known neighbor explosion problem, i.e., the exponentially increasing dependencies of nodes with the number of message passing layers. Subgraph-wise sampling methods-a promising class of mini-batch training techniques-discard messages outside the mini-batches in backward passes to avoid the neighbor explosion problem at the expense of gradient estimation accuracy. This poses significant challenges to their convergence analysis and convergence speeds, which seriously limits their reliable real-world applications. To address this challenge, we propose a novel subgraph-wise sampling method with a convergence guarantee, namely Local Message Compensation (LMC). To the best of our knowledge, LMC is the first subgraph-wise sampling method with provable convergence. The key idea of LMC is to retrieve the discarded messages in backward passes based on a message passing formulation of backward passes. By efficient and effective compensations for the discarded messages in both forward and backward passes, LMC computes accurate mini-batch gradients and thus accelerates convergence. We further show that LMC converges to first-order stationary points of GNNs. Experiments on large-scale benchmark tasks demonstrate that LMC significantly outperforms state-of-the-art subgraph-wise sampling methods in terms of efficiency.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph neural networks (GNNs) are powerful frameworks that generate node embeddings for graphs via the iterative message passing (MP) scheme <ref type="bibr" target="#b13">(Hamilton, 2020)</ref>. At each MP layer, GNNs aggregate messages from each node's neighborhood and then update node embeddings based on aggregation results. Such a scheme has achieved great success in many real-world applications involving graph-structured data, such as search engines <ref type="bibr" target="#b0">(Brin &amp; Page, 1998)</ref>, recommendation systems <ref type="bibr" target="#b7">(Fan et al., 2019)</ref>, materials engineering <ref type="bibr" target="#b10">(Gostick et al., 2016)</ref>, and molecular property prediction <ref type="bibr" target="#b21">(Moloi &amp; Ali, 2005;</ref><ref type="bibr">Kearnes et al., 2016)</ref>.</p><p>However, the iterative MP scheme poses challenges to training GNNs on large-scale graphs. One commonly-seen approach to scale deep models to arbitrarily large-scale data with limited GPU memory is to approximate full-batch gradients by mini-batch gradients. Nevertheless, for the graphstructured data, the computational costs for computing the loss across a mini-batch of nodes and the corresponding mini-batch gradients are expensive due to the well-known neighbor explosion problem. Specifically, the embedding of a node at the k-th MP layer recursively depends on the embeddings of its neighbors at the (k -1)-th MP layer. Thus, the complexity grows exponentially with the number of MP layers.</p><p>To deal with the neighbor explosion problem, recent works propose various sampling techniques to reduce the number of nodes involved in message passing <ref type="bibr" target="#b19">(Ma &amp; Tang, 2021)</ref>. For example, nodewise <ref type="bibr" target="#b12">(Hamilton et al., 2017;</ref><ref type="bibr" target="#b1">Chen et al., 2018a)</ref> and layer-wise <ref type="bibr">(Chen et al., 2018b;</ref><ref type="bibr" target="#b30">Zou et al., 2019;</ref><ref type="bibr" target="#b15">Huang et al., 2018)</ref> sampling methods recursively sample neighbors over MP layers to estimate node embeddings and corresponding mini-batch gradients. Unlike the recursive fashion, subgraph-wise sampling methods <ref type="bibr" target="#b4">(Chiang et al., 2019;</ref><ref type="bibr" target="#b28">Zeng et al., 2020;</ref><ref type="bibr" target="#b9">Fey et al., 2021;</ref><ref type="bibr" target="#b29">Zeng et al., 2021)</ref> adopt a cheap and simple one-shot sampling fashion, i.e., sampling the same subgraph constructed based on a mini-batch for different MP layers. By discarding messages outside the mini-batches, subgraphwise sampling methods restrict message passing to the mini-batches such that the complexity grows linearly with the number of MP layers. Moreover, subgraph-wise sampling methods are applicable to a wide range of GNN architectures by directly running GNNs on the subgraphs constructed by the sampled mini-batches <ref type="bibr" target="#b9">(Fey et al., 2021)</ref>. Because of these advantages, subgraph-wise sampling methods have recently drawn increasing attention.</p><p>Despite the empirical success of subgraph-wise sampling methods, discarding messages outside the mini-batch sacrifices the gradient estimation accuracy, which poses significant challenges to their convergence behaviors. First, recent works <ref type="bibr" target="#b1">(Chen et al., 2018a;</ref><ref type="bibr" target="#b5">Cong et al., 2020)</ref> demonstrate that the inaccurate mini-batch gradients seriously hurt the convergence speeds of GNNs. Second, in Section 7.3, we demonstrate that many subgraph-wise sampling methods are difficult to resemble full-batch performance under small batch sizes, which we usually use to avoid running out of GPU memory in practice. These issues seriously limit the real-world applications of GNNs.</p><p>In this paper, we propose a novel subgraph-wise sampling method with a convergence guarantee, namely Local Message Compensation (LMC), which uses efficient and effective compensations to correct the biases of mini-batch gradients and thus accelerates convergence. To the best of our knowledge, LMC is the first subgraph-wise sampling method with provable convergence. Specifically, we first propose unbiased mini-batch gradients for the one-shot sampling fashion, which helps decompose the gradient computation errors into two components: the bias from the discarded messages and the variance of the unbiased mini-batch gradients. Second, based on a message passing formulation of backward passes, we retrieve the messages discarded by existing subgraph-wise sampling methods during the approximation to the unbiased mini-batch gradients. Finally, we propose efficient and effective compensations for the discarded messages with a combination of incomplete up-to-date messages and messages generated from historical information in previous iterations, avoiding the exponentially growing time and memory consumption. An appealing feature of the resulting mechanism is that it can effectively correct the biases of mini-batch gradients, leading to accurate gradient estimation and the speed-up of convergence. We further show that LMC converges to first-order stationary points of GNNs. Notably, the convergence of LMC is based on the interactions between mini-batch nodes and their 1-hop neighbors, without the recursive expansion of neighborhoods to aggregate information far away from the mini-batches. Experiments on largescale benchmark tasks demonstrate that LMC significantly outperforms state-of-the-art subgraphwise sampling methods in terms of efficiency. Moreover, under small batch sizes, LMC outperforms the baselines and resembles the prediction performance of full-batch methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this section, we discuss some works related to our proposed method.</p><p>Subgraph-wise Sampling Methods. Subgraph-wise sampling methods sample a mini-batch and then construct the same subgraph based on it for different MP layers <ref type="bibr" target="#b19">(Ma &amp; Tang, 2021)</ref>. For example, Cluster-GCN <ref type="bibr" target="#b4">(Chiang et al., 2019)</ref> and GraphSAINT <ref type="bibr" target="#b28">(Zeng et al., 2020)</ref> construct the subgraph induced by a sampled mini-batch. They encourage connections between the sampled nodes by graph clustering methods (e.g., METIS <ref type="bibr" target="#b16">(Karypis &amp; Kumar, 1998)</ref> and <ref type="bibr">Graclus (Dhillon et al., 2007)</ref>), edge, node, or random-walk-based samplers. GNNAutoScale (GAS) <ref type="bibr" target="#b9">(Fey et al., 2021)</ref> and MVS-GNN <ref type="bibr" target="#b5">(Cong et al., 2020)</ref> use historical embeddings to generate messages outside a sampled subgraph, maintaining the expressiveness of the original GNNs. they are efficient in training and inference, they are not applicable to powerful GNNs with a trainable aggregation process.</p><p>Historical Values as an Affordable Approximation. The historical values are affordable approximations of the exact values in practice. However, they suffer from frequent data transfers to/from the GPU and the staleness problem. For example, in node-wise sampling, VR-GCN <ref type="bibr" target="#b1">(Chen et al., 2018a)</ref> uses historical embeddings to reduce the variance from neighbor sampling <ref type="bibr" target="#b12">(Hamilton et al., 2017)</ref>. GAS <ref type="bibr" target="#b9">(Fey et al., 2021)</ref> proposes a concurrent mini-batch execution to transfer the active historical embeddings to and from the GPU, leading to comparable runtime with the standard full-batch approach. GraphFM-IB and GraphFM-OB <ref type="bibr" target="#b27">(Yu et al., 2022)</ref> apply a momentum step on historical embeddings for node-wise and subgraph-wise sampling methods with historical embeddings, respectively, to alleviate the staleness problem. Both LMC and GraphFM-OB use the node embeddings in the mini-batch to alleviate the staleness problem of the node embeddings outside the mini-batch. We discuss the main differences between LMC and GraphFM-OB in Appendix C.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRELIMINARIES</head><p>We introduce notations and graph neural networks in Sections 3.1 and 3.2, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">NOTATIONS</head><p>A graph G = (V, E) is defined by a set of nodes V = {v1, v2, . . . , vn} and a set of edges E among these nodes. The set of nodes consists of labeled nodes VL and unlabeled nodes VU := V \ VL. Let (vi, vj) ? E denote an edge going from node vi ? V to node vj ? V, N (vi) = {vj ? V|(vi, vj) ? E} denote the neighborhood of node vi, and N (vi) denote N (vi) ? {vi}. We assume that G is undirected, i.e., vj ? N (vi) ? vi ? N (vj). Let N (S) = {v ? V|(vi, vj) ? E, vi ? S} denote the neighborhoods of a set of nodes S and N (S) denote N (S) ? S. For a positive integer L, [L] denotes {1, . . . , L}. Let the boldface character xi ? R dx denote the feature of node vi with dimension dx. Let hi ? R d be the d-dimensional embedding of the node vi. Let X = (x1, x2, . . . , xn) ? R dx?n and H = (h1, h2, . . . , hn) ? R d?n . We also denote the embeddings of a set of nodes</p><formula xml:id="formula_0">S = {vi k } |S| k=1 by HS = (hi k ) |S| k=1 ? R d?|S| . For a p ? q matrix A ? R p?q , A ? R pq</formula><p>denotes the vectorization of A, i.e., Aij = A i+(j-1)p . We denote the j-th columns of A by Aj.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">GRAPH NEURAL NETWORKS</head><p>For the semi-supervised node-level prediction, Graph Neural Networks (GNNs) aim to learn node embeddings H with parameters ? by minimizing the objective function L = 1</p><formula xml:id="formula_1">|V L | i?V L w (hi, yi)</formula><p>such that H = GN N (X, E; ?), where w is the composition of an output layer with parameters w and a loss function.</p><p>GNNs follow the message passing framework in which vector messages are exchanged between nodes and updated using neural networks. An L-layer GNN performs L message passing iterations with different parameters ? = (? l ) L l=1 to generate the final node embeddings</p><formula xml:id="formula_2">H = H L as H l = f ? l (H l-1 ; X), l ? [L],<label>(1)</label></formula><p>where H 0 = X and f ? l is the message passing function of the l-th layer with parameters ? l .</p><p>The message passing function f ? l follows an aggregation and update scheme, i.e.,</p><formula xml:id="formula_3">h l i = u ? l h l-1 i , m l-1 N (vi) , x i ; m l-1 N (vi) = ? ? l g ? l (h l-1 j ) | v j ? N (v i ) , l ? [L],<label>(2)</label></formula><p>where g ? l is the function generating individual messages for each neighbor of vi in the l-th message passing iteration, ? ? l is the aggregation function mapping a set of messages to the final message m l-1 N (v i ) , and u ? l is the update function that combines previous node embedding h l-1 i , message</p><formula xml:id="formula_4">m l-1 N (v i )</formula><p>, and features xi to update node embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">MESSAGE PASSING IN BACKWARD PASSES</head><p>In Section 4.1, we introduce the gradients of GNNs and formulate the backward passes as message passing. Then we propose backward SGD, which is an SGD variant, in Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">BACKWARD PASSES AND MESSAGE PASSING FORMULATION</head><p>The gradient ?wL is easy to compute and we hence introduce the chain rule to compute ??L in this section, where</p><formula xml:id="formula_5">? = (? l ) L l=1 . Let V l ? H l L for l ? [L] be auxiliary variables. It is easy to compute V L = ? H L L = ? H L.</formula><p>By the chain rule, we iteratively compute V l based on V l+1 as</p><formula xml:id="formula_6">V l = ? ? l+1 (V l+1 ) (? H l f ? l+1 ) V l+1<label>(3)</label></formula><p>and</p><formula xml:id="formula_7">V l = ? ? l+1 ? ? ? ? ? ? ? L (V L ).<label>(4)</label></formula><p>Then, we compute the gradient</p><formula xml:id="formula_8">? ? l L = (? ? l f ? l ) V l , l ? [L]</formula><p>by using autograd packages for vector-Jacobian product.</p><p>We formulate backward passes, i.e., the processes of iterating Equation (3), as message passing. To see this, we need to notice that Equation ( <ref type="formula" target="#formula_6">3</ref>) is equivalent to</p><formula xml:id="formula_9">V l i = vj ?N (vi) ? h l i u ? l+1 (h l j , m l N (vj ) , x j ) V l+1 j , i ? [n],<label>(5)</label></formula><p>where</p><formula xml:id="formula_10">V l k is the k-th column of V l and m l N (v j ) is a function of h l i defined in Equation (2). Equa- tion (5) uses ? h l i u ? l+1 (h l j , m l N (v j ) , xj) V l+1 j</formula><p>, sum aggregation, and the identity mapping as the generation function, the aggregation function, and the update function, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">BACKWARD SGD</head><p>In this section, we develop an SGD variant-backward SGD, which provides unbiased gradient estimations based on the message passing formulation of backward passes. Backward SGD is the basis of our proposed subgraph-wise sampling method, i.e., LMC, in Section 5.</p><p>Given a sampled mini-batch VB, suppose that we have computed exact node embeddings (H l</p><formula xml:id="formula_11">V B ) L l=1 and auxiliary variables (V l V B ) L l=1 of nodes in VB.</formula><p>To simplify the analysis, we assume that VB is uniformly sampled from V and the corresponding set of labeled nodes VL B := VB ? VL is uniformly sampled from VL. When the sampling is not uniform, we use the normalization technique <ref type="bibr" target="#b28">(Zeng et al., 2020)</ref> to enforce the assumption (please see Appendix A.3.1).</p><p>First, backward SGD computes the mini-batch gradient gw(VB) for parameters w by the derivative of mini-batch loss LV B = 1</p><formula xml:id="formula_12">|V L B | v j ?V L B</formula><p>w (hj, yj) as</p><formula xml:id="formula_13">g w (V B ) = 1 |V L B | vj ?V L B ? w w (h j , y j ).<label>(6)</label></formula><p>Then, backward SGD computes the mini-batch gradient g ? l (VB) for parameters ? l as</p><formula xml:id="formula_14">g ? l (V B ) = |V| |V B | vj ?V B ? ? l u ? l (h l-1 j , m l-1 N (vj ) , x j ) V l j , l ? [L].<label>(7)</label></formula><p>Note that the mini-batch gradients g ? l (VB) for different l ? [L] are based on the same mini-batch VB, which facilitates designing subgraph-wise sampling methods based on backward SGD. Another appealing feature of backward SGD is that the mini-batch gradients gw(VB) and g ? l (VB), l ? [L] are unbiased, as shown in the following theorem. Please see Appendix D.1 for the detailed proof. Theorem 1. Suppose that a mini-batch VB is uniformly sampled from V and the corresponding labeled nodes VL B = VB ? VL is uniformly sampled from VL. Then the mini-batch gradients gw(VB) and g ? l (VB), l ? [L] in Equations ( <ref type="formula" target="#formula_13">6</ref>) and (7) are unbiased.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">LOCAL MESSAGE COMPENSATION</head><p>The exact mini-batch gradients gw(VB) and g ? l (VB), l ? [L] computed by backward SGD depend on exact embeddings and auxiliary variables of nodes in the mini-batch VB rather than the whole graph. However, backward SGD is not scalable, as the exact (H l</p><formula xml:id="formula_15">V B ) L l=1 and (V l V B ) L</formula><p>l=1 are expensive to compute due to the neighbor explosion problem.</p><p>In this section, to deal with the neighbor explosion problem, we develop a novel and scalable subgraph-wise sampling method for GNNs, namely Local Message Compensation (LMC). LMC first efficiently estimates (H l</p><formula xml:id="formula_16">V B ) L l=1 and (V l V B ) L</formula><p>l=1 by convex combinations of the incomplete upto-date values and the historical values, and then computes the mini-batch gradients as shown in </p><formula xml:id="formula_17">v 4 v 1 v 3 v 2 v 5 (a) Original graph v 3 v 4 v 1 v 2 v 3 v 4 v 1 v 2 v 3 v 4 v 1 v 2 Layer 2 Layer 1 Layer 0 (b) Forward passes of GAS v 3 v 4 v 1 v 2 v 3 v 4 v 1 v 2 v 3 v 4 v 1 v 2</formula><formula xml:id="formula_18">v 3 v 4 v 1 v 2 v 3 v 4 v 1 v 2 v 3 v 4 v 1 v 2 Layer 2</formula><p>Layer 1</p><p>Layer 0</p><p>(d) Backward passes of GAS</p><formula xml:id="formula_19">v 3 v 4 v 1 v 2 v 3 v 4 v 1 v 2 v 3 v 4 v 1 v 2 Layer 2</formula><p>Layer 0</p><p>Layer 1 (e) Backward passes of LMC Figure <ref type="figure">1</ref>: Comparison of LMC with GNNAutoScale (GAS) <ref type="bibr" target="#b9">(Fey et al., 2021)</ref>. (a) shows the original graph with in-batch nodes, 1-hop out-of-batch nodes, and other out-of-batch nodes in orange, blue, and grey, respectively. (b) and (d) show the computation graphs of forward passes and backward passes of GAS, respectively. (c) and (e) show the computation graphs of forward passes and backward passes of LMC, respectively.</p><p>Equations ( <ref type="formula" target="#formula_13">6</ref>) and ( <ref type="formula" target="#formula_14">7</ref>). We show that LMC converges to first-order stationary points of GNNs in Section 6. In Algorithm 1 and Section 6, we denote a value in the l-th layer at the k-th iteration by In forward passes, we initialize the temporary embeddings for l = 0 as H 0 = X and update historical embeddings of nodes in VB, i.e., H l V B , in the order of l = 1, 2, . . . , L. Specifically, in the l-th layer, we first update the historical embedding of each node vi ? VB as</p><formula xml:id="formula_20">h l i = u ? l (h l-1 i , m l-1 N (vi) , x i ); m l-1 N (vi) = ? ? l g ? l (h l-1 j ) | v j ? N (v i ) ? V B ? g ? l ( h l-1 j ) | v j ? N (v i ) \ V B . (8)</formula><p>Then, we compute the temporary embedding of each neighbor vi ? N (VB) \ VB as</p><formula xml:id="formula_21">h l i = (1 -? i )h l i + ? i h l i ,<label>(9)</label></formula><p>where ?i ? [0, 1] is the convex combination coefficient for node v i , and</p><formula xml:id="formula_22">h l i = u ? l ( h l-1 i , m l-1 N (vi) , x i ); m l-1 N (vi) = ? ? l g ? l (h l-1 j ) | v j ? N (v i ) ? V B ? g ? l ( h l-1 j ) | v j ? N (V B ) ? N (v i ) \ V B . (<label>10</label></formula><formula xml:id="formula_23">) We call C l f ? ? l g ? l ( h l-1 j ) | vj ? N (vi) \ VB</formula><p>the local message compensation in the l-th layer in forward passes. For l ? [L], h l i is an approximation to h l i computed by Equation (2). Notice that the total size of Equations ( <ref type="formula">8</ref>)-( <ref type="formula" target="#formula_22">10</ref>) is linear with |N (VB)| rather than the size of the whole graph. Suppose that the maximum neighborhood size is nmax and the number of layers is L, then the time complexity in forward passes is O(L(nmax|VB|d + |VB|d 2 )).</p><p>In backward passes, we initialize the temporary auxiliary variables for l = L as V L = ? H L and update historical auxiliary variables of nodes in VB, i.e., V l V B , in the order of l = L -1, . . . , 1.</p><p>Specifically, in the l-th layer, we first update the historical auxiliary variable of each vi ? VB as</p><formula xml:id="formula_24">V l i = vj ?N (vi)?V B ? h l i u ? l+1 (h l j , m l N (vj ) , x j ) V l+1 j + vj ?N (vi)\V B ? h l i u ? l+1 ( h l j , m l N (vj ) , x j ) V l+1 j ,<label>(11)</label></formula><p>where h l j , m l N (v j ) , and h l j are computed as shown in Equations ( <ref type="formula">8</ref>)-( <ref type="formula" target="#formula_22">10</ref>). Then, we compute the temporary auxiliary variable of each neighbor vi ? N (VB) \ VB as</p><formula xml:id="formula_25">V l i = (1 -? i )V l i + ? i V l i , (<label>12</label></formula><formula xml:id="formula_26">)</formula><p>where ?i is the convex combination coefficient used in Equation ( <ref type="formula" target="#formula_21">9</ref>), and</p><formula xml:id="formula_27">V l i = vj ?N (vi)?V B ? h l i u ? l+1 (h l j , m l N (vj ) , x j ) V l+1 j + vj ?N (V B )?N (vi)\V B ? h l i u ? l+1 ( h l j , m l N (vj ) , x j ) V l+1 j . (<label>13</label></formula><formula xml:id="formula_28">) We call C l b v j ?N (v i )\V B ? h l i u ? l+1 ( h l j , m l N (v j ) , xj) V l+1 j the local message compensation in the l-th layer in backward passes. For l ? [L], V l i is an approximation to V l i computed by Equation (3). Similar to forward passes, the time complexity in backward passes is O(L(nmax|VB|d + |VB|d 2 )),</formula><p>where nmax is the maximum neighborhood size and L is the number of layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Local Message Compensation</head><p>1: Input: The learning rate ? and the convex combination coefficients (?i</p><formula xml:id="formula_29">) n i=1 . 2: Partition V into B parts (V b ) B b=1 3: for k = 1, . . . , N do 4: Randomly sample V b k from (V b ) B b=1 5: Initialize H 0,k = H 0,k = X 6: for l = 1, . . . , L do 7: Update H l,k V b k (8) 8: Compute H l,k N (V b k )\V b k</formula><p>(9) and ( <ref type="formula" target="#formula_22">10</ref>)</p><formula xml:id="formula_30">9:</formula><p>end for 10:</p><formula xml:id="formula_31">Initialize V L,k = V L,k = ? H L L 11: for l = L -1, . . . , 1 do 12: Update V l,k V b k (11) 13: Compute V l,k N (V b k )\V b k</formula><p>(12) and ( <ref type="formula" target="#formula_27">13</ref>) 14:</p><p>end for</p><formula xml:id="formula_32">15: Compute g k w and g k ? l , l ? [L]</formula><p>(6) and ( <ref type="formula" target="#formula_14">7</ref>)</p><p>16:</p><p>Update parameters by 17:</p><formula xml:id="formula_33">w k = w k-1 -? g k w 18: ? l,k = ? l,k-1 -? g k ? l , l ? [L]</formula><p>19: end for LMC additionally stores the historical node embeddings H l and auxiliary vari-</p><formula xml:id="formula_34">ables V l for l ? [L].</formula><p>As pointed out in <ref type="bibr" target="#b9">(Fey et al., 2021)</ref>, we can store the majority of historical values in RAM or hard drive storage rather than GPU memory. Thus, the active historical values in forward and backward passes employ O(nmaxL|VB|d) and O(nmaxL|VB|d) GPU memory, respectively (see Appendix B). As the time and memory complexity are independent of the size of the whole graph, i.e., |V|, LMC is scalable. We summarize the computational complexity in Appendix B.</p><p>Figure <ref type="figure">1</ref> shows the message passing mechanisms of GAS <ref type="bibr" target="#b9">(Fey et al., 2021)</ref> and LMC. Compared with GAS, LMC proposes compensation messages between inbatch nodes and their 1-hop neighbors simultaneously in forward and backward passes. This corrects the biases of minibatch gradients and thus accelerates convergence.</p><p>Algorithm 1 summarizes LMC. Unlike above, we add a superscript k for each value to indicate that it is the value at the k-th iteration. At preprocessing step, we partition V into B parts (V b ) B b=1 . At the k-th training step, LMC first randomly samples a subgraph constructed by V b k . Notice that we sample more subgraphs to build a large graph in experiments whose convergence analysis is consistent with that of sampling a single subgraph. Then, LMC updates the stored historical node embeddings H l,k V b k in the order of l = 1, . . . , L by Equations ( <ref type="formula">8</ref>)-( <ref type="formula" target="#formula_22">10</ref>), and the stored historical auxiliary variables V l,k V b k in the order of l = L -1, . . . , 1 by Equations ( <ref type="formula" target="#formula_24">11</ref>)-( <ref type="formula" target="#formula_27">13</ref>). By the randomly updating, the historical values get close to the exact up-to-date values. Finally, for l ? [L] and vj ? V b k , by replacing h l,k j , m l,k N (v j ) and V l,k j in Equations ( <ref type="formula" target="#formula_13">6</ref>) and ( <ref type="formula" target="#formula_14">7</ref>) with h l,k j , m l,k N (v j ) , and V l,k j , respectively, LMC computes mini-batch gradients gw, g ? 1 , . . . , g ? L to update parameters w, ? 1 , . . . , ? L .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">THEORETICAL ANALYSIS</head><p>In this section, we provide the theoretical analysis of LMC. Theorem 2 shows that the biases of mini-batch gradients computed by LMC can tend to an arbitrarily small value by setting a proper learning rate and convex combination coefficients. Then, Theorem 3 shows that LMC converges to first-order stationary points of GNNs. We provide detailed proofs of the theorems in Appendix D. In the theoretical analysis, we suppose that the following assumptions hold in this paper. Assumption 1. Assume that (1) at the k-th iteration, a batch of nodes V k B is uniformly sampled from V and the corresponding labeled node set</p><formula xml:id="formula_35">V k L B = V k B ?VL is uniformly sampled from VL, (2) functions f ? l , ? ? l , ?wL, ? ? l L, ?w w , and ? ? l u ? l are ?-Lipschitz with ? &gt; 1, ? l ? [L], (3) norms H l,k F , H l,k F , H l,k F , H l,k F , V l,k F , V l,k F , V l,k F , V l,k F , ?wL 2, ? ? l L 2, g ? l 2, and gw 2 are bounded by G &gt; 1, ? l ? [L], k ? N * . Theorem 2. Suppose that Assumption 1 holds, then with ? = O(? 2 ) and ?i = O(? 2 ), i ? [n], there exists C &gt; 0 and ? ? (0, 1) such that E[ g w (w k ) -? w L(w k ) 2 ] ? C? + C? k-1 2 + Var(g w (w k )) 1 2 , ? k ? N * , E[ g ? l (? l,k ) -? ? l L(? l,k ) 2 ] ? C? + C? k-1 2 + Var(g ? l (? l,k )) 1 2 , ? l ? [L], k ? N * . Theorem 3. Suppose that Assumption 1 holds. Besides, assume that the optimal value L * = infw,? L(w, ?) is bounded by G. Then, with ? = O(? 4 ), ?i = O(? 4 ), i ? [n], and N = O(? -6 ), LMC ensures to find an ?-stationary solution such that E[ ? w,? L(w R , ? R ) 2 ] ? ? after running for N iterations, where R is uniformly selected from [N ] and ? R = (? l,R ) L l=1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">EXPERIMENTS</head><p>We introduce experimental settings in Section 7.1. We then evaluate the convergence and efficiency of LMC in Section 7.2. The experiments are carried out on a single GeForce RTX 2080 Ti (11 GB).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">EXPERIMENTAL SETTINGS</head><p>Datasets. Some recent works <ref type="bibr" target="#b14">(Hu et al., 2020)</ref> have indicated that many frequently-used graph datasets are too small compared with graphs in real-world applications. Therefore, we evaluate LMC on four large datasets, PPI, REDDIT , FLICKR <ref type="bibr" target="#b12">(Hamilton et al., 2017)</ref>, and Ogbn-arxiv <ref type="bibr" target="#b14">(Hu et al., 2020)</ref>. These datasets contain thousands or millions of nodes/edges and have been widely used in previous works <ref type="bibr" target="#b9">(Fey et al., 2021;</ref><ref type="bibr" target="#b28">Zeng et al., 2020;</ref><ref type="bibr" target="#b12">Hamilton et al., 2017;</ref><ref type="bibr" target="#b4">Chiang et al., 2019;</ref><ref type="bibr" target="#b1">Chen et al., 2018a;</ref><ref type="bibr" target="#b10">b)</ref>. For more details, please refer to Appendix A.1.</p><p>Baselines and Implementation Details. In terms of prediction performance, our baselines include node-wise sampling methods (GraphSAGE <ref type="bibr" target="#b12">(Hamilton et al., 2017)</ref> and VR-GCN <ref type="bibr" target="#b1">(Chen et al., 2018a)</ref>), layer-wise sampling method (FASTGCN <ref type="bibr">(Chen et al., 2018b)</ref> and LADIES <ref type="bibr" target="#b30">(Zou et al., 2019)</ref>), subgraph-wise sampling methods (CLUSTER-GCN <ref type="bibr" target="#b4">(Chiang et al., 2019)</ref>, GRAPHSAINT <ref type="bibr" target="#b28">(Zeng et al., 2020)</ref>, FM <ref type="bibr" target="#b27">(Yu et al., 2022)</ref>, and GAS <ref type="bibr" target="#b9">(Fey et al., 2021)</ref>), and a precomputing method (SIGN <ref type="bibr" target="#b24">(Rossi et al., 2020)</ref>). By noticing that GAS and FM achieve the state-of-the-art prediction performance (Table <ref type="table" target="#tab_2">1</ref>) among the baselines, we further compare the efficiency of LMC with GAS, FM, and CLUSTER-GCN, another subgraph-wise sampling method using METIS partition. We implement LMC, FM, and CLUSTER-GCN based on the codes and toolkits of GAS <ref type="bibr" target="#b9">(Fey et al., 2021)</ref> to ensure a fair comparison. For other implementation details, please refer to Appendix A.3.</p><p>Hyperparameters. To ensure a fair comparison, we follow the data splits, training pipeline, and most hyperparameters in <ref type="bibr" target="#b9">(Fey et al., 2021)</ref> except for the additional hyperparameters in LMC such as ? i . We use the grid search to find the best ? i (see Appendix A.4 for more details).</p><p>7.2 LMC IS FAST WITHOUT SACRIFICING ACCURACY Table <ref type="table" target="#tab_2">1</ref> reports the prediction performance of LMC and the baselines. We report the mean and the standard deviation by running each experiment five times for GAS, FM, and LMC. LMC, FM, and GAS all resemble full-batch performance on all datasets while other baselines may fail, especially on the FLICKR dataset. Moreover, LMC, FM, and GAS with deep GNNs, i.e., GCNII <ref type="bibr" target="#b3">(Chen et al., 2020)</ref> outperform other baselines on all datasets. As LMC, FM, and GAS share the similar prediction performance, we additionally compare the convergence speed of LMC, FM, GAS, and CLUSTER-GCN, another subgraph-wise sampling method using METIS partition, in Figure <ref type="figure" target="#fig_0">2</ref> and Table <ref type="table" target="#tab_3">2</ref>. We use a sliding window to smooth the convergence curve in Figure <ref type="figure" target="#fig_0">2</ref> as the accuracy on test data is unstable. The solid curves correspond to the mean, and the shaded regions correspond to values within plus or minus one standard deviation of the mean. To further illustrate the convergence of LMC, we compare the errors of mini-batch gradients computed by CLUSTER, GAS, and LMC. At epoch training step, we record the relative errors</p><formula xml:id="formula_36">g ? l -? ? l L 2 / ? ? l L 2,</formula><p>where ? ? l L is the full-batch gradient for the parameters ? l at the l-th MP layer and the g ? l is a mini-batch gradient. To avoid the randomness of the full-batch gradient ? ? l L, we set the dropout rate as zero. We report average relative errors during training in Figure <ref type="figure" target="#fig_1">3</ref>. LMC enjoys the smallest estimated errors in the experiments.   An appealing feature of mini-batch training methods is that they can avoid the out-of-memory issue by decreasing the batch size. Thus, we evaluate the prediction performance of LMC on Ogbn-arxiv datasets with different batch sizes (numbers of clusters). We conduct experiments under different sizes of sampled clusters per mini-batch. We run each experiment with the same epoch and search learning rates in the same set. We report the best prediction accuracy in Table <ref type="table" target="#tab_4">3</ref>. LMC outperforms GAS under small batch sizes (batch size = 1 or 2) and achieve comparable performance with GAS (batch size = 5 or 10).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">ABLATION</head><p>The improvement of LMC is due to two parts: the compensation in forward passes C l f and the compensation in back passes C l b . Compared with GAS, the compensation in forward passes C l f additionally combines the incomplete up-to-date messages. Figure <ref type="figure" target="#fig_2">4</ref> shows the convergence curves of LMC using both C l f and C l b (denoted by C f &amp;C b ), LMC using only C l f (denoted by C f ), and GAS on the Ogbn-arxiv dataset. Under small batch sizes, the improvement mainly is due to C l b and the incomplete up-to-date messages in forward passes may hurt the performance. This is because the mini-batch and the union of their neighbors are hard to contain most neighbors of out-of-batch nodes when the batch size is small. Thus, the compensation in back passes C l b is the most important component by correcting the bias of the mini-batch gradients. Under large batch sizes, the improvement is due to C l f , as the large batch sizes decrease the discarded messages and improve the accuracy of the mini-batch gradients (see Table <ref type="table">7</ref> in Appendix). Notably, C l b still slightly improves the performance. We provide more ablation studies about ?i in Appendix E.4. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>In this paper, we propose a novel subgraph-wise sampling method with a convergence guarantee, namely Local Message Compensation (LMC). LMC uses efficient and effective compensations to correct the biases of mini-batch gradients and thus accelerates convergence. We show that LMC converges to first-order stationary points of GNNs. To the best of our knowledge, LMC is the first subgraph-wise sampling method for GNNs with provable convergence. Experiments on large-scale benchmark tasks demonstrate that LMC significantly outperforms state-of-the-art subgraph-wise sampling methods in terms of efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A MORE DETAILS ABOUT EXPERIMENTS</head><p>In this section, we introduce more details about our experiments, including datasets, training and evaluation protocols, and implementations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 DATASETS</head><p>We evaluate LMC on four large datasets, PPI, REDDIT, FLICKR <ref type="bibr" target="#b12">(Hamilton et al., 2017)</ref>, and Ogbnarxiv <ref type="bibr" target="#b14">(Hu et al., 2020)</ref>. All of the datasets do not contain personally identifiable information or offensive content. Table <ref type="table" target="#tab_5">4</ref> shows the summary statistics of the datasets. Details about the datasets are as follows.</p><p>? PPI contains 24 protein-protein interaction graphs. Each graph corresponds to a human tissue. Each node indicates a protein with positional gene sets, motif gene sets and immunological signatures as node features. Edges represent interactions between proteins. The task is to classify protein functions. ? REDDIT is a post-to-post graph constructed from REDDIT. Each node indicates a post and each edge between posts indicates that the same user comments on both. The task is to classify REDDIT posts into different communities based on (1) the GloVe CommonCrawl word vectors <ref type="bibr" target="#b23">(Pennington et al., 2014)</ref> of the post titles and comments, (2) the post's scores, and (3) the number of comments made on the posts. ? Ogbn-arxiv is a directed citation network between all Computer Science (CS) arXiv papers indexed by MAG <ref type="bibr" target="#b25">(Wang et al., 2020)</ref>. Each node is an arXiv paper and each directed edge indicates that one paper cites another one. The task is to classify unlabeled arXiv papers into different primary categories based on labeled papers and node features, which are computed by averaging word2vec <ref type="bibr" target="#b20">(Mikolov et al., 2013)</ref> embeddings of words in papers' title and abstract. ? FLICKR categorizes types of images based on their descriptions and properties <ref type="bibr" target="#b9">(Fey et al., 2021;</ref><ref type="bibr" target="#b28">Zeng et al., 2020)</ref>. We run all the experiments on a single GeForce RTX 2080 Ti (11 GB). All the models are implemented in Pytorch <ref type="bibr" target="#b22">(Paszke et al., 2019)</ref> and PyTorch Geometric <ref type="bibr" target="#b8">(Fey &amp; Lenssen, 2019)</ref> based on the official implementation of <ref type="bibr" target="#b9">(Fey et al., 2021)</ref> <ref type="foot" target="#foot_0">1</ref> . We will release our code once the paper is accepted to be published.</p><p>Data Splitting. We use the data splitting strategies following previous works <ref type="bibr" target="#b9">(Fey et al., 2021;</ref><ref type="bibr" target="#b11">Gu et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 IMPLEMENTATION DETAILS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.1 NORMALIZATION TECHNIQUE</head><p>In Section 4.2 in the main text, we assume that the subgraph V B is uniformly sampled from V and the corresponding set of labeled nodes VL B = VB ? VL is uniformly sampled from VL. To enforce the assumption, we use the normalization technique to reweight Equations ( <ref type="formula" target="#formula_13">6</ref>) and ( <ref type="formula" target="#formula_14">7</ref>) in the main text.</p><p>Suppose we partition the whole graph V into b parts {V Bi } b i=1 and then uniformly sample c clusters without replacement to construct subgraph V B . By the normalization technique, Equation ( <ref type="formula" target="#formula_13">6</ref>) becomes</p><formula xml:id="formula_37">g w (V B ) = b|V L B | c|V L | 1 |V L B | vj ?V L B ? w w (h j , y j ),<label>(14)</label></formula><p>where</p><formula xml:id="formula_38">b|V L B | c|V L |</formula><p>is the corresponding weight. Similarly, Equation ( <ref type="formula" target="#formula_14">7</ref>) becomes</p><formula xml:id="formula_39">g ? (V B ) = b|V B | c|V| |V| |V B | vj ?V B ? ? u(h j , m N (vj ) , x j )V j ,<label>(15)</label></formula><p>where b|V B | c|V| is the corresponding weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.2 INCORPORATING BATCH NORMALIZATION</head><p>We uniformly sample a mini-batch of nodes V B and generate the induced subgraph of N (V B ). If we directly feed the H (l)</p><p>N (V B ) to a batch normalization layer, the learned mean and standard deviation of the batch normalization layer may be biased. Thus, LMC first feeds the embeddings of the minibatch H (l)</p><p>V B to a batch normalization layer and then feeds the embeddings outside the mini-batch H (l) N (V B )\V B to another batch normalization layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 SELECTION OF ? i</head><p>We select ? i = score(i)? for each node v i , where ? ? [0, 1] is a hyperparameter and score is a function to measure the quality of the incomplete up-to-date messages. We search score in a {f </p><formula xml:id="formula_40">(x) = x 2 , f (x) = 2x -x 2 , f (x) = x, f (x) = 1; x = deg local (i)/deg global (i)},</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B COMPUTATIONAL COMPLEXITY</head><p>We summarize the computational complexity in Table <ref type="table">5</ref>, where n max is the maximum of neighborhoods, L is the number of message passing layers, V B is a set of nodes in a sampled mini-batch, d is the embedding dimension, V is the set of nodes in the whole graph, and E is the set of edges in the whole graph. As GD, backward SGD, CLUSTER, GAS, and LMC share the same memory complexity of parameters ? (l) , we omit it in Table <ref type="table">5</ref>.</p><p>Table <ref type="table">5</ref>: Time and memory complexity per gradient update of message passing based GNNs (e.g. GCN <ref type="bibr" target="#b18">(Kipf &amp; Welling, 2017)</ref> and GCNII <ref type="bibr" target="#b3">(Chen et al., 2020)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Time Memory</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GD and backward SGD O(L(|E|d</head><formula xml:id="formula_41">+ |V|d 2 )) O(L|V|d) CLUSTER (Chiang et al., 2019) O(L(n max |V B |d + |V B |d 2 ))</formula><p>O(L|V B |d) GAS <ref type="bibr" target="#b9">(Fey et al., 2021)</ref> O(L(</p><formula xml:id="formula_42">n max |V B |d + |V B |d 2 )) O(n max L|V B |d) LMC O(L(n max |V B |d + |V B |d 2 )) O(n max L|V B |d) C ADDITIONAL RELATED WORK C.1 MAIN DIFFERENCES BETWEEN LMC AND GRAPHFM</formula><p>? First, LMC focuses on the convergence of subgraph-wise sampling methods, which is orthogonal to the idea of GraphFM-OB to alleviate the staleness problem of historical values. The advanced approach to alleviating the staleness problem of historical values can further improve the performance of LMC and it is easy to establish provable convergence by the extension of LMC.</p><p>? Second, LMC uses nodes in both mini-batches and their 1-hop neighbors to compute incomplete up-to-date messages. In contrast, GraphFM-OB only uses nodes in the mini-batches. For the nodes whose neighbors are contained in the union of the nodes in mini-batches and their 1-hop neighbors, the aggregation results of LMC are exact, while those of GraphFM-OB are not.</p><p>? Third, by noticing that aggregation results are biased and the I/O bottleneck for the history access, LMC does not update the historical values in the storage for nodes outside the mini-batches. However, GraphFM-OB updates them based on the aggregation results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D DETAILED PROOFS</head><p>Notations. Unless otherwise specified, C and C with any superscript or subscript denotes constants.</p><p>We denote the learning rate by ?.</p><p>In this section, we suppose that Assumption 1 holds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 PROOF OF THEOREM 1: UNBIASED MINI-BATCH GRADIENTS OF BACKWARD SGD</head><p>In this subsection, we give the proof of Theorem 1, which shows that the mini-batch gradients computed by backward SGD are unbiased.</p><p>Proof.</p><formula xml:id="formula_43">As V L B = V B ? V L is uniformly sampled from V L , the expectation of g w (V B ) is E[g w (V B )] = E[ 1 |V L B | vj ?V L B ? w w (h j , y j )] = ? w E[ w (h j , y j )] = ? w L.</formula><p>As the subgraph V B is uniformly sampled from V, the expectation of g ? l (V B ) is</p><formula xml:id="formula_44">E[g ? l (V B )] = E[ |V| |V B | vj ?V B ? ? l u ? l (h l-1 j , m l-1 N (vj ) , x j ) V l j ] = |V|E[? ? l u ? l (h l-1 j , m l-1 N (vj ) , x j )V l j ] = |V| 1 |V| vj ?V ? ? l u ? l (h l-1 j , m l-1 N (vj ) , x j )V l j = vj ?V ? ? l u ? l (h l-1 j , m l-1 N (vj ) , x j )V l j = ? ? l L, ? l ? [L].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 DIFFERENCES BETWEEN EXACT VALUES AT ADJACENT ITERATIONS</head><p>We first show that the differences between the exact values of the same layer in two adjacent iterations can be bounded by setting a proper learning rate. Lemma 1. Suppose that Assumption 1 holds. Given an L-layer GNN, for any ? &gt; 0, by letting</p><formula xml:id="formula_45">? ? ? (2?) L G &lt; ?,</formula><p>we have</p><formula xml:id="formula_46">H l,k+1 -H l,k F &lt; ?, ? l ? [L], k ? N * . Proof. Since ? ? ? (2?) L G &lt; ? ?(2?) L-1 G , we have H 1,k+1 -H 1,k F = f ? 1,k+1 (X) -f ? 1,k (X) F ? ? ? 1,k+1 -? 1,k ? ? g ? 1 ? &lt; ?G? ?(2?) L-1 G = ? (2?) L-1 . Then, because ? ? ? (2?) L G &lt; ? (2?) L-1 G , we have H 2,k+1 -H 2,k F = f ? 2,k+1 (H 1,k+1 ) -f ? 2,k (H 1,k ) F ? f ? 2,k+1 (H 1,k+1 ) -f ? 2,k (H 1,k+1 ) F + f ? 2,k (H 1,k+1 ) -f ? 2,k (H 1,k ) F ? ? ? 2,k+1 -? 2,k + ? H 1,k+1 -H 1,k F ? ?G? + ? 2(2?) L-2 &lt; ? 2(2?) L-2 + ? 2(2?) L-2 = ? (2?) L-2 .</formula><p>And so on, we have</p><formula xml:id="formula_47">H l,k+1 -H l,k F &lt; ? (2?) L-l , ? l ? [L], k ? N * . Since (2?) L-l &gt; 1, we have H l,k+1 -H l,k F &lt; ?, ? l ? [L], k ? N * .</formula><p>Lemma 2. Suppose that Assumption 1 holds. Given an L-layer GNN, for any ? &gt; 0, by letting</p><formula xml:id="formula_48">? ? ? (2?) L-1 G &lt; ?, we have V l,k+1 -V l,k F &lt; ?, ? l ? [L], k ? N * . Proof. Since ? ? ? (2?) L-1 G &lt; ? ?(2?) L-2 G , we have V L-1,k+1 -V L-1,k F = ? ? L,k+1 (? H L) -? ? L,k (? H L) F ? ? ? L,k+1 -? L,k ? ? g ? L ? &lt; ?G? ?(2?) L-2 G = ? (2?) L-2 . Then, because ? ? ? (2?) L-1 G &lt; ? (2?) L-2 G , we have V L-2,k+1 -V L-2,k F = ? ? L-1,k+1 (V L-1,k+1 ) -? ? L-1,k (V L-1,k ) F ? ? ? L-1,k+1 (V L-1,k+1 ) -? ? L-1,k (V L-1,k+1 ) F + ? ? L-1,k (V L-1,k+1 ) -? ? L-1,k (V L-1,k ) F ? ? ? L-1,k+1 -? L-1,k + ? V L-1,k+1 -V L-1,k F ? ?G? + ? 2(2?) L-3 &lt; ? 2(2?) L-3 + ? 2(2?) L-3 = ? (2?) L-3 .</formula><p>And so on, we have</p><formula xml:id="formula_49">V l,k+1 -V l,k F &lt; ? (2?) l-1 , ? l ? [L], k ? N * . Since (2?) l-1 &gt; 1, we have V l,k+1 -V l,k F &lt; ?, ? l ? [L], k ? N * .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 HISTORICAL VALUES AND TEMPORARY VALUES</head><p>Suppose that we uniformly sample a mini-batch</p><formula xml:id="formula_50">V k B ? V at the k-th iteration and |V k B | = S.</formula><p>For the simplicity of notations, we denote the temporary node embeddings and auxiliary variables in the l-th layer by H l,k and V l,k , respectively, where</p><formula xml:id="formula_51">H l,k i = h l,k i , v i ? N (V k B ) \ V k B , h l,k i , otherwise, and V l,k i = v l,k i , v i ? N (V k B ) \ V k B , v l,k i , otherwise.</formula><p>We abbreviate the process that LMC updates the node embeddings and auxiliary variables of V k B in the l-th layer at the k-th iteration as</p><formula xml:id="formula_52">H l,k V k B = [f ? l,k ( H l-1,k )] V k B , V l,k V k B = [? ? l+1,k ( H l+1,k )] V k B .</formula><p>For each v i ? V k B , the update process of v i in the l-th layer at the k-th iteration can be expressed by</p><formula xml:id="formula_53">h l,k i = f ? l,k ,i ( H l-1,k ), V l,k i = ? ? l+1,k ,i ( V l+1,k</formula><p>), where f ? l,k ,i and ? ? l+1,k ,i are the components for node v i of f ? l,k and ? ? l+1,k , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3.1 CONVEX COMBINATION COEFFICIENTS</head><p>We first focus on convex combination coefficients ? i , i ? [n]. For the simplicity of analysis, we assume</p><formula xml:id="formula_54">? i = ? for i ? [i].</formula><p>The analysis of the case where (? i ) n i=1 are different from each other is the same. Lemma 3. Suppose that Assumption 1 holds. For any ? &gt; 0, by letting</p><formula xml:id="formula_55">? ? ? 2G , ? l ? [L], i ? [n],</formula><p>we have</p><formula xml:id="formula_56">H l,k -H l,k F ? H l,k -H l,k F + ?, ? l ? [L], k ? N * . Proof. Since H l,k = (1 -?)H l,k + ? H l,k , we have H l,k -H l,k F = (1 -?)H l,k + ? H l,k -(1 -?)H l,k + ?H l,k F ? (1 -?) H l,k -H l,k F + ? H l,k -H l,k F ? H l,k -H l,k F + 2?G. Hence letting ? ? ? 2G leads to H l,k -H l,k F ? H l,k -H l,k F + ?.</formula><p>Lemma 4. Suppose that Assumption 1 holds. For any ? &gt; 0, by letting</p><formula xml:id="formula_57">? ? ? 2G , ? l ? [L], i ? [n],</formula><p>we have</p><formula xml:id="formula_58">V l,k -V l,k F ? V l,k -V l,k F + ?. Proof. Since H l,k = (1 -?)H l,k + ? H l,k , we have H l,k -H l,k = (1 -?)H l,k + ? H l,k -(1 -?)H l,k + ?H l,k F ? (1 -?) H l,k -H l,k F + ? H l,k -H l,k F ? H l,k -H l,k F + 2?G. Hence letting ? ? ? 2G leads to H l,k -H l,k F ? H l,k -H l,k F + ?.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3.2 APPROXIMATION ERRORS OF HISTORICAL VALUES</head><p>Next, we focus on the approximation errors of historical node embeddings and auxiliary variables</p><formula xml:id="formula_59">d l,k h := E[ H l,k -H l,k 2 F ] 1 2 , l ? [L], d l,k v := E[ V l,k -V l,k 2 F ] 1 2 , l ? [L -1].</formula><p>Lemma 5. For an L-layer GNN, suppose that Assumption 1 holds. Besides, we suppose that</p><formula xml:id="formula_60">1. (d l,1 h ) 2 is bounded by G &gt; 1, ? l ? [L], 2. there exists N ? N * such that H l,k -H l,k F ? H l,k -H l,k F + 1 N 2 3 , ? l ? [L], k ? N * , H l,k -H l,k-1 F ? 1 N 2 3 , ? k ? N * ,</formula><p>then there exist constants C * ,1 , C * ,2 , and C * ,3 that do not depend on k, l, N , and ?, such that</p><formula xml:id="formula_61">(d l,k+1 h ) 2 ? C * ,1 ? + C * ,2 ? k + C * ,3 N 2 3 , ? l ? [L], k ? N * ,</formula><p>where ? = n-S n &lt; 1, n = |V|, and S is number of sampled nodes at each iteration.</p><p>Proof. We have</p><formula xml:id="formula_62">(d l+1,k+1 h ) 2 = E[ H l+1,k+1 -H l+1,k+1 2 F ] = E[ n i=1 h l+1,k+1 i -h l+1,k+1 i 2 F ] = E[ vi?V k B f ? l+1,k+1 ,i ( H l,k+1 ) -f ? l+1,k+1 ,i (H l,k+1 ) 2 F + vi ?V k B h l+1,k i -f ? l+1,k+1 ,i (H l,k+1 ) 2 F ] = E[ S n n i=1 f ? l+1,k+1 ,i ( H l,k+1 ) -f ? l+1,k+1 ,i (H l,k+1 ) 2 F + n -S n n i=1 h l+1,k i -f ? l+1,k+1 ,i (H l,k+1 ) 2 F ] ? S n n i=1 E[ f ? l+1,k+1 ,i ( H l,k+1 ) -f ? l+1,k+1 ,i (H l,k+1 ) 2 F ] + n -S n n i=1 E[ h l+1,k i -f ? l+1,k+1 ,i (H l,k+1 ) 2 F ].</formula><p>About the first term, for l ? 1, we have</p><formula xml:id="formula_63">E[ f ? l+1,k+1 ,i ( H l,k+1 ) -f ? l+1,k+1 ,i (H l,k+1 ) 2 F ] ? ? 2 E[ H l,k+1 -H l,k+1 2 F ] ? ? 2 E[( H l,k+1 -H l,k+1 F + 1 N 2 3 ) 2 ] ? 2? 2 E[ H l,k+1 -H l,k+1 2 F ] + 2? 2 N 4 3 = 2? 2 (d l,k+1 h ) 2 + 2? 2 N 4 3</formula><p>.</p><p>For l = 0, we have</p><formula xml:id="formula_64">E[ f ? l+1,k+1 ,i ( H l,k+1 ) -f ? l+1,k+1 ,i (H l,k+1 ) 2 F ] = E[ f ? 1,k+1 ,i ( H 0,k+1 ) -f ? 1,k+1 ,i (H 0,k+1 ) 2 F ] = E[ f ? 1,k+1 ,i (X) -f ? 1,k+1 ,i (X) 2 F ] = 0.</formula><p>About the second term, for l ? 1, we have</p><formula xml:id="formula_65">E[ h l+1,k i -f ? l+1,k+1 ,i (H l,k+1 ) 2 F ] ? E[ h l+1,k i -h l+1,k i + h l+1,k i -f ? l+1,k+1 ,i (H l,k+1 ) 2 F ] ? E[ h l+1,k i -h l+1,k i 2 F ] + E[ h l+1,k i -f ? l+1,k+1 ,i (H l,k+1 ) 2 F ] + 2E[ h l+1,k i -h l+1,k i , h l+1,k i -f ? l+1,k+1 ,i (H l,k+1 ) ] ? E[ h l+1,k i -h l+1,k i 2 F ] + E[ h l+1,k i -f ? l+1,k+1 ,i (H l,k ) + f ? l+1,k+1 ,i (H l,k ) -f ? l+1,k+1 ,i (H l,k+1 ) 2 F ] + 24E[ h l+1,k i -h l+1,k i , h l+1,k i -f ? l+1,k+1 ,i (H l,k+1 ) ] ? E[ h l+1,k i -h l+1,k i 2 F ] + 2E[ h l+1,k i -f ? l+1,k+1 ,i (H l,k ) 2 F ] + 2E[ f ? l+1,k+1 ,i (H l,k ) -f ? l+1,k+1 ,i (H l,k+1 ) 2 F ] + 4GE[ h l+1,k i -f ? l+1,k+1 ,i (H l,k+1 ) F ] ? E[ h l+1,k i -h l+1,k i 2 F ] + 2? 2 E[ ? l+1,k -? l+1,k+1 2 ] + 2? 2 E[ H l,k -H l,k+1 2 F ] + 4G?E[ ? l+1,k -? l+1,k+1 + H l,k -H l,k+1 F ] ? E[ h l+1,k i -h l+1,k i 2 F ] + 2? 2 G 2 ? 2 + 4G 2 ?? + 2? 2 N 4 3 + 4G? N 2 3 ? E[ h l+1,k i -h l+1,k i 2 F ] + 2G 2 ?(? + 2)? + 2?(? + 2G) N 2 3</formula><p>.</p><p>For l = 0, we have</p><formula xml:id="formula_66">E[ h l+1,k i -f ? l+1,k+1 ,i (H l,k+1 ) 2 F ] ? E[ h l+1,k i -h l+1,k i + h l+1,k i -f ? l+1,k+1 ,i (H l,k+1 ) 2 F ] ? E[ h l+1,k i -h l+1,k i 2 F ] + E[ h l+1,k i -f ? l+1,k+1 ,i (H l,k+1 ) 2 F ] + 2E[ h l+1,k i -h l+1,k i , h l+1,k i -f ? l+1,k+1 ,i (H l,k+1 ) ] ? E[ h l+1,k i -h l+1,k i 2 F ] + E[ h l+1,k i -f ? l+1,k+1 ,i (H l,k ) + f ? l+1,k+1 ,i (H l,k ) -f ? l+1,k+1 ,i (H l,k+1 ) 2 F ] + 2E[ h l+1,k i -h l+1,k i , h l+1,k i -f ? l+1,k+1 ,i (H l,k+1 ) ] ? E[ h l+1,k i -h l+1,k i 2 F ] + 2E[ h l+1,k i -f ? l+1,k+1 ,i (H l,k ) 2 F ] + 2E[ f ? l+1,k+1 ,i (H l,k ) -f ? l+1,k+1 ,i (H l,k+1 ) 2 F ] + 4GE[ h l+1,k i -f ? l+1,k+1 ,i (H l,k+1 ) F ] ? E[ h l+1,k i -h l+1,k i 2 F ] + 2? 2 E[ ? l+1,k -? l+1,k+1 2 ] + 4G?E[ ? l+1,k -? l+1,k+1 ] ? E[ h l+1,k i -h l+1,k i 2 F ] + 2? 2 G 2 ? 2 + 4G 2 ??, ? E[ h l+1,k i -h l+1,k i 2 F ] + 2G 2 ?(? + 2)? + 4G 2 ??.</formula><p>Hence we have</p><formula xml:id="formula_67">(d l+1,k+1 h ) 2 ? (n -S) n (d l+1,k h ) 2 + 2(n -S)?(? + 2)G 2 ? + 0, l = 0, 2? 2 S(d l,k+1 h ) 2 + 4n?(?+G) N 2 3 , l ? 1. Let ? = n-S</formula><p>n &lt; 1. For l = 0, we have</p><formula xml:id="formula_68">(d 1,k+1 h ) 2 - 2(n -S)?(? + 2)G 2 ? 1 -? ? ?((d 1,k h ) 2 - 2(n -S)?(? + 2)G 2 ? 1 -? ) ? ? 2 ((d 1,k-1 h ) 2 - 2(n -S)?(? + 2)G 2 ? 1 -? ) ? ? ? ? ? ? k ((d 1,1 h ) 2 - 2(n -S)?(? + 2)G 2 ? 1 -? ) ? ? k G,</formula><p>which leads to</p><formula xml:id="formula_69">(d 1,k+1 h ) 2 ? 2(n -S)?(? + 2)G 2 1 -? ? + ? k G = C 1,1 ? + ? k G.</formula><p>Then, for l = 1 we have</p><formula xml:id="formula_70">(d 2,k+1 h ) 2 ? ?(d 2,k h ) 2 + C 2,1 ? + C 2,2 ? k + C 2,3 N 2 3</formula><p>, where C 2,1 , C 2,2 , and C 2,3 are all constants. Hence we have</p><formula xml:id="formula_71">(d 2,k+1 h ) 2 - C 2,1 ? + C 2,2 ? k + C2,3 N 2 3 1 -? ? ?((d 2,k h ) 2 - C 2,1 ? + C 2,2 ? k + C2,3 N 2 3 1 -? ) ? ? ? ? ? ? k ((d 2,1 h ) 2 - C 2,1 ? + C 2,2 ? k + C2,3 N 2 3 1 -? ) ? ? k G,</formula><p>which leads to</p><formula xml:id="formula_72">(d 2,k+1 h ) 2 ? C 2,1 ? + C 2,2 ? k + C 2,3 N 2 3</formula><p>.</p><p>And so on, there exist constants C * ,1 , C * ,2 , and C * ,3 that are independent with ?, k, l, N such that</p><formula xml:id="formula_73">(d l,k+1 h ) 2 ? C * ,1 ? + C * ,2 ? k + C * ,3 N 2 3 , ? l ? [L], k ? N * .</formula><p>Lemma 6. For an L-layer GNN, suppose that Assumption 1 holds. Besides, we suppose that</p><formula xml:id="formula_74">1. (d l,1 h ) 2 is bounded by G &gt; 1, ? l ? [L], 2. there exists N ? N * such that V l,k -V l,k F ? V l,k -V l,k F + 1 N 2 3 , ? l ? [L], k ? N * , V l,k -V l,k-1 F ? 1 N 2 3 , ? k ? N * ,</formula><p>then there exist constants C * ,1 , C * ,2 , and C * ,3 that are independent with k, l, ? * , and ?, such that</p><formula xml:id="formula_75">(d l,k+1 v ) 2 ? C * ,1 ? + C * ,2 ? k + C * ,3 N 2 3 , ? l ? [L], k ? N * ,</formula><p>where ? = n-S n &lt; 1, n = |V|, and S is number of sampled nodes at each iteration.</p><p>Proof. Similar to the proof of Lemma 5.</p><p>Proof. We have</p><formula xml:id="formula_76">g w (w k ) -g w (w k ) 2 = 1 |V k L | vj ?V k L ? w w k (h L,k j , y j ) -? w w k (h L,k j , y j ) 2 ? 1 |V k L | vj ?V k L ? w w k (h L,k j , y j ) -? w w k (h L,k j , y j ) 2 ? ? |V k L | vj ?V k L h L,k j -h L,k j 2 ? ? |V k L | vj ?V k L H L,k -H L,k F = ? |V k L | ? |V k L | ? H L,k -H L,k F = ? H L,k -H L,k F</formula><p>Lemma 8. Suppose that Assumption 1 holds. For any k ? N * and l ? [L], the difference between g ? l (? l,k ) and g ? l (? l,k ) can be bounded as</p><formula xml:id="formula_77">g ? l (? l,k ) -g ? l (? l,k ) 2 ? |V|G V l,k -V l,k F + |V|G? H l,k -H l,k F . Proof. As Aa-Bb 2 ? A F a-b 2 + A-B F b 2 , we can bound g ? l (? l,k )-g ? l (? l,k ) 2 by g ? l (? l,k ) -g ? l (? l,k ) 2 ? |V| |V k B | vi?V k B ? ? l u ? l,k (h l-1,k j , m l-1,k N (vj ) , x j ) V l,k j -? ? l u ? l,k (h l-1,k j , m l-1,k N (vj ) , x j ) V l,k j 2 ? |V| max vi?V k B ? ? l u ? l,k (h l-1,k j , m l-1,k N (vj ) , x j ) V l,k j -? ? l u ? l,k (h l-1,k j , m l-1,k N (vj ) , x j ) V l,k j 2 ? |V| max vi?V k B { ? ? l u ? l,k (h l-1,k j , m l-1,k N (vj ) , x j ) F V l,k j -V l,k j 2 + ? ? l u ? l,k (h l-1,k j , m l-1,k N (vj ) , x j ) -? ? l u ? l,k (h l-1,k j , m l-1,k N (vj ) , x j ) F V l,k j 2 } ? |V|G V l,k -V l,k F + |V|G? H l,k -H l,k F .</formula><p>Lemma 9. For an L-layer ConvGNN, suppose that Assumption 1 holds. For any N ? N * , by letting</p><formula xml:id="formula_78">? ? 1 (2?) L G 1 N 2 3 = O( 1 N<label>2 3</label></formula><p>) and</p><formula xml:id="formula_79">? i ? 1 2G 1 N 2 3 = O( 1 N 2 3 ), i ? [n],</formula><p>there exists G 2, * &gt; 0 and ? ? (0, 1) such that for any k ? N * we have</p><formula xml:id="formula_80">E[ ? k w 2 2 ] = (Bias( g w (w k ))) 2 + Var(g w (w k )), E[ ? k ? l 2 2 ] = (Bias( g ? l (? l,k ))) 2 + Var(g ? l (? l,k )), where Var(g w (w k )) = E[ g w (w k ) -? w L(w k ) 2 2 ], Bias( g w (w k )) = E[ g w (w k ) -g w (w k ) 2 2 ] 1 2 , Var(g ? l (? l,k )) = E[ g ? l (? l,k ) -? ? l L(? l,k ) 2 2 ], Bias( g ? l (? l,k )) = E[ g ? l (? l,k ) -g ? l L(? l,k ) 2 2 ] 1 2 and Bias( g w (w k )) ? G 2, * (? 1 2 + ? k-1 2 + 1 N 1 3 ), Bias( g ? l (? l,k )) ? G 2, * (? 1 2 + ? k-1 2 + 1 N 1 3</formula><p>).</p><p>Proof. By Lemmas 1 and 2 we know that</p><formula xml:id="formula_81">H l,k+1 -H l,k F &lt; 1 N 2 3 , ? l ? [L], k ? N * , V l,k+1 -V l,k F &lt; 1 N 2 3 , ? l ? [L], k ? N * .</formula><p>By Lemmas 3 and 4 we know that for any k ? N * and l ? [L] we have</p><formula xml:id="formula_82">H l,k -H l,k F ? H l,k -H l,k F + 1 N 2 3 and V l,k -V l,k F ? V l,k -V l,k F + 1 N 2 3</formula><p>. Thus, by Lemmas 5 and 6 we know that there exist C * ,1 , C * ,2 , and C * ,3 that do not depend on k, l, ?, N such that for ?</p><formula xml:id="formula_83">l ? [L] and k ? N * hold d l,k h ? C * ,1 ? + C * ,2 ? k-1 + C * ,3 N 2 3 ? C * ,1 ? 1 2 + C * ,2 ? k-1 2 + C * ,3 1 N 1 3 and d l,k v ? C * ,1 ? + C * ,2 ? k-1 + C * ,3 N 2 3 ? C * ,1 ? 1 2 + C * ,2 ? k-1 2 + C * ,3 1 N 1 3 . We can decompose ? k w 2 2 as ? k w 2 2 = g w (w k ) -? w L(w k ) 2 2 = g w (w k ) -g w (w k ) + g w (w k ) -? w L(w k ) 2 2 = g w (w k ) -g w (w k ) 2 2 + g w (w k ) -? w L(w k ) 2 2 + 2 g w (w k ) -g w (w k ), g w (w k ) -? w L(w k ) .</formula><p>We take expectation of both sides of the above expression, leading to</p><formula xml:id="formula_84">E[ ? k w 2 2 ] = (Bias( g w (w k ))) 2 + Var(g w (w k )),<label>(16)</label></formula><p>where</p><formula xml:id="formula_85">Bias( g w (w k )) = E[ g w (w k ) -g w (w k ) 2 2 ] 1 2 , Var(g w (w k )) = E[ g w (w k ) -? w L(w k ) 2 2 ] as E[ g w (w k ) -g w (w k ), g w (w k ) -? w L(w k ) ] = 0.</formula><p>By Lemma 7, we can bound the bias term as</p><formula xml:id="formula_86">Bias( g w (w k )) = E[ g w (w k ) -g w (w k ) 2 2 ] 1 2 ? ? 2 E[ H L,k -H L,k 2 F ] 1 2 = ? ? d L,k h ? ?( C * ,1 ? 1 2 + C * ,2 ? k-1 2 + C * ,3 1 N 1 3 ) ? G 2,1 (? 1 2 + ? k-1 2 + 1 N 1 3 ),<label>(17)</label></formula><p>where</p><formula xml:id="formula_87">G 2,1 = ? max{ C * ,1 , C * ,2 , C * ,3 }.</formula><p>Similar to Eq. equation 16, we can decompose</p><formula xml:id="formula_88">E[ ? k ? l 2 2 ] as E[ ? k ? l 2 2 ] = (Bias( g ? l (? l,k ))) 2 + Var(g ? l (? l,k )),<label>where</label></formula><formula xml:id="formula_89">Bias( g ? l (? l,k )) = E[ g ? l (? l,k ) -g ? l (? l,k ) 2 2 ] 1 2 , Var(g ? l (? l,k )) = E[ g ? l (? l,k ) -? ? l (? l,k )) 2 2 ]</formula><p>. By Lemma 8, we can bound the bias term as</p><formula xml:id="formula_90">Bias( g ? l (? l,k )) = E[ g ? l (? l,k ) -g ? l (? l,k ) 2 2 ] 1 2 ? (2|V| 2 G 2 E[ V l,k -V l,k 2 F ] + 2|V| 2 G 2 ? 2 E[ H l,k -H l,k 2 F ]) 1 2 ? ? 2|V|Gd l,k v + ? 2|V|G?d l,k h ? G 2,2 (? 1 2 + ? k-1 2 + 1 N 1 3 ),<label>(18)</label></formula><p>where</p><formula xml:id="formula_91">G 2,2 = ? 2|V|G(1 + ?) max{ C * ,1 , C * ,2 , C * ,3 }. Let G 2, * = max{G 2,1 , G 2,2 }, then we have Bias( g w (w k )) ? G 2, * (? 1 2 + ? k-1 2 + 1 N 1 3 ), Bias( g ? l (? l,k )) ? G 2, * (? 1 2 + ? k-1 2 + 1 N 1 3</formula><p>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>By letting</head><formula xml:id="formula_92">? = 1 N 1 3</formula><p>and C = 2G 2, * , we have</p><formula xml:id="formula_93">Bias( g w (w k )) ? C? + C? k-1 2 , Bias( g ? l (? l,k )) ? C? + C? k-1 2 , which leads to E[ ? k w 2 ] ? E[ ? k w 2 2 ] 1 2 ? (Bias( g w (w k ))) 2 + Var(g w (w k )) 1 2 ? Bias( g w (w k )) + Var(g w (w k )) 1 2 ? C? + C? k-1 2 + Var(g w (w k )) 1 2 and E[ ? k ? l 2 ] ? E[ ? k ? l 2 2 ] 1 2 ? (Bias( g ? l (? l,k ))) 2 + Var(g ? l (? l,k )) 1 2 ? Bias( g ? l (? l,k )) + Var(g ? l (? l,k )) 1 2 ? C? + C? k-1 2 + Var(g ? l (? l,k ))<label>1</label></formula><p>2 . Theorem 2 and Theorem 4 follow immediately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.5 PROOF OF THEOREM 3: CONVERGENCE GUARANTEES</head><p>In this subsection, we give the convergence guarantees of LMC. We first give sufficient conditions for convergence. Lemma 10. Suppose that function f : R n ? R is continuously differentiable. Consider an optimization algorithm with any bounded initialization x 1 and an update rule in the form of</p><formula xml:id="formula_94">x k+1 = x k -?d(x k ),</formula><p>where ? &gt; 0 is the learning rate and d(x k ) is the estimated gradient that can be seen as a stochastic vector depending on x k . Let the estimation error of the gradient be ? k = d(x k )-?f (x k ). Suppose that 1. the optimal value f * = inf x f (x) is bounded; 2. the gradient of f is ?-Lipschitz, i.e., ?f (y) -?f (x) 2 ? ? yx 2 , ? x, y ? R n ;</p><p>3. there exists G 0 &gt; 0 that does not depend on ? such that</p><formula xml:id="formula_95">E[ ? k 2 2 ] ? G 0 , ? k ? N * ;</formula><p>4. there exists N ? N * and ? ? (0, 1) that do not depend on ? such that</p><formula xml:id="formula_96">|E[ ?f (x k ), ? k ]| ? G 0 (? 1 2 + ? k-1 2 + 1 N 1 3 ), ? k ? N * ,</formula><p>where G 0 is the same constant as that in Condition 3, then by letting ? = min{ 1 ? , 1 N 2 3</p><p>}, we have</p><formula xml:id="formula_97">E[ ?f (x R ) 2 2 ] ? 2(f (x 1 ) -f * + G 0 ) N 1 3 + ?G 0 N 2 3 + G 0 N (1 - ? ?) = O( 1 N 1 3</formula><p>),</p><p>where R is chosen uniformly from [N ].</p><p>Proof. As the gradient of f is ?-Lipschitz, we have Then, we have</p><formula xml:id="formula_98">f (x k+1 ) ? f (x k ) + ?f (x k ), x k+1 -x k + ? 2 x k+1 -x k 2 2 = f (x k ) -? ?f (x k ), d(x k ) + ? 2 ? 2 d(x k ) 2 2 = f (x k ) -? ?f (x k ), ? k -? ?f (x k ) 2 2 + ? 2 ? 2 ( ? k 2 2 + ?f (x k ) 2 2 + 2 ? k , ?f (x k ) ) = f (x k ) -?(1 -??) ?f (x k ), ? k -?(1 - ?? 2 ) ?f (x k ) 2 2 + ? 2 ? 2 ? k 2 2 .</formula><p>By taking expectation of both sides, we have</p><formula xml:id="formula_99">E[f (x k+1 )] ? E[f (x k )] -?(1 -??)E[ ?f (x k ), ? k ] -?(1 - ?? 2 )E[ ?f (x k ) 2 2 ] + ? 2 ? 2 E[ ? k 2 2 ].</formula><p>By summing up the above inequalities for k ? [N ] and dividing both sides by N ?(1 -?? 2 ), we have</p><formula xml:id="formula_100">N k=1 E[ ?f (x k ) 2 2 ] N ? f (x 1 ) -E[f (x N )] N ?(1 -?? 2 ) + ?? 2 -?? N k=1 E[ ? k 2 2 ] N - (1 -??) (1 -?? 2 ) N k=1 E[ ?f (x k ), ? k ] N ? f (x 1 ) -f * N ?(1 -?? 2 ) + ?? 2 -?? N k=1 E[ ? k 2 2 ] N + N k=1 |E[ ?f (x k ), ? k ]| N ,</formula><p>where the second inequality comes from ?? &gt; 0 and f (x k ) ? f * . According to the above conditions, we have</p><formula xml:id="formula_101">N k=1 E[ ?f (x k ) 2 2 ] N ? f (x 1 ) -f * N ?(1 -?? 2 ) + ?? 2 -?? G 0 + G 0 N k=1 ? 1 2 + ? k-1 2 N + G 0 N 1 3 ? f (x 1 ) -f * N ?(1 -?? 2 ) + ?? 2 -?? G 0 + ? 1 2 G 0 + G 0 N ? k=1 ? k-1 2 + G 0 N 1 3 = f (x 1 ) -f * N ?(1 -?? 2 ) + ?? 2 -?? G 0 + ? 1 2 G 0 + G 0 N (1 - ? ?) + G 0 N 1 3</formula><p>.</p><p>Notice that</p><formula xml:id="formula_102">E[ ?f (x R ) 2 2 ] = E R [E [?f (x R ) 2 2 | R]] = N k=1 E[ ?f (x k ) 2 2 ] N ,</formula><p>where R is uniformly chosen from [N ], hence we have</p><formula xml:id="formula_103">E[ ?f (x R ) 2 2 ] ? f (x 1 ) -f * N ?(1 -?? 2 ) + ?? 2 -?? G 0 + ? 1 2 G 0 + G 0 N (1 - ? ?) + G 0 N 1 3</formula><p>.</p><formula xml:id="formula_104">By letting ? = min{ 1 ? , 1 N 2 3</formula><p>}, we have</p><formula xml:id="formula_105">E[ ?f (x R ) 2 2 ] ? 2(f (x 1 ) -f * ) N 1 3 + ?G 0 N 2 3 + G 0 N 1 3 + G 0 N (1 - ? ?) + G 0 N 1 3 ? 2(f (x 1 ) -f * + G 0 ) N 1 3 + ?G 0 N 2 3 + G 0 N (1 - ? ?) = O( 1 N 1 3</formula><p>).</p><p>by letting</p><formula xml:id="formula_106">? ? 1 (2?) L G 1 N 2 3 = O( 1 N 2 3</formula><p>) and</p><formula xml:id="formula_107">? i ? 1 2G 1 N 2 3 = O( 1 N 2 3</formula><p>), i ? [n].</p><p>Proof. By Eqs. ( <ref type="formula" target="#formula_86">17</ref>) and ( <ref type="formula" target="#formula_90">18</ref>) we know that there exists G 2, * such that for any k ? N * we have</p><formula xml:id="formula_108">E[ g w (w k ) -g w (w k ) 2 ] ? E[ g w (w k ) -g w (w k ) 2 2 ]</formula><p>1 2</p><p>? G 2, * (?</p><formula xml:id="formula_109">1 2 + ? k-1 2 + 1 N 1 3</formula><p>) and</p><formula xml:id="formula_110">E[ g ? l (? l,k ) -g ? l (? l,k ) 2 ] ? E[ g ? l (? l,k ) -g ? l (? l,k ) 2 2 ]</formula><p>1 2</p><p>? G 2, * (?</p><formula xml:id="formula_111">1 2 + ? k-1 2 + 1 N 1 3</formula><p>),</p><formula xml:id="formula_112">where ? = n-S n &lt; 1 is a constant. Hence |E[ ? w L, ? k w ]| = |E[ ? w L, g w (w k ) -? w L(w k ) ]| = |E[ ? w L, g w (w k ) -g w (w k ) ]| ? E[ ? w L 2 g w (w k ) -g w (w k ) 2 ] ? GE[ g w (w k ) -g w (w k ) 2 ],</formula><p>? G 2 (?</p><formula xml:id="formula_113">1 2 + ? k-1 2 + 1 N 1 3</formula><p>)</p><p>and</p><formula xml:id="formula_114">|E[ ? ? l L, ? k ? l ]| = |E[ ? ? l L, g ? l (? l,k ) -? ? l L(? l,k ) ]| = |E[ ? ? l L, g ? l (? l,k ) -g ? l (? l,k ) ]| ? E[ ? ? l L 2 g ? l (? l,k ) -g ? l (? l,k ) 2 ] ? GE[ g ? l (? l,k ) -g ? l (? l,k ) 2 ] ? G 2 (? 1 2 + ? k-1 2 + 1 N 1 3</formula><p>),</p><p>where G 2 = GG 2, * .</p><p>According to Lemmas 11 and 12, the conditions in Lemma 10 hold. By letting</p><formula xml:id="formula_115">? = 2(f (x 1 ) -f * + G 0 ) N 1 3 + ?G 0 N 2 3 + G 0 N (1 - ? ?) 1 2 = O( 1 N 1 6</formula><p>),</p><p>Theorem 3 follows immediately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E MORE EXPERIMENTS</head><p>E.1 PERFORMANCE ON SMALL DATASETS Figure <ref type="figure">5</ref> reports the convergence curves GD, GAS, and LMC for GCN on three small datasets, i.e., Cora, Citeseer, and PubMed from Planetoid <ref type="bibr" target="#b26">(Yang et al., 2016)</ref>. LMC is faster than GAS, especially on the CiteSeer and PubMed datasets. Notably, the key bottleneck on the small datasets is graph sampling rather than forward and backward passes. Thus, GD is faster than GAS and LMC, as it avoids graph sampling by directly using the whole graph.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Testing accuracy and training loss w.r.t. runtimes (s).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: The average relative estimated errors of mini-batch gradients computed by CLUSTER, GAS, and LMC for GCN models.7.3 LMC IS ROBUST IN TERMS OF BATCH SIZES</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The improvement of the compensations on the Ogbn-arxiv dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>where deg global (i) is the degree of node i in the whole graph and deg local (i) is the degree of node i in the subgraph induced by N (V B ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>t(y -x)), yx dt = f (x) + ?f (x), yx + 1 0 ?f (x + t(y -x)) -?f (x), yx dt ? f (x) + ?f (x), yx + 1 0 ?f (x + t(y -x)) -?f (x) 2 yx 2 dt ? f (x) + ?f (x), yx + 1 0 ?t yx 2 2 dt ? f (x) + ?f (x), yx + ? 2 yx 2 2 ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>(?) l,k , but elsewhere we omit the superscript k and denote it by (?) l .</figDesc><table><row><cell cols="2">In each training iteration, we sample a mini-batch of nodes VB and propose to approximate values,</cell></row><row><cell cols="2">i.e., node embeddings and auxiliary variables, outside VB by convex combinations of historical</cell></row><row><cell>values, denoted H l V\V B and V</cell><cell>l V\V B , and incomplete up-to-date values, denoted H l V\V B and V l V\V B .</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Prediction performance on large graph datasets. OOM denotes the out-of-memory issue. Bold font indicates the best result and underline indicates the second best result.</figDesc><table><row><cell cols="2"># nodes</cell><cell>230K</cell><cell>57K</cell><cell>89K</cell><cell>169K</cell></row><row><cell cols="2"># edges</cell><cell>11.6M</cell><cell>794K</cell><cell>450K</cell><cell>1.2M</cell></row><row><cell cols="2">Method</cell><cell>REDDIT</cell><cell>PPI</cell><cell>FLICKR</cell><cell>ogbn-arxiv</cell></row><row><cell cols="2">GRAPHSAGE</cell><cell>95.40</cell><cell>61.20</cell><cell>50.10</cell><cell>71.49</cell></row><row><cell cols="2">VR-GCN</cell><cell>94.50</cell><cell>85.60</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">FASTGCN</cell><cell>93.70</cell><cell>-</cell><cell>50.40</cell><cell>-</cell></row><row><cell cols="2">LADIES</cell><cell>92.80</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">CLUSTER-GCN</cell><cell>96.60</cell><cell>99.36</cell><cell>48.10</cell><cell>-</cell></row><row><cell cols="2">GRAPHSAINT</cell><cell>97.00</cell><cell>99.50</cell><cell>51.10</cell><cell>-</cell></row><row><cell cols="2">SIGN</cell><cell>96.80</cell><cell>97.00</cell><cell>51.40</cell><cell>-</cell></row><row><cell>GD</cell><cell>GCN GCNII</cell><cell>95.43 OOM</cell><cell>97.58 OOM</cell><cell>53.73 55.28</cell><cell>71.64 72.83</cell></row><row><cell>GAS</cell><cell>GCN GCNII</cell><cell cols="4">95.35?0.01 98.91?0.03 53.44?0.11 71.54?0.19 96.73?0.04 99.36?0.02 55.42?0.27 72.50?0.28</cell></row><row><cell>FM</cell><cell>GCN GCNII</cell><cell cols="4">95.27?0.03 98.91?0.01 53.48?0.17 71.49?0.33 96.52?0.06 99.34?0.03 54.68?0.27 72.54?0.27</cell></row><row><cell>LMC</cell><cell>GCN GCNII</cell><cell cols="4">95.44?0.02 98.87?0.04 53.80?0.14 71.44?0.23 96.88?0.03 99.32?0.01 55.36?0.49 72.76?0.22</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Table2reports the number of epochs, the runtime to reach the full-batch accuracy in Table1, and the GPU memory. As shown in Table2and Figure2a, LMC is significantly faster than GAS, especially with a speed-up of 2x on the REDDIT dataset. Notably, the test accuracy of LMC is more stable than GAS, and thus the smooth test accuracy of LMC outperforms GAS in Figure2b. Although GAS finally resembles full-batch performance in Table1by selecting the best performance on the valid data, it may fail to resemble under small batch sizes due to its unstable process (see Section 7.3). Another appealing feature of LMC is that LMC shares the comparable GPU memory costs with GAS, and thus LMC avoids the neighbor explosion problem. FM is slower than other methods, as they additionally update historical embeddings in the storage for the nodes outside the mini-batches. Please see Appendix E.2 for the comparison in terms of training time per epoch. Efficiency of CLUSTER-GCN, GAS, FM, and LMC.</figDesc><table><row><cell>Dataset &amp; GNN</cell><cell cols="2">Epochs CLUSTER GAS</cell><cell>FM</cell><cell cols="4">Runtime (s) LMC CLUSTER GAS FM</cell><cell cols="4">Memory (MB) LMC CLUSTER GAS FM</cell><cell>LMC</cell></row><row><cell>Ogbn-arxiv &amp; GCN</cell><cell>211.0</cell><cell cols="3">176.0 152.4 124.4</cell><cell>108</cell><cell>79</cell><cell>115</cell><cell>55</cell><cell>424</cell><cell>452</cell><cell>460</cell><cell>557</cell></row><row><cell>FLICKR &amp; GCN</cell><cell>379.2</cell><cell cols="3">389.4 400.0 334.2</cell><cell>127</cell><cell cols="2">117 181</cell><cell>85</cell><cell>310</cell><cell>375</cell><cell>380</cell><cell>376</cell></row><row><cell>REDDIT &amp; GCN</cell><cell>239.0</cell><cell cols="3">372.4 400.0 166.8</cell><cell>516</cell><cell cols="3">790 2269 381</cell><cell>1193</cell><cell cols="2">1508 1644 1829</cell></row><row><cell>PPI &amp; GCN</cell><cell>428.0</cell><cell cols="3">293.6 286.4 290.2</cell><cell>359</cell><cell cols="2">179 224</cell><cell>179</cell><cell>212</cell><cell>214</cell><cell>218</cell><cell>267</cell></row><row><cell>Ogbn-arxiv &amp; GCNII</cell><cell>-</cell><cell cols="3">234.8 373.6 197.4</cell><cell>-</cell><cell cols="2">218 381</cell><cell>178</cell><cell>-</cell><cell>453</cell><cell>454</cell><cell>568</cell></row><row><cell>FLICKR &amp; GCNII</cell><cell>-</cell><cell cols="3">352 400.0 356</cell><cell>-</cell><cell cols="2">465 576</cell><cell>475</cell><cell>-</cell><cell>396</cell><cell>402</cell><cell>468</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Performance under different batch sizes on the Ogbn-arxiv dataset.</figDesc><table><row><cell>Batch size</cell><cell>GCN GAS LMC GAS LMC GCNII</cell></row><row><cell>1</cell><cell>70.56 71.65 71.34 72.11</cell></row><row><cell>2</cell><cell>71.11 71.89 72.25 72.55</cell></row><row><cell>5</cell><cell>71.99 71.84 72.23 72.87</cell></row><row><cell>10</cell><cell>71.60 72.14 72.82 72.80</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Statistics of the datasets used in our experiments.</figDesc><table><row><cell>Dataset</cell><cell cols="4">#Graphs #Classes Total #Nodes Total #Edges</cell></row><row><cell>PPI</cell><cell>24</cell><cell>121</cell><cell>56,944</cell><cell>793,632</cell></row><row><cell>REDDIT</cell><cell>1</cell><cell>41</cell><cell>232,965</cell><cell>11,606,919</cell></row><row><cell>Ogbn-arxiv</cell><cell>1</cell><cell>40</cell><cell>169,343</cell><cell>1,157,799</cell></row><row><cell>FLICKR</cell><cell>1</cell><cell>7</cell><cell>89,250</cell><cell>449,878</cell></row><row><cell cols="3">A.2 TRAINING AND EVALUATION PROTOCOLS</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/rusty1s/pyg_autoscale. The owner does not mention the license.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>where</p><p>. Suppose that Assumption 1 holds, then with ? = O(? 2 ) and ?i = O(? 2 ), i ? [n], there exist C &gt; 0 and ? ? (0, 1) such that for any k ? N * and l ? [L], the bias terms can be bounded as</p><p>Lemma 7. Suppose that Assumption 1 holds. For any k ? N * , the difference between g w (w k ) and g w (w k ) can be bounded as</p><p>Runtime (s) </p><p>is the degree in the sampled subgraph rather than the whole graph. The normalized adjacency matrix is difficult to store and reuse, as the sampled subgraph may be different. FM is slower than other methods, as they additionally update historical embeddings in the storage for the nodes outside the mini-batches. 2 CLUSTER-GCN proposes to sample clusters to construct various subgraphs at each training step and LMC follows it. If a subgraph-wise sampling method prunes an edge at the current step, the GNN may observe the pruned edge at the next step by resampling subgraphs. This avoids GNN overfitting the graph which drops some important edges as shown in Section 3.2 in <ref type="bibr" target="#b4">(Chiang et al., 2019)</ref> (we also observe that GAS achieves the accuracy of 71.5% and 71.1% under stochastic subgraph partition and fixed subgraph partition respectively on the Ogbn-arxiv dataset).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 COMPARISON IN TERMS OF MEMORY UNDER DIFFERENT BATCH SIZES</head><p>In Table <ref type="table">7</ref>, we report the GPU memory consumption, and the proportion of reserved messages b k=1 A alg V b k 0/ A 0 in forward and backward passes for GCN, where A is the adjacency matrix of full-batch GCN, A alg</p><p>is the adjacency matrix used in a subgraph-wise method alg (e.g., CLUSTER, GAS, and LMC), and ? 0 denotes the 0-norm. As shown in Table <ref type="table">7</ref>, LMC makes full use of all sampled nodes in both forward and backward passes, which is the same as full-batch GD. Default indicates the default batch size used in the codes and toolkits of GAS <ref type="bibr" target="#b9">(Fey et al., 2021)</ref>.</p><p>Table <ref type="table">7</ref>: GPU memory consumption (MB) and the proportion of reserved messages (%) in forward and backward passes of GD, CLUSTER, GAS, and LMC for training GCN. Default indicates the default batch size used in the codes and toolkits of GAS <ref type="bibr" target="#b9">(Fey et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Batch size Methods</head><p>Ogbn As shown in Section A.4, ? i = score(i)? in LMC. We report the prediction performance under ? ? {0.0, 0.2, 0.4, 0.6, 0.8, 1.0} and score ? {f <ref type="table">8</ref> and<ref type="table">9</ref> respectively. When exploring the effect of a specific hyper-parameter, we fix the other hyper-parameters as their best values. Notably, ? = 0 implies that LMC directly uses the historical values as affordable without alleviating their staleness, which is the same as that in GAS. Under large batch sizes, LMC achieves the best performance with large ? i = 1, as large batch sizes improve the quality of the incomplete up-to-date messages. Under small batch sizes, LMC achieves the best performance with small ? i = 0.4score 2x-x 2 (i), as small learning rates alleviate the staleness of the historical values.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F POTENTIAL SOCIETAL IMPACTS</head><p>In this paper, we propose a novel and efficient subgraph-wise sampling method for the training of GNNs, i.e., LMC. This work is promising in many practical and important scenarios such as search engine, recommendation systems, biological networks, and molecular property prediction.</p><p>Published as a conference paper at ICLR 2023 Nonetheless, this work may have some potential risks. For example, using this work in search engine and recommendation systems to over-mine the behavior of users may cause undesirable privacy disclosure.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The anatomy of a large-scale hypertextual web search engine</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Page</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh International Conference on World Wide Web 7</title>
		<meeting>the Seventh International Conference on World Wide Web 7</meeting>
		<imprint>
			<publisher>Elsevier Science Publishers B. V</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="107" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Stochastic training of graph convolutional networks with variance reduction</title>
		<author>
			<persName><forename type="first">Jianfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">Jennifer</forename><surname>Dy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Andreas</forename><surname>Krause</surname></persName>
		</editor>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018-07">Jul 2018a</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="10" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">FastGCN: Fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cao</forename><surname>Xiao</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rytstxWAW" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations, 2018b</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Simple and deep graph convolutional networks</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhewei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zengfeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="http://proceedings.mlr.press/v119/chen20v.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">Hal</forename><surname>Daum?</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Iii</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Aarti</forename><surname>Singh</surname></persName>
		</editor>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020-07">Jul 2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="13" to="18" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks</title>
		<author>
			<persName><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="257" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Minimal variance sampling with provable guarantees for fast training of graph neural networks</title>
		<author>
			<persName><forename type="first">Weilin</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rana</forename><surname>Forsati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahmut</forename><surname>Kandemir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehrdad</forename><surname>Mahdavi</surname></persName>
		</author>
		<idno type="DOI">10.1145/3394486.3403192</idno>
		<ptr target="https://doi.org/10.1145/3394486.3403192" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD &apos;20</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD &apos;20<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1393" to="1403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Weighted graph cuts without eigenvectors a multilevel approach</title>
		<author>
			<persName><forename type="first">S</forename><surname>Inderjit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqiang</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><surname>Kulis</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2007.1115</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2007.1115" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<idno type="ISSN">0162-8828</idno>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1944" to="1957" />
			<date type="published" when="2007-11">nov 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Graph neural networks for social recommendation</title>
		<author>
			<persName><forename type="first">Wenqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference, WWW &apos;19</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="417" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast graph representation learning with PyTorch Geometric</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop on Representation Learning on Graphs and Manifolds</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Gnnautoscale: Scalable and expressive graph neural networks via historical embeddings</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Weichert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="https://proceedings.mlr.press/v139/fey21a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">Marina</forename><surname>Meila</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2021-07">Jul 2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="18" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Openpnm: A pore network modeling package</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gostick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aghighi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hinebaugh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tranter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Hoeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Day</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Spellacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Sharqawy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bazylak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lehnert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Putz</surname></persName>
		</author>
		<idno type="DOI">10.1109/MCSE.2016.49</idno>
	</analytic>
	<monogr>
		<title level="j">Computing in Science &amp; Engineering</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">04</biblScope>
			<biblScope unit="page" from="60" to="74" />
			<date type="published" when="2016-07">jul 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Implicit graph neural networks</title>
		<author>
			<persName><forename type="first">Fangda</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somayeh</forename><surname>Sojoudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><forename type="middle">El</forename><surname>Ghaoui</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/file/8" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="11984" to="11995" />
		</imprint>
	</monogr>
	<note>b5c8441a8ff8e151b191c53c1842a38-Paper.pdf</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Graph representation learning</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Artificial Intelligence and Machine Learning</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="159" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="22118" to="22133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adaptive sampling towards fast graph representation learning</title>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2018/file/01" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
	<note>eee509ee2f68dc6014898c309e86bf-Paper.pdf</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A fast and high quality multilevel scheme for partitioning irregular graphs</title>
		<author>
			<persName><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vipin</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on scientific Computing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="359" to="392" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Molecular graph convolutions: Moving beyond fingerprints</title>
		<author>
			<persName><forename type="first">Steven</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Berndl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Riley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer-Aided Molecular Design</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="8" to="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster). OpenReview.net</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Deep Learning on Graphs</title>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An iterative global optimization algorithm for potential energy minimization</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Moloi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Ali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Optim. Appl</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="119" to="132" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1162</idno>
		<ptr target="https://aclanthology.org/D14-1162" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-10">October 2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Sign: Scalable inception graph neural networks</title>
		<author>
			<persName><forename type="first">Emanuele</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabrizio</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Eynard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<idno>CoRR, abs/2004.11198</idno>
		<ptr target="https://arxiv.org/abs/2004.11198" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Microsoft academic graph: When experts are not enough</title>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chieh-Han</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anshul</forename><surname>Kanakia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quantitative Science Studies</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="396" to="413" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v48/yanga16.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">Maria</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Florina</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<meeting>The 33rd International Conference on Machine Learning<address><addrLine>New York, New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-22">20-22 Jun 2016</date>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">GraphFM: Improving large-scale GNN training via feature momentum</title>
		<author>
			<persName><forename type="first">Haiyang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bokun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianbao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="https://proceedings.mlr.press/v162/yu22g.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">Kamalika</forename><surname>Chaudhuri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Csaba</forename><surname>Szepesvari</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sivan</forename><surname>Sabato</surname></persName>
		</editor>
		<meeting>the 39th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2022-07">Jul 2022</date>
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="page" from="17" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Graphsaint: Graph sampling based inductive learning method</title>
		<author>
			<persName><forename type="first">Hanqing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongkuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajitesh</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajgopal</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viktor</forename><surname>Prasanna</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BJe8pkHFwS" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Decoupling the depth and scope of graph neural networks</title>
		<author>
			<persName><forename type="first">Hanqing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinglong</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajitesh</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Malevich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajgopal</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viktor</forename><surname>Prasanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ren</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=_IY3_4psXuf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">Wortman</forename><surname>Vaughan</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Layerdependent importance sampling for training deep and large graph convolutional networks</title>
		<author>
			<persName><forename type="first">Difan</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yewen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanquan</forename><surname>Gu</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2019/file/91" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Alch?-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
	<note>ba4a4478a66bee9812b0804b6f9d1b-Paper.pdf</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
