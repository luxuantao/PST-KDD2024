<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MiDi: Mixed Graph and 3D Denoising Diffusion for Molecule Generation</title>
				<funder ref="#_Jktk84Q">
					<orgName type="full">The Alan Turing Institute</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-06-05">5 Jun 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Cl?ment</forename><surname>Vignac</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LTS4</orgName>
								<orgName type="institution">EPFL</orgName>
								<address>
									<settlement>Lausanne</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nagham</forename><surname>Osman</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">EEE Dept</orgName>
								<orgName type="institution">University College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Laura</forename><surname>Toni</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">EEE Dept</orgName>
								<orgName type="institution">University College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pascal</forename><surname>Frossard</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LTS4</orgName>
								<orgName type="institution">EPFL</orgName>
								<address>
									<settlement>Lausanne</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MiDi: Mixed Graph and 3D Denoising Diffusion for Molecule Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-06-05">5 Jun 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2302.09048v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Diffusion Model</term>
					<term>Drug Discovery</term>
					<term>Graph Generation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work introduces MiDi, a novel diffusion model for jointly generating molecular graphs and their corresponding 3D arrangement of atoms. Unlike existing methods that rely on predefined rules to determine molecular bonds based on the 3D conformation, MiDi offers an endto-end differentiable approach that streamlines the molecule generation process. Our experimental results demonstrate the effectiveness of this approach. On the challenging GEOM-DRUGS dataset, MiDi generates 92% of stable molecules, against 6% for the previous EDM model that uses interatomic distances for bond prediction, and 40% using EDM followed by an algorithm that directly optimize bond orders for validity. Our code is available at github.com/cvignac/MiDi.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Modern drug discovery requires the development of effective machine learning models that can correctly capture the vast chemical space and sample from it. These models need to understand properties of molecules that depend both on their molecular graph and their conformation in the 3D space. The molecular graph (or 2D structure) determines the existence and type of the chemical bonds, and allows the identification of functional groups in a compound. This provides information about its chemical properties and enables to predict synthetic pathways. On the other hand, the 3D conformation of a compound plays a key role in its interaction with other molecules, and governs in particular its biological Fig. <ref type="figure" target="#fig_1">1</ref>: Samples from our model. MiDi generates simultaneously a 2D graph structure and a 3D conformation that is consistent with this structure. activity and binding affinity to proteins. To explore the chemical space adequately, it is therefore crucial to consider both aspects simultaneously.</p><p>Unfortunately, existing generative models for molecules are restricted to one of these two data modalities. While models that exclusively generate molecular graphs have been vastly researched <ref type="bibr" target="#b7">[8]</ref>, current 3D molecule generation are on the contrary only trained to generate conformers, thus ignoring bond information. These models rely on a subsequent step that predicts the 2D structure using either interatomic distances <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b36">37]</ref> or chemical software such as OpenBabel <ref type="bibr" target="#b10">[11]</ref>. As a result, these models are not end-to-end differentiable, which hampers their ability to be fully optimized for various downstream tasks. This severely limits the potential of 3D molecule generators, particularly for complex tasks like pocket-conditioned generation.</p><p>We propose here a new model, called Mixed Graph+3D Denoising Diffusion (MiDi), which overcomes this limitation by simultaneously generating a molecular graph and its corresponding 3D coordinates. MiDi represents molecules as graphs embedded in 3D that contain node features (atom types and formal charges) and edges features (bond types). Our model progressively corrupts data with noise, and trains a neural network to predict clean data from noisy inputs. New molecules can then be generated by sampling pure noise and iteratively denoising it with the neural network, similarly to other diffusion models <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b41">42]</ref>. As the model is trained to denoise both the graph and 3D coordinates in tandem, it is able to produce stable molecular graphs that are consistent with the generated conformers.</p><p>While previous diffusion models for molecules relied on either Gaussian noise or discrete diffusion, MiDi uses both noise models simultaneously: the 3D coordinates are corrupted with Gaussian noise, while the other components use discrete diffusion which was found to be effective for graph generation <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b46">47]</ref>. To further enhance the quality of the generated samples, we introduce a noise schedule whose parameters are adjusted to each component. Specifically, we add noise to the atom types and formal charges at a faster rate than to the coordinates and bond types. This encourages the denoising network to first focus on generating a realistic 3D conformation and corresponding bond types, before refining the atom types and formal charges.</p><p>Our second contribution considers the denoising network: the Transformer architecture we propose incorporates a novel rEGNN layer, which improves upon the popular EGNN layers <ref type="bibr" target="#b38">[39]</ref> by leveraging features that are not translationinvariant. We show that, due to the use of Gaussian noise in the zero center-of-mass subspace of the molecules, the resulting model is nevertheless equivariant to translations and rotations, which is crucial for achieving high performance.</p><p>We showcase the effectiveness of our model on unconditional molecule generation. On the challenging GEOM-DRUGS dataset, the previous EDM model <ref type="bibr" target="#b15">[16]</ref> can generate stable molecules at a rate of 5.5%, which can be improved to 40.3% by using the Open Babel bond prediction algorithm <ref type="bibr" target="#b34">[35]</ref>. In contrast, the proposed method achieves 91.6% of stable molecules, demonstrating the superiority of our end-to-end differentiable approach. Furthermore, MiDi can be readily applied to various drug-discovery tasks beyond unconditional generation, which confirms its versatility and potential for improving drug discovery pipelines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Concurrent work Concurrently to our work, <ref type="bibr" target="#b16">[17]</ref> and <ref type="bibr" target="#b35">[36]</ref> also proposed 2D+3D diffusion models for molecule generation. These models also leverage Gaussian diffusion for the 3D coordinates and discrete diffusion for the graph, using absorbing transitions in <ref type="bibr" target="#b35">[36]</ref> and uniform transitions in <ref type="bibr" target="#b16">[17]</ref>. Each model also features unique contributions: <ref type="bibr" target="#b16">[17]</ref> proposes richer positional encodings for the transformer layer, while <ref type="bibr" target="#b35">[36]</ref> introduces a guidance mechanism to help the network predict accurate bond lengths. MiDi is the only model that improves upon the standard EGNN layers, and that is capable of handling formal charges. It is also the only model that presents results on the more complex GEOM-DRUGS dataset with explicit hydrogens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Molecule generation in 3D</head><p>The idea of representing molecules as attributed 3D point cloud has been used both in one-shot settings <ref type="bibr" target="#b36">[37]</ref> and in autoregressive methods such as GSchNet <ref type="bibr" target="#b10">[11]</ref> . Recently, the Equivariant Diffusion Model (EDM) <ref type="bibr" target="#b15">[16]</ref> was proposed for this task, improving significantly over previous results. This model was later extended by limiting the message-passing computations to neighboring nodes <ref type="bibr" target="#b18">[19]</ref> and using a more expressive denoising network <ref type="bibr" target="#b33">[34]</ref>. All these diffusion models can be conditioned on molecule-level properties using guidance mechanisms <ref type="bibr" target="#b3">[4]</ref> or on another point cloud. Conditioning on a second point cloud has been employed to generate molecules that bind to a specific protein <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b39">40]</ref> and to generate linkers between molecular fragments <ref type="bibr" target="#b19">[20]</ref>. The main drawback of these models is that they do not learn the connectivity structure of the molecule. It needs to be obtained in a second stage using interatomic distances <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b36">37]</ref> or specialized software such as Open Babel <ref type="bibr" target="#b34">[35]</ref>. This results in limited performance for complex molecules, but also prevents end-to-end differentiability for downstream applications.</p><p>Graph Generation Another line of work has focused on generating graphs without associated 3D coordinates. Early denoising diffusion models for this task used Gaussian noise applied to the adjacency matrix <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b24">25]</ref> or graph eigenvalues <ref type="bibr" target="#b29">[30]</ref>. <ref type="bibr" target="#b46">[47]</ref> and <ref type="bibr" target="#b11">[12]</ref> however found that discrete diffusion is more effective, as it better respects the discrete nature of graphs. These diffusion models tend to outperform autoregressive methods except on validity metrics, as autoregressive models can perform validity checks at each sampling step <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33]</ref>. In contrast to the proposed method, which operates at the node level, fragment-based methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b31">32]</ref> learn to combine chemically-relevant substructures from a fixed or learned dictionary <ref type="bibr" target="#b47">[48]</ref>, but are harder to adapt to 3D.</p><p>In order to generate molecules in 3D, graph-based models could be combined with conformer generation models <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b51">52]</ref> which predict a 3D structure from Table <ref type="table">1</ref>: Gaussian and categorical distributions enable the efficient computation of the key quantities involved in training diffusion models and sampling from them. Formulas for all parameters can be found in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Noise model</head><p>Gaussian diffusion Discrete diffusion q(zt|zt-1)</p><formula xml:id="formula_0">N (?tzt-1, ? 2 t I) zt-1 Qt q(zt|x) N ( ?tx, ?2 t I) x Qt x p ? (zt-1|x, zt)dp ? (x|zt) N (?t x + ?tzt, ?2 t I) ? x p ? (x)(ztQ ? t ? x Qt-1)</formula><p>an input graph. As these methods assume that the graph is known, they are able to exploit symmetries of the molecule (such as rotatable bonds), which is more difficult on unconditional generation tasks. Unfortunately, combining graph generation and conformer generation models would again break end-to-end differentiability and restrict performance.</p><p>Protein Generation While existing diffusion models for molecules operate on molecules of moderate size (up to 180 atoms), recent diffusion models for proteins have managed to scale to much larger structures <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b49">50]</ref>. These methods leverage the chain structure of proteins, which implies that the adjacency matrix does not need to be predicted. Furthermore, instead of predicting 3D coordinates for each atom, they only predict the angles between successive C ? carbons, which significantly reduces the degrees of freedom and encodes roto-translation invariance in the representation. Those improvements are unfortunately specific to chain graphs, and cannot be used for arbitrary molecules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background</head><p>Denoising Diffusion Models Diffusion models consist of two essential elements: a noise model and a denoising neural network. The noise model q takes as input a data point x and generates a trajectory of increasingly corrupted data points (z 1 , ..., z T ). The corruption process is chosen to be Markovian, i.e.,</p><formula xml:id="formula_1">q(z 1 , . . . , z T |x) = q(z 1 |x) T t=2 q(z t |z t-1 ).</formula><p>The denoising network ? ? takes noisy data z t as input, and learns to invert the diffusion trajectories. While it would be natural to naively train the network to predict z t-1 , this strategy would lead to noisy targets, as z t-1 depends on the sampled diffusion trajectory. Instead, modern diffusion models <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref> predict the clean input x from z t , or equivalently, the noise added to it. The diffusion sequences are then inverted by marginalizing over the network predictions p ? (x|z t ):</p><formula xml:id="formula_2">p ? (z t-1 |z t ) = x p ? (z t-1 | x, z t ) dp ? (x|z t )<label>(1)</label></formula><p>Although Eq. 1 leads to more efficient training, it requires the efficient computation of p ? (z t-1 |x, z t ) and the integral, which is not always possible. Two main frameworks have been proposed under which Eq. 1 is tractable: Gaussian noise, which is suitable for continuous data, and discrete state-space diffusion for categorical data. Table <ref type="table">1</ref> summarizes the main properties of the two related noise models.</p><p>Gaussian diffusion processes are defined by q(z t |z t-1 ) ? N (? t z t , ? 2 t I), where (? t ) t?T controls how much signal is retained at each step and (? t ) t?T how much noise is added. As normal distributions are stable under composition, we have</p><formula xml:id="formula_3">q(z t |x) ? N (? t z t , ?2 t I), with ?t = t s=1 ? s and ?2 t = ? 2 t -? 2 t .</formula><p>While any noise schedule is in principle possible, variance-preserving processes are most often used, which satisfy ?2 t + ?2 t = 1. The posterior of the transitions conditioned on x can also be computed in closed-form. It satisfies</p><formula xml:id="formula_4">q(z t-1 |z t , x) ? N (? t x + ? t z t , ?2 t I), with ? t = ?s (1 -? 2 t ?2 t-1 /? 2 t ), ? t = ? t ?2 t-1 /? 2 t and ?t = ?2 t-1 (1 -? 2 t ?2 t-1 /? 2 t ).</formula><p>On the contrary, discrete diffusion considers that data points x belong to one of d classes <ref type="bibr" target="#b0">[1]</ref>. The transition matrices (Q 1 , ..., Q T ) are square matrices of size d ? d that represent the probability of jumping from one class to another at each time step. Given previous state z t-1 , the noise model for the next state z t is a categorical distribution over the d possible classes which reads as q(z t |z t-1 ) ? C(z t-1 Q t ), where z t-1 is a row vector encoding the class of z t-1 . Since the process is Markovian, we simply have</p><formula xml:id="formula_5">q(z t = j|x) = [x Qt ] j with Qt = Q 1 Q 2 ...Q t .</formula><p>The posterior distribution q(z t-1 |z t , x) can also be computed in closed form using Bayes rule and the Markovian property. If ? denotes a pointwise product and Q ? is the transpose of Q, it can be written as</p><formula xml:id="formula_6">q(z t-1 |z t , x) ? z t (Q t ) ? ? x Qt-1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SE(3)-Equivariance with Diffusion Models</head><p>Molecules are dynamic entities that can undergo translation and rotation, and the arrangement of their atoms does not have a predetermined order. To effectively model molecules using generative models and avoid augmenting the data with random transformations, it is essential to ensure that the models are equivariant to these inherent symmetries. In diffusion models, equivariance to a transformation group G can be achieved through several conditions. First, the noise model must be equivariant to the action of G: ?g ? G, q(g.z t |g.x) = q(z t |x). Second, the prior distribution q ? used at inference should be invariant to the group action, i.e., q ? (g.z T ) = q ? (z T ), and this noise should be processed by an equivariant neural network in order to ensure that p ? (g.z t-1 |g.z t ) = p ? (z t-1 |z t ). Finally, the network should be trained with a loss function that satisfies l(p ? (g.x|g.z t ), g.x) = l(p ? (x|z t ), x). Together, these requirements create an architecture that is agnostic to the group elements used to represent the training data <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b51">52]</ref>.</p><p>To ensure equivariance to the special Euclidean group SE(3), a number of architectures have been proposed as possible denoising networks for a diffusion model <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b43">44]</ref>. However, these networks can be computationally expensive due to their manipulation of spherical harmonics. As a result, many generative models for molecules <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b39">40]</ref> use the more affordable EGNN layers <ref type="bibr" target="#b38">[39]</ref>. At a high level, EGNN recursively updates the coordinates (r i ) of a graph with node features (x i ) and edge features (y ij ) using:</p><formula xml:id="formula_7">r i ? r i + j c ij m(||r i -r j ||, x i , x j , y ij )(r j -r i )</formula><p>The crucial feature of this parameterization is that the message function m takes only rotation-invariant arguments. This, combined with the linear term in r j -r i , ensures that the network is rotation-equivariant. Finally, we note that the normalization term c ij = ||r i -r j || + 1 is necessary for numerical stability when concatenating many layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Proposed Model</head><p>We now present the Mixed Graph+3D denoising diffusion (MiDi) model. We represent each molecule as a graph G = (x, c, R, Y ), where x and c are vectors of length n containing the type and formal charge associated to all atoms. The n ? 3 matrix R = [r i ] 1?i?n contains the coordinates of each atom, and Y is an n ? n matrix containing the bond types. Similarly to previous diffusion models for graphs, we consider the absence of a bond as a particular bond type and generate dense adjacency tensors. We denote the one-hot encoding of x, c, and Y by X, C, and Y, respectively. Time steps are denoted by superscripts, so, for example, r t i denotes the coordinates of node v i at time t. The transpose of matrix X is denoted by X ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Noise Model</head><p>Our noise model corrupts the features of each node and edge independently, using a noise model that depends on the data type. For the positions, we use a Gaussian noise within the zero center-of-mass (CoM) subspace of the molecule ? ? N CoM (? t R t-1 , (? t ) 2 I), which is required to obtain a roto-translation equivariant architecture <ref type="bibr" target="#b51">[52]</ref>. This means that the noise follows a Gaussian distribution on the linear subspace of dimension 3(n-1) that satisfies n i=1 ? i = 0. For atom types, formal charges and bond types, we use discrete diffusion, where the noise model is a sequence of categorical distributions. We choose the marginal transition model proposed in <ref type="bibr" target="#b46">[47]</ref>. For instance, when m ? R a represents the marginal distribution of atom types in the training set, we define Q t x = ? t I + ? t 1 a m ? . We similarly define Q t c and Q t y . The resulting noise model is given by: Adaptive cosine noise schedule atoms types + charges bond types coordinates Fig. <ref type="figure">2</ref>: The noise schedule is tuned separately for each component. Atom coordinates and bond types are denoised earlier during sampling, while atom types and formal charges are updated later in the process. Experimentally, the adaptive schedule allows to obtain better 3D conformers and more stable molecules.</p><formula xml:id="formula_8">q(G t |G t-1 ) ? N CoM (? t R t-1 , (? t ) 2 I) ? C(X t-1 Q t x ) ? C(C t-1 Q t c ) ? C(Y t-1 Q t y ).</formula><p>When generating new samples, we define the posterior as a product as well:</p><formula xml:id="formula_9">p ? (G t-1 |G t ) = 1?i?n p ? (r t-1 i |G t )p ? (x t-1 i |G t )p ? (c t-1 i |G t ) 1?i,j?n p ? (Y t-1 ij |G t ),</formula><p>We calculate each term by marginalizing over the network predictions. For instance,</p><formula xml:id="formula_10">p ? (x t-1 i |G t ) = xi p ? (x t-1 i | x i , G t ) dp ? (x i |G t ) = x?X q(x t-1 i |x i = x, G t ) p X ? (x i = x),</formula><p>where p X ? (x i = x) is the neural network estimate for the probability that node v i in the clean graph G is of type x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Adaptive Noise Schedule</head><p>Although the MiDi model corrupts the coordinates, atom types, bond types and formal charges simultaneously, these components do not play a symmetrical role. For instance, while the 2D connectivity structure can be predicted relatively well from the 3D conformation, the converse is not true as the conformation is not unique for a given structure. Similarly, the formal charges serve as an adjustable variable used to match the valency of each atom with its electronic structure, but they do not constitute a very fundamental property of the molecules.</p><p>Based on these observations, we propose an adaptation of the noise model in order to encourage the denoising network to first generate correctly the most important components, namely the atom coordinates and bond types, before moving on to predict the atom types and formal charges. To achieve this, we modify the noise schedule to vary according to the component. We modify the popular cosine schedule by adding an exponent ? that controls the rate at which the noise is added to the model:</p><formula xml:id="formula_11">?t = cos ? 2 (t/T + s) ? 1 + s 2 ,</formula><p>where the parameter ? can take the form of ? r , ? x , ? y and ? c for the atom coordinates, types, bond types, and charges, respectively. On the QM9 dataset, we use ? r = 2.5, ? y = 1.5, ? x = ? c = 1, while GEOM-DRUGS uses ? r = 2.</p><p>The noise schedule used for QM9 is shown in Fig. <ref type="figure">2</ref>. This choice means that rough estimates for the atom coordinates and the bond types are first generated at inference, before the other components start to play a significant role. This aligns with previous work on 2D molecular graph generation which found that predicting the bond types before the atom types is beneficial <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b45">46]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Denoising Network</head><p>The denoising network takes a noisy graph as input and learns to predict the corresponding clean graph. It manipulates graph-level features w, node coordinates R, node features (atom types and formal charges, treated together in the matrix X), and edge features Y. Coordinates are treated separately from the other node features in order to guarantee SE(3) equivariance. The neural network architecture is summarized in Figure <ref type="figure">3</ref>. It consists of a Transformer architecture <ref type="bibr" target="#b44">[45]</ref>, with a succession of self-attention module followed by normalization layers and feedforward networks. We give more details about the different blocks below.</p><p>Relaxed Equivariant Graph Neural Networks (rEGNNs) In our proposed method, we leverage the effective yet affordable EGNN layers <ref type="bibr" target="#b38">[39]</ref> for processing the coordinates. However, we enhance these layers by exploiting the fact that, when the data and the noise reside in the zero Center-Of-Mass subspace, it is not necessary for the neural network to be translation invariant. This can be interpreted as defining a canonical pose for the translation group, which is a valid way to achieve equivariance <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b25">26]</ref>. Rather than simply relying on pairwise distances ||r i -r j || 2 , we can therefore use other rotation invariant descriptors such as ||r i || 2 or cos(r i , r j ). We therefore propose the following relaxedEGNN (rEGNN) layer:</p><formula xml:id="formula_12">[? r ] ij = cat(||r i -r j || 2 , ||r i || 2 , ||r j || 2 , cos(r i , r j )) r i ? r i + j ? m (X i , X j , [? r ] ij , Y ij ) (r j -r i )</formula><p>Similar to EGNN layers, the rEGNN layer combines a rotation-invariant message function with a linear update in r j -r i , which guarantees rotation equivariance. Notably, the additional features ||r i || 2 , ||r j || 2 , and cos(r i , r j ) are computed relative to the center-of-mass of the molecule, which is set to 0 by Fig. <ref type="figure">3</ref>: The denoising neural network of MiDi jointly predicts the 2D graph and 3D coordinates of the clean graph from a noisy input. It follows a graph Transformer architecture with layers tailored to maintain SE(3) equivariance. In the update block, each component is updated using the other features. While the graph-level features w do not play a direct role in the final prediction, they serve as an effective means of storing and organizing pertinent information throughout the transformer layers. definition. In our experiments, we have observed that these features facilitate the generation of a higher proportion of connected molecules, thereby mitigating an issue previously observed with both 3D <ref type="bibr" target="#b15">[16]</ref> and 2D-based denoising diffusion models <ref type="bibr" target="#b46">[47]</ref>.</p><p>Update Block To improve the model's ability to process all features simultaneously, our new rEGNN layer is integrated into a larger update block that processes each component using all other ones. The edge features are first updated using ? r , the node features, and the global features. The node features are updated using a self-attention mechanism, where the attention coefficients also use the edge features and ? r . After the attention heads have been flattened, the obtained values are modulated by the pooled edge features, ? r and the global features. For pooling pairwise features (Y and ? r ) into node representations, we use PNA layers <ref type="bibr" target="#b5">[6]</ref>: PNA(Y ) i = W T cat[mean, min, max, std] j (y ij ). The global features are updated by pooling all other features at the graph level. Finally, the coordinates are updated using a rEGNN update, where the message function takes as input ? r and the updated edge features. Note that we do not use the normalization term of EGNN: our layers are integrated in a Transformer architecture as discussed next, and we empirically found SE(3) normalization layers to be more effective than the EGNN normalisation term at controlling the magnitude of the activations.</p><p>Integration into a Transformer Architecture Transformers have proved to be a very efficient way to stabilize the self-attention mechanism over many layers. We describe below the changes to the feed-forward neural network and normalization layers that are required to ensure SE(3)-equivariance.</p><p>Our feed-forward neural network processes each component using MLPs applied in parallel on each node and each edge. As the coordinates cannot be treated separately (it would break SE(3)-equivariance), we define</p><formula xml:id="formula_13">PosMLP(R) = ? CoM (MLP(||R||) R ||R|| + ? ) ? R n?3 ,</formula><p>where ||R|| ? R n?1 contains the norm ||r i || 2 of each point, MLP(||R||) ? R n?1 as well, ? is a small positive constant, and ? CoM is the projection of the coordinates on the linear subspace with center-of-mass at 0:</p><formula xml:id="formula_14">? CoM (R) i = r i - 1 n n i=1 r i .</formula><p>The choice of the normalization layer also depends on the problem symmetries: while batch normalization <ref type="bibr" target="#b21">[22]</ref> is used in some graph transformer models <ref type="bibr" target="#b8">[9]</ref>, this layer is not equivariant in contrast to Set Normalization <ref type="bibr" target="#b52">[53]</ref> or Layer Normalization <ref type="bibr" target="#b2">[3]</ref>. For SE(3) equivariance, the normalization of <ref type="bibr" target="#b27">[28]</ref> should be used. Applied to 3D coordinates, it writes</p><formula xml:id="formula_15">E3Norm(R) = ? ||R|| n + ? R ||R|| = ? R n + ? with n = 1 n n i=1 ||r i || 2 ,</formula><p>with a learnable parameter ? ? R initialized at 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Training Objective</head><p>The denoising network of MiDi is trained to predict the clean molecule from a noisy input G t , which is reflected in the choice of loss function used during model training. The estimation of the coordinates R is a regression problem that can simply be solved with mean-squared error, whereas the prediction p X ? for the atom types, p C ? for the formal charges and p Y ? for the bond types corresponds to a classification problem which can be addressed through a cross-entropy loss (CE in the equations). Note that the network's position predictions result in pointwise estimates R, while for the other terms, the prediction is a distribution over classes. The final loss is a weighted sum of these components:</p><formula xml:id="formula_16">l(G, pG ) = ? r || R -R|| 2 + ? x CE(X, p X ? ) + ? c CE(C, p C ? ) + ? y CE(Y, p Y ? )</formula><p>The (? i ) were initially chosen in order to balance the contribution of each term and cross-validated starting from this initial value. Our final experiments use ? r = 3, ? x = 0.4, ? c = 1, ? y = 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Settings</head><p>We evaluate MiDi's performance on unconditional molecule generation tasks. To the best of our knowledge, MiDi is the first method to generate both the graph structure and the conformer simultaneously, leaving no end-to-end differentiable method to compare to. We therefore compare MiDi to 3D models on top of which a bond predictor is applied. We consider two such predictors: either a simple lookup table, as used in <ref type="bibr" target="#b15">[16]</ref>, or the optimization procedure of OpenBabel<ref type="foot" target="#foot_0">3</ref>  <ref type="bibr" target="#b34">[35]</ref> used in other works such as <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b39">40]</ref>. The latter algorithm optimizes the bond orders of neighboring atoms in order to create a valid molecule, removing all control on the generated graphs. In terms of dataset comparison, EDM <ref type="bibr" target="#b15">[16]</ref> was previously the only method that could scale up to the large GEOM-DRUGS dataset, so it is our only direct competitor in that case. For the QM9 dataset, we also compare MiDi's performance to that of the GSchNet method <ref type="bibr" target="#b10">[11]</ref>, which employed the OpenBabel algorithm and achieved good results.</p><p>To facilitate comparison with previous methods, such as <ref type="bibr" target="#b36">[37]</ref> and <ref type="bibr" target="#b15">[16]</ref>, we benchmark our models on the full molecular graphs that include explicit hydrogens atoms. However, we acknowledge that, for most practical applications, hydrogen atoms can be inferred from the heavy atoms in the structure, and thus can be removed. In fact, methods trained solely on heavy atoms usually perform better since they consider smaller graphs.</p><p>We measure validity using the success rate of RDKit sanitization over 10,000 molecules. Uniqueness is the proportion of valid molecules with different canonical SMILES. Atom and molecule stability are metrics proposed in <ref type="bibr" target="#b37">[38]</ref> -they are similar to validity but, in contrast to RDKit sanitization, they do not allow for adding implicit hydrogens to satisfy the valency constraints. Novelty is the proportion of unique canonical SMILES strings obtained that are not in the training set. Since all training molecules have a single connected component, we also measure the proportion of the generated molecules that are connected.</p><p>We also compare the histograms of several properties of the generated set with a test set. The atom and bond total variations (AtomTV and BondTV) measure the l1 distance between the marginal distribution of atom types and bond types, respectively, in the generated set and test set. The Wasserstein distance between valencies is a weighted sum over the valency distributions for each atom types: ValencyW 1 = x?atom types p(x)W 1 ( Dval (x), D val (x)), where p X (x) is the marginal distribution of atom types in the training set, Dval (x) is the marginal distribution of valencies for atoms of type x in the generated set, D val (x) the same distribution in the test set. Here, the Wasserstein distance between histograms is used rather than total variation, as it allows to better respect the structure of ordinal data.</p><p>In previous methods, graph-based metrics were predominantly used. However, in our approach, we also introduce 3D metrics based on histograms of bond lengths and bond angles. This allows us to evaluate the efficacy of our approach not only in terms of the graph structure but also in generating accurate conformers. To this end, we report a weighted sum of the distance between bond lengths for each bond type:</p><formula xml:id="formula_17">BondLenghtsW 1 = y?bond types p(y)W 1 ( Ddist (y), D dist (y)),</formula><p>where p Y (y) is the proportion of bonds of type y in the training set, Ddist (y) is the generated distribution of bond lengths for bond of type y, and D dist (y) is the same distribution computed over the test set. The output is value in Angstrom.</p><p>Finally, BondAnglesW 1 (in degrees) compares the distribution of bond angles (in degrees) for each atom type. We compute a weighted sum of these values using the proportion of each atom type in the dataset. This calculation is restricted to atoms with two or more neighbors to ensure that angles can be defined: where pX (x) denotes the proportion of atoms of type x in the training set, restricted to atoms with two neighbors or more, and D angles (x) is the distribution of geometric angles of the form ?(r k -r i , r j -r i ), where i is an atom of type x, and k and j are neighbors of i. The reported metrics are mean and 95% confidence intervals on 5 different samplings from the same checkpoint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">QM9</head><p>We first evaluate our model on the standard QM9 dataset <ref type="bibr" target="#b50">[51]</ref> containing molecules with up to 9 heavy atoms. We split the dataset into 100k molecules for training, 20k for validation, and 13k for testing. Results are presented in Table <ref type="table" target="#tab_0">2</ref>. The data line represents the results of the training set compared with the test set, while the other entries compare the generated molecules to the test molecules. As we observe in Table <ref type="table" target="#tab_0">2</ref>, predicting the bonds only from the interatomic distances and atom types has limited performance. Therefore, MiDi outperforms EDM on 2D metrics, while obtaining similar 3D metrics for the generated conformers. It is worth noting that our list of allowed bonds is not identical to that used in <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b37">38]</ref>, which may explain why our results for EDM <ref type="bibr" target="#b15">[16]</ref> do not match those of the original paper perfectly. Nonetheless, the optimization algorithm of OpenBabel performs very well on this dataset of simple molecules. As QM9 contains molecules with only up to 9 atoms, the molecular conformations are easy to understand and the bonds can be determined easily.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">GEOM-DRUGS</head><p>We then assess our model on the much larger GEOM-DRUGS dataset <ref type="bibr" target="#b1">[2]</ref> which comprises 430,000 drug-sized molecules with an average of 44 atoms and up to 181 atoms. As this dataset features drug-like compounds, it is therefore better suited for downstream applications than QM9. We split the dataset into 80% for training, 10% for validation, and 10% for testing. For each molecule, we extract the 5 lowest energy conformations to build the dataset. Results are presented in Table <ref type="table">3</ref>. On this large dataset, we did not train the adaptive version of MiDi from scratch, but instead fine-tuned it using a checkpoint of MiDi with uniform noise schedule. As this dataset contains molecules that are much more complex than those in QM9, the bonds in the molecules cannot be determined solely from pairwise distances. This explains why EDM, which performs relatively well on 3D-based metrics, produces very few valid and stable molecules. Furthermore, many structures in this dataset are too complex for the Open Babel algorithm. While the latter achieves good atom stability, there is at least one invalid atom in most molecules, leading to low molecular stability. The advantages of an end-to-end Table <ref type="table">3</ref>: Unconditional generation on GEOM-Drugs with explicit hydrogens. EDM was previously the only method that scaled to this dataset. On this complex dataset, the benefits of an integrated models are very clear, as MiDi significantly outperforms Open Babel on most metrics. 95% confidence intervals are reported on five samplings of the same checkpoint. model that generates both a graph structure and its conformation are evident on this dataset: MiDi not only generates better molecular graphs, but also predicts 3D conformers with more realistic bond angles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We propose MiDi, a novel denoising diffusion model that jointly generates a molecular graph and a corresponding conformation for this molecule. Our model combines Gaussian and discrete diffusion in order to define a noise model that is best suited to each component. The noise schedule is further adapted to the different components, with the network initially generating a rough estimate of the conformation and the graph structure, before tuning the atom types and charges. A graph transformer network is trained to denoise this model, that features novel rEGNN layers. While rEGNN layers manipulate features that are translation invariant, they still result in a SE(3) equivariant network when the input molecules are centered. On unconditional generation tasks with complex datasets, MiDi clearly outperforms prior 3D molecule generation methods that predict bonds from the conformation using predefined rules. While our model is currently evaluated on unconditional generation tasks, we believe that the end-toend training of the graph structure and the conformation can offer even greater benefits for other downstream tasks such as pocket-conditioned generation.     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>BondAnglesW 1 (</head><label>1</label><figDesc>generated, target) = x?atom types p(x)W 1 ( Dangles (x), D angles (x)),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Non-curated samples on QM9 with implicit hydrogens.</figDesc><graphic url="image-3.png" coords="21,134.77,152.88,345.84,170.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Non-curated samples on QM9 with explicit hydrogens.</figDesc><graphic url="image-4.png" coords="21,134.77,426.88,345.83,177.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Non-curated samples on GEOM-drugs with implicit hydrogens.</figDesc><graphic url="image-5.png" coords="22,134.77,156.23,345.84,166.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: Non-curated samples on QEOM-drugs with explicit hydrogens.</figDesc><graphic url="image-6.png" coords="22,134.77,433.61,345.83,166.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-2.png" coords="9,169.35,115.83,276.68,240.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>Unconditional generation on QM9 with explicit hydrogens with uniform and adaptive noise schedules. While MiDi outperforms the base EDM model on graph-based metrics, the Open Babel optimization procedure is very effective on this simple dataset, as the structures are simple enough for the bonds to be determined unambiguously from the conformation.</figDesc><table><row><cell>Metrics (?)</cell><cell cols="6">Mol stable At stable Validity Uniqueness Novelty Connected</cell></row><row><cell>Data</cell><cell>98.7</cell><cell>99.8</cell><cell>98.9</cell><cell>99.9</cell><cell>-</cell><cell>100.0</cell></row><row><cell>GSchNet</cell><cell>92.0</cell><cell>98.7</cell><cell>98.1</cell><cell>94.5</cell><cell>80.5</cell><cell>97.1</cell></row><row><cell>EDM</cell><cell>90.7</cell><cell>99.2</cell><cell>91.7</cell><cell>98.5</cell><cell>75.9</cell><cell>99.3</cell></row><row><cell cols="2">EDM + OBabel 97.9</cell><cell>99.8</cell><cell>99.0</cell><cell>98.5</cell><cell>77.8</cell><cell>99.7</cell></row><row><cell cols="2">MiDi (uniform) 96.1?.2</cell><cell>99.7?.0</cell><cell>96.6?.2</cell><cell>97.6?.1</cell><cell cols="2">64.9?.5 99.8?.0</cell></row><row><cell cols="2">MiDi (adaptive) 97.5?.1</cell><cell>99.8?.0</cell><cell>97.9?.1</cell><cell>97.6?.1</cell><cell cols="2">67.5?.3 99.9?.0</cell></row><row><cell>Metrics (?)</cell><cell cols="6">Valency(e-2) Atom(e-2) Bond(e-2) Angles (?) Bond Lengths (e-2 ?)</cell></row><row><cell>Data</cell><cell>0.1</cell><cell>0.3</cell><cell>? 0</cell><cell>0.12</cell><cell>? 0</cell><cell></cell></row><row><cell>GSchNet</cell><cell>4.9</cell><cell>4.2</cell><cell>1.1</cell><cell>1.68</cell><cell>0.5</cell><cell></cell></row><row><cell>EDM</cell><cell>1.1</cell><cell>2.1</cell><cell>0.2</cell><cell>0.44</cell><cell>0.1</cell><cell></cell></row><row><cell cols="2">EDM + OBabel 1.1</cell><cell>2.1</cell><cell>0.1</cell><cell>0.44</cell><cell>0.1</cell><cell></cell></row><row><cell cols="2">MiDi (uniform) 0.4?.0</cell><cell>0.9?.0</cell><cell>0.1?0.0</cell><cell>0.67?.02</cell><cell>1.6?.7</cell><cell></cell></row><row><cell cols="2">MiDi (adaptive) 0.3?.0</cell><cell>0.3?.1</cell><cell>0.0?.0</cell><cell>0.62?.02</cell><cell>0.3?.1</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Unconditional generation on QM9 with implicit hydrogens. On this simple dataset, all methods achieve very good results, although the lookup table of EDM sometimes fail to generate correct edge types, resulting in invalid molecules.</figDesc><table><row><cell>Metric (?)</cell><cell>Validity</cell><cell cols="3">Uniqueness Novelty Connected</cell></row><row><cell>Data</cell><cell>99.5</cell><cell>99.9</cell><cell>-</cell><cell>100</cell></row><row><cell>EDM</cell><cell>96.8</cell><cell>96.6</cell><cell>45.5</cell><cell>99.9</cell></row><row><cell cols="2">EDM + OBabel 100.0</cell><cell>96.1</cell><cell>45.4</cell><cell>99.9</cell></row><row><cell cols="2">MiDi (uniform) 99.5?.1</cell><cell>95.8?.2</cell><cell cols="2">49.2?0.4 100?.0</cell></row><row><cell cols="2">MiDi (adaptive) 99.7?.0</cell><cell>93.9?.2</cell><cell>44.7?.5</cell><cell>100?.0</cell></row><row><cell>Metric (?)</cell><cell cols="4">Valency(e-2) Atom(e-2) Bond(e-2) Bond Lengths(e-2 ?)</cell></row><row><cell>Data</cell><cell>0.6</cell><cell>0.1</cell><cell>0.1</cell><cell>0.3</cell></row><row><cell>EDM</cell><cell>4.3</cell><cell>2.9</cell><cell>0.9</cell><cell>0.2</cell></row><row><cell cols="2">EDM + OBabel 3.8</cell><cell>2.9</cell><cell>0.3</cell><cell>0.2</cell></row><row><cell cols="2">MiDi (uniform) 2.3?1.7</cell><cell>3.1?3.7</cell><cell>0.4?.1</cell><cell>0.8?.3</cell></row><row><cell cols="2">Midi (adaptive) 5.9?.2</cell><cell>11.7?0.2</cell><cell>1.1?.0</cell><cell>1.6?.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Unconditional generation on GEOM-DRUGS with implicit hydrogens.</figDesc><table><row><cell>Metric (?)</cell><cell>Validity</cell><cell cols="3">Uniqueness Novelty Connected</cell></row><row><cell>Data</cell><cell>100.0</cell><cell>100.0</cell><cell>-</cell><cell>99.8</cell></row><row><cell cols="2">MiDi (uniform) 93.2?.1</cell><cell cols="3">100.0?.0 100.0?.0 98.0?.1</cell></row><row><cell cols="2">MiDi (adaptive) 96.7?.1</cell><cell cols="3">100.0?.0 100.0?.0 98.7?.1</cell></row><row><cell>Metric (?)</cell><cell cols="4">Valency(e-2) Atom(e-2) Bond(e-2) Bond Lengths(e-2 ?)</cell></row><row><cell>Data</cell><cell>? 0</cell><cell>? 0</cell><cell>7.9</cell><cell>? 0</cell></row><row><cell cols="2">MiDi (uniform) 13.6?.1</cell><cell>9.5?.3</cell><cell>8.8?.0</cell><cell>1.6?.2</cell></row><row><cell cols="2">Midi (adaptive) 5.3?.1</cell><cell>6.3?0.1</cell><cell>8.0?.0</cell><cell>0.6?.3</cell></row><row><cell cols="4">A.2 GEOM-DRUGS with implicit hydrogens</cell><cell></cell></row><row><cell cols="3">B Samples from our model</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>http://openbabel.org/wiki/Bond_Orders</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7">Acknowledgements</head><p>This work was partially supported by <rs type="funder">The Alan Turing Institute</rs>. Cl?ment Vignac thanks the <rs type="institution">Swiss Data Science Center</rs> for supporting him through the <rs type="programName">PhD fellowship program</rs> (grant <rs type="grantNumber">P18-11</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Jktk84Q">
					<idno type="grant-number">P18-11</idno>
					<orgName type="program" subtype="full">PhD fellowship program</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Additional Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 QM9 with Implicit Hydrogens</head><p>The results are presented in Table <ref type="table">4</ref> for 5 samplings of the same checkpoint. While all methods overall achieve good results on this simple dataset, we observe that the lookup table of EDM sometimes fails to predict the bond type, resulting in much more invalid molecules than our model. Interestingly, the adaptive noise schedule that allowed for important improvements on the GEOM-DRUG dataset is not effective on this simpler dataset, and the uniform schedule seems to perform better. The reasons for this phenomenon. We finally observe that the bond length predictions are overall good for all methods, but that MiDi is not as precise as EDM, which can be explained by the fact that EDM uses both learning rate decay and an exponential moving average.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Structured denoising diffusion models in discrete state-spaces</title>
		<author>
			<persName><forename type="first">J</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Van Den Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Geom: Energy-annotated molecular conformations for property prediction and molecular generation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Axelrod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gomez-Bombarelli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.05531</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<title level="m">Layer normalization</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.15408</idno>
		<title level="m">Equivariant energy-guided sde for inverse molecular design</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Brandstetter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hesselink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Van Der Pol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Bekkers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.02905</idno>
		<title level="m">Geometric and physical quantities improve e (3) equivariant message passing</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Principal neighbourhood aggregation for graph nets</title>
		<author>
			<persName><forename type="first">G</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cavalleri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>St?rk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.01776</idno>
		<title level="m">Diffdock: Diffusion steps, twists, and turns for molecular docking</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Molgensurvey: A systematic survey in machine learning models for molecule design</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.14500</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A generalization of transformer networks to graphs</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI Workshop on Deep Learning on Graphs: Methods and Applications</title>
		<imprint>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Gemnet: Universal directional graph neural networks for molecules</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gasteiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Symmetry-adapted generation of 3d point sets for the targeted discovery of molecules</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">W</forename><surname>Gebauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gastegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Sch?tt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.00957</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Haefeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Martinkus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Perraudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wattenhofer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.01549</idno>
		<title level="m">Diffusion models for graphs benefit from discrete state spaces</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A decade of fragment-based drug design: strategic advances and lessons learned</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Hajduk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Greer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature reviews Drug discovery</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note>d967f1ab10179ca4b-Paper.pdf 2</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Argmax flows and multinomial diffusion: Learning categorical distributions</title>
		<author>
			<persName><forename type="first">E</forename><surname>Hoogeboom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Forr?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Equivariant diffusion for molecule generation in 3d</title>
		<author>
			<persName><forename type="first">E</forename><surname>Hoogeboom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">G</forename><surname>Satorras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Vignac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note>PMLR (2022) 2, 3, 5, 9</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Mudiff: Unified diffusion for complete molecule generation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.14621</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Graphgdp: Generative diffusion processes for permutation invariant graph generation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lv</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.01842</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Wong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.05710</idno>
		<title level="m">Mdm: Molecular diffusion model for 3d molecule generation</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Igashov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>St?rk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Vignac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">G</forename><surname>Satorras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Correia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.05274</idno>
		<title level="m">Equivariant 3d-conditional diffusion models for molecular linker design</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Illuminating protein space with a programmable generative model</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ingraham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Baranov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Costello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Frappier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ismail</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Obermeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Beam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hierarchical generation of molecular graphs using structural motifs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.02514</idno>
		<title level="m">Score-based generative modeling of graphs via the system of stochastic differential equations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Equivariance with learned canonicalization functions</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">O</forename><surname>Kaba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Mondal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=pVD" />
	</analytic>
	<monogr>
		<title level="m">NeurIPS 2022 Workshop on Symmetry and Geometry in Neural Representations</title>
		<imprint>
			<date type="published" when="2022">2022. 1k8ge25a 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficient graph generation with graph recurrent attention networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Equiformer: Equivariant graph attention transformer for 3d atomistic graphs</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">L</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Smidt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.11990</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Constrained graph variational autoencoders for molecule design</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gaunt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.08892</idno>
		<title level="m">Fast graph generative model via spectral diffusion</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Madhawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ishiguro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nakago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11600</idno>
		<title level="m">Graphnvp: An invertible flow model for generating molecular graphs</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning to extend molecular scaffolds with structural motifs</title>
		<author>
			<persName><forename type="first">K</forename><surname>Maziarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Jackson-Flux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cameron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sirockin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Stiefl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Segler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Graph networks for molecular design</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mercado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rastemo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lindel?f</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Engkvist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Bjerrum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning: Science and Technology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Morehead</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.04313</idno>
		<title level="m">Geometry-complete diffusion for 3d molecule generation</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Open babel: An open chemical toolbox</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>O'boyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Banck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Morley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vandermeersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Hutchison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of cheminformatics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Moldiff: Addressing the atom-bond inconsistency problem in 3d molecule diffusion generation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.07508</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">E(n) equivariant normalizing flows</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">G</forename><surname>Satorras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hoogeboom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">G</forename><surname>Satorras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hoogeboom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">B</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.09016</idno>
		<title level="m">E (n) equivariant normalizing flows</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">E (n) equivariant graph neural networks</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">G</forename><surname>Satorras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hoogeboom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.09844</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Schneuing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jamasb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Igashov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.13695</idno>
		<title level="m">Structure-based drug design with equivariant diffusion models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Protein sequence and structure co-design with equivariant translation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.08761</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep unsupervised learning using nonequilibrium thermodynamics</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</editor>
		<meeting>the 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Generative modeling by estimating gradients of the data distribution</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Smidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kohlhoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Riley</surname></persName>
		</author>
		<idno>CoRR abs/1802.08219</idno>
		<title level="m">Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point clouds</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Top-n: Equivariant set and graph generation without exchangeability</title>
		<author>
			<persName><forename type="first">C</forename><surname>Vignac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.02096</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Digress: Discrete denoising diffusion for graph generation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Vignac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Krawczuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Siraudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Cevher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=UaAD-Nu86WX2" />
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note>3, 5</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Baraniuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.11126</idno>
		<title level="m">Retrieval-based controllable molecule generation</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Broadly applicable and accurate protein design by integrating structure prediction networks and diffusion generative models</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Juergens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Trippe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">E</forename><surname>Eisenach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ahern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Borst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Ragotte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Milles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V D</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Amini</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.15611</idno>
		<title level="m">Protein structure generation via folding diffusion</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Moleculenet: a benchmark for molecular machine learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Geniesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Leswing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
		<idno type="DOI">10.1039/C7SC02664A</idno>
		<ptr target="http://dx.doi.org/10.1039/C7SC02664A12" />
	</analytic>
	<monogr>
		<title level="j">Chem. Sci</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="513" to="530" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Geodiff: A geometric diffusion model for molecular conformation generation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=PzcvxEMzvQC3" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Set norm and equivariant skip connections: Putting the deep in deep sets</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Tozzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=MDT30TEtaVY10" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
