<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-05-25">25 May 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhengyi</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Comp. Sci. &amp; Tech</orgName>
								<orgName type="institution">Tsinghua-Bosch Joint ML Center</orgName>
								<address>
									<settlement>BNRist Center</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">ShengShu</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Cheng</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Comp. Sci. &amp; Tech</orgName>
								<orgName type="institution">Tsinghua-Bosch Joint ML Center</orgName>
								<address>
									<settlement>BNRist Center</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yikai</forename><surname>Wang</surname></persName>
							<email>yikaiw@outlook.com</email>
						</author>
						<author>
							<persName><forename type="first">Fan</forename><surname>Bao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Comp. Sci. &amp; Tech</orgName>
								<orgName type="institution">Tsinghua-Bosch Joint ML Center</orgName>
								<address>
									<settlement>BNRist Center</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">ShengShu</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chongxuan</forename><surname>Li</surname></persName>
							<email>chongxuanli@ruc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Comp. Sci. &amp; Tech</orgName>
								<orgName type="institution">Tsinghua-Bosch Joint ML Center</orgName>
								<address>
									<settlement>BNRist Center</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="laboratory">Key Laboratory of Big Data Management and Analysis Methods</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing, Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hang</forename><surname>Su</surname></persName>
							<email>suhangss@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Comp. Sci. &amp; Tech</orgName>
								<orgName type="institution">Tsinghua-Bosch Joint ML Center</orgName>
								<address>
									<settlement>BNRist Center</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="laboratory">Pazhou Laboratory (Huangpu)</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Comp. Sci. &amp; Tech</orgName>
								<orgName type="institution">Tsinghua-Bosch Joint ML Center</orgName>
								<address>
									<settlement>BNRist Center</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">ShengShu</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="laboratory">Pazhou Laboratory (Huangpu)</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-05-25">25 May 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2305.16213v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Score distillation sampling (SDS) has shown great promise in text-to-3D generation by distilling pretrained large-scale text-to-image diffusion models, but suffers from over-saturation, over-smoothing, and low-diversity problems. In this work, we propose to model the 3D parameter as a random variable instead of a constant as in SDS and present variational score distillation (VSD), a principled particle-based variational framework to explain and address the aforementioned issues in text-to-3D generation. We show that SDS is a special case of VSD and leads to poor samples with both small and large CFG weights. In comparison, VSD works well with various CFG weights as ancestral sampling from diffusion models and simultaneously improves the diversity and sample quality with a common CFG weight (i.e., 7.5). We further present various improvements in the design space for text-to-3D such as distillation time schedule and density initialization, which are orthogonal to the distillation algorithm yet not well explored. Our overall approach, dubbed ProlificDreamer, can generate high rendering resolution (i.e., 512 ? 512) and high-fidelity NeRF with rich structure and complex effects (e.g., smoke and drops). Further, initialized from NeRF, meshes fine-tuned by VSD are meticulously detailed and photo-realistic. Project page: https://ml.cs.tsinghua.edu.cn/prolificdreamer/.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>3D content and technologies enable us to visualize, comprehend, and interact with complex objects and environments that are reflective of our real-life experiences. Their pivotal role extends across a wide array of domains, encompassing architecture, animation, gaming, and the rapidly evolving fields of virtual and augmented reality. In spite of the extensive applications, the production of premium 3D content often remains a formidable task. It necessitates a significant investment of time and effort, even when undertaken by professional designers. This challenge has prompted the development of text-to-3D methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b15">16]</ref>. By automating the generation of 3D content based on textual descriptions, these innovative methods present a promising way towards streamlining the 3D content creation process. Furthermore, they stand to make this process more accessible, potentially encouraging a significant paradigm shift in the aforementioned fields.</p><p>A snail on a leaf.</p><p>A chimpanzee dressed like Henry VIII king of England. A pineapple.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A model of a house in</head><p>Tudor style.</p><p>Michelangelo style statue of dog reading news on a cellphone.</p><p>An astronaut is riding a horse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>An elephant skull.</head><p>A tarantula, highly detailed.</p><p>(a) ProlificDreamer can generate meticulously detailed and photo-realistic 3D textured meshes.</p><p>A DSLR photo of a Space Shuttle.</p><p>A Matte painting of a castle made of cheesecake surrounded by a moat made of ice cream.</p><p>Inside of a smart home, realistic detailed photo, 4k.</p><p>A DSLR photo of a hamburger inside a restaurant.</p><p>A red fire hydrant spraying water.</p><p>A DSLR photo of a table with dim sum on it.</p><p>(b) ProlificDreamer can generate high rendering resolution (i.e., 512 ? 512) and high-fidelity NeRF with rich structures and complex effects. Besides, the bottom results show that ProlificDreamer can generate complex scenes with 360 ? views because of our scene initialization (see Sec. <ref type="bibr" target="#b3">4</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.2).</head><p>A highly detailed sand castle.</p><p>A hotdog in a tutu skirt.</p><p>(c) ProlificDreamer can generate diverse and semantically correct 3D scenes given the same text.</p><p>Figure <ref type="figure" target="#fig_9">1</ref>: Text-to-3D samples generated by ProlificDreamer from scratch. Our base model is Stable Diffusion and we do not employ any other assistant model or user-provided shape guidance (see Table <ref type="table">1</ref>). See our accompanying videos in our project page for better visual quality.</p><p>Diffusion models <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b47">48]</ref> have significantly advanced text-to-image synthesis <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b0">1]</ref>, particularly when trained on large-scale datasets <ref type="bibr" target="#b41">[42]</ref>. Inspired by these developments, DreamFusion <ref type="bibr" target="#b32">[33]</ref> employs a pretrained, large-scale text-to-image diffusion model for the generation of 3D content from text in the wild, circumventing the need for any 3D data. DreamFusion introduces the Score Distillation Sampling (SDS) algorithm to optimize a single 3D representation such that the image rendered from any view maintains a high likelihood as evaluated by the diffusion model, given the text. Despite its wide application <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b53">54]</ref>, empirical observations <ref type="bibr" target="#b32">[33]</ref> indicate that SDS often suffers from over-saturation, over-smoothing, and low-diversity problems, which have yet to be thoroughly explained or adequately addressed. Additionally, orthogonal elements in the design space for text-to-3D, such as rendering resolution and distillation time schedule, have not been fully explored, suggesting a significant potential for further improvement. In this paper, we present a systematic study of all these elements to obtain elaborate 3D representations.</p><p>We first present Variational Score Distillation (VSD), which treats the corresponding 3D scene given a textual prompt as a random variable instead of a single point as in SDS <ref type="bibr" target="#b32">[33]</ref>. VSD optimizes a distribution of 3D scenes such that the distribution induced on images rendered from all views aligns as closely as possible, in terms of KL divergence, with the one defined by the pretrained 2D diffusion model (see <ref type="bibr">Sec. 3.1)</ref>. Under this variational formulation, VSD naturally characterizes the phenomenon that multiple 3D scenes can potentially align with one prompt. To solve it efficiently, VSD adopts particle-based variational inference <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b8">9]</ref>, and maintains a set of 3D parameters as particles to represent the 3D distribution. We derive a novel gradient-based update rule for the particles via the Wasserstein gradient flow (see Sec. 3.2) and guarantee that the particles will be samples from the desired distribution when the optimization converges (see Theorem 2). Our update requires estimating the score function of the distribution on diffused rendered images, which can be efficiently and effectively implemented by a low-rank adaptation (LoRA) <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b38">39]</ref> of the pretrained diffusion model. The final algorithm alternatively updates the particles and score function.</p><p>We show that SDS is a special case of VSD, by using a single-point Dirac distribution as the variational distribution (see Sec. <ref type="bibr">3.3)</ref>. This insight explains the restricted diversity and fidelity of the generated 3D scenes by SDS. Moreover, even with a single particle, VSD can learn a parametric score model, potentially offering superior generalization over SDS. We also empirically compare SDS and VSD in 2D space by using an identity rendering function that isolates other 3D factors. Similar to ancestral sampling from diffusion models, VSD is able to produce realistic samples using a normal CFG weight (i.e., 7.5). In contrast, SDS exhibits inferior results, sharing the same issues previously observed in text-to-3D, such as over-saturation and over-smoothing <ref type="bibr" target="#b32">[33]</ref>.</p><p>We further systematically study other elements orthogonal to the algorithm for text-to-3D and present a clear design space in Sec. 4. Specifically, we propose a high rendering resolution of 512 ? 512 during training and an annealed distilling time schedule to improve the visual quality. We also propose scene initialization, which is crucial for complex scene generation. Comprehensive ablations in Sec. 5 demonstrate the effectiveness of all the aforementioned elements particularly for VSD. Our overall approach can generate high-fidelity and diverse 3D results. We term it as ProlificDreamer<ref type="foot" target="#foot_0">3</ref> .</p><p>As shown in Fig. <ref type="figure" target="#fig_9">1</ref> and Sec. 5, ProlificDreamer can generate 512 ? 512 rendering resolution and high-fidelity Neural Radiance Fields (NeRF) with rich structure and complex effects (e.g., smoke and drops). Besides, for the first time, ProlificDreamer can successfully construct complex scenes with multiple objects in 360 ? views given the textual prompt. Further, initialized from the generated NeRF, ProlificDreamer can generate meticulously detailed and photo-realistic 3D textured meshes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>We present preliminaries on diffusion models, score distillation sampling, and 3D representations.</p><p>Diffusion models. A diffusion model <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b47">48]</ref> involves a forward process {q t } t?[0,1] to gradually add noise to a data point x 0 ? q 0 (x 0 ) and a reverse process {p t } t?[0,1] to denoise/generate data. The forward process is defined by q t (x t |x 0 ) := N (? t x 0 , ? 2 t I) and q t (x t ) := q t (x t |x 0 )q 0 (x 0 )dx 0 , where ? t , ? t &gt; 0 are hyperparameters satisfying ? 0 ? 1, ? 0 ? 0, ? 1 ? 0, ? 1 ? 1; and the reverse process is defined by denoising from p 1 (x 1 ) := N (0, I) with a parameterized noise prediction network ? ? (x t , t) to predict the noise added to a clean data x 0 , which is trained by minimizing</p><formula xml:id="formula_0">L Diff (?) := E x0?q0(x0),t?U (0,1),??N (0,I) [?(t)?? ? (? t x 0 + ? t ?) -?? 2 2 ],<label>(1)</label></formula><p>where ?(t) is a time-dependent weighting function. After training, we have p t ? q t and thus we can draw samples from p 0 ? q 0 . Moreover, the noise prediction network can be used for approximating the score function of both q t and p t by ? xt log q t (x t ) ? ? xt log p t (x t ) ? -? ? (x t , t)/? t .</p><p>One of the most successful applications of diffusion models is text-to-image generation <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b36">37]</ref>,</p><p>where the noise prediction model ? ? (x t , t, y) is conditioned on a text prompt y. In practice, classifierfree guidance (CFG <ref type="bibr" target="#b14">[15]</ref>) is a key technique for trading off the quality and diversity of the samples, which modifies the model by ?? (x t , t, y) := (1 + s)? ? (x t , t, y) -s? ? (x t , t, ?), where ? is a special "empty" text prompt representing for the unconditional case, and s &gt; 0 is the guidance scale. A larger guidance scale usually improves the text-image alignment but reduces diversity.</p><p>Text-to-3D generation by score distillation sampling (SDS) <ref type="bibr" target="#b32">[33]</ref>. SDS is an optimization method by distilling pretrained diffusion models, also known as Score Jacobian Chaining (SJC) <ref type="bibr" target="#b53">[54]</ref>. It is widely used in text-to-3D generation <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b3">4]</ref> with great promise. Given a pretrained text-to-image diffusion model p t (x t |y) with the noise prediction network ? pretrain (x t , t, y), SDS optimizes a single 3D representation with parameter ? ? ?, where ? is the space of ? with the Euclidean metric. Given a camera parameter c with a distribution p(c) and a differentiable rendering mapping g(?, c) : ? ? R d , denote q ? t (x t |c) as the distribution at time t of the forward diffusion process starting from the rendered image g(?, c) with the camera c and 3D parameter ?. SDS optimizes the parameter ? by solving</p><formula xml:id="formula_1">min ??? L SDS (?) := E t,c (? t /? t )?(t)D KL (q ? t (x t |c) ? p t (x t |y)) ,<label>(2)</label></formula><p>where t ? U(0.02, 0.98), ? ? N (0, I), and x t = ? t g(?, c) + ? t ?. Its gradient is approximated by</p><formula xml:id="formula_2">? ? L SDS (?) ? E t,?,c ?(t)(? pretrain (x t , t, y) -?) ?g(?, c) ?? .<label>(3)</label></formula><p>Notwithstanding this progress, empirical observations <ref type="bibr" target="#b32">[33]</ref> show that SDS often suffers from oversaturation, over-smoothing, and low-diversity issues, which have yet to be thoroughly explained or adequately addressed.</p><p>3D representations. We employ NeRF <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b30">31]</ref> (Neural Radiance Fields) and textured mesh <ref type="bibr" target="#b43">[44]</ref> as two popular and important types of 3D representations. In particular, NeRF represents 3D objects using a multilayer perceptron (MLP) that takes coordinates in a 3D space as input and outputs the corresponding color and density. Here, ? corresponds to the parameters of the MLP. Given camera pose c, the rendering process g(?, c) is defined as casting rays from pixels and computing the weighted sum of the color of the sampling points along each ray to composite the color of each pixel. NeRF is flexible for optimization and is capable of representing extremely complex scenes. Textured mesh <ref type="bibr" target="#b43">[44]</ref> represents the geometry of a 3D object with triangle meshes and the texture with color on the mesh surface. Here the 3D parameter ? consists of the parameters to represent the coordinates of triangle meshes and parameters of the texture. The rendering process g(?, c) given camera pose c is defined by casting rays from pixels and computing the intersections between rays and mesh surfaces to obtain the color of each pixel. The textured mesh allows high-resolution and fast rendering with differentiable rasterization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Variational Score Distillation</head><p>We now present Variational Score Distillation (VSD) (see Sec. 3.1) that learns to sample from a distribution of the 3D scenes. By using 3D parameter particles to represent the target 3D distribution, we derive a principled gradient-based update rule for the particles via the Wasserstein gradient flow (see Sec. 3.2). We further show that SDS is a special case of VSD and constructs an experiment in 2D space to study the optimization algorithm isolated from the 3D representations, explaining the practical issues of SDS both theoretically and empirically (see Sec. 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sampling from 3D Distribution as Variational Inference</head><p>In principle, given a valid text prompt y, there exists a probabilistic distribution of all possible 3D representations. Under a 3D representation (e.g., NeRF) parameterized by ?, such a distribution can  be modeled as a probabilistic density ?(?|y). Denote q ? 0 (x 0 |c, y) as the (implicit) distribution of the rendered image x 0 := g(?, c) given the camera c with the rendering function g(?, c), q ? 0 (x 0 |y) := q ? 0 (x 0 |c, y)p(c)dc as the marginalized distribution w.r.t. the camera distribution p(c), and p 0 (x 0 |y) as the marginal distribution of t = 0 defined by the pretrained text-to-image diffusion model. To obtain 3D representations of high visual quality, we propose to optimize the distribution ? to align its samples with the pretrained diffusion model by solving</p><formula xml:id="formula_3">min ? D KL (q ? 0 (x 0 |y) ? p 0 (x 0 |y)).<label>(4)</label></formula><p>This is a typical variational inference problem that uses the variational distribution q ? 0 (x 0 |y) to approximate (distill) the target distribution p 0 (x 0 |y).</p><p>Directly solving problem (4) is hard because p 0 is rather complex and the high-density regions of p 0 may be extremely sparse in high dimension <ref type="bibr" target="#b45">[46]</ref>. Inspired by the success of diffusion models <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b47">48]</ref>, we construct a series of optimization problems with different diffused distributions indexed by t. As t increases to T , the optimization problem becomes easier because the diffused distributions get closer to the standard Gaussian. We simultaneously solve an ensemble of these problems (termed as variational score distillation or VSD) as follows:</p><formula xml:id="formula_4">? * := arg min ? E t [(? t /? t )?(t)D KL (q ? t (x t |y) ? p t (x t |y))] = arg min ? E t,c [(? t /? t )?(t)D KL (q ? t (x t |c, y) ? p t (x t |y))] ,<label>(5)</label></formula><p>where q ? t (x t |c, y) := q ? 0 (x 0 |c, y)p t0 (x t |x 0 )dx 0 . Here q ? t (x t |y) := q ? 0 (x 0 |y)p t0 (x t |x 0 )dx 0 and p t (x t |y) = p 0 (x 0 |y)p t0 (x t |x 0 )dx 0 are the corresponding noisy distributions at time t with the Gaussian transition p t0 (x t |x 0 ) = N (x t |? t x 0 , ? 2 t I), ?(t) is a time-dependent weighting function, and the second equation is because of the property of KL-divergence.</p><p>Compared with SDS that optimizes for the single point ?, VSD optimizes for the whole distribution ?, from which we sample ?. Notably, we prove that introducing the additional KL-divergence for t &gt; 0 in VSD does not affect the global optimum of the original problem (4), as shown below. Theorem 1 (Global optimum of VSD, proof in Appendix C.4.). For each t &gt; 0, we have</p><formula xml:id="formula_5">D KL (q ? t (x t |y) ? p t (x t |y)) = 0 ? q ? 0 (x 0 |y) = p 0 (x 0 |y).<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Update Rule for Variational Score Distillation</head><p>To solve problem (5), a direct way can be to train another parameterized generative model for ?, but it may bring much computation cost and optimization complexity. Inspired by previous particle-based variational inference <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b8">9</ref>] methods, we maintain n 3D parameters <ref type="foot" target="#foot_1">4</ref> {?} n i=1 as particles and derive a novel update rule for them. Intuitively, we use {?} n i=1 to "represent" the current distribution ?, and ? (i) will be samples from the optimal distribution ? * if the optimization converges. Such optimization can be realized by simulating an ODE w.r.t. ?, as shown in the following theorem. Theorem 2 (Wasserstein gradient flow of VSD, proof in Appendix C). Starting from an initial distribution ? 0 , denote the Wasserstein gradient flow minimizing problem (5) in the distribution  (function) space at each time ? ? 0 as {? ? } ? ?0 with ? ? = ? * . Then we can sample ? ? from ? ? by firstly sampling ? 0 ? ? 0 (? 0 |y) and then simulating the following ODE:</p><formula xml:id="formula_6">d? ? d? = -E t,?,c ?(t) -? t ? xt log p t (x t |y) score of noisy real images -(-? t ? xt log q ?? t (x t |c, y)) score of noisy rendered images ?g(? ? , c) ?? ? ,<label>(7)</label></formula><p>where q ?? t is the corresponding noisy distribution at diffusion time t w.r.t. ? ? at ODE time ? .</p><p>According to Theorem 2, we can simulate the ODE in Eq. ( <ref type="formula" target="#formula_6">7</ref>) for a large enough ? to approximately sample from the desired distribution ? * . The ODE involves the score function of noisy real images and that of noisy rendered images at each time <ref type="foot" target="#foot_2">5</ref> ? . The score function of noisy real images -? t ? xt log p t (x t |y) can be approximated by the pretrained diffusion model ? pretrain (x t , t, y). The score function of noisy rendered images -? t ? xt log q ?? t (x t |c, y) is estimated by another noise prediction network ? ? (x t , t, c, y), which is trained on the rendered images by {? (i) } n i=1 with the standard diffusion objective (see Eq. ( <ref type="formula" target="#formula_0">1</ref>)):</p><formula xml:id="formula_7">min ? n i=1 E t?U (0,1),??N (0,I),c?p(c) ?? ? (? t g(? (i) , c) + ? t ?, t, c, y) -?? 2 2 .<label>(8)</label></formula><p>In practice, we parameterize ? ? by either a small U-Net <ref type="bibr" target="#b37">[38]</ref> or a LoRA (Low-rank adaptation <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b38">39]</ref>) of the pretrained model ? pretrain (x t , t, y), and add additional camera parameter c to the condition embeddings in the network. In most cases, we find that using LoRA can greatly improve the fidelity of the obtained samples (e.g., see results in Fig. <ref type="figure" target="#fig_9">1</ref>). We believe that it is because LoRA is designed for efficient few-shot fine-tuning and can leverage the prior information in ? pretrain (the information of both images and text corresponding to y).</p><p>Note that at each ODE time ? , we need to ensure ? ? matches the current distribution q ?? t . Thus, we optimize ? ? and ? (i) alternately, and each particle ? (i) is updated by</p><formula xml:id="formula_8">? (i) ? ? (i) -?? ? L VSD (? (i) ),</formula><p>where ? &gt; 0 is the step size (learning rate). According to Theorem 2, the corresponding gradient is</p><formula xml:id="formula_9">? ? L VSD (?) ? E t,?,c ?(t) (? pretrain (x t , t, y) -? ? (x t , t, c, y)) ?g(?, c) ?? ,<label>(9)</label></formula><p>where x t = ? t g(?, c)+? t ?. We show the approach of VSD in Fig. <ref type="figure" target="#fig_3">3</ref> (see pseudo code in Appendix F).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparison with SDS</head><p>We now systematically compare VSD with SDS in both theory and practice.</p><p>SDS as a speical case of VSD. Theoretically, comparing the update rules of SDS (Eq. ( <ref type="formula" target="#formula_2">3</ref>)) and VDS (Eq. ( <ref type="formula" target="#formula_9">9</ref>)), SDS is a special case of VSD by using a single-point Dirac distribution ?(?|y) ? ?(? -? (1) )</p><p>Table <ref type="table">1</ref>: Design space of text-to-3D via 2D diffusion. We highlight the contributions of this paper that improve the fidelity, diversity and ability to generate complex scenes by * , ? and ? respectively.</p><p>Method DreamFusion <ref type="bibr" target="#b32">[33]</ref> Magic3D <ref type="bibr" target="#b19">[20]</ref> Fantasia3D <ref type="bibr" target="#b3">[4]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mesh Training</head><p>Distillation objective * ? -SDS (Eq. ( <ref type="formula" target="#formula_2">3</ref>)) SDS (Eq. ( <ref type="formula" target="#formula_2">3</ref>)) VSD (Eq. ( <ref type="formula" target="#formula_9">9</ref>)) CFG * -100 100 7.5</p><p>as the variational distribution (see Appendix C.3 for derivation). In particular, VSD not only employs potentially multiple particles but also learns a parametric score function ? ? even for a single particle (i.e., n = 1), potentially offering superior generalization over SDS. Moreover, by using LoRA, VSD can additionally exploit the text prompt y in the estimation ? ? (x t , t, c, y), while the Gaussian noise ? used in SDS cannot leverage the information from y.</p><p>VSD is friendly to CFG. As VSD aims to sample ? from the optimal ? * defined by the pretrained model ? pretrain , the effects by tuning the CFG in ? pretrain for 3D samples ? by VSD are quite similar to the effects for the 2D samples by the traditional ancestral sampling <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b26">27]</ref>. Therefore, VSD can tune CFG as flexibly as the classic text-to-image methods, and we use the same setting of CFG (e.g. 7.5) as the common text-to-image generation task for the best performance. To the best of our knowledge, this for the first time addresses the problem in previous SDS <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b27">28]</ref> that it usually requires a large CFG (i.e., 100).</p><p>VSD vs. SDS in 2D experiments that isolate 3D representations. To directly compare SDS and VSD, we consider a special case of the rendering function g(?) to decouple the optimization algorithm from 3D representations. In particular, we set g(?, c) ? ? for any c. Then the rendered image x = g(?, c) = ? is the same 2D image as ?. In such a case, optimizing the parameter ? is equivalent to generating an image in 2D space, thereby independent of the 3D representation. We show the results of different sampling methods in Fig. <ref type="figure" target="#fig_3">3</ref>. SDS exhibits failure under both small and large CFG weights. Particularly with the default CFG weight (i.e., 100) used in SDS, the 2D samples share the same issues previously observed in text-to-3D such as over-saturation and oversmoothing <ref type="bibr" target="#b32">[33]</ref>. In contrast, VSD demonstrates flexibility in accommodating various CFG weights and produces realistic samples using a normal CFG weight (i.e., 7.5), behaving similarly to ancestral sampling from diffusion models. See more details and analysis in Appendix H.</p><p>As other 3D factors are isolated in this comparison, these theoretical and empirical results suggest that the aforementioned practical issues of SDS <ref type="bibr" target="#b32">[33]</ref> stem from the oversimplified variational distribution and large CFG employed by SDS. Such results strongly motivate us to employ VSD for text-to-3D generation, where it still substantially and consistently outperforms SDS (see evidence in Sec 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">ProlificDreamer</head><p>We further present a clear design space for text-to-3D in Sec. 4.1 and systematically study other elements orthogonal to the distillation algorithm in Sec. 4.2. Combining all improvements highlighted in Tab. 1, we arrive at ProlificDreamer, an advanced text-to-3D approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DreamFusion Magic3D Ours</head><p>A plate piled high with chocolate chip cookies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fantasia3D</head><p>A 3D model of an adorable cottage with a thatched roof.</p><p>Figure <ref type="figure">4</ref>: Comparison with baselines. Our results have higher fidelity and more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Design Space of Text-to-3D Generation</head><p>We adopt the two-stage approach <ref type="bibr" target="#b19">[20]</ref>, with several improvements in the design space of text-to-3D generation as summarized in Table <ref type="table">1</ref>. Specifically, in the first stage, we optimize a high-resolution (e.g., 512) NeRF by VSD to utilize its high flexibility for generating scenes with complex geometry.</p><p>In the second stage, we use DMTet <ref type="bibr" target="#b43">[44]</ref> to extract textured mesh from the NeRF obtained in the first stage, and further fine-tune the textured mesh for high-resolution details. The second stage is optional because both NeRF and mesh have their own advantages in representing 3D content and are preferred in certain cases. Nevertheless, ProlificDreamer can generate both high-fidelity NeRFs and meshes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">3D Representation and Training</head><p>We systematically study other elements orthogonal to the algorithmic formulation. Specifically, we propose a high rendering resolution of 512 ? 512 during training and an annealed distilling time schedule to improve the visual quality. We also carefully design a scene initialization, which is crucial for complex scene generation.</p><p>High-resolution rendering for NeRF training. We choose Instant NGP <ref type="bibr" target="#b30">[31]</ref> for efficient highresolution rendering and optimize NeRF with up to 512 training resolution using VSD. Specifically, we find that SDS is one of the main bottlenecks for optimizing NeRFs in prior works <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b19">20]</ref>. Instead, by applying VSD, we can obtain high-fidelity NeRFs with resolutions varying from 64 to 512.</p><p>Scene initialization for NeRF training. We initialize the density for NeRF as</p><formula xml:id="formula_10">? init (?) = ? ? (1 - ||?||2 r )</formula><p>, where ? ? is the density strength, r is the density radius, and ? is the coordinate. For objectcentric scenes, we follow the object-centric initialization used in Magic3D <ref type="bibr" target="#b19">[20]</ref> with ? ? = 10 and r = 0.5; For complex scenes, we propose scene initialization by setting ? ? = -10 to make the density "hollow" and r = 2.5 that encloses the camera. We show in Appendix E that the scene initialization can help to generate high-fidelity complex scenes without other modifications to the existing algorithm. In addition, we can further add a centric object to the complex scene by using object-centric initialization for ||?|| 2 &lt; 5/6 and scene initialization for others, where the hyperparameter 5/6 ensures the initial density function is continuous.</p><p>Annealed time schedule for score distillation. We utilize a simple two-stage annealing of time step t in the score distillation objective, suitable for both SDS (Eq. ( <ref type="formula" target="#formula_2">3</ref>)) and VSD (Eq. ( <ref type="formula" target="#formula_9">9</ref>)). For the first several steps we sample time steps t ? U(0.02, 0.98) and then anneal into t ? U(0.02, 0.50).</p><p>The key insight is that, essentially, we aim to match the original q ? 0 (x 0 |y) with p 0 (x 0 |y). The KL-divergence for larger t can provide reasonable optimization direction during the early stage of training. During training, while x is approaching the support of p 0 (x), a smaller t can narrow the gap between p t (x|y) and p 0 (x|y), and provide elaborate details aligning with p 0 (x|y).</p><p>Mesh representation and fine-tuning. We adopt a coordinate-based hash grid encoder inherited from NeRF stage to represent the mesh texture. We follow Fantasia3D <ref type="bibr" target="#b3">[4]</ref> to disentangle the optimization of geometry and texture by first optimizing the geometry using the normal map and then optimizing the texture. In our initial experiments, we find that optimizing geometry with VSD provides no more details than using SDS. This may be because the mesh resolution is not large enough to represent high-frequency details. Thus, we optimize geometry with SDS for efficiency. But unlike Fantasia3D <ref type="bibr" target="#b3">[4]</ref>, our texture optimization is supervised by VSD with CFG = 7.5 with the annealed time schedule, which can provide more details than SDS.</p><p>5 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Results of ProlificDreamer</head><p>We show the generated results of ProlificDreamer in Fig. <ref type="figure" target="#fig_9">1</ref>(a) and 1(b), including high-fidelity mesh and NeRF results. All the results are generated by VSD. For all experiments without mentioned, VSD uses n = 1 particle for a fair comparison with SDS. In Fig. <ref type="figure" target="#fig_9">1</ref>(c), we also demonstrate VSD can generate diverse results, showing that different particles in a round are diverse (with n = 4).</p><p>Object-centric generation. We compare our method with three SOTA baselines, DreamFusion <ref type="bibr" target="#b32">[33]</ref>, Magic3D <ref type="bibr" target="#b19">[20]</ref> and Fantasia3D <ref type="bibr" target="#b3">[4]</ref>. All of the baselines are based on SDS. Since none of them is open-sourced, we use the figures from their papers. As shown in Fig. <ref type="figure">4</ref>, ProlificDreamer generates 3D objects with higher fidelity and more details, which demonstrates the effectiveness of our method.</p><p>Large scene generation. As shown in Fig. <ref type="figure" target="#fig_9">1</ref>(b), our method can generate 360 ? scenes with highfidelity and fine details. The depth map shows that the scenes have geometry instead of being a 360 ? textured sphere, verifying that with our scene initialization alone we can generate high-fidelity large scenes without much modification to existing components. See more results in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ablation Study</head><p>Ablation on NeRF training. Fig. <ref type="figure" target="#fig_4">5</ref> provides the ablation on NeRF training. Starting from the common setting <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b19">20]</ref> with 64 rendering resolution and SDS loss, we ablate our proposed improvements step by step, including increasing resolution, adding annealed time schedule, and adding VSD all improve the generated results. It demonstrates the effectiveness of our proposed components. We provide more ablation on large scene generation in Appendix E, with a similar conclusion.</p><p>Ablation on mesh fine-tuning. We ablate between SDS and VSD on mesh fine-tuning, as shown in Appendix E. Fine-tuning texture with VSD provides higher fidelity than SDS. As the fine-tuned results of textured mesh are highly dependent on the initial NeRF, getting a high-quality NeRF at the first stage is crucial. Note that the provided results of both VSD and SDS in mesh fine-tuning are based on and benefit from the high-fidelity NeRF results in the first stage by our VSD.</p><p>Ablation on CFG. We perform ablation to explore how CFG affects generation diversity. We find that smaller CFG encourages more diversity. Our VSD works well with small CFG and provides considerable diversity, while SDS cannot generate plausible results with small CFG (e.g., 7.5), which limits its ability to generate diverse results. Results and more details are shown in Appendix I.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we systematically study the problem of text-to-3D generation. In terms of the algorithmic formulation, we propose variation score distillation (VSD), a principled particle-based variational framework that treats the 3D parameter as a random variable and infers its distribution. VSD naturally generalizes SDS in the variational formulation and explains and addresses the practical issues of SDS observed before. With other orthogonal improvements to 3D representations, our overall approach, ProlificDreamer, can generate high-fidelity NeRF and photo-realistic textured meshes.</p><p>Limitations and broader impact. Although ProlificDreamer achieves remarkable text-to-3D results, currently the generation takes hours of time, which is much slower than image generation by a diffusion model. Although large scene generation can be achieved with our scene initialization, the camera poses during training are regardless of the scene structure, which may be improved by devising an adaptive camera pose range according to the scene structure for better-generated details. Also, like other generative models, our method may be utilized to generate fake and malicious contents, which needs more attention and caution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Additional Experiment Results of 3D Textured Meshes</head><p>A sliced loaf of fresh bread.</p><p>A plate of fried chicken and waffles with maple syrup on them.</p><p>A blue tulip.</p><p>A delicious croissant.</p><p>A rabbit, animated movie character, high detail 3d model.</p><p>A lionfish.</p><p>A small saguaro cactus planted in a clay pot.</p><p>A cauldron full of gold coins. We provide more results of ProlificDreamer of 3D textured meshes in Fig. <ref type="figure" target="#fig_5">6</ref> and Fig. <ref type="figure" target="#fig_6">7</ref>.</p><p>A DSLR photo of an imperial state crown of England.</p><p>A rotary telephone carved out of wood.</p><p>A marble bust of a mouse.</p><p>A typewriter.</p><p>A praying mantis wearing roller. A plush dragon toy.</p><p>A Matte painting of a castle made of cheesecake surrounded by a moat made of ice cream. An old vintage car.</p><p>A baby bunny sitting on top of a stack of pancakes.</p><p>A banana peeling itself.</p><p>A delicious croissant.</p><p>A blue jay standing on a large basket of rainbow macarons. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours DreamFusion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional Experiment Results of 3D NeRF</head><p>Here, we provide more generated results in Fig. <ref type="figure" target="#fig_7">8</ref> of high quality NeRF. We compare with Dream-Fusion, as DreamFusion also uses NeRF representation. It can be seen from the figure that Pro-lificDreamer generates better NeRF results in terms of fidelity and details, which demonstrates the effectiveness of our VSD against SDS.</p><p>In Fig. <ref type="figure">9</ref>, we provide more result of large scene with 360 ? environment using our scene initialization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Theory of Variational Score Distillation C.1 Particle-based Variational Inference</head><p>Particle-based variational inference (ParVI) <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b8">9]</ref> aims to draw (particle) samples from a desired distribution by minimizing the KL-divergence between particle samples and the desired distribution, and is widely-used in Bayesian inference <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b20">21]</ref>. Specifically, denote P(?) as the set containing all the distributions on a support space ? with Euclidean distance, and W 2 (?) := {? ? P(?) : ?? 0 ? ? s.t. E ?(?) [?? -? 0 ? 2 ] &lt; +?} as the 2-Wasserstein space <ref type="bibr" target="#b51">[52]</ref> equipped with the Wasserstein 2-distance <ref type="bibr" target="#b51">[52]</ref>. Given a desired distribution ? * ? P(?), ParVI starts with some (n) particles {? (i) 0 } n i=1 sampled from an initial distribution ? 0 ? P(?) and update Figure <ref type="figure">9</ref>: Result of high quality NeRF from ProlificDreamer, a large scene with 360 ? environment using our scene initialization. The prompt here is Small lavender isometric room, soft lighting, unreal engine render, voxels.</p><p>these particles with a vector field v ? (?) on ? by d?</p><formula xml:id="formula_11">(i) ? = v ? (? (i)</formula><p>? )d? at each time ? ? 0, such that the distribution ? ? of {? (i) ? } n i=0 converges to ? * as ? ? ? and n ? ?. Typically, the measures ? ? follows the "steepest descending curves" of a functional on W 2 (?), which is known as the Wasserstein gradient flow. A common setting is to define the functional of ? ? as the KL-divergence D KL (? ? ? ? * ), and then ? ? follows ? ? ? ? = -? ? ? (? ? ? ? log ? * ?? ), and the corresponding vector field is v ? (? ? ) = ? ? log ? * ?? . However, directly computing the vector field is intractable because it is non-trival to compute the time-dependent score function ? ? log ? ? (?). To tackle this issue, traditional ParVI methods either restrict the functional gradient within RKHS and leverage analytical kernels to approximate the vector field <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b21">22]</ref>, or learn an neural network to estimate the vector field <ref type="bibr" target="#b7">[8]</ref>; but all of these methods are hard to scale up to high-dimensional data such as images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 VSD as Particle-based Variational Inference</head><p>In this section, we formally propose the theoretical guarantee of VSD.</p><p>Denote W 2 (?) as the 2-Wasserstein space <ref type="bibr" target="#b51">[52]</ref> of the probability distributions defined on the parameter space ?. For any distribution ? ? W 2 (?) with a corresponding random variable ? ? ?(?|y), denote the implicit distribution for x 0 := g(?, c) as q ? 0 (x 0 |c, y), and let q ? t (x t |c, y) := q ? 0 (x 0 |c, y)q t0 (x t |x 0 )dx 0 be the marginal distribution of the random variable x t during the diffusion process. We want to optimize the distribution ?(?|y) in the function space W 2 (?) by distilling a pretrained diffusion model p t (x t |y) via minimizing the following functional of ?:</p><formula xml:id="formula_12">min ??W2(?) E[?] := E t,?,c ? t ? t ?(t)D KL (q ? t (x t |c, y) ? p t (x t |y)) .<label>(10)</label></formula><p>Compared with the vanilla SDS in Eq. ( <ref type="formula" target="#formula_1">2</ref>) which optimizes the parameter ? in the parameter space ?, here we aim to optimize the distribution (measure) ?(?|y) in the function space W 2 (?). Inspired by previous ParVI techniques <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b54">55]</ref>, we firstly derive the gradient flow minimizing</p><formula xml:id="formula_13">E[?] in W 2 (?)</formula><p>and the update rule of each particle ? ? ?(?), as shown in the following theorem. Theorem 3 (Wasserstein gradient flow of score distillation). Starting from an initial distribution ? 0 (?|y), the gradient flow </p><formula xml:id="formula_14">{? ? } ? ?0 minimizing E[? ? ] on W 2 (?) at each time ? ? 0 satisfies ?? ? (?|y) ?? = -? ? ? ? ? (?|y)E t,</formula><formula xml:id="formula_15">? ? ? ?g(? ? , c) ?? ? ? ? ? . (<label>12</label></formula><formula xml:id="formula_16">)</formula><p>Theorem 3 shows that by letting the random variable ? ? ? ? ? (? ? |y) move across the ODE trajectory in Eq. ( <ref type="formula" target="#formula_15">12</ref>), its underlying distribution ? ? will move by the direction of the steepest descent that minimizes E[?]. Therefore, to obtain samples (in ?) from ? * = arg min ? E[?], we can simulate the ODE in Eq. ( <ref type="formula" target="#formula_15">12</ref>) by estimating two score functions ? xt log p t (x t |y) and ? xt log q ?? t (x t |c, y) at each ODE time ? , which corresponds to the VSD objective in Eq. ( <ref type="formula" target="#formula_9">9</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Discussions of SDS / SJC and VSD</head><p>There are three main differences between our proposed VSD and previous SDS <ref type="bibr" target="#b32">[33]</ref> / SJC <ref type="bibr" target="#b53">[54]</ref>: (1) VSD can optimize multiple (n ? 1) 3D objects, while SDS / SJC only optimizes a single (n = 1) 3D object. (2) VSD can generate high-fidelity results with the common (e.g., 7.5) CFG (for both n = 1 and n &gt; 1), while SDS / SJC needs extremely large (e.g. 100) CFG. (3) SDS / SJC is a special case of VSD by using a single-point Dirac distribution ?(?|y) ? ?(? -? (1) ) as the variational distribution.</p><p>SDS / SJC as a special case of VSD. Here we explain SDS / SJC under our variational inference framework. If we directly approximate ?(?|y) ? ?(? -? (1) ) by its empirical distribution with the single sample ? (1) , then we have q ? t (x t |c, y) ? N (x t |? t g(? (1) , c), ? 2 t I) and thus -? t ? xt log q ? t (x t |c, y) ? (x t -? t g(? (1) , c))/? t = ?, which recovers the vanilla SDS / SJC. Therefore, SDS / SJC is a special case of VSD for the underlying distribution ?(?|y) by the empirical distribution with a single point. Such an approximation has no generalization ability for ? ? = ? (1) , and thus the updating direction by the score function -? t ? xt log q ? t (x t |c, y) (or equivalently, the Gaussian noise ?) may be rather bad at low-density regions, resulting poor samples for the final ? (1) . Instead, VSD regards ? (1) as samples from the underlying distribution ?(?|y) and trains a neural network ? ? to approximate the corresponding score functions, thus can leverage the generalization ability of neural networks for better approximating the underlying distribution ?(?|y). Moreover, by using LoRA, VSD can additionally exploit the text prompt y in the estimation ? ? (x t , t, c, y), while the Gaussian noise ? used in SDS cannot leverage the information from y. SDS / SJC as mode-seeking, VSD as sampling. VSD aims to sample ? from the optimal ? * , while SDS / SJC aims to find the optimal ? * minimizing the objective in Eq. ( <ref type="formula" target="#formula_1">2</ref>). As the global optimum ? * in SDS / SJC is the mode of Eq. ( <ref type="formula" target="#formula_1">2</ref>), SDS / SJC is also known as performing mode-seeking <ref type="bibr" target="#b32">[33]</ref>. To demonstrate the differences between sampling and mode-seeking, we consider a special case of the rendering function g(?) to decouple the optimization algorithm and the 3D representation. In particular, we set g(?, c) ? ? for any c and ? ? R d , then the rendered image x = g(?, c) = ? is the same 2D image as ?. In such a case, we have q ? 0 = ?, and it is easy to prove that ? * = p 0 (according to Theorem 1). For VSD, sampling ? from ? * is corresponding to the traditional ancestral sampling <ref type="bibr" target="#b26">[27]</ref> from p 0 ; while for SDS / SJC, we have</p><formula xml:id="formula_17">? ? L SDS (?) = E t,? [?(t)? pretrain (x t , t, y)] ? E t,? [-? t ?(t)? xt log p t (x t |y)],</formula><p>and thus the optimal ? * is the mode of the "averaged likelihood" of p t for all t. However, it is common that the mode of deep generative models may have poor sample quality <ref type="bibr" target="#b31">[32]</ref>. We show in Fig. <ref type="figure" target="#fig_3">3</ref> that under the same CFG (7.5), both VSD and ancestral sampling can generate good samples but the sample quality of SDS is quite poor. SDS / SJC requires large CFG, while VSD is friendly to CFG. As VSD aims to sample ? from the optimal ? * defined by the pretrained model ? pretrain , the effects by tuning the CFG in ? pretrain for 3D samples ? by VSD are quite similar to the effects for the 2D samples by the traditional ancestral sampling <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b26">27]</ref>. Therefore, VSD can tune CFG as flexibly as the classic text-to-image methods, and we use the same setting of CFG (e.g. 7.5) as the common text-to-image generation task for the best performance. To the best of our knowledge, this for the first time addresses the problem in previous SDS <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b27">28]</ref> that it usually requires a large CFG (i.e., 100). Specifically, SDS (Eq. ( <ref type="formula" target="#formula_2">3</ref>)) uses (? pretrain (x t , t, y) -?) while VSD (Eq. ( <ref type="formula" target="#formula_9">9</ref>))) uses (? pretrain (x t , t, y) -? ? (x t , t, c)). For example, for the 2D special case of g(?, c) ? ?, we have</p><formula xml:id="formula_18">? ? L SDS (?) = E t,? [?(t)? pretrain (x t , t, y)] and ? ? L VSD (?) = E t,? [?(t)(? pretrain (x t , t, y) -? ? (x t , t, c))].</formula><p>Intuitively, to obtain highly detailed samples, the updating direction for ? needs to be "fine" and "sharp". As SDS only depends on ? pretrain , it needs a large CFG (= 100) to make sure ? pretrain to be "sharp" enough; however, large CFG, in turn, reduces the diversity of the results and also hurts the quality. Instead, VSD leverages an additional score function ? ? (x t , t, c) to give a more elaborate direction than the zero-mean Gaussian noise, and the updating direction can be rather "fine" and "sharp" due to the difference between ? pretrain and ? ? . We empirically find that VSD can obtain much better quality than SDS in both 2D and 3D generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Proof of Main Theorem</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4.1 Proof of Theorem 1</head><p>Proof of Theorem 1. Denote x q 0 as the random variable following q ? 0 (x 0 |y) and x q t as the random variable following q ? t (x t |y). By the definition of q ? t , we have</p><formula xml:id="formula_19">x q t = ? t x q 0 + ? t ?,<label>(13)</label></formula><p>where ? ? N (0, I). Therefore, the characteristic functions of q ? t and q ? 0 satisfy</p><formula xml:id="formula_20">? q ? t (s) = ? q ? 0 (? t s) ? ? N (0,I) (? t s) = e -? 2 t s 2 2 ? q ? 0 (? t s).<label>(14)</label></formula><p>Similarly, the characteristic functions of p t and p 0 satisfy</p><formula xml:id="formula_21">? pt (s) = e -? 2 t s 2 2 ? p0 (? t s).<label>(15)</label></formula><p>Therefore, we have</p><formula xml:id="formula_22">D KL (q ? t (x t |y) ? p t (x t |y)) = 0 ? q ? t = p t ? ? q ? t = ? pt ? ? q ? 0 = ? p0 ? q ? 0 = p 0<label>(16)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4.2 Proof of Theorem 3</head><p>As the gradient flow minimizing</p><formula xml:id="formula_23">E[?] on W 2 (?) satisfies ?? ? ?? = -? W2 E[?] = ? ? ? ? ? ? ? ?E[? ? ] ?? ? ,<label>(17)</label></formula><p>thus we only need to compute the first variation ?E [?]  ?? . We propose the following lemmas for computing ?E[?]  ?? . Lemma 1. For p, q ? W 2 (R d ), for any x ? R d , ?D KL (q ? p) ?q [x] = log q(x) -log p(x) + 1 <ref type="bibr" target="#b17">(18)</ref> Proof. This is a classic conclusion in particle-based variational inference (e.g., see Sec.3.2. in <ref type="bibr" target="#b2">[3]</ref>).</p><p>Lemma 2. For a fixed c and x t ,</p><formula xml:id="formula_24">?q ? t (x t |c, y) ?? [?] = q t0 (x t |x 0 ) = N (x t |? t x 0 , ? 2 t I),<label>(19)</label></formula><p>where x 0 = g(?, c).</p><p>Proof of Lemma 2. By the defination of q ? t , we have</p><formula xml:id="formula_25">q ? t (x t |c, y) = E q ? 0 (x0|c,y) [q t0 (x t |x 0 )] = E ?(?|y) [q t0 (x t |g(?, c))],<label>(20)</label></formula><formula xml:id="formula_26">so ?q ? t (x t |c, y) ?? [?] = q t0 (x t |g(?, c)) = N (x t |? t g(?, c), ? 2 t I)<label>(21)</label></formula><p>Lemma 3.</p><formula xml:id="formula_27">?D KL (q ? t (x t |c, y) ? p t (x t |y)) ?? [?] = E ? [log q ? t (x t |c, y) -log p t (x t |y) + 1],<label>(22)</label></formula><p>where x t = ? t g(?, c) + ? t ?, ? ? N (?|0, I).</p><p>Proof of Lemma 3. By the chain rule of functional derivative, according to Lemma 1 and Lemma 2, we have</p><formula xml:id="formula_28">?D KL (q ? t (x t |c, y) ? p t (x t |y)) ?? [?] = ?D KL (q ? t ? p t ) ?q ? t [x t ] ? ?q ? t (x t |c, y) ?? [?]dx t<label>(23)</label></formula><p>= (log q ? t (x t |c, y) -log p t (x t |y) + 1)q t0 (x t |x 0 )dx t (24)</p><formula xml:id="formula_29">= E qt0(xt|x0) [log q ? t (x t |c, y) -log p t (x t |y) + 1]<label>(25)</label></formula><formula xml:id="formula_30">= E ? [log q ? t (x t |c, y) -log p t (x t |y) + 1] ,<label>(26)</label></formula><p>where ? ? N (?|0, I), x 0 = g(?, c), x t = ? t x 0 + ? t ?.</p><p>Below we provide the proof of Theorem 3.</p><p>Proof of Theorem 3. According to Lemma 3, we have</p><formula xml:id="formula_31">?E[?] ?? [?] = E t,?,c ? t ? t ?(t) (log q ? t (x t |c, y) -log p t (x t |y) + 1) ,<label>(27)</label></formula><p>where</p><formula xml:id="formula_32">x t = ? t g(?, c) + ? t ?. So ? ? ?E[?] ?? [?] = E t,?,c ? t ? t ?(t) (? xt log q ? t (x t |c, y) -? xt log p t (x t |y)) ?x t ??<label>(28)</label></formula><formula xml:id="formula_33">= E t,?,c ? t ?(t) (? xt log q ? t (x t |c, y) -? xt log p t (x t |y)) ?g(?, c) ?? .<label>(29)</label></formula><p>Thus, the measure ? ? at step ? during the Wasserstein gradient flow minimizing</p><formula xml:id="formula_34">E[?] on W 2 (?) satisfies ?? ? ?? = -? W2 E[?]<label>(30)</label></formula><formula xml:id="formula_35">= ? ? ? ? ? ? ? ?E[? ? ] ?? ?<label>(31)</label></formula><formula xml:id="formula_36">= ? ? ? ? ? (?|y)E t,?,c ? t ?(t) (? xt log q ?? t (x t |c, y) -? xt log p t (x t |y)) ?g(?, c) ?? .<label>(32)</label></formula><p>By the definition of Fokker-Planck equation, the corresponding process of each particle ? ? at time ? satisfies</p><formula xml:id="formula_37">d? ? d? = E t,?,c ? t ?(t) (? xt log p t (x t |y) -? xt log q ?? t (x t |c, y)) ?g(?, c) ?? .<label>(33)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Related Works</head><p>Diffusion models. Score-based generative model <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48]</ref> and diffusion models <ref type="bibr" target="#b13">[14]</ref> have shown great performance in image synthesis <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b6">7]</ref>. Recently, large-scale diffusion models have shown great performance in text-to-image synthesis <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b36">37]</ref>, which provides an opportunity to utilize it for zero-shot text-to-3D generation.</p><p>Text-to-3D generation. DreamField <ref type="bibr" target="#b18">[19]</ref> proposes a text-to-3D method using CLIP <ref type="bibr" target="#b33">[34]</ref> guidance. DreamFusion <ref type="bibr" target="#b32">[33]</ref> proposes a text-to-3D method using 2D diffusion models. Score Jacobian Chaining (SJC) <ref type="bibr" target="#b53">[54]</ref> derives the training objective of text-to-3D using a 2D diffusion model from another theoretical basis. Magic3D <ref type="bibr" target="#b19">[20]</ref> extends text-to-3D to a higher resolution with mesh <ref type="bibr" target="#b43">[44]</ref> representation. Latent-NeRF <ref type="bibr" target="#b27">[28]</ref> optimizes NeRF in latent space. Fantasia3D <ref type="bibr" target="#b3">[4]</ref> optimizes a mesh with DMTet <ref type="bibr" target="#b43">[44]</ref> from scratch. Although Fantasia3D achieves remarkable zero-shot text-to-3D generation, it requires user-provided shape guidance for generation of complex geometry. <ref type="bibr" target="#b16">[17]</ref> propose score debiasing and prompt debiasing to mitigate multiface problem and is orthogonal to our work. TextMesh <ref type="bibr" target="#b50">[51]</ref> is contemporary with us and proposes a different pipeline for high-fidelity text-to-3D generation. 3DFuse <ref type="bibr" target="#b42">[43]</ref> proposes to incorporate 3D awareness into 2D diffusion for better 3D consistency in text-to-3D generation. In addition, adjusting time schedule has also been discussed in previous works <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b53">54]</ref>. However, previous works either require carefully devising the schedule <ref type="bibr" target="#b11">[12]</ref> or perform inferior to the simple random time schedule <ref type="bibr" target="#b53">[54]</ref>. Instead, our 2-stage annealing schedule is easy to train and achieves better generation quality than the random time schedule.</p><p>Text-driven large scene generation. Text2Room <ref type="bibr" target="#b15">[16]</ref> can generate indoors room from a given prompt. However, it uses additional monocular depth estimation models as prior, and we do not use any additional models. Our method generates with in the wild text prompt and use only a text to image diffusion model. Set-the-scene <ref type="bibr" target="#b4">[5]</ref> is a contemporary work with us aimed for large scene generation with a different pipeline.</p><p>Overall, ProlificDreamer uses an advanced optimization algorithm, i.e, VSD with our proposed two-stage annealed time schedule, which has a significant advantage over the previous SDS / SJC (see Appendix C.3 for details). As a result, ProlificDreamer achieves high-fidelity NeRF with rich structure and complex effects (e.g., smoke and drops) and photo-realistic mesh results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Additional Ablation Study E.1 Ablation Study on Large Scene Generation</head><p>Here we perform an ablation study on large scene generation to validate the effectiveness of our proposed improvements. We start from 64 rendering resolution, with SDS loss and our scene initialization. The results are shown in Figure <ref type="figure" target="#fig_8">10</ref>. It can be seen from the figure that, with our scene initialization, the results are with 360 ? surroundings instead of being object-centric. Increasing rendering resolution is slightly beneficial. Adding annealed time schedule improves the visual quality of the results. Replacing SDS with VSD makes the results more realistic with more details.   Figure <ref type="figure" target="#fig_9">11</ref>: Pipeline of ProlificDreamer along with ablation study of VSD. After generating a highquality NeRF, we extract and finetune a textured mesh. VSD provides high-fidelity texture, while SDS tends to generate smoother results. Note that both VSD and SDS in mesh fine-tuning are based on and benefit from the high-fidelity NeRF results by our VSD. The prompt here is an elephant skull.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Ablation Study on Mesh Fine-tuning</head><p>Here we provide an ablation study on mesh fine-tuning. Fine-tuning with textured mesh further improves the quality compared to the NeRF result. Fine-tuning texture with VSD provides higher fidelity than SDS. Note that both VSD and SDS in mesh fine-tuning is based on and benefit from the high-fidelity NeRF results by our VSD. And it's crucial to get a high-quality NeRF with VSD at the first stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 Ablation on Number of Particles</head><p>Here we provide ablation study on number of particles. We vary the number of particles in 1, 2, 4, 8 and examine how will the number of particles affects the generated results. The CFG of VSD is set as 7.5. The results are shown in Fig. <ref type="figure" target="#fig_11">12</ref>. As is shown in the figure, the diversity of the generated results is slightly larger as the number of particles increases. Meanwhile, the quality of generated results is not affected much by the number of particles. Owing to the high computation overhead to optimize 3D representations and limitations on computation resources, we now only test at most 8 particles.</p><p>We provide a 2D experiment with 2048 particles in Appendix H to demonstrate the scalability of VSD. We leave the experiments of more particles in 3D as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.4 Ablation on Rendering Resolution</head><p>Here we provide an ablation study on the rendering resolution during NeRF training with VSD.</p><p>As shown in Fig. <ref type="figure" target="#fig_9">13</ref>, training with a higher resolution produces better results with finer details. In addition, our VSD still provides competitive results under a lower training resolution (128 or 256), which is more computationally efficient than the 512 resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Algorithm for Variational Score Distillation</head><p>We provide a summarized algorithm of variational score distillation in Algorithm 1.</p><p>We initialize one or several 3D structures {? (i) } n i=1 , a noise prediction model ? ? parameterized by ?. At each iteration, we sample a camera pose c from a pre-defined distribution as previous works <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b19">20]</ref>. Then we render 2D image from 3D structures at pose c with differentiable rendering x 0 = g(?, c). To optimize 3D parameters ?, we compute the gradient direction of 2D image and then back propagate to the parameter of NeRF, using VSD as Eq. <ref type="bibr" target="#b8">(9)</ref>. To model the score of the variational distribution, we train an addition diffusion model ? ? parameterized by LoRA. We optimize Eq. ( <ref type="formula" target="#formula_7">8</ref>) to train LoRA after optimization of 3D parameters, using the rendered image x 0 = g(?, c) and pose c. Note that, at each iteration, we perform differentiable rendering only once but use the rendered image twice for both computing gradient direction with VSD and training LoRA. Thus the computation cost will not increase much compared to SDS. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G More Details on Implementation and Hyper-Parameters</head><p>Training details. We use v-prediction <ref type="bibr" target="#b40">[41]</ref> to train our additional diffusion model ? ? . The camera pose c is fed into a 2-layer MLP and then added to timestep embedding at each U-Net block. NeRF/mesh and LoRA batch sizes are set to 1 owing to the computation limit. A larger batch size of NeRF/mesh may improve the generated quality and we leave it in future work. The learning rate of LoRA is 0.0001 and the learning rate of hash grid encoder is 0.01. We render in RGB color space for high-resolution synthesis, unlike <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b53">54]</ref> that render in latent space. In addition, we set ?(t) = ? 2 t . For most experiments, we only use n = 1 particle for VSD to reduce the computation time (and we only use a batch size of 1, due to the computation resource limits). For NeRF rendering, we sample 96 points along each ray, with 64 samples in coarse stage and 32 samples in fine stage. We choose a (1) 64 rendering.</p><p>(2) 128 rendering.</p><p>(3) 256 rendering.</p><p>(4) 512 rendering. Randomly sample ? ? {? (i) } n i=1 and a camera pose c.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>Render the 3D structure ? at pose c to get a 2D image x 0 = g(?, c).</p><p>5:</p><formula xml:id="formula_38">? ? ? -? 1 E t,?,c ?(t) (? pretrain (x t , t, y) -? ? (x t , t, c, y)) ?g(?,c) ?? 6: ? ? ? -? 2 ? ? E t,? ||? ? (x t , t, c, y) -?||<label>2</label></formula><p>2 . 7: end while 8: return single-layer MLP to decode the color and volumetric density from the hash grid encoder as previous work <ref type="bibr" target="#b19">[20]</ref>.</p><p>For object-centric scenes, we set the camera radius as U(1.0, 1.5). The bounding box size is set as 1.0. For large scene generation, we enlarge the range of camera radius to U(0.1, 2.3) for better details and geometry consistency. We enlarge the bounding box size to 5.0. For large scenes with a centric object, we set the range of camera radius as U(1.0, 2.0). DreamFusion <ref type="bibr" target="#b32">[33]</ref> introduces an explicit shading model based on normal vectors during the training process to enhance the geometry. We currently disable the shadings and it reduces the computational cost and memory consumption. We leave incorporating the shading model into our method as future work.</p><p>Optimization. We optimize 25k steps for each particle with AdamW <ref type="bibr" target="#b25">[26]</ref> optimizer in NeRF stage. We optimize 15k steps for geometry fine-tuning and 30k steps for texture fine-tuning. At each stage, for the first 5k steps we sample time steps t ? U(0.02, 0.98) and then directly change into t ? U(0.02, 0.50). For large scene generation, we delay the annealing time to 10k steps since large scene generation requires more iterations to converge. The NeRF training stage consumes 17/17/18/27GB GPU memory with 64/128/256/512 rendering resolution and batch size of 1. The Mesh fine-tuning stage consumes around 17GB GPU memory with 512 rendering resolution and batch size of 1. The whole optimization process takes around several hours per particle on a single NVIDIA A100 GPU. We believe adding more GPUs in parallel will accelerate the generation process, and we leave it for future work.</p><p>Licenses Here we provide the URL, citations and licenses of the open-sourced assets we use in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H 2D Experiments of Variational Score Distillation</head><p>Here we describe the details of the experiments on 2D images with Variation Score Distillation in the main text of Fig. <ref type="figure" target="#fig_3">3</ref>. Here we set the number of particles as 8. We train a smaller U-Net from scratch to estimate the variational score since the distribution of several 2D images is much simpler than the distribution of images rendered from different poses of 3D structures. The optimization takes around 6000 steps. Additional U-Net is optimized 1 step on every optimization step of the particle images. The learning rate of particle images is 0.03 and the learning rate of U-Net is 0.0001. The batch size of particles and U-Net are both set as 8. Here we do not use annealed time schedule for both VSD and SDS for a fair comparison.</p><p>To further demonstrate the effectiveness of our VSD, we increase the number of particles to a large number of 2048. The optimization takes around 20k steps. The batch size of U-Net is 192 and the batch size of particles is set as 16. The results are shown in Figure <ref type="figure" target="#fig_13">14</ref>. It can be seen from the figure that our VSD generates plausible results even under a large number of particles, which demonstrates the scalability of VSD. Although, due to the high optimization cost of 3D representation, the number of particles in 3D experiments is relatively small, we demonstrate that VSD has the potential to scale up to more particles.  The number of particles is 6. Our VSD provides high-fidelity and diverse results in this setting. The prompt is an astronaut is riding a horse.</p><p>We also provide the results of SDS for comparison in Fig. <ref type="figure" target="#fig_15">16</ref>. Compared to our VSD, the results of SDS are smoother and lack details. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I How will CFG weights affect diversity?</head><p>In this section, we explore how CFG affects the diversity of generated results. For VSD, we set the number of particles as 4 and run experiments with different CFG. For SDS, we run 4 times of generation with different random seeds. The results are shown in Figure <ref type="figure" target="#fig_9">17</ref>. As shown in the figure, smaller CFG provides more diversity. We conjecture that this is because the distribution of smaller guidance weights has more diverse modes. However, when the CFG becomes too small (e.g., CFG= 1), it cannot provide enough guidance strength to generate plausible results. Therefore, for all our 3D experiments shown in the results, we set CFG = 7.5 as a trade-off between diversity and optimization stability. Note that SDS could not work well in such small CFG weights. Instead, our VSD provides a trade-off option between CFG weight and diversity, and it can generate more diverse results by simply setting a smaller CFG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J Limitations and Discussions</head><p>Although ProlificDreamer achieves remarkable text-to-3D results, the generation takes hours (especially for high-resolution NeRF training), which is much slower than the vanilla image generation by a diffusion model. Speeding up the text-to-3D generation is another critical problem, and we leave it in future work.</p><p>Besides, our proposed scene initialization demonstrates its effectiveness in generating expansive scenes, yet there are still limitations, particularly with respect to camera positioning. Our current model sets the camera with a fixed view toward the scene's center, which serves object-centric scenes effectively but may be suboptimal for scenes with intricate geometry and detailed textures. Furthermore, despite our efforts to produce outcomes with a rich structure, occasional failures occur and the geometry reverts to a simplistic textured sphere. In order to address these limitations, future research could focus on developing improved camera poses capable of capturing and rendering scenes in finer detail. Furthermore, our present model relies solely on the text-to-image diffusion model without the assistance of other models. The integration of off-the-shelf depth estimation models <ref type="bibr" target="#b35">[36]</ref>, as utilized in other studies <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b56">57]</ref>, could potentially enhance the accuracy and detail of the scene generation process.</p><p>In addition, the correspondence between text prompts and generated results is sometimes insufficient, especially for complex prompts. We conjecture that it is because the ability to generate from complex prompts is limited by the text encoder of Stable Diffusion. In addition, the multi-face Janus problem <ref type="bibr" target="#b55">[56]</ref> also exists in our case. Nevertheless, we believe our proposed VSD and other contributions are orthogonal to these problems, and the generation quality can be further improved by introducing more techniques, such as using a more powerful text-to-image diffusion model that understands view-dependent prompts better <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b39">40]</ref>, or a diffusion model with more 3D priors <ref type="bibr" target="#b23">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K User Study</head><p>For completeness, we follow previous works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b3">4]</ref> and conduct a user study by comparing ProlificDreamer with DreamFusion <ref type="bibr" target="#b32">[33]</ref>, Magic3D <ref type="bibr" target="#b19">[20]</ref> and Fantasia3D <ref type="bibr" target="#b3">[4]</ref> under 15 prompts, 5 prompts for each baseline. Since none of the baselines have released their codes, we can only use the figures from their papers, which limits the number of results from baselines for us to compare. So we currently only compare under 15 prompts. The volunteers are shown the generated results of our ProlificDreamer and baselines and asked to choose the better one in terms of fidelity, details and vividness. We collect results from 109 volunteers, yielding 1635 pairwise comparisons. The results are shown in Table <ref type="table" target="#tab_2">3</ref>. Our method outperforms all of the baselines. Here, we provide more comparisons with baselines in Fig. <ref type="figure" target="#fig_16">18</ref>, Fig. <ref type="figure" target="#fig_9">19</ref>, Fig. <ref type="figure" target="#fig_18">20</ref>, Fig. <ref type="figure" target="#fig_19">21</ref>, Fig. <ref type="figure" target="#fig_20">22</ref> and Fig. <ref type="figure" target="#fig_10">23</ref>. Since none of the baselines have released their codes, we can only directly copy the figures from the corresponding papers. Some baselines are missing given a specific prompt because the prompt is not included in the corresponding papers. To demonstrate geometry, some baselines choose textureless shading <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b19">20]</ref>, while the other <ref type="bibr" target="#b3">[4]</ref> prefers the normal map. For ProlificDreamer, we uniformly show the normal map for consistency. As shown in the figures, our method achieves better results in terms of fidelity and details.</p><p>Michelangelo style statue of dog reading news on a cellphone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DreamFusion Ours Ours DreamFusion</head><p>A model of a house in Tudor style.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours</head><p>A chimpanzee dressed like Henry VIII king of England.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DreamFusion</head><p>A plate of fried chicken and waffles with maple syrup on them.   A Matte painting of a castle made of cheesecake surrounded by a moat made of ice cream.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours DreamFusion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fantasia 3D Ours Ours Magic3D</head><p>A small saguaro cactus planted in a clay pot. A delicious croissant.</p><p>A rabbit, animated movie character, high detail 3d model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fantasia 3D Ours</head><note type="other">DreamFusion Ours DreamFusion Magic 3D</note></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overview of VSD. The 3D representation is differentiably rendered at a random pose c. The rendered image is sent to the pretrained diffusion and the score of the variational distribution (estimated by LoRA) to compute the gradient of VSD. LoRA is also updated on the rendered image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) SDS [33] (CFG = 7.5) (b) SDS [33] (CFG = 100) (c) Ancestral sampling [27] (CFG = 7.5) (d) VSD (CFG = 7.5, ours)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Samples of different methods in 2D space. Similarly to ancestral sampling, VSD generates realistic images with a common CFG weight of 7.5 and outperforms SDS significantly. The prompts from left to right are hamburger, horse, and a monster truck, respectively. See details in Appendix H.</figDesc><graphic url="image-61.png" coords="6,371.99,152.13,58.08,58.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Ablation study of proposed improvements for high-fidelity NeRF generation. The prompt is an elephant skull. (1) The common setting [33, 20] adopts 64 rendering resolution and SDS loss. (2) We improve the generated quality by increasing the rendering resolution. (3) Annealed time schedule adds more details to the generated result. (4) VSD makes the results even better with richer details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: More results of ProlificDreamer of 3D textured meshes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: More results of ProlificDreamer of 3D textured meshes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Results of high quality NeRF from ProlificDreamer. Compared with DreamFusion, our ProlificDreamer generates better NeRF results in terms of fidelity and details, which demonstrates the effectiveness of our VSD against SDS.</figDesc><graphic url="image-155.png" coords="16,109.92,362.19,321.34,53.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Ablation study on scene generation. With our scene initialization, the results are with 360 ? surroundings instead of being object-centric. Our annealed time schedule and VSD are both beneficial for the generation quality. The prompt here is Inside of a smart home, realistic detailed photo, 4k.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>( 1 )</head><label>1</label><figDesc>High fidelity NeRF generated by VSD (ours).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>( 2 )</head><label>2</label><figDesc>Extract and fine-tune geometry from NeRF. (3a) Texture fine-tuned by VSD (ours).(3b) Texture fine-tuned by SDS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Ablation on how number of particles affects the results. The diversity of the generated results is slightly larger as the number of particles increases. The quality of generated results is not affected much by the number of particles. The prompt is A high quality photo of an ice cream sundae.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 13 :Algorithm 1</head><label>131</label><figDesc>Figure 13: Ablation study on rendering resolution during NeRF training. Training with higher resolution produces better results with finer details. However, our VSD still provides competitive results under a lower resolution (128 or 256), which is more computationally efficient. The prompt here is an elephant skull.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Selected samples from 2048 particles in a 2D experiment of VSD. Our VSD generates plausible results even under a large number of particles, which demonstrates the scalability of VSD. The prompt is an astronaut is riding a horse.</figDesc><graphic url="image-235.png" coords="25,305.94,402.14,66.00,66.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: 2D experiments of VSD (CFG= 7.5) results with LoRA and annealed time schedule.The number of particles is 6. Our VSD provides high-fidelity and diverse results in this setting. The prompt is an astronaut is riding a horse.</figDesc><graphic url="image-241.png" coords="25,108.61,575.90,394.82,66.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 16 :</head><label>16</label><figDesc>Figure 16: SDS result in 2D (CFG= 100). Compared to our VSD, the results of SDS are smoother. The prompt is an astronaut is riding a horse.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 18 :</head><label>18</label><figDesc>Figure 18: More results of ProlificDreamer compared with baselines.</figDesc><graphic url="image-296.png" coords="29,228.45,590.65,56.57,56.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Ours</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 20 :</head><label>20</label><figDesc>Figure 20: More results of ProlificDreamer compared with baselines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 21 :</head><label>21</label><figDesc>Figure 21: More results of ProlificDreamer compared with baselines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 22 :</head><label>22</label><figDesc>Figure 22: More results of ProlificDreamer compared with baselines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>?,c ? t ?(t) (? xt log p t (x t |y) -? xt log q ?? t (x t |c, y))</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>?g(?, c) ??</cell><cell>,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(11)</cell></row><row><cell cols="4">and the corresponding update rule for each ? ? ? ? ? satisfies</cell></row><row><cell></cell><cell></cell><cell>?</cell><cell>?</cell></row><row><cell>d? ? d?</cell><cell>= -E t,?,c</cell><cell>? ??(t)</cell><cell>? ?-?t?x</cell></row></table><note><p>t log p t (x t |y) ??pretrain(xt,t,y) -(-? t ? xt log q ?? t (x t |c, y)) ?? ? (xt,t,c,y)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Results of user study. The percentage of user preference (?) is reported in the table.</figDesc><table><row><cell>Name</cell><cell cols="3">DreamFusion Magic3D Fantasia3D</cell></row><row><cell>Prefer baseline</cell><cell>6.87</cell><cell>5.50</cell><cell>9.73</cell></row><row><cell cols="2">Prefer ProlificDreamer (Ours) 94.13</cell><cell>94.50</cell><cell>90.27</cell></row><row><cell cols="2">L More Comparisons with Baselines</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>A prolific dreamer is someone who experiences vivid dreams quite regularly<ref type="bibr" target="#b48">[49]</ref>, which corresponds to the high-fidelity and diverse results of our method.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>We optimize up to n = 4 particles due to the computation resource limit. See details in Appendix E.3.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>Note that we have two variables of time: one is the diffusion time t ? [0, T ] and the other is the gradient flow time ? , corresponding to the optimization iteration for each ?.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Yogesh</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinsheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><surname>Kreis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<title level="m">ediff-i: Text-to-image diffusion models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>with an ensemble of expert denoisers</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mipnerf 360: Unbounded anti-aliased neural radiance fields</title>
		<author>
			<persName><forename type="first">Jonathan T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dor</forename><surname>Verbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Pratul P Srinivasan</surname></persName>
		</author>
		<author>
			<persName><surname>Hedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="5470" to="5479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A unified particleoptimization framework for scalable bayesian sampling</title>
		<author>
			<persName><forename type="first">Changyou</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenlin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqun</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.11659</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Rui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongwei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ningxin</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.13873</idno>
		<title level="m">Fantasia3d: Disentangling geometry and appearance for high-quality text-to-3d content creation</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Dana</forename><surname>Cohen-Bar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gal</forename><surname>Metzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raja</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.13450</idno>
		<title level="m">Setthe-scene: Global-local training for generating controllable nerf scenes</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A stein variational newton method</title>
		<author>
			<persName><forename type="first">Gianluca</forename><surname>Detommaso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiangang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youssef</forename><surname>Marzouk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessio</forename><surname>Spantini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Scheichl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Diffusion models beat gans on image synthesis</title>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Nichol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8780" to="8794" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Lauro</forename><surname>Langosco Di Langosco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Fortuin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiko</forename><surname>Strathmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.10731</idno>
		<title level="m">Neural variational gradient descent</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Particle-based variational inference with preconditioned functional gradient flow</title>
		<author>
			<persName><forename type="first">Hanze</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.13954</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Learning to draw samples with amortized stein variational gradient descent</title>
		<author>
			<persName><forename type="first">Yihao</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06626</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Clement Fuji</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Shugrina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Francois Lafleche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Towaki</forename><surname>Takikawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiehan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Loop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenzheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishna</forename><forename type="middle">Murthy</forename><surname>Jatavallabhula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Artem</forename><surname>Rozantsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Or</forename><surname>Perel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianchang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gavriel</forename><surname>State</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Gorski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommy</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rev</forename><surname>Lebaredian</surname></persName>
		</author>
		<author>
			<persName><surname>Kaolin</surname></persName>
		</author>
		<ptr target="https://github.com/NVIDIAGameWorks/kaolin" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Graikos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Malkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nebojsa</forename><surname>Jojic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Samaras</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.09012</idno>
		<title level="m">Diffusion models as plug-and-play priors</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Ying-Tian</forename><surname>Yuan-Chen Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi-Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guan</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chia-Hao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan-Pei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song-Hai</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
		<ptr target="https://github.com/threestudio-project/threestudio" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.12598</idno>
		<title level="m">Classifier-free diffusion guidance</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Lukas</forename><surname>H?llein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.11989</idno>
		<title level="m">Text2room: Extracting textured 3d meshes from 2d text-to-image models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Debiasing scores and prompts of 2d diffusion for robust text-to-3d generation</title>
		<author>
			<persName><forename type="first">Susung</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghoon</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungryong</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.15413</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shean</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Lora</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09685</idno>
		<title level="m">Low-rank adaptation of large language models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Zero-shot text-guided object generation with dream fields</title>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Chen-Hsuan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Towaki</forename><surname>Takikawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohui</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><surname>Kreis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.10440</idno>
		<title level="m">Magic3d: High-resolution text-to-3d content creation</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Understanding and accelerating particle-based variational inference</title>
		<author>
			<persName><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingwei</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengyu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4082" to="4092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Accelerated first-order methods on the wasserstein space for bayesian inference</title>
		<author>
			<persName><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingwei</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengyu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">stat</title>
		<imprint>
			<biblScope unit="volume">1050</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Stein variational gradient descent: A general purpose bayesian inference algorithm</title>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Ruoshi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rundi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Basile</forename><surname>Van Hoorick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Zakharov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>Zero-1-to-3: Zero-shot one image to 3d object</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Geometry in sampling methods: A review on manifold mcmc and particle-based variational inference methods</title>
		<author>
			<persName><forename type="first">Chang</forename><surname>Liua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhub</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advancements in Bayesian Methods and Implementations</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page">239</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<title level="m">Decoupled weight decay regularization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongxuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><surname>Dpm-Solver</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.00927</idno>
		<title level="m">A fast ODE solver for diffusion probabilistic model sampling in around 10 steps</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Or Patashnik, Raja Giryes, and Daniel Cohen-Or. Latent-nerf for shape-guided generation of 3d shapes and textures</title>
		<author>
			<persName><forename type="first">Gal</forename><surname>Metzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Richardson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.07600</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Nerf: Representing scenes as neural radiance fields for view synthesis</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pratul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ren</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="99" to="106" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Clip-mesh: Generating textured meshes from text using pretrained image-text models</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Nasir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianhao</forename><surname>Khalid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiberiu</forename><surname>Belilovsky</surname></persName>
		</author>
		<author>
			<persName><surname>Popa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH Asia 2022 Conference Papers</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Instant neural graphics primitives with a multiresolution hash encoding</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Schied</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (ToG)</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Do deep generative models know what they don</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Nalisnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akihiro</forename><surname>Matsukawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilan</forename><surname>Gorur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.09136</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">t know? arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.14988</idno>
		<title level="m">Dreamfusion: Text-to-3d using 2d diffusion</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Hierarchical text-conditional image generation with clip latents</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Casey</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.06125</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer</title>
		<author>
			<persName><forename type="first">Ren?</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katrin</forename><surname>Lasinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1623" to="1637" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Highresolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bj?rn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10684" to="10695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">18th International Conference</title>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015-10-05">2015. October 5-9, 2015. 2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
	<note>Proceedings, Part III 18</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Low-rank adaptation for fast text-to-image diffusion fine-tuning</title>
		<author>
			<persName><forename type="first">Simo</forename><surname>Ryu</surname></persName>
		</author>
		<ptr target="https://github.com/cloneofsimo/lora.2023" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Photorealistic text-to-image diffusion models with deep language understanding</title>
		<author>
			<persName><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lala</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Whang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seyed</forename><surname>Kamyar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seyed</forename><surname>Ghasemipour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Burcu</forename><surname>Karagol Ayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Mahdavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rapha</forename><surname>Gontijo Lopes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.11487</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Progressive distillation for fast sampling of diffusion models</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Laion-5b: An open large-scale dataset for training next generation image-text models</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Schuhmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romain</forename><surname>Beaumont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Vencu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cade</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Cherti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theo</forename><surname>Coombes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aarush</forename><surname>Katta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clayton</forename><surname>Mullis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Wortsman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.08402</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Junyoung</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wooseok</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min-Seop</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaehoon</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyeonsu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junho</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin-Hwa</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiyoung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungryong</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.07937</idno>
		<title level="m">Let 2d diffusion model know 3d-consistency for robust text-to-3d generation</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep marching tetrahedra: a hybrid representation for high-resolution 3d shape synthesis</title>
		<author>
			<persName><forename type="first">Tianchang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kangxue</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep unsupervised learning using nonequilibrium thermodynamics</title>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niru</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2256" to="2265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Generative modeling by estimating gradients of the data distribution</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Generative modeling by estimating gradients of the data distribution</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Alch?-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Score-based generative modeling through stochastic differential equations</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">The experiencing of the dream and the transference</title>
		<author>
			<persName><forename type="first">Harold</forename><surname>Stewart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Psycho-Analysis</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page">345</biblScope>
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Stable-dreamfusion: Text-to-3d with stable-diffusion</title>
		<author>
			<persName><forename type="first">Jiaxiang</forename><surname>Tang</surname></persName>
		</author>
		<ptr target="https://github.com/ashawkey/stable-dreamfusion" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">Christina</forename><surname>Tsalicoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessio</forename><surname>Tonioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName><surname>Textmesh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.12439</idno>
		<title level="m">Generation of realistic 3d meshes from text prompts</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Optimal transport: old and new</title>
		<author>
			<persName><forename type="first">C?dric</forename><surname>Villani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">338</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">Suraj</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Lozhkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Cuenca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kashif</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mishig</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Davaadorj</surname></persName>
		</author>
		<author>
			<persName><surname>Wolf</surname></persName>
		</author>
		<ptr target="https://github.com/huggingface/diffusers" />
		<title level="m">Diffusers: State-of-the-art diffusion models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Score jacobian chaining: Lifting pretrained 2d diffusion models for 3d generation</title>
		<author>
			<persName><forename type="first">Haochen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><forename type="middle">A</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.00774</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Function space particle optimization for bayesian neural networks</title>
		<author>
			<persName><forename type="first">Ziyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tongzheng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09754</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<ptr target="https://en.wikipedia.org/w/index.php?title=Janus" />
		<title level="m">Wikipedia contributors. Janus -Wikipedia, the free encyclopedia</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Table 2: URL, citations and licenses of the open-sourced assets we use in this work</title>
		<author>
			<persName><forename type="first">Jingbo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyu</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Can</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Liao</surname></persName>
		</author>
		<idno>53] Apache License 2.0</idno>
		<ptr target="https://github.com/huggingface/diffusers" />
	</analytic>
	<monogr>
		<title level="m">Citation License</title>
		<imprint>
			<publisher>Apache License</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
	<note>MIT License</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
