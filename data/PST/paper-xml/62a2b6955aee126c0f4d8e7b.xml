<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Prompt Search</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-06-09">9 Jun 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yuanhan</forename><surname>Zhang</surname></persName>
							<email>yuanhan002@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">S-Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kaiyang</forename><surname>Zhou</surname></persName>
							<email>kaiyang.zhou@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">S-Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
							<email>ziwei.liu@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">S-Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lora</forename><surname>Lora</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">S-Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Adapter</forename><surname>Nolinear</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">S-Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Noah</forename><surname>Vpt</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">S-Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Adapter</forename><surname>Lora</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">S-Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Prompt Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-06-09">9 Jun 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2206.04673v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The size of vision models has grown exponentially over the last few years, especially after the emergence of Vision Transformer. This has motivated the development of parameter-efficient tuning methods, such as learning adapter layers or visual prompt tokens, which allow a tiny portion of model parameters to be trained whereas the vast majority obtained from pre-training are frozen. However, designing a proper tuning method is non-trivial: one might need to try out a lengthy list of design choices, not to mention that each downstream dataset often requires custom designs. In this paper, we view the existing parameter-efficient tuning methods as "prompt modules" and propose Neural prOmpt seArcH (NOAH), a novel approach that learns, for large vision models, the optimal design of prompt modules through a neural architecture search algorithm, specifically for each downstream dataset. By conducting extensive experiments on over 20 vision datasets, we demonstrate that NOAH (i) is superior to individual prompt modules, (ii) has good few-shot learning ability, and (iii) is domain-generalizable. The code and models are available at https://github.com/Davidzhangyuanhan/NOAH.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The size of vision models has grown exponentially from tens of millions a few years ago (e.g., ResNet <ref type="bibr" target="#b13">[13]</ref>) to today's hundreds of millions <ref type="bibr" target="#b9">[9]</ref>, or even billions <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b39">39]</ref>, for Transformers <ref type="bibr" target="#b40">[40]</ref>. Such an increase can cause a number of problems to transfer learning <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b19">19]</ref>, and the first and foremost is that fine-tuning becomes more difficult as large model size can easily lead to overfitting in a typical-sized dataset, let alone the increase of compute and storage costs.</p><p>Recently, there is a growing interest in developing parameter-efficient tuning methods <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b19">19]</ref>. The key idea is to insert a tiny trainable module to a large pre-trained model and only adjust its parameters by optimizing some task-specific losses like the cross-entropy for classification problems. The most representative methods are Adapter <ref type="bibr" target="#b17">[17]</ref>, Low-Rank Adaptation (LoRA) <ref type="bibr" target="#b18">[18]</ref>, and Visual Prompt Tuning (VPT) <ref type="bibr" target="#b19">[19]</ref>. As exemplified in Fig. <ref type="figure">1(a)</ref>, Adapter is a bottleneck-shaped neural network appended to a network block's output; LoRA is a "residual" layer consisting of rank decomposition matrices; VPT prepends additional tokens to the input of a Transformer block, which can be seen as adding learnable "pixels."</p><p>By evaluating the three parameter-efficient tuning methods on a commonly-used transfer learning benchmark, i.e., VTAB-1k <ref type="bibr" target="#b47">[47]</ref>, we identify a couple of critical issues. First, none of the three methods performs consistently well on all datasets, as illustrated in Fig. <ref type="figure">1(b</ref>). For instance, when it comes to scene structure understanding tasks, VPT outperforms Adapter and LoRA on Small-NORB/azimuth <ref type="bibr" target="#b24">[24]</ref>, but its performance plunges on SmallNORB/location <ref type="bibr" target="#b24">[24]</ref> and Clevr/count <ref type="bibr" target="#b20">[20]</ref>, which is largely behind the two competitors. The results suggest that, for a specific dataset, one needs to perform an extensive evaluation on different tuning methods in order to identify the most suitable one. Second, performance is found to be sensitive to the selection of model parameters, such as Figure <ref type="figure">1</ref>: Our approach, neural prompt search, or NOAH for short, subsumes three representative parameter-efficient tuning methods (i.e., Adapter <ref type="bibr" target="#b17">[17]</ref>, LoRA <ref type="bibr" target="#b18">[18]</ref> and VPT <ref type="bibr" target="#b19">[19]</ref>) and learns from data the optimal design through neural architecture search (a). The approach is motivated by the observation that none of the three individuals shows dominance on the VTAB-1k benchmark (b). The colors of the datasets' names indicate which method performs the best. Clearly, NOAH is the best overall approach.</p><p>Adapter's feature dimension or the token length in VPT-this is also observed by Jia et al. <ref type="bibr" target="#b19">[19]</ref> that the optimal token length in VPT varies from 1 to 200 on different datasets.</p><p>In this work, we view the existing parameter-efficient tuning methods as prompt modules and propose to automatically search for the optimal prompt design from data via a neural architecture search (NAS) algorithm. Specifically, we introduce the concept of Neural prOmpt seArcH (NOAH) for large vision models, particularly those equipped with the Transformer block <ref type="bibr" target="#b9">[9]</ref>. The search space is constructed by subsuming Adapter <ref type="bibr" target="#b17">[17]</ref>, LoRA <ref type="bibr" target="#b18">[18]</ref> and VPT <ref type="bibr" target="#b19">[19]</ref> into each Transformer block, as depicted in Fig. <ref type="figure">1(a</ref>). The specific model parameters, including the feature dimension for Adapter and LoRA and the token length for VPT, are determined by a one-shot NAS algorithm.</p><p>We conduct extensive experiments on VTAB-1k <ref type="bibr" target="#b47">[47]</ref>, which is composed of 19 diverse vision datasets and covers a wide spectrum of visual domains like objects, scenes, textures and satellite imagery. The results show that NOAH significantly outperforms the individual prompt modules on 10 out of 19 datasets while the performance on the remaining is highly competitive (see Fig. <ref type="figure">1</ref>(b) for an overview of the results). We also evaluate on few-shot learning and domain generalization where the results also confirm the superiority of NOAH to the hand-crafted prompt modules.</p><p>Our contributions are summarized as follows. (i) We present a systematic study of three representative prompt modules and expose some critical issues associated with performance and efficiency. (ii) A novel concept, neural prompt search, is proposed to address the challenge of hand-engineering prompt modules. (iii) An efficient NAS-based implementation of NOAH is provided. (iv) We demonstrate that NOAH is better than individual prompt modules in downstream transfer learning, few-shot learning, and domain generalization. The models and code will be released.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Neural Prompt Search</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Background</head><p>Vision Transformer We first briefly review Vision Transformer (ViT) <ref type="bibr" target="#b9">[9]</ref>, to which our approach is mainly applied. ViT consists of alternating blocks of multihead self-attention (MSA) and multi-layer perceptron (MLP). Given an input sequence x ? R N ?D where N denotes the token length and D is the embedding dimension, MSA first maps x to queries Q ? R N ?d , keys K ? R N ?d and values V ? R N ?d using three projection matrices, W q ? R D?d , W k ? R D?d and W v ? R D?d , respectively, where d means the hidden dimension. Then, MSA computes the weighted sums over the values based on the self-attention between the queries and keys,</p><formula xml:id="formula_0">Attention(Q, K, V ) = softmax( QK T ? d )V,<label>(1)</label></formula><p>where 1 ? d is a scaling factor. Below we briefly review the three representative-and top-performing-parameter-efficient tuning methods, i.e., Adapter <ref type="bibr" target="#b17">[17]</ref>, LoRA <ref type="bibr" target="#b18">[18]</ref> and VPT <ref type="bibr" target="#b19">[19]</ref>, which will be incorporated into our search space. An illustration of these three methods can be found in Fig. <ref type="figure">1</ref>(a). Note that VPT has been studied for vision models while Adapter and LoRA have only been studied for language models.</p><p>Adapter is essentially a bottleneck-like neural network consisting of a down-sample layer W down ? R d?r and an up-sample layer W up ? R r?d , where r denotes the down-sampled dimension. A nonlinear activation function ?(?), such as ReLU, is inserted in-between. The computation can be formulated as</p><formula xml:id="formula_1">h = ?(hW down )W up ,<label>(2)</label></formula><p>where h ? R N ?d is a normalized output of the MLP in a Transformer block.</p><p>LoRA aims to update the two projection layers, W q (for queries) and W k (for keys), in an indirect way by optimizing their rank-decomposed changes, W q = W down q W up q and W k = W down k W up k , where W down q/k ? R D?r and W up q/k ? R r?d (r is the down-projection dimension). For a specific input x, we have</p><formula xml:id="formula_2">Q = xW q + s ? xW down q W up q , K = xW k + s ? xW down k W up k ,<label>(3)</label></formula><p>where s is a fixed scaling parameter for modulating the updates.</p><p>Visual Prompt Tuning (VPT) prepends a set of learnable tokens to the input of a Transformer block, which can be viewed as adding some learnable pixels in the input space. We investigate the best-performing version, VPT-Deep, which applies prompt tuning to multiple layers <ref type="bibr" target="#b19">[19]</ref>. We call this module VPT for brevity hereafter and formulate it in mathematical terms below. A typical input x ? R N ?D to a Transformer block contains a learnable class token [CLS] of D-dimension and a sequence of image patch embeddings E = {e i |e i ? R D , i = 1, ..., N -1} where the positional embeddings are omitted. VPT adds m learnable tokens, P = {p k |p k ? R D , k = 1, ..., m}, to x, which then becomes x = [CLS, P, E]. (4)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Prompt Search Algorithm</head><p>As discussed, none of the individual parameter-efficient tuning methods, or prompt modules called in this paper, shows dominance in the transfer learning benchmark. Our approach, neural prompt search (NOAH), incorporates Adapter <ref type="bibr" target="#b17">[17]</ref>, LoRA <ref type="bibr" target="#b18">[18]</ref> and VPT <ref type="bibr" target="#b19">[19]</ref> into each Transformer block and learns the design that best suits a dataset through neural architecture search (NAS). Specifically, we employ a one-shot NAS algorithm, AutoFormer <ref type="bibr" target="#b4">[4]</ref>, for prompt module search. Our supernet is a ViT-like model composed of 12 Transformer blocks (layers). Below we detail the search space and how the search is done.</p><p>Search Space As shown in Fig. <ref type="figure">1</ref>(a), we embed the three prompt modules into each Transformer block following the guidelines proposed in the original work <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b19">19]</ref>. Concretely, we install VPT in the input position, add LoRA alongside the two projection matrices as residuals, and insert Adapter after the normalized output of the MLP. The search space mainly contains the model parameters associated with the three prompt modules. Specifically, each prompt module has two sets of parameters to search from: (i) the embedding dimension ? {5, 10, 50, 100}; (ii) the depth ? {0, 3, 6, 9, 12}. A depth means up to which layer a module is applied, e.g., depth = 3 for VPT means layers 0, 1 and 2 have VPT installed while the remaining layers, 3 to 11, do not have VPT.  (a) Inputs: {{10, 5, 5, 5, 10, 50, 0,?, 0},{5, 5, 10, 0, 0, 0,?, 0},{5, 5, 5, 0?, 0}} {{10, 5, 5, 5, 10, 50 , 0,?,0},{5, 5, 10, 0, 0, 0,?,0},{5, 5, 5, 0?,0}} {{10, 5, 5, 0, 0, 0,?, 0}, { 5, 5, 10, 5, 10, 5,?, 0},{5, 5, 5, 0,?,0}} For VPT, the embedding dimension means the token length whereas for Adapter and LoRA, the embedding dimension means the down-sampled dimension, i.e., r.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supernet Training</head><p>The supernet, as mentioned, has 12 Transformer layers, each containing the three prompt modules with full embedding dimension, i.e., 100. During each forward pass, a subnet is randomly sampled from the supernet for training. Specifically, for each prompt module, a depth is first sampled from {0, 3, 6, 9, 12} to determine which layers should have the module. Then, for each layer within the depth range, an embedding dimension is chosen from {5, 10, 50, 100}, all with a uniform probability. Note that only the prompt modules' parameters are learned while the pre-trained model is kept fixed. AutoFormer <ref type="bibr" target="#b4">[4]</ref> allows the weights in each prompt module to be entangled during training, meaning that different weights are maximally shared, e.g., in a VPT module, if 100 tokens are selected for training, the previously trained tokens, such as 50, will be reused and trained together with other 50 tokens. This way, as suggested in AutoFormer <ref type="bibr" target="#b4">[4]</ref>, leads to faster convergence and low memory cost.</p><p>Evolutionary Search After the supernet is trained, evolutionary search is conducted to obtain the optimal subnet architecture under a parameter size limit <ref type="bibr" target="#b4">[4]</ref>. Specifically, we first select K random architectures, from which the top k architectures (with the best performance) are used as parents to produce the next generation through crossover and mutation. For crossover, two candidates are randomly chosen and crossed to produce a "child" architecture. For mutation, a candidate mutates its prompt module design with a probability. See Fig. <ref type="figure" target="#fig_0">2</ref> for an illustration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>In this section, we mainly address the following questions: (i) Is NOAH better than the individual prompt modules? (ii) Can NOAH work in a few-shot setting? (iii) Are models learned by NOAH robust to domain shift? The answers are discussed in Sec. 3.1, 3.2 and 3.3, respectively. We also conduct some analyses in Sec. 3.4 to have a deeper understanding of NOAH, such as what a subnet looks like and whether it is transferable beyond the dataset in which the architecture was found.</p><p>Baselines The main competitors are the three representative prompt modules subsumed by NOAH, which are Adapter <ref type="bibr" target="#b17">[17]</ref>, LoRA <ref type="bibr" target="#b18">[18]</ref> and VPT <ref type="bibr" target="#b19">[19]</ref>. Among them, only VPT is specifically designed for vision models while the other two are originally developed for language models. We also compare two common fine-tuning methods on the VTAB-1k benchmark: full tuning (Full) and linear probing (Linear). Full simply tunes the entire model parameters whereas Linear freezes the pre-trained part and only adjusts the newly added linear classification layer. <ref type="foot" target="#foot_1">3</ref> It is worth mentioning that Full has been considered as a strong baseline in existing studies <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b17">17]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Structured</head><p>Figure <ref type="figure">3</ref>: Group-wise average results on VTAB-1k. NOAH performs the best in the Natural and Structured groups while its performance in the Specialized group is similar to that of LoRA-but NOAH does not require a manual search over the architecture and hyper-parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>We keep the training parameters identical across all experiments throughout this paper. ViT-B/16 <ref type="bibr" target="#b9">[9]</ref> pre-trained on ImageNet-22K <ref type="bibr" target="#b7">[7]</ref> is used as the base model, which is strong enough so the results are fair and convincing. The supernet for NOAH is trained for 300 epochs and the ultimate subnet is trained for 100 epochs-note that "subnet" means the prompt modules/architectures. Since the AutoFormer <ref type="bibr" target="#b4">[4]</ref> algorithm allows a subnet to be used without retraining, we demonstrate later that the subnet found by NOAH without retraining is also comparable to the retrained one. The evolutionary search in NOAH takes 5 epochs in total and each step of random pick/crossover/mutation produces 50 new subnets. The probability for crossover and mutation is set to 0.2, which follows AutoFormer <ref type="bibr" target="#b4">[4]</ref>. The individual prompt modules, i.e., Adapter <ref type="bibr" target="#b17">[17]</ref>, LoRA <ref type="bibr" target="#b18">[18]</ref> and VPT <ref type="bibr" target="#b19">[19]</ref>, are constructed using the best recipes suggested by the original papers (also trained for 100 epochs; see the Supplementary for more details). The parameter sizes for Adapter, LoRA and VPT are 0.33M, 0.35M and 0.46M, respectively. For fair comparison, we set the upper-limit of parameter size of the final subnet in NOAH to 0.46M so the resulting size would be comparable to the baselines. More implementation details including image augmentation and other hyper-parameters are provided in the Supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experiments on VTAB-1k</head><p>Datasets We choose the VTAB-1k <ref type="bibr" target="#b47">[47]</ref> benchmark to evaluate the transfer learning performance of our approach. VTAB-1k consists of 19 vision datasets, which are clustered into three groups: Natural, Specialized and Structured. The Natural group contains natural images that are captured by standard cameras and cover a broad spectrum of concepts including generic, fine-grained and abstract objects. The Specialized group contains images captured by specialist equipment for remote sensing (like aerial images) and medical purposes. The Structured group is designed specifically for scene structure understanding, such as object counting, depth prediction and orientation prediction. Each dataset in VTAB-1k contains 1,000 labeled examples, which are split into a train (80%) and a val (20%) set (the latter is used for hyper-parameter tuning), while the test data comes from the original test set. The final model used for evaluation is trained using the full 1,000 examples in each dataset. Top-1 classification accuracy is used as the performance measure. Results Table <ref type="table" target="#tab_0">1</ref> presents the full results on the VTAB-1k benchmark. A high-level summary is shown earlier in Fig. <ref type="figure">1</ref>(b). The average performance within each group is summarized in Fig. <ref type="figure">3</ref>. We have the following observations.</p><p>Observation 1: Overall, NOAH is the best parameter-efficient tuning method. First and foremost, we demonstrate that searching for the optimal combination of the individual prompt modules works the best. This is evidenced by the 1% average gain over the strongest prompt module, i.e., LoRA. Given the diversity of the benchmark, the 1% average gain can be considered to be significant. It is also worth mentioning that Adapter was previously proved to be the best-performing prompt module in NLP <ref type="bibr" target="#b30">[30]</ref>, but in our study for computer vision tasks, LoRA takes over the seat. This further confirms that search is a better option than hand-engineering in practice.</p><p>Observation 2: NOAH slightly dims in the Specialized group. The results suggest that NOAH's weakness seems to be in the Specialized tasks where the individual modules achieve the on-par performance: NOAH's results are not too far from those of the competitors, e.g., NOAH's 84.8 vs LoRA's 84.6 on average. And while NOAH is superior on a Camelyon, it lags on the other datasets, especially on the Retinopathy. Since the individual modules require a manual search over architecture and hyper-parameters, NOAH is more compelling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experiments on Few-Shot Learning</head><p>Datasets We choose five fine-grained visual recognition datasets, which include Food101 <ref type="bibr" target="#b2">[2]</ref>, Ox-fordFlowers102 <ref type="bibr" target="#b33">[33]</ref>, StandfordCars <ref type="bibr" target="#b22">[22]</ref>, OxfordPets <ref type="bibr" target="#b34">[34]</ref>, and FGVCAircraft <ref type="bibr" target="#b29">[29]</ref>. The categories in these datasets cover a wide range of visual concepts closely related to our daily life: food, plant, vehicle and animal. We follow existing studies <ref type="bibr" target="#b51">[51,</ref><ref type="bibr" target="#b36">36]</ref> to evaluate on 1, 2, 4, 8 and 16 shots, which are sufficient for observing the trend.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>The results are summarized in Fig. <ref type="figure" target="#fig_3">4</ref>. In terms of the average performance, we can observe that: (i) In the low-data regime like 1 or 2 shots, NOAH, LoRA and Adapter perform similarly but VPT largely lags behind; (ii) NOAH shows clear dominance when more shots become available, e.g., with 16 shots the gap between NOAH and the runner-up is around 2%. By looking at the individual graphs, we can see that none of the individual prompt modules performs consistently well on all datasets, which, again, justifies that search is better than hand-engineering. Figure <ref type="figure">5</ref>: Average subnets (architectures) for the three groups in VTAB-1k. Adapter and LoRA tend to live in shallow layers while VPT is found nearly in all depths. The demands for VPT (indicated by the embedding dimension) differ in different groups. The co-existence of the three modules, especially in shallow layers, serves as strong evidence of their complementarity, and such a synergy is difficult to obtain by hand-engineering. Datasets Since domain shift is ubiquitous in real-world applications <ref type="bibr" target="#b50">[50]</ref>, we are interested to know how our searchbased approach compares with the individual prompt modules in terms of domain generalization ability. Following prior studies <ref type="bibr" target="#b51">[51,</ref><ref type="bibr" target="#b52">52]</ref>, we first train a model on Im-ageNet <ref type="bibr" target="#b7">[7]</ref> (using 16 shots per category) and then directly test it on four other variants of ImageNet that undergo different types of domain shift. Specifically, the test datasets include (i) ImageNetV2 <ref type="bibr" target="#b38">[38]</ref>, which is collected from different sources than ImageNet but following the same collection protocol, (ii) ImageNet-Sketch <ref type="bibr" target="#b42">[42]</ref>, which is composed of sketch images of the same 1,000 classes in ImageNet, (iii) ImageNet-A <ref type="bibr" target="#b16">[16]</ref>, which contains adversarially-filtered images, (iv) ImageNet-R <ref type="bibr" target="#b15">[15]</ref>, which is a rendition of ImageNet. Both ImageNet-A and -R have 200 classes derived from a subset of ImageNet's 1000 classes. All results are averaged over three random seeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Experiments on Domain Generalization</head><p>Results Table <ref type="table" target="#tab_1">2</ref> compares NOAH with the three individual prompt modules. On ImageNet, which is the source dataset, the gap between NOAH and the individual modules is small, which is about 1%. However, on the four test datasets, NOAH demonstrates significantly stronger robustness than the baselines: over 6.8%, 4.8%, 5% and 5.2% improvements on -V2, -Sketch, -A and -R, respectively. The results, together with those from previous subsections, justify that our search-based approach is superior to the individual prompt modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Further Analysis</head><p>Architecture of Subnet A key question to answer is: how does NOAH's subnet, i.e., the ultimate architecture, look like. To make the results convincing, we visualize the average architecture-instead of individual ones-found within each group of VTAB-1k, as well as the global average over all datasets, in Fig. <ref type="figure">5</ref>. The x-axis represents the network depth while the y-axis represents the embedding dimension. An intriguing observation is that Adapter and LoRA, across all groups, mainly appear in shallow layers with the embedding dimension less than 75 and reduced when depth increases. In contrast, VPT can be found nearly in all depths (layers) but the dimensions vary significantly in different groups, which indicate different demands for VPT. For instance, in the Specialized group (Fig. <ref type="figure">5(c</ref>)), shallow layers need more VPT modules; but in the Structured group (Fig. <ref type="figure">5(d</ref>)), middle layers need more VPT modules. Moreover, the co-existence of the three modules, especially in shallow layers, suggests that they are complementary to each other-such a synergy is difficult to obtain by manual design. In summary, the observed high variances in the module designs strongly indicate that search is much more efficient than hand-engineering when it comes to developing parameter-efficient tuning methods.  Transferability of Subnet As discussed previously, the subnet (i.e., architecture) found for different datasets differs dramatically. Here we study whether, or in what circumstances, the subnet found from one dataset can be transferred to another. To this end, we train NOAH on ImageNet and apply the ultimate subnet to the VTAB-1k benchmark where the model is retrained and evaluated. To measure transferability, we compare the ImageNet subnet with the dataset-specific subnets on VTAB-1k. Fig. <ref type="figure" target="#fig_6">6</ref> shows the comparisons. Overall, the gap between the ImageNet subnet and the 19 dataset-specific subnets on VTAB-1k is below 3%, meaning that NOAH has fair transferability. By digging deeper into the results, we find that the transfer gap is smaller when the source (i.e., ImageNet) and target datasets are closer, and vice versa. For instance, the gaps in the Natural group are less than 1%, which make sense because the ImageNet images and those from the Natural group share similar visual concepts, such as generic objects, flowers and animals. With vs Without Retraining Thanks to the weight entanglement strategy in Auto-Former <ref type="bibr" target="#b4">[4]</ref>, the subnet extracted from the supernet can be directly deployed for use without retraining. To verify if such a rule also applies to NOAH, we compare the subnets with and without retraining on VTAB-1k. The results averaged over each group are shown in Table <ref type="table" target="#tab_2">3</ref> where we observe that NOAH with and without retraining (denoted as inherited) do not make any significant difference: the inherited version still outperforms the individual prompt modules. The results suggest that the retraining cost can be safely removed without incurring any significant loss.</p><p>4 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Parameter-Efficient Tuning</head><p>A recent trend in transfer learning is to develop parameter-efficient tuning methods <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b51">51]</ref>, which is spurred by the rapid increase in model size. Existing methods can be generally divided into two groups. The first group fine-tunes a small portion of the internal parameters, such as biases <ref type="bibr" target="#b45">[45]</ref>. The second group adds tiny learnable modules like Adapter <ref type="bibr" target="#b17">[17]</ref> or LoRA <ref type="bibr" target="#b18">[18]</ref>, which is more relevant to our research and thus the focus here. Adapter <ref type="bibr" target="#b17">[17]</ref> and LoRA <ref type="bibr" target="#b18">[18]</ref> essentially share similar architectures-both look like a bottleneck-but are installed at different places: Adapter is often installed at the output of a block while LoRA is treated as residuals to the projection matrices in a Transformer <ref type="bibr" target="#b40">[40]</ref> block. It is worth noting that these methods are first studied in natural language processing (NLP) since pre-trained language models <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b3">3]</ref> typically have an enormous parameter size that reaches the billion level. Another popular design in NLP is prompt learning <ref type="bibr" target="#b49">[49,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b25">25]</ref>, which turns some text prompt tokens into learnable vectors. Such an idea has recently been applied to vision-language models <ref type="bibr" target="#b51">[51,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b28">28]</ref> and is also the source of inspiration for the recently proposed VPT <ref type="bibr" target="#b19">[19]</ref>, which adds learnable "pixels" to the input of ViT <ref type="bibr" target="#b9">[9]</ref>.</p><p>More relevant to our work are those trying to unify different parameter-efficient tuning methods <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b30">30]</ref>. He et al. <ref type="bibr" target="#b12">[12]</ref> build a connection between Adapter and prompt learning and cast the problem into the learning of a modification vector, which leads to a unified view and a stronger baseline. UNIPELT <ref type="bibr" target="#b30">[30]</ref> is another unified framework, which subsumes several prompt modules in a block and learns a set of gating functions to selectively activate them. Our work differs from these studies in two crucial ways: (i) we target computer vision problems whereas the previous studies <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b30">30]</ref> focus on NLP; (ii) we unify prompt modules from the NAS perspective with a much more fine-grained control over the model hyper-parameters (e.g., token length and embedding dimension). This allows our model to be deployed in a resource-constrained environment. In the future, we plan to apply our approach to NLP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Neural Architecture Search</head><p>Neural architecture search (NAS) consists of two crucial components: search space and search algorithm. A search space can subsume various designs of how neurons are connected <ref type="bibr" target="#b55">[55]</ref>, diverse combinations of model hyper-parameters <ref type="bibr" target="#b54">[54]</ref>, or different arrangements of specific modules like normalization layers <ref type="bibr" target="#b53">[53]</ref>. When it comes to the search algorithm part, the community has witnessed significant advances: from costly methods like reinforcement learning <ref type="bibr" target="#b54">[54]</ref> or evolutionary search <ref type="bibr" target="#b37">[37]</ref> to more efficient ones based on weight-sharing <ref type="bibr" target="#b35">[35]</ref> or differentiable optimization <ref type="bibr" target="#b27">[27]</ref>. The most relevant work to ours is AutoFormer <ref type="bibr" target="#b4">[4]</ref>, which is a one-shot NAS method focusing on Transformer <ref type="bibr" target="#b40">[40]</ref> models. AutoFormer features a weight entanglement strategy, which allows different subnets sampled from a big supernet to share weights among each other. Our work leverages AutoFormer to solve the problem of engineering parameter-efficient tuning methods, which we hope can inspire future work to address efficient transfer learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion, Limitation and Future Work</head><p>With the proliferation of large-scale pre-training data <ref type="bibr" target="#b48">[48,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b44">44]</ref>, model size in neural networks has also been increased correspondingly in order to reach a certain learning capacity. On the other hand, the rapid increase in model size has also spurred interests in developing efficient transfer learning methods <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b51">51]</ref>.</p><p>Our research presents timely studies on how some recently-proposed parameter-efficient tuning methods, or prompt modules, fare in computer vision problems. Crucially, our studies expose a critical issue that, for any specific downstream dataset, hand-designing an optimal prompt module is extremely challenging. More importantly, we for the first time solve the problem from a NAS perspective and demonstrate the potential of our search-based approach in terms of downstream transfer learning performance, the ability to work in low-data regimes, and robustness to domain shift, which is ubiquitous in real-world data <ref type="bibr" target="#b50">[50]</ref>.</p><p>Our studies also unveil some intriguing phenomena. In particular, we find that the ultimate subnet exhibits different architectural patterns for the three prompt modules across datasets of different natures. Since neural networks' features, as often suggested <ref type="bibr" target="#b46">[46]</ref>, progress from low-level visual primitives in bottom layers to high-level abstractions in top layers, the aforementioned findings entail that different prompt modules work best for features at different levels. We hope such findings and insights can inspire future work on designing more advanced prompt modules.</p><p>In terms of limitations, NOAH requires additional training for the supernet, which inevitably increases the development cost. Moreover, as suggested by the few-shot learning results, NOAH's advantages become clearer when more labeled images are available. In other words, NOAH would require more labels to unleash its full power in practice. For future work, we plan to dig deeper into the mechanisms behind NOAH for better interpretation of the intriguing results and apply NOAH to broader application domains beyond computer vision, such as NLP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Implementation Details</head><p>A.1 Datasets Table <ref type="table" target="#tab_3">4</ref> briefly introduces dataset that we used. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Training Details</head><p>Augmentation For the VTAB-1k <ref type="bibr" target="#b47">[47]</ref>, we follows its default augmentation settings, implementing the resizing and normalization for input images. Specifically, we resize a input image to 224 ? 224, followed by normalizing it with ImageNet <ref type="bibr" target="#b7">[7]</ref> means and standard deviation. For few-shot learning and domain generalization experiments, we implement color-jitters with the factor as 0.4, and RandAugmentation with magnitude equals 9, magnitude standard deviation equals 0.5.</p><p>Hyperparameters We set the embedding dimension of Adapter <ref type="bibr" target="#b17">[17]</ref> and LoRA <ref type="bibr" target="#b18">[18]</ref> in all experiments as 8. As for VPT <ref type="bibr" target="#b19">[19]</ref>, we set its prompt length following the instruction of the paper. For few-shot learning and domain generalization experiments, we consistently set the VPT prompt length as 8 for each dataset. Extensive parameters are shown below.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>2</head><label>2</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of crossover and mutation in the evolutionary search method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Results of few-shot learning on five fine-grained visual recognition datasets. NOAH beats the individual modules on average.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure6: Evaluation on the transferability of subnets. Dataset-specific subnet means the architecture is found from the target dataset. ImageNet subnet means the architecture is found from ImageNet and transferred to the target dataset. All target datasets come from VTAB-1k. In general, better transferability is achieved when the source and target datasets are closer, and vice versa.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Illustration of the subnets architectures for the datasets in VTAB-1k. Subnet architectures show different characteristic in different groups.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-1.png" coords="2,258.47,96.02,306.23,172.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Full results on the VTAB-1k benchmark. The first block contains conventional tuning methods while the second block contains parameter-efficient tuning methods, which is the main focus in this paper. NOAH achieves the best overall performance, which is more than 1% higher on average than the individual prompt modules. Adapter<ref type="bibr" target="#b17">[17]</ref> 0.33 69.2 90.1 68.0 98.8 89.9 82.8 54.3 84.0 94.9 81.9 75.5 72.9 62.4 48.6 78.3 74.8 48.5 29.9 41.6 73.4 LoRA [18] 0.35 67.1 91.4 69.4 98.8 89.4 85.3 54.0 84.9 95.3 84.4 73.6 74.1 62.1 49.8 78.5 81.8 47.1 31.0 35.9 73.8 NOAH 0.42 70.7 91.6 68.2 98.9 90.2 88.4 54.0 85.9 95.3 84.2 73.6 81.7 63.1 49.0 78.5 82.3 45.0 31.8 43.5 74.8</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Natural</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Specialized</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Structured</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell># param (M)</cell><cell>Cifar100</cell><cell>Caltech101</cell><cell>DTD</cell><cell>Flower102</cell><cell>Pets</cell><cell>SVHN</cell><cell>Sun397</cell><cell>Camelyon</cell><cell>EuroSAT</cell><cell>Resisc45</cell><cell>Retinopathy</cell><cell>Clevr-Count</cell><cell>Clevr-Dist</cell><cell>DMLab</cell><cell>KITTI-Dist</cell><cell>dSpr-Loc</cell><cell>dSpr-Ori</cell><cell>sNORB-Azim</cell><cell>sNORB-Ele</cell><cell>Average</cell></row><row><cell cols="2">Full [19]</cell><cell cols="20">85.8 68.9 87.7 64.3 87.2 86.9 87.4 38.8 79.7 95.7 84.2 73.9 56.3 58.6 41.7 65.5 57.5 46.7 25.7 29.1 68.9</cell></row><row><cell cols="22">Linear [19] 0.04 64.4 85.0 63.2 97.0 86.3 36.6 51.0 78.5 87.5 68.5 74.0 34.3 30.6 33.2 55.4 12.5 20.0 9.6 19.2 57.6</cell></row><row><cell cols="2">VPT [19]</cell><cell cols="20">0.46 78.8 90.8 65.8 98.0 88.3 78.1 49.6 81.8 96.1 83.4 68.4 68.5 60.0 46.5 72.8 73.6 47.9 32.9 37.8 72.0</cell></row><row><cell></cell><cell></cell><cell cols="2">Natural</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Specialized</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>82.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>85.0</cell><cell></cell><cell></cell><cell></cell><cell>84.6</cell><cell></cell><cell>84.8</cell><cell></cell><cell>60.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>59.4</cell></row><row><cell>Score (%)</cell><cell>79.0 80.0 81.0</cell><cell>78.5</cell><cell>79.0</cell><cell>79.5</cell><cell>80.9</cell><cell></cell><cell>Score (%)</cell><cell>82.0 83.0 84.0</cell><cell>82.4</cell><cell></cell><cell>84.1</cell><cell></cell><cell></cell><cell></cell><cell>Score (%)</cell><cell>54.0 56.0 58.0</cell><cell>55.0</cell><cell></cell><cell>57.1</cell><cell>57.5</cell></row><row><cell></cell><cell>78.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>81.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>52.0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="4">VPT Adapter LoRA NOAH</cell><cell></cell><cell></cell><cell></cell><cell cols="4">VPT Adapter LoRA</cell><cell cols="2">NOAH</cell><cell></cell><cell></cell><cell cols="5">VPT Adapter LoRA NOAH</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results on domain generalization. NOAH is significantly better than the individual prompt modules on the four domain-shifted datasets.</figDesc><table><row><cell></cell><cell>Source</cell><cell></cell><cell cols="2">Target</cell></row><row><cell></cell><cell cols="4">ImageNet -V2 -Sketch -A</cell><cell>-R</cell></row><row><cell>Adapter [17]</cell><cell>70.5</cell><cell>59.1</cell><cell>16.4</cell><cell cols="2">5.5 22.1</cell></row><row><cell>VPT [19]</cell><cell>70.5</cell><cell>58.0</cell><cell>18.3</cell><cell cols="2">4.6 23.2</cell></row><row><cell>LoRA [18]</cell><cell>70.8</cell><cell>59.3</cell><cell>20.0</cell><cell cols="2">6.9 23.3</cell></row><row><cell>NOAH</cell><cell>71.5</cell><cell>66.1</cell><cell>24.8</cell><cell cols="2">11.9 28.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>With vs without retraining NOAH's subnets. The results show that there is no significant difference between them, suggesting that retraining can be safely removed in practice if the compute resource is limited.</figDesc><table><row><cell></cell><cell>Nat.</cell><cell>Spe.</cell><cell>Str.</cell><cell>Average</cell></row><row><cell>VPT [19]</cell><cell>78.5</cell><cell>82.4</cell><cell>55.0</cell><cell>72.0</cell></row><row><cell>Adapter [17]</cell><cell>79.0</cell><cell>84.1</cell><cell>57.1</cell><cell>73.4</cell></row><row><cell>LoRA [18]</cell><cell>79.5</cell><cell>84.6</cell><cell>57.5</cell><cell>73.9</cell></row><row><cell>NOAH (inherited)</cell><cell>79.5</cell><cell>84.2</cell><cell>58.7</cell><cell>74.1</cell></row><row><cell>NOAH (retrained)</cell><cell>80.9</cell><cell>84.8</cell><cell>59.4</cell><cell>75.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>The first block introduces the information of the datasets in the VTAB-1k<ref type="bibr" target="#b47">[47]</ref> benchmark. The datasets in the second block are five fine-grain dataset used in the CoOp<ref type="bibr" target="#b51">[51]</ref> few-shot learning benchmark. Especially, Research only indicates that this dataset is for research purposes only.</figDesc><table><row><cell></cell><cell>Dataset</cell><cell cols="2">#Classes Train</cell><cell>Val</cell><cell>Test</cell><cell>License</cell></row><row><cell></cell><cell>CIFAR100 [23]</cell><cell>100</cell><cell></cell><cell></cell><cell cols="2">10,000 Research only</cell></row><row><cell></cell><cell>Caltech101 [10]</cell><cell>102</cell><cell></cell><cell></cell><cell cols="2">6,084 Research only</cell></row><row><cell></cell><cell>DTD [6]</cell><cell>47</cell><cell></cell><cell></cell><cell cols="2">1,880 Research only</cell></row><row><cell></cell><cell>Oxford-Flowers102 [33]</cell><cell>102</cell><cell></cell><cell></cell><cell cols="2">6,149 Research only</cell></row><row><cell></cell><cell>Oxford-Pets [34]</cell><cell>37</cell><cell></cell><cell></cell><cell cols="2">3,669 CC BY-SA 4.0</cell></row><row><cell></cell><cell>SVHN [32]</cell><cell>10</cell><cell></cell><cell></cell><cell cols="2">26,032 CC</cell></row><row><cell></cell><cell>Sun397 [43]</cell><cell>397</cell><cell></cell><cell></cell><cell cols="2">21,750 Research only</cell></row><row><cell></cell><cell>Patch Camelyon [41]</cell><cell>2</cell><cell></cell><cell></cell><cell cols="2">32,768 CC0</cell></row><row><cell></cell><cell>EuroSAT [14]</cell><cell>10</cell><cell></cell><cell></cell><cell cols="2">5,400 Research only</cell></row><row><cell>VTAB-1k [47]</cell><cell>Resisc45 [5]</cell><cell>45</cell><cell>800/1,000</cell><cell>200</cell><cell cols="2">6,300 Research only</cell></row><row><cell></cell><cell>Retinopathy [21]</cell><cell>5</cell><cell></cell><cell></cell><cell cols="2">42,670 Research only</cell></row><row><cell></cell><cell>Clevr/count [20]</cell><cell>8</cell><cell></cell><cell></cell><cell cols="2">15,000 CC BY 4.0</cell></row><row><cell></cell><cell>Clevr/distance [20]</cell><cell>6</cell><cell></cell><cell></cell><cell cols="2">15,000 CC BY 4.0</cell></row><row><cell></cell><cell>DMLab [1]</cell><cell>6</cell><cell></cell><cell></cell><cell cols="2">22,735 Research only</cell></row><row><cell></cell><cell>KITTI-Dist [11]</cell><cell>4</cell><cell></cell><cell></cell><cell>711</cell><cell>CC BY-NC-SA 3.0</cell></row><row><cell></cell><cell>dSprites/location [31]</cell><cell>16</cell><cell></cell><cell></cell><cell cols="2">73,728 Research only</cell></row><row><cell></cell><cell>dSprites/orientation [31]</cell><cell>16</cell><cell></cell><cell></cell><cell cols="2">73,728 Research only</cell></row><row><cell></cell><cell cols="2">SmallNORB/azimuth [24] 18</cell><cell></cell><cell></cell><cell cols="2">12,150 Research only</cell></row><row><cell></cell><cell cols="2">SmallNORB/elevation [24] 18</cell><cell></cell><cell></cell><cell cols="2">12,150 Research only</cell></row><row><cell></cell><cell>Food-101 [2]</cell><cell>101</cell><cell></cell><cell cols="3">20,200 30,300 Research only</cell></row><row><cell></cell><cell>Stanford Cars [22]</cell><cell>196</cell><cell></cell><cell cols="3">1,635 8,041 Research only</cell></row><row><cell>Few-shot [51]</cell><cell>Oxford-Flowers102 [33]</cell><cell>102</cell><cell>(1/2/4/8/16)*(#Classes)</cell><cell cols="3">1,633 2,463 Research only</cell></row><row><cell></cell><cell>FGVC-Aircraft [29]</cell><cell>100</cell><cell></cell><cell cols="3">3,333 3,333 Research only</cell></row><row><cell></cell><cell>Oxford-Pets [34]</cell><cell>37</cell><cell></cell><cell>736</cell><cell cols="2">3,669 CC BY-SA 4.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Training setting.</figDesc><table><row><cell></cell><cell cols="5">Optimizer Batch Size Learning Rate Scheduler Weight Decay Warmup Epochs</cell></row><row><cell>VPT</cell><cell></cell><cell>5e-3</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Adapter</cell><cell></cell><cell>5e-3</cell><cell></cell><cell></cell><cell></cell></row><row><cell>LoRA</cell><cell>AdamW 64</cell><cell>5e-3</cell><cell>Cosine</cell><cell>1e-3</cell><cell>10</cell></row><row><cell>NOAH-supernet</cell><cell></cell><cell>5e-4</cell><cell></cell><cell></cell><cell></cell></row><row><cell>NOAH-subnet</cell><cell></cell><cell>5e-3</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>Zero-based indexing is adopted.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>Note that all methods have a new linear classification layer to learn.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>We illustrate the subnets architectures of each dataset in Figure 7. Across all datasets, Adapter and LoRA are usually inserted in the shallow layers. The embedding dimension and depth of VPT are various in different datasets. Specifically, in most datasets of VTAB-natural, VPT has small embedding dimensions, i.e., less than or equal to 50, and lies in shallow depth (layers). In contrast, in the KITTI-Dist [11], DMLab [1], Clevr-Dist [20], and <rs type="person">Clevr-Count</rs> [20] datasets of VTAB-structured, VPT plays an important role in deeper layers with larger embedding dimensions. We note that the variation of searched VPT by different groups coincides with that of manually designed VPT in [19], i.e., VPT has a smaller embedding dimension for VTAB-natural and a larger embedding dimension for VTAB-structured, which indicates the superiority of NOAH that can automatically search the optimal subnet without carefully custom designs.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>{{10</surname></persName>
		</author>
		<imprint>
			<date>5, 5, 5, 10, 50 , 0,?,0},{5, 5, 10, 0, 0, 0,?,0},{5, 5, 5, 0?,0}}</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Charles</forename><surname>Beattie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Teplyashin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heinrich</forename><surname>K?ttler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Lefrancq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V?ctor</forename><surname>Vald?s</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Sadik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03801</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Deepmind lab. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Food-101-mining discriminative components with random forests</title>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Bossard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems (NeuIPS)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Autoformer: Searching transformers for visual recognition</title>
		<author>
			<persName><forename type="first">Minghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houwen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2009">2021. 3, 4, 5, 8, 9</date>
			<biblScope unit="page" from="12270" to="12280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Remote sensing image scene classification: Benchmark and state of the art</title>
		<author>
			<persName><forename type="first">Gong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqiang</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1865" to="1883" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Describing textures in the wild</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2009">2020. 1, 2, 5, 9</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">conference on computer vision and pattern recognition workshop</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="178" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Towards a unified view of parameter-efficient transfer learning</title>
		<author>
			<persName><forename type="first">Junxian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunting</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.04366</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Helber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Bischke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Dengel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damian</forename><surname>Borth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</title>
		<imprint>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The many faces of robustness: A critical analysis of out-of-distribution generalization</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Norman</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Dorundo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samyak</forename><surname>Parajuli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8340" to="8349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Natural adversarial examples</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="15262" to="15271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Parameter-efficient transfer learning for nlp</title>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><surname>Giurgiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislaw</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruna</forename><surname>Morrone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>De Laroussilhe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Gesmundo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Attariyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019">2019. 1, 2, 3, 4, 5, 7, 8, 10</date>
			<biblScope unit="page" from="2790" to="2799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shean</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Lora</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09685</idno>
		<title level="m">Low-rank adaptation of large language models</title>
		<imprint>
			<date type="published" when="2021">2021. 1, 2, 3, 4, 5, 7, 8, 10</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Menglin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bor-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ser-Nam</forename><surname>Lim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.12119</idno>
		<imprint>
			<date type="published" when="2009">2022. 1, 2, 3, 4, 5, 7, 8, 9</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Visual prompt tuning. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Clevr: A diagnostic dataset for compositional language and elementary visual reasoning</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lawrence Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Kaggle diabetic retinopathy detection</title>
		<author>
			<persName><forename type="first">Eyepacs</forename><surname>Kaggle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">3d object representations for finegrained categorization</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision workshops</title>
		<meeting>the IEEE international conference on computer vision workshops</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning methods for generic object recognition with invariance to pose and lighting</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leon</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">The power of scale for parameter-efficient prompt tuning</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08691</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Prefix-tuning: Optimizing continuous prompts for generation</title>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00190</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.09055</idno>
		<title level="m">Darts: Differentiable architecture search</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Prompt distribution learning</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianzhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonggang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yajing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinmei</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Finegrained visual classification of aircraft</title>
		<author>
			<persName><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esa</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juho</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.5151</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Unipelt: A unified framework for parameter-efficient language model tuning</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lambert</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amjad</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.07577</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Loic</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Demis</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Lerchner</surname></persName>
		</author>
		<title level="m">dsprites: Disentanglement testing sprites dataset</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A visual vocabulary for flower classification</title>
		<author>
			<persName><forename type="first">M-E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Cats and dogs</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Omkar M Parkhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Efficient neural architecture search via parameters sharing</title>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melody</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno>PMLR, 2018. 9</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning (ICML)</title>
		<imprint>
			<biblScope unit="page" from="4095" to="4104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Large-scale evolution of image classifiers</title>
		<author>
			<persName><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherry</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Selle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutaka</forename><surname>Leon Suematsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName><surname>Kurakin</surname></persName>
		</author>
		<idno>PMLR, 2017. 9</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page" from="2902" to="2911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Do imagenet classifiers generalize to imagenet</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rebecca</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vaishaal</forename><surname>Shankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5389" to="5400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konrad</forename><surname>Zolna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emilio</forename><surname>Parisotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergio</forename><forename type="middle">Gomez</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Novikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Barth-Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mai</forename><surname>Gimenez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yury</forename><surname>Sulsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jackie</forename><surname>Kay</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.06175</idno>
		<title level="m">Jost Tobias Springenberg, et al. A generalist agent</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NeuIPS)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Rotation equivariant CNNs for digital pathology</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bastiaan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jasper</forename><surname>Veeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jim</forename><surname>Linmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taco</forename><surname>Winkens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-06">June 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning robust global representations by penalizing local predictive power</title>
		<author>
			<persName><forename type="first">Haohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songwei</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeuIPS)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Sun database: Large-scale scene recognition from abbey to zoo</title>
		<author>
			<persName><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krista</forename><forename type="middle">A</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE computer society conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="3485" to="3492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Ling</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuedong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.11432</idno>
		<title level="m">A new foundation model for computer vision</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models</title>
		<author>
			<persName><forename type="first">Elad</forename><surname>Ben Zaken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shauli</forename><surname>Ravfogel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.10199</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">A large-scale study of representation learning with the visual task adaptation benchmark</title>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Ruyssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Riquelme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josip</forename><surname>Djolonga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andre</forename><forename type="middle">Susano</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.04867</idno>
		<imprint>
			<date type="published" when="2005">2019. 1, 2, 5</date>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">Yuanhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zexin</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenfei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.07845</idno>
		<title level="m">Building mega-scale vision dataset continually with human-machine synergy</title>
		<meeting><address><addrLine>Bamboo</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Factual probing is [mask]: Learning vs. learning to recall</title>
		<author>
			<persName><forename type="first">Zexuan</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.05240</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">Kaiyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.02503</idno>
		<title level="m">Domain generalization: A survey</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Learning to prompt for vision-language models</title>
		<author>
			<persName><forename type="first">Kaiyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingkang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.01134</idno>
		<imprint>
			<date type="published" when="2009">2021. 6, 7, 8, 9</date>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Conditional prompt learning for vision-language models</title>
		<author>
			<persName><forename type="first">Kaiyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingkang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning generalisable omni-scale representations for person re-identification</title>
		<author>
			<persName><forename type="first">Kaiyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Cavallaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="page" from="2021" to="2029" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01578</idno>
		<title level="m">Neural architecture search with reinforcement learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8697" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
