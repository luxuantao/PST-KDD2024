<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Epigram: Practical Programming with Dependent Types</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Conor</forename><surname>Mcbride</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Information Technology</orgName>
								<orgName type="institution">University of Nottingham</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Epigram: Practical Programming with Dependent Types</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F1BBC04EAD7AE4ACFA444DECA70655F4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Find the type error in the following Haskell expression:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>if null xs then tail xs else xs</head><p>You can't, of course: this program is obviously nonsense unless you're a typechecker. The trouble is that only certain computations make sense if the null xs test is True, whilst others make sense if it is False. However, as far as the type system is concerned, the type of the then branch is the type of the else branch is the type of the entire conditional. Statically, the test is irrelevant. Which is odd, because if the test really were irrelevant, we wouldn't do it. Of course, tail [] doesn't go wrong-well-typed programs don't go wrong-so we'd better pick a different word for the way they do go.</p><p>Abstraction and application, tupling and projection: these provide the 'software engineering' superstructure for programs, and our familiar type systems ensure that these operations are used compatibly. However, sooner or later, most programs inspect data and make a choice-at that point our familiar type systems fall silent. They simply can't talk about specific data. All this time, we thought our programming was strongly typed, when it was just our software engineering. In order to do better, we need a static language capable of expressing the significance of particular values in legitimizing some computations rather than others. We should not give up on programming.</p><p>James McKinna and I designed Epigram [27,<ref type="bibr" target="#b25">26]</ref> to support a way of programming which builds more of the intended meaning of functions and data into their types. Its style draws heavily from the Alf system <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b20">21]</ref>; its substance from my to Randy Pollack's Lego system <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b22">23]</ref> Epigram is in its infancy and its implementation is somewhat primitive. We certainly haven't got everything right, nor have we yet implemented the whole design. We hope we've got something right. In these notes, I hope to demonstrate that such nonsense as we have seen above is not inevitable in real life, and that the extra articulacy which dependent types offer is both useful and usable. In doing so, I seek to stretch your imaginations towards what programming can be if we choose to make it so.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>mention-hence depend on-x . We still write S → T when T doesn't depend on x . For example, matrix multiplication may be typed 1 mult : ∀i , j , k : Nat ⇒ Matrix i j → Matrix j k → Matrix i k Datatypes like Matrix i j may depend on values fixing some particular property of their elements-a natural number indicating size is but one example. A function can specialize its return type to suit each argument. The typing rules for abstraction and application show how:</p><p>x : S t : T λx ⇒ t : ∀x : S ⇒ T f : ∀x : S ⇒ T s : S f s : [s/x ]T Correspondingly, mult 2 3 1 : Matrix 2 3 → Matrix 3 1 → Matrix 2 1 is the specialized multiplier for matrices of the given sizes.</p><p>We're used to universal quantification expressing polymorphism, but the quantification is usually over types. Now we can quantify over all values, and these include the types, which are values in . Our ∀ captures many forms of abstraction uniformly. We can also see ∀x : S ⇒ T as a logical formula and its inhabitants as a function which computes a proof of [s/x ]T given a particular value s in S. It's this correspondence between programs and proofs, the Curry-Howard Isomorphism, with the slogan 'Propositions-as-Types', which makes dependent type systems particularly suitable for representing computational logics.</p><p>However, if you want to check dependent types, be careful! Look again at the application rule; watch s hopping over the copula, 2 from the term side in the argument hypothesis (eg., our specific dimensions) to the type side in the conclusion (eg., our specific matrix types). With expressions in types, we must think again about when types are equal. Good old syntactic equality won't do: mult (1+1) should have the same type as mult 2, so Matrix (1+1) 1 should be the same type as Matrix 2 1! If we want computation to preserve types, we need at least to identify types with the same normal forms. Typechecking requires the evaluation of previously typechecked expressions-the phase distinction is still there, but it's slipperier.</p><p>What I like about dependent types is their precise language of data structures. In Haskell, we could define a sequence of types for lists of fixed lengths data List0 x = Nil data List1 x = Cons0 x (List0 x) data List2 x = Cons1 x (List1 x) but we'd have to stop sooner or later, and we'd have difficulty abstracting over either the whole collection, or specific subcollections like lists of even length. In 1 We may write ∀x : X ; y : Y ⇒ T for ∀x : X ⇒ ∀y : Y ⇒ T and ∀x1 , x2 : X ⇒ T for ∀x1 : X ; x2 : X ⇒ T . We may drop the type annotation where inferrable. 2 By copula, I mean the ':' which in these notes is used to link a term to its typing:</p><p>Haskell uses '::', and the bold use the set-theoretic '∈'.</p><p>Epigram, we can express the lot in one go, giving us the family of vector types with indices from Nat representing length. Nat is just an ordinary datatype.</p><p>data Nat : where zero : Nat ; n : Nat suc n : Nat data n : Nat ; X : Vec n X : where vnil : Vec zero X</p><p>x : X ; xs : Vec n X vcons x xs : Vec (suc n) X</p><p>Inductive families <ref type="bibr" target="#b14">[15]</ref>, like Vec, are collections of datatypes, defined mutually and systematically, indexed by other data. Now we can use the dependent function space to give the 'tail' function a type which prevents criminal behaviour: vtail : ∀n : Nat ⇒ ∀X : ⇒ Vec (suc n) X → Vec n X For no n is vtail n X vnil well typed. Indexed types state properties of their data which functions can rely on. They are the building blocks of Epigram programming. Our Matrix i j can just be defined as a vector of columns, say: let rows, cols : Nat Matrix rows cols :</p><p>Matrix rows cols ⇒ Vec cols (Vec rows Nat)</p><p>Already in Haskell, there are hooks available to crooks who want more control over data. One can exploit non-uniform polymorphism to enforce some kinds of structural invariant <ref type="bibr" target="#b30">[31]</ref>, like this data Rect col x = Columns [col] | Longer (Rect (col, x)) type Rectangular = Rect () although this type merely enforces rectangularity, rather than a specific size. One can also collect into a Vec type class those functors which generate vector structures <ref type="bibr" target="#b23">[24]</ref>. Matrix multiplication then acquires a type like mult :: (Vec f,Vec g,Vec h)=&gt; f (g Int) -&gt; g (h Int) -&gt; f (h Int)</p><p>Programming with these 'fake' dependent types is an entertaining challenge, but let's be clear: these techniques are cleverly dreadful, rather than dreadfully clever. Hideously complex dependent types certainly exist, but they express basic properties like size in a straightforward way-why should the length of a list be anything less ordinary than a number ? In Epigram, it doesn't matter whether the size of a matrix is statically determined or dynamically supplied-the size invariants are enforced, maintained and exploited, regardless of phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">What is Epigram?</head><p>Epigram is a dependently typed functional programming language. On the surface, the system is an integrated editor-typechecker-interpreter for the language, owing a debt to the Alf <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b20">21]</ref> and Agda <ref type="bibr" target="#b11">[12]</ref> family of proof editors. Underneath, Epigram has a tactic-driven proof engine, like those of Coq <ref type="bibr" target="#b10">[11]</ref> and Epigram's immediate ancestor, the 'Oleg' variant of Lego <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b22">23]</ref>. The latter has proof tactics which mimic Alf's pattern matching style of proof in a more spartan type theory (Luo's UTT <ref type="bibr" target="#b18">[19]</ref>); James McKinna and I designed Epigram <ref type="bibr" target="#b26">[27]</ref> as a 'high-level programming' interface to this technology. An Epigram program is really a tree of proof tactics which drive the underlying construction in UTT.</p><p>But this doesn't answer the wider cultural question of what Epigram is. How it does it relate to functional languages like SML <ref type="bibr" target="#b28">[29]</ref> and Haskell <ref type="bibr" target="#b31">[32]</ref>? How does it relate to previous dependently typed languages like DML <ref type="bibr" target="#b38">[39]</ref> and Cayenne <ref type="bibr" target="#b3">[4]</ref>? How does it relate pragmatically to more conventional ways of working in type theory in the systems mentioned above? What's new? I'll return to these questions at the end of these notes, when I've established more of the basis for a technical comparison, but I can say this much now: DML refines the ML type system with numerical indexing, but the programs remain the same-erase the indices and you have an ML program; Cayenne programs are LazyML programs with a more generous type system, including programs at the type level, but severely restricted support for inductive families. Epigram is not an attempt to strap a more powerful type system to standard functional programming constructs-it's rather an attempt to rethink what programming can become, given such a type system.</p><p>Dependent types can make explicit reference to programs and data. They can talk about programming in a way that simple types can't. In particular, an induction principle is a dependent type. We learned this one as children:</p><p>NatInd : ∀P : Nat → ⇒ P zero → (∀n : Nat ⇒ P n → P (suc n)) → ∀n : Nat ⇒ P n</p><p>It gives rise to a proof technique-to give a proof of a more general proposition P n, give proofs that P holds for more specific patterns which n can take. Now cross out 'proof' and write 'program'. The induction principle for Nat specifies a particular strategy of case analysis and recursion, and Epigram can read it as such. Moreover, we can readily execute 'proofs' by induction, recursively applying the step program to the base value, to build a proof for any specific n:</p><p>NatInd P mz ms zero ; mz NatInd P mz ms (suc n) ; ms n (NatInd P mz ms n)</p><p>Usually, functional languages have hard-wired constructs for constructor case analysis and general recursion; Epigram supports programming with any matching and recursion which you can specify as an induction principle and implement as a function. Epigram also supports a first-order method of implementing new induction principles-they too arise from inductive families.</p><p>It may surprise (if not comfort) functional programmers to learn that dependently typed programming seems odd to type theorists too. Type theory is usually seen either as the integration of 'ordinary' programming with a logical superstructure, or as a constructive logic which permits programs to be quietly extracted from proofs. Neither of these approaches really exploits dependent types in the programs and data themselves. At time of writing, neither Agda nor Coq offers substantial support for the kind of data structures and programs we shall develop in these notes, even though Alf and 'Oleg' did! There is a tendency to see programming as a fixed notion, essentially untyped. In this view, we make sense of and organise programs by assigning types to them, the way a biologist classifies species, and in order to classify more the exotic creatures, like printf or the zipWith family, one requires more exotic types. This conception fails to engage with the full potential of types to make a positive contribution to program construction. Given what types can now express, let us open our minds afresh to the design of programming language constructs, and of programming tools and of the programs we choose to write anyway.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Overview of the Remaining Sections</head><p>2 Warm Up; Add Up tries to give an impression of Epigram's interactive style programming and the style of the programs via very simple examplesaddition and the Fibonacci function. I expose the rôle of dependent types behind the scenes, even in simply typed programming. 3 Vectors and Finite Sets introduces some very basic datatype families and operations-I explore Vec and also the family Fin of finite enumeration types, which can be used to index vectors. I show how case analysis for dependent types can be more powerful and more subtle than its simply typed counterpart. 4 Representing Syntax illustrates the use of dependent types to enforce key invariants in expression syntax-in particular, the λ-calculus. I begin with untyped de Bruijn terms after the manner of Bird and Paterson <ref type="bibr" target="#b8">[9]</ref> and end with simply typed de Bruijn terms in the manner of Altenkirch and Reus <ref type="bibr" target="#b1">[2]</ref>. <ref type="foot" target="#foot_0">3</ref>On the way, I'll examine some pragmatic issues in data structure design. 5 Is Looking Seeing? homes in on the crux of dependently typed programmingevidence. Programs over indexed datatypes may enforce invariants, but how do we establish them? This section explores our approach to data analysis <ref type="bibr" target="#b26">[27]</ref>, expressing inspection as a form of induction and deriving induction principles for old types by defining new families. 6 Well Typed Programs which Don't Go Wrong shows the development of two larger examples-a typechecker for simply typed λ-calculus which yields a typed version of its input or an informative diagnostic, and a tagless and total evaluator for the well typed terms so computed. 7 Epilogue reflects on the state of Epigram and its future in relation to what's happening more widely in type theory and functional programming.</p><p>I've dropped from these notes a more formal introduction to type theory: which introductory functional programming text explains how the typechecker works within the first forty pages? A precise understanding of type theory isn't necessary to engage with the ideas, get hold of the basics and start programming. I'll deal with technicalities as and when we encounter them. If you do feel the need to delve deeper into the background, there's plenty of useful literature out there-the next subsection gives a small selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4">Some Useful Reading</head><p>Scholars of functional programming and of type theory should rejoice that they now share much of the same ground. It would be terribly unfortunate for the two communities each to fail to appreciate the potential contribution of the other, through cultural ignorance. We must all complain less and read more! For a formal presentation of the Epigram language, see 'The view from the left' <ref type="bibr" target="#b26">[27]</ref> For a deeper exploration of its underlying type theory-see 'Computation and Reasoning: A Type Theory for Computer Science' <ref type="bibr" target="#b18">[19]</ref> by Zhaohui Luo. User documentation, examples and solutions to exercises are available online <ref type="bibr" target="#b25">[26]</ref>.</p><p>Much of the impetus for Epigram comes from proof assistants. Proof and programming are similar activities, but the tools have a different feel. I can recommend 'Coq'Art' <ref type="bibr" target="#b6">[7]</ref> by Yves Bertot and Pierre Castéran as an excellent tutorial for this way of working, and for the Coq system in particular. The tactics of a theorem prover animate the rules of its underlying type theory, so this book also serves as a good practical introduction to the more formal aspects.</p><p>The seminal textbook on type theory as a programming language is 'Programming in Martin-Löf's type theory: an introduction' <ref type="bibr" target="#b34">[35]</ref> by Bengt Nordström, Kent Petersson and Jan Smith. It is now fifteen years old and readily available electronically, so there's no excuse to consider type theory a closed book.</p><p>Type theorists should get reading too! Modern functional programming uses richer type systems to express more of the structure of data and capture more patterns of computation. I learned a great deal from 'Algebra of Programming' by Richard Bird and Oege de Moor <ref type="bibr" target="#b7">[8]</ref>. It's a splendid and eye-opening introduction to a more categorical and calculational style of correct program construction. 'Purely Functional Data Structures' by Chris Okasaki <ref type="bibr" target="#b29">[30]</ref> is a delightful compendium of data structures and algorithms, clearly showing the advantages of fitting datatypes more closely to algorithms.</p><p>The literature on overloading, generic programming, monads, arrows, higherorder polymorphism is too rich to enumerate, but it raises important issues which type theorists must address if we want to make a useful contribution to functional programming in practice. I'd advise hungry readers to start with this very series of Advanced Functional Programming lecture notes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.5">For Those of you Watching in Black &amp; White</head><p>Before we start in earnest, let's establish typographical conventions and relate the system's display with these notes. Epigram's syntax is two-dimensional: the buffer contains a document with a rectangular region selected-highlighted with a bright background. A document is a vertical sequence of lines; a line is a horizontal sequence of boxes; a box is either a character, or a bracket containing a document. A bracket is either a group,</p><formula xml:id="formula_0">( ! ! • • • ! ! )</formula><p>, which has the usual functions of parenthesis, or a shed,</p><formula xml:id="formula_1">[ ! ! • • • ! ! ]</formula><p>, where you can tinker with text as you please. Yellow backgrounds come about when typing constraints cannot yet be solved, but it's still possible for the variables they involve to become more instantiated, allowing for a solution in the future.</p><p>There are some pieces of ascii syntax which I cannot bring myself to uglify in L A T E X. I give here the translation table for tokens and for the extensible delimiters of two-dimensional syntax:</p><formula xml:id="formula_2">∀ λ → ∧ ⇒ ⇐ * all lam -&gt; /\ =&gt; &lt;= ( ! ! ) [ ! ! ] ---</formula><p>Moreover, to save space here, I adopt an end-of-line style with braces {}, where the system puts them at the beginning.</p><p>At the top level, the document is a vertical sequence of declarations delineated by rules. A rule is a sequence of at least three ---. The initial document has just one declaration, consisting of a shed, waiting for you to start work. The 'two-dimensional' version is a first-order presentation in the style of natural deduction rules<ref type="foot" target="#foot_2">5</ref>  <ref type="bibr" target="#b33">[34]</ref>. Above the line go hypotheses typing the arguments; below, the conclusion typing a template for a value. The declarations 'zero is a Nat; if n is a Nat, then so is suc n' tell us what Nats look like. We get the actual types of zero and suc implicitly, by discharging the hypotheses.</p><p>We may similarly declare our function in the natural deduction style: let</p><p>x , y : Nat plus x y : Nat This signals our intention to define a function called plus which takes two natural numbers and returns a natural number. The machine responds plus x y [ ] by way of asking 'So plus has two arguments, x and y. What should it do with them?'. Our type signature has become a programming problem to be solved interactively. The machine supplies a left-hand side, consisting of a function symbol applied to patterns, binding pattern variables-initially, the lefthand side looks just like the typical application of plus which we declared and the pattern variables have the corresponding names. But they are binding occurrences, not references to the hypotheses. The scope of a rule's hypotheses extends only to its conclusion; each problem and subproblem has its own scope.</p><p>Sheds [ ] are where we develop solutions. The basic editing operation in Epigram is to expose the contents of a shed to the elaborator. We can write the whole program in one shed, then elaborate it; or we can work a little at a time. If we select a shed, Epigram will tell us what's in scope (here, x , y in Nat), and what we're supposed to be doing with it (here, explaining how to compute plus x y in Nat). We may proceed by filling in a right-hand side, explaining how to reduce the current problem to zero or more subproblems.</p><p>I suggest that we seek to define plus by structural recursion on x , entering the right-hand side ⇐ recx . The '⇐' is pronounced 'by': it introduces right-hand sides which explain by what means to reduce the problem. Here we get</p><formula xml:id="formula_3">plus x y ⇐ rec x { plus x y [ ] }</formula><p>Apparently, nothing has changed. There is no presumption that recursion on x will be accompanied immediately (or ever) by case analysis on x . If you select the shed, you'll see that something has changed-the context of the problem has acquired an extra hypothesis, called a memo-structure. A precise explanation must wait, but the meaning of the problem is now 'construct plus x y, given x , y and the ability to call to plus on structural subterms of x '. So let us now analyse x , proceeding ⇐ case x .</p><formula xml:id="formula_4">plus x y ⇐ rec x { plus x y ⇐ case x { plus zero y [ ] plus (suc x ) y [ ] }}</formula><p>The two subproblems are precisely those corresponding to the two ways x could have been made by constructors of Nat. We can certainly finish the first one off, entering ⇒ y. (The ' ⇒ ' is 'return'.) We can make some progress on the second by deciding to return the successor of something, ⇒ suc [ ] .</p><formula xml:id="formula_5">plus x y ⇐ rec x { plus x y ⇐ case x { plus zero y ⇒ y plus (suc x ) y ⇒ suc [ ] }}</formula><p>Select the remaining shed and you'll see that we have to fill in an element of Nat, given x , y and a subtly different memo-structure. Case analysis has instantiated the original argument, so we now have the 'ability to make recursive calls to plus on structural subterms of (suc x )' which amounts to the more concrete and more useful 'ability to make recursive calls to plus on structural subterms of x , and on x itself'. Good! We can finish off as follows:</p><formula xml:id="formula_6">plus x y ⇐ rec x { plus x y ⇐ case x { plus zero y ⇒ y plus (suc x ) y ⇒ suc (plus x y) }}</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Who did the Work?</head><p>Two pages to add unary numbers? And that's a simple example? If it's that much like hard work, do we really want to know? Well, let's look at how much was work and how much was culture shock. We wrote the bits in the boxes:</p><formula xml:id="formula_7">let x , y : Nat plus x y : Nat ; plus x y ⇐ rec x { plus x y ⇐ case x { plus zero y ⇒ y plus (suc x ) y ⇒ suc (plus x y) }}</formula><p>We wrote the type signature: we might have done that for virtue's sake, but virtue doesn't pay the rent. Here, we were repaid-we exchanged the usual hand-written left-hand sides for machine-generated patterns resulting from the conceptual step (usually present, seldom written) ⇐ case x . This was only possible because the machine already knew the type of x . We also wrote the ⇐ rec x , a real departure from conventional practice, but we got repaid for that too-we (humans and machines) know that plus is total.</p><p>Perhaps it's odd that the program's text is not entirely the programmer's work. It's the record of a partnership where we say what the plan is and the machine helps us carry it out. Contrast this with the 'type inference' model of programming, where we write down the details of the execution and the machine tries to guess the plan. In its pure form, this necessitates the restriction of plans to those which are blatant enough to be guessed. As we move beyond the Hindley-Milner system, we find ourselves writing down type information anyway.</p><p>'Type inference' thus has two aspects: 'top-level inference'-inferring type schemes, as with Hindley-Milner 'let'-and 'program inference given types'inferring details when a scheme is instantiated, as with Hindley-Milner variables. Epigram rejects the former, but takes the latter further than ever. As types represent a higher-level design statement than programs, we should prefer to write types if they make programs cheaper.</p><p>Despite its interactive mode of construction, Epigram is fully compliant with the convention that a file of source code, however manufactured, contains all that's required for its recognition as a program. The bare text, without colour or other markup, is what gets elaborated. The elaboration process for a large code fragment just reconstructs a suitable interactive development offstage, cued by the program text-this is how we reload programs. You are free to negotiate your own compromise between incremental and batch-mode programming.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Where are the Dependent Types?</head><p>The type of plus is unremarkably simple, but if you were watching closely, you'll have noticed that the machine was using dependent types the whole time. Let's take a closer look. Firstly, a thought experiment-define a primitive recursion operator for Nat in Haskell as follows: Now we must fill in the methods mz and ms, but do their types show what rôle they play? There are seven occurrences <ref type="foot" target="#foot_3">6</ref> of Nat in their types-which is which? Perhaps you can tell, because you understand primRec, but how would a machine guess? And if we were defining a more complex function this way, we might easily get lost-try defining equality for lists using foldr.</p><formula xml:id="formula_8">primRec{ -p -} :: Nat -&gt; p -&gt; (Nat -&gt; p -&gt; p) -&gt; p primRec{ -p -} Zero mz ms = mz primRec{ -p -} (Suc n) mz ms = ms n (primRec{ -p -} n mz</formula><p>However, recall our NatInd principle with operational behaviour just like primRec but a type which makes clear the relationship between the methods and the patterns for which they apply. If we're careful, we can use that extra information to light our way. Where primRec takes a constant type parameter, NatInd takes a function P : Nat → . If we take P x ; Nat → Nat, we get the primRec situation. How might we use P 's argument to our advantage? Internally, Epigram doesn't build plus : Nat → Nat → Nat but rather a proof ♦plus : ∀x , y : Nat ⇒ plus x y : Nat We can interpret plus x y : Nat as the property of x and y that 'plus x y is a computable element of Nat'. This type is equipped with a constructor which packs up values and a function which runs computations From this proof, you can read off both the high-level program and the its low-level operational behaviour in terms of primitive recursion. And that's basically how Epigram works! Dependent types aren't just the basis of the Epigram languagethe system uses them to organise even simply typed programming.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">What are case and rec?</head><p>In the plus we actually wrote, we didn't use induction-we used case x and rec x . These separate induction into its aspects of distinguishing constructors and of justifying recursive calls. The keywords case and rec cannot stand alone, but case e and rec e are meaningful whenever e belongs to a datatype-Epigram constructs their meaning from the structure of that datatype.</p><p>In our example, x : Nat, and Epigram give us case x : ∀P : Nat → ⇒ (P zero) → (∀x :</p><formula xml:id="formula_9">Nat ⇒ P (suc x )) → P x</formula><p>This is an induction principle instantiated at x with its inductive hypotheses chopped off: it just says, 'to do P with x , show how to do P with each of these patterns'. The associated computational behaviour puts proof into practice:</p><p>(case zero) P mz ms ; mz (case (suc x )) P mz ms ; ms x</p><p>There is nothing special about case x . When elaborating ⇐ e, it's the type of e which specifies how to split a problem into subproblems. If, as above, we take P ; λx ⇒ ∀y : Nat ⇒ plus x y : Nat then the types of mz and ms give us the split we saw when we wrote the program. What about rec x ?</p><formula xml:id="formula_10">rec x : ∀P : Nat → ⇒ (∀x : Nat ⇒ (memo x ) P → P x ) → P x</formula><p>This says 'if you want to do P x , show how to do it given access to P for everything structurally smaller than x '. This (memox ) is another gadget generated by Epigram from the structure of x 's type-it uses the power of computation in types to capture the notion of 'structurally smaller':</p><p>(memo zero) P ; One (memo (suc n)) P ; (memo n) P ∧ P n That is (memox )P is the type of a big tuple which memoizes P for everything smaller than x . If we analyse x , the memo-structure computes, <ref type="foot" target="#foot_4">7</ref> giving us the trivial tuple for the zero case, but for (suc n), we gain access to P n. Let's watch the memo-structure unfolding in the inevitable Fibonacci example.</p><formula xml:id="formula_11">let n : Nat fib n : Nat ; fib n ⇐ rec n { fib n ⇐ case n { fib zero ⇒ zero fib (suc n) ⇐ case n { fib (suc zero) ⇒ suc zero fib (suc (suc n)) ⇒ [ ] }}}</formula><p>If you select the remaining shed, you will see that the memo structure in the context has unfolded (modulo trivial algebra) to:</p><formula xml:id="formula_12">(memo n) (λx ⇒ fib x : Nat ) ∧ fib n : Nat ∧ fib (suc n) : Nat</formula><p>which is just as well, as we want to fill in plus (fib n) (fib (suc n)).</p><p>At this stage, the approach is more important than the details. The point is that programming with ⇐ imposes no fixed notion of case analysis or recursion. Epigram does not have 'pattern matching'. Instead, ⇐ admits whatever notion of problem decomposition is specified by the type of the expression (the eliminator ) which follows it. The value of the eliminator gives the operational semantics to the program built from the solutions to the subproblems.</p><p>Of course, Epigram equips every datatype with case and rec, giving us the usual notions of constructor case analysis structural recursion. But we are free to make our own eliminators, capturing more sophisticated analyses or more powerful forms of recursion. By talking about patterns, dependent types give us the opportunity to specify and implement new ways of programming.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Pause for Thought</head><p>Concretely, we have examined one datatype and two programs. Slightly more abstractly, we have seen the general shape of Epigram programs as decision trees. Each node has a left-hand side, stating a programming problem p, and a right-hand-side stating how to attack it. The leaves of the tree p ⇒ t explain directly what value to return. The internal nodes p ⇐ e use the type of the eliminator e as a recipe for reducing the problem statement to subproblem statements, and the value of e as a recipe for solving the whole problem, given the solutions to the subproblems. Every datatype is equipped with eliminators of form case x for constructor case analysis and rec x for structural recursion, allowing us to construct obviously total programs in a pattern matching style.</p><p>However, the ⇐ construct is a more general tool than the case construct of conventional languages. We answer Wadler's question of how to combine data abstraction with notions of pattern matching <ref type="bibr" target="#b37">[38]</ref> by making notions of pattern matching first-class values.</p><p>It's reasonable to ask 'Can't I write ordinary programs in an ordinary way? Must I build decision trees?'. I'm afraid the answer, for now, is 'yes', but it's just a matter of syntactic sugar. We're used to prioritized lists of patterns with a 'take the first match' semantics <ref type="bibr" target="#b27">[28]</ref>. Lennart Augustsson showed us how to compile these into trees of case-on-variables <ref type="bibr" target="#b2">[3]</ref>. Programming in Epigram is like being Augustsson's compiler-you choose the tree, and it shows you the patterns. The generalization lies in what may sit at the nodes. One could flatten those regions of the tree with nonempty ⇐ case x nodes and use Augustsson's algorithm to recover a decision tree, leaving ⇐ explicit only at 'peculiar' nodes.</p><p>Of course, this still leaves us with explicit ⇐ rec x supporting structural recursion. Can we get rid of that? There are various methods of spotting safe recursive calls <ref type="bibr" target="#b16">[17]</ref>; some even extend to tracking guardedness through mutual definitions <ref type="bibr" target="#b0">[1]</ref>. We could use these to infer obvious appeals to rec, leaving only sophisticated recursions explicit. Again, it's a question of work. Personally, I want to write functions which are seen to be total. Some might complain 'What's the point of a programming language that isn't Turing complete?', but I ask in return, 'Do you demand compulsory ignorance of totality?'. Let's guarantee totality explicitly whenever we can <ref type="bibr" target="#b36">[37]</ref>. It's also possible, contrary to popular nonsense, to have dependent types and general recursive programs, preserving decidable typechecking: the cheapest way to do this is to work under an assumed eliminator with type ∀P : ⇒ (P → P ) → P to which only the run time system gives a computational behaviour; a less drastic way is to treat general recursion as an impure monadic effect.</p><p>But in any case, you might be surprised how little you need general recursion. Dependent types make more programs structurally recursive, because dependent types have more structure. Inductive families with inductive indices support recursion on the data itself and recursion on the indices. For example, firstorder unification <ref type="bibr" target="#b35">[36]</ref> becomes structurally recursive when you index terms by the number of variables over which they are constructed-solving a variable may blow up the terms, but it decreases this index <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Some Familiar Datatypes</head><p>Just in time for the first set of exercises, let's declare some standard equipment. We shall need Bool, which can be declared like so: data Bool :</p><p>where true, false : Bool</p><p>The standard Maybe type constructor is also useful: data X : Maybe X :</p><p>where nothing : Maybe X ;</p><p>x : X just x : Maybe X Note that I didn't declare X in the rules for nothing and just. The hypotheses of a rule scope only over its conclusion, so it's not coming from the Maybe rule. Rather, in each rule Epigram can tell from the way X is used that it must be a type, and it silently generalizes the constructors, just the way the Hindley-Milner system generalizes definitions.</p><p>It's the natural deduction notation which triggers this generalization. We were able to define Bool without it because there was nothing to generalize. Without the rule to 'catch' the X , plain nothing : Maybe X wouldn't exactly be an error. The out-of-scope X is waiting to be explained by some prior definition: the nothing constructor would then be specific to that X.</p><p>Rule-induced generalization is also happening here, for polymorphic lists:   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Vectors and Finite Sets</head><p>Moving on to dependent data structures now, let's take a closer look at Vec:</p><formula xml:id="formula_13">data n : Nat ; X : Vec n X : where vnil : Vec zero X x : X ; xs : Vec n X vcons x xs : Vec (suc n) X</formula><p>The generalization mechanism ensures that all the previously undeclared variables arising inside each deduction rule are silently quantified in the resulting type, with the implicit ∀ quantifier. Written out in full, we have declared data</p><p>Vec : Nat → → where vnil : ∀ X : ⇒ Vec zero X vcons :</p><formula xml:id="formula_14">∀ X : ⇒ ∀ n : Nat ⇒ X → Vec n X → Vec (suc n) X</formula><p>On usage, Epigram tries to infer arguments for expressions with implicitly quantified types, just the way the Hindley-Milner system specializes polymorphic things-by solving the equational constraints which arise in typechecking. However, Epigram needs and supports a 'manual override': the postfix operator inhibits inference and makes an implicit function explicit, so vnil : ∀X : ⇒ Vec zero X vnil Nat : Vec zero Nat To save space, I often write overridden arguments as subscripts-eg., vnil Nat .</p><p>Given this definition, let's start to write some simple programs: In the latter, not only do we get that it's vcons as opposed to vnil: it's the particular vcons which extends vectors of the length we need. What's going on? Much as Thierry Coquand proposed in <ref type="bibr" target="#b12">[13]</ref>, Epigram is unifying the scrutinee of the case with the possible constructor patterns, in both term and type:</p><formula xml:id="formula_15">ys : Vec (suc m) Y unifier vnil X : Vec zero X impossible vcons X n x xs : Vec (suc n) X X = Y , n = m, xs = vcons Y m x xs</formula><p>Only the vcons case survives-Epigram then tries to choose names for the pattern variables which maintain a 'family resemblance' to the scrutinee, hence the (vcons y ys) in the patterns. This unification doesn't just rule cases in or out: it can also feed information to type-level computations. Here's how to append vectors: The point is that by looking at the first vector, we learn about its length. This lets plus compute exactly as we need for ys : Vec (plus zero n) X in the vnil case. For vcons, the return type is Vec (plus (suc m) n) X ; Vec (suc (plus m n)) X , which is what we supply.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Finite Sets</head><p>Let's examine the consequences of dependent case analysis for a different family: Here, inspecting i forces n to be non-zero in each case, so xs can only be a vcons.</p><p>The same result is achieved either way, but in both definitions, we rely on the impact the first case analysis has on the possibilities for the second. It may seem a tautology that dependent case analyses are not independent, but its impact is profound. We should certainly ask whether the traditional case expression, only expressing the patterns of its scrutinee, is as appropriate as it was in the past.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Refining Programming Problems</head><p>Our unification tables give some intuition to what is happening with case analysis. In Thierry Coquand's presentation of dependent pattern matching <ref type="bibr" target="#b12">[13]</ref>, constructor case analysis is hard-wired and unification is built into the typing rules. In Epigram, we have the more generic notion of refining a programming problem by an eliminator, ⇐ e. If we take a closer look at the elaboration of this construct, we'll see how unification arises and is handled inside the type theory. I'll maintain both the general case and the vtail example side by side.</p><p>As we saw with plus, when we say </p><formula xml:id="formula_16">Vec (suc m) Y ⇒ n=(suc m) → X =Y → xs=ys → vtail m Y ys : Vec m Y</formula><p>This is just Henry Ford's old joke. Our motive is to produce a proof of ∀∆ ⇒ f p : T , for 'any Θ we like as long as it's t '-the t are the only Θ we keep in stock. For our example, that means 'any vector you like as long as it's nonempty and its elements are from Y '. This = is heterogeneous equality, 9  which allows any elements of arbitrary types to be proclaimed equal. Its one constructor, refl, says that a thing is equal to itself. s : S ; t : T s=t : refl : t=t Above, the types of xs and ys are different, but they will unify if we can solve the prior equations. Hypothetical equations don't change the internal rules by which the typechecker compares types-this is lucky, as hypotheses can lie.</p><p>If we can construct the methods, m i , then we're done: </p><formula xml:id="formula_17">m i : ∀∆ i ; ∆ ⇒ s i = t → f p : T In our example, we need m 1 : ∀ X ; m; Y ; ys : Vec (suc m) Y ⇒ zero=(suc m) → X =Y → vnil=ys → vtail m Y ys : Vec m Y m 2 : ∀ X ; n; x ; xs : Vec n X ; m; Y ; xs : Vec (suc m) Y ⇒ (suc n)=(suc m) → X =Y → (vcons x xs)=ys → vtail m Y ys : Vec m Y</formula><p>Look at the equations! They express exactly the unification problems for case analysis which we tabulated informally. Now to solve them: the rules of first-order unification for data constructors-see figure <ref type="figure">1</ref> (read backwards) simplifies a problem with an equational hypothesis. We apply these simplifications to the method types. The conflict and cycle rules dispose of 'impossible case' subproblems. Meanwhile, the substitution rule instantiates pattern variables. In general, the equations s i = t will be reduced as far as possible by first-order unification, and either the subproblem will be dismissed, or it will yield some substitution, instantiating the patterns p.</p><p>In our example, the vnil case goes by conflict, and the vcons case becomes:</p><formula xml:id="formula_18">∀ Y ; m; x : Y ; xs : Vec Y m ⇒ vtail Y m (vcons x xs) : Vec Y m</formula><p>After 'cosmetic renaming' gives x and xs names more like the original ys, we get</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>vtail (vcons y ys) [ ]</head><p>To summarize, elaboration of ⇐ e proceeds as follows: (1) choose a motive with equational constraints;</p><p>(2) simplify the constraints in the methods by first-order unification;</p><p>(3) leave the residual methods as the subproblems to be solved by subprograms.</p><p>In the presence of defined functions and higher types, unification problems won't always be susceptible to first-order unification, but Epigram will make what progress it can and leave the remaining equations unsolved in the hypotheses of subproblems-later analyses may reduce them to a soluble form. Moreover, there is no reason in principle why we should not consider a constraint-solving procedure which can be customized by user-supplied rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Reflection on Inspection</head><p>We If we have ys : Vec (suc m) Y , then preVtail ys refl will be well typed. We could even use this function with a more informative conditional expression: condInfo : ∀P : ; b : Bool ⇒ (b=true → P ) → (b=false → P ) → P However, this way of working is clearly troublesome. Moreover, given a nonempty vector xs, there is more than just a stylistic difference between decomposing it with vhead and vtail and decomposing it with (case xs)-the destructor functions give us an element and a shorter vector; the case analysis tells us that xs is the vcons of them, and if any types depend on xs, that might just be important. Again, we can construct a proof of xs=vcons (vhead xs) (vtail xs), but this is much harder to work with.</p><p>In the main, selectors-and-destructors are poor tools for working with data on which types depend. We really need forms of inspection which yield static information. This is a new issue, so there's no good reason to believe that the old design choices remain appropriate. We need to think carefully about how to reflect data's new rôle as evidence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Vectorized Applicative Programming</head><p>Now that we've seen how dependent case analysis is elaborated, let's do some more work with it. The next example shows a key difference between Epigram's implicit syntax and parametric polymorphism. The operation let</p><p>x : X vec x : Vec n X makes a vector of copies of its argument. For any given usage of vec, the intended type determined the length, but how are we to define vec? We shall need to work by recursion on the intended length, hence we shall need to make this explicit at definition time. The following declaration achieves this:</p><formula xml:id="formula_19">let n : Nat ; x : X vec n x : Vec n X ; vec n x ⇐ rec n { vec n x ⇐ case n { vec zero x ⇒ vnil vec (suc n) x ⇒ vcons x (vec n x ) }}</formula><p>Note that in vec's type signature, I explicitly declare n first, thus making it the first implicit argument: otherwise, X might happen to come first. By the way, we don't have to override the argument in the recursive call vec n x -it's got to be a Vec n X -but it would perhaps be a little disconcerting to omit the n, especially as it's the key to vec's structural recursion.</p><p>The following operation-vectorized application-turns out to be quite handy. As it happens, the combination of vec and va equip us with 'vectorized applicative programming', with vec embedding the constants, and va providing application. Transposition is my favourite example of this: You should find that fmax and fweak partition the finite sets, just as fz and fs do. Imagine how we might pretend they're an alternative set of constructors. . . Exercise 14 (vtab) Implement vtab, the inverse of vproj, tabulating a function over finite sets as a vector. </p><formula xml:id="formula_20">let xij : Vec i (Vec j X ) transpose xij : Vec j (Vec i X ) transpose xij ⇐ rec xij { transpose xij ⇐ case xij { transpose vnil ⇒</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Representing Syntax</head><p>The Fin family can represent de Bruijn indices in nameless expressions <ref type="bibr" target="#b13">[14]</ref>. As Françoise Bellegarde and James Hook observed in <ref type="bibr" target="#b5">[6]</ref>, and Richard Bird and Ross Paterson were able to implement in <ref type="bibr" target="#b8">[9]</ref>, you can do this in Haskell, up to a point-here are the λ-terms with free variables given by v:</p><formula xml:id="formula_21">data Term v = Var v | App (Term v) (Term v) | Lda (Term (Maybe v))</formula><p>Under a Lda, we use (Maybe v) as the variable set for the body, with Nothing being the new free variable and Just embedding the old free variables. Renaming is just fmap, and substitution is just the monadic 'bind' operator &gt;&gt;=.</p><p>However, Term is a bit too polymorphic. We can't see the finiteness of the variable context over which a term is constructed. In Epigram, we can take the number of free variables to be a number n, and choose variables from Fin n.  Not so long ago, we were quite excited about the power of non-uniform datatypes to capture useful structural invariants. Scoped de Bruijn terms gave a good example, but most of the others proved more awkward even than the 'fake' dependent types you can cook up using type classes <ref type="bibr" target="#b23">[24]</ref>.</p><p>Real dependent types achieve more with less fuss. This is mainly due to the flexibility of inductive families. For example, if you wanted to add 'weakening' to delay explicitly the shifting of a term as you push it under a binder-in Epigram, but not Haskell or Cayenne, you could add the constructor t : Tm n weak t : Tm (suc n)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Exercises: Renaming and Substitution</head><p>If Fin m is a variable set, then some ρ : Fin m → Fin n is a renaming. If we want to apply a renaming to a term, we need to be able to push it under a lda. Hence we need to weaken the renaming, mapping the new source variable to the new target variable, and renaming as before on the old variables.  We've seen untyped λ-calculus: let's look at how to enforce stronger invariants, by representing a typed λ-calculus. Recall the rules of the simply typed λ-calculus:</p><formula xml:id="formula_22">Γ ; x ∈ σ; Γ x ∈ σ Γ ; x ∈ σ t ∈ τ Γ λx ∈ σ. t ∈ σ ⊃ τ Γ f ∈ σ ⊃ τ Γ s ∈ σ Γ f s ∈ τ</formula><p>Well-typed terms are defined with respect to a context and a type. Let's just turn the rules into data! I add a base type, to make things more concrete. </p><formula xml:id="formula_23">where i : Fin n svar i : STm Γ (sproj Γ i ) ; t : STm (bind Γ σ) τ slda t : STm Γ (sFun σ τ ) f : STm Γ (sFun σ τ ) ; s : STm Γ σ sapp f s : STm Γ τ</formula><p>This is a precise definition of the simply-typed λ-terms. But is it any good? Well, just try writing programs with it. How would you implement renaming? As before, we could represent a renaming as a function ρ : Fin m → Fin n. Can we rename a term in STm Γ τ to get a STm ∆ τ , where Γ : SCtxt m and ∆ : SCtxt n? Here comes the crunch:</p><formula xml:id="formula_24">• • • ren n Γ ∆ ρ (svar i ) ⇒ svar (ρ i)</formula><p>The problem is that svar i : STm Γ (sproj Γ i ), so we want a STm ∆ (sproj Γ i ) on the right, but we've got a STm ∆ (sproj ∆ (ρ i )). We need to know that ρ is type-preserving! Our choice of variable representation prevents us from building this into the type of ρ. We are forced to state an extra condition:</p><formula xml:id="formula_25">∀i : Fin m ⇒ sproj Γ i =sproj ∆ (ρ i )</formula><p>We'll need to repair our program by rewriting with this proof. 11 But it's worse than that! When we move under a slda, we'll lift the renaming, so we'll need a different property:</p><formula xml:id="formula_26">∀i : Fin (suc m) ⇒ sproj (bind Γ σ) i =sproj (bind ∆ σ) (lift ρ i )</formula><p>This follows from the previous property, but it takes a little effort. My program has just filled up with ghastly theorem-proving. Don't dependent types make life a nightmare? Stop the world I want to get off! If you're not afraid of hard work, you can carry on and make this program work. I think discretion is the better part of valour-let's solve the problem instead. We're working with typed terms but untyped variables, and our function which gives types to variables does not connect the variable clearly to the context. For all we know, sproj always returns sNat! No wonder we need 'logical superstructure' to recover the information we've thrown away.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Dependent Types to the Rescue</head><p>Instead of using a program to assign types to variables and then reasoning about it, let's just have typed variables, as Thorsten Altenkirch and Bernhard Reus <ref type="bibr" target="#b1">[2]</ref>. This family strongly resembles Fin. Its constructors target only nonempty contexts; it has one constructor which references the 'newest' variable; the other constructor embeds the 'older' variables. You may also recognize this family as an inductive definition of context membership. Being a variable means being a member of the context. Fin just gives a data representation for variables without their meaning. Now we can replace our awkward svar with i : SVar Γ τ svar i : STm Γ τ A renaming from Γ to ∆ becomes an element of ∀τ : SType ⇒ SVar Γ τ → SVar ∆ τ Bad design makes for hard work, whether you're making can-openers, doing mathematics or writing programs. It's often tempting to imagine that once we've made our representation of data tight enough to rule out meaningless values, our job is done and things should just work out. This experience teaches us that more is required-we should use types to give meaningful values their meaning. Fin contains the right data, but SVar actually explains it.</p><p>Exercise 20 Construct simultaneous renaming and simultaneous substitution for this revised definition of STm. Just lift the pattern from the untyped version!</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Is Looking Seeing?</head><p>It's one thing to define data structures which enforce invariants and to write programs which respect invariants, but how can we establish invariants?</p><p>We've seen how to use a finite set to index a vector, enforcing the appropriate bounds, but what if we only have a number, sent to us from the outside world? We've seen how to write down the STms, but what if we've read in a program from a file? How do we compute its type-safe representation if it has one?</p><p>If we want to index a Vec n X by m : Nat, it's no good testing the Boolean m &lt; n. The value true or false won't explain whether m can be represented by some i : Fin n. If we have a f : STm Γ (sFun σ τ ) and some a : STm Γ α, we could check Boolean equality σ == α, but true doesn't make α into σ, so we can't construct sapp f a.</p><p>Similar issues show up in the 'Scrap Your Boilerplate' library of dynamically typed traversal operators by Ralf Lämmel and Simon Peyton Jones <ref type="bibr" target="#b17">[18]</ref>. The whole thing rests on a 'type safe cast' operator, comparing types at run time: This program does not, of itself, make sense. The best we can say is that we can make sense of it, provided typeOf has been correctly implemented. The machine looks at the types but does not see when they are the same, hence the unsafeCoerce. The significance of the test is obscure, so blind obedience is necessary. Of course, I trust them, but I think they could aspire for better.</p><p>The trouble is that representing the result of a computation is not enough: you need to know the meaning of the computation if you want to justify its consequences. A Boolean is a bit uninformative. To see when we look, we need a new way of looking. Take the vector indexing example. We can explain which number is represented by a given i : Fin n by forgetting its bound: In one case, we get a bounded i , so we can apply bounds-safe projection. In the other, we clearly fail. Moreover, if the return type were to depend on m, that's fine: not only do we see what m must be, Epigram sees it too! But checkBound has quite a complicated higher-order type. Do I really expect you to dump good old m &lt; n for some bizarre functional? Of course I don't: I'll now explain the straightforward first-order way to construct checkBound.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">A Funny Way of Seeing Things</head><p>Constructor case analysis is the normal way of seeing things. Suppose I have a funny way of seeing things. We know that ⇐ doesn't care-a 'way of seeing things' is expressed by a type and interpreted as a way of decomposing a programming problem into zero or more subproblems. But how do I establish that my funny way of seeing at things makes sense?</p><p>Given n, m : Nat, we want to see m as either (fFin i ) for some i : Fin n, or else some (plus n m ). We can write a predicate which characterizes the n and m for which this is possible-it's possible for the very patterns we want.  and that's exactly the type of checkBoundnm. This construction on a predicate is sufficiently useful that Epigram gives it a special name, (view bc). That's to say, the machine-generated eliminator which just looks at BoundCheck's indices in terms of its constructors. Logically, view gives a datatype family its relation induction principle. But to use this 'view', we need bc : BoundCheck n m. That is, we must show that every n and m are checkable in this way:</p><formula xml:id="formula_27">let boundCheck n m : BoundCheck n m boundCheck n m ⇐ rec n { boundCheck n m ⇐ case n { boundCheck zero m ⇒ outOfBound m boundCheck (suc n) m ⇐ case m { boundCheck (suc n) zero ⇒ inBound fz boundCheck (suc n) (suc m) ⇐ view (boundCheck n m) { boundCheck (suc n) (suc (fFin i)) ⇒ inBound (fs i ) boundCheck (suc n) (suc (plus n m )) ⇒ outOfBound m }}}}</formula><p>There's no trouble using the view we're trying to establish: the recursive call is structural, but used in an eliminator rather than a return value. This function works much the way subtraction works. The only difference is that it has a type which establishes a connection between the output to the function and its inputs, shown directly in the patterns! We may now take The difference between Epigram views and Phil Wadler's views <ref type="bibr" target="#b37">[38]</ref> is that Epigram views cannot lie. Epigram views talk directly about the values being inspected in terms of the forgetful operations which generate them. Wadler's views ascribe that informative significance to an independent value, whether or not it's justified. We shouldn't criticize Typechecking is a form of looking. It relies on two auxiliary forms of lookinglooking up a variable in the context, and checking that two types are the same. Our svar constructor takes context-references expressed in terms of SVar, and our sapp constructor really needs the domain of function to be the same as the type of the argument, so just looking is not enough. Let's see.</p><p>An SVar is a context-reference; a Fin is merely a context-pointer. We can clearly turn a reference into a pointer by forgetting what's referred to:</p><formula xml:id="formula_28">let Γ : SCtxt n ; i : SVar Γ τ fV τ i : Fin n ; fV τ i ⇐ rec i { fV τ i ⇐ case i { fV τ vz ⇒ fz fV τ (vs i ) ⇒ fs (fV τ i ) }}</formula><p>Why is τ an explicit argument? Well, the point of writing this forgetful map is to define a notion of pattern for finite sets which characterizes projection. We need to see the information which the pattern throws away. Let's establish the view-it's just a more informative vproj, telling us not only the projected thing, but that it is indeed the projection we wanted. data Γ : SCtxt n ; i : Fin n Find Γ i : where i :</p><formula xml:id="formula_29">SVar Γ τ found τ i : Find Γ (fV τ i ) let find Γ i : Find Γ i find Γ i ⇐ rec Γ { find Γ i ⇐ case i { find Γ fz ⇐ case Γ { find (bind Γ σ) fz ⇒ found σ vz } find Γ (fs i ) ⇐ case Γ { find (bind Γ σ) (fs i) ⇐ view (find Γ i ) { find (bind Γ σ) (fs (fV τ i )) ⇒ found τ (vs i ) }}}}</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Term and Terror</head><p>We can follow the same recipe for typechecking as we did for context lookup. Help me fill in the details: Next, let's start on the proof of checkability-sorry, the typechecker:</p><formula xml:id="formula_30">let check Γ r : Check Γ r check Γ r ⇐ rec r { check Γ r ⇐ case r { check Γ (rvar i ) ⇐ view (find Γ i ) { check Γ (rvar (fV τ i )) ⇒ good (svar i ) } check Γ (rapp f s) ⇐ view (check Γ f ) { check Γ (rapp (fTm φ f ) s) ⇐ case φ { check Γ (rapp (fTm sNat f ) s) ⇒ bad [ ] check Γ (rapp (fTm (sFun σ τ ) f ) s) ⇐ view (check Γ s) { check Γ (rapp (fTm (sFun σ τ ) f ) (fTm α s)) [ ] check Γ (rapp (fTm (sFun σ τ ) f ) (fTError e)) ⇒ bad [ ] }} check Γ (rapp (fTError e) s) ⇒ bad [ ] } check Γ (rlda σ t) ⇐ view (check (bind Γ σ) t) { check Γ (rlda σ (fTm τ t)) ⇒ good (slda t) check Γ (rlda σ (fTError e)) ⇒ bad [ ] }}}</formula><p>The story so far: we used find to check variables; we used check recursively to check the body of an rlda and packed up the successful outcome. Note that we don't need to write the types of the good terms-they're implicit in STm.</p><p>We also got some way with application: checking the function; checking that the function inhabits a function space; checking the argument. The only trouble is that our function expects a σ and we've got an α. We need to see if they're the same: that's the missing exercise. You'll need to define a representation of STypes which differ from a given σ and a forgetful map fDiff which forgets this difference.</p><p>How to go about it? Wait and see. Let's go back to application. . .</p><formula xml:id="formula_31">check Γ (rapp (fTm (sFun σ τ ) f ) (fTm α s)) ⇐ view (compare σ α) { check Γ (rapp (fTm (sFun σ τ ) f ) (fTm σ s)) ⇒ good (sapp f s) check Γ (rapp (fTm (sFun σ τ ) f ) (fTm (fDiff σ σ ) s)) ⇒ bad [ ] }</formula><p>If we use your compare view, we can see directly that the types match in one case and mismatch in the other. For the former, we can now return a well typed application. The latter is definitely wrong.</p><p>We've done all the good cases, and we're left with choosing inhabitants of TError Γ for the bad cases. There's no reason why you shouldn't define TError Γ to make this as easy as possible. Just pack up the information which is lying around! For the case we've just seen, you could have:</p><formula xml:id="formula_32">13 σ : Diff σ ; f : STm Γ (sFun σ τ ) ; s : STm Γ (fDiff σ σ ) mismatchError σ f s : TError Γ fTError (mismatchError σ f s) ⇒ rapp (fTm ? f ) (fTm ? s)</formula><p>This recipe gives one constructor for each bad case, and you don't have any choice about its declaration. There are two basic type errors-the above mismatch and the application of a non-function. The remaining three bad cases just propagate failure outwards: you get a type of located errors.</p><p>Of course, you'll need to develop comparable first. To define Diff, just play the same type-of-diagnostics game. Develop the equality test, much as you would with the Boolean version, but using the view recursively in order to see when the sources and targets of two sFuns are the same. If you need a hint, see <ref type="bibr" target="#b26">[27]</ref>.</p><p>What have we achieved? We've written a typechecker which not only returns some well typed term or error message, but, specifically, the well typed term or error message which corresponds to its input by fTm or fTError. That correspondance is directly expressed by a very high level derived form of pattern matching: not rvar, rapp or rlda, but 'well typed' or 'ill typed'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">A Typesafe and Total Interpreter</head><p>Once you have a well typed term, you can extract some operational benefit from its well-typedness-you can execute it without run-time checks. This example was inspired by Lennart Augustsson and Magnus Carlsson's interpreter for terms with a typing proof <ref type="bibr" target="#b4">[5]</ref>. Epigram's inductive families allow us a more direct approach: we just write down a denotational semantics for well typed terms. Firstly, we must interpret SType: </p><formula xml:id="formula_33">let τ : SType Value τ : ; Value τ ⇐ rec τ { Value τ ⇐ case τ { Value sNat ⇒ Nat Value (sFun σ τ ) ⇒ Value σ →</formula><formula xml:id="formula_34">τ ; evar γ i ⇐ rec i { evar γ i ⇐ case i { evar γ vz ⇐ case γ { evar (ebind γ v ) vz ⇒ v } evar γ (vs i ) ⇐ case γ { evar (ebind γ v ) (vs i ) ⇒ evar γ i }}}</formula><p>Finally, interpret the well typed terms:</p><formula xml:id="formula_35">let γ : Env Γ ; t : STm Γ τ eval γ t : Value τ eval γ t ⇐ rec t { eval γ t ⇐ case t { eval γ (svar i ) ⇒ evar γ i eval γ (sapp f s) ⇒ eval γ f (eval γ s) eval γ (slda t) ⇒ λv ⇒ eval (ebind γ v ) t }}</formula><p>Exercise 25 Make an environment whose entries are the constructors for Nat, together with some kind of iterator. Add two and two.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Epilogue</head><p>Well, we've learned to add two and two. It's true that Epigram is currently little more than a toy, but must it necessarily remain so? There is much work to do. I hope I have shown that precise data structures can manipulated successfully and in a highly articulate manner. You don't have to be content with giving orders to the computer and keeping your ideas to yourself. What has become practical is a notion of program as effective explanation, rather than merely an effective procedure. Upon what does this practicality depend?</p><p>-adapting the programming language to suit dependent types Our conventional programming constructs are not well-suited either to cope with or to capitalize on the richness of dependent data structures. We have had to face up to the fact that inspecting one value can tell us more about types and about other values. And so it should: at long last, testing makes a difference! Moreover, the ability of types to talk about values gives us ready access to a new, more articulate way of programming with the high-level structure of values expressed directly as patterns. -using type information earlier in the programming process With so much structure-and computation-at the type level, keeping yourself type correct is inevitably more difficult. But it isn't necessary! Machines can check types and run programs, so use them! Interactive programming shortens the feedback loop, and it makes types a positive input to the programming process, not just a means to police its output. -changing the programs we choose to write</p><p>We shouldn't expect dependently typed programming merely to extend the functional canon with new programs which could not be typed before. In order to exploit the power of dependent types to express and enforce stronger invariants, we need a new style of programming which explicitly establishes those invariants. We need to rework old programs, replacing uninformative types with informative ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Related Work</head><p>Epigram's elder siblings are DML <ref type="bibr" target="#b38">[39]</ref> and Cayenne <ref type="bibr" target="#b3">[4]</ref>. DML equips ML programs with types refined by linear integer constraints and equips the typechecker with a constraint-solver. Correspondingly, many basic invariants, especially those involving sizes and ranges, can be statically enforced-this significantly reduces the overhead of run time checking <ref type="bibr" target="#b39">[40]</ref>. Epigram has no specialist constraintsolver for arithmetic, although such a thing is a possible and useful extension.</p><p>Epigram's strength is in the diversity of its type-level language. Cayenne is much more ambitious than DML and a lot closer to Epigram. It's notorious for its looping typechecker, although (contrary to popular misconception) this is not an inevitable consequence of mixing dependent types with general recursion-recursion is implemented via fixpoints, so even structurally recursive programs can loop-you can always expand a fixpoint.</p><p>Cayenne's main drawback is that it doesn't support the kind of inductive families which Epigram inherited from the Alf system <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b12">13]</ref>. It rules out those in which constructors only target parts of a family, the way vnil makes empty vectors and vcons makes nonempty vectors. This also rules out SVar, STm and all of our 'views'. All of these examples can be given a cumbersome encoding if you are willing to work hard enough: I for one am not.</p><p>The Agda proof assistant <ref type="bibr" target="#b11">[12]</ref>, like Epigram, is very much in the spirit of Alf, but it currently imposes the same restrictions on inductive definitions as Cayenne and hence would struggle to support the programs in these notesthis unfortunate situation is unlikely to continue. Meanwhile Coq <ref type="bibr" target="#b10">[11]</ref> certainly accepts the inductive definitions in this paper: it just has no practical support for programming with them-there is no good reason for this to remain so.</p><p>In fact, the closest programming language to Epigram at time of writing is Haskell, with ghc's new 'generalised algebraic data types' <ref type="bibr" target="#b32">[33]</ref>. These turn out to be, more or less, inductive families! Of course, in order to preserve the rigid separation of static types and dynamic terms, GADTs must be indexed by type expressions. It becomes quite easy to express examples like the typesafe interpreter, which exploit the invariants enforced by indexing. What is still far from obvious is how to establish invariants for run time data, as we did in our typechecker-this requires precisely the transfer of information from the dynamic to the static which is still excluded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">What is to be done?</head><p>We have only the very basic apparatus of dependently typed programming in place at the moment. We certainly need some way to analyse the results of intermediate computations in a way which reflects their significance for the existing type and value information-I have studiously avoided this issue in these notes. In <ref type="bibr" target="#b26">[27]</ref>, we propose a construct which adds the result of an intermediate computation to the collection of values being scrutinized on the left-hand side, at the same time abstracting it from types. This is not yet implemented.</p><p>We shall certainly need coinductive data in order to develop interactive systems. Inspired by the success of monads in Haskell, we shall also need to investigate the enhanced potential for 'categorical packaging' of programming in a language where the notion of category can be made abstract. And of course, there are all the 'modern conveniences': infix operators, ad-hoc polymorphism, generics, and so forth. These require design effort: the underlying expressivity is available, but we need good choices for their high-level presentation.</p><p>Work has already begun on a compiler for Epigram <ref type="bibr" target="#b9">[10]</ref>: we have barely started to exploit our new wealth of static information for performance. We have the benefit of a large total fragment, in which evaluation strategy is unimportant and program transformation is no longer troubled by ⊥. The fact that partial evaluation is already a fact of life for us must surely help also.</p><p>We need a library, but it's not enough to import the standard presentation of standard functionality. Our library must support the idioms of dependently typed programming, which may well be different. Standardizing too early might be a mistake: we need to explore the design space for standard equipment.</p><p>But the greatest potential for change is in the tools of program development. Here, we have barely started. Refinement-style editing is great when you have a plan, but often we don't. We need to develop refactoring technology for Epigram, so that we can sharpen our definitions as we learn from experiments. It's seldom straight away that we happen upon exactly the indexed data structure we need.</p><p>Moreover, we need editing facilities that reflect the idioms of programming. Many data structures have a rationale behind them-they are intended to relate to other data structures in particular ways and support particular operations. At the moment we write none of this down. The well typed terms are supposed to be a more carefully indexed version of the raw terms-we should have been able to construct them explicitly as such. If only we could express our design principles then we could follow them deliberately. Currently, we engineer coincidences, dreaming up datatypes and operations as if from thin air.</p><p>But isn't this just wishful thinking? I claim not. Dependent types, seen through the Curry-Howard lens, can characterize types and programs in a way which editing technology can exploit. We've already seen one class of logical principle reified as a programming operation-the ⇐ construct. We've been applying reasoning to the construction of programs on paper for years. We now have what we need to do the same effectively on a computer: a high-level programming language in which reasons and programs not merely coexist but coincide.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>ms) I've made primRec's type parameter explicit in a comment so we can follow what happens as we put it to work. How might we write plus? Try applying primRec to the first argument, then checking what's left to do: plus :: Nat -&gt; Nat -&gt; Nat plus = \ x -&gt; primRec{ -Nat -&gt; Nat -} x mz ms where mz :: Nat -&gt; Nat --fill this in ms :: Nat -&gt; (Nat -&gt; Nat) -&gt; Nat -&gt; Nat --fill this in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>n : Nat return plus x y n : plus x y : Nat c : plus x y : Nat call plus x y c : Nat such that call plus x y (return plus x y n) ; n Given ♦plus, we may readily extract plus-apply and run! plus ; λx , y ⇒ call plus x y (♦plus x y) : Nat → Nat → Nat Now, let's build ♦plus as a proof by NatInd: ♦plus ; NatInd (λx ⇒ ∀y : Nat ⇒ plus x y : Nat ) mz ms where mz : ∀y : Nat ⇒ plus zero y : Nat ms : ∀x : Nat ⇒ (∀y : Nat ⇒ plus x y : Nat ) → ∀y : Nat ⇒ plus (suc x ) y : Nat It's not hard to see how to generate the left-hand sides of the subproblems for each case-just read them off from their types! Likewise, it's not hard to see how to translate the right-hand sides we supplied into the proofs-pack them up with return • • • translate recursive calls via call • • • : mz ; λy ⇒ return plus zero y y ms ; λx ⇒ λxhyp ⇒ λy ⇒ return plus (suc x ) y (suc (call plus x y (xhyp y)))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>data X : List X : where nil : List X ; x : X ; xs : List X cons x xs : List X We also need the binary trees with N -labelled nodes and L-labelled leaves. data N , L : Tree N L : where l : L leaf l : Tree N L ; n : N ; s, t : Tree N L node n s t : Tree N L 2.6 Exercises: Structural Merge-Sort To get used to the system, and to programming with structural recursion, try these exercises, which only involve the simple types above Exercise 1 (le) Define the 'less-or-equal' test: let x , y : Nat le x y : Bool Does it matter which argument you do rec on? Exercise 2 (cond) Define the conditional expression: let b : Bool ; then, else : T cond b then else : T Exercise 3 (merge) Use the above to define the function which merges two lists, presumed already sorted into increasing order, into one sorted list containing the elements from both. let xs, ys : List Nat merge xs ys : List Nat Is this function structurally recursive on just one of its arguments? Nested recs combine lexicographically.Exercise 4 (flatten) Use merge to implement a function flattening a tree which may have numbers at the leaves, to produce a sorted list of those numbers. Ignore the node labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>let t :</head><label>:</label><figDesc>Tree N (Maybe Nat) flatten t : List Nat We can have a structurally recursive O(n log n) sorting algorithm if we can share out the elements of a list into a balanced tree, then flatten it.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>let ys : Vec (suc m) Y vhead ys : Y ; vhead ys [ &lt;= case ys ] What happens when we elaborate? Well, consider which constructors can possibly have made ys. Certainly not vnil, unless zero = suc n. We just get a vcons case-the one case we want, for 'head' and 'tail': let ys : Vec (suc m) Y vhead ys : Y ; vhead ys ⇐ case ys { vhead (vcons y ys) ⇒ y } let ys : Vec (suc m) Y vtail ys : Vec m Y ; vtail ys ⇐ case ys { vtail (vcons y ys) ⇒ ys }</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>let xs :</head><label></label><figDesc>Vec m X ; ys : Vec n X vappend m xs ys : Vec (plus m n) X vappend m xs ys ⇐ rec xs { vappend m xs ys ⇐ case xs { vappend zero vnil ys ⇒ ys vappend (suc m) (vcons x xs) ys ⇒ vcons x (vappend m xs ys) }} I've overridden the length arguments just to show what's happening-you can leave them implicit if you like.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>♦</head><label></label><figDesc>fsub ; λ∆ ⇒ e P m 1 . . . m n ∆ refl . . . refl : ∀∆ ⇒ f p : T ♦vtail ; λm; Y ; ys ⇒ (case ys) P m 1 m 2 m Y ys refl refl refl : ∀m : Nat; Y : ; ys : Vec (suc m) Y ⇒ vtail m Y ys : Vec m Y But what are the methods? We must find, for each i</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>let fs :</head><label></label><figDesc>Vec n (S → T ) ; ss : Vec n S va fs ss : Vec n T va fs ss ⇐ rec fs { va fs ss ⇐ case fs { va vnil ss ⇐ case ss { va vnil vnil ⇒ vnil } va (vcons f fs) ss ⇐ case ss { va (vcons f fs) (vcons s ss) ⇒ vcons (f s) (va fs ss) }}}</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>let n : Nat ; f : Fin n → X vtab n f : Vec n X Note that vtab and vproj offer alternative definitions of matrix operations. Exercise 15 (OPF, opf ) Devise an inductive family, OPF m n which gives a unique first-order representation of exactly the order-preserving functions in Fin m → Fin n. Give your family a semantics by implementing let f : OPF m n ; i : Fin m opf f i : Fin n Exercise 16 (iOPF, cOPF) Implement identity and composition: let iOPF n : OPF n n let f OPF m n ; g OPF l m cOPF f g : OPF l n Which laws should relate iOPF, cOPF and opf ?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>data</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>10</head><label>10</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>let ρ :</head><label>ρ</label><figDesc>Fin m → Fin n ; i : Fin (suc m) wren ρ i : Fin (suc n) wren ρ i ⇐ case i { wren ρ fz ⇒ fz wren ρ (fs i) ⇒ fs (ρ i ) } You get to finish the development. Exercise 17 (ren) Use wren to help you implement the renaming traversal let ρ : Fin m → Fin n ; t : Tm m ren ρ t : Tm n Now repeat the pattern for substitutions-functions from variables to terms. Exercise 18 (wsub, sub) Develop weakening for substitutions, then use it to go under lda in the traversal: let σ : Fin m → Tm n ; i : Fin (suc m) wsub σ i : Tm (suc n) let σ : Fin m → Tm n ; t : Tm m sub σ t : Tm n Exercise 19 (For the brave.) Refactor this development, abstracting the weakeningthen-traversal pattern. If you need a hint, see chapter 7 of [23]. 4.2 Stop the World I Want to Get Off ! (a first try at typed syntax)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>SType sFun σ τ : STypeWe could use Vec for contexts, but I prefer contexts which grow on the right. data n : Nat SCtxt n : where empty : SCtxt zero Γ : SCtxt n ; σ : SType bind Γ σ : SCtxt (suc n) Now, assuming we have a projection function sproj, defined in terms of SCtxt and Fin the way we defined vproj, we can just turn the inference rules of the typing relation into constructors: data Γ : SCtxt n ; τ : SType STm Γ τ :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>data Γ :</head><label>:</label><figDesc>SCtxt n ; τ : SType SVar Γ τ : where vz : SVar (bind Γ σ) σ ; i : SVar Γ τ vs i : SVar (bind Γ σ) τ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>cast :: (Typeable a, Typeable b) =&gt; a -&gt; Maybe b cast x = r where r = if typeOf x == typeOf (get r) then Just (unsafeCoerce x) else Nothing get :: Maybe a -&gt; a get x = undefined</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>let i :</head><label>:</label><figDesc>Fin n fFin i : Nat ; fFin i ⇐ rec i { fFin i ⇐ case i { fFin fz ⇒ zero fFin (fs i ) ⇒ suc (fFin i) }} Now, for a given n and m, m is either -(fFin i ) for some i : Fin n, or -(plus n m ) for some m : Nat Our types can talk about values-we can say that! checkBound n m : ∀P: Nat → Nat → ⇒ (∀n : Nat; i : Fin n ⇒ P n (fFin i )) → (∀n, m : Nat ⇒ P n (plus n m )) → P n mThat's to say: 'whatever P you want to do with n and m, it's enough to explain P for n and (fFin i ) and also for n and (plus n m )'. Or 'you can match n, m against the patterns, n, (fFin i) and n, (plus n m )'. I designed the above type to look like a case principle, so that I can program with it. Note that I don't just get an element either of (Fin i ) or of Nat from an anonymous informant; it really is my very own n and m which get analysed-the type says so! If I have checkBound, then I can check m like this: let xs : Vec n X ; m : Nat mayProj n xs m : Maybe X mayProj n xs m ⇐ checkBound n m { mayProj n xs (fFin i ) ⇒ just (vproj xs i ) mayProj n xs (plus n m ) ⇒ nothing }</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>12</head><label>12</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>Fin n inBound i : BoundCheck n (fFin i ) m : Nat outOfBound m : BoundCheck n (plus n m ) A value bc : BoundCheck n m tells us something about n and m, and it's just n and m that we care about here-bc is just a means to this end. The eliminator (case bc) expects a motive abstracting over n, m and bc, allowing us to inspect bc also. If we restrict the motive to see only n and m, we get λP : Nat → Nat → ⇒ (case bc) (λ n ; m ; bc ⇒ P n m ) : ∀P : Nat → Nat → ⇒ (∀n : Nat; i : Fin n ⇒ P n (fFin i )) → (∀n, m : Nat ⇒ P n (plus n m )) → P n m</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head></head><label></label><figDesc>checkBound n m ; view (boundCheck n m) 4.6 Patterns Forget; Matching Is Remembering What has 'pattern matching' become? In general, a pattern is a forgetful operation. Constructors like zero and suc forget themselves-you can't tell from the type Nat, which constructor you've got. Case analysis remembers what constructors forget. And so it is with our funny patterns: the fFin function forgets bounded whilst (plus n m ) forgets by how much its output exceeds n. Our view remembers what these patterns forget.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Exercise 23</head><label>23</label><figDesc>Develop an equality view for SType: data σ, τ : SType Compare σ τ : where same : Compare τ τ ; σ : Diff σ diff σ : Compare σ (fDiff σ σ ) let compare σ τ : Compare σ τ :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>block of source code. white (light green when selected) indicates successful elaboration yellow indicates that Epigram cannot yet see why a piece of code is good brown indicates that Epigram can see why a piece of code is bad</head><label></label><figDesc>An Epigram line may thus occupy more than one ascii line. If a bracket is opened on a physical line, it must either be closed on that line or suspended with a !, then resumed on the next physical line with another !. I hasten to add that the Epigram editor does all of this box-drawing for you. You can fit two Epigram lines onto one physical line by separating them with ;, and split one Epigram line into two physical lines by prefixing the second with %.The Epigram document is a syntax tree in which leaves may be sheds-their contents are monochrome and belong to you. You can edit any shed without Epigram spying on you, but the rest of the document gets elaborated -managed, typechecked, translated to UTT, typeset, coloured in and generally abused by the system. In particular, Epigram colours recognized identifiers. There is only one namespace-this colour is just for show. If you can't see the colours in your copy of these notes, don't worry: I've adopted font and case conventions instead.</figDesc><table><row><cell cols="3">Blue sans serif, uppercase initial type constructor</cell></row><row><cell>red</cell><cell cols="2">sans serif, lowercase initial data constructor</cell></row><row><cell>green</cell><cell>serif, boldface</cell><cell>defined variable</cell></row><row><cell>purple</cell><cell>serif, italic</cell><cell>abstracted variable</cell></row><row><cell>black</cell><cell>serif, underlined</cell><cell>reserved word</cell></row><row><cell cols="3">These conventions began in my handwritten slides-the colour choices are</cell></row><row><cell cols="3">more or less an accident of the pens available, but they seem to have stuck.</cell></row><row><cell cols="3">Epigram also has a convention for background colour, indicating the elaboration</cell></row><row><cell>status of a</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>data Nat : where zero : Nat ; suc : Nat → Nat In Haskell, we would write 'data Nat = Zero | Suc Nat'.</head><label></label><figDesc></figDesc><table><row><cell cols="2">1.6 Acknowledgements</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">I'd like to thank the editors, Tarmo Uustalu and Varmo Vene, and the anonymous</cell></row><row><cell cols="6">referees, for their patience and guidance. I'd also like to thank my colleagues</cell></row><row><cell cols="6">and friends, especially James McKinna, Thorsten Altenkirch, Zhaohui Luo, Paul</cell></row><row><cell cols="6">Callaghan, Randy Pollack, Peter Hancock, Edwin Brady, James Chapman and</cell></row><row><cell cols="6">Peter Morris. Sebastian Hanowski and Wouter Swierstra deserve special credit</cell></row><row><cell cols="6">for the feedback they have given me. Finally, to those who were there in Tartu,</cell></row><row><cell cols="6">thank you for making the experience one I shall always value.</cell></row><row><cell cols="6">This work was supported by EPSRC grants GR/R72259 and EP/C512022.</cell></row><row><cell cols="3">2 Warm Up; Add Up</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">Let's examine the new technology in the context of a simple and familiar prob-</cell></row><row><cell cols="6">lem: adding natural numbers. We have seen this Epigram definition: 4</cell></row><row><cell>data</cell><cell>Nat :</cell><cell>where</cell><cell>zero : Nat</cell><cell>;</cell><cell>n : Nat suc n : Nat</cell></row><row><cell cols="6">We can equally well define the natural numbers in Epigram as follows:</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Exercise 5 (insert) Implement the insertion of a number into a tree: let n : Nat ; t : Tree Bool (Maybe Nat) insert n t : Tree Bool (Maybe Nat) Maintain this balancing invariant throughout: in (node true s t), s and t contain equally many numbers, whilst in (node false s t), s contains exactly one more number than t. List Nat so that sort sorts its input in O(n log n) time.</figDesc><table><row><cell cols="2">Exercise 6 (share, sort) Implement</cell></row><row><cell>let</cell><cell>ns : List Nat share ns : Tree Bool (Maybe Nat)</cell></row><row><cell>let</cell><cell>ns : List Nat sort ns :</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Fin (suc n) impossible fs n j : Fin (suc n) impossible So the finished product is just magic i ⇐ case i The idea is that Fin n is an enumeration type containing n values. Let's tabulate the first few members of the family, just to see what's going on. (I'll show the implicit arguments as subscripts, but write in decimal to save space.) fs 1 fz 0 fs 2 fz 1 fs 3 fz 2 . . . Fin n, using fs n , and adding a 'new' element fz n . Fin n provides a representation of numbers bounded by n, which can be used as 'array subscripts': Fin zero, as a harmless fiction. Of course, we could have analysed the arguments the other way around: vproj xs i ⇐ rec xs { vproj xs i ⇐ case i { vproj xs fz ⇐ case xs { vproj (vcons x xs) fz ⇒ x } vproj xs (fs i ) ⇐ case xs { vproj (vcons x xs) (fs i ) ⇒ vproj xs i }}}</figDesc><table><row><cell>data</cell><cell>n : Nat Fin n :</cell><cell>where</cell><cell cols="3">fz : Fin (suc n)</cell><cell>;</cell><cell>i : Fin n fs i : Fin (suc n)</cell></row><row><cell cols="5">What happens when we elaborate this?</cell><cell></cell></row><row><cell></cell><cell>let</cell><cell cols="2">i : Fin zero magic i : X</cell><cell cols="3">; magic i [ &lt;= case i ]</cell></row><row><cell cols="6">You've probably guessed, but let's just check:</cell></row><row><cell></cell><cell></cell><cell cols="3">i : Fin zero</cell><cell>unifier</cell></row><row><cell></cell><cell cols="4">fz n : Fin 0 Fin 1 Fin 2 Fin 3</cell><cell>Fin 4</cell><cell>• • •</cell></row><row><cell></cell><cell></cell><cell>fz 0 fz 1</cell><cell>fz 2</cell><cell></cell><cell>fz 3</cell><cell>• • •</cell></row></table><note><p><p><p>fs 2 (fs 1 fz 0 ) fs 3 (fs 2 fz 1 ) . . .</p>fs</p>3 (fs 2 (fs 1 fz 0 )) . . . . . . Fin zero is empty, and each Fin (suc n) is made by embedding the n 'old' elements of let xs : Vec n X ; i : Fin n vproj xs i : X ; vproj xs i ⇐ rec xs { vproj xs i ⇐ case xs { vproj vnil i ⇐ case i vproj (vcons x xs) i ⇐ case i { vproj (vcons x xs) fz ⇒ x vproj (vcons x xs) (fs i ) ⇒ vproj xs i }}} We need not fear projection from vnil, for we can dismiss i :</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>We call P the motive-it says what we gain from the elimination. In particular, we'll have a proof of P for the t. The m i are the methods by which the motive is to be achieved for each s i . James McKinna taught me to choose this motive:</figDesc><table><row><cell>P ; λΘ ⇒</cell></row><row><cell>∀∆ ⇒ Θ= t →</cell></row><row><cell>f p : T</cell></row></table><note><p>8 let Γ f Γ : R let ys : Vec (suc m) Y vtail ys : Vec m Y Epigram initiates the development of a proof ♦f : ∀Γ ⇒ f Γ : R ♦vtail : ∀m : Nat; Y : ; ys : Vec (suc m) Y ⇒ vtail m Y ys : Vec m Y The general form of a subproblem in this development is ♦fsub : ∀∆ ⇒ f p : T ♦vtail : ∀m : Nat; Y : ; ys : Vec (suc m) Y ⇒ vtail m Y ys : Vec m Y where p are patterns-expressions over the variables in ∆. In the example, I've chosen the initial patterns given by vtails formal parameters. Note that patterns in Epigram are not a special subclass of expression. Now let's proceed f p ⇐ e vtail ys ⇐ case ys e : ∀ P : ∀Θ ⇒ m 1 : ∀∆ 1 ⇒ P s 1 . . . m n : ∀∆ n ⇒ P s n ⇒ P t case ys : ∀ P : ∀n; X ; xs : Vec n X ⇒ m 1 : ∀ X : ⇒ P zero X vnil m 2 : ∀ X : ; n : Nat x : X ; xs : Vec n X ⇒ P (suc n) X (vcons x xs) ⇒ P (suc m) Y ys P ; λn; X ; xs ⇒ ∀m : Nat; Y : ; ys :</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>-are derivable in UTT. Each rule</figDesc><table><row><cell>deletion</cell><cell>P →</cell></row><row><cell></cell><cell>x =x → P</cell></row><row><cell>conflict</cell><cell>chalk s=cheese t → P</cell></row></table><note><p>injectivity ( s= t → P) → chalk s=chalk t → P substitution P t → x =t → P x x , t : T ; x ∈ FV (t ) cycle x =t → P x constructor-guarded in t Fig. 1. derivable unification rule schemes</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>'re not used to thinking about what functions really tell us, because simple types don't say much about values, statically. For example, we could write let n : Nat nonzero n : Bool ; nonzero n ⇐ case n { nonzero zero ⇒ false nonzero (suc n) ⇒ true } but suppose we have xs : Vec n X -what do we learn by testing nonzero n? All we get is a Bool, with no direct implications for our understanding of n or xs. We are in no better position to apply vtail to xs after inspecting this Bool than before. Instead, if we do case analysis on n, we learn what n is statically as well as dynamically, and in the suc case we can apply vtail to xs.</figDesc><table><row><cell cols="2">Of course, we could think of writing a preVtail function which operates on</cell></row><row><cell cols="2">any vector but requires a precondition, like</cell></row><row><cell></cell><cell></cell></row><row><cell>let</cell><cell> </cell></row></table><note><p>xs : Vec n X ; q : nonzero n=true preVtail xs q : Vec [ ] X    I'm not quite sure what to write as the length of the returned vector, so I've left a shed: perhaps it needs some kind of predecessor function with a precondition.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>vec vnil transpose (vcons xj xij ) ⇒ va (va (vec vcons) xj ) (transpose xij ) }} 3.5 Exercises: Matrix Manipulation Exercise 7 (vmap, vZipWith) Show how vec and va can be used to generate the vector analogues of Haskell's map, zipWith, and the rest of the family. (A glance at [16] may help.) Exercise 8 (vdot) Implement vdot, the scalar product of two vectors of Nats. Matrix m n as a linear map Vec n Nat → Vec m Nat.)</figDesc><table><row><cell cols="3">Now, how about matrices? Recall the vector-of-columns representation:</cell></row><row><cell>let</cell><cell>rows, cols : Nat Matrix rows cols :</cell><cell>Matrix rows cols ⇒ Vec cols (Vec rows Nat)</cell></row><row><cell cols="3">Exercise 9 (zero, identity) How would you compute the zero matrix of a</cell></row><row><cell cols="3">given size? Also implement a function to compute any identity matrix.</cell></row><row><cell cols="3">Exercise 10 (matrix by vector) Implement matrix-times-vector multiplica-</cell></row><row><cell cols="3">tion. (ie, interpret a Exercise 11 (matrix by matrix) Implement matrix-times-matrix multiplica-</cell></row><row><cell cols="3">tion. (ie, implement composition of linear maps.)</cell></row></table><note><p>Exercise 12 (monad) (Mainly for Haskellers.) It turns out that for each n, Vec n is a monad, with vec playing the part of return. What should the corresponding notion of join do? What plays the part of ap? 3.6 Exercises: Finite Sets Exercise 13 (fmax, fweak) Implement fmax (each nonempty set's maximum value) and fweak (the function preserving fz and fs, incrementing the index). let fmax n : Fin (suc n) let i : Fin n fweak i : Fin (suc n)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Wadler for this-dependent types can see where simple types can only look. Of course, to work with dependent types, we need to be able to see. If we want to generate values in types which enforce strong invariants, we need to see that those invariants hold. Exercise 21 Show that fmax and fweak cover Fin by constructing a view. Let's have a larger example of derivable pattern matching-building simplytyped terms in the STm family by typechecking 'raw' untyped terms from</figDesc><table><row><cell>data</cell><cell>n : Nat RTm n :</cell><cell></cell><cell></cell></row><row><cell>where</cell><cell>i : Fin n rvar i : RTm n</cell><cell>;</cell><cell>f , s : RTm n rapp f s : RTm n</cell></row><row><cell></cell><cell cols="3">σ : SType ; b : RTm (suc n)</cell></row><row><cell></cell><cell cols="3">rlda σ b : RTm n</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Exercise 24 (TError, fTError) Fill out the definition of TError and implement fTError. (This will be easy, once you've done the missing exercise. The TErrors will jump out as we write the typechecker-they pack up the failure cases.)Let's start on the typechecking view. First, the checkability relation:</figDesc><table><row><cell cols="3">Exercise 22 (fTm) Implement the forgetful map:</cell></row><row><cell>let</cell><cell cols="2">Γ SCtxt n ; t : STm Γ τ fTm τ t : RTm n</cell></row><row><cell cols="3">But not every raw term is the forgetful image of a well typed term. We'll need</cell></row><row><cell>data</cell><cell>Γ : SCtxt n TError Γ :</cell><cell>where • • •</cell></row><row><cell>let</cell><cell cols="2">Γ : SCtxt n ; e : TError Γ fTError e : RTm n</cell></row></table><note><p>data Γ : SCtxt n : r : RTm n Check Γ r : where t : STm Γ τ good t : Check Γ (fTm τ t) ; e : TError Γ bad e : Check Γ (fTError e)</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>As I'm fond of pointing out, these papers were published almost simultaneously and have only one reference in common. I find that shocking!</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>Unary numbers are not an essential design feature; rest assured that primitive binary numbers will be provided eventually<ref type="bibr" target="#b9">[10]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>Isn't this a big step backwards for brevity? Or does Haskell's brevity depend on some implicit presumptions about what datatypes can possibly be?</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3"><p>If I had chosen a first-order recursion, there would have been as few as four, but that would presume to fix the second argument through the course of the recursion.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_4"><p>Following a suggestion by Thierry Coquand, Eduardo Giménez shows how to separate induction into case and rec in<ref type="bibr" target="#b16">[17]</ref>. He presents memo-structures inductively, to justify the syntactic check employed by Coq's Fix construct. The computational version is mine<ref type="bibr" target="#b22">[23]</ref>; its unfolding memo-structures give you a menu of recursive calls.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_5"><p>I write Greek capitals for sequences of variables with type assignments in binders and also for their unannotated counterparts as argument sequences.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_6"><p>Categorists! Note wren makes suc a functor in the category of Fin-functions. What other structure can you sniff out here?</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_7"><p>I forgot that I'm programming: of course, I mean 'datatype family'.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_8"><p>The ? means 'please infer'-it's often useful when writing forgetful maps. Why?</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A predicative analysis of structural recursion</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Abel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Altenkirch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Functional Programming</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Monadic presentations of lambda-terms using generalized inductive types</title>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Altenkirch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Reus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Science Logic 1999</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Functional Programming Languages and Computer Architecture</title>
		<author>
			<persName><forename type="first">Lennart</forename><surname>Augustsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">LNCS</title>
		<editor>
			<persName><forename type="first">Jean-Pierre</forename><surname>Jouannaud</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">201</biblScope>
			<biblScope unit="page" from="368" to="381" />
			<date type="published" when="1985">1985</date>
			<publisher>Springer-Verlag</publisher>
		</imprint>
	</monogr>
	<note>Compiling Pattern Matching</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cayenne-a language with dependent types</title>
		<author>
			<persName><forename type="first">Lennart</forename><surname>Augustsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Functional Programming &apos;98</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">An exercise in dependent types: A well</title>
		<author>
			<persName><forename type="first">Lennart</forename><surname>Augustsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Magnus</forename><surname>Carlsson</surname></persName>
		</author>
		<ptr target="http://www.cs.chalmers.se/~augustss/cayenne/interp.ps" />
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Substitution: A formal methods case study using monads and transformations</title>
		<author>
			<persName><forename type="first">Françoise</forename><surname>Bellegarde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science of Computer Programming</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Interactive Theorem Proving and Program Development, Coq&apos;Art: The Calculus of Inductive Constructions</title>
		<author>
			<persName><forename type="first">Yves</forename><surname>Bertot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Castéran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Texts in Theoretical Computer Science</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Springer-Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Richard</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oege</forename><surname>De Moor</surname></persName>
		</author>
		<title level="m">Algebra of Programming</title>
		<imprint>
			<publisher>Prentice Hall</publisher>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">de Bruijn notation as a nested datatype</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Paterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Functional Programming</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="77" to="92" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Practical Implementation of a Dependently Typed Functional Programming Language</title>
		<author>
			<persName><forename type="first">Edwin</forename><surname>Brady</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>University of Durham</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">L' Équipe</forename><surname>Coq</surname></persName>
		</author>
		<ptr target="http://pauillac.inria.fr/coq/doc/main.html" />
		<title level="m">The Coq Proof Assistant Reference Manual</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Structured Type Theory</title>
		<author>
			<persName><forename type="first">Catarina</forename><surname>Coquand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Coquand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Logical Frameworks and Metalanguages</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pattern Matching with Dependent Types</title>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Coquand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Electronic Proceedings of the Third Annual BRA Workshop on Logical Frameworks</title>
		<editor>
			<persName><forename type="first">Bengt</forename><surname>Nordström</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kent</forename><surname>Petersson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Gordon</forename><surname>Plotkin</surname></persName>
		</editor>
		<meeting><address><addrLine>Båstad, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Lambda Calculus notation with nameless dummies: a tool for automatic formula manipulation</title>
		<author>
			<persName><forename type="first">Nicolas</forename><forename type="middle">G</forename><surname>De Bruijn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Indagationes Mathematicae</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="381" to="392" />
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Inductive Sets and Families in Martin-Löf&apos;s Type Theory</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Dybjer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Logical Frameworks. CUP</title>
		<editor>
			<persName><forename type="first">Gérard</forename><surname>Huet</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Gordon</forename><surname>Plotkin</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Do we need dependent types</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Fridlender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mia</forename><surname>Indrika</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Functional Programming</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="409" to="415" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Codifying guarded definitions with recursive schemes</title>
		<author>
			<persName><forename type="first">Eduardo</forename><surname>Giménez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Types for Proofs and Programs</title>
		<editor>
			<persName><forename type="first">Peter</forename><surname>Dybjer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Bengt</forename><surname>Nordström</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jan</forename><surname>Smith</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="39" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Scrap your boilerplate: a practical design pattern for generic programming</title>
		<author>
			<persName><forename type="first">Ralf</forename><surname>Lämmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><forename type="middle">Peyton</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the ACM SIGPLAN Workshop on Types in Language Design and Implementation</title>
		<meeting>of the ACM SIGPLAN Workshop on Types in Language Design and Implementation</meeting>
		<imprint>
			<publisher>TLDI</publisher>
			<date type="published" when="2003-03">March 2003. 2003</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="26" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Computation and Reasoning: A Type Theory for Computer Science</title>
		<author>
			<persName><forename type="first">Zhaohui</forename><surname>Luo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">LEGO Proof Development System: User&apos;s Manual</title>
		<author>
			<persName><forename type="first">Zhaohui</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Pollack</surname></persName>
		</author>
		<idno>ECS-LFCS-92-211</idno>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
		<respStmt>
			<orgName>Laboratory for Foundations of Computer Science, University of Edinburgh</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The ALF proof editor and its proof engine</title>
		<author>
			<persName><forename type="first">Lena</forename><surname>Magnusson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bengt</forename><surname>Nordström</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Selected papers from the Int. Workshop TYPES &apos;93</title>
		<title level="s">LNCS</title>
		<editor>
			<persName><forename type="first">Henk</forename><surname>Barendregt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tobias</forename><surname>Nipkow</surname></persName>
		</editor>
		<meeting><address><addrLine>Nijmegen</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1993">1994. May 1993</date>
			<biblScope unit="volume">806</biblScope>
		</imprint>
	</monogr>
	<note>Types for Proofs and Programs</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A theory of types</title>
		<author>
			<persName><forename type="first">Per</forename><surname>Martin-Löf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
	<note>manuscript</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Dependently Typed Functional Programs and their Proofs</title>
		<author>
			<persName><forename type="first">Conor</forename><surname>Mcbride</surname></persName>
		</author>
		<ptr target="http://www.lfcs.informatics.ed.ac.uk/reports/00/ECS-LFCS-00-419/" />
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
		<respStmt>
			<orgName>University of Edinburgh</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Faking It (Simulating Dependent Types in Haskell)</title>
		<author>
			<persName><forename type="first">Conor</forename><surname>Mcbride</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Functional Programming</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4&amp; 5</biblScope>
			<biblScope unit="page" from="375" to="392" />
			<date type="published" when="2002">2002</date>
			<publisher>Special Issue on Haskell</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">First-Order Unification by Structural Recursion</title>
		<author>
			<persName><forename type="first">Conor</forename><surname>Mcbride</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Functional Programming</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Conor</forename><surname>Mcbride</surname></persName>
		</author>
		<author>
			<persName><surname>Epigram</surname></persName>
		</author>
		<ptr target="http://www.dur.ac.uk/CARG/epigram" />
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The view from the left</title>
		<author>
			<persName><forename type="first">Conor</forename><surname>Mcbride</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Mckinna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Functional Programming</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Computer Aided Manipulation of Symbols</title>
		<author>
			<persName><forename type="first">Fred</forename><surname>Mcbride</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1970">1970</date>
		</imprint>
		<respStmt>
			<orgName>Queen&apos;s University of Belfast</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Robin</forename><surname>Milner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mads</forename><surname>Tofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Macqueen</surname></persName>
		</author>
		<title level="m">The Definition of Standard ML</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
	<note>revised edition</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Purely Functional Data Structures</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Okasaki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">From Fast Exponentiation to Square Matrices: An Adventure in Types</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Okasaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Functional Programming &apos;99</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<ptr target="http://www.haskell.org/definition" />
		<title level="m">Haskell&apos;98: A Non-Strict Functional Language</title>
		<editor>
			<persName><forename type="first">Simon</forename><surname>Peyton</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jones</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">John</forename><surname>Hughes</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Wobbly types: type inference for generalised algebraic data types</title>
		<author>
			<persName><forename type="first">Peyton</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Washburn</surname></persName>
		</author>
		<author>
			<persName><surname>Weirich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>Unpublished</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Natural Deduction-A proof theoretical study. Almquist and Wiksell</title>
		<author>
			<persName><forename type="first">Dag</forename><surname>Prawitz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1965">1965</date>
			<pubPlace>Stockholm</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Programming in Martin-Löf&apos;s type theory: an introduction</title>
		<author>
			<persName><forename type="first">Bengt</forename><surname>Nordström</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kent</forename><surname>Petersson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A Machine-oriented Logic Based on the Resolution Principle</title>
		<author>
			<persName><forename type="first">Alan</forename><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="23" to="41" />
			<date type="published" when="1965">1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Elementary Strong Functional Programming</title>
		<author>
			<persName><forename type="first">David</forename><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Functional Programming Languages in Education, First International Symposium</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">1022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Views: A way for pattern matching to cohabit with data abstraction</title>
		<author>
			<persName><forename type="first">Philip</forename><surname>Wadler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of POPL &apos;87</title>
		<meeting>POPL &apos;87</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Dependent Types in Practical Programming</title>
		<author>
			<persName><forename type="first">Hongwei</forename><surname>Xi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
		<respStmt>
			<orgName>Department of Mathematical Sciences, Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Eliminating array bound checking through dependent types</title>
		<author>
			<persName><forename type="first">Hongwei</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Pfenning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Programming Language Design and Implementation (PLDI&apos;98)</title>
		<meeting>the Conference on Programming Language Design and Implementation (PLDI&apos;98)</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
