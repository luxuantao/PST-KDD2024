<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Plannable Representations with Causal InfoGAN</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Thanard</forename><surname>Kurutach</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Aviv</forename><surname>Tamar</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ge</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Stuart</forename><surname>Russell</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
						</author>
						<title level="a" type="main">Learning Plannable Representations with Causal InfoGAN</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">AD760A0FCD0817F37F13ECE19C3B6C1A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent years, deep generative models have been shown to 'imagine' convincing high-dimensional observations such as images, audio, and even video, learning directly from raw data. In this work, we ask how to imagine goal-directed visual plans -a plausible sequence of observations that transition a dynamical system from its current configuration to a desired goal state, which can later be used as a reference trajectory for control. We focus on systems with high-dimensional observations, such as images, and propose an approach that naturally combines representation learning and planning. Our framework learns a generative model of sequential observations, where the generative process is induced by a transition in a low-dimensional planning model, and an additional noise. By maximizing the mutual information between the generated observations and the transition in the planning model, we obtain a low-dimensional representation that best explains the causal nature of the data. We structure the planning model to be compatible with efficient planning algorithms, and we propose several such models based on either discrete or continuous states. Finally, to generate a visual plan, we project the current and goal observations onto their respective states in the planning model, plan a trajectory, and then use the generative model to transform the trajectory to a sequence of observations. We demonstrate our method on imagining plausible visual plans of rope manipulation 3 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>For future robots to perform general tasks in unstructured environments such as homes or hospitals, they must be able to reason about their domain and plan their actions accordingly. In AI literature, this general problem has been investigated under two main paradigms -automated planning and scheduling <ref type="bibr" target="#b33">[34]</ref> (henceforth, AI planning) and reinforcement learning <ref type="bibr" target="#b40">[41]</ref> (RL).</p><p>Classical work in AI planning has drawn on the remarkable capability of humans to perform long-term reasoning and planning by using abstract representations of the world. For example, humans might think of "cup on table" as a state rather than detailed coordinates or a precise image of such a scene. Interestingly, powerful classical planners exist that can reason very effectively with these kinds of representations, as demonstrated by results in the International Planning Competition <ref type="bibr" target="#b43">[44]</ref>. However, such logical representations of the world can be difficult to specify correctly. As an example, consider designing a logical representation for the state of a deformable object such as a rope. Moreover, logical representations that are not grounded a priori in real-world observation require a perception module that can identify, for example, exactly when the cup is considered "on the table".</p><p>In RL, on the other hand, a task is solved directly through trial and error, guided by a manually provided reward signal. Recent advances in model-free RL (e.g., <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b22">23]</ref>) have shown remarkable success in learning policies that act directly on high-dimensional observations, such as raw images. Designing a reward function that depends on such observations can be challenging, however, and most recent studies either relied on domains where the reward can be instrumented <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b32">33]</ref>, or required successful demonstrations as guidance <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b38">39]</ref>. Moreover, since RL is guided by the reward to solve a particular task, it does not automatically generalize to different tasks <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b17">18]</ref>.</p><p>In principle, model-based RL can solve the generalization problem by learning a dynamics model and planning with that model. However, applying model-based RL to domains with high-dimensional observations has been challenging <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b11">12]</ref>. Deep learning approaches to learning dynamics models (e.g., action-conditional video prediction models <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b11">12]</ref>) tend to get bogged down in pixel-level detail, tend to be computationally expensive, and are far from accurate over longer time scales. Moreover, the representations learned using such approaches are typically unstructured, high-dimensional continuous vectors, which cannot be used in efficient planning algorithms.</p><p>In this work, we aim to combine the merits of deep learning dynamics models and classical AI planning, and propose a framework for long-term reasoning and planning that is grounded in real-world perception. We present Causal InfoGAN (CIGAN), a method for learning plannable representations of dynamical systems with high-dimensional observations such as images. By plannable, we mean representations that are structured in such a way that makes them amenable for efficient search, through AI planning tools. In particular, we focus on discrete and deterministic dynamics models, which can be used with graph search methods, and on continuous models where planning is done by linear interpolation, though our framework can be generalized to other model types.</p><p>In our framework, a generative adversarial net (GAN; <ref type="bibr" target="#b14">[15]</ref>) is trained to generate sequential observation pairs from the dynamical system. The GAN generator takes as input both unstructured random noise and a structured pair of consecutive states from a low-dimensional, parametrized dynamical system termed the planning model. The planning model is meant to capture the features that are most essential for representing the causal properties in the data, and are therefore important for planning future outcomes. To learn such a model, we follow the InfoGAN idea <ref type="bibr" target="#b4">[5]</ref>, and add to the GAN training loss a term that maximizes the mutual information between the observation pairs and the transitions that induced them.</p><p>The CIGAN model can be trained using random exploration data from the system. After learning, given an observation of an initial configuration and a goal configuration, it can generate a "walkthrough" sequence of feasible observations that lead from the initial state to the goal. This walkthrough can be later used as a reference signal for a controller to execute the task in the real system. We demonstrate convincing walkthrough generation on synthetic tasks and real image data collected by Nair et al. <ref type="bibr" target="#b28">[29]</ref> of a robot randomly poking a rope.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries and Problem Formulation</head><p>Let H denote the entropy of a random variable, and I denote the mutual information between two random variables <ref type="bibr" target="#b7">[8]</ref>.</p><p>GAN and InfoGAN: Deep generative models aim to generate samples from the real distribution of observations, P data . In this work we build on the GAN framework <ref type="bibr" target="#b14">[15]</ref>, which is composed of a generator, G(z) = o, mapping a noise input z ∼ P noise (z) to an observation o, and a discriminator, D(o), mapping an observation to the probability that it was sampled from the real data. The GAN training optimizes a game between the generator and discriminator, min</p><formula xml:id="formula_0">G max D V (G, D) = min G max D E o∼Pdata [log D(o)] + E z∼Pnoise [log (1 -D(G(z)))] .</formula><p>One can view the noise vector z in the GAN as a representation of the observation o. In GAN training, however, there is no incentive for this representation to display any structure at all, making it difficult to interpret, or use in a downstream task. The InfoGAN method <ref type="bibr" target="#b4">[5]</ref> aims to mitigate this issue. The idea in InfoGAN is to add to the generator input an additional 'state'<ref type="foot" target="#foot_3">4</ref> component s ∼ P (s), and add to the GAN objective a loss that induces maximal mutual information between the generated observation and the state. The InfoGAN objective is given by: min</p><formula xml:id="formula_1">G max D V (G, D) -λI (s; G(z, s)) ,<label>(1)</label></formula><p>where λ &gt; 0 is a weight parameter, and V (G, D) is the GAN loss above. Intuitively, this objective induces the state to capture the most salient properties of the observation. Optimizing the objective in (1) directly is difficult without access to the posterior distribution P (s|o), and a variational lower bound was proposed in <ref type="bibr" target="#b4">[5]</ref>. Define an auxiliary distribution Q(s|o) to approximate the posterior P (s|o). Then, I (s; G(z, s)) ≥ E s∼P (s),o∼G(z,s) [log Q(s|o)] + H(s). Using this bound, the InfoGAN objective (1) can be optimized using stochastic gradient descent.</p><p>Problem Formulation: We consider a fully observable and deterministic dynamical system, o t+1 = f (o t , u t ), where o t and u t denote the observation and action at time t, respectively. The function f is assumed to be unknown. We are provided with data D in the form of N trajectories of observations o i 1 , u i 1 . . . , o i Ti i∈1,...,N , generated from f , where the actions are generated by an arbitrary exploration policy. <ref type="foot" target="#foot_4">5</ref>We say that two observations o, o are h-reachable if there exists a sequence of actions that takes the system from o to o within h steps or less. We consider the problem of generating a walkthrough -a sequence of reachable observations along a feasible path between the start and the goal: Problem 1 Walkthrough Planning: Given D, h, and two observations o start , o goal , generate a sequence of observations o start , . . . , o goal such that every two consecutive observations in the sequence are h-reachable. If such a sequence does not exist, return ∅.</p><p>The motivation to solve problem 1 is that it breaks the long horizon planning problem (from o start to o goal ) into a sequence of short h-horizon planning problems which can be later solved effectively using other methods such as inverse dynamics or model-free RL <ref type="bibr" target="#b28">[29]</ref>. This concept of temporal abstraction has been fundamental in AI planning (e.g., <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b41">42]</ref>). Since we are searching for a sequence of way point observations, the actions are not relevant for our problem, and in the sequel we omit them from the discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Causal InfoGAN</head><p>A natural approach for solving the walkthrough planning problem in Section 2 is learning some model of the dynamics f from the data, and searching for a plan within that model. This leads to a trade-off. On the one hand, we want to be expressive, and learn all the transitions possible from every o within a horizon h. When o is a high dimensional image observation, this typically requires mapping the image to an extensive feature space <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b11">12]</ref>. On the other hand, however, we want to plan efficiently, which generally requires either low dimensional state spaces or well-structured representations. We approach this challenge by proposing Causal InfoGAN -an expressive generative model with a structured representation that is compatible with planning algorithms. In this section we present the Causal InfoGAN generative model, and in Section 4 we explain how to use the model for planning.</p><p>Let o and o denote a pair of sequential observations from the dynamical system f , and let P data (o, o ) denote their probability, as displayed in the data D. We posit that a generative model that can accurately learn P data (o, o ) has to capture the features that are important for representing the causality in the data -what next observations o are reachable from the current observation o. Naturally, such features would be useful later for planning.</p><p>We build on the GAN framework <ref type="bibr" target="#b14">[15]</ref>. Applied to our setting, a vanilla GAN would be composed of a generator, o, o = G(z), mapping a noise input z ∼ P noise (z) to an observation pair, and a discriminator, D(o, o ), mapping an observation pair to the probability that it was sampled from the real data D and not from the generator. One can view the noise vector z in such a GAN as a feature vector, containing some representation of the transition to o from o. The problem, however, is that the structure of this representation is not necessarily easy to decode and use for planning. Therefore, we propose to design a generator with a structured input that can be later used for planning. In particular, we propose a GAN generator that is driven by states sampled from a parametrized dynamical system.</p><p>Let M denote a dynamical system with state space S, which we term the set of abstract-states, and a parametrized, stochastic transition function T M (s |s): s ∼ T M (s |s), where s, s ∈ S are a pair of consecutive abstract states. We denote by P M (s) the prior probability of an abstract state s. We emphasize that the abstract state space S can be different from the space of real observations o. For reasons that will become clear later on, we term M as the latent planning system.</p><p>We propose to structure the generator as taking in a pair of consecutive abstract states s, s in addition to the noise vector z. The GAN objective in this case is therefore (cf. Section 2):</p><formula xml:id="formula_2">V (G, D) = E o,o ∼Pdata [log D(o, o )] + E z∼Pnoise,s∼P M (s),s ∼T M (s) [log (1 -D(G(z, s, s )))] . (2)</formula><p>The idea is that s and s would represent the abstract features that are important for understanding the causality in the data, while z would model variations that are less informative, such as pixel level details. To learn such representations, we follow InfoGAN <ref type="bibr" target="#b4">[5]</ref>, and add to the GAN objective a term that maximizes mutual information between the generated pair of observations and the abstract states.</p><p>We propose the Causal InfoGAN objective:</p><formula xml:id="formula_3">min M,G max D V (G, D) -λI (s, s ; o, o ) , s.t. o, o ∼ G(z, s, s ); s ∼ P M ; s ∼ T M (s), (3)</formula><p>where λ &gt; 0 is a weight parameter, and V (G, D) is given in <ref type="bibr" target="#b1">(2)</ref>. Intuitively, this objective induces the abstract model to capture the most salient possible changes that can be effected on the observation.</p><p>Optimizing the objective in (3) directly is difficult, since we do not have access to the posterior distribution, P (s, s |o, o ), when using an expressive generator function. Following InfoGAN <ref type="bibr" target="#b4">[5]</ref>, we optimize a variational lower bound of (3). Define an auxiliary distribution Q(s, s |o, o ) to approximate the posterior P (s, s |o, o ). We have, following a similar derivation to <ref type="bibr" target="#b4">[5]</ref>:</p><formula xml:id="formula_4">I ((s, s ); G(z, s, s )) ≥ E s∼P M ,s ∼T M (s), o,o ∼G(z,s,s ) [log Q(s, s |o, o )] + H(s, s ) . = I V LB (G, Q). (<label>4</label></formula><formula xml:id="formula_5">)</formula><p>To encourage the same mapping between s, o and s , o , we propose the disentangled posterior approximation,</p><formula xml:id="formula_6">Q(s, s |o, o ) = Q(s|o)Q(s |o ) (see Appendix B.)</formula><p>We plug the lower bound (4) in Eq. ( <ref type="formula">3</ref>) to obtain the following loss function:</p><formula xml:id="formula_7">min G,Q,M max D V (G, D) -λI VLB (G, Q),<label>(5)</label></formula><p>where λ &gt; 0 is a constant. The loss in ( <ref type="formula" target="#formula_7">5</ref>) can be optimized effectively using stochastic gradient descent, and we provide a detailed algorithm in Appendix C.  First, an abstract state s is sampled from a prior P M (s). Given s, the next state s is sampled using the transition model T M (s |s). The states s, s are fed, together with a random noise sample z, into the generator which outputs o, o . The discriminator D maps an observation pair to the probability of the pair being real. Finally, the approximate posterior Q maps from each observation to the distribution of the state it associates with. The causal InfoGAN loss function in Equation ( <ref type="formula" target="#formula_7">5</ref>) encourages Q to predict each state accurately from each observation. (b) Planning paradigm (cf. Section 4). Given start and goal observations, we first map them to abstract states, and then we apply planning algorithms using the model M to search for a path from s start to s goal . Finally, from the plan in abstract states, we generate back a sequence of observations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Planning with Causal InfoGAN models</head><p>In this section, we discuss how to use the Causal InfoGAN model for planning goal directed trajectories. We first present our general methodology, and then propose several model configurations for which (5) can be optimized efficiently, and the latent planning system is compatible with efficient planning algorithms. We then describe how to combine these ideas for solving the walkthrough planning problem in various domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">General Planning Paradigm</head><p>Our general paradigm for goal directed planning is described in Figure <ref type="figure" target="#fig_1">1b</ref>. We start by training a Causal InfoGAN model from the data, as described in the previous section. Then, we perform the following 3 steps, which are detailed in the rest of this section:</p><p>1. Given a pair of observations o start , o goal , we first encode them into a pair of corresponding states s start , s goal . This is described in Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Type</head><p>Values s Prior P M (s)</p><formula xml:id="formula_8">Transition T M (s |s) Planning algorithms Discrete -one-hot [N ] U{1, . . . , N } s ∼ Sof tmax(s θ) Dijkstra Discrete -binary {0, 1} N U{0, 1} N See eq. 6 Dijkstra Continuous R N U(-1, 1) N s ∼ N (s, Σ θ (s)) Linear interpolation</formula><p>Table <ref type="table">1</ref>: Various latent planning systems. In all cases, N is the state dimension. The parameters θ of the transition T M depending on the state type. In the one-hot case, θ is a matrix in R N ×N . In the binary case, θ denotes parameters in a stochastic neural network; see Eq. ( <ref type="formula" target="#formula_9">6</ref>). In the continuous case θ represents the parameters of a neural network that controls the variance of the transition.</p><p>2. Using the transition probabilities in the planning model M, we plan a feasible state trajectory from s start to s goal : s start , s 1 , . . . , s m , s goal . This is described in Section 4.3. 3. Finally, we decode the state trajectory into a corresponding trajectory of observations o start , o 1 , . . . , o m , o goal . This is described in Section 4.4. The specific method for each step in the planning paradigm can depend on the problem at hand. For example, some systems are naturally described by discrete abstract states, while others are better described by continuous states. In the following, we describe several models and methods that worked well for us, under the general planning paradigm described above. This list is by no means exhaustive. On the contrary, we believe that the Causal InfoGAN framework provides a basis for further investigation of deep generative models that are compatible with planning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Encoding an Observation to a State</head><p>For mapping an observation to a state, we can simply use the disentangled posterior Q(s|o). We found this approach to work well in low-dimensional observation spaces. However, for highdimensional image observations we found that the learned Q(s|o) was accurate in classifying generated observations (by the generator), but inaccurate for classifying real observations. This is explained by the fact that in Causal InfoGAN, Q is only trained on generated observations, and can overfit to generated images. For images, we therefore opted for a different approach. Following <ref type="bibr" target="#b44">[45]</ref>, we performed a search over the latent space to find the best latent state mapping s * (o): s * (o) = arg min s min s ,z o-G(s, s , z) 2 . Another approach, which could scale to complex image observations, is to add to the GAN training an explicit encoding network <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b48">49]</ref>. In our experiments, the simple search approach worked well and we did not require additional modifications to the GAN training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Latent Planning Systems</head><p>We now present several latent planning systems that are compatible with efficient planning algorithms. Table <ref type="table">1</ref> summarizes the different models. In all cases, optimizing the model parameters with respect to the expectation in the loss (4) is done using the reparametrization trick (following <ref type="bibr" target="#b19">[20]</ref> for continuous states, and <ref type="bibr" target="#b15">[16]</ref> for discrete states).</p><p>Discrete Abstract States -One-Hot Representation. We start from a simple abstract state representation, in which each s ∈ S is represented as a N -dimensional one-hot vector. We denote by θ ∈ R N ×N the model parameters, and compute transition probabilities as: T M (s |s) = Sof tmax(s θ). Discrete Abstract States -Binary Representation. We present a more expressive abstract state representation using binary states. Binary state representations are common in AI planning, where each binary element is known as a predicate, and corresponds to a particular object property being true or false <ref type="bibr" target="#b33">[34]</ref>. Using Causal InfoGAN, we learn the predicates directly from data.</p><p>We propose a parametric transition model that is suitable for binary representations. Let s ∈ {0, 1} N be an N -dimensional binary vector, drawn from P M (s). We generate the next state s by first drawing a random action vector a ∈ {0, 1} M with some probability P M (a) <ref type="foot" target="#foot_5">6</ref> . Let l i denote a feed-forward neural network with sigmoid output parametrized by θ that maps the state s and action a to the Bernoulli's parameter of s i |s, a. Thus, the probability of the next state s is finally given by:</p><formula xml:id="formula_9">TM(s = v|s) = Ea i TM(s i = vi|s, a) = Ea i li(s, a) v i (1 -li(s, a)) (1-v i ) .<label>(6)</label></formula><p>We emphasize that there is not necessarily any correspondence between the action vector a and the real actions that generated the observation pairs in the data. The action a is simply a means to induce stochasticity to the state transition network.</p><p>For planning with discrete models, we interpret the stochastic transition model T M as providing the possible state transitions, i.e., for every s such that T M (s |s) &gt; there exists a possible transition from s to s . For planning, we require abstract state representations that are compatible with efficient AI planning algorithms. The one-hot and binary representations above can be directly plugged in to graph-planning algorithms such as Dijkstra's shortest-path algorithm <ref type="bibr" target="#b33">[34]</ref>.</p><p>Continuous Abstract States. For some domains, such as the rope manipulation in our experiments, a continuous abstract state is more suitable. We consider a model where an s ∈ S is an N -dimensional continuous vector. Planning in high-dimensional continuous domains, however, is hard in general.</p><p>Here, we propose a simple and effective solution: we will learn a latent planning system such that linear interpolation between states makes for feasible plans. To bring about such a model, we consider transition probabilities T M (s |s) given as Gaussian perturbations of the state: s = s + δ, where δ ∼ N (0, Σ θ (s)) and Σ θ is a diagonal covariance matrix, represented by a MLP with parameters θ. The key idea here is that, if only small local transitions are possible in the system, then a linear interpolation between two states s start , s goal has a high probability, and therefore represents a feasible trajectory in the observation space. To encourage such small transitions, we add an L2 norm of the convariance matrix to the loss function ( <ref type="formula" target="#formula_7">5</ref>):</p><formula xml:id="formula_10">L cont (M) = E s∼P M Σ θ (s) 2 .</formula><p>The prior probability P M for each element of s is uniform in [-1, 1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Decoding a State Trajectory to an Observation Walkthrough Trajectory</head><p>We now discuss how to generate a feasible sequence of observations from the planned state trajectory.</p><p>Here, as before, we separate the discussion for systems with low-dimensional observations and systems with image observations, as we found that different practices work best for each.</p><p>For low-dimensional observations, we structure the GAN generator G to have an observationconditional form: For high-dimensional image observations, the sequential generator does not work well, since small errors in the image generation tend to get accumulated when fed back into the generator. We therefore follow a different approach. To generate the i'th observation in the trajectory o i , we use the generator with the input s i , s i+1 , and a noise z that is fixed throughout the whole trajectory. The generator actually outputs a pair of sequential images, but we discard the second image in the pair.</p><formula xml:id="formula_11">o = G 1 (z, s, s ), o = G 2 (z,</formula><p>To further improve the planning result we generate K random trajectories with different random noise z, and select the best trajectory by using a discriminator D to provide a confidence score for each trajectory. In the low-dimensional case, we use the GAN discriminator. In the high-dimensional case, however, we find that the discriminator tends to overfit to the generator. Therefore, we trained an auxiliary discriminator for novelty detection, as described in the Experiment Section 6.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Combining deep generative models with structured dynamical systems has been explored in the context of variational autoencoders (VAEs), where the latent space was continuous <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b16">17]</ref>. Watter et al. <ref type="bibr" target="#b45">[46]</ref> used such models for planning, by learning latent linear dynamics, and using a linear quadratic Gaussian control algorithm. Disentangled video prediction <ref type="bibr" target="#b8">[9]</ref> separates object content and position, but has not been used for planning. Very recently, Corneil et al. <ref type="bibr" target="#b6">[7]</ref> suggested Variational State Tabulation (VaST) -a VAE-based approach for learning latent dynamics over binary state representations, and planning in the latent space using prioritized sweeping to speed up RL. Causal InfoGAN shares several similarities with VaST, such as using Gumbel-Softmax to backprop through transitions of discrete binary states, and leveraging the structure of the binary states for planning. However, VaST is formulated to require the agent actions, and is thus limited to single time step predictions. More generally, our work is developed under the GAN formulation, which, to date, has several benefits over VAEs such as superior quality of image generation <ref type="bibr" target="#b18">[19]</ref>. Causal InfoGAN can also be used with continuous abstract states.</p><p>The semiparametric topological memory (SPTM) <ref type="bibr" target="#b36">[37]</ref> is another recent approach for solving problems such as Problem 1, by planning in a graph where every observation in the data is a node, and connectivity is decided using a learned similarity metric between pairs of observations. SPTM has shown impressive results on image-based navigation. However, Causal InfoGAN's parametric approach of learning a compact, model for planning has the potential to scale up to more complex problems, in which the increasing amount of data required would make the nonparametric SPTM approach difficult to apply.</p><p>Learning state aggregation and state representation has a long history in RL. Methods such as in <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b37">38]</ref> exploit the value function for measuring state similarity, and are therefore limited to the task defined by the reward. Methods for general state aggregation have also been proposed, based on spectral clustering <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b23">24]</ref>, and variants of K-means <ref type="bibr" target="#b2">[3]</ref>. All these approaches rely in some form on the Euclidean distance as a metric between observation features. As we show in our experiments, the Euclidean distance can be unsuitable even on low-dimensional continuous domains.</p><p>Recent work in deep RL explored learning goal-conditioned value functions and policies <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b31">32]</ref>, and policies with an explicit planning computation <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b38">39]</ref>. These approaches require a reward signal for learning (or supervision from an expert <ref type="bibr" target="#b38">[39]</ref>). In our work, we do not require a reward signal, and learn a general model of the dynamical system, which is used for goal-directed planning.</p><p>Our work is also related to learning models of intuitive physics. Previous work explored feedforward neural networks for predicting outcomes of physical experiments <ref type="bibr" target="#b21">[22]</ref>, neural networks for modelling relations between objects <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b35">36]</ref>, and prediction based on physics simulators <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b47">48]</ref>. To the best of our knowledge, these approaches cannot be used for planning. However, related ideas would likely be required for scaling our method to more complex domains, such as manipulating several objects.</p><p>In the planning literature, most studies relied on manually designed state representations. In a recent work, Konidaris et al. <ref type="bibr" target="#b20">[21]</ref> automatically extracted state representations from raw observations, but relied on a prespecified set of skills for the task. In our work, we automatically extract state representations by learning salient features that describe the causal structure of the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>In our experiments, we aim to (1) visualize the abstract states and planning in CIGAN; (2) compare CIGAN with recent state-aggregation methods in the literature; <ref type="bibr" target="#b2">(3)</ref> show that CIGAN can produce realistic visual plans in a complex dynamical system; and (4) show that CIGAN significantly outperforms baseline methods. We begin our investigation with a set of toy tasks, specifically designed to demonstrate the benefits of CIGAN, where we can also perform an extensive quantitative evaluation. We later present experiments on a real dataset of robotic rope manipulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Illustrative Experiments</head><p>In this section we evaluate CIGAN on a set of 2D navigation problems. These problems abstract away the challenges of learning visual features, allowing an informative comparison on the task of learning causal structure in data, and using it for planning. For details of the training data see Appendix ??.</p><p>Our toy domains involve a particle moving in a 2-dimensional continuous domain with impenetrable obstacles, as depicted in Figure <ref type="figure" target="#fig_2">2</ref>, and Figure <ref type="figure">5</ref>. in Appendix A.1. The observations are the (x, y) coordinates of the particle in the plane, and, in the door-key domain, also a binary indicator for holding the key. We generate data trajectories by simulating a random motion of the particle, started from random initial points. We consider the following various geometrical arrangements of the domain, chosen to demonstrate the properties of our method.</p><p>1. Tunnels: the domain is partitioned into two unconnected rooms (top/bottom), where in each room there is an obstacle, positioned such that transitioning between the left/right quadrants is through a narrow tunnel. 2. Door-key: two rooms are connected by a door. The door can be traversed only if the agent holds the key, which is obtained by moving to the red-marked area in the top right corner of the upper room. Holding the key is represented as a binary 0/1 element in the observation. 3. Rescaled door-key: Same as door key domain, but the key observation is rescaled to be a small when the agent is holding the key, and 0 otherwise.</p><p>Our domains are designed to distinguish when standard state aggregation methods, which rely on the Euclidean metric, can work well. In the tunnel domain, the Euclidean metric is not informative about the dynamics in the task -two points in different rooms inside the tunnel can be very close in Euclidean distance, but not connected, while points in the same room can be more distant but connected. In the door-key domain, the Euclidean distance is informative if observations with key and without key are very distant in Euclidean space, as in the 0/1 representation (compared to the domain size which is in [-1, 1]). In the rescaled door-key, we make the Euclidean distance less informative by changing the key observation to be 0/ .</p><p>We compare CIGAN with several recent methods for aggregating observation features into states for planning. Note that in these simple 2D domains, feature extraction is not necessary as the observations are already low dimensional vectors. The simplest baseline is K-means, which relies on the Euclidean distance between observations. In <ref type="bibr" target="#b2">[3]</ref>, a variant of K-means for temporal data was proposed, using a window of consecutive observations to measure a smoothed Euclidean distance to a cluster centroids. We refer to this method as temporal K-means. In <ref type="bibr" target="#b25">[26]</ref>, and more recently <ref type="bibr" target="#b39">[40]</ref> and <ref type="bibr" target="#b24">[25]</ref>, spectral clustering (SC) was used to cluster observations. For continuous observations, SC requires a distance function to build a connectivity graph, and previous studies <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b24">25]</ref> relied on the Euclidean distance, using either nearest neighbor to connect nodes, or exponentiated-distance weighted edges.</p><p>In Figure <ref type="figure" target="#fig_2">2</ref>, we show the CIGAN classification of observations to abstract states, Q(s|o), and compare with the K-means baseline; the other baselines gave qualitatively similar results. Note that CIGAN learned a clustering that is related to the dynamical properties of the domain, while the baselines, which rely on a Euclidean distance, learned clusters that are not informative about the real possible transitions. As a result, CIGAN clearly separates abstract states within each room, while the K-means baseline clusters observations across the wall. This demonstrates the potential of CIGAN to learn meaningful state abstractions without requiring a distance function in observation space. Similarly, the results for door-key domains are shown in Appendix A.1.</p><p>To evaluate planning performance, we hand-coded an oracle function that evaluates whether an observation trajectory is feasible or not (e.g., does not cross obstacles, correctly reports ∅ when a trajectory does not exist). For CIGAN, we ran the planning algorithm described in Section 4. For baselines, we calculated cluster transitions from the data, and generated planning trajectories in observation space by using the cluster centroids. We chose algorithm parameters and stopping criteria by measuring the average feasibility score on a validation set of start/goal observations, and report the average feasibility on a held out test set of start/goal observations. Our results in Table <ref type="table" target="#tab_1">2</ref> show that by learning more informative clusters, CIGAN resulted in significantly better planning.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Rope Manipulation</head><p>In this section we demonstrate CIGAN on the task of generating realistic walkthroughs of robotic rope manipulation. Then, we show that CIGAN generates significantly better trajectories than those generated by the state-of-the-art generative model baselines both visually and quantitatively.</p><p>The rope manipulation dataset <ref type="bibr" target="#b28">[29]</ref> contains sequential images of rope manipulated in a selfsupervised manner, by a robot randomly choosing a point on the rope and perturbing it slightly. Using these data, the task is to manipulate the rope in a goal-oriented fashion, from one configuration to another, where a goal is represented as an image of the desired rope configuration. In the original study, Nair et al. <ref type="bibr" target="#b28">[29]</ref> used the data to learn an inverse dynamics model for manipulating the rope between two images of similar rope configurations. Then, to solve long-horizon planning, Nair  et al. required a human to provide the walkthrough sequence of rope poses, and used the learned controller to execute the short-horizon transitions within the plan. In our experiment, we show that CIGAN can be used to generate walkthrough plans directly from data for long-horizon tasks, without requiring additional human guidance. We train a CIGAN model on the rope manipulation data of <ref type="bibr" target="#b28">[29]</ref>.</p><p>We pre-processed the data by removing the background, and applying a grayscale transformation. We chose the continuous abstract state representation with the linear interpolation planner as described in Section 4. Note that, as described in Section 4, the encoding, planning, and decoding methods in this case are not specific to CIGAN, and can be used with a GAN or InfoGAN generative model, allowing a fair comparison with alternative representation learning methods. In Figure <ref type="figure" target="#fig_3">3</ref>, we generate walkthroughs using CIGAN, InfoGAN, and DCGAN Noticeably, Causal InfoGAN resulted in a smooth latent space where linear interpolation indeed corresponds to plausible trajectories.</p><p>To numerically evaluate planning performance we propose a visual fidelity score, inspired by the Inception score for evaluating GANs <ref type="bibr" target="#b34">[35]</ref>, we train a binary classifier to classify whether two images are sequential in the data or not <ref type="foot" target="#foot_6">7</ref> . For an image pair, the classifier output therefore provides a score between 0 and 1 for the feasibility of the transition. We then compute the trajectory score -the average classifier score of image pairs in the trajectory. Note that this classifier is trained independent of the generative models, making for an impartial metric. For each start and goal, we pick the best trajectory score out of 400 samples of the noise variable z. <ref type="foot" target="#foot_7">8</ref> As shown in Figure <ref type="figure" target="#fig_4">4</ref>, Causal InfoGAN achieved a significantly higher trajectory score averaged over 57 task configurations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We presented Causal InfoGAN, a framework for learning deep generative models of sequential data with a structured latent space. By choosing the latent space to be compatible with efficient planning algorithms, we developed a framework capable of generating goal-directed trajectories from high-dimensional dynamical systems.</p><p>Our results for generating realistic manipulation plans of rope suggest promising applications in robotics, where designing models and controllers for manipulating deformable objects is challenging.</p><p>The binary latent models we explored provide a connection between deep representation learning and classical AI planning, where Causal InfoGAN can be seen as a method for learning object predicates directly from data. In future work we intend to investigate this direction further, and incorporate object-oriented models, which are a fundamental component in classical AI.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Causal InfoGAN model s start s 1 s 2 ... s m s goal o start o 1 o 2 ... o m o goal o</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: The Causal InfoGAN framework. (a) Generative model (cf. Section 3). First, an abstract state s is sampled from a prior P M (s). Given s, the next state s is sampled using the transition model T M (s |s). The states s, s are fed, together with a random noise sample z, into the generator which outputs o, o . The discriminator D maps an observation pair to the probability of the pair being real. Finally, the approximate posterior Q maps from each observation to the distribution of the state it associates with. The causal InfoGAN loss function in Equation (5) encourages Q to predict each state accurately from each observation. (b) Planning paradigm (cf. Section 4). Given start and goal observations, we first map them to abstract states, and then we apply planning algorithms using the model M to search for a path from s start to s goal . Finally, from the plan in abstract states, we generate back a sequence of observations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: 2D particle results on tunnel domain. (a) The domain -top/bottom rooms are not connected. Left/right quadrants are connected through a narrow tunnel. An example of several random walk trajectories are shown. (b) Clustering found by CIGAN. (c) Clustering found by K-means. (d)Example walkthrough trajectories generated by CIGAN, from a point at the top right to five other locations on the map, marked by colored circles. For trajectories that were not found only the target is shown. Note that CIGAN learned clusters that correspond to the possible dynamics of the particle in the task, and was therefore able to generate reasonable planning trajectories.</figDesc><graphic coords="8,207.98,316.77,89.10,89.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Rope walkthroughs generated from CIGAN, InfoGAN, and DCGAN. Red crosses show unfeasible one-step transitions with respect to the data. See more plans in Appendix A.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Evaluation of walkthrough planning in rope domain. We trained a classifier to predict whether two observations are sequential or not (1=sequential, 0=not sequential), and compare the average classification score for different generative models. Note that CIGAN significantly outperforms the baselines, in alignment with the qualitative results of Figure 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Planning results for 2D tasks.</figDesc><table><row><cell></cell><cell cols="3">Tunnels Door-key Rescaled</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>door-key</cell><cell>Table shows average feasibility of plans</cell></row><row><cell>CIGAN</cell><cell>98%</cell><cell>98%</cell><cell>97%</cell><cell>(higher is better) generated by the dif-</cell></row><row><cell>K-means</cell><cell>12.25%</cell><cell>100%</cell><cell>0.0%</cell><cell>ferent algorithms. Note that CIGAN</cell></row><row><cell>Temporal K-means</cell><cell>7.0%</cell><cell>100%</cell><cell>0.0%</cell><cell>significantly outperforms baselines in</cell></row><row><cell>Spectral clustering</cell><cell>8.75%</cell><cell>60%</cell><cell>20.0%</cell><cell>domains where the Euclidean distance is not informative for planning.</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Berkeley AI Research, University of California, Berkeley</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Department of Physics, University of Chicago</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Code is available online at http://github.com/thanard/causal-infogan.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>In<ref type="bibr" target="#b4">[5]</ref>, s is referred to as a code. Here we term it as a state, to correspond with our subsequent development of structured GAN input from a dynamical system.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>In this work, we do not concern the problem of how to best generate the exploration data.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>See Appendix B.2 Binary States for experimental choices.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>The positive data are the pairs of rope images that are 1 step apart and the negative data are randomly chosen pairs that are from different runs which are highly likely to be farther than 1 step apart.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7"><p>This selection process is applied the same way to the DCGAN and InfoGAN baselines.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was funded in part by ONR PECASE N000141612723 and Siemens. Thanard Kurutach and Aviv Tamar were supported by ONR PECASE N000141612723. Aviv Tamar was partially supported by the Technion Viterbi scholarship. The authors wish to thank Tom Zahavy and Aravind Srinivas for sharing their code for our comparisons with MDP state aggregation baselines, and Elinor Tamar for timely assistance in preprocessing the rope data.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to poke by poking: Experiential learning of intuitive physics</title>
		<author>
			<persName><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5074" to="5082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Hindsight experience replay</title>
		<author>
			<persName><forename type="first">M</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcgrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tobin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5048" to="5058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Baram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zahavy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
		<title level="m">Spatio-temporal abstractions in reinforcement learning through neural encoding</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Simulation as an engine of physical scene understanding</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">45</biblScope>
			<biblScope unit="page" from="18327" to="18332" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">InfoGAN: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2172" to="2180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A recurrent latent variable model for sequential data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kastner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Efficient model-based deep reinforcement learning with variational state tabulation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Corneil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gerstner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Brea</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04325</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Elements of information theory</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Thomas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised learning of disentangled representations from video</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Birodkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="4414" to="4423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.09782</idno>
		<title level="m">Adversarial feature learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning and executing generalized robot plans</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Fikes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Nilsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Readings in Artificial Intelligence</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1981">1981</date>
			<biblScope unit="page" from="231" to="249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep visual foresight for planning robot motion</title>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2786" to="2793" />
		</imprint>
	</monogr>
	<note>Robotics and Automation (ICRA)</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Guided cost learning: Deep inverse optimal control via policy optimization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="49" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep spatial autoencoders for visuomotor learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Y</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics and Automation (ICRA), 2016 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="512" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01144</idno>
		<title level="m">Categorical reparameterization with gumbel-softmax</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Composing graphical models with neural networks for structured representations and fast inference</title>
		<author>
			<persName><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wiltschko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Datta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2946" to="2954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Schema networks: Zero-shot transfer with a generative causal model of intuitive physics</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Mély</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Eldawy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lázaro-Gredilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dorfman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sidor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Phoenix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>George</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1809" to="1818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10196</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">From skills to symbols: Learning symbolic representations for abstract high-level planning</title>
		<author>
			<persName><forename type="first">G</forename><surname>Konidaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Kaelbling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lozano-Perez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="215" to="289" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning physical intuition of block towers by example</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01312</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">End-to-end training of deep visuomotor policies</title>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Machado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tesauro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Campbell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.04065</idno>
		<title level="m">The eigenoption-critic framework</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A laplacian framework for option discovery in reinforcement learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Machado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bowling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00956</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Proto-value functions: A laplacian framework for learning representation and control in markov decision processes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maggioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="2169" to="2231" />
			<date type="published" when="2007-10">Oct. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dynamic abstraction in reinforcement learning via clustering</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Menache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hoze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twenty-first international conference on Machine learning</title>
		<meeting>the twenty-first international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page">71</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Combining self-supervised learning and imitation for vision-based rope manipulation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="2146" to="2153" />
		</imprint>
	</monogr>
	<note>Robotics and Automation (ICRA)</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Action-conditional video prediction using deep networks in atari games</title>
		<author>
			<persName><forename type="first">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2863" to="2871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Value prediction network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6118" to="6128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Temporal difference models: Model-free deep RL for model-based control</title>
		<author>
			<persName><forename type="first">V</forename><surname>Pong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno>CoRR, abs/1802.09081</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Learning by playing-solving sparse reward tasks from scratch</title>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lampe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neunert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Degrave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Van De Wiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.10567</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Norvig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence -A Modern</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Pearson Education</publisher>
		</imprint>
	</monogr>
	<note>Approach (3. internat. ed.</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A simple neural network module for relational reasoning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4974" to="4983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Savinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.00653</idno>
		<title level="m">Semi-parametric topological memory for navigation</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Dynamic catalog mailing policies</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">I</forename><surname>Simester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Tsitsiklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management science</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="683" to="696" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.00645</idno>
		<title level="m">Universal planning networks</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Option discovery in hierarchical reinforcement learning using spatio-temporal clustering</title>
		<author>
			<persName><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ravindran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.05359</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT press Cambridge</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="181" to="211" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Value iteration networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tamar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2146" to="2154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">The 2014 international planning competition: Progress and trends</title>
		<author>
			<persName><forename type="first">M</forename><surname>Vallati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chrpa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grześ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Mccluskey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sanner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="90" to="98" />
			<pubPlace>Ai Magazine</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tamar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.08534</idno>
		<title level="m">Safer classification by synthesis</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Embed to control: A locally linear latent dynamics model for control from raw images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Watter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Boedecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2746" to="2754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Watters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zoran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.01433</idno>
		<title level="m">Visual interaction networks</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning to see physics via visual de-animation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="152" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Toward multimodal image-to-image translation</title>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="465" to="476" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
