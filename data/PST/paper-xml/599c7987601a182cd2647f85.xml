<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generative Adversarial Trainer: Defense to Adversarial Perturbations with GAN</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hyeungill</forename><surname>Lee</surname></persName>
							<email>hilee@wspl.snu.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of electrical and computer engineering</orgName>
								<orgName type="institution">Seoul National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sungyeob</forename><surname>Han</surname></persName>
							<email>syhan@wspl.snu.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of electrical and computer engineering</orgName>
								<orgName type="institution">Seoul National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jungwoo</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of electrical and computer engineering</orgName>
								<orgName type="institution">Seoul National University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Generative Adversarial Trainer: Defense to Adversarial Perturbations with GAN</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a novel technique to make neural network robust to adversarial examples using a generative adversarial network. We alternately train both classifier and generator networks. The generator network generates an adversarial perturbation that can easily fool the classifier network by using a gradient of each image. Simultaneously, the classifier network is trained to classify correctly both original and adversarial images generated by the generator. These procedures help the classifier network to become more robust to adversarial perturbations. Furthermore, our adversarial training framework efficiently reduces overfitting and outperforms other regularization methods such as Dropout. We applied our method to supervised learning for CIFAR datasets, and experimantal results show that our method significantly lowers the generalization error of the network. To the best of our knowledge, this is the first method which uses GAN to improve supervised learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, deep learning has advanced in all areas of artificial intelligence, including image classification and speech recognition <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10]</ref>. These advances was owing to deep neural networks which can be easily trained by backpropagation, so they can represent complex probability distributions over high dimensional data. Despite these advances, deep neural networks remain imperfect. In particular, they show weaknesses with respect to adversarial examples when compared to humans <ref type="bibr" target="#b16">[17]</ref>. Adversarial examples can effectively fool a neural network to change its predictions, and the human eye cannot distinguish such examples from original images.</p><p>Several studies have attempted to make neural networks robust to such adversarial examples <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>. Adversarial training is one of the methods that retrains a neural network to predict correct labels for adversarial examples. In adversarial training, adversarial examples are generated in the inner loop of the adversarial training algorithms. Thus, this process should be fast to help adversarial training to be practical. Goodfellow et al. <ref type="bibr" target="#b4">[5]</ref> proposed the fast gradient sign method, which is a simple and fast method of generating adversarial examples. <ref type="bibr">Goodfellow et al. [4]</ref> also introduced generative adversarial networks (GAN). GAN is a framework to train generative models, and shows a state-of-the-art performance for image generation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13]</ref>. The main idea of GAN is that two networks play a minimax game so that they converge gradually to an optimal solution.</p><p>In this paper, we propose a novel adversarial training method using a GAN framework. Similar to GAN, we alternately train both a classifier network (trainee) and a generator network (trainer). The generator network attempts to generate adversarial perturbations that can easily fool the classifier network, whereas the classifier network tries to classify correctly both original and adversarial images produced by the generator network. These procedures gradually help the classifier network to be robust to adversarial perturbations. Our experimental results show that our method outperforms other arXiv:1705.03387v1 [cs.LG] 9 May 2017 adversarial training using a fast gradient method. We also observe that generalization errors are remarkably lower than that of other regularization methods such as dropout <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Backgrounds</head><p>In this section, we briefly review the adversarial training (with fast gradient method) and GAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Adversarial Training</head><p>Goodfellow et al. <ref type="bibr" target="#b4">[5]</ref> introduced a rational explanation for the adversarial example, and proposed a fast technique to generate adversarial perturbation. The authors observed that adversarial examples exist because models are too linear. They suggested a fast gradient sign method such that</p><formula xml:id="formula_0">η = sign(∇ x J(θ, x, y)) (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where η is an adversarial perturbation, θ denotes parameters of the network, x is the input with the label y, and J(θ, x, y) denotes the cost function to train the classifier network.</p><p>Deep neural networks trained by standard supervised methods are vulnerable to adversarial examples.</p><p>Adversarial training helps neural networks to be robust to adversarial perturbation. The practical method of adversarial training is to introduce an adversarial objective function using fast gradient sign method. Let J(θ, x, y) be the loss function of adversarial training, which optimizes a neural network against an adversary.</p><formula xml:id="formula_2">J(θ, x, y) = αJ(θ, x, y) + (1 − α)J(θ, x + sign(∇ x J(θ, x, y)), y)<label>(2)</label></formula><p>Eq. ( <ref type="formula" target="#formula_2">2</ref>) consists of two cost functions. The first cost function is the original cross-entropy loss function for a neural network. The second is the loss function with adversarial perturbations, which is added to each input x. Note that α is a hyperparameter that adjusts the ratio between the two cost functions. In <ref type="bibr" target="#b13">[14]</ref>, the authors analyzed the principle of adversarial training and found a strong connection between robust optimization and regularization. This establishes a minimization-maximization approach for adversarial training, which in turn makes neural networks stable in a neighborhood around training points. The authors also generalized the fast gradient sign method and proposed another adversarial training method with L 2 norm constraint <ref type="bibr" target="#b13">[14]</ref>. </p><formula xml:id="formula_3">η = ∇ x J(θ, x, y) ∇ x J(θ, x, y) 2<label>(3)</label></formula><formula xml:id="formula_4">J(θ, x, y) = αJ(θ, x, y) + (1 − α)J(θ, x + ∇ x J(θ, x, y) ∇ x J(θ, x, y) 2 , y)<label>(4</label></formula><formula xml:id="formula_5">min G max D V (D, G) = E x∼p data (x) [log D(x)] + E z∼pz(z) [log(1 − D(G(z)))]<label>(5)</label></formula><p>The competition in this minimax game forces both models to improve their ability until the discriminator cannot distinguish a generated sample from a data sample. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head><p>In this section, we propose a novel adversarial training framework. However, we first introduce a generative adversarial trainer (GAT), which plays a major role in adversarial training. The objective of the GAT is to generate adversarial perturbations that can easily fool the classifier network using a gradient of images. A classifier is trained to classify correctly both original and adversarial images generated by the GAT. The entire procedure is shown in Fig. <ref type="figure" target="#fig_0">1</ref>. In Section 3.1, we describe our notation. Section 3.2 describes the structure of the GAT, Section 3.3 explains the adversarial training mechanism used with the GAT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Notations</head><p>We denote a labeled training set by {(</p><formula xml:id="formula_6">x (i) , y (i) )} N i=1</formula><p>, where x (i) ∈ R H×W ×C represents input images with height H, width W , and channel C, and y (i) ∈ {1, • • • , K} is a label for an input x (i) . We use two neural networks in the proposed method. One is a standard K-class classifier network F (x; θ f ) which is defined by:</p><formula xml:id="formula_7">F : R H×W ×C → R K , F (x) = [F (x) 1 , F (x) 2 , • • • , F (x) K ] T<label>(6)</label></formula><p>where F (x) represents the class probability vector computed using the softmax function. The other is a GAT G(∆; θ g ) which is defined by:</p><formula xml:id="formula_8">G : R H×W ×C → R H×W ×C<label>(7)</label></formula><p>Note that G(∆) represents the perturbation of the input image x, where ∆ = ∂F (x)y ∂x denotes the gradient of input images with respect to the class probability of the label. We use a cross entropy loss function for the classifier F (x; θ f ), which is denoted by:</p><formula xml:id="formula_9">J(θ f , x, y) = − log F (x; θ f ) y (8)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Generative Adversarial Trainer</head><p>The main idea of the GAT is to use a neural network to find the perturbation generator function specific to the classifier rather than just the sign or normalized functions used in the fast gradient method. The objective of the GAT is to find best perturbation image using the gradient of each image.</p><p>To achieve this goal, the loss function is defined as follows:</p><formula xml:id="formula_10">L G (∆, y) = F (x + G(∆)) y + c g • G(∆) 2 2<label>(9)</label></formula><p>The loss function of GAT consists of the two cost functions. One is the loss function, which is used to find perturbation images that lower the classifier's class probability. The other cost function restricts the power of the perturbation so that it is not too large. In ( <ref type="formula" target="#formula_10">9</ref>), c g is a hyperparameter that adjusts the ratio between two cost functions. If c g is too low, it will find only a trivial solution with very high perturbation power. If c g is too high, it will generate only a zero perturbation image. Therefore, finding the appropriate c g through a hyperparameter search is crucial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Adversarial Training with GAT</head><p>As an analogy, our adversarial training framework is similar to the spring training of a baseball team.</p><p>A trainer analyzes the vulnerable points of a player and, based on this analysis, trains his weakest parts in addition to providing general training. This process is repeated over and over again. The goal at the end of the spring camp, is that the player overcomes most of the weaknesses and becomes a better player.</p><p>GAT plays a similar role to a trainer for a baseball team. During each training step, GAT learns to generate the best adversarial perturbation for each input. Simultaneously, a classifier network is trained to classify correctly both original and adversarial examples generated by GAT. The loss function of GAT is given as <ref type="bibr" target="#b8">(9)</ref>, whereas that of the classifier network is based on the adversarial objective function.</p><formula xml:id="formula_11">L F = α • J(θ f , x, y) + (1 − α) • J(θ f , x + G(∆), y)<label>(10)</label></formula><p>For the simplicity's sake, we used α = 0.5 in all experiments. Similar to GAN, completely optimizing GAT in the inner loop of training is computationally expensive and would result in overfitting if we do not have a large number of datasets. Instead, we alternately optimize generator network k steps and the classifier network 1 step. GAT is maintained near its optimal solution if the classifier network varies in a sufficiently slow enough. We used k = 1 in all of our experiments. The entire procedure is presented in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Adversarial Training with GAT</head><p>Input: training data {(x (i) , y (i) )} N i=1 , classifier network F (x; θ f ), generator network G(x; θ g ) Output: Robust parameter vector θ f initialize θ f , θ g for number of training iterations do for k steps do Sample minibatch of m examples {(x (1) , y (1) ), ..., (x (m) , y (m) )} from data distribution. Update the generator by descending its stochastic gradient:</p><formula xml:id="formula_12">∇ θg 1 m m i=1 F x (i) + G(∆ (i) ) y (i) + c g • G(∆ (i) ) 2 2</formula><p>end for Sample minibatch of m examples {(x (1) , y (1) ), ..., (x (m) , y (m) )} from data distribution. Update the classifier by descending its stochastic gradient:</p><formula xml:id="formula_13">∇ θ f 1 m m i=1 α • J θ f , x (i) , y (i) + (1 − α) • J θ f , x (i) + G(∆ (i) ), y (i)</formula><p>end for</p><p>The gradient-based updates can use any standard gradient-based learning rule. We used the Adam-Optimizer in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>To verify the performance of our proposed method, we experimented on two CIFAR datasets <ref type="bibr" target="#b8">[9]</ref>: CIFAR-10 and CIFAR-100, which consist of 60, 000 32 × 32 colour images in 10 or 100 classes, respectively, with 6, 000 images per class. The two datasets each contain 50, 000 training samples and 10, 000 test samples. We split the original 50, 000 training samples into 45, 000 training samples and 5, 000 validation samples, and used the latter to tune the hyperparameters. We performed all experiments using TensorFlow <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental setup</head><p>The network architectures used in our experiments are described in Table <ref type="table" target="#tab_1">1</ref>. We used a small version of Allconvnet proposed by Springenberg et al. <ref type="bibr" target="#b14">[15]</ref> as a classifier (trainee) because this model has a simple structure and yields good performance. The input image to our ConvNets was a fixed-size 32 × 32 × 3 and normalized between 0 and 1. We used 3 × 3 convolutions with rectified linear units (ReLUs) as activation functions. The number of convolutional filters were increased linearly with each down-sampling which was implemented as sub-sampling with stride 2. Fully connected layers were replaced by simple 1 × 1 convolutions and the scores of each class were averaged over the spatial dimensions.</p><p>The generator network (trainer) had a simpler structure. The input gradient image of the generator network was computed from the classifier network. The network consisted of six 3 × 3 convolutional layers followed by two 1 × 1 convolutional layers, and used hyperbolic tangent as an activation function of the output layer.</p><p>Our model had no batch normalization <ref type="bibr" target="#b6">[7]</ref>, no dropout <ref type="bibr" target="#b15">[16]</ref>, and no weight decay. These techniques may have slightly improved our results but were not a major concern of our study. We trained our model using the Adam optimizer <ref type="bibr" target="#b7">[8]</ref> with a learning rate 1 × 10 −3 for the classifier and 1 × 10 −6 for the generator. The network weights were initialized using Xavier initialization method <ref type="bibr" target="#b2">[3]</ref>, which improved the speed of convergence. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Perturbations generated by GAT</head><p>Is it possible to find stronger perturbation images than that generated by fast gradient method when we only know the gradient of the original images? Surprisingly, the answer is yes. To verify this, we performed the following experiment. First, we trained a ConvNet without using an adversarial training algorithm. We call this network a "baseline network". The classification accuracy of the baseline network was approximately 77% and 44% for CIFAR-10 and CIFAR-100 datasets, respectively. We next trained our generator network (GAT) using the loss function given in <ref type="bibr" target="#b8">(9)</ref>. We used early stopping with validation loss, and the accuracy of the adversarial images generated by the generator network was computed using the test images. We compared the accuracy of the adversarial images generated by the fast gradient method (L 2 or L ∞ ) with the same perturbation power generated by the generator network. Experiments were repeated with different c g values given in <ref type="bibr" target="#b8">(9)</ref>, so that adversarial perturbations with various powers could be generated. The results are shown in Fig. <ref type="figure" target="#fig_1">2</ref>. The generator network efficiently generates stronger adversarial images than those generated by the other methods when the perturbation power is low. Unlike the fast gradient method, the generator network requires several iterations to achieve optimal state. However, in adversarial training phase, full optimization of the generator in each training step is not required because we alternately optimize the generator and classifier network.  gradient method with L 2 norm constraint using (3). Note that adversarial examples were generated for each network. Over the various perturbation powers, we measured the classification accuracy of each network for adversarial examples that were generated from each network's parameters. The results are shown in Fig. <ref type="figure" target="#fig_2">3</ref>. The proposed method (adversarial training with GAT) showed the best classification accuracy without perturbation. These results showed that our algorithm is also effective in regularizing the neural network. Furthermore, they showed that our method is more robust against adversarial perturbations than is the fast gradient sign method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Indirect attack</head><p>We next examined the robustness of the network against indirect attacks. Similar to previous experiments, an adversarial example was generated based on the fast gradient method with L 2 norm constraint. However, because attackers would be unfamiliar with the internal structure of each network, adversarial examples were generated using another baseline network. This kind of attack is effective because a large fraction of adversarial examples are misclassified by networks trained from scratch with different hyperparameters <ref type="bibr" target="#b16">[17]</ref>. We compared the proposed adversarial training method with the baseline network and with the fast gradient method with L ∞ norm constraint. The classification accuracy for the adversarial examples with various perturbation powers is shown in Fig. <ref type="figure" target="#fig_4">4</ref>. It shows similar tendency as in the previous experiment, but classification accuracy did not decrease more rapidly in an indirect attack than in a direct attack. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Generalization effect</head><p>Szegedy et al. <ref type="bibr" target="#b16">[17]</ref> showed that a neural network could be regularized through adversarial training.</p><p>We performed an experiment to compare the performance of our method with other regularization methods such as dropout, random perturbation, and adversarial training using fast gradient method. We used the set of hyperparameters that achieved the best performance on the validation data for each regularization method. We repeated this procedure 50 times using different weight initializations and reported the average and standard deviation of the test accuracy. The results for the CIFAR-10 and CIFAR-100 datasets are shown in Table <ref type="table" target="#tab_2">2</ref>.</p><p>The results show that the regularization effect of our method is remarkably superior to other regularization method. Previous regularization algorithms show aproximately 1 ∼ 2% performance improvement over the existing baseline network, whereas our proposed method shows a remarkable 4 ∼ 6% improvement in accuracy. Our proposed method is not only robust against adversarial examples, but also highly effective for regularization, which is another advantage of the proposed method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>We proposed a novel adversarial training method by combining adversarial training <ref type="bibr" target="#b4">[5]</ref> and GAN <ref type="bibr" target="#b3">[4]</ref>.</p><p>Experimental results show that our proposed method is not only robust against adversarial examples, but also effective in improving the generalization accuracy of the classifier. We believe that there are two main reasons to explain the better performance than the conventional fast gradient method.</p><p>First, the classifier has different robustness for each training data. In some images, the classifier can be easily fooled when having only low perturbation power. However, in other images, it cannot be easily fooled even with very high perturbation power. Because the conventional fast gradient method normalizes the size of each gradient, it generates adversarial images of the same perturbation power for all training images, which means that it is difficult for networks to converge. Generating adaptive adversarial examples based on the degree of robustness of each image can help efficiently train the network.</p><p>Second, the classifier network is a non-linear function. If the classifier network is a perfect linear function, finding better adversarial images than those found by the fast gradient method is impossible. However, because the classifier network is a non-linear function, GAT can detect non-linear patterns in the classifier network and use a gradient to produce better perturbations than with the fast gradient method.</p><p>Our proposed GAT effectively solves both problems because it does not normalize the gradient vector and evolves adaptively as the classifier is trained. Therefore, training a classifier that is robust to various adversarial examples is possible, and accordingly, effectively regularizes the model. However, further study is required because the proposed method takes 3 to 4 times longer training (depending on the capacity of the generator network) than the conventional fast gradient method. In addition, hyperparameters of each network should be carefully tuned because of the properties of GAN training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper, we proposed a method to make a classifier robust to adversarial examples using a GAN framework. The generator network generates a perturbation by finding the weaknesses of the classifier, and the classifier re-learns the image generated by the generator back to the original label.</p><p>As the two networks learn alternately, the classifier network becomes more robust to the adversary image, and eventually the generator network will not be able to find an proper image that could fool the classifier. Our adversarial training method is also practical since it does not need expensive optimization process in the inner loop to find optimal adversarial images. The classifier with our adversarial training method is highly robust to the adversarial examples. Furthermore, it was found that the proposed method was surprisingly effective in regularizing neural networks.</p><p>To the best of our knowledge, this is the first method to apply a GAN framework to adversarial training (or supervised learning). Therefore, much work remains to improve the method. What is the optimal capacity of a generator network? Does additional information other than a gradient exist that can help the generator to find a better adversarial image? When our method is applied to larger networks such as Inception <ref type="bibr" target="#b17">[18]</ref>, can similar results to those in this study be achieved? Further research is required to address these issues.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Adversarial training with Generative Adversarial Trainer: (1) Generative Adversarial Trainer G is trained to generate an adversarial perturbation that can fool the classifier network using the gradient of each image. (2) Classifier Network F is trained to classify correctly both original and adversarial examples generated by G.</figDesc><graphic url="image-1.png" coords="3,108.00,72.00,396.00,191.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Comparison of the fast gradient attack (with L 2 and L ∞ norm constraint) and our proposed GAT. Adversarial examples were generated for use with each method with various perturbation powers, and classification accuracy is plotted as shown.</figDesc><graphic url="image-2.png" coords="6,108.00,72.00,198.00,148.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>4. 3</head><label>3</label><figDesc>Adversarial training with GAT Mainly two types of attacks against deep neural networks exist. One is a direct attack which generates adversarial examples based on a precise understanding of the model internals or its training data. The other is an indirect attack, which generates adversarial examples without knowing precise information about the model. We experimented with adversarial examples generated in these two cases to assess the robustness of our network against various attacks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>4. 3 . 1 Figure 3 :</head><label>313</label><figDesc>Figure 3: Comparison of the baseline and robustified networks. Adversarial examples were generated using (3) from each network for various values of , and classification accuracy is plotted as shown.</figDesc><graphic url="image-4.png" coords="6,108.00,526.77,198.00,148.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Comparison of the baseline and robustified networks. Adversarial examples were generated using (3) from another baseline network for various values of , and classification accuracy is plotted as shown.</figDesc><graphic url="image-6.png" coords="7,108.00,316.09,198.00,148.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Model description for CIFAR datasets</figDesc><table><row><cell>classifier network (trainee)</cell><cell>generator network (trainer)</cell></row><row><cell>Input: 32 × 32 × 3 RGB image</cell><cell>Input: 32 × 32 × 3 gradient image</cell></row><row><cell>3 × 3 conv 48 ReLu</cell><cell>3 × 3 conv 48 ReLu</cell></row><row><cell>3 × 3 conv 48 ReLu, stride=2</cell><cell>3 × 3 conv 48 ReLu</cell></row><row><cell>3 × 3 conv 96 ReLu</cell><cell>3 × 3 conv 48 ReLu</cell></row><row><cell>3 × 3 conv 96 ReLu, stride=2</cell><cell>3 × 3 conv 48 ReLu</cell></row><row><cell>3 × 3 conv 96 ReLu</cell><cell>3 × 3 conv 48 ReLu</cell></row><row><cell>1 × 1 conv 96 ReLu</cell><cell>3 × 3 conv 48 ReLu</cell></row><row><cell>1 × 1 conv 10 (or 100)</cell><cell>1 × 1 conv 48 ReLu</cell></row><row><cell>global averaging over 8 × 8 image</cell><cell>1 × 1 conv 3</cell></row><row><cell>softmax</cell><cell>tanh</cell></row><row><cell cols="2">Output: 10 (or 100) class probabilities Output: 32 × 32 × 3 perturbation image</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Test accuracy for CIFAR datasets</figDesc><table><row><cell cols="2">(a) CIFAR-10</cell><cell cols="2">(b) CIFAR-100</cell></row><row><cell>Method</cell><cell>Test accuracy(%)</cell><cell>Method</cell><cell>Test accuracy(%)</cell></row><row><cell>Baseline</cell><cell>77.48 ± 0.46</cell><cell>Baseline</cell><cell>44.32 ± 0.63</cell></row><row><cell>Dropout</cell><cell>78.49 ± 0.64</cell><cell>Dropout</cell><cell>46.29 ± 0.61</cell></row><row><cell>Random Perturbation</cell><cell>77.59 ± 0.57</cell><cell>Random Perturbation</cell><cell>44.43 ± 0.71</cell></row><row><cell>Adv. training (FG, L ∞ )</cell><cell>78.12 ± 0.59</cell><cell>Adv. training (FG, L ∞ )</cell><cell>45.16 ± 0.73</cell></row><row><cell>Adv. training (FG, L 2 )</cell><cell>77.99 ± 0.45</cell><cell>Adv. training (FG, L 2 )</cell><cell>45.67 ± 0.63</cell></row><row><cell>Adv. training (GAT)</cell><cell>80.33 ± 0.44</cell><cell>Adv. training (GAT)</cell><cell>50.44 ± 0.56</cell></row><row><cell>Dropout + GAT</cell><cell>81.62 ± 0.34</cell><cell>Dropout + GAT</cell><cell>50.71 ± 0.49</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Began: Boundary equilibrium generative adversarial networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schumm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10717</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Aistats</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="256" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6572</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>-R. Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>-I. Maeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nakae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ishii</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.00677</idno>
		<title level="m">Distributional smoothing with virtual adversarial training</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Distillation as a defense to adversarial perturbations against deep neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Security and Privacy (SP), 2016 IEEE Symposium on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="582" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Understanding adversarial training: Increasing local stability of neural nets through robust optimization</title>
		<author>
			<persName><forename type="first">U</forename><surname>Shaham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Negahban</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05432</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6806</idno>
		<title level="m">Striving for simplicity: The all convolutional net</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6199</idno>
		<title level="m">Intriguing properties of neural networks</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alemi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07261</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
