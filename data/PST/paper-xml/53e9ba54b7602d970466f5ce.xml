<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards a View Invariant Gait Recognition Algorithm</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Amit</forename><surname>Kale</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Automation Research</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<postCode>20742</postCode>
									<settlement>College Park</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Amit</forename><forename type="middle">K Roy</forename><surname>Chowdhury</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Automation Research</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<postCode>20742</postCode>
									<settlement>College Park</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Automation Research</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<postCode>20742</postCode>
									<settlement>College Park</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Towards a View Invariant Gait Recognition Algorithm</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F3971850F971C1B3B4252887C3CC9EAE</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Human gait is a spatio-temporal phenomenon and typifies the motion characteristics of an individual. The gait of a person is easily recognizable when extracted from a sideview of the person. Accordingly, gait-recognition algorithms work best when presented with images where the person walks parallel to the camera (i.e. the image plane). However, it is not realistic to expect that this assumption will be valid in most real-life scenarios. Hence it is important to develop methods whereby the side-view can be generated from any other arbitrary view in a simple, yet accurate, manner. That is the main theme of this paper. We show that if the person is far enough from the camera, it is possible to synthesize a side view (referred to as canonical view) from any other arbitrary view using a single camera. Two methods are proposed for doing this: i) by using the perspective projection model, and ii) by using the optical flow based structure from motion equations. A simple camera calibration scheme for this method is also proposed. Examples of synthesized views are presented. Preliminary testing with gait recognition algorithms gives encouraging results. A by-product of this method is a simple algorithm for synthesizing novel views of a planar scene.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Human identification forms an important component of visual surveillance. In many such applications established non-invasive biometrics such as face or iris may not be available at sufficient resolution to be used for recognition. A biometric that can address some of these shortcomings is "gait", which is motivated by the fact that humans exhibit the capability of recognizing people even from impoverished displays of gait <ref type="bibr" target="#b0">[1]</ref>, indicating the presence of identity information. Gait can be detected and measured even in low resolution video and can also be used with IR imagery <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>. The gait of a person is best reflected when he/she presents a side view (referred to in this paper as a canonical view) to the camera. Hence, most gait recognition algorithms rely on the availability of the side view of the sub-ject. The situation is analogous to face recognition where it is useful to have frontal views of the person's face.</p><p>In realistic surveillance scenarios, however, it is unreasonable to assume that a subject would always present a side-view to the camera and hence, gait recognition algorithms need to work in a situation where the person walks at an arbitrary angle to the camera.There are two effects of a change in viewing direction. One is simply the foreshortening or lengthening that occurs as the person walks away or towards the camera. The second is the change in the apparent stride length. The most general solution to this problem would involve estimating a 3D model of the person from which the required canonical view can be generated. This problem requires the solution of the structure from motion (SfM) or stereo reconstruction problems <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>, which are known to be hard. To circumvent the problems associated with the estimation of 3D models, several approaches have been proposed for the gait recognition problem. Bobick and Johnson <ref type="bibr" target="#b7">[8]</ref> use linear regression to map static parameters across views. In <ref type="bibr" target="#b8">[9]</ref>, Shakhnarovich et al. compute an image based visual hull from a set of monocular views which is then used to render virtual views for tracking and recognition. In this paper, we propose an alternative approach that can work with only a single camera and can synthesize canonical views of high quality in a way that uses the 3D structure only implicitly. These synthesized views can then be used for gait recognition. The order of computation is ¢¡ ¤£ ¦¥ ¨ § , where £ and ¥</p><p>are the dimensions of the bounding box around the person.</p><p>Consider a person walking along a straight line which subtends an angle © with the image plane (AC in Figure <ref type="figure">1</ref>). If the distance, , of the person from the camera is much larger than the width, , of the person, then it is reason- able to replace the scaling factor ¨! " for perspective pro- jection by an average scaling factor . In other words, for objects far enough from the camera, we can approximate the actual 3D object as being represented by a planar object. Assume that we are given a video of a person walking at a fixed angle © (Figure <ref type="figure">1</ref>). We show that by tracking the direction of motion, # , in the video sequence, we can accurately estimate the angle © in the 3D world. This can be done is two ways: a)</p><formula xml:id="formula_0">PLANE (0,0,0) Y f A B C X Z PROJECTION (x1,y1) (X1,Y1,Z1) Z1&gt;&gt;f Z1</formula><p>Figure <ref type="figure">1</ref>: Imaging Geometry by using the perspective projection matrix, or b) by using the optical flow based SfM equations. We also show that a simple, yet precise, camera calibration scheme can be designed for this problem. Under the assumption of planarity, using the angle © and the calibration parameters, we can synthesize side-views or canonical views of the person, which can then be passed on to the gait recognition algorithms. Since the planar approximation is reasonable for many surveillance scenarios where the distance between the camera and people is large, this is a practical approach for synthesizing canonical views required by many gait recognition algorithms. A by-product of the above method is a simple algorithm to synthesize novel views of a planar scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Theory</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Imaging Geometry</head><p>The imaging setup is shown in Figure <ref type="figure">1</ref>. The coordinate frame is attached rigidly to a camera with the origin at the center of perspective projection and the $ -axis perpendicular to the image plane. Assume that the person walks with a translational velocity % '&amp; )( 10 32 4 65 4 60 37 68 @9 along the line AC.</p><p>The line AB is parallel to the image plane XY and this is the direction of the canonical view which needs to be synthesized. The angle between the straight line AB and AC, i.e.</p><p>© , represents a rotation about the vertical axis we hence we shall call this the azimuth angle. We will use the notation that ( A B4 DC E4 6 F8 denotes the coordinates of a point in 3D and ( G H4 6I 38 its projection on the image plane.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Estimating the Azimuth Angle from Video Sequence</head><p>We present two ways of estimating the angle © from the video sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Perspective Projection Approach</head><p>We assume that the person is walking along the straight line AC in Figure <ref type="figure">1</ref>. Under exact perspective projection, straight lines map to straight lines. Thus the direction of motion in the 3D world corresponds to a straight line in the image plane, which can be estimated by tracking some points which move approximately rigidly as the person walks.</p><p>Consider the equation of the 3D line which is at a height P from the ground plane and parallel to it, i.e.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Q&amp; SR UT ¥ H¡</head><formula xml:id="formula_1">© § A 'V W 4 C X&amp; YP a`<label>(1)</label></formula><p>Under perspective projection this line transforms to (see Appendix)</p><formula xml:id="formula_2">I b&amp; P c d P R UT ¥ H¡ © § G H4<label>(2)</label></formula><p>where G &amp; ec f 9 hg 6i qp sr ut f , I v&amp; ec w 9 hg 6i qp sr ut f and c denotes the focal length of the camera. Thus if the slope of the line in the image plane, viz. R UT ¥ H¡ # § , is known, then given x y&amp; d</p><p>, the azimuth angle © can be computed as</p><formula xml:id="formula_3">R UT ¥ H¡ © § &amp; x R UT ¥ H¡ # § `(3)</formula><p>x can be obtained as a part of the calibration procedure.</p><p>Note that using the orthographic projection model will result in giving a straight line I ¢&amp; QP which does not reflect the azimuth angle variation in the image plane. Thus our method will not work under orthographic projection assumptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Optical Flow Based SfM Approach</head><p>Assume that the motion between two consecutive frames in the video sequence is small. Using the optical flow based </p><formula xml:id="formula_4">¡ G H4 6I § &amp; ¡ G d c G § a¡ G H4 uI § (4) ¡ G H4 6I § &amp; I a¡ G H4 uI § 4<label>(5)</label></formula><p>where</p><formula xml:id="formula_5">¨¡ G h4 6I § &amp; 0 7 $ ¡ G H4 6I § is the scaled inverse scene depth and G &amp; v ¡ © § &amp; 6 4 uI &amp; d is the focus of expansion (FOE). When 0 7 &amp; e5 but 0 2 )f &amp; g5</formula><p>, we see that © h&amp; Y5 , i.e. the canonical direction of walk, AB. Also, in this case ¡ G H4 uI § &amp; X5 . For the case when the person walks at an azimuth angle © f &amp; e5 , dividing (4) by ( <ref type="formula" target="#formula_4">5</ref>) we obtain, (6)   where v ¡ # ¡ G H4 6I § § &amp; lk p 2 nm o t p p 2 qm o t . For a fixed point in the im- age (e.g. centroid of the head) in the ( <ref type="formula">6</ref>), we have</p><formula xml:id="formula_6">v ¡ # ¡ G H4 6I § § &amp; Yi ¡ G h4 6I § d £ j¡ I 4 6c § v ¡ © § 4</formula><formula xml:id="formula_7">v ¡ # § &amp; e d r v ¡ © § `<label>(7)</label></formula><p>i ¡ G H4 uI § and £ j¡ G H4 uI § can be obtained from calibration data. By considering one particular point in a number of images, we can robustly estimate © from # . Equation (3) was derived under the perspective projection model, while equation ( <ref type="formula" target="#formula_7">7</ref>) was derived using perspective projection and optical flow. Hence the difference in the two equations. However, both give numerically close results as explained in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Coordinate Transformation to Canonical View</head><p>Having obtained the angle © , we need to synthesize the canonical view. Let denote the distance of the object from the image plane. If the dimensions of the object are small compared to , then the variation in © , s t© Uu e5 . This essen- tially corresponds to assuming a planar approximation to the object. Let ( A r 4 C r 4 6 r 8 wv denote the coordinates of any point on the person who is walking at an angle © yx z5 to the image plane (as shown in the Figure <ref type="figure">1</ref>). Then</p><formula xml:id="formula_8">{ | A } C ~ &amp; e ¡ © § { | A r C r r 4<label>(8)</label></formula><p>where</p><formula xml:id="formula_9">¡ © § &amp; { | v ¡ © § 5 ¦ ¥ H¡ © § 5 5 d " ¥ H¡ © § 5 v y ¡ © § `<label>(9)</label></formula><p>Denoting the corresponding image plane coordinates as ( G r 4 6I r 8 v and ( 1G 4 uI 8 v (for © &amp; 5 ) and using the perspective transformation, we can obtain the equations for ( 1G 4 6I 8 wv as</p><formula xml:id="formula_10">(see Appendix) G &amp; c G r v ¡ © § d c " ¥ H¡ © § d G r " ¥ H¡ © § V Wc v ¡ © § I &amp; c I r d G r " ¥ H¡ © § V Wc v ¡ © § a4<label>(10)</label></formula><p>where</p><formula xml:id="formula_11">G &amp; Yc A</formula><p>$ and I ¢&amp; Yc C $ Èquation <ref type="bibr" target="#b9">(10)</ref> is particularly attractive since it does not involve the 3D depth; rather it is a direct transformation of the 2D image plane coordinates in the non-canonical view to get the image plane coordinates in the canonical one.</p><p>Thus knowing the azimuth angle © we can obtain a synthetic canonical view using <ref type="bibr" target="#b9">(10)</ref> and a suitable texture mapping rule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Synthesis of Arbitrary Planar Views</head><p>The extension of the above method to synthesize arbitrary planar views is straight-forward. Suppose we are given a video sequence of a person walking at an angle © . This can be estimated from the direction of motion of the person in the video sequence (as explained above). Once this is done we can synthesize the view at an angle © 3 by applying the transformation of <ref type="bibr" target="#b9">(10)</ref> with © &amp; l© d © . Thus, for planar scenes, we are able to generate synthetic views purely from the video data. This is important for many applications other than gait recognition, such as multimedia.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Application to Gait Recognition</head><p>Approaches in computer vision to the gait recognition problem can be broadly classified as being either model-based or model-free. Methods which assume a priori models <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref> match the 2-D image sequences to the model data. In <ref type="bibr" target="#b2">[3]</ref>, the authors proposed a method where several ellipses are fitted to different parts of the binarized silhouette of the person and the parameters of these ellipses such as location of its centroid, eccentricity etc. are used as a feature to represent the gait of a person. Recognition is achieved by template matching. Model-free methods <ref type="bibr" target="#b3">[4]</ref> establish correspondence between successive frames based upon the prediction or estimation of features related to position, velocity, shape, texture and color. In <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13]</ref>, the sum of the white pixels along each row of the boxed silhouette image, referred to as the width vector, has been used as a feature. We now show that it is possible to obtain the transformed width vector in the canonical view directly from images obtained at an arbitrary view. Let R l ( 1G r 4 6I r 8 ( 1G 4 uI 8 as represented in <ref type="bibr" target="#b9">(10)</ref> and ¡ G H4 uI § denote the image intensity at ¡ G h4 6I §</p><p>. Also, let r represent the synthesized image at angle © and r ¡ I § the width vector for a particular row, I , in the image. Given the azimuth angle we can synthesize the width vector in the canonical view as</p><formula xml:id="formula_12">¡ I § ! &amp; 2 ¡ G 4 6I § &amp; 2 3 p 2 3m o 6 t s 9 t p 2 m o t ¡ G r 4 6I r § D § 4 (11)<label>(12)</label></formula><p>where</p><formula xml:id="formula_13">¡ G h4 6I § &amp; if ¡ G H4 6I § f &amp; l5 and ¡ G H4 uI § &amp; l5 , other- wise.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Obtaining Camera Calibration Parameters</head><p>Using Equations ( <ref type="formula">3</ref>), ( <ref type="formula">6</ref>) and ( <ref type="formula" target="#formula_10">10</ref>) requires a knowledge of the parameters c , x , and r , which are essentially the camera calibration parameters for this problem. In order to compute c , we used a calibration grid marked with 20 points  <ref type="formula" target="#formula_14">13</ref>). We solve this nonlinear regression using the Gauss-Newton method to obtain c « h&amp; argmin ¡ c § , where</p><formula xml:id="formula_14">B&amp; § m r ¬¨ G § d c G § r ¨ v ¡ © D¡ § d c " ¥ H¡ © D¡ § c v ¡ © ¡ § V ®G § r ¬¨ ¥ H¡ © D¡ § ¯ V I § d c I ª § r d G § r ©¨ " ¥ H¡ © D¡ § V ®c v ¡ © D¡ § ª¯ `<label>(13)</label></formula><p>Next, we consider the estimation of x , and r . In or- der to do this, we captured videos of a person walking at we know the angle © which traces out the straight line at the angle # . Given the corresponding values of # and © , we can estimate x from (3). Similarly, we can obtain i ¡ G H4 uI § and £ j¡ G H4 uI § in <ref type="bibr" target="#b5">(6)</ref>. Let and r be the corresponding representations for a particular point ¡ G H4 uI §</p><formula xml:id="formula_15">©</formula><p>. The direction of motion of this line, v ¡ # § &amp; k p , is constant, since it moves along a straight line. Hence a robust estimate of # can be obtained by considering the Thus and r can be determined from ( <ref type="formula" target="#formula_7">7</ref>) by considering all the corresponding values of # and © .</p><p>Figure <ref type="figure" target="#fig_3">4</ref> plots the true values of # and © , as well as the two regression lines ( <ref type="formula">3</ref>) and ( <ref type="formula" target="#formula_7">7</ref>) obtained from the calibration procedure. Even though (3) and ( <ref type="formula" target="#formula_7">7</ref>) were derived under different physical models, the straight lines in both the cases are good approximations of the true data. The main source of error is due to the assumption of straight line motion of a point, which is never precisely true in practice. Also, the effect of changing the image coordinate system can be taken into account easily by making the appropriate modifications in the perspective projection equations <ref type="bibr" target="#b5">[6]</ref>, at the cost of increasing the number of intrinsic camera parameters that need to be estimated during the calibration procedure.</p><p>Given a test video, we can estimate the value of # in a way similar to that shown in Figure <ref type="figure" target="#fig_1">3</ref>. Thereafter, the value of © can be read off directly from the calibration lines in Fig- <ref type="figure" target="#fig_3">ure 4</ref>. The choice of which of the two lines should be used is left to the discretion of the user, who can determine that depending upon the validity of the assumptions in the particular case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>In this section, we present results of our method for synthesizing canonical views of people from videos of them walking along arbitrary directions. We use the canonical views for gait recognition. The experiments are on a small number of people and conducted with the motivation of presenting a proof of concept for our algorithm. Detailed gait recognition experiments is the focus of future work. Our database consists of 12 people, who walk along Since © is known for the calibration procedure, x , and r were computed using the method described in Section 3.</p><p>For the video of an unknown person in the database the above image processing operations are repeated to compute the image plane angle # . Using the calibration line shown in Figure <ref type="figure" target="#fig_3">4</ref>, the azimuth angle © was obtained. Using this value of © and the value of c obtained as a part of the cal- ibration procedure, the view of the person was synthesized using the <ref type="bibr" target="#b9">(10)</ref>. Some of the synthesis results are shown in Figure <ref type="figure" target="#fig_6">5</ref>, along with the images from the original video sequences. Note that the height of the synthesized silhouette is almost constant similar to the true zero azimuth case shown in Figure <ref type="figure" target="#fig_6">5(a)</ref>. It is also instructive to look at the width profile (defined as the number of pixels in each row between the extremities of the binarized silhouette) of one person plotted as a function of time as shown in Figure <ref type="figure" target="#fig_7">6</ref>. The lower halves of these width plots correspond to the leg regions. In Figure <ref type="figure" target="#fig_7">6</ref> both the foreshortening and the effect of viewing direction on the leg-swings can be observed. In particular it can be seen that the leg swing as observed from a noncanonical view is smaller than what it would be from the exact side-view. Usage of such an unnormalized gait sequence for recognition will give poor results. Our method provides a systematic way of handling both these effects as can be seen from the width vector of the synthesized images in For the case when © &amp; À we find that in the torso re- gion, the reconstructed silhouette is broader than the orig-  inal. The reason for this is the limitation of the planarity assumption for the torso region. For non-canonical views, parts of the torso unseen in the canonical view, appear. To appreciate this better, consider that we approximate the torso as a rectangular block. In the canonical view, just one face of this block is visible. For non-canonical views parts of the other faces of this block are visible too. The synthesis algorithm, which interprets this as a plane, renders a broader reproduction of the torso part. Notice however that this effect is somewhat lesser in the leg portions of the silhouette. As can be seen from the Figure <ref type="figure" target="#fig_7">6</ref>, the lower parts are clearly distinguishable and similar for the different angles, though the upper halves of the plots become more and more noisy as the value of © increases. In order to study the per- formance of gait recognition on the synthesized images we used a rather simple variant of the baseline gait recognition algorithm <ref type="bibr" target="#b14">[15]</ref>. Our gallery consists of people walking at © AE&amp; Ç5 , i.e. the canonical view. The probes are video se- quences where people walk at arbitrary angles © . We take È contiguous boxed images of a person in the gallery when he/she is walking at an azimuth © e&amp; 5 . For every image of the probe É transformed to the zero azimuth from the az- imuth © 2 , we compute the similarity matrix "r &amp; Q r ¡ 4 ¬É § taking the usual binary correlation in <ref type="bibr" target="#b13">(14)</ref>, we also computed the similarity matrices for just the lower half of the bounding box. This is approximately equivalent to considering just the leg portion of the body. This is motivated by the fact that the planarity assumption is more strictly adhered to in the leg portion than in the torso. Gait recognition performance can be improved further by fusing other static cues about the person, such as height. We fuse height information with the leg dynamics by scaling each entry ¡ 4 ¬É § of the similarity matrix by the corresponding height ratio, max ¡ qÙ p @ § t Ù p ¡ t 4 6Ú d Ù p w § wt Ù p ¡ t § . The similarity matrices, yield as a by-product a quantitative assessment of the quality of the synthesized images as</p><formula xml:id="formula_16">Û r &amp; Ü ÞÝ § r ¡ D4 u § 4<label>(15)</label></formula><p>for each © h&amp; Y© 2 and Ü persons. This is plotted as a function of © in Figure <ref type="bibr" target="#b6">(7)</ref>. The cumulative match characteristics <ref type="bibr" target="#b14">[15]</ref> are shown in Figure <ref type="bibr" target="#b8">(9)</ref> for the full body, leg only and leg and height fusion cases. The rise of the solid curves (representing the leg dynamics, with or without height fusion) is faster  than the dotted ones (representing full body). This means that recognition performance is better when only the leg dynamics, instead of the whole body, is used. The reason for this is the incorrect broadening of the torso region. The performance in the case where height information is fused with leg dynamics is even better. Interestingly, <ref type="bibr" target="#b14">[15]</ref> notes that the lower 20 % of the silhouette accounts for roughly 90% of the recognition. Similarly <ref type="bibr" target="#b1">[2]</ref> showed that the gait recognition in the case when the subject was carrying a ball, viz. no upper body dynamics, the recognition rates were better. The fact that the gait recognition results are encouraging upto angles of degrees allows us to hypothesize that it is possible to do reasonable human identification using gait with only two cameras (installed perpendicular to each other).</p><p>In order to study the efficiency of gait recognition with synthesized views, we compute the Receiver Operating Characteristic (ROC), which is a plot of the probability of detection (i.e. correct recognition), á , vs. the probability of a false alarm (i.e. false acceptance), â , for azimuth an- gles © h&amp; 3 4 6 q5 and degrees. The plots are shown in Fig- ure <ref type="figure" target="#fig_10">8</ref>. The performance degradation with increasing © can be understood from these plots. The ROC curves indicate that the proper detection threshold should vary with © , so as to obtain a performance characteristic with small â and large á .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>In this paper, we have proposed a method for synthesizing arbitrary views of planar objects, and applying the synthesized views for gait recognition whenpeople are walking at any arbitrary angle to the camera. Our method used a perspective projection model and an optical flow based structure from motion model for estimating the azimuth angle of the original view from monocular video data. Thereafter, a video sequence at the new view was synthesized. The entire process was done in 2D, though 3D structure of the scene played an implicit role. A simple, yet accurate, camera calibration procedure was also proposed. Examples of synthesized views are presented. Preliminary results of gait recognition on a database of people was reported using these synthesized views. Development of appropriate gait recognition algorithms for people walking at arbitrary angles is one of our future research directions. Though the method has been explained from the motivation of the gait recognition problem, it has important applications in other areas too, like multimedia and video processing. That, too, forms a part of our future research into this problem. I &amp; Sc C r d A r " ¥ H¡ © § V r v ¡ © § `(26) Substituting for A r and C r from (24) yields Equation (10).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>SfM equations, let ¡ G h4 6I §and ¡ G h4 6I § represent the horizontal and vertical velocity fields of a point ¡ G H4 uI § in the image plane. Since we consider straight line motion along AC, and are related to the 3D object motion and scene depth by<ref type="bibr" target="#b9">[10]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Tracked points in the video sequence and the best fit straight lines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure (3), with slopes R UT ¥ H¡ # ¡ © § § . The top line is the case when © &amp; ³5 . The lines for © &amp; 3 is the one immediately below this line and so on. As may be expected, larger azimuth angles lead to larger image plane angles. The upper right corner where the lines intersect approximately, corresponds to the point from where the subjects start walking. These straight lines are the projections of the straight lines (one for each angle) traced out by the motion of the tracked rigid point in the 3D world. For the calibration procedure,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Calibration curves for ¶µ • D¹ º vs. ¶µ • D1» ¨º . The dots represent the true values, the solid line represents the calibration curve using (3) and the dashed and dotted line represents the calibration curve using ((7).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Calibration grid placed at (a) 0 (b) 15 (c) 30 and (d) 45 degrees.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Figures 6(c) and (d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: (a) represents different stances of a person walking parallel to the camera; (b) (d) and (f) represent different stances of a person walking at angles 15, 30 and 45 degrees to the camera; (c) (e) and (g) represent side-views synthesized from original videos where the person walks at angles of 15, 30 and 45 degrees to the camera.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>FramesFigure 6 :</head><label>6</label><figDesc>Figure 6: Width profile as a function of time for (a) Canonical View (¹ UÁ WÂ );(b) Unnormalized sequence for ¹ Á ¿Ã Ä ;(c) and (d) Synthesized non-canonical Views for ¹ UÁ ¤Å 3Â and ¹ UÁ ¿Ã Ä respectively.</figDesc><graphic coords="6,72.94,196.64,226.07,96.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>¬É § &amp; ËÊ maxcorr ¡ ©Ì r m ¡ 4 ©Í § Î § 4 (14) and Ì r m ¡ refers to the P th image in the sequence synthe- sized from © 2 for the probe É . Í b § Î &amp; Ï£ Ð U § 4 3 4 Ð Ñ § Î ¥ is the set of È contiguous images for the zero azimuth for gallery person , and maxcorr ¡ ©Ì r m ¡ 4 ©Í § Î § &amp; QÒ Ó 3Ô Õ È BÖ £ ¡ Ð U § Õ q× Ì r m ¡ § È BÖ £ ¡ Ð § Õ qØ Ì r m ¡ § Besides</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Quality degradation of the synthesized images as a function of angle.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: ROC curves for ¹ Á zß dÄ ªà Å Â and Ã Ä degrees for the case of leg and height fusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Cumulative Match Characteristics for Synthesized images (solid line represents the full body used for matching, dash-dotted line represents the case where only the leg is used for matching) for (a) ¹ UÁ Wß dÄ (b) ¹ UÁ ¤Å 3Â and (c) ¹ UÁ aeÃ Ä .</figDesc></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix Proof of Equation (2):</head><p>Consider Equation (1) and the perspective projection model. Then,</p><p>Dividing Equation ( <ref type="formula">17</ref>) by ( <ref type="formula">16</ref>) we get (except for the few degenerate points where the denominator is zero),</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I G &amp; P A `(18)</head><p>Now consider Equation ( <ref type="formula">16</ref>): </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Equation (10):</head><p>Using Equations ( <ref type="formula">8</ref>) and ( <ref type="formula">9</ref>), we get</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Recognizing friends by their walk:gait perception without familiarity cues</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cutting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kozlowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bulletin of the Psychonomic Society</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="353" to="356" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A framework for activity-specific human recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cuntoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>the International Conference on Acoustics, Speech and Signal Processing<address><addrLine>Orlando, FL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-05">May 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Gait analysis for recognition and classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E L</forename><surname>Grimson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Face and Gesture Recognition</title>
		<meeting>the IEEE Conference on Face and Gesture Recognition</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="155" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Recognizing humans by gait via parametric canonical space</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Nixon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence in Engineering</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="359" to="366" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Silhouette-based human identification from body shape and gait</title>
		<author>
			<persName><forename type="first">R</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Face and Gesture Recognition</title>
		<meeting>IEEE Conference on Face and Gesture Recognition</meeting>
		<imprint>
			<date type="published" when="2002-05">May 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Three-Dimensional Computer Vision: A Geometric Viewpoint</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">D</forename><surname>Faugeras</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Multiple View Geometry in Computer Vision</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">I</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Gait recognition using static activity-specific parameters</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Bobick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Integrated face and gait recognition from multiple views</title>
		<author>
			<persName><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2001-12">December 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A Guided Tour of Computer Vision</title>
		<author>
			<persName><forename type="first">Vishwjit</forename><surname>Nalwa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Addison-Wesley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Gait extraction and description by evidence-gathering</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cunado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Nash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Nixon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Carter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Audio and Video Based Biometric Person Authentication</title>
		<meeting>of the International Conference on Audio and Video Based Biometric Person Authentication</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="43" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Walking patterns of normal men</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Drought</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Kory</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Bone and Joint surgery</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="335" to="360" />
			<date type="published" when="1964">1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Gait sequence analysis using frieze patterns</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tsin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="657" to="671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Non-parametric model for background subtraction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Elgammal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Harwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
	<note>FRAME-RATE Workshop</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The gait identification challenge problem: Data sets and baseline algorithm</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Robledo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Grother</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc of the International Conference on Pattern Recognition</title>
		<meeting>of the International Conference on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
