<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shijie</forename><surname>Wu</surname></persName>
							<email>shijie.wu@jhu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mark</forename><surname>Dredze</surname></persName>
							<email>mdredze@cs.jhu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Pretrained contextual representation models <ref type="bibr" target="#b24">(Peters et al., 2018;</ref><ref type="bibr" target="#b8">Devlin et al., 2019)</ref> have pushed forward the state-of-the-art on many NLP tasks. A new release of BERT (Devlin, 2018) includes a model simultaneously pretrained on 104 languages with impressive performance for zero-shot cross-lingual transfer on a natural language inference task. This paper explores the broader cross-lingual potential of mBERT (multilingual) as a zero-shot language transfer model on 5 NLP tasks covering a total of 39 languages from various language families: NLI, document classification, NER, POS tagging, and dependency parsing. We compare mBERT with the best-published methods for zero-shot cross-lingual transfer and find mBERT competitive on each task. Additionally, we investigate the most effective strategy for utilizing mBERT in this manner, determine to what extent mBERT generalizes away from language-specific features, and measure factors that influence cross-lingual transfer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Pretrained language representations with selfsupervised objectives have become standard in a variety of NLP tasks <ref type="bibr" target="#b24">(Peters et al., 2018;</ref><ref type="bibr" target="#b12">Howard and Ruder, 2018;</ref><ref type="bibr" target="#b26">Radford et al., 2018;</ref><ref type="bibr" target="#b8">Devlin et al., 2019)</ref>, including sentence-level classification <ref type="bibr" target="#b37">(Wang et al., 2018)</ref>, sequence tagging (e.g. NER) <ref type="bibr" target="#b35">(Tjong Kim Sang and De Meulder, 2003)</ref> and SQuAD question answering <ref type="bibr" target="#b27">(Rajpurkar et al., 2016)</ref>. Self-supervised objectives include language modeling, the cloze task <ref type="bibr" target="#b32">(Taylor, 1953)</ref> and next sentence classification. These objectives continue key ideas in word embedding objectives like CBOW and skip-gram <ref type="bibr" target="#b16">(Mikolov et al., 2013a)</ref>.</p><p>Code is available at https://github.com/ shijie-wu/crosslingual-nlp At the same time, cross-lingual embedding models have reduced the amount of cross-lingual supervision required to produce reasonable models; <ref type="bibr" target="#b5">Conneau et al. (2017)</ref>; <ref type="bibr">Artetxe et al. (2018)</ref> use identical strings between languages as a pseudo bilingual dictionary to learn a mapping between monolingual-trained embeddings. Can jointly training contextual embedding models over multiple languages without explicit mappings produce an effective cross-lingual representation? Surprisingly, the answer is (partially) yes. BERT, a recently introduced pretrained model <ref type="bibr" target="#b8">(Devlin et al., 2019)</ref>, offers a multilingual model (mBERT) pretrained on concatenated Wikipedia data for 104 languages without any cross-lingual alignment <ref type="bibr" target="#b7">(Devlin, 2018)</ref>. mBERT does surprisingly well compared to cross-lingual word embeddings on zeroshot cross-lingual transfer in XNLI <ref type="bibr" target="#b6">(Conneau et al., 2018)</ref>, a natural language inference dataset. Zeroshot cross-lingual transfer, also known as singlesource transfer, refers trains and selects a model in a source language, often a high resource language, then transfers directly to a target language.</p><p>While XNLI results are promising, the question remains: does mBERT learn a cross-lingual space that supports zero-shot transfer? We evaluate mBERT as a zero-shot cross-lingual transfer model on five different NLP tasks: natural language inference, document classification, named entity recognition, part-of-speech tagging, and dependency parsing. We show that it achieves competitive or even state-of-the-art performance with the recommended fine-tune all parameters scheme <ref type="bibr" target="#b8">(Devlin et al., 2019)</ref>. Additionally, we explore different fine-tuning and feature extraction schemes and demonstrate that with parameter freezing, we further outperform the suggested fine-tune all approach. Furthermore, we explore the extent to which mBERT generalizes away from a specific language by measuring accuracy on language ID arXiv:1904.09077v2 [cs.CL] 3 Oct 2019 using each layer of mBERT. Finally, we show how subword tokenization influences transfer by measuring subword overlap between languages.</p><p>2 Background (Zero-shot) Cross-lingual Transfer Crosslingual transfer learning is a type of transductive transfer learning with different source and target domain <ref type="bibr" target="#b21">(Pan and Yang, 2010)</ref>. A cross-lingual representation space is assumed to perform the cross-lingual transfer. Before the widespread use of cross-lingual word embeddings, task-specific models assumed coarse-grain representation like part-of-speech tags, in support of a delexicalized parser <ref type="bibr" target="#b41">(Zeman and Resnik, 2008)</ref>. More recently cross-lingual word embeddings have been used in conjunction with task-specific neural architectures for tasks like named entity recognition <ref type="bibr" target="#b39">(Xie et al., 2018)</ref>, part-of-speech tagging <ref type="bibr" target="#b13">(Kim et al., 2017)</ref> and dependency parsing <ref type="bibr" target="#b0">(Ahmad et al., 2019)</ref>.</p><p>Cross-lingual Word Embeddings. The quality of the cross-lingual space is essential for zero-shot cross-lingual transfer. <ref type="bibr" target="#b28">Ruder et al. (2017)</ref> surveys methods for learning cross-lingual word embeddings by either joint training or post-training mappings of monolingual embeddings. <ref type="bibr" target="#b5">Conneau et al. (2017)</ref> and <ref type="bibr">Artetxe et al. (2018)</ref> first show two monolingual embeddings can be aligned by learning an orthogonal mapping with only identical strings as an initial heuristic bilingual dictionary. Contextual Word Embeddings ELMo <ref type="bibr" target="#b24">(Peters et al., 2018)</ref>, a deep LSTM <ref type="bibr" target="#b11">(Hochreiter and Schmidhuber, 1997)</ref> pretrained with a language modeling objective, learns contextual word embeddings. This contextualized representation outperforms standalone word embeddings, e.g. Word2Vec <ref type="bibr" target="#b17">(Mikolov et al., 2013b)</ref> and Glove <ref type="bibr" target="#b22">(Pennington et al., 2014)</ref>, with the same task-specific architecture in various downstream tasks. Instead of taking the representation from a pretrained model, GPT <ref type="bibr" target="#b26">(Radford et al., 2018)</ref> and <ref type="bibr" target="#b12">Howard and Ruder (2018)</ref> also fine-tune all the parameters of the pretrained model for a specific task. Also, GPT uses a transformer encoder <ref type="bibr" target="#b36">(Vaswani et al., 2017)</ref> instead of an LSTM and jointly fine-tunes with the language modeling objective. <ref type="bibr" target="#b12">Howard and Ruder (2018)</ref>   <ref type="formula">2019</ref>) trains a single ELMo on distantly related languages and shows mixed results as to the benefit of pretaining. Parallel to our work, <ref type="bibr" target="#b25">Pires et al. (2019)</ref> shows mBERT has good zero-shot cross-lingual transfer performance on NER and POS tagging. They show how subword overlap and word ordering effect mBERT transfer performance. Additionally, they show mBERT can find translation pairs and works on code-switched POS tagging. In comparison, our work looks at a larger set of NLP tasks including dependency parsing and ground the mBERT performance against previous state-of-the-art on zeroshot cross-lingual transfer. We also probe mBERT in different ways and show a more complete picture of the cross-lingual effectiveness of mBERT.</p><p>3 Multilingual BERT BERT <ref type="bibr" target="#b8">(Devlin et al., 2019)</ref> is a deep contextual representation based on a series of transformers trained by a self-supervised objective. One of the main differences between BERT and related work like ELMo and GPT is that BERT is trained by the Cloze task <ref type="bibr" target="#b32">(Taylor, 1953)</ref>, also referred to as masked language modeling, instead of right-to-left or left-to-right language modeling. This allows the model to freely encode information from both directions in each layer. Additionally, BERT also optimizes a next sentence classification objective. At training time, 50% of the paired sentences are consecutive sentences while the rest of the sentences are paired randomly. Instead of operating on words, BERT uses a subword vocabulary with WordPiece <ref type="bibr" target="#b38">(Wu et al., 2016)</ref>, a data-driven approach to break up a word into subwords.</p><p>Fine-tuning BERT BERT shows strong performance by fine-tuning the transformer encoder followed by a softmax classification layer on various sentence classification tasks. A sequence of shared softmax classifications produces sequence tagging models for tasks like NER. Fine-tuning usually takes 3 to 4 epochs with a relatively small learning rate, for example, 3e-5.</p><p>Multilingual BERT mBERT <ref type="bibr" target="#b7">(Devlin, 2018)</ref> follows the same model architecture and training procedure as BERT, except with data from Wikipedia in 104 languages. Training makes no use of explicit cross-lingual signal, e.g. pairs of words, sentences or documents linked across languages. In mBERT, the WordPiece modeling strategy allows the model to share embeddings across languages. For example, "DNA" has a similar meaning even in distantly related languages like English and Chinese 1 . To account for varying sizes of Wikipedia training data in different languages, training uses a heuristic to subsample or oversample words when running WordPiece as well as sampling a training batch, random words for cloze and random sentences for next sentence classification.</p><p>Transformer For completeness, we describe the Transformer used by BERT. Let x, y be a sequence of subwords from a sentence pair. A special token [CLS] is prepended to x and [SEP] is appended to both x and y. The embedding is obtained by</p><formula xml:id="formula_0">ĥ0 i = E(x i ) + E(i) + E(1 x ) ĥ0 j+|x| = E(y j ) + E(j + |x|) + E(1 y ) h 0 • = Dropout(LN( ĥ0 • ))</formula><p>where E is the embedding function and LN is layer normalization <ref type="bibr" target="#b4">(Ba et al., 2016)</ref>. M transformer blocks are followed by the embeddings. In each transformer block,</p><formula xml:id="formula_1">h i+1 • = Skip(FF, Skip(MHSA, h i • )) Skip(f, h) = LN(h + Dropout(f (h))) FF(h) = GELU(hW 1 + b 1 )W 2 + b 2</formula><p>where GELU is an element-wise activation function <ref type="bibr" target="#b10">(Hendrycks and Gimpel, 2016)</ref>. In practice,</p><formula xml:id="formula_2">h i ∈ R (|x|+|y|)×d h , W 1 ∈ R 4d h ×d h , b 1 ∈ R 4d h , W 2 ∈ R d h ×4d h , and b 2 ∈ R d h . MHSA is the multi-heads self-attention function. We show how one new position ĥi is computed. [• • • , ĥi , • • • ] = MHSA([h 1 , • • • , h |x|+|y| ]) = W o Concat(h 1 i , • • • , h N i ) + b o 1 "</formula><p>DNA" indeed appears in the vocabulary of mBERT as a stand-alone lexicon.</p><p>In each attention, referred to as attention head,</p><formula xml:id="formula_3">h j i = |x|+|y| k=1 Dropout(α (i,j) k )W j V h k α (i,j) k = exp (W j Q h i ) W j K h k √ d h /N |x|+|y| k =1 exp (W j Q h i ) W j K h k √ d h /N</formula><p>where N is the number of attention heads,</p><formula xml:id="formula_4">h j i ∈ R d h /N , W o ∈ R d h ×d h , b o ∈ R d h , and W j Q , W j K , W j V ∈ R d h /N ×d h .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Tasks</head><p>Does mBERT learn a cross-lingual representation, or does it produce a representation for each language in its own embedding space? We consider five tasks in the zero-shot transfer setting. We assume labeled training data for each task in English, and transfer the trained model to a target language.</p><p>We select a range of different tasks: document classification, natural language inference, named entity recognition, part-of-speech tagging, and dependency parsing. We cover zero-shot transfer from English to 38 languages in the 5 different tasks as shown in Tab. 1. In this section, we describe the tasks as well as task-specific layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Document Classification</head><p>We use MLDoc <ref type="bibr" target="#b30">(Schwenk and Li, 2018)</ref>, a balanced subset of the Reuters corpus covering 8 languages for document classification. The 4-way topic classification task decides between CCAT (Corporate/Industrial), ECAT (Economics), GCAT (Government/Social), and MCAT (Markets). We only use the first two sentences<ref type="foot" target="#foot_0">2</ref> of a document for classification due to memory constraint. The sentence pairs are provided to the mBERT encoder.</p><p>The task-specific classification layer is a linear function mapping</p><formula xml:id="formula_5">h 12 0 ∈ R d h into R 4</formula><p>, and a softmax is used to get class distribution. We evaluate by classification accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Natural Language Inference</head><p>We use XNLI <ref type="bibr" target="#b6">(Conneau et al., 2018)</ref> which cover 15 languages for natural language inference. The 3-way classification includes entailment, neutral, and contradiction given a pair of sentences. We ar bg ca cs da de el en es et fa fi fr he hi hr hu id it ja ko la lv nl no pl pt ro ru sk sl sv sw th tr uk ur vi zh MLDoc NLI NER POS Parsing Table <ref type="table">1</ref>: The 39 languages used in the 5 tasks. feed a pair of sentences directly into mBERT and the task-specific classification layer is the same as §4.1. We evaluate by classification accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Named Entity Recognition</head><p>We use the CoNLL 2002 and 2003 NER shared tasks <ref type="bibr" target="#b34">(Tjong Kim Sang, 2002;</ref><ref type="bibr" target="#b35">Tjong Kim Sang and De Meulder, 2003</ref>) (4 languages) and a Chinese NER dataset <ref type="bibr">(Levow, 2006)</ref>. The labeling scheme is BIO with 4 types of named entities. We add a linear classification layer with softmax to obtain word-level predictions. Since mBERT operates at the subword-level while the labeling is word-level, if a word is broken into multiple subwords, we mask the prediction of non-first subwords. NER is evaluated by F1 of predicted entity (F1). Note we use a simple post-processing heuristic to obtain a valid span.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Part-of-Speech Tagging</head><p>We use a subset of Universal Dependencies (UD) Treebanks (v1.4) <ref type="bibr">(Nivre et al., 2016)</ref>, which cover 15 languages, following the setup of <ref type="bibr" target="#b13">Kim et al. (2017)</ref>. The task-specific labeling layer is the same as §4.3. POS tagging is evaluated by the accuracy of predicted POS tags (ACC).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Dependency parsing</head><p>Following the setup of <ref type="bibr" target="#b0">Ahmad et al. (2019)</ref>, we use a subset of Universal Dependencies (UD) Treebanks (v2.2) <ref type="bibr">(Nivre et al., 2018)</ref>, which includes 31 languages. Dependency parsing is evaluated by unlabelled attachment score (UAS) and labeled attachment score (LAS) 3 . We only predict the coarsegrain dependency label following Ahmad et al. We use the model of <ref type="bibr" target="#b9">Dozat and Manning (2016)</ref>, a graph-based parser as a task-specific layer. Their LSTM encoder is replaced by mBERT. Similar to §4.3, we only take the representation of the first subword of each word. We use masking to prevent the parser from operating on non-first subwords.</p><p>3 Punctuations (PUNCT) and symbols (SYM) are excluded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We use the base cased multilingual BERT, which has N = 12 attention heads and M = 12 transformer blocks. The dropout probability is 0.1 and d h is 768. The model has 179M parameters with about 120k vocabulary.</p><p>Training For each task, no preprocessing is performed except tokenization of words into subwords with WordPiece. We use Adam (Kingma and Ba, 2014) for fine-tuning with β 1 of 0.9, β 2 of 0.999 and L2 weight decay of 0.01. We warm up the learning rate over the first 10% of batches and linearly decay the learning rate.</p><p>Maximum Subwords Sequence Length At training time, we limit the length of subwords sequence to 128 to fit in a single GPU for all tasks.</p><p>For NER and POS tagging, we additionally use the sliding window approach. After the first window, we keep the last 64 subwords from the previous window as context. In other words, for a non-first window, only (up to) 64 new subwords are added for prediction. At evaluation time, we follow the same approach as training time except for parsing.</p><p>We threshold the sentence length to 140 words, including words and punctuation, following <ref type="bibr" target="#b0">Ahmad et al. (2019)</ref>. In practice, the maximum subwords sequence length is the number of subwords of the first 140 words or 512, whichever is smaller.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyperparameter Search and Model Selection</head><p>We select the best hyperparameters by searching a combination of batch size, learning rate and the number of fine-tuning epochs with the following range: learning rate {2×10 −5 , 3×10 −5 , 5×10 Artetxe and <ref type="bibr">Schwenk (2018)</ref> use bitext between English/Spanish and the rest of languages to pretrain a multilingual sentence representation with a sequence-to-sequence model where the decoder only has access to a max-pooling of the encoder hidden states. mBERT outperforms (Tab. 2) multilingual word embeddings and performs comparably with a multilingual sentence representation, even though mBERT does not have access to bitext. Interestingly, mBERT outperforms <ref type="bibr" target="#b3">Artetxe and Schwenk (2018)</ref> in distantly related languages like Chinese and Russian and under-performs in closely related Indo-European languages.</p><p>XNLI We include three strong baselines, <ref type="bibr" target="#b3">Artetxe and Schwenk (2018)</ref> and <ref type="bibr" target="#b15">Lample and Conneau (2019)</ref> are concurrent to our work. <ref type="bibr" target="#b15">Lample and Conneau (2019)</ref> with MLM is similar to mBERT; the main difference is that it only trains with the 15 languages of XNLI, has 249M parameters (around 40% more than mBERT), and MLM+TLM also uses bitext as training data<ref type="foot" target="#foot_1">4</ref> . <ref type="bibr" target="#b6">Conneau et al. (2018)</ref> use supervised multilingual word embeddings with an LSTM encoder and max-pooling. After an English encoder and classifier are trained, the target encoder is trained to mimic the English encoder with ranking loss and bitext.</p><p>In Tab. 3, mBERT outperforms one model with bitext training but (as expected) falls short of models with more cross-lingual training information. Interestingly, mBERT and MLM are mostly the same except for the training languages, yet we observe that mBERT under-performs MLM by a large margin. We hypothesize that limiting pretraining to only those languages needed for the downstream task is beneficial. The gap between <ref type="bibr" target="#b3">Artetxe and Schwenk (2018)</ref> and mBERT in XNLI is larger than MLDoc, likely because XNLI is harder.  <ref type="bibr" target="#b5">(Conneau et al., 2017)</ref> with a hybrid of a character-level/word-level LSTM, self-attention, and a CRF. Pseudo training data is built by word-to-word translation with an induced dictionary from bilingual word embeddings.</p><p>mBERT outperforms a strong baseline by an average of 6.9 points absolute F1 and an 11.8 point absolute improvement in German with a simple one layer 0 th -order CRF as a prediction function (Tab. 4). A large gap remains when transferring to distantly related languages (e.g. Chinese) compared to a supervised baseline. Further effort should focus on transferring between distantly related languages. In §5.4 we show that sharing subwords across languages helps transfer. <ref type="bibr" target="#b13">Kim et al. (2017)</ref> as a reference. They utilized a small amount of supervision in the target language as well as English supervision so the results are not directly comparable. Tab. 5 shows a large (average) gap between mBERT and Kim et al. Interestingly, mBERT still outperforms <ref type="bibr" target="#b13">Kim et al. (2017)</ref> with 320 sentences in German (de), Polish (pl), Slovak (sk) and Swedish (sv).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>POS We use</head><p>Dependency Parsing We use the best performing model on average in <ref type="bibr" target="#b0">Ahmad et al. (2019)</ref> as a zero-shot transfer baseline, i.e. transformer encoder with graph-based parser <ref type="bibr" target="#b9">(Dozat and Manning, 2016)</ref>, and dictionary supervised cross-lingual embeddings <ref type="bibr" target="#b31">(Smith et al., 2017)</ref>. Dependency parsers, including Ahmad et al., assume access to gold POS tags: a cross-lingual representation. We consider two versions of mBERT: with and without gold POS tags. When tags are available, a tag embedding is concatenated with the final output of mBERT.</p><p>Tab. 6 shows that mBERT outperforms the base- Summary Across all five tasks, mBERT demonstrate strong (sometimes state-of-the-art) zeroshot cross-lingual performance without any crosslingual signal. It outperforms cross-lingual embeddings in four tasks. With a small amount of target language supervision and cross-lingual signal, mBERT may improve further; we leave this as future work. In short, mBERT is a surprisingly effective cross-lingual model for many NLP tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Question #2: Does mBERT vary layer-wise?</head><p>The goal of a deep neural network is to abstract to higher-order representations as you progress up the hierarchy <ref type="bibr" target="#b40">(Yosinski et al., 2014)</ref>. <ref type="bibr" target="#b24">Peters et al. (2018)</ref> empirically show that for ELMo in English the lower layer is better at syntax while the up-per layer is better at semantics. However, it is unclear how different layers affect the quality of cross-lingual representation. For mBERT, we hypothesize a similar generalization across the 13 layers, as well as an abstraction away from a specific language with higher layers. Does the zero-shot transfer performance vary with different layers?</p><p>We consider two schemes. First, we follow the feature-based approach of ELMo by taking a learned weighted combination of all 13 layers of mBERT with a two-layer bidirectional LSTM with d h hidden size (Feat). Note the LSTM is trained from scratch and mBERT is fixed. For sentence and document classification, an additional max-pooling is used to extract a fixed-dimension vector. We train the feature-based approach with Adam and learning rate 1e-3. The batch size is 32. The learning rate is halved whenever the development evaluation does not improve. The training is stopped early when learning rate drop below 1e-5. Second, when fine-tuning mBERT, we fix the bottom n layers (n included) of mBERT, where layer 0 is the input embedding. We consider n ∈ {0, 3, 6, 9}.</p><p>Freezing the bottom layers of mBERT, in general, improves the performance of mBERT in all five tasks (Fig. <ref type="figure" target="#fig_2">1</ref>). For sentence-level tasks like document classification and natural language inference, we observe the largest improvement with n = 6. For word-level tasks like NER, POS tagging, and parsing, we observe the largest improvement with n = 3. More improvement in under-performing languages is observed.</p><p>In each task, the feature-based approach with LSTM under-performs fine-tuning approach. We hypothesize that initialization from pretraining with lots of languages provides a very good starting point that is hard to beat. Additionally, the LSTM could also be part of the problem. In <ref type="bibr" target="#b0">Ahmad et al. (2019)</ref>   <ref type="table">6</ref>: Dependency parsing results by language (UAS/LAS). * denotes delexicalized parsing in the baseline. S and Z denotes supervised learning and zeroshot transfer. Bold and underline denotes best and second best. We order the languages by word order distance to English.</p><p>was worse than a transformer when transferring to languages with high word ordering distance to English.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Question #3: Does mBERT retain</head><p>language specific information?</p><p>mBERT may learn a cross-lingual representation by abstracting away from language-specific information, thus losing the ability to distinguish between languages. We test this by considering language identification: does mBERT retain language-specific information? We use WiLI-2018 <ref type="bibr" target="#b33">(Thoma, 2018)</ref>, which includes over 200 languages from Wikipedia. We keep only those languages included in mBERT, leaving 99 languages<ref type="foot" target="#foot_2">5</ref> . We take various layers of bag-of-words mBERT representation of the first two sentences of the test paragraph and add a linear classifier with softmax. We fix mBERT and train only the classifier the same as the featurebased approach in §5.2. All tested layers achieved around 96% accuracy (Fig. <ref type="figure">2</ref>), with no clear difference between layers. This suggests each layer contains language-specific information; surprising given the zero-shot crosslingual abilities. As mBERT generalizes its representations and creates cross-lingual representations, it maintains language-specific details. This may be encouraged during pretraining since mBERT needs to retain enough language-specific information to perform the cloze task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Question #4: Does mBERT benefit by sharing subwords across languages?</head><p>As discussed in §3, mBERT shares subwords in closely related languages or perhaps in distantly related languages. At training time, the representation of a shared subword is explicitly trained to contain enough information for the cloze task in all languages in which it appears. During fine-tuning for zero-shot cross-lingual transfer, if a subword in the target language test set also appears in the source language training data, the supervision could be leaked to the target language explicitly. However, all subwords interact in a non-interpretable way inside a deep network, and subword representations could overfit to the source language and potentially hurt transfer performance. In these experiments, we investigate how sharing subwords across languages effects cross-lingual transfer.    To quantify how many subwords are shared across languages in any task, we assume V en train is the set of all subwords in the English training set, V test is the set of all subwords in language test set, and c w is the count of subword w in test set of language . We then calculate the percentage of ob-served subwords at type-level p type and token-level p token for each target language .</p><formula xml:id="formula_6">p type = |V obs | |V test | × 100 p token = w∈V obs c w w∈V test c w ×<label>100</label></formula><p>where V obs = V en train ∩ V test . In Fig. <ref type="figure" target="#fig_4">3</ref>, we show the relation between crosslingual zero-shot transfer performance of mBERT and p type or p token for all five tasks with Pearson correlation. In four out of five tasks (not XNLI) we observed a strong positive correlation (p &lt; 0.05) with a correlation coefficient larger than 0.5. In Indo-European languages, we observed p token is usually around 50% to 75% while p type is usually less than 50%. This indicates that subwords shared across languages are usually high frequency 6 . We 6 With the data-dependent WordPiece algorithm, subwords q q qq q q q q R = 0.8 , p = 0.017 de en es fr it ja ru zh q q q q q q q q R = 0.7 , p = 0.053 de en es fr it ja ru zh q q q q q q q q q q q q q q q R = − 0.036 , p = 0.9 ar bg de el en es fr hi ru sw th tr ur vi zh q q q q q q q q q q q q q q q R = 0.36 , p = 0.18 ar bg de el en es fr hi ru sw th tr ur vi zh q q q q q R = 0.99 , p = 0.0014 de en es nl zh q q q q q R = 0.98 , p = 0.0045 de en es nl zh q q q q q q q q q q q q qq q R = 0.58 , p = 0.025 bg da de en es fa hu it nl pl pt ro sk sl sv q q q q q q q q q q q q q q q R = 0.55 , p = 0.035 bg da de en es fa hu it nl pl pt ro sk sl sv q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q R = 0.5 , p = 0.004 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q R = 0.6 , p = 0.00034 hypothesize that this could be used as a simple indicator for selecting source language in crosslingual transfer with mBERT. We leave this for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>We show mBERT does well in a cross-lingual zeroshot transfer setting on five different tasks covering a large number of languages. It outperforms crosslingual embeddings, which typically have more cross-lingual supervision. By fixing the bottom layers of mBERT during fine-tuning, we observe further performance gains. Language-specific information is preserved in all layers. Sharing subwords helps cross-lingual transfer; a strong correlation is observed between the percentage of overlapping subwords and transfer performance. mBERT effectively learns a good multilingual representation with strong cross-lingual zero-shot transfer performance in various tasks. We recommend building future multi-lingual NLP models on top of mBERT or other models pretrained similarly. Even without explicit cross-lingual supervision, these models do very well. As we show with XNLI in §5.1, while bitext is hard to obtain in low resource settings, a variant of mBERT pretrained with bitext <ref type="bibr" target="#b15">(Lample and Conneau, 2019)</ref> shows even stronger performance. Future work could investigate how to use weak supervision to produce a better cross-lingual mBERT, or adapt an already trained model for cross-lingual use. With POS tagging in §5.1, we show mBERT, in general, that appear in multiple languages with high frequency are more likely to be selected. under-performs models with a small amount of supervision while <ref type="bibr" target="#b8">Devlin et al. (2019)</ref> show that in English NLP tasks, fine-tuning BERT only needs a small amount of data. Future work could investigate when cross-lingual transfer is helpful in NLP tasks of low resource languages. With such strong cross-lingual NLP performance, it would be interesting to prob mBERT from a linguistic perspective in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>NER</head><label></label><figDesc>We use Xie et al. (2018) as a zero-shot cross-lingual transfer baseline, which is state-ofthe-art on CoNLL 2002 and 2003. It uses unsupervised bilingual word embeddings</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Performance of different fine-tuning approaches compared with fine-tuning all mBERT parameters. Color denotes absolute difference and number in each entry is the evaluation in the corresponding setting. Languages are sorted by mBERT zero-shot transfer performance. Three downward triangles indicate performance drop more than the legends lower limit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Relation between cross-lingual zero-shot transfer performance with mBERT and percentage of observed subwords at both type-level and token-level. Pearson correlation coefficient and p-value are shown in red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>MLDoc experiments. ♠ denotes the model is pretrained with bitext, and † denotes concurrent work. Bold and underline denote best and second best.</figDesc><table><row><cell>−5 };</cell></row></table><note>MLDoc We include two strong baselines.<ref type="bibr" target="#b30">Schwenk and Li (2018)</ref> use MultiCCA, multilingual word embeddings trained with a bilingual dictionary<ref type="bibr" target="#b1">(Ammar et al., 2016)</ref>, and convolution neural networks. Concurrent to our work,</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>XNLI experiments. ♠ denotes the model is pretrained with cross-lingual signal including bitext or bilingual dictionary, † denotes concurrent work, and ♦ denotes model selection with target language dev set.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>en</cell><cell>fr</cell><cell>es</cell><cell>de</cell><cell>el</cell><cell>bg</cell><cell>ru</cell><cell>tr</cell><cell>ar</cell><cell>vi</cell><cell>th</cell><cell>zh</cell><cell>hi</cell><cell>sw</cell><cell>ur</cell><cell>Average</cell></row><row><cell cols="11">Pseudo supervision with machine translated training data from English to target language</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Lample and Conneau (2019) (MLM+TLM) ♠  †</cell><cell cols="12">85.0 80.2 80.8 80.3 78.1 79.3 78.1 74.7 76.5 76.6 75.5 78.6 72.3 70.9 63.2</cell><cell>76.7</cell></row><row><cell>mBERT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="12">82.1 76.9 78.5 74.8 72.1 75.4 74.3 70.6 70.8 67.8 63.2 76.2 65.3 65.3 60.6</cell><cell>71.6</cell></row><row><cell cols="3">Zero-shot cross-lingual transfer</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Conneau et al. (2018) (X-LSTM) ♠ ♦</cell><cell></cell><cell></cell><cell cols="12">73.7 67.7 68.7 67.7 68.9 67.9 65.4 64.2 64.8 66.4 64.1 65.8 64.1 55.7 58.4</cell><cell>65.6</cell></row><row><cell cols="3">Artetxe and Schwenk (2018) ♠  †</cell><cell></cell><cell></cell><cell cols="12">73.9 71.9 72.9 72.6 73.1 74.2 71.5 69.7 71.4 72.0 69.2 71.4 65.5 62.2 61.0</cell><cell>70.2</cell></row><row><cell cols="17">Lample and Conneau (2019) (MLM+TLM) ♠ ♦  † 85.0 78.7 78.9 77.8 76.6 77.4 75.3 72.5 73.1 76.1 73.2 76.5 69.6 68.4 67.3</cell><cell>75.1</cell></row><row><cell cols="4">Lample and Conneau (2019) (MLM) ♦  †</cell><cell></cell><cell cols="12">83.2 76.5 76.3 74.2 73.1 74.0 73.1 67.8 68.5 71.2 69.2 71.9 65.7 64.6 63.4</cell><cell>71.5</cell></row><row><cell>mBERT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="12">82.1 73.8 74.3 71.1 66.4 68.9 69.0 61.6 64.9 69.5 55.8 69.3 60.0 50.4 58.0</cell><cell>66.3</cell></row><row><cell></cell><cell>en</cell><cell>nl</cell><cell>es</cell><cell>de</cell><cell>zh</cell><cell cols="3">Average (-en,-zh)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">In language supervised learning</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Xie et al. (2018)</cell><cell>-</cell><cell cols="3">86.40 86.26 78.16</cell><cell>-</cell><cell></cell><cell>83.61</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>mBERT</cell><cell cols="5">91.97 90.94 87.38 82.82 93.17</cell><cell></cell><cell>87.05</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Zero-shot cross-lingual transfer</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Xie et al. (2018) ♦</cell><cell>-</cell><cell cols="3">71.25 72.37 57.76</cell><cell>-</cell><cell></cell><cell>67.13</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>mBERT</cell><cell cols="5">91.97 77.57 74.96 69.56 51.90</cell><cell></cell><cell>74.03</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>NER tagging experiments. ♦ denotes model selection with target language dev set.</figDesc><table><row><cell>line on average by 7.3 point UAS and 0.4 point</cell></row><row><cell>LAS absolute improvement even without gold POS</cell></row><row><cell>tags. Note in practice, gold POS tags are not always</cell></row><row><cell>available, especially for low resource languages. In-</cell></row><row><cell>terestingly, the LAS of mBERT tends to weaker</cell></row><row><cell>than the baseline in languages with less word order</cell></row><row><cell>distance, in other words, more closely related to</cell></row><row><cell>English. With the help of gold POS tags, we further</cell></row><row><cell>observe 1.6 points UAS and 4.7 point LAS absolute</cell></row><row><cell>improvement on average. It appears that adding</cell></row><row><cell>gold POS tags, which provide clearer cross-lingual</cell></row><row><cell>representations, benefit mBERT.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>for dependency parsing, an LSTM encoder POS tagging.<ref type="bibr" target="#b13">Kim et al. (2017)</ref> use small amounts of training data in the target language.</figDesc><table><row><cell>lang</cell><cell></cell><cell>bg</cell><cell>da</cell><cell>de</cell><cell>en</cell><cell>es</cell><cell>fa</cell><cell>hu</cell><cell>it</cell><cell>nl</cell><cell>pl</cell><cell>pt</cell><cell>ro</cell><cell>sk</cell><cell>sl</cell><cell>sv</cell><cell>Average (-en)</cell></row><row><cell cols="3">In language supervised learning</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>mBERT</cell><cell></cell><cell cols="15">99.0 97.9 95.2 97.1 97.1 97.8 96.9 98.7 92.1 98.5 98.3 97.8 97.0 98.9 98.4</cell><cell>97.4</cell></row><row><cell cols="4">Low resource cross-lingual transfer</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Kim et al. (2017) (1280) 95.7 94.3 90.7</cell><cell>-</cell><cell cols="11">93.4 94.8 94.5 95.9 85.8 92.1 95.5 94.2 90.0 94.1 94.6</cell><cell>93.3</cell></row><row><cell cols="2">Kim et al. (2017) (320)</cell><cell cols="3">92.4 90.8 89.7</cell><cell>-</cell><cell cols="11">90.9 91.8 90.7 94.0 82.2 85.5 94.2 91.4 83.2 90.6 90.7</cell><cell>89.9</cell></row><row><cell cols="3">Zero-shot cross-lingual transfer</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>mBERT</cell><cell></cell><cell cols="15">87.4 88.3 89.8 97.1 85.2 72.8 83.2 84.7 75.9 86.9 82.1 84.7 83.6 84.2 91.3</cell><cell>84.3</cell></row><row><cell></cell><cell cols="7">Dist mBERT(S) Baseline(Z) mBERT(Z) mBERT(Z+POS)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>en</cell><cell>0.00 91.5/81.3</cell><cell cols="2">90.4/88.4</cell><cell cols="2">91.5/81.3</cell><cell cols="2">91.8/82.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>no</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>it fr sv no de hr es nl da ca pt sk bg uk pl cs ro ru sl he fi id et lv hi ar ko zh la ja AVER .9 69.4 67.6 66.4 63.6 63.1 62.3 62.9 62.7 61.3 62.6 59.7 60.0 58.0 58.7 55.6 55.8 56.9 49.6 46.0 45.5 42.8 41.5 41.1 25.8 33.1 25.1 25.5 29.4 13.6 52.1 81.3 75.4 73.3 71.5 69.2 67.0 66.3 65.8 65.6 64.9 65.6 65.4 64.3 62.7 60.9 60.0 59.4 59.4 59.1 51.9 47.5 47.9 46.7 44.7 41.4 33.8 29.2 28.6 27.2 26.4 16.1 54.8 81.3 76.3 73.8 72.3 69.9 67.3 66.9 66.3 65.9 65.7 66.1 65.7 64.6 64.0 61.0 62.0 60.0 60.4 59.6 54.2 47.8 49.6 47.8 46.8 43.7 33.0 28.9 28.3 27.7 30.5 15.7 55.6 80.3 75.2 72.4 71.2 69.1 66.0 66.0 64.4 65.1 64.8 63.8 64.6 64.1 63.7 59.8 62.2 59.9 59.9 58.6 53.9 46.9 48.2 44.4 45.5 44.0 31.5 29.5 25.8 26.3 30.8 15.0 54.6</figDesc><table><row><cell></cell><cell>Lay 9 Lay 6 Lay 3 Lay 0 Feat</cell><cell>en sv de da bg pl es it 96.1 89.7 86.1 86.7 86.2 83.4 83.0 82.4 83.1 82.2 81.5 81.9 80.9 75.5 67.8 83.1 ro sl sk hu pt nl fa AVER 96.6 91.3 89.4 88.1 87.6 86.9 85.2 85.1 84.7 84.7 84.4 82.9 82.3 76.0 71.4 85.1 96.9 91.5 89.9 88.4 87.2 87.1 85.5 85.2 84.9 84.1 83.1 82.8 82.7 75.8 72.8 85.2 97.0 91.3 89.2 88.4 86.9 85.1 84.4 84.4 83.7 83.7 83.6 83.0 81.6 75.2 71.3 84.6 96.7 90.4 86.4 87.8 85.8 82.2 83.9 82.1 81.7 82.2 82.4 82.4 81.4 75.2 68.7 83.3</cell><cell>-0.8 -0.5 -0.2 0.2 0.5 0.8</cell></row><row><cell></cell><cell></cell><cell>(d) POS tagging (ACC)</cell></row><row><cell>Feat Lay 0 Lay 3 Lay 6 Lay 9</cell><cell cols="2">77.5 717</cell><cell>-4.7 -2.8 -0.9 0.9 2.8 4.7</cell></row></table><note>en 76.6 67.7 65.3 66.6 65.5 60.3 56.1 57.9 61.1 60.9 56.8 59.6 56.4 58.9 49.7 55.8 51.4 52.4 50.8 48.1 41.2 42.6 36.9 39.1 39.3 25.8 25.4 21.5 22.1 26.1 12.2 48.(e) Dependency parsing (LAS)</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">We only use the first sentence if the document only contains one sentence. Documents are segmented into sentences with NLTK<ref type="bibr" target="#b23">(Perkins, 2014)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1">They also use language embeddings as input and exclude the next sentence classification objective</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2">Hungarian, Western-Punjabi, Norwegian-Bokmal, and Piedmontese are not covered by WiLI.</note>
		</body>
		<back>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>0.06 93.6/85.9 80.8/72.8 80.6/68.9 82.7/72.1 sv 0.07 91.2/83.1 81.0/73.2 82.5/71.2 84.3/73.7 fr 0.09 91.7/85.4 77.9/72.8 82.7/72.7 83.8/76.2 pt 0.09 93.2/87.2 76.6/67.8 77.1/64.0 78.3/66.9 da 0.10 89.5/81.9 76.6/67.9 77.4/64.7 79.3/68.1 es 0.12 92.3/86.5 74.5/66.4 78.1/64.9 79.0/68.9 it 0.12 94.8/88.7 80.8/75.8 84.6/74.4 86.0/77.8 ca 0.13 94.3/89.5 73.8/65.1 78.1/64.6 79.0/67.9 hr 0.13 92.4/83.8 61.9/52.9 80.7/65.8 80.4/68.2 pl 0.13 94.7/79.9 74.6/62.2 82.8/59.4 85.7/65.4 sl 0.13 88.0/77.8 68.2/56.5 72.6/51.4 75.9/59.2 uk 0.13 90.6/83.4 60.1/52.3 76.7/60.0 76.5/65.5 bg 0.14 95.2/85.5 79.4/68.2 83.3/62.3 84.4/68.1 cs 0.14 94.2/86.6 63.1/53.8 76.6/58.7 77.4/63.6 de 0.14 86.1/76.5 71.3/61.6 80.4/66.3 83.5/71.2 he 0.14 91.9/83.6 55.3/48.0 67.5/48.4 67.0/54.3 nl 0.14 94.0/85.0 68.6/60.3 78.0/64.8 79.9/67.1 ru 0.14 94.7/88.0 60.6/51.6 73.6/58.5 73.2/61.5 ro 0.15 92.2/83.2 65.1/54.1 77.0/58.5 76.9/62.6 id 0.17 86.3/75.4 49.2/43.5 62.6/45.6 59.8/48.6 sk 0.17 93.8/83.3 66.7/58.2 82.7/63.9 82.9/67.8 lv 0.18 87.3/75.3 70.8/49.3 66.0/41.4 70.4/48.5 et 0.20 88.8/79.7 65.7/44.9 66.9/44.3 70.8/50.7 fi 0.20 91.3/81.8 66.3/48.7 68.4/47.5 71.4/52.5 zh* 0.23 88.3/81.2 42.5/25.1 53.8/26.8 53.4/29.0 ar 0.26 87.6/80.6 38.1/28.0 43.9/28.3 44.7/32.9 la 0.28 85.2/73.1 48.0/35.2 47.9/26.1 50.9/32.2 ko 0.33 86.0/74.8 34.5/16.4 52.7/27.5 52.3/29.4 hi 0.40 94.8/86.7 35.5/26.5 49.8/33.2 58.9/44.0 ja* 0.49 94.2/87.4 28.2/20.9 36.6/15.7 41.3/30.9 AVER 0.17 91.3/82.6 64.1/53.8 71.4/54.2 73.0/58.9</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On difficulties of cross-lingual transfer with order differences: A case study on dependency parsing</title>
		<author>
			<persName><forename type="first">Wasi</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhisong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1253</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2440" to="2452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Mulcaire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.01925</idno>
		<title level="m">Massively multilingual word embeddings</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings</title>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1073</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="789" to="798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Massively multilingual sentence embeddings for zeroshot cross-lingual transfer and beyond</title>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.10464</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lei Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Ryan Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<title level="m">Layer normalization</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><forename type="middle">'</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.04087</idno>
		<title level="m">Word translation without parallel data</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">XNLI: Evaluating cross-lingual sentence representations</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruty</forename><surname>Rinott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1269</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2475" to="2485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Multilingual bert readme document</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01734</idno>
		<title level="m">Deep biaffine attention for neural dependency parsing</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<title level="m">Bridging nonlinearities and stochastic regularizers with gaussian error linear units</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Universal language model fine-tuning for text classification</title>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1031</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="328" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cross-lingual transfer learning for POS tagging without cross-lingual resources</title>
		<author>
			<persName><forename type="first">Joo-Kyung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Young-Bum</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Fosler-Lussier</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1302</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2832" to="2838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The third international Chinese language processing bakeoff: Word segmentation and named entity recognition</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.07291</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing</title>
				<meeting>the Fifth SIGHAN Workshop on Chinese Language Processing<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006">2019. 2006</date>
			<biblScope unit="page" from="108" to="117" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Crosslingual language model pretraining</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<title level="m">Efficient estimation of word representations in vector space</title>
				<imprint>
			<date type="published" when="2013">2013a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2013">2013b</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Polyglot contextual representations improve crosslingual transfer</title>
		<author>
			<persName><forename type="first">Phoebe</forename><surname>Mulcaire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungo</forename><surname>Kasai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1392</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3912" to="3918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Agnieszka Patejuk, Siyao Peng, Cenel-Augusto Perez, Guy Perrier</title>
		<author>
			<persName><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Abrams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Željko</forename><surname>Agić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Ahrenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lene</forename><surname>Antonsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Jesus Aranzabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gashaw</forename><surname>Arutie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masayuki</forename><surname>Asahara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luma</forename><surname>Ateyah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Attia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitziber</forename><surname>Atutxa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liesbeth</forename><surname>Augustinus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Badmaeva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esha</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Bank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barbu</forename><surname>Verginica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Mititelu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandra</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kepa</forename><surname>Bellato</surname></persName>
		</author>
		<author>
			<persName><surname>Bengoetxea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Riyaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erica</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eckhard</forename><surname>Biagetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rogier</forename><surname>Bick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victoria</forename><surname>Blokland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Bobicev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristina</forename><surname>Börstell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gosse</forename><surname>Bosco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Bouma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriane</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aljoscha</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie</forename><surname>Burchardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Candito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gauthier</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Caron ; Giuseppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Savas</forename><surname>Celano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabricio</forename><surname>Cetin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinho</forename><surname>Chalub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongseok</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jayeol</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvie</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurélie</forename><surname>Cinková</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Collomb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miriam</forename><surname>¸agrı C ¸öltekin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marine</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Courtin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carly</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Dickerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaja</forename><surname>Dirix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Dobrovoljc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kira</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Puneet</forename><surname>Droganova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marhaba</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Eli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binyam</forename><surname>Elkahky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomaž</forename><surname>Ephrem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aline</forename><surname>Erjavec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richárd</forename><surname>Etienne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hector</forename><surname>Farkas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Fernandez Alcalde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cláudia</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katarína</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Gajdošová</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcos</forename><surname>Galbraith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moa</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename><surname>Gärdenfors</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filip</forename><surname>Gerdes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iakes</forename><surname>Ginter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koldo</forename><surname>Goenaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Memduh</forename><surname>Gojenola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Gökırmak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Berta</forename><forename type="middle">Gonzáles</forename><surname>Gómez Guinovart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matias</forename><surname>Saavedra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Normunds</forename><surname>Grioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Grūzītis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Céline</forename><surname>Guillaume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nizar</forename><surname>Guillot-Barbance</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Habash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Hajič</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linh</forename><surname>Hajič Jr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Na-Rae</forename><surname>Hà Mỹ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dag</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barbora</forename><surname>Haug</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaroslava</forename><surname>Hladká</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florinel</forename><surname>Hlaváčová</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petter</forename><surname>Hociung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jena</forename><surname>Hohle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Ion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomáš</forename><surname>Irimia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anders</forename><surname>Jelínek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fredrik</forename><surname>Johannsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hüner</forename><surname>Jørgensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Kas ¸ıkara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroshi</forename><surname>Kahane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenna</forename><surname>Kanayama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tolga</forename><surname>Kanerva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Václava</forename><surname>Kayadelen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Kettnerová</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Kirchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kotsyba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sookyoung</forename><surname>Krek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veronika</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Laippala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatiana</forename><surname>Lambertino</surname></persName>
		</author>
		<author>
			<persName><surname>Lando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dian</forename><surname>Septina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Larasati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Lavrentiev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phng</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Lê H`ông</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saran</forename><surname>Lenci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herman</forename><surname>Lertpradit</surname></persName>
		</author>
		<author>
			<persName><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Cheuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keying</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyungtae</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikola</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Ljubešić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Loginova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teresa</forename><surname>Lyashevskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivien</forename><surname>Lynn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aibek</forename><surname>Macketanz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Makazhanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Mandl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruli</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cȃtȃlina</forename><surname>Manurung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Mȃrȃnduc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katrin</forename><surname>Mareček</surname></persName>
		</author>
		<author>
			<persName><surname>Marheinecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martínez</forename><surname>Héctor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">André</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuji</forename><surname>Mašek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Matsumoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustavo</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niko</forename><surname>Mendonc ¸a</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Miekka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cȃtȃlin</forename><surname>Missilä</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusuke</forename><surname>Mititelu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simonetta</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Montemagni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><forename type="middle">Moreno</forename><surname>More</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shinsuke</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bjartur</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bohdan</forename><surname>Mortensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kadri</forename><surname>Moskalevskyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yugo</forename><surname>Muischnek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaili</forename><surname>Murawaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pinkey</forename><surname>Müürisep</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><surname>Nainwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ignacio</forename><forename type="middle">Navarro</forename><surname>Horñiacek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Nedoluzhko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunta</forename><surname>Nešpore-Bērzkalne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lng</forename><surname>Nguy˜ên Thi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nguy˜ên</forename><surname>Huy`ên</surname></persName>
		</author>
		<author>
			<persName><surname>Thi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitaly</forename><surname>Minh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rattima</forename><surname>Nikolaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanna</forename><surname>Nitisaroj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stina</forename><surname>Nurmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adédayo</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mai</forename><surname>Olúòkun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petya</forename><surname>Omura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Osenova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lilja</forename><surname>Östling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niko</forename><surname>Øvrelid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Partanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName><surname>Passarotti ; Mojgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mo</forename><surname>Seraji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atsuko</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muh</forename><surname>Shimada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Shohibussirri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Sichinava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Silveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Simi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katalin</forename><surname>Simionescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mária</forename><surname>Simkó</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiril</forename><surname>Šimková</surname></persName>
		</author>
		<author>
			<persName><surname>Simov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thierry Poibeau, Martin Popel, Lauma Pretkalnin ¸a, Sophie Prévost, Prokopis Prokopidis, Adam Przepiórkowski, Tiina Puolakainen, Sampo Pyysalo, Andriela Rääbis, Alexandre Rademaker, Loganathan Ramasamy, Taraka Rama, Carlos Ramisch</title>
				<editor>
			<persName><forename type="first">Livy</forename><surname>Vinit Ravishankar</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Siva</forename><surname>Real</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Georg</forename><surname>Reddy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Michael</forename><surname>Rehm</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Larissa</forename><surname>Rießler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Laura</forename><surname>Rinaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Luisa</forename><surname>Rituma</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mykhailo</forename><surname>Rocha</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rudolf</forename><surname>Romanenko</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Davide</forename><surname>Rosa</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Valentin</forename><surname>Rovati</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Olga</forename><surname>Roca</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Shoval</forename><surname>Rudina</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Shadi</forename><surname>Sadde</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tanja</forename><surname>Saleh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Stephanie</forename><surname>Samardžić</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Manuela</forename><surname>Samson</surname></persName>
		</editor>
		<editor>
			<persName><surname>Sanguinetti</surname></persName>
		</editor>
		<meeting><address><addrLine>Slav Petrov, Jussi Piitulainen, Emily Pitler, Barbara Plank; Baiba Saulīte, Yanin Sawanakunanon, Nathan Schneider, Sebastian Schuster, Djamé Seddah, Wolfgang Seeker; Aaron Smith, Isabela Soares-Bastos, Antonio Stella, Milan Straka; Anna Trukhina, Reut Tsarfaty, Francis Tyers; Lars Wallin, Jonathan North Washington</addrLine></address></meeting>
		<imprint>
			<publisher>Gertjan van Noord, Viktor Varga, Veronika Vincze</publisher>
		</imprint>
		<respStmt>
			<orgName>Marie-Catherine de Marneffe, Valeria de Paiva, Arantza Diaz de Ilarraza, ; Jana Strnadová, Alane Suhr, Umut Sulubacak, Zsolt Szántó, Dima Taji, Yuta Takahashi, Takaaki Tanaka, Isabelle Tellier, Trond Trosterud ; Charles University</orgName>
		</respStmt>
	</monogr>
	<note>Faculty of Mathematics and Physics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Željko</forename><surname>Agić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Ahrenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Jesus Aranzabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masayuki</forename><surname>Asahara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitziber</forename><surname>Atutxa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kepa</forename><surname>Bengoetxea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yevgeni</forename><surname>Berzak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Riyaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eckhard</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Bick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristina</forename><surname>Börstell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gosse</forename><surname>Bosco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Bouma</surname></persName>
		</author>
		<author>
			<persName><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><surname>Güls ¸en Cebiroglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giuseppe</forename><forename type="middle">G A</forename><surname>Eryigit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabricio</forename><surname>Celano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chalub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miriam</forename><surname>¸agrı C ¸öltekin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Catherine</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantza</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaja</forename><surname>Diaz De Ilarraza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Dobrovoljc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kira</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Puneet</forename><surname>Droganova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marhaba</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomaž</forename><surname>Eli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richárd</forename><surname>Erjavec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Farkas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudia</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katarína</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Gajdošová</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcos</forename><surname>Galbraith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moa</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gärdenfors</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filip</forename><surname>Garza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iakes</forename><surname>Ginter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koldo</forename><surname>Goenaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Memduh</forename><surname>Gojenola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Gökırmak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Berta</forename><forename type="middle">Gonzáles</forename><surname>Gómez Guinovart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matias</forename><surname>Saavedra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Normunds</forename><surname>Grioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Grūzītis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Guillaume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linh</forename><surname>Hajič</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dag</forename><surname>Hà Mỹ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barbora</forename><surname>Haug</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Hladká</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Ion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anders</forename><surname>Irimia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fredrik</forename><surname>Johannsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hüner</forename><surname>Jørgensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroshi</forename><surname>Kas ¸ıkara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenna</forename><surname>Kanayama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Kanerva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Kenney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kotsyba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veronika</forename><surname>Krek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Laippala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phng</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Lê H`ông</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikola</forename><surname>Lenci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Ljubešić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teresa</forename><surname>Lyashevskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aibek</forename><surname>Lynn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Makazhanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cȃtȃlina</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Mȃrȃnduc</surname></persName>
		</author>
		<author>
			<persName><surname>Mareček</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martínez</forename><surname>Héctor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">André</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuji</forename><surname>Mašek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Matsumoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Verginica</forename><surname>Missilä</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusuke</forename><surname>Mititelu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simonetta</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keiko</forename><forename type="middle">Sophie</forename><surname>Montemagni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shunsuke</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bohdan</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kadri</forename><surname>Moskalevskyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nina</forename><surname>Muischnek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaili</forename><surname>Mustafina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lng</forename><surname>Müürisep</surname></persName>
		</author>
		<author>
			<persName><surname>Nguy˜ên Thi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nguy˜ên</forename><surname>Huy`ên</surname></persName>
		</author>
		<author>
			<persName><surname>Thi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitaly</forename><surname>Minh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanna</forename><surname>Nikolaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petya</forename><surname>Nurmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Osenova</surname></persName>
		</author>
		<author>
			<persName><surname>Östling</surname></persName>
		</author>
		<title level="m">Lauma Pretkalnin ¸a, Prokopis Prokopidis, Tiina Puolakainen, Sampo Pyysalo, Alexandre Rademaker, Loganathan Ramasamy, Livy Real</title>
				<editor>
			<persName><forename type="first">Laura</forename><surname>Rituma</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rudolf</forename><surname>Rosa</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Shadi</forename><surname>Saleh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Baiba</forename><surname>Saulīte</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sebastian</forename><surname>Schuster</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Wolfgang</forename><surname>Seeker</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mojgan</forename><surname>Seraji</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Lena</forename><surname>Shakurova</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mo</forename><surname>Shen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Natalia</forename><surname>Silveira</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Maria</forename><surname>Simi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Radu</forename><surname>Simionescu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Katalin</forename><surname>Simkó</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mária</forename><surname>Šimková</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kiril</forename><surname>Simov</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Aaron</forename><surname>Smith</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Carolyn</forename><surname>Spadine</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alane</forename><surname>Suhr</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Umut</forename><surname>Sulubacak</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Zsolt</forename><surname>Szántó</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Takaaki</forename><surname>Tanaka</surname></persName>
		</editor>
		<meeting><address><addrLine>Valeria Paiva, Elena Pascual, Marco Passarotti, Cenel-Augusto Perez, Slav Petrov, Jussi Piitulainen, Barbara Plank, Martin Popel; Reut Tsarfaty, Francis Tyers; Daniel Zeman</addrLine></address></meeting>
		<imprint>
			<publisher>Amir Zeldes</publisher>
		</imprint>
		<respStmt>
			<orgName>Sumire Uematsu, Larraitz Uria, Gertjan van Noord, Viktor Varga, Veronika Vincze, Lars Wallin, Jing Xian Wang, Jonathan North Washington, Mats Wirén, Zdeněk Žabokrtský ; Charles University</orgName>
		</respStmt>
	</monogr>
	<note>Faculty of Mathematics and Physics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName><forename type="first">Jialin</forename><surname>Sinno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1162</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Python 3 text processing with NLTK 3 cookbook</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Perkins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Packt Publishing Ltd</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1202</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">How multilingual is multilingual BERT?</title>
		<author>
			<persName><forename type="first">Telmo</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eva</forename><surname>Schlinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Garrette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4996" to="5001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1264</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Vulić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.04902</idno>
		<title level="m">A survey of cross-lingual word embedding models</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cross-lingual alignment of contextual word embeddings, with applications to zeroshot dependency parsing</title>
		<author>
			<persName><forename type="first">Tal</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ori</forename><surname>Ram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Globerson</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1162</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1599" to="1613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A corpus for multilingual document classification in eight languages</title>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC-2018)</title>
				<meeting>the Eleventh International Conference on Language Resources and Evaluation (LREC-2018)<address><addrLine>Miyazaki, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>European Languages Resources Association (ELRA</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">David</forename><surname>Samuel L Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Hp Turban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nils</forename><forename type="middle">Y</forename><surname>Hamblin</surname></persName>
		</author>
		<author>
			<persName><surname>Hammerla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.03859</idno>
		<title level="m">Offline bilingual word vectors, orthogonal transformations and the inverted softmax</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">cloze procedure: A new tool for measuring readability</title>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journalism Bulletin</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="415" to="433" />
			<date type="published" when="1953">1953</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">The wili benchmark dataset for written language identification</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Thoma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07779</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2002 shared task: Language-independent named entity recognition</title>
		<author>
			<persName><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING-02: The 6th Conference on Natural Language Learning</title>
				<imprint>
			<date type="published" when="2002">2002. 2002. CoNLL-2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fien</forename><surname>De Meulder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003</title>
				<meeting>the Seventh Conference on Natural Language Learning at HLT-NAACL 2003</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-5446</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 EMNLP Workshop Black-boxNLP: Analyzing and Interpreting Neural Networks for NLP</title>
				<meeting>the 2018 EMNLP Workshop Black-boxNLP: Analyzing and Interpreting Neural Networks for NLP<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="353" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<title level="m">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Neural crosslingual named entity recognition with minimal resources</title>
		<author>
			<persName><forename type="first">Jiateng</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1034</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="369" to="379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hod</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3320" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Crosslanguage parser adaptation between related languages</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Zeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IJCNLP-08 Workshop on NLP for Less Privileged Languages</title>
				<meeting>the IJCNLP-08 Workshop on NLP for Less Privileged Languages</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
