<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Branch Prediction and the Performance of Interpreters -Don&apos;t Trust Folklore</title>
				<funder ref="#_G2EjSdT">
					<orgName type="full">European Research Council Advanced</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Erven</forename><surname>Rohou</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Inria</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bharath</forename><surname>Narasimha</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Inria</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Swamy</forename><surname>Andr?</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Inria</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Branch Prediction and the Performance of Interpreters -Don&apos;t Trust Folklore</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Interpreters have been used in many contexts. They provide portability and ease of development at the expense of performance. The literature of the past decade covers analysis of why interpreters are slow, and many software techniques to improve them. A large proportion of these works focuses on the dispatch loop, and in particular on the implementation of the switch statement: typically an indirect branch instruction. Folklore attributes a significant penalty to this branch, due to its high misprediction rate. We revisit this assumption, considering state-of-the-art branch predictors and the three most recent Intel processor generations on current interpreters. Using both hardware counters on Haswell, the latest Intel processor generation, and simulation of the IT-TAGE, we show that the accuracy of indirect branch prediction is no longer critical for interpreters. We further compare the characteristics of these interpreters and analyze why the indirect branch is less important than before.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Interpreters go back to the infancy of computer science. At some point, just-in-time (JIT) compilation technology matured enough to deliver better performance, and was made popular by Java <ref type="bibr" target="#b5">[6]</ref>. Writing a JIT compiler, though, is a complicated task. Conversely, interpreters provide ease of implementation, and portability, at the expense of speed.</p><p>Interpreters are still widely used. They are much easier to develop, maintain, and port applications on new architectures. Some languages used by domain scientists are executed mainly through interpreters, e.g. R, Python, Matlab... Some properties of widely adopted languages, such as dynamic typing, also make it more difficult to develop efficient JIT compilers. These dynamic features turn out to be heavily used in real applications <ref type="bibr" target="#b24">[25]</ref>. On lower-end systems, where short time-to-market is key, JIT compilers may also not be commercially viable, and they rely on interpreters.</p><p>Scientists from both CERN and Fermilab report <ref type="bibr" target="#b22">[23]</ref> that "many of LHC experiments' algorithms are both designed and used in interpreters". As another example, the need for an interpreter is also one of the three reasons motivating the choice of Jython for the data analysis software of the Herschel Space Observatory <ref type="bibr" target="#b35">[36]</ref>. Scientists at CERN also developed an interpreter for C/C++ <ref type="bibr" target="#b6">[7]</ref>.</p><p>Although they are designed for portability, interpreters are often large and complex codes. Part of this is due to the need for performance. The core of an interpreter is an infinite loop that reads the next bytecode, decodes it, and performs the appropriate action. Naive decoding implemented in C consists in a large switch statement (see Figure <ref type="figure" target="#fig_0">1 (a)</ref>), that gets translated into a jump table and an indirect jump. Conventional wisdom states that this indirect jump incurs a major performance degradation on deeply pipelined architectures because it is hardly predictable (see Section 6 for related work).</p><p>The contributions of this paper are the following.</p><p>? We revisit the performance of switch-based interpreters, focusing on the impact the indirect branch instruction, on the most recent Intel processor generations(Nehalem, Sandy Bridge and Haswell) and current interpreted languages (Python, Javascript, CLI). Our experiments and measures show that on the latest processor generation, the performance of the predictors and the characteristics of interpreters make the indirect branch much less critical than before. The global branch misprediction rate observed when executing interpreters drops from a dramatic 12-20 MPKI range on Nehalem to a only 0.5-2 MPKI range on Haswell.</p><p>? We evaluate the performance of a state-of-the-art indirect branch predictor, ITTAGE <ref type="bibr" target="#b30">[31]</ref>, proposed in the literature on the same interpreters, and we show that, when executing interpreters, the branch prediction accuracy observed on Haswell and on ITTAGE are in the same range.</p><p>The rest of this paper is organized as follows. Section 2 motivates our work: it analyzes in more details the performance of switch-based interpreters, it introduces jump threading, and measures its impact using current interpreters. Section 3 reviews the evolution of branch prediction over the last decades, and presents the state-of-the-art branch predictors TAGE for conditional branches and ITTAGE <ref type="bibr" target="#b30">[31]</ref> for indirect branches. Section 4 presents experimental setup. In Section 5, we present our experimental results and our findings on branch prediction impact on interpreter performance. Section 6 reviews related work. <ref type="bibr">Section</ref>   Terminology This work is concerned with interpreters executing bytecodes. This bytecode can be generated statically before actual execution (as in the Java model), just before execution (as in Python, which loads a pyc file when already present, and generates it when missing), or at runtime (as in Javascript). What is important is that the execution system is not concerned with parsing, the code is already available in a simple intermediate representation. Throughout this paper, the word bytecode refers to a virtual instruction (including operands), while opcode is the numerical value of the bytecode (usually encoded as a C enum type). Instructions refer to native instructions of the processor. The virtual program counter is the pointer to the current executed bytecode. Dispatch is the set of instructions needed to fetch the next bytecode, decode it (figure out that it is e.g. an add) and jump to the chunk of code that implements its semantics, i.e. the payload (the code that adds of the appropriate values).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Performance of Interpreters</head><p>Quoting Cramer et al. <ref type="bibr" target="#b5">[6]</ref>, Interpreting bytecodes is slow.</p><p>An interpreter is basically an infinite loop that fetches, decodes, and executes bytecodes, one after the other. Figure <ref type="figure" target="#fig_0">1</ref> (a) illustrates a very simplified interpreter written in C, still retaining some key characteristics. Lines 1 and 13 implement the infinite loop. Line 2 fetches the next bytecode from the address stored in virtual program counter vpc, and increments vpc (in case the bytecode is a jump, vpc must be handled separately). Decoding is typically implemented as a large switch statement, starting at line 3. Many interpreted languages implement an evaluation stack. This is the case of Java, CLI, Python (Dalvik is a notable exception <ref type="bibr" target="#b8">[9]</ref>). Lines 5-7 illustrate an add</p><p>The principal overhead of interpreters comes from the execution of the dispatch loop. Every bytecode typically requires ten instructions when compiled directly from standard C (as measured on our own interpreter compiled for x86, described in Section 5.2.3). See Figure <ref type="figure" target="#fig_0">1</ref> (b) for a possible translation of the main loop to x86. This compares to the single native instruction needed for most bytecodes when the bytecode is JIT compiled. Additional costs come from the implementation of the switch statement. All compilers we tried (GCC, icc, LLVM) generate a jump table and an indirect jump instruction (line 7 of Figure <ref type="figure" target="#fig_0">1 (b)</ref>). This jump has hundreds of potential targets, and has been previously reported to be difficult to predict <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b18">19]</ref>, resulting typically in an additional 20 cycle penalty. Finally, operands must be retrieved from the evaluation stack, and results stored back to it (lines 9-10) and the stack adjusted (line 11), while native code would have operands in registers in most cases (when not spilled by the register allocator).</p><p>A minor overhead consists in two instructions that compare the opcode value read from memory with the range of valid bytecodes before accessing the jump table (lines <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref>. By construction of a valid interpreter, values must be within the valid range, but a compiler does not have enough information to prove this. However, any simple branch prediction will correctly predict this branch.</p><p>This paper addresses the part of the overhead due to the indirect branches. We revisit previous work on the predictability of the branch instructions in interpreters, and the techniques proposed to address this cost. Other optimizations related to optimizing the dispatch loop are briefly reviewed in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Jump threading</head><p>As mentioned, a significant part of the overhead of the dispatch loop is thought to come from the poorly predicted indirect jump that implements the switch statement. Jump threading is the name of an optimization that addresses this cost. It basically bypasses the mechanism of the switch, and jumps from one case entry to the next. Figure <ref type="figure" target="#fig_2">2</ref> illustrates how this can be written. Jump threading, though, cannot be implemented in standard C. It is commonly implemented with the GNU extension named Labels as Values<ref type="foot" target="#foot_0">1</ref> . And while many compilers now support this extensions (in particular, we checked GCC, icc, LLVM), older versions and proprietary, processor specific compilers may not support it.</p><p>The intuition behind the optimization derives from increased branch correlation: firstly, a single indirect jump with many targets is now replaced by many jumps; secondly, each jump is more likely to capture a repeating sequence, simply because application bytecode has patterns (e.g. a compare is often followed by a jump).</p><p>Many interpreters check if this extension is available in the compiler to decide whether to exploit it, or to revert to the classical switch-based implementation. Examples include Javascript and Python, discussed in this paper. Previous work <ref type="bibr" target="#b11">[12]</ref> reports that it is also the case for Ocaml, YAP and Prolog. This double implementation, however, results in cumbersome code, #ifdefs, as well as the need to disable several code transformations that could de-optimize it (the source code of Python mentions global common subexpression elimination and cross-jumping).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Motivation Example</head><p>Current versions of Python-3 and Javascript automatically take advantage of threaded code when supported by the compiler. The implementation consists in two versions (plain switch and threaded code), one of them being selected at compile time, based on compiler support for the Labels as Values extension. Threaded code can be easily disabled though the configure script or a #define.</p><p>In 2001, Ertl and Gregg <ref type="bibr" target="#b10">[11]</ref> observed that:</p><p>"for current branch predictors, threaded code interpreters cause fewer mispredictions, and are almost twice as fast as switch based interpreters on modern superscalar architectures".</p><p>The current source code of Python-3 also says:</p><p>"At the time of this writing, the threaded code version is up to 15-20% faster than the normal switch version, depending on the compiler and the CPU architecture."</p><p>We tracked this comment back to January 2009. We experimented with Python-3.3.2, both with and without threaded code, and the Unladen Swallow benchmarks (selecting only the benchmarks compatible with Python 2 and Python 3, with flag -b 2n3). Figure <ref type="figure">3</ref> (a) shows the performance improvement due to threaded code on three microarchitectures: Nehalem, Sandy Bridge, and Haswell.</p><p>Nehalem shows a few outstanding speedups (in the 30 %-40 % range), as well as Sandy Bridge to a lesser extent, but the average speedups (geomean of individual speedups) for Nehalem, Sandy Bridge, and Haswell are respectively 10.1 %, 4.2 %, and 2.8 % with a few outstanding values for each microarchitecture. The benefits of threaded code decreases with each new generation of microarchitecture.</p><p>Python-2 also supports a limited version of threading, implemented in standard C, aimed at the most frequent pairs of successive opcodes. Nine pairs of opcodes are identified and hard-coded in the dispatch loop. The speedups are reported in Figure <ref type="figure">3 (b)</ref>. Respective averages are 2.8 %, 3.2 % and 1.8 % for Nehalem, Sandy Bridge, and Haswell.</p><p>As illustrated by this simple experiment, the speedup brought by jump threading on modern hardware is much less it used to be. And a better branch prediction is not the single factor contributing to the speedup. As already observed by Piumarta and Riccardi <ref type="bibr" target="#b23">[24]</ref> threaded code also executes fewer instructions because part of the dispatch loop is bypassed. For example, on Python-3, we measured the average reduction in number of instructions to be on average 3.3 %, in the same range as the performance gain on the three tested architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Revisiting Conventional Wisdom</head><p>Conventional wisdom considers that the indirect branch that drives the dispatch loop of a switch-based interpreter is inherently difficult to predict. Much effort has been devoted -in the literature and in actual source code -to improving its predictability and reducing its overhead. This was certainly true in the past, and we review related work in Section 6. However, branch predictors have significantly evolved, and they achieve much better performance. In this paper, we study the properties of current interpreters and state-of-theart branch prediction, and we show that the behavior of the main indirect branch is now a minor issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">(Indirect) Branch Predictors</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">State-of-the-art</head><p>Many proposals have been introduced for improving the accuracy of conditional branch prediction during the two past decades, e.g. two-level branch prediction <ref type="bibr" target="#b36">[37]</ref>, hybrid predictors <ref type="bibr" target="#b19">[20]</ref>, de-aliased predictors <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b32">33]</ref>, multiple history length use <ref type="bibr" target="#b29">[30]</ref>, and more recently perceptroninspired predictors <ref type="bibr" target="#b14">[15]</ref> and geometric history length predictors <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b30">31]</ref>. All these propositions have influenced the design of the predictors embedded in state-of-art processors.</p><p>While effective hardware predictors probably combine several prediction schemes (a global history component, a loop predictor and maybe a local history predictor), TAGE <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b30">31]</ref> is generally considered as the state-of-the-art in global history based conditional branch prediction. TAGE features several (partially) tagged predictor tables. The tables are indexed with increasing global history length, the set of history lengths forming a geometric series. The prediction is given by the longest hitting table. TAGE predictors featuring maximum history length of several hundreds of bits can be implemented in real hardware at an affordable storage cost. Therefore TAGE is able to capture correlation between branches distant by several hundreds or even thousands of instructions.</p><p>For a long time, indirect branch targets were naively predicted by the branch target buffer, i.e. the target of the last occurrence of the branch was predicted. However the accuracy of conditional branch predictors is becoming higher and higher. The penalties for a misprediction on a conditional branch or on an indirect branch are in the same range. Therefore even on an application featuring a moderate amount of indirect branches, the misprediction penalty contribution of indirect branches may be very significant if one neglects the indirect branch prediction. Particularly on applications featuring switches with many case statements, e.g. interpreters, the accuracy of this naive prediction is quite low. To limit indirect branch mispredictions, Chang et al. <ref type="bibr" target="#b3">[4]</ref> propose to leverage the global (conditional) branch history to predict the indirect branch targets, i.e. a gshare-like indexed table is used to store the indirect branch targets. However Driesen and Holzle <ref type="bibr" target="#b7">[8]</ref> point out that many indirect branches are correctly predicted by a simple PC-based table, since at execution time they feature a single dynamic target. They proposed the cascaded indirect branch predictor to associate a PC-based table (might be the branch target buffer) with a tagged (PC+global branch history) indexed table.</p><p>More recently, Seznec and Michaud <ref type="bibr" target="#b30">[31]</ref> derived IT-TAGE from their TAGE predictor. Instead of simple conditional branch directions, ITTAGE stores the complete target in tagged tables indexed with increasing history lengths which form a geometric series. As for TAGE, the hitting table featuring the longest history length provides the prediction. At the recent 3rd championship on branch prediction in 2011 2 , TAGE-based (resp. ITTAGE-based) predictors were shown to outperform other conditional branch predictors (resp. indirect predictors).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Intuition of ITTAGE on interpreters</head><p>TAGE performs very well at predicting the behavior of conditional branches that exhibit repetitive patterns and very long patterns. Typically when a given (maybe very long) sequence of length L branches before the current program counter was always biased in a direction in the past, then 2 http://www.jilp.org/jwac-2/ TAGE -provided it features sufficient number of entrieswill correctly predict the branch, independently of the minimum history le needed to discriminate between the effective biased path and another path. This minimum path is captured by one of the tables indexed with history longer than le. With TAGE, the outcomes of branches correlated with close branches are captured by short history length tables, and the outcomes of branches correlated with very distant branches are captured by long history length tables. This optimizes the application footprint on the predictor. The same applies for indirect branches.</p><p>When considering interpreters, the executed path is essentially the main loop around the execution of each bytecode. When running on the succession of basic block bytecodes, the execution pattern seen by the switch reflects the control path in the interpreted application: in practice the history of the recent targets of the jump is the history of opcodes. For instance, if this history is -load load add load mul store add-and if this sequence is unique, then the next opcode is also uniquely determined. This history is in some sense a signature of the virtual program counter, it determines the next virtual program counter.</p><p>When running interpreters, ITTAGE is able to capture such patterns and even very long patterns spanning over several bytecode basic blocks, i.e. to "predict" the virtual program counter. Branches bytecodes present the particularity to feature several possible successors. However, if the interpreted application is control-flow predictable, the history also captures the control-flow history of the interpreted application. Therefore ITTAGE will even predict correctly the successor of the branch bytecodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Setup</head><p>This section details our interpreters and benchmarks. We discuss how we collect data for actual hardware and simulation, and we make sure that both approaches are consistent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Interpreters and Benchmarks</head><p>We experimented with switch-based (no threading) interpreters for three different input languages: Javascript, Python, and the Common Language Infrastructure (CLI, aka .NET), and several inputs for each interpreter. Javascript benchmarks consist in Google's octane suite<ref type="foot" target="#foot_1">3</ref> as of Feb 2014, and Mozilla's kraken <ref type="foot" target="#foot_2">4</ref> . For Python, we used the Unladen Swallow Benchmarks. Finally, we used a subset of SPEC 2000 (train input set) for CLI. All benchmarks are run to completion (including hundreds of hours of CPU for the simulation). See Table <ref type="table" target="#tab_1">1</ref> for an exhaustive list.</p><p>We used Python 3.3.2. The motivation example of Section 2 also uses Python 2.7.5. Unladen Swallow benchmarks were run with the flag --rigorous. We excluded iterative_count, spectral_norm and threaded_count from the suite because they were not properly handled by our measurement setup. Javascript experiments rely on SpiderMonkey 1.8.5</p><p>We used GCC4CLI <ref type="bibr" target="#b4">[5]</ref> to compile the SPEC 2000 benchmarks. It is a port of GCC that generates CLI from C. The CLI interpreter is a proprietary virtual machine that executes applications written in the CLI format. Most of standard C is supported by the compiler and interpreter, however a few features are missing, such as UNIX signals, setjmp, or some POSIX system calls. This explains why a few benchmarks are missing (namely: 176.gcc, 253.perlbmk, 254.gap, 255.vortex, 300.twolf). This is also the reason for not using SPEC 2006: more unsupported C features are used, and neither C++ nor Fortran are supported.</p><p>The interpreters are compiled with Intel icc version 13, using flag -xHost that targets the highest ISA and processor available on the compilation host machine. Some compilers force the alignment of each case entry to a cache line, presumably in an attempt to fit short entries to a single line, thus improving the performance. The downside is that many more targets of the indirect branch alias in the predictor because fewer bits can be used to disambiguate them. Visual inspection confirmed that this is not the case in our setup. McCandless and Gregg <ref type="bibr" target="#b18">[19]</ref> reported this phenomenon and developed a technique that modifies the alignment of individual case entries to improve the overall performance. We manually changed the alignment of the entries in various ways, and observed no difference in performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Branch Predictors</head><p>We experimented with both commercially available hardware and recent proposals in the literature. Section 4.2.3 discusses the coherence of actual and simulated results. Unfortunately, neither architecture has hardware counters for retired indirect jumps, but for "speculative and retired indirect branches" <ref type="bibr" target="#b13">[14]</ref>. It turns out that non-retired indirect branches are rare. On the one hand, we know that the number of retired indirect branches is at least equal to the number of executed bytecodes. On the other hand, the value provided by the counter may overestimate the number of retired indirect branches in case of wrong path execution. That is:</p><formula xml:id="formula_0">n bytecodes ? n retired ? n speculative or equivalently: 1 ? n retired n bytecodes ? n speculative n bytecodes</formula><p>where n speculative is directly read from the PMU, and n bytecodes is easily obtained from the interpreter statistics.</p><p>In most cases (column ind/bc of Tables 3, 4, 5), the upper bound is very close to 1, guaranteeing that non retired indirect branches are negligible. In the remaining cases, we counted the number of indirect branches with a pintool <ref type="bibr" target="#b16">[17]</ref> and confirmed that the PMU counter is a good estimate of retired indirect branches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">TAGE -Simulation</head><p>We also experimented with a state-of-the-art branch predictor from the literature: TAGE and ITTAGE <ref type="bibr" target="#b30">[31]</ref>. The performance is provided through simulation of traces produced by Pin <ref type="bibr" target="#b16">[17]</ref>. We used two (TAGE+ITTAGE) configurations. Both have 8 KB TAGE. TAGE1 assumes a 12.62 KB IT-TAGE, TAGE2 assumes a 6.31 KB ITTAGE (see Table <ref type="table" target="#tab_2">2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Coherence of Measurements</head><p>Our experiments involve different tools and methodologies, namely the PMU collected by the Tiptop tool <ref type="bibr" target="#b25">[26]</ref> on existing hardware, as well as results of simulations driven by traces obtained using Pin <ref type="bibr" target="#b16">[17]</ref>. This section is about confirming that these tools lead to comparable instruction counts, therefore experiment numbers are comparable. Potential discrepancies include the following:</p><p>? non determinism inherent to the PMU <ref type="bibr" target="#b34">[35]</ref> or the system/software stack <ref type="bibr" target="#b21">[22]</ref>;</p><p>? the x86 instruction set provides a rep prefix. The PMU counts prefixed instructions as one (made of many microops), while pintools may count each separately;</p><p>? Pin can only capture events in user mode, while the PMU has the capability to monitor also kernel mode events;</p><p>? tiptop starts counting a bit earlier than Pin: the former starts right before the execvp system call, while the latter starts when the loader is invoked. This difference is constant and negligible in respect of our running times;</p><p>? applications under the control of Pin sometimes execute more instructions in the function dl_relocate_symbol.</p><p>Because Pin links with the application, more symbols exist in the executable, and the resolution may require more work. This happens only once for each executed symbol, and is also negligible for our benchmarks.</p><p>Since Pin only traces user mode events, we configured the PMU correspondingly. To quantify the impact of kernel events, we ran the benchmarks in both modes and we measured the number of retired instructions as well as the instruction mix (loads, stores, and jumps). Not surprisingly for an interpreter, the difference remains under one percentage point. For jumps, it is even below 0.05 percentage point.</p><p>The main loop of interpreters is identical on all architectures, even though we instructed the compiler to generate specialized code. The average variation of the number of executed instructions, due to slightly different releases of the operating system and libraries, is also on the order of 1 %. Finally, PMU and Pin also report counts within 1 %. = MP KI ? average penalty. For the sake simplicity and for a rough analysis, we will assume on the considered architectures an average penalty of 20 cycles. Our measures clearly show that, on Nehalem, on most benchmarks, 12 to 16 MPKI are encountered, that is about 240 to 320 cycles are lost every 1 kiloinstructions. On the next processor generation Sandy Bridge, the misprediction rate is much lower: generally about 4 to 8 MPKI on Javascript applications for instance, i.e. decreasing the global penalty to 80 to 160 cycles every Kiloinstructions. On the most recent processor generation Haswell, the misprediction rate further drops to 0.5 to 2 MPKI in most cases, that is a loss of 10 to 40 cycles every kiloinstructions. Interestingly, the misprediction rates simulated assuming at TAGE + ITTAGE branch predictor scheme are in the same range as the misprediction rates measured on Haswell. This rough analysis illustrates that, in the execution time of interpreted applications, total branch misprediction penalty has gone from a major component on Nehalem to only a small fraction on Haswell.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Overview</head><p>The rough analysis presented above can be refined with models using performance monitoring counters. Intel <ref type="bibr" target="#b17">[18]</ref> describes such methodology to compute wasted instruction issue slots in the processor front-end. We relied on Andi Kleen's implementation pmu-tools 5 and backported the formulas. Unfortunately, only Sandy Bridge and Haswell are supported. In the front-end, issue slots can be wasted in several cases: branch misprediction, but also memory ordering violations, self modifying code (SMC), and AVX-Figure <ref type="figure">7</ref>. Correlation between MPKI and lost slots related We confirmed that AVX never occurs and SMC is negligible in our experiments. Moreover, apart a very few cases, the number of memory ordering violations is two to three orders of magnitude less than the number of branch mispredictions. Nearly all wasted issue slots can be attributed to branch mispredictions.</p><p>Figure <ref type="figure">7</ref> shows, for all our benchmarks, how MPKI relates to wasted slots. Lower MPKI correlates with fewer wasted slots. On average Haswell wastes 7.8 % of the instruction slots due to branch mispredictions, while Sandy Bridge wastes 14.5 %. Better branch prediction on Haswell results in 50 % fewer wasted slots.</p><p>This confirms that, on last generation Intel processors, Haswell, branch misprediction penalty has only a limited impact on interpreted applications execution time.</p><p>In the remainder of this section, we present a detailed analysis of the measures and simulations for each interpreter separately. We also debunk the folklore on the "hard to predict" branches in the interpreter loop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Detailed Analysis</head><p>We present results for each interpreter (Tables <ref type="table" target="#tab_4">3,</ref><ref type="table" target="#tab_5">4,</ref><ref type="table" target="#tab_6">5</ref>). The left part reports general characteristics of the interpreter. For each benchmark, the tables show the number of executed bytecodes (in millions), the number of native instructions (in billions), the overall measured performance reported as IPC (instructions per cycle), the number of executed instructions per bytecode, the fraction of branch instructions, the fraction of indirect branch instructions, and finally the number of indirect branch instructions per bytecode.</p><p>The second half the tables report on the performance of each architecture in terms of branch prediction (MPKI) for each benchmark on Nehalem, Sandy Bridge, Haswell, TAGE1 and TAGE2.</p><p>Generally all three interpreters run at quite high performance with median values of 1.5, 1.4, 1.2 IPC for respectively Python, Javascript and CLI on Nehalem, 1.7, 1.5 and 1.2 on Sandy Bridge, and 2.4, 2.4 and 2.2 on Haswell.</p><p>Our experiments clearly show that between three recent generations of Intel processors, the improvement on the branch prediction accuracy on interpreters is dramatic for Python and Javascript. Our simulations of TAGE and IT-TAGE show that, as long as the payload in the bytecode remains limited and do not feature significant amount of extra indirect branches, then the misprediction rate on the interpreter can be even become insignificant (less than 0.5 MPKI). For CLI, the results on Nehalem and Sandy Bridge are much more mitigated than Python and Javascript, with Sandy Bridge only reducing the misprediction rate by at most 25 % and often much less, e.g. on art, it even loses accuracy. Haswell, however, predicts much better, achieving results close to TAGE+ITTAGE.</p><p>To summarize, the predictability of branches, both indirect and conditional, should not be considered as an issue anymore for Javascript and Python interpreters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Python</head><p>Python is implemented in C. The syntax of the Python language differs slightly between versions 2 and 3 (a converter is provided), and so does the bytecode definition. Still, both have a similar set of roughly 110 opcodes.</p><p>The dispatch loop is identical on Nehalem, Sandy Bridge and Haswell (with the exception of stack offsets), it consists in 24 instructions for bytecodes without arguments, and 6 additional instructions to handle an argument. These instructions check for exit and tracing conditions, loading the next opcode, accessing the jump table, and directing control to the corresponding address. A few instructions detect pending exceptions, and handle objects allocation/deallocation. Only one indirect branch is part of the dispatch loop.</p><p>Table <ref type="table" target="#tab_4">3</ref> report our results with the Python interpreter. The performance (measured as IPC) on Nehalem and Sandy Bridge is fairly good, showing that no serious problem (such as cache misses or branch misprediction) is degrading performance. It is even better on Haswell, with a median value of 2.4 IPC, and up to 3.4 IPC.</p><p>It takes 120 to 150 instructions to execute a bytecode. Considering the overhead of the dispatch, about 100 instructions are needed to execute the payload of a bytecode. This rather high number is due to dynamic typing. A simple add must check the types of the arguments (numbers or strings), and even in the case of integers, an overflow can occur, requiring special treatment.</p><p>In a few cases, the number is much higher, as for the fastpickle or regex benchmarks. This is because they apply heavier processing implemented in native libraries for performance. In the case of fastpickle, the benchmark serializes objects by calling a dedicated routine.</p><p>There is generally a single indirect branch per bytecode. Values significantly larger than 1 are correlated with a high number of instructions per bytecode, revealing that the execution has left the interpreter main loop proper to execute a dedicated routine.</p><p>Figure <ref type="figure" target="#fig_4">4</ref> plots the MPKI for each benchmark and branch predictor. It is clear that the Sandy Bridge predictor significantly outperforms Nehalem's, and the same applies for Haswell and TAGE when compared to Sandy Bridge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>109</head><p>In practice, one can note that when the average payload is around 120 to 150 instructions and there are no (or very few) indirect branches apart the main switch, i.e., ind bc ? 1.02, then TAGE+ITTAGE predicts the interpreter quasiperfectly. When the average payload is larger or some extra indirect branches are encountered then misprediction rate of TAGE+ITTAGE becomes higher and may become in the same order as the one of Haswell.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Javascript</head><p>SpiderMonkey Javascript is implemented in C++. We compiled it without JIT support, and we manually removed the detection of Labels as Values. The bytecode consists in 244 entries. The dispatch loop consists in 16 instructions, significantly shorter that Python.</p><p>Table <ref type="table" target="#tab_5">4</ref> reports the characteristics of the Javascript interpreter. With the exception of code-load in octane, and parse-financial and stringify-tinderbox in kraken, indirect branches come from the switch statement. These benchmarks also have a outstanding number of instructions per bytecode. Excluding them, it takes on average in the order of 60 instructions per bytecode.</p><p>Table <ref type="table" target="#tab_5">4</ref> also reports on the performance of the branch predictors, and Figure <ref type="figure">5</ref> illustrates the respective MPKI. As for Python, Haswell and TAGE consistently outperform Sandy Bridge, which also outperforms Nehalem.</p><p>As for the Python interpreters, with the exception of three outliers, TAGE predict quasi perfectly the interpreter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">CLI</head><p>The CLI interpreter is written in standard C, hence dispatch is implemented with a switch statement. The internal IR consists in 478 opcodes. This IR resembles the CLI bytecode from which it is derived. CLI operators are not typed, the same add (for example) applies to all integer and floating point types. The standard, though, requires that types can be statically derived to prove the correctness of the program before execution. The interpreter IR specializes the operators with the computed types to remove some burden from the interpreter execute loop. This explains the rather large number of different opcodes. As per Brunthaler's definition <ref type="bibr" target="#b2">[3]</ref>, the CLI interpreter is a low abstraction level interpreter.</p><p>The dispatch loop consists in only seven instructions, illustrated on Figure <ref type="figure" target="#fig_6">8</ref>. This is possible because each opcode is very low level (strongly typed, and derived from C operators), and there is no support for higher abstractions such as garbage collection, or exceptions.</p><p>Table <ref type="table" target="#tab_6">5</ref> reports on the characteristics of the CLI interpreter and the behavior of the predictors. The number of speculative indirect jumps is between 1.01 and 1.07 bytecodes. In fact, most of the code is interpreted, even libraries such as libc and libm. Execution goes to native code at a cutpoint similar to a native libc does a system call. The short loop is also the reason why the fraction of indirect branch instructions is higher than Javascript or Python.  The compiler could not achieve stack caching <ref type="foot" target="#foot_4">6</ref> , probably due to the limited number of registers. Figure <ref type="figure" target="#fig_6">8</ref> illustrates the x86 assembly code of the dispatch loop, as well as entry for the ADD bytecode. The dispatch consists in seven instructions. Two of them (lines 2 and 3) perform the useless range check that the compiler could not remove. Note the instructions at line 9-11: the compiler chose to replicate the code typically found at the top of the infinite loop and move it at the bottom of each case entry (compare with Figure <ref type="figure" target="#fig_0">1</ref>). Across all benchmarks, 21 instructions are needed to execute one bytecode. Nehalem, Sandy Bridge and TAGE are ranked in the same order as for the other two interpreters. Hawell's predictor is comparable to TAGE, with occasional wins for TAGE (2.6 vs 0.21 MPKI on 177.mesa) and for Haswell (0.2 vs 0.38 on 179.art).</p><p>However, with the CLI interpreter, the accuracy of TAGE +ITTAGE is particularly poor on vpr and crafty, i.e. misprediction rate exceeds 1 MPKI. We remarked that in these cases the accuracy of the smaller ITTAGE (TAGE2) is much lower than the one with the larger ITTAGE (TAGE1). Therefore the relatively low accuracy seems to be associated with interpreter footprint issues on the ITTAGE predictor. To confirm this observation, we run an extra simulation TAGE3 where the ITTAGE predictor is 50 KB. A 50KB ITTAGE predictor allows to reduce the misprediction rate to the "normal" rate except for crafty which would still need a larger ITTAGE. The very large footprint required by the interpreter on the ITTAGE predictor is also associated with the huge number of possible targets (478) in the main switch of the interpreter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Folklore on "hard to predict" branches</head><p>The indirect branch of the dispatch loop in each interpreter is generally considered as the one that is very hard to predict. Simulation allows us to observe the individual behavior of specific branch instructions. We measured the misprediction ratio of the indirect branch of the dispatch loop in each interpreter. Moreover the source code of Python refers to two "hard to predict" branches. The first is the indirect branch that implements the switch statement. The second comes for the macro HAS_ARG that checks whether an opcode has an argument. For Python, we also considered this conditional branch. Table <ref type="table" target="#tab_7">6</ref> reports the misprediction numbers for these branches for all benchmarks for the three sizes of ITTAGE predictors, 6 KB, 12 KB and 50 KB. On Python, the indirect jumps are most often very well predicted for most benchmarks, even by the 6 KB ITTAGE. However, in several cases the prediction of indirect jump is poor (see for example the Python chaos, django-v2, formatted-log, go). However, these cases except go are in practice near perfectly predicted by the 12 KB configuration: that is in practice the footprint of the Python application on the indirect jump predictor is too large for the 6 KB configuration, but fits the 12 KB configuration. go needs an even larger predictor as illustrated by the results on the 50 KB configuration. HAS_ARG turns out to be very easily predicted by the conditional branch predictor TAGE at the exceptions of the same few outliers with 1% to 4% mispredictions.</p><p>For Javascript, the indirect branch also appears as quite easy to predict with misprediction rates generally lower than 1% with the 12 KB ITTAGE. More outliers than for Python are encountered, particularly code-load and type_script. However these outliers are all amenable to low misprediction rates with a 50 KB predictor at the exception of code-load. However, code-load executes more than 4,000 instructions per bytecode on average (see Table <ref type="table" target="#tab_5">4</ref>) and therefore the predictability of the indirect jump in the interpreter dispatch loop has a very limited impact on the overall performance.</p><p>With the CLI interpreter, the main indirect branch suffers from a rather high misprediction rate when executing vpr and crafty (and bzip2 to some extent) with a 12 KB IT-TAGE. But a 50 KB ITTAGE predictor strictly reduces this misprediction rate except for crafty which would need an even larger ITTAGE predictor.</p><p>Therefore the folklore on the unpredictability of the indirect branch in the dispatch loop is rather unjustified: this indirect branch is very predictable provided the usage of a large enough efficient indirect jump predictor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Other Related Work</head><p>This paper is about the interaction of interpreters with branch predictors. The most relevant work on branch prediction is covered by Section 3. The overhead of interpreters compared to native code derives mostly from two sources: the management of the evaluation stack and the dispatch loop.</p><p>Very recent work by Savrun-Yenic ?eri et al. <ref type="bibr" target="#b26">[27]</ref> still references the original work of Ertl and Gregg <ref type="bibr" target="#b11">[12]</ref>. Their approach, however, is very different: they consider host-VM targeted interpreters, i.e. interpreters for languages such as Python or Javascript implemented on top of the Java VM. Performance results are difficult to compare with ours.</p><p>Vitale and Abdelrahman <ref type="bibr" target="#b33">[34]</ref> eliminate the dispatch overhead with a technique called catenation. It consists in copying and combining at run-time sequences of native code produced when the interpreter was compiled. Half the bench-  McCandless and Gregg <ref type="bibr" target="#b18">[19]</ref> propose to optimize code at the assembly level to eliminate interferences between targets of the indirect branch. Two techniques are developed: forcing alignment, and reordering entries. The hardware used for experiments is a Core2 processor. We claim that modern branch predictors are quite insensitive to target placement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Stack Caching and Registers</head><p>With the notable exception of the Dalvik virtual machine <ref type="bibr" target="#b8">[9]</ref>, most current interpreters perform their computations on an evaluation stack. Values are stored in a data structure which resides in memory (recall Figure <ref type="figure" target="#fig_0">1</ref> for illustration). Ertl proposed stack caching <ref type="bibr" target="#b9">[10]</ref> to force the top of the stack into registers. Together with Gregg, they later proposed to combine it with dynamic superinstructions <ref type="bibr" target="#b12">[13]</ref> for additional performance. Stack caching is orthogonal to the behavior of the branch predictor. While it could decrease the number of cycles of the interpreter loop, and hence increase the relative impact of a misprediction, this data will typically hit in the L1 cache and aggressive out-of-order architectures are less likely to benefit, especially for rather long loops, and the already reasonably good performance (IPC). Register allocators have a hard time keeping the relevant values in registers 7 because of the size and complexity of the main interpreter loop. Stack caching also adds significant complexity to the code base.</p><p>As an alternative to stack caching, some virtual machines are register-based. Shi et al. <ref type="bibr" target="#b31">[32]</ref> show they are more efficient when sophisticated translation and optimizations are applied. This is orthogonal to the dispatch loop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Superinstructions and Replication</head><p>Sequences of bycodes are not random. Some pairs are more frequent than others (e.g. a compare is often followed by a branch). Superinstructions <ref type="bibr" target="#b23">[24]</ref> consist in such sequences of frequently occurring tuples of bytecode. New bytecodes are defined, whose payloads are the combination of the payloads of the tuples. The overhead of the dispatch loop is unmodified but the gain comes from a reduced number of iterations of the loop, hence a reduced average cost. Ertl and Gregg <ref type="bibr" target="#b11">[12]</ref> discuss static and dynamic superinstructions.</p><p>Replication, also proposed by Ertl and Gregg, consists in generating many opcodes for the same payload, specializing each occurrence, in order to maximize the performance of 7 Even with the help of the GNU extension register asm, or manually allocating a few top-of-stack elements in local variable. branch target buffers. Modern predictors no longer need it to capture patterns in applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Jump Threading</head><p>We discuss jump threading in general terms in previous sections. To be more precise, several versions of threading have been proposed: token threading (illustrated in Figure <ref type="figure" target="#fig_2">2</ref>), direct threading <ref type="bibr" target="#b0">[1]</ref>, inline threading <ref type="bibr" target="#b23">[24]</ref>, or context threading <ref type="bibr" target="#b1">[2]</ref>. All forms of threading require extensions to ANSI C. Some also require limited forms of dynamic code generation and walk away from portability and ease of development.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>Despite mature JIT compilation technology, interpreters are very much alive. They provide ease of development and portability. Unfortunately, this is at the expense of performance: interpreters are slow. Many studies have investigated ways to improve interpreters, and many design points have been proposed. But many studies go back when branch predictors were not very aggressive, folklore has retained that a highly mispredicted indirect jump is one of the main reasons for the inefficiency of switch-based interpreters.</p><p>In this paper, we shed new light on this claim, considering current interpreters and state-of-the-art branch predictors. We show that the accuracy of branch prediction on interpreters has been dramatically improved over the three last Intel processor generations. This accuracy on Haswell, the most recent Intel processor generation, has reached a level where it can not be considered as an obstacle for performance anymore. We have also shown that this accuracy is on par with the one of the literature state-of-the-art ITTAGE. While the structure of the Haswell indirect jump predictor is undisclosed, we were able to confirm with simulations of ITTAGE that the few cases where the prediction accuracy is relatively poor are due to footprint issues on the predictor and not inherent to the prediction scheme.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Main loop of naive interpreter</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>void ? l a b e l s [ ] = { &amp;&amp;ADD, &amp;&amp;SUB . . . } ; . . . goto ? l a b e l s [? vpc + + ] ; ADD: x = pop ( stack ) ; y = pop ( stack ) ; push ( s t a c k , x+y ) ; goto ? l a b e l s [? vpc + + ] ; SUB : . . . goto ? l a b e l s [? vpc + + ] ;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Token threading, using a GNU extension</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>2 Figure 3 .</head><label>23</label><figDesc>Figure 3. Speedups in Python</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figures 4 ,</head><label>4</label><figDesc>Figures 4, 5 and 6  illustrate the branch misprediction rates measured in our experiments on respectively Python, Javascript and CLI interpreters. The branch misprediction rates are measured in MPKI, misprediction per kilo instructions. MPKI is generally considered as a quite illustrative metric for branch predictors, since it allows to get at a first glance a very rough estimation of the lost cycles per kiloinstructions: lost cycles KI</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .Figure 5 .Figure 6 .</head><label>456</label><figDesc>Figure 4. Python MPKI for all predictors</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Dispatch loop of the CLI interpreter (+ADD)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>4799-8161-8/15/$31.00 ?2015 IEEE</head><label></label><figDesc></figDesc><table><row><cell>1 2 3 4</cell><cell cols="2">w h i l e ( 1 ) { opc = ?vpc ++; s w i t c h ( opc ) { c a s e ADD:</cell><cell></cell><cell>1 2 3 4</cell><cell>loop : add movzwl ( e s i ) , e d i 0x2 , e s i cmp 0 x299 , e d i</cell></row><row><cell>5</cell><cell></cell><cell>x = p o p(stack);</cell><cell></cell><cell>5</cell><cell>j a</cell><cell>&lt;loop&gt;</cell></row><row><cell>6</cell><cell></cell><cell>y = p o p(stack);</cell><cell></cell><cell>6</cell><cell>mov</cell><cell>0 x . . ( , edi , 4 ) , eax</cell></row><row><cell>7 8</cell><cell></cell><cell cols="2">push(stack,x + y); break ;</cell><cell>7 8</cell><cell>jmp ...</cell><cell>?eax</cell></row><row><cell>9</cell><cell></cell><cell></cell><cell></cell><cell>9</cell><cell>mov</cell><cell>0x10 ( ebx ) , eax</cell></row><row><cell>10</cell><cell></cell><cell>c a s e SUB : . . .</cell><cell cols="2">10</cell><cell>add</cell><cell>eax , 0x20 ( ebx )</cell></row><row><cell cols="2">11 12 }</cell><cell>}</cell><cell cols="2">11 12</cell><cell>add jmp</cell><cell>0 x f f f f f f f 0 , ebx &lt;loop&gt;</cell></row></table><note><p>7 concludes. 2015 IEEE/ACM International Symposium on Code Generation and Optimization 978-1-4799-8161-8/15/$31.00 c 2015 IEEE 103 1-</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Benchmarks</figDesc><table><row><cell>Python</cell><cell>regex v8</cell><cell>crypto</cell></row><row><cell>call method</cell><cell>richards</cell><cell>deltablue</cell></row><row><cell>call method slots</cell><cell>silent logging</cell><cell>earley-boyer</cell></row><row><cell cols="2">call method unknown simple logging</cell><cell>gbemu</cell></row><row><cell>call simple</cell><cell>telco</cell><cell>mandreel</cell></row><row><cell>chaos</cell><cell>unpack sequence</cell><cell>navier-stokes</cell></row><row><cell>django v2</cell><cell>Javascript (kraken)</cell><cell>pdf</cell></row><row><cell>fannkuch</cell><cell>ai-astar</cell><cell>raytrace</cell></row><row><cell>fastpickle</cell><cell>audio-beat-detection</cell><cell>regexp</cell></row><row><cell>fastunpickle</cell><cell>audio-dft</cell><cell>richards</cell></row><row><cell>float</cell><cell>audio-fft</cell><cell>splay</cell></row><row><cell>formatted logging</cell><cell>audio-oscillator</cell><cell>typescript</cell></row><row><cell>go</cell><cell>imaging-darkroom</cell><cell>zlib</cell></row><row><cell>hexiom2</cell><cell>imaging-desaturate</cell><cell>CLI</cell></row><row><cell>json dump v2</cell><cell>imaging-gaussian-blur</cell><cell>164.gzip</cell></row><row><cell>json load</cell><cell>json-parse-financial</cell><cell>175.vpr</cell></row><row><cell>meteor contest</cell><cell cols="2">json-stringify-tinderbox 177.mesa</cell></row><row><cell>nbody</cell><cell>crypto-aes</cell><cell>179.art</cell></row><row><cell>nqueens</cell><cell>crypto-ccm</cell><cell>181.mcf</cell></row><row><cell>pathlib</cell><cell>crypto-pbkdf2</cell><cell>183.equake</cell></row><row><cell>pidigits</cell><cell>crypto-sha256-iterative</cell><cell>186.crafty</cell></row><row><cell>raytrace</cell><cell>Javascript (octane)</cell><cell>188.ammp</cell></row><row><cell>regex compile</cell><cell>box2d</cell><cell>197.parser</cell></row><row><cell>regex effbot</cell><cell>code-load</cell><cell>256.bzip2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Branch predictor parameters</figDesc><table><row><cell>Parameter</cell><cell>TAGE</cell><cell cols="2">ITTAGE 2 ITTAGE 1</cell></row><row><cell>min history length</cell><cell>5</cell><cell>2</cell><cell>2</cell></row><row><cell>max history length</cell><cell>75</cell><cell>80</cell><cell>80</cell></row><row><cell>num tables (N)</cell><cell>5</cell><cell>8</cell><cell>8</cell></row><row><cell>num entries table T 0</cell><cell>4096</cell><cell>256</cell><cell>512</cell></row><row><cell>num entries tables T 1 T N 1</cell><cell>1024</cell><cell>64</cell><cell>128</cell></row><row><cell>storage (kilobytes)</cell><cell>8 KB</cell><cell>6.31 KB</cell><cell>12.62 KB</cell></row><row><cell cols="4">4.2.1 Existing Hardware -Performance Counters</cell></row><row><cell cols="4">Branch prediction data is collected from the PMU (perfor-</cell></row><row><cell cols="4">mance monitoring unit) on actual Nehalem (Xeon W3550</cell></row><row><cell cols="4">3.07 GHz), Sandy Bridge (Core i7-2620M 2.70 GHz), and</cell></row><row><cell cols="4">Haswell (Core i7-4770 3.40 GHz) architectures running</cell></row><row><cell cols="4">Linux. Both provide counters for cycles, retired instructions,</cell></row><row><cell cols="4">retired branch instructions, and mispredicted branch instruc-</cell></row><row><cell cols="4">tions. We relied on Tiptop [26] to collect data from the PMU.</cell></row><row><cell cols="4">Events are collected per process (not machine wide) on an</cell></row><row><cell cols="4">otherwise unloaded workstation. Only user-land events are</cell></row><row><cell cols="3">collected (see also discussion in Section 4.2.3).</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Python characteristics and branch prediction for Nehalem (Neh.), Sandy Bridge (SB), Haswell (Has.) and TAGE</figDesc><table><row><cell>benchmark</cell><cell>Mbc</cell><cell>Gins</cell><cell></cell><cell>IPC</cell><cell></cell><cell>ins/bc</cell><cell>br</cell><cell>ind</cell><cell>ind/bc</cell><cell></cell><cell></cell><cell>MPKI</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Neh.</cell><cell cols="2">SB Has.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Neh.</cell><cell cols="3">SB Has. TAGE 1 TAGE 2</cell></row><row><cell>call method</cell><cell>6137</cell><cell>771</cell><cell>1.84</cell><cell>2.2</cell><cell>2.93</cell><cell>125.6</cell><cell cols="2">20 % 0.8 %</cell><cell>1.01</cell><cell cols="2">4.8 0.6</cell><cell>0.1</cell><cell>0.067</cell><cell>0.067</cell></row><row><cell>call method slots</cell><cell>6137</cell><cell>766</cell><cell cols="2">1.87 2.18</cell><cell>2.90</cell><cell>124.8</cell><cell cols="2">20 % 0.8 %</cell><cell>1.02</cell><cell cols="2">5.9 0.7</cell><cell>0.1</cell><cell>0.068</cell><cell>0.068</cell></row><row><cell>call method unknown</cell><cell>7300</cell><cell>803</cell><cell cols="2">1.99 2.22</cell><cell>2.88</cell><cell>110.0</cell><cell cols="2">20 % 0.9 %</cell><cell>1.00</cell><cell cols="2">4.2 0.3</cell><cell>0.1</cell><cell>0.058</cell><cell>0.058</cell></row><row><cell>call simple</cell><cell>5123</cell><cell>613</cell><cell cols="2">1.89 2.33</cell><cell>3.12</cell><cell>119.6</cell><cell cols="2">19 % 0.8 %</cell><cell>1.00</cell><cell cols="2">4.1 0.5</cell><cell>0.1</cell><cell>0.086</cell><cell>0.086</cell></row><row><cell>chaos</cell><cell>1196</cell><cell>162</cell><cell cols="2">1.34 1.55</cell><cell>2.21</cell><cell>135.4</cell><cell cols="2">22 % 0.8 %</cell><cell>1.07</cell><cell cols="2">18.4 4.4</cell><cell>1.8</cell><cell>0.680</cell><cell>2.548</cell></row><row><cell>django v2</cell><cell>1451</cell><cell>332</cell><cell cols="2">1.22 1.44</cell><cell>2.10</cell><cell>228.7</cell><cell cols="2">21 % 0.6 %</cell><cell>1.33</cell><cell cols="2">15.9 3.9</cell><cell>1.5</cell><cell>0.529</cell><cell>1.829</cell></row><row><cell>fannkuch</cell><cell>7693</cell><cell>747</cell><cell cols="2">1.52 1.67</cell><cell>2.44</cell><cell>97.1</cell><cell cols="2">23 % 1.3 %</cell><cell>1.23</cell><cell cols="2">18.4 6.1</cell><cell>0.9</cell><cell>0.578</cell><cell>0.592</cell></row><row><cell>fastpickle</cell><cell>34</cell><cell>351</cell><cell cols="2">1.62 1.89</cell><cell>2.63</cell><cell cols="3">10277 22 % 0.5 %</cell><cell>54</cell><cell cols="2">19 2.6</cell><cell>1.4</cell><cell>2.258</cell><cell>2.290</cell></row><row><cell>fastunpickle</cell><cell>25</cell><cell>278</cell><cell cols="2">1.47 1.77</cell><cell>2.06</cell><cell cols="3">11320 22 % 0.8 %</cell><cell>91</cell><cell>16.5</cell><cell>3</cell><cell>1.7</cell><cell>2.365</cell><cell>2.673</cell></row><row><cell>float</cell><cell>1280</cell><cell>180</cell><cell cols="2">1.67 1.73</cell><cell>2.27</cell><cell>140.6</cell><cell cols="2">22 % 0.8 %</cell><cell>1.15</cell><cell>12</cell><cell>3</cell><cell>1.5</cell><cell>0.364</cell><cell>0.365</cell></row><row><cell>formatted logging</cell><cell>750</cell><cell>125</cell><cell cols="2">0.89 1.13</cell><cell>1.74</cell><cell>166.9</cell><cell cols="2">22 % 0.7 %</cell><cell>1.20</cell><cell cols="2">17.5 5.4</cell><cell>2.8</cell><cell>0.633</cell><cell>4.220</cell></row><row><cell>go</cell><cell>2972</cell><cell>344</cell><cell>1.43</cell><cell>1.6</cell><cell>2.20</cell><cell>115.7</cell><cell cols="2">21 % 0.9 %</cell><cell>1.01</cell><cell cols="2">14 5.2</cell><cell>2.4</cell><cell>1.121</cell><cell>1.979</cell></row><row><cell>hexiom2</cell><cell>33350</cell><cell>3674</cell><cell cols="2">1.72 1.86</cell><cell>2.52</cell><cell>110.2</cell><cell cols="2">21 % 1.0 %</cell><cell>1.08</cell><cell cols="2">11.9 2.9</cell><cell>0.8</cell><cell>0.563</cell><cell>0.832</cell></row><row><cell>json dump v2</cell><cell>5042</cell><cell>1656</cell><cell cols="2">1.48 1.62</cell><cell>2.44</cell><cell>328.5</cell><cell cols="2">23 % 0.8 %</cell><cell>2.61</cell><cell cols="2">17.2 3.5</cell><cell>0.6</cell><cell>0.827</cell><cell>0.859</cell></row><row><cell>json load</cell><cell>195</cell><cell>271</cell><cell cols="2">1.57 1.81</cell><cell>2.50</cell><cell cols="3">1391 26 % 0.5 %</cell><cell>6.76</cell><cell cols="2">15.7 3.3</cell><cell>2.1</cell><cell>3.074</cell><cell>3.198</cell></row><row><cell>meteor contest</cell><cell>868</cell><cell>106</cell><cell cols="2">1.36 1.52</cell><cell>1.89</cell><cell>122.3</cell><cell cols="2">22 % 0.9 %</cell><cell>1.05</cell><cell cols="2">16.7 6.9</cell><cell>5.5</cell><cell>3.507</cell><cell>3.519</cell></row><row><cell>nbody</cell><cell>2703</cell><cell>184</cell><cell cols="2">1.78 1.73</cell><cell>2.45</cell><cell>68.2</cell><cell cols="2">23 % 1.5 %</cell><cell>1.00</cell><cell cols="2">13.8 5.9</cell><cell>2.1</cell><cell>0.700</cell><cell>0.701</cell></row><row><cell>nqueens</cell><cell>1296</cell><cell>152</cell><cell cols="2">1.34 1.46</cell><cell>2.35</cell><cell>117.0</cell><cell cols="2">22 % 0.9 %</cell><cell>1.06</cell><cell cols="2">16.5 3.9</cell><cell>0.9</cell><cell>0.549</cell><cell>0.549</cell></row><row><cell>pathlib</cell><cell>836</cell><cell>188</cell><cell cols="2">0.79 1.08</cell><cell>1.88</cell><cell>225.3</cell><cell cols="2">22 % 0.6 %</cell><cell>1.45</cell><cell cols="2">16.5 4.7</cell><cell>1.2</cell><cell>0.397</cell><cell>0.633</cell></row><row><cell>pidigits</cell><cell>94</cell><cell>223</cell><cell cols="2">2.33 2.37</cell><cell>2.76</cell><cell>2366</cell><cell cols="2">7 % 0.1 %</cell><cell>1.42</cell><cell cols="2">1.2 0.5</cell><cell>0.4</cell><cell>0.356</cell><cell>0.363</cell></row><row><cell>raytrace</cell><cell>4662</cell><cell>766</cell><cell cols="2">1.40 1.66</cell><cell>2.21</cell><cell>164.3</cell><cell cols="2">21 % 0.7 %</cell><cell>1.12</cell><cell cols="2">15.2 3.6</cell><cell>1.8</cell><cell>0.577</cell><cell>1.017</cell></row><row><cell>regex compile</cell><cell>1938</cell><cell>214</cell><cell cols="2">1.33 1.52</cell><cell>2.17</cell><cell>110.6</cell><cell cols="2">21 % 1.0 %</cell><cell>1.05</cell><cell cols="2">15.1 5.3</cell><cell>2.1</cell><cell>1.588</cell><cell>2.257</cell></row><row><cell>regex effbot</cell><cell>7</cell><cell>52</cell><cell cols="2">2.45 2.53</cell><cell>3.35</cell><cell cols="3">6977 22 % 2.1 %</cell><cell>146</cell><cell cols="2">6.1 0.1</cell><cell>0.0</cell><cell>0.026</cell><cell>0.027</cell></row><row><cell>regex v8</cell><cell>31</cell><cell>37</cell><cell cols="2">1.79 1.79</cell><cell>2.39</cell><cell cols="3">1182 22 % 1.7 %</cell><cell>21</cell><cell cols="2">9.8 1.1</cell><cell>0.6</cell><cell>0.506</cell><cell>0.534</cell></row><row><cell>richards</cell><cell>961</cell><cell>108</cell><cell cols="2">1.39 1.59</cell><cell>2.27</cell><cell>111.9</cell><cell cols="2">21 % 0.9 %</cell><cell>1.02</cell><cell cols="2">13.2 5.8</cell><cell>1.8</cell><cell>0.824</cell><cell>1.518</cell></row><row><cell>silent logging</cell><cell>353</cell><cell>53</cell><cell cols="2">1.75 1.83</cell><cell>2.59</cell><cell>148.8</cell><cell cols="2">21 % 0.7 %</cell><cell>1.08</cell><cell cols="2">10.3 3.7</cell><cell>0.4</cell><cell>0.035</cell><cell>0.035</cell></row><row><cell>simple logging</cell><cell>731</cell><cell>120</cell><cell cols="2">0.93 1.14</cell><cell>1.78</cell><cell>164.4</cell><cell cols="2">22 % 0.7 %</cell><cell>1.19</cell><cell cols="2">17.4 5.3</cell><cell>2.8</cell><cell>0.669</cell><cell>4.748</cell></row><row><cell>telco</cell><cell>34</cell><cell>6</cell><cell cols="2">1.28 1.28</cell><cell>2.24</cell><cell>174.6</cell><cell cols="2">21 % 1.3 %</cell><cell>2.35</cell><cell cols="2">15.6 5.7</cell><cell>1.5</cell><cell>1.143</cell><cell>1.150</cell></row><row><cell>unpack seq</cell><cell>966</cell><cell>38</cell><cell cols="2">1.90 2.04</cell><cell>2.86</cell><cell>39.5</cell><cell cols="2">21 % 2.6 %</cell><cell>1.01</cell><cell cols="2">8.9 4.3</cell><cell>2.2</cell><cell>0.056</cell><cell>0.057</cell></row><row><cell>average</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">12.8 3.5</cell><cell>1.4</cell><cell>0.8</cell><cell>1.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Javascript characteristics and branch prediction for Nehalem (Neh.), Sandy Bridge (SB), Haswell (Has.) and TAGE</figDesc><table><row><cell>benchmark</cell><cell>Mbc</cell><cell>Gins</cell><cell></cell><cell>IPC</cell><cell></cell><cell>ins/bc</cell><cell>br</cell><cell cols="2">ind ind/bc</cell><cell></cell><cell></cell><cell>MPKI</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Neh.</cell><cell cols="2">SB Has.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Neh.</cell><cell cols="2">SB Has.</cell><cell>T1</cell><cell>T2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>kraken</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ai-astar</cell><cell>5713</cell><cell>296</cell><cell cols="2">1.55 1.55</cell><cell>2.36</cell><cell cols="3">51.8 20 % 1.9 %</cell><cell>1.002</cell><cell>17.5</cell><cell>6.4</cell><cell>0.0 0.01 0.01</cell></row><row><cell>audio-beat-detection</cell><cell>4567</cell><cell>195</cell><cell cols="2">1.42 1.59</cell><cell>2.56</cell><cell cols="3">42.7 19 % 2.3 %</cell><cell>1.002</cell><cell>13.4</cell><cell>6.6</cell><cell>1.5 0.14 0.16</cell></row><row><cell>audio-dft</cell><cell>3311</cell><cell>169</cell><cell cols="2">1.62 1.58</cell><cell>2.57</cell><cell cols="3">51.0 20 % 2.0 %</cell><cell>1.004</cell><cell>14.5</cell><cell>4.3</cell><cell>1.1 0.01 0.01</cell></row><row><cell>audio-fft</cell><cell>4459</cell><cell>189</cell><cell cols="2">1.42 1.59</cell><cell>2.65</cell><cell cols="3">42.4 19 % 2.4 %</cell><cell>1.002</cell><cell>13.5</cell><cell>6.7</cell><cell>1.3 0.12 0.12</cell></row><row><cell>audio-oscillator</cell><cell>2541</cell><cell>162</cell><cell cols="2">1.69 1.61</cell><cell>2.61</cell><cell cols="3">63.8 21 % 1.6 %</cell><cell>1.033</cell><cell>10.5</cell><cell>5.0</cell><cell>1.0 0.01 0.01</cell></row><row><cell>imaging-darkroom</cell><cell>4387</cell><cell>234</cell><cell cols="2">1.40 1.33</cell><cell>2.31</cell><cell cols="3">53.3 20 % 1.9 %</cell><cell>1.022</cell><cell>14.7</cell><cell>9.3</cell><cell>2.1 0.07 0.08</cell></row><row><cell>imaging-desaturate</cell><cell>8117</cell><cell>368</cell><cell cols="2">1.71 1.73</cell><cell>2.72</cell><cell cols="3">45.3 19 % 2.2 %</cell><cell>1.007</cell><cell>7.4</cell><cell>4.1</cell><cell>1.2 0.01 0.01</cell></row><row><cell>imaging-gaussian-blur</cell><cell>24490</cell><cell>1278</cell><cell cols="2">1.60 1.59</cell><cell>2.70</cell><cell cols="3">52.2 19 % 1.9 %</cell><cell>1.060</cell><cell>15.8</cell><cell>6.6</cell><cell>1.1 0.17 0.17</cell></row><row><cell>json-parse-financial</cell><cell>0.12</cell><cell>6</cell><cell cols="2">1.77 2.15</cell><cell>2.53</cell><cell cols="3">50000 21 % 1.2 %</cell><cell>569</cell><cell>17.3</cell><cell>1.1</cell><cell>1.0 1.76 1.77</cell></row><row><cell>json-stringify-tinderbox</cell><cell>0.30</cell><cell>4</cell><cell cols="2">2.09 2.27</cell><cell>2.84</cell><cell cols="3">13333 24 % 0.2 %</cell><cell>23.7</cell><cell>12.0</cell><cell>1.3</cell><cell>1.2 1.71 1.71</cell></row><row><cell>crypto-aes</cell><cell>1679</cell><cell>68</cell><cell cols="2">1.41 1.40</cell><cell>2.51</cell><cell cols="3">40.5 17 % 2.5 %</cell><cell>1.008</cell><cell>15.0</cell><cell>9.3</cell><cell>1.8 0.29 2.13</cell></row><row><cell></cell><cell></cell><cell>43</cell><cell cols="2">1.40 1.39</cell><cell>2.31</cell><cell cols="3">41.6 17 % 2.4 %</cell><cell>1.016</cell><cell>14.7</cell><cell>9.4</cell><cell>2.9 1.07 1.62</cell></row><row><cell>crypto-pbkdf2</cell><cell>3592</cell><cell>139</cell><cell cols="2">1.29 1.34</cell><cell>2.42</cell><cell cols="3">38.7 16 % 2.6 %</cell><cell>1.006</cell><cell>14.4</cell><cell>9.4</cell><cell>2.3 0.71 1.24</cell></row><row><cell>crypto-sha256-iterative</cell><cell>1136</cell><cell>45</cell><cell cols="2">1.36 1.41</cell><cell>2.54</cell><cell cols="3">39.6 16 % 2.5 %</cell><cell>1.011</cell><cell>13.9</cell><cell>8.5</cell><cell>2.2 0.87 1.01</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>octane</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>box2d</cell><cell>1958</cell><cell>131</cell><cell cols="2">1.26 1.30</cell><cell>2.01</cell><cell cols="3">66.9 20 % 1.6 %</cell><cell>1.016</cell><cell>16.5</cell><cell>9.2</cell><cell>3.2 1.64 2.46</cell></row><row><cell>code-load</cell><cell>2.8</cell><cell>12</cell><cell cols="2">1.31 1.33</cell><cell>1.70</cell><cell cols="3">4286 21 % 0.3 %</cell><cell>14.3</cell><cell>13.0</cell><cell>5.5</cell><cell>5.0 4.44 4.53</cell></row><row><cell>crypto</cell><cell>39480</cell><cell>1543</cell><cell>1.34</cell><cell></cell><cell>2.91</cell><cell cols="3">39.1 17 % 2.6 %</cell><cell>1.001</cell><cell>15.4</cell><cell></cell><cell>0.2 0.11</cell><cell>0.2</cell></row><row><cell>deltablue</cell><cell>9443</cell><cell>824</cell><cell cols="2">1.43 1.46</cell><cell>2.13</cell><cell cols="3">87.3 21 % 1.1 %</cell><cell>1.012</cell><cell>11.1</cell><cell>5.7</cell><cell>2.0 0.13 0.44</cell></row><row><cell>earley-boyer</cell><cell>12113</cell><cell>819</cell><cell cols="2">1.38 1.46</cell><cell>2.19</cell><cell cols="3">67.6 20 % 1.6 %</cell><cell>1.072</cell><cell>13.8</cell><cell>6.5</cell><cell>1.4 0.48 1.15</cell></row><row><cell>gbemu</cell><cell>1928</cell><cell>122</cell><cell cols="2">1.60 1.65</cell><cell>2.23</cell><cell cols="3">63.3 20 % 1.7 %</cell><cell>1.092</cell><cell>13.0</cell><cell>5.4</cell><cell>1.5 0.37 0.53</cell></row><row><cell>mandreel</cell><cell>3582</cell><cell>165</cell><cell cols="2">1.60 1.57</cell><cell>2.36</cell><cell cols="3">46.1 18 % 2.4 %</cell><cell>1.090</cell><cell>13.9</cell><cell>6.2</cell><cell>2.3 0.74 1.29</cell></row><row><cell>navier-stokes</cell><cell>10586</cell><cell>563</cell><cell cols="2">1.61 1.54</cell><cell>2.77</cell><cell cols="3">53.2 20 % 1.9 %</cell><cell>1.037</cell><cell>12.1</cell><cell>6.1</cell><cell>1.0 0.01 0.01</cell></row><row><cell>pdf</cell><cell>977</cell><cell>54</cell><cell cols="2">1.67 1.71</cell><cell>2.66</cell><cell cols="3">55.3 19 % 2.0 %</cell><cell>1.074</cell><cell>14.1</cell><cell>3.9</cell><cell>0.7 0.37 0.54</cell></row><row><cell>raytrace</cell><cell>4984</cell><cell>482</cell><cell cols="2">1.35 1.34</cell><cell>1.99</cell><cell cols="3">96.7 21 % 1.1 %</cell><cell>1.100</cell><cell>14.7</cell><cell>5.7</cell><cell>2.2 0.99 2.47</cell></row><row><cell>regexp</cell><cell>595</cell><cell>75</cell><cell cols="2">1.67 1.78</cell><cell>2.34</cell><cell>126.1</cell><cell cols="2">21 % 1.2 %</cell><cell>1.561</cell><cell>10.7</cell><cell>1.7</cell><cell>1.0 0.85</cell><cell>0.9</cell></row><row><cell>richards</cell><cell>12523</cell><cell>1029</cell><cell cols="2">1.38 1.25</cell><cell>2.26</cell><cell cols="3">82.2 21 % 1.3 %</cell><cell>1.000</cell><cell>12.4</cell><cell>8.8</cell><cell>1.9 0.42 0.71</cell></row><row><cell>splay</cell><cell>783</cell><cell>83</cell><cell cols="2">1.50 1.57</cell><cell>2.09</cell><cell>106.0</cell><cell cols="2">19 % 1.1 %</cell><cell>1.152</cell><cell>10.8</cell><cell>3.3</cell><cell>1.4 0.79 1.13</cell></row><row><cell>typescript</cell><cell>1263</cell><cell>120</cell><cell cols="2">1.26 1.31</cell><cell>1.85</cell><cell cols="3">95.0 20 % 1.1 %</cell><cell>1.079</cell><cell>14.2</cell><cell>6.5</cell><cell>3.0 2.64 3.86</cell></row><row><cell>zlib</cell><cell>41051</cell><cell>2106</cell><cell cols="2">1.47 1.45</cell><cell>2.47</cell><cell cols="3">51.3 18 % 2.2 %</cell><cell>1.138</cell><cell>13.5</cell><cell>8.0</cell><cell>2.0 0.46 1.40</cell></row><row><cell>average</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>13.6</cell><cell>6.3</cell><cell>1.7</cell><cell>0.7</cell><cell>1.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>CLI characteristics and branch prediction for Nehalem (Neh.), Sandy Bridge (SB), Haswell (Has.) and TAGE</figDesc><table><row><cell>benchmark</cell><cell>Gbc</cell><cell>Gins</cell><cell></cell><cell>IPC</cell><cell></cell><cell>ins/bc</cell><cell>br</cell><cell>ind</cell><cell>ind/bc</cell><cell></cell><cell></cell><cell cols="2">MPKI</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Neh.</cell><cell cols="2">SB Has.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Neh.</cell><cell cols="2">SB Has.</cell><cell>T1</cell><cell>T2</cell><cell>T3</cell></row><row><cell>164.gzip</cell><cell cols="2">78.8 1667.2</cell><cell cols="2">1.20 1.23</cell><cell>2.55</cell><cell>21.2</cell><cell cols="2">23 % 4.8 %</cell><cell>1.02</cell><cell cols="2">18.7 14.5</cell><cell>0.5</cell><cell>0.64</cell><cell>1.58 0.62</cell></row><row><cell>175.vpr</cell><cell>18.4</cell><cell>400.9</cell><cell>0.97</cell><cell>1</cell><cell>1.80</cell><cell>21.8</cell><cell cols="2">23 % 4.7 %</cell><cell>1.03</cell><cell cols="2">24.8 21.9</cell><cell>6.3</cell><cell>6.49 13.62 1.08</cell></row><row><cell>177.mesa</cell><cell cols="2">118.6 3177.1</cell><cell>1.32</cell><cell>1.3</cell><cell>2.13</cell><cell>26.8</cell><cell cols="2">23 % 4.0 %</cell><cell>1.07</cell><cell cols="2">13.8 11.1</cell><cell>2.6</cell><cell>0.21</cell><cell>0.59 0.20</cell></row><row><cell>179.art</cell><cell>4.7</cell><cell>58.5</cell><cell cols="2">1.64 1.49</cell><cell>2.70</cell><cell>12.5</cell><cell cols="2">26 % 8.0 %</cell><cell>1.00</cell><cell>7.3</cell><cell>9.3</cell><cell>0.2</cell><cell>0.38</cell><cell>0.38 0.37</cell></row><row><cell>181.mcf</cell><cell>13.3</cell><cell>181.2</cell><cell cols="2">1.13 1.19</cell><cell>2.19</cell><cell>13.7</cell><cell cols="2">25 % 7.4 %</cell><cell>1.01</cell><cell cols="2">17.5 15.1</cell><cell>1.1</cell><cell>1.33</cell><cell>2.09 1.08</cell></row><row><cell>183.equake</cell><cell>40.5</cell><cell>726.1</cell><cell cols="2">1.09 1.09</cell><cell>2.34</cell><cell>17.9</cell><cell cols="2">24 % 5.7 %</cell><cell>1.02</cell><cell>20.8</cell><cell>19</cell><cell>1.7</cell><cell>0.47</cell><cell>0.68 0.43</cell></row><row><cell>186.crafty</cell><cell cols="2">35.8 1047.3</cell><cell cols="2">1.02 1.03</cell><cell>1.42</cell><cell>29.2</cell><cell cols="2">22 % 3.5 %</cell><cell>1.04</cell><cell cols="2">21.1 19.2</cell><cell cols="2">11.6 11.87 16.31 4.01</cell></row><row><cell>188.ammp</cell><cell cols="2">91.3 1665.9</cell><cell cols="2">1.15 1.24</cell><cell>2.18</cell><cell>18.3</cell><cell cols="2">24 % 5.7 %</cell><cell>1.04</cell><cell cols="2">19.5 14.4</cell><cell>2.9</cell><cell>0.39</cell><cell>1.14 0.30</cell></row><row><cell>197.parser</cell><cell>12.6</cell><cell>447.5</cell><cell cols="2">1.19 1.24</cell><cell>2.18</cell><cell>35.4</cell><cell cols="2">22 % 3.0 %</cell><cell>1.06</cell><cell cols="2">14.4 10.8</cell><cell>1.4</cell><cell>1.01</cell><cell>2.75 0.70</cell></row><row><cell>256.bzip2</cell><cell>28.3</cell><cell>460.8</cell><cell cols="2">1.16 1.32</cell><cell>2.29</cell><cell>16.3</cell><cell cols="2">24 % 6.2 %</cell><cell>1.02</cell><cell cols="2">17.5 12.7</cell><cell>1.6</cell><cell>1.55</cell><cell>2.15 0.44</cell></row><row><cell>average</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">17.5 14.8</cell><cell>3.0</cell><cell>2.4</cell><cell>4.1</cell><cell>0.9</cell></row><row><cell cols="7">marks, however, run slower, because of the induced code</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">bloat, and the instruction cache behavior degradation. They</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">report the switch dispatch to be 12 instructions and 19 cy-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">cles on an UltraSparc-III processor, and on average 100 cy-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">cles per bytecode for the factorial function written in Tcl.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>(IT)TAGE misprediction results for "hard to predict" branch, TAGE 1, TAGE 2 and TAGE 3 (all numbers in %)</figDesc><table><row><cell>Python</cell><cell></cell><cell>indirect</cell><cell></cell><cell>ARG</cell><cell></cell><cell>Javascript</cell><cell></cell><cell></cell><cell></cell><cell>CLI</cell></row><row><cell></cell><cell>IT 1</cell><cell>IT 2</cell><cell cols="2">IT3 TAGE</cell><cell></cell><cell>IT 1</cell><cell>IT 2</cell><cell>IT3</cell><cell></cell><cell>IT 1</cell><cell>IT 2</cell><cell>IT 3</cell></row><row><cell>call-meth</cell><cell>0.827</cell><cell cols="2">0.827 0.827</cell><cell>0.000</cell><cell>ai-astar</cell><cell>0.012</cell><cell>0.012</cell><cell>0.012</cell><cell>164.gzip</cell><cell>0.612</cell><cell>2.698 0.569</cell></row><row><cell>call-meth-slots</cell><cell>0.827</cell><cell cols="2">0.827 0.827</cell><cell>0.000</cell><cell>a-beat-detec.</cell><cell>0.064</cell><cell>0.130</cell><cell>0.063</cell><cell>175.vpr</cell><cell cols="2">12.527 27.812 0.905</cell></row><row><cell>call-meth-unk</cell><cell>0.626</cell><cell cols="2">0.626 0.626</cell><cell>0.000</cell><cell>audio-dft</cell><cell>0.003</cell><cell>0.003</cell><cell>0.003</cell><cell>177.mesa</cell><cell>0.050</cell><cell>1.026 0.019</cell></row><row><cell>call-simple</cell><cell>0.991</cell><cell cols="2">0.991 0.990</cell><cell>0.000</cell><cell>audio-fft</cell><cell>0.064</cell><cell>0.064</cell><cell>0.064</cell><cell>179.art</cell><cell>0.077</cell><cell>0.083 0.075</cell></row><row><cell>chaos</cell><cell cols="3">0.165 21.362 0.163</cell><cell>3.456</cell><cell>a-oscillator</cell><cell>0.001</cell><cell>0.001</cell><cell>0.001</cell><cell>181.mcf</cell><cell>1.020</cell><cell>1.994 0.681</cell></row><row><cell>django-v2</cell><cell cols="3">0.308 22.421 0.016</cell><cell>0.372</cell><cell>i-darkroom</cell><cell>0.023</cell><cell>0.083</cell><cell>0.023</cell><cell>183.equake</cell><cell>0.185</cell><cell>0.541 0.113</cell></row><row><cell>fannkuch</cell><cell>0.652</cell><cell cols="2">0.718 0.630</cell><cell>0.001</cell><cell>i-desaturate</cell><cell>0.000</cell><cell>0.000</cell><cell>0.000</cell><cell>186.crafty</cell><cell cols="2">32.405 45.311 9.688</cell></row><row><cell>fastpickle</cell><cell>0.478</cell><cell cols="2">0.634 0.397</cell><cell>0.042</cell><cell>i-gaussian-blur</cell><cell>0.062</cell><cell>0.062</cell><cell>0.062</cell><cell>188.ammp</cell><cell>0.382</cell><cell>1.752 0.222</cell></row><row><cell>fastunpickle</cell><cell>0.723</cell><cell cols="2">3.730 0.547</cell><cell>0.060</cell><cell>j-parse-financial</cell><cell>0.317</cell><cell>0.572</cell><cell>0.163</cell><cell>197.parser</cell><cell>1.881</cell><cell>7.939 0.786</cell></row><row><cell>float</cell><cell>0.008</cell><cell cols="2">0.010 0.005</cell><cell>0.821</cell><cell>j-s-tinderbox</cell><cell>2.460</cell><cell>2.753</cell><cell>2.043</cell><cell>256.bzip2</cell><cell>2.190</cell><cell>3.102 0.470</cell></row><row><cell>formatted-log</cell><cell cols="3">0.136 48.212 0.021</cell><cell>1.216</cell><cell>c-aes</cell><cell>0.323</cell><cell>7.725</cell><cell>0.288</cell><cell></cell><cell></cell></row><row><cell>go</cell><cell cols="3">4.407 13.521 1.728</cell><cell>0.980</cell><cell>c-ccm</cell><cell>3.429</cell><cell>5.666</cell><cell>0.252</cell><cell></cell><cell></cell></row><row><cell>hexiom2</cell><cell>1.749</cell><cell cols="2">4.510 0.571</cell><cell>1.042</cell><cell>c-pbkdf2</cell><cell>0.100</cell><cell>2.039</cell><cell>0.094</cell><cell></cell><cell></cell></row><row><cell>json-dump-v2</cell><cell>0.015</cell><cell cols="2">0.200 0.001</cell><cell>0.841</cell><cell>c-sha256-it.</cell><cell>0.305</cell><cell>0.827</cell><cell>0.085</cell><cell></cell><cell></cell></row><row><cell>json-load</cell><cell>2.580</cell><cell cols="2">3.676 0.057</cell><cell>1.630</cell><cell>box2d</cell><cell cols="2">7.362 12.618</cell><cell>0.936</cell><cell></cell><cell></cell></row><row><cell>meteor-contest</cell><cell>0.460</cell><cell cols="2">0.558 0.281</cell><cell>0.583</cell><cell>code-load</cell><cell cols="3">31.662 36.134 24.257</cell><cell></cell><cell></cell></row><row><cell>nbody</cell><cell>0.002</cell><cell cols="2">0.003 0.002</cell><cell>0.758</cell><cell>crypto</cell><cell>0.166</cell><cell>0.494</cell><cell>0.108</cell><cell></cell><cell></cell></row><row><cell>nqueens</cell><cell>0.518</cell><cell cols="2">0.526 0.515</cell><cell>0.357</cell><cell>deltablue</cell><cell>0.380</cell><cell>3.042</cell><cell>0.032</cell><cell></cell><cell></cell></row><row><cell>pathlib</cell><cell>0.104</cell><cell cols="2">3.635 0.027</cell><cell>0.965</cell><cell>earley-boyer</cell><cell>0.902</cell><cell>5.264</cell><cell>0.748</cell><cell></cell><cell></cell></row><row><cell></cell><cell>1.719</cell><cell cols="2">3.169 0.719</cell><cell>2.958</cell><cell>gbemu</cell><cell>1.690</cell><cell>2.688</cell><cell>0.569</cell><cell></cell><cell></cell></row><row><cell>raytrace</cell><cell>0.873</cell><cell cols="2">6.691 0.228</cell><cell>3.861</cell><cell>mandreel</cell><cell>2.356</cell><cell>4.742</cell><cell>0.366</cell><cell></cell><cell></cell></row><row><cell>regex-compile</cell><cell cols="3">9.852 16.284 0.567</cell><cell>0.589</cell><cell>navier-stokes</cell><cell>0.024</cell><cell>0.029</cell><cell>0.022</cell><cell></cell><cell></cell></row><row><cell>regex-effbot</cell><cell>1.311</cell><cell cols="2">1.608 0.848</cell><cell>0.177</cell><cell>pdf</cell><cell>0.506</cell><cell>1.424</cell><cell>0.351</cell><cell></cell><cell></cell></row><row><cell>regex-v8</cell><cell>1.678</cell><cell cols="2">2.374 0.918</cell><cell>0.112</cell><cell>raytrace</cell><cell cols="2">6.100 20.337</cell><cell>0.439</cell><cell></cell><cell></cell></row><row><cell>richards</cell><cell>0.420</cell><cell cols="2">6.775 0.337</cell><cell>1.602</cell><cell>regexp</cell><cell>0.427</cell><cell>0.813</cell><cell>0.324</cell><cell></cell><cell></cell></row><row><cell>silent-logging</cell><cell>0.316</cell><cell cols="2">0.321 0.308</cell><cell>0.004</cell><cell>richards</cell><cell>0.544</cell><cell>2.769</cell><cell>0.474</cell><cell></cell><cell></cell></row><row><cell>simple-logging</cell><cell cols="3">0.030 53.552 0.021</cell><cell>1.197</cell><cell>splay</cell><cell>1.016</cell><cell>4.509</cell><cell>0.895</cell><cell></cell><cell></cell></row><row><cell>telco</cell><cell>0.264</cell><cell cols="2">0.342 0.172</cell><cell>0.044</cell><cell>typescript</cell><cell cols="2">18.291 29.630</cell><cell>4.100</cell><cell></cell><cell></cell></row><row><cell cols="2">unpack-sequence 0.027</cell><cell></cell><cell>0.025</cell><cell>0.001</cell><cell>zlib</cell><cell>2.079</cell><cell>6.771</cell><cell>0.228</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Alternatively, inline assembly can be used, at the expense of portability.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>http://code.google.com/p/octane-benchmark</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>http://krakenbenchmark.mozilla.org/kraken-1.1/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>https://github.com/andikleen/pmu-tools</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>despite a number of attempts...</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgment</head><p>This work was partially supported by the <rs type="funder">European Research Council Advanced</rs> Grant <rs type="grantNumber">DAL No 267175</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_G2EjSdT">
					<idno type="grant-number">DAL No 267175</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">J. R. Bell. Threaded code. CACM</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Context threading: A flexible and efficient dispatch technique for virtual machine interpreters</title>
		<author>
			<persName><forename type="first">M</forename><surname>Berndl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Vitale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaleski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CGO</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Virtual-machine abstraction and optimization techniques</title>
		<author>
			<persName><forename type="first">S</forename><surname>Brunthaler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<biblScope unit="volume">253</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="3" to="14" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>Electronic Notes in Theoretical</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Target prediction for indirect jumps</title>
		<author>
			<persName><forename type="first">P.-Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">CLI back-end in GCC</title>
		<author>
			<persName><forename type="first">R</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Ornstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rohou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GCC Developers&apos; Summit</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Compiling Java just in time</title>
		<author>
			<persName><forename type="first">T</forename><surname>Cramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Seberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wolczko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">17</biblScope>
			<pubPlace>Micro, IEEE</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cint: a RISC interpreter for the C programming language</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Gresh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Interpreters and interpretive techniques</title>
		<imprint>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Accurate indirect branch prediction</title>
		<author>
			<persName><forename type="first">K</forename><surname>Driesen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>H?lzle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The Dalvik virtual machine architecture</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ehringer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Stack caching for interpreters</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PLDI</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The behavior of efficient virtual machine interpreters on modern architectures</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Ertl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gregg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EuroPar</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Optimizing indirect branch prediction accuracy in virtual machine interpreters</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Ertl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gregg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PLDI</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Combining stack caching with dynamic superinstructions</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Ertl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gregg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Interpreters, Virtual Machines and Emulators</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Intel 64 and IA-32 Architectures Software Developer&apos;s Manual</title>
		<author>
			<persName><surname>Intel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dynamic branch prediction with perceptrons</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jim?nez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The bi-mode branch predictor</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I.-C</forename><forename type="middle">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Mudge</surname></persName>
		</author>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pin: building customized program analysis tools with dynamic instrumentation</title>
		<author>
			<persName><forename type="first">C.-K</forename><surname>Luk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PLDI</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">How to tune applications using a top-down characterization of microarchitectural issues</title>
		<author>
			<persName><forename type="first">J</forename><surname>Marusarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cepeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yasin</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note type="report_type">Technical report</note>
	<note>Intel</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Compiler techniques to improve dynamic branch prediction for indirect jump and call instructions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mccandless</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gregg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>ACM TACO</publisher>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Combining branch predictors</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mcfarling</surname></persName>
		</author>
		<idno>TN-36</idno>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
		<respStmt>
			<orgName>Digital Western Research Laboratory</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Trading conflict and capacity aliasing in conditional branch predictors</title>
		<author>
			<persName><forename type="first">P</forename><surname>Michaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Seznec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Uhlig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGARCH Comput. Archit. News</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Producing wrong data without doing anything obviously wrong</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mytkowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Diwan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hauswirth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Sweeney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The role of interpreters in high performance computing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACAT in Physics Research</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Optimizing direct threaded code by selective inlining</title>
		<author>
			<persName><forename type="first">I</forename><surname>Piumarta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Riccardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGPLAN Not</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An analysis of the dynamic behavior of JavaScript programs</title>
		<author>
			<persName><forename type="first">G</forename><surname>Richards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lebresne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Burg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vitek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PLDI</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Tiptop: Hardware Performance Counters for the Masses</title>
		<author>
			<persName><forename type="first">E</forename><surname>Rohou</surname></persName>
		</author>
		<idno>RR-7789</idno>
	</analytic>
	<monogr>
		<title level="j">INRIA</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficient interpreter optimizations for the JVM</title>
		<author>
			<persName><forename type="first">G</forename><surname>Savrun-Yenic ?eri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PPPJ</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Analysis of the O-GEometric History Length Branch Predictor</title>
		<author>
			<persName><forename type="first">A</forename><surname>Seznec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A new case for the TAGE branch predictor</title>
		<author>
			<persName><forename type="first">A</forename><surname>Seznec</surname></persName>
		</author>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Design tradeoffs for the Alpha EV8 conditional branch predictor</title>
		<author>
			<persName><forename type="first">A</forename><surname>Seznec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sazeides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A case for (partially) TAgged GEometric history length branch prediction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Seznec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Michaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JILP</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Virtual machine showdown: Stack versus registers</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Casey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Ertl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gregg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TACO</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The agree predictor: A mechanism for reducing negative branch history interference</title>
		<author>
			<persName><forename type="first">E</forename><surname>Sprangle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Chappell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alsup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Catenation and specialization for Tcl virtual machine performance</title>
		<author>
			<persName><forename type="first">B</forename><surname>Vitale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Abdelrahman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Interpreters, virtual machines and emulators</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Can hardware performance counters produce expected, deterministic results?</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Weaver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dongarra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FHPM</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The HERSCHEL/PACS Common Software System as Data Reduction System</title>
		<author>
			<persName><forename type="first">E</forename><surname>Wieprecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ADASS</title>
		<imprint>
			<biblScope unit="volume">XIII</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Two-level adaptive training branch prediction</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICRO</title>
		<imprint>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
