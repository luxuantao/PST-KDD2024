<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Graph Similarity Computation with Alignment Regularization</title>
				<funder ref="#_qD3XbNd">
					<orgName type="full">Shenzhen Baisc Research Fund</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wei</forename><surname>Zhuo</surname></persName>
							<email>zhuow5@mail2.sysu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shenzhen Campus of Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guang</forename><surname>Tan</surname></persName>
							<email>tanguang@mail.sysu.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Shenzhen Campus of Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient Graph Similarity Computation with Alignment Regularization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider the graph similarity computation (GSC) task based on graph edit distance (GED) estimation. State-of-the-art methods treat GSC as a learningbased prediction task using Graph Neural Networks (GNNs). To capture finegrained interactions between pair-wise graphs, these methods mostly contain a node-level matching module in the end-to-end learning pipeline, which causes high computational costs in both the training and inference stages. We show that the expensive node-to-node matching module is not necessary for GSC, and highquality learning can be attained with a simple yet powerful regularization technique, which we call the Alignment Regularization (AReg). In the training stage, the AReg term imposes a node-graph correspondence constraint on the GNN encoder. In the inference stage, the graph-level representations learned by the GNN encoder are directly used to compute the similarity score without using AReg again to speed up inference. We further propose a multi-scale GED discriminator to enhance the expressive ability of the learned representations. Extensive experiments on real-world datasets demonstrate the effectiveness, efficiency and transferability of our approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graph similarity computation (GSC) is a fundamental task in graph databases and plays a critical role in many real-world applications, including drug design <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b27">28]</ref>, program analysis <ref type="bibr" target="#b15">[16]</ref>, and social group identification <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b26">27]</ref>. For example, one can search a drug database for a query chemical compound, in order to identify drugs with high similarity in structures or attributes and thus similar curative effects as desired <ref type="bibr" target="#b22">[23]</ref>. To measure the similarity between pair-wise graphs, Graph Edit Distance (GED) <ref type="bibr" target="#b4">[5]</ref> has been a major metric due to its generality, and many other graph similarity measures have been proven to be its special cases <ref type="bibr" target="#b16">[17]</ref>. Unfortunately, computing exact GED is an NP-hard problem in general <ref type="bibr" target="#b11">[12]</ref>.</p><p>With the provably expressive power in distinguishing graph structures <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b33">34]</ref>, Graph Neural Networks (GNNs) have been adopted for GED approximation and shown to achieve superior performance on accuracy. Most state-of-the-art GNN-based GSC models <ref type="bibr">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b15">16]</ref> contain two sequential submodules in the end-to-end learnable pipeline (left of Fig. <ref type="figure" target="#fig_0">1</ref>): (1) a GNN encoder, which is shared across two graphs to embed nodes into representation vectors to capture the intra-graph structure and feature information; (2) a matching model, which computes cross-graph node-level similarity, i.e., how a node in one graph relates to all the nodes in the other graph. The model outputs a summarized vector that fuses the node-level similarities between two graphs. Then, the similarity score is predicted based on the summarized vector via a regression head. The computational cost of such a sequential framework mainly comes from the matching model, which requires computational and memory cost quadratic in the number of nodes and sometimes involves additional parameters such as attention weights <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b15">16]</ref>, leading to heavy time consumption in both the training and inference stages. Especially in the inference stage, we need to query every testing graph from the database. The recent approach EGSC <ref type="bibr" target="#b21">[22]</ref> speeds up the similarity learning by dropping the matching model from SimGNN <ref type="bibr">[1]</ref>. However, since cross-graph node-level interactions are ignored, EGSC cannot capture finer-grained similarity information, and thus results in suboptimal prediction performance. To overcome the intrinsic tension between predictive accuracy and speed, we propose a separated neural structure (right of Fig. <ref type="figure" target="#fig_0">1</ref>) that detaches the matching model from the sequential pipeline, where the matching model only acts as a regularization term in the training stage to help the GNN encoder capture fine-grained similarity information, while in the inference (testing) stage, since the latent cross-graph interactions have been learned by the GNN encoder, the final similarity score can be directly computed based on the output representations of the GNN encoder without invoking the matching model again. We show that explicitly learning the cross-graph nodeto-node similarity is unnecessary, as the correlation information contained in the features and graph topology, when properly exploited, is sufficient to reflect such cross-graph interactions (see Section 3). Specifically, the problem of GED computation is equivalent to finding an optimal permutation such that the adjacency matrices of the two graphs can be best aligned. Hence, by analyzing the necessary conditions under the optimal permutation, we find that the best matching between two graphs can be inferred by minimizing the difference between the intra-graph node-graph similarity and cross-graph node-graph similarity. It motivates us to design a task-agnostic matching model based on the input data itself, with a novel regularization technique, called the Alignment Regularization (AReg). AReg obviates the need of a matching model in the inference stage, thus making the model more efficient. AReg is also model-agnostic and can be applied to other GNN-based GSC models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GNN Encoder</head><note type="other">Matching</note><p>On the other hand, GNN-based GSC models usually use a single GED discriminator followed by a regression head to fuse graph representations from pair-wise input graphs and output a final similarity prediction score. We show that a single GED discriminator does not fully capture the dissimilarity between two graphs, while diverse discriminators may provide complementary information to reflect GED more accurately. Thus, we propose a multi-scale GED discriminator to improve the discriminability of the learned representations. We call the overall framework ERIC: Efficient gRaph sImilarity Computation, and conduct extensive experiments to verify the efficacy of our design.</p><p>Results on several real-world datasets demonstrate that ERIC achieves state-of-the-art performance by significantly outperforming the baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminary</head><p>Problem Formulation of GSC. Given a graph database D and a set of query graphs Q, the problem of graph similarity computation is to produce a similarity score y between ?G i ? Q and ?G j ? D, i.e., y = s(G i , G j ) where s :</p><formula xml:id="formula_0">D ? Q ? R (0,1] is a similarity estimator. A graph G ? D ? Q is defined as G = (V, E), where V = {v k } N k=1</formula><p>is a node set and E ? V ? V is an edge set. In our setting, all the accessible graphs are unweighted and undirected. V and E jointly formulate an adjacency matrix A ? R N ?N . If nodes are accompanied by features (e.g., labels or attribute vectors), they are represented as X ? R N ?d with dimension d.</p><p>Graph Edit Distance (GED). GED as a graph similarity measure has been popularly adopted on graph search queries, due to its capacity to capture the structural and feature differences between graphs. As shown in Fig. <ref type="figure">2</ref>, GED is defined as the number of edit operations in the optimal path that transforms G i into G j , where the possible edit operations under consideration include edge deletion/insertion, node deletion<ref type="foot" target="#foot_0">1</ref> /insertion, and node relabeling. To well fit the end-to-end regression task, instead of directly estimating the GED between two graphs, we convert the GED to the ground truth similarity score that the learning model aims to approximate. Following <ref type="bibr">[1]</ref>, the normalized GED is defined as nGED(G i , G j ) = GED(Gi,Gj ) (|Vi|+|Vj |)/2 , and the ground truth similarity score between G i and G j is defined as the normalized exponential of GED, resulting in a value ranging (0, 1], i.e., S ij = exp(-nGED(G i , G j )). Figure <ref type="figure">2</ref>: The optimal edit path with 3 edit operations to transform G i to G j . As a result, GED(G i , G j ) = 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Motivation: Analyzing GED in Embedding Space</head><p>Given two graphs G i = (A i , X i ) and G j = (A j , X j ) with the same number of nodes N (If they have different numbers of nodes, pad the smaller A and X with zeros to make the two graphs equal in size), ?(?) is a node index permutation that preserves the adjacency matrix. ?(A) denotes the adjacency matrix after node index permuting. We divide the computation of GED between G i and G j into two steps: (i) finding a permutation for G j , such that</p><formula xml:id="formula_1">c = min ? k,l (A i -?(A j )) [k, l] ,<label>(1)</label></formula><p>where c is the sum of the absolute values of all elements in matrix A i -?(A j ). We denote the optimal permutation satisfying Eq. ( <ref type="formula" target="#formula_1">1</ref>) as ? ? . (ii) counting the number of cross-graph node pairs with the same indices yet different features, denoted as m. Then GED(G i , G j ) = c 2 + m. Obviously, if two graphs are topologically isomorphic, there exists ? = ? ? such that A i = ? ? (A j ), i.e., c = 0. Hence the GED is only determined by distinct features. For another example, when the node permutation ? assigns indices to G j as shown in Fig. <ref type="figure">2</ref>, the structure of G i and G j can be best aligned, i.e., c = 4. Then, only one pair of nodes with the same index across graphs have different features (e.g., node 4). Thus, GED(G i , G j ) = 3. The core of this two-step method is finding the optimal permutation to best align two graphs, then m is determined under such alignment. Traversing all possible permutations to find ? ? is also NP-hard, so we make some heuristic rules from Eq. ( <ref type="formula" target="#formula_1">1</ref>) to guide the model design. Under the optimal permutation ? ? , the row similarity between A i and ? ? (A j ) is maximized, so given an injective function f ? (?) : R N ? R d parameterized by ? to guarantee that nodes with different connectivity can be distinguished, a necessary condition is that the distance between</p><formula xml:id="formula_2">f ? (A i [k]) and f ? (? ? (A j )[k]) is minimized for all k ? {1, ? ? ? , N }.</formula><p>From a global view, the matrix similarity between A i and ? ? (A j ) is also maximized. Given an injective function g ? (?) : R N ?N ? R d parameterized by ?, another necessary condition under the optimal permutation is that the distance between g ? (A i ) and g ? (? ? (A j )) is minimized. Thus, the optimal permutation ? ? in Eq. ( <ref type="formula" target="#formula_1">1</ref>) also satisfies the following function,</p><formula xml:id="formula_3">? ? = arg min ? DIST (f ? (A i [k]), f ? (?(A j )[k]))+DIST (g ? (A i ), g ? (?(A j ))) ?k ? {1, ? ? ? , N },<label>(2)</label></formula><p>where DIST(?, ?) is a distance metric. The two terms in Eq. (2) can reflect GED at node-level and global-level respectively. Further, we regard {f</p><formula xml:id="formula_4">? (A i [k])} N k=1 ? {f ? (? ? (A j )[k])} N k=1</formula><p>as 2N anchors and assume N ? d. When the second term of Eq. (2) reaches a minimum, it indicates that g ? (A i ) and g ? (? ? (A j )) have similar distances to all anchors, i.e., the following ? i and ? j take the minimum value when ? = ? ? , The GNN encoder is shared across two graphs. The green lines denote AReg. The summarized graph representations Z i and Z j are combinations of graph representations learned in each layer, which are fed into the GED discriminator followed by a regression function to obtain the prediction value. In the inference stage, the AReg submodule is removed.</p><formula xml:id="formula_5">? i = N k ?DIST (f ? (A i [k]), g ? (A i )) -DIST (f ? (A i [k]), g ? (?(A j )))? 2<label>(3)</label></formula><formula xml:id="formula_6">? j = N k ?DIST (f ? (?(A j )[k]), g ? (A i )) -DIST (f ? (?(A j )[k]), g ? (?(A j )))? 2 .<label>(4)</label></formula><p>On the other hand, the first term of Eq. ( <ref type="formula" target="#formula_3">2</ref>) is a finer-grained alignment between two graphs, and ? ? causes the first term of Eq. ( <ref type="formula" target="#formula_3">2</ref>) to reach a minimum. Since the only difference between ? i and ? j is that</p><formula xml:id="formula_7">? j replaces f ? (A i [k]) in ? i with f ? (?(A j )[k]), the distance between f ? (A i [k]) and f ? (?(A j )[k]</formula><p>) can be reflected by the difference between ? i and ? j , i.e., ?? i -? j ? 2 . Hence, under the optimal permutation ? = ? ? , G i and G j are best aligned, and so Eq. ( <ref type="formula" target="#formula_3">2</ref>) is established, which is equivalent to ? i , ? j , and ?? i -? j ? 2 all taking the minimum values. These terms reflect similarities between pair-wise graphs at both node-and graph-levels, which can be inferred from the graph structure itself without consulting the ground-truth GEDs. It motivates us to separate the matching model from the end-to-end pipeline. In other words, instead of directly searching the optimal permutation ? ? , we derive the necessary conditions under ? ? , which impose additional constraints on the coordinates of nodes in the embedding space. These conditions are not necessarily sufficient, nevertheless they can provide useful knowledge for the model to learn representations that are better tailored to the GSC task. In Appendix A, we further analyze the rationality of this approximation by analyzing the sufficiency of these conditions. Assuming g ? is a permutation-invariant function, then all permutation operators in Eq. (3) and Eq. ( <ref type="formula" target="#formula_6">4</ref>) can be removed. Moreover, instead of computing cross-graph node-to-node similarity to compute a 'soft' alignment as done in most related work, ? i and ? j only compute node-to-graph similarity, which is more efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Proposed Model: ERIC</head><p>The proposed ERIC framework mainly consists of two submodules: the Alignment Regularization (AReg) module and the Multi-Scale GED Discriminator, as depicted in Fig. <ref type="figure" target="#fig_2">3</ref>. AReg aims to train the shared GNN encoder to enable the GNN to capture underlying alignment information between pair-wise graphs. The multi-scale GED discriminator trains the same GNN encoder so that the learned representations of two graphs can accurately reflect the GED. Such a paradigm follows the intuition of the GED definition that when two graphs are best aligned, the difference between them is the GED.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Alignment Regularization</head><p>Learning the optimal alignment between graphs is crucial for GED estimation, however, most existing GSC models rely on intricate cross-graph node-to-node similarity computation to learn a 'soft' alignment. Furthermore, learning node-to-node similarity inevitably yields a dense similarity matrix, which could introduce more noise. To address this issue, we introduce Alignment Regularization (AReg) as a part of our framework. AReg is a model-and task-agnostic regularization term, which can easily be combined with existing embedding-based GSC models in a plug-and-play manner.</p><p>The design of AReg is motivated by the analysis of GED in Section 3. Recall that GED is the difference between two graphs when they are best aligned under the optimal permutation ? ? , whose necessary conditions are that ? i , ? j as well as ?? i -? j ? 2 take the minimum values. Thus, we expect that the learning-based model can couple with the nature of GED as much as possible, i.e., the GNN encoder can preserve the best alignment, in which case the difference between the learned graph representations can reflect the GED. To that end, we design the model based on the necessary conditions under ? ? . Specifically, based on Eq. ( <ref type="formula" target="#formula_5">3</ref>) and Eq. ( <ref type="formula" target="#formula_6">4</ref>), we instantiate f ? (?) as the L-layer Graph Isomorphism Network (GIN) <ref type="bibr" target="#b31">[32]</ref>, then at the ?-th layer f</p><formula xml:id="formula_8">(?) ? (A i [k]</formula><p>) can be represented as:</p><formula xml:id="formula_9">H (?) i [k] = f (?) ? (A i [k]) = MLP (?) ? (1 + ? (?) )H (?-1) i [k] + A i [k]H (?-1) i ,<label>(5)</label></formula><p>where</p><formula xml:id="formula_10">? (?) is a learnable parameter, H (?) i ? R N ?d (?)</formula><p>is the feature matrix of G i at the ?-th layer where d (?) denotes the feature dimension, H (0) i = X i . On the other hand, AReg implements the readout function g ? with a one-layer permutation-invariant DeepSets <ref type="bibr" target="#b32">[33]</ref> to guarantee injectiveness, taking the form:</p><formula xml:id="formula_11">Z (?) i = g (?) ? (A i ) = MLP (?) ? N k f (?) ? (A i [k]) ,<label>(6)</label></formula><p>where MLP MLP . Since g ? (A j ) = g ? (? ? (A j )), and ? i and ? j are sums over all N nodes, hence they are permutation-invariant and we can remove all operation ?(?) in Eq. (3) and Eq. (4). DIST(?, ?) can be defined as any distance metrics such as cosine similarity. Let ? (?) i and ? (?) j be the value of Eq. (3) and Eq. ( <ref type="formula" target="#formula_6">4</ref>) at the ?-th layer, by considering multi-scale cross-graph interactions, AReg is represented as L AReg where:</p><formula xml:id="formula_12">L AReg = 1 L L ? ? (?) i + ? (?) j + ? (?) i -? (?) j 2 . (<label>7</label></formula><formula xml:id="formula_13">)</formula><p>Since ? i and ? j induced from Eq. ( <ref type="formula" target="#formula_3">2</ref>) integrate graph-level alignment and finer-grained node-level alignment, L AReg therefore preserves underlying cross-graph interactions without computing complicated node-to-node similarity, also making the training stage more efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Multi-Scale GED Discriminator</head><p>Now we have L graph-level representations for G i and G j respectively, denoted as {Z</p><formula xml:id="formula_14">(1) i , ? ? ? , Z (L) i } and {Z (1) j , ? ? ? , Z (L) j }.</formula><p>We concatenate the layer-wise graph representations for G as: Z = Z (?) L ?=1 , where denotes the concatenation operator along the last dimension to combine the graph representation in each layer, i.e., Z i , Z j ? R dms , and</p><formula xml:id="formula_15">d ms = ? d (?)</formula><p>MLP . Then they are fed into a GED discriminator that generates score vectors as GED similarity embedding for the graph pair. Neural Tensor Network (NTN) <ref type="bibr" target="#b25">[26]</ref> has demonstrated strong power to model the relation between the graph-level embeddings of two graphs <ref type="bibr">[1,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b29">30]</ref> thanks to its capacity of exploring the element-wise dependence among the features. However, directly using NTN as the discriminator may be expensive when the dimension of Z i and Z j is high. Thus, we decompose the weight matrix W t ? R dms?dms into two matrices W t 1 ? R dms?d ? and W t 2 ? R d ? ?dms where d ? ? d ms to reduce the number of parameters. Then NTN with decomposed weight matrices is used to measure the similarity between Z i and Z j :</p><formula xml:id="formula_16">s NTN (G i , G j ) = ? NTN ( Z i W t 1 )(W t 2 Z ? j ) : t ? {1, ? ? ? , T } ? + W 3 Z i , Z j + b ,<label>(8) where</label></formula><formula xml:id="formula_17">W 1 ? R dms?d ? ?T , W 2 ? R d ? ?dms?T , W 3 ? R T ?2dms</formula><p>, and b ? R T are learnable; T is a hyper-parameter controlling the output dimension, which is assigned as 16 for all datasets in our settings. [?] in Eq. ( <ref type="formula" target="#formula_16">8</ref>) means computing</p><formula xml:id="formula_18">( Z i W t 1 )(W t 2 Z ? j</formula><p>) for all t ? {1, ? ? ? , T } and stacking them as a T -dimensional tensor. ? NTN : R T ? R (0,1] is a fully-connected neural network with Sigmoid activation as a regression function to project the similarity score vector to the final predicted similarity value. NTN is an expressive and general similarity discriminator because it can approximate many similarity measures. The first term of Eq. ( <ref type="formula" target="#formula_16">8</ref>) can be regarded as a multi-head weighted cosine similarity function. It can also approximate kernel similarity between Z i and Z j according to the universal approximation theorem <ref type="bibr" target="#b12">[13]</ref>. The second term captures the residual knowledge. However, it is difficult for NTN to approximate high-order Minkowski distance between Z i and Z j , while diverse similarity discriminators may provide complementary information to reflect GED more accurately as shown in Fig. <ref type="figure">4</ref>. Hence, we consider an additional similarity discriminator based on exponential p-order Minkowski distance:</p><formula xml:id="formula_19">s p (G i , G j ) = ? p exp -Z i -Z j p ,<label>(9)</label></formula><p>where ? p : R dms ? R (0,1] is also a fully-connected neural network with Sigmoid activation. For simplicity, we uniformly set p = 2 (i.e., ? 2 distance) for all datasets, and analyze the sensitivity of the hyper-parameter p in Section 5.4. After two similarity scores s NTN (G i , G j ) and s p (G i , G j ) are obtained, the final estimated similarity score is given by:</p><formula xml:id="formula_20">s(G i , G j ) = ?s NTN (G i , G j ) + ?s p (G i , G j ),<label>(10)</label></formula><p>where ? and ? are trainable scalars denoting the weights of two similarity discriminators respectively. Given a graph database D, the GED discriminator is trained on a set of n training pairs (G i , G j ) ? D ? D. The predicted similarity is compared against the ground-truth similarity S ij based on GEDs with Mean Squared Error (MSE) loss function as:</p><formula xml:id="formula_21">L GED = 1 n (Gi,Gj )?D?D MSE (s(G i , G j ), S ij ) .<label>(11)</label></formula><p>Model training: Combining AReg and GED discriminator, the training stage aims to minimize the following overall objective function L = L GED + ?L AReg , where ? is an adjustable hyper-parameter controlling the strength of the regularization term.</p><p>Model inference: Given a set of query graphs Q and a graph database D, in the testing stage, all pairs of graphs (G i , G j ) ? Q ? D are fed into ERIC, and directly computing a similarity score for each pair based on Eq. ( <ref type="formula" target="#formula_20">10</ref>) without any node-level interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Complexity:</head><p>The time complexity of GIN in AReg is O(m) <ref type="bibr" target="#b30">[31]</ref> where m is the number of edges. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we empirically evaluate ERIC on the graph similarity computation task.  </p><formula xml:id="formula_22">(?10 -3 ) ? ? ? ? ? p@10 ? p@20 ? mse (?10 -3 ) ? ? ? ? ? p@10 ? p@20 ? mse (?10 -3 ) ? ? ? ? ? p@10 ? p@20 ? mse (?10 -3 ) ? ? ? ? ? p@</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Settings</head><p>Baselines. We implement two groups of baselines for comparison: (1) Combinatorial search-based Methods: Beam <ref type="bibr" target="#b19">[20]</ref>, Hungarian <ref type="bibr" target="#b14">[15]</ref>, and VJ <ref type="bibr" target="#b8">[9]</ref>; (2) GNN-based Methods: SimGNN <ref type="bibr">[1]</ref>, GMN <ref type="bibr" target="#b15">[16]</ref>, GraphSim <ref type="bibr" target="#b2">[3]</ref>, MGMN <ref type="bibr" target="#b17">[18]</ref>, and EGSC <ref type="bibr" target="#b21">[22]</ref>. We re-implemented all baselines with the same hyperparameters as the original literature provides, or carefully tuned the parameters to get the optimal results when they are not provided. We give more implementation details of ERIC and baselines in Appendix B.2.</p><p>Evaluation Metric. To comprehensively evaluate our model on the similarity computation task, following <ref type="bibr">[1,</ref><ref type="bibr" target="#b21">22]</ref> five metrics are adopted to evaluate results for fair comparisons: Mean Squared Error (MSE) which measures the average squared differences between the predicted and the groundtruth similarity. Spearman's Rank Correlation Coefficient (?) and Kendall's Rank Correlation Coefficient (? ) which evaluates the ranking correlations between the predicted and the true ranking results. Precision@k where k = 10, 20, which computes the interactions of the predicted and ground-truth top-k results divided by k. The smaller the MSE, the better performance of models; for ?, ? , and p@k, the larger the better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Main Results</head><p>The results of ERIC and the baselines on our benchmarks are reported in Table <ref type="table" target="#tab_0">1</ref>. For datasets with relatively small graphs whose ground-truth GEDs are exactly computed by the A ? algorithm, ERIC consistently achieves state-of-the-art performance across all evaluation metrics as shown in Table <ref type="table" target="#tab_0">1</ref>. For datasets with large graphs whose ground-truth GEDs are computed approximately, ERIC still achieves the best results on NCI109, and comparable results on IMDB. The suboptimal performance of the methods based on node-to-node similarity (SimGNN, GraphSim, GMN, and MGMN) demonstrate that overly dense and fine-grained similarity computation may not always bring a beneficial boost. EGSC also uses GIN as the backbone and totally ignores cross-graph node-level interactions, and it achieves the best results on metrics ? and ? on the IMDB dataset. The reason is that EGSC adopts intra-graph attention pooling and NTN in all layers to enhance the expression ability of representation vectors. However, the attention mechanism and full NTN need additional parameters which increase the burden of learning. Although EGSC proposed a student model based on knowledge distillation to speed up the inference process, it would sacrifice the prediction accuracy in most cases. The performance of ERIC over baselines illustrates the importance of fine-grained interactions and the proper design of such interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablation Study</head><p>Effectiveness of AReg and Multiple GED Discriminators. ERIC contains two key components: AReg and Multi-Scale GED Discriminator. To glean a deeper insight into how different components help ERIC to achieve highly competitive results, we conduct ablation experiments by removing individual components separately. Specifically, we alter the loss by removing the AReg term to  study the effect of the cross-graph node-graph interaction, with the results reported in ERIC (w/o AReg) of Table <ref type="table" target="#tab_2">2</ref>. We find that only using the MSE loss L GED will lead to a performance drop on the extracted three evaluation metrics, which confirms the necessity of AReg. In our design, the final similarity score is obtained by the weighted average of two similarity scores based on the NTN discriminator s NTN and the ? 2 discriminator s p respectively. To further investigate the impact of multiple GED discriminators, we remove one of s NTN and s p to study the effect of each discriminator.</p><p>As shown in Table <ref type="table" target="#tab_2">2</ref>, both ERIC (w/o NTN) and ERIC (w/o ? 2 ) cause a decrease in effectiveness, which demonstrates both of them contribute to the final performance, while the model benefits more from NTN.</p><p>Transferability of AReg Further, since AReg is a model-agnostic regularization term, we are interested in the transferability of AReg, so we evaluate the performance of applying AReg to other GSC models. We use SimGNN and EGSC as baselines. For SimGNN, we use AReg to replace the node-to-node similarity computation. For EGSC, we directly add AReg to the loss function. Table <ref type="table" target="#tab_3">3</ref> shows the effect of AReg on SimGNN and EGSC. As expected, the advantage of SimGNN+AReg over SimGNN shows that integrating the dense similarity matrix into the final similarity score may bring noise which affects performance. While the performance on EGSC proves that combining fine-grained similarity in a proper way, i.e., node-graph rather than node-node, can improve the model.</p><p>Sensitivity of Order p in ? p In the multi-scale GED discriminator module, we adopt the exponential p-order Minkowski distance as a similarity measure to further improve the separability of clusters in the graph embedding space. For simplicity, we directly use the 2-order Minkowski distance, i.e., ? 2 distance. Results in Table <ref type="table" target="#tab_2">2</ref> show the effectiveness of considering the ? 2 distance, and Fig. <ref type="figure">4</ref> demonstrates the complementarity of different similarity measures. To investigate the sensitivity of p in our model, we set p from <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8]</ref>, and run 10 times for each value of p.Then the results of mean square error with standard deviation are reported in Fig. <ref type="figure" target="#fig_6">5</ref>. We can observe that the performance increases first and then the error becomes stable when p ? 2. It shows that using the ? 2 distance as the GED discriminator is suitable for our model and higher-order Minkowski distance would not improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Inference Time</head><p>In ERIC, the cross-graph alignment only acts as a regularization term in the training stage but is no longer used in the inference stage. To evaluate the efficiency, we compare the performance of ERIC with baselines in terms of inference time in Table <ref type="table" target="#tab_4">4</ref>. In Table <ref type="table">5</ref> of Appendix B.1 we list the number of graph pairs in the inference stage (#Testing pairs), and all experiments are implemented with a single machine with 1 NVIDIA Quadro RTX 8000 GPU. As can be observed, node-to-node      similarity computation is more time-consuming on all datasets. Our proposed node-graph similarity computation does not incur substantial additional running time in practice. To summarize, ERIC is faster than all baseline models, while still achieving significantly higher accuracy on all datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Visualization of Graph Search</head><p>The goal of the graph search task is to find k graphs from the dataset that are most similar to the given query graph, which is a routine task in drug discovery <ref type="bibr" target="#b23">[24]</ref>. In Fig. <ref type="figure" target="#fig_9">6</ref> we show a case based on the AIDS700 dataset, where each graph represents a functional group. Comparing the exact similarity ranking computed by A ? and the estimated one computed by ERIC, we see that the ranking of ERIC has a high consistency with the exact ranking, which proves that ERIC is able to extract graphs that contain the similar substructure from around 700 graphs with varying size and structure and the top-ranked graph has a high degree of isomorphism with the query. It also demonstrates that ERIC can capture structural patterns shared across graphs. More results of visualization are provided in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Analysis of Node-to-Node Similarity</head><p>In Fig. <ref type="figure" target="#fig_10">7</ref>, we show the node-to-node similarity between a query graph and the graphs at different ranking positions in terms of normalized GEDs. It can be found that for graph pairs with a small GED, the node representations generated by ERIC show a clear correspondence between nodes as shown in Fig. <ref type="figure" target="#fig_10">7</ref> (a). As the GED increases, the correspondence between nodes across the graph gradually weakens, i.e., cross-graph node-to-node similarity reduces. Thus, the similarity matrices in Fig. <ref type="figure" target="#fig_10">7</ref> can guide us to find a better alignment between pair-wise graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>The goal of graph similarity computation (GSC) is to quantify the similarity between graphs under a specific similarity measure. Various similarity measures have been well studied in prior works, such as graph edit distance (GED) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b19">20]</ref> and maximum common subgraph (MCS) <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10]</ref>. Among these, GED is the most popular one, and many other similarity measures can be proven to be its special cases <ref type="bibr" target="#b16">[17]</ref>. However, computing the exact GED between two graphs is known to be NP-hard. In practice, the computation becomes challenging when the number of nodes is more than 16 <ref type="bibr" target="#b3">[4]</ref> using exact GED solvers such as the A ? algorithm <ref type="bibr" target="#b24">[25]</ref>. Thus, approximate algorithms have been proposed for GED-based GSC. These approximate methods can be broadly divided into two classes: (1) Combinatorial search-based methods, which aim to exploit combinatorial structures or theoretical lower-bounds to approximate GED. Beam <ref type="bibr" target="#b19">[20]</ref> is a GED estimator based on Beam Search; Hungarian <ref type="bibr" target="#b14">[15]</ref> is proposed based on the famous Hungarian algorithm for weighted graph matching; Hausdorff approximation <ref type="bibr" target="#b10">[11]</ref> provides a lower bound for the GED approximation; VJ <ref type="bibr" target="#b8">[9]</ref> uses the Volgenant and Jonker algorithm for GED approximation. However, these methods are highly heuristic-driven and run with either sub-exponential time or cubic time complexity, limiting the scalability as the graphs grow in size. Also, these methods totally ignore the node feature information, so that the underlying semantic similarity can not be captured. ( <ref type="formula" target="#formula_3">2</ref>) Learning-based methods, which are data-driven and aim to learn graph similarity from the data itself, hopefully with higher accuracy and far lower time costs compared with search-based methods. GMN <ref type="bibr" target="#b15">[16]</ref> introduces a cross-graph attention layer that allows the nodes in the two graphs to interact with each other and predicts graph similarity using the representation vectors that fuse cross-graph information. SimGNN <ref type="bibr">[1]</ref> relies on a shared GNN encoder, a neural tensor network, and a pairwise node comparison module to compute the similarity between two graphs. GraphSim <ref type="bibr" target="#b2">[3]</ref> extends SimGNN by using convolutional neural networks to capture the multi-scale node-level interactions. EGSC <ref type="bibr" target="#b21">[22]</ref> simplifies SimGNN by ignoring node-level interactions and uses knowledge distillation to accelerate the inference stage. MGMN <ref type="bibr" target="#b17">[18]</ref> employs a node-graph matching network to capture cross-level features between nodes of a graph and the other whole graph, where the cross-graph aggregation weights are computed by node-to-node attention coefficients. Our proposed ERIC is also a learning-based method. Unlike the above approaches that either discard node-level interactions entirely, or performs interactions between all node pairs, our method proposes a novel node-graph interaction paradigm that avoids dense similarity computation while preserving fine-grained interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We propose ERIC, a simple yet powerful GNN-based framework for the graph similarity computation task. Specifically, we first give a deep insight into the graph edit distance, and propose Alignment Regularization (AReg) which is a separated structure independent of the end-to-end learning pipeline. AReg frees the model from complicated node-to-node interaction for similarity computation. Further, we propose a multi-scale GED discriminator to improve the discriminative ability of the learned representations. We show the effectiveness of our model through a comprehensive set of experiments and analyses. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of separating the matching model from the end-to-end GSC framework to achieve a fast model (right side). In the fast model, the dotted arrow means the matching model does not participate in the similarity computation in the inference stage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: Overview of ERIC. The GNN encoder is shared across two graphs. The green lines denote AReg. The summarized graph representations Z i and Z j are combinations of graph representations learned in each layer, which are fed into the GED discriminator followed by a regression function to obtain the prediction value. In the inference stage, the AReg submodule is removed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure 4: t-SNE [29] visualization of the IMDB dataset. Each point is a graph encoded by the GNN encoder of ERIC. The green cross means a randomly sampled query graph; red points mean the top 50% of graph datasets that are most similar to the query graph based on ground-truth GEDs; blue points mean the remaining 50% graphs in the dataset. (a): Using NTN as the discriminator, many similar graphs do not cluster together around the query graph even if each cluster is tight. (b): Using ? 2 distance as the discriminator, different clusters are separated clearly but the query graph is close to the cluster boundary; in addition each cluster is dispersive. (c): By adaptively combining NTN and ? 2 distance, our model makes similar graphs closely located around the query, while dissimilar graphs are far away from the query.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>?</head><label></label><figDesc>have the same output dimension d (?)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>The cross-graph node-graph interactions have complexity O(max(N i , N j )). The time complexity of the GED discriminator is O(d ms d ? T ). Thus the complexity of ERIC is O(m + max(N i , N j ) + d ms d ? T ) in the training stage, while in the inference stage the complexity is O(m + d ms d ? T ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Impact of different order p on AIDS700 and LINUX datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Visualization of graph search examples on the AIDS700 dataset. Nodes with different labels are assigned different colors. (a) and (b) are similarity rankings based on the normalized GEDs computed by A ? (exact) and ERIC (estimation) respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Heatmaps of cross-graph node-to-node cosine similarity based on the node representations learned by the GNN encoder of ERIC. Y-axis means the node in a randomly sampled query graph.(a)~(e) are ranked by the exact normalized GEDs between the query graph and graphs of particular ranks in the database. For example, 0.571 (10th) means the nGED of the graph that ranks 10th in similarity to the query graph is 0.571. The color depth in the heatmap represents the similarity of the node pair; the deeper the color, the higher the similarity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>( d )</head><label>d</label><figDesc>Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] 2. If you are including theoretical results... (a) Did you state the full set of assumptions of all theoretical results? [Yes] See Section 3 (b) Did you include complete proofs of all theoretical results? [Yes] See Section 3 3. If you ran experiments... (a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] See Supplemental Material (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] See Appendix B.2 (c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] See Section 5.4 (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See Section 5.5 4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators? [Yes] (b) Did you mention the license of the assets? [No] (c) Did you include any new assets either in the supplemental material or as a URL? [No] (d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [No] (e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [No] 5. If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Evaluation on benchmarks. Bold : best.</figDesc><table><row><cell>AIDS700</cell><cell>LINUX</cell><cell>IMDB</cell><cell>NCI109</cell></row><row><cell>mse</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>10 ? p@20 ? set, and query set, respectively. The training set together with the validation set are called the database. More details of the datasets are presented in Appendix B.1.</figDesc><table><row><cell>Beam</cell><cell>12.090</cell><cell>0.609 0.463</cell><cell>0.481</cell><cell>0.493</cell><cell>9.268</cell><cell>0.827 0.714</cell><cell>0.973</cell><cell>0.924</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>VJ</cell><cell>29.157</cell><cell>0.517 0.383</cell><cell>0.310</cell><cell>0.345</cell><cell>63.86</cell><cell>0.581 0.450</cell><cell>0.287</cell><cell>0.251</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Hungarian</cell><cell>25.296</cell><cell>0.510 0.378</cell><cell>0.360</cell><cell>0.392</cell><cell>29.81</cell><cell>0.638 0.517</cell><cell>0.913</cell><cell>0.836</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SimGNN</cell><cell>1.573</cell><cell>0.835 0.678</cell><cell>0.417</cell><cell>0.489</cell><cell>2.479</cell><cell>0.912 0.791</cell><cell>0.635</cell><cell>0.650</cell><cell>1.437</cell><cell cols="2">0.871 0.752</cell><cell>0.710</cell><cell>0.769</cell><cell>7.767</cell><cell cols="2">0.576 0.435</cell><cell>0.023</cell><cell>0.040</cell></row><row><cell>GraphSim</cell><cell>2.014</cell><cell>0.839 0.662</cell><cell>0.401</cell><cell>0.499</cell><cell>0.762</cell><cell>0.953 0.882</cell><cell>0.956</cell><cell>0.951</cell><cell>1.924</cell><cell cols="2">0.825 0.821</cell><cell>0.813</cell><cell>0.825</cell><cell>8.752</cell><cell cols="2">0.557 0.497</cell><cell>0.086</cell><cell>0.032</cell></row><row><cell>GMN</cell><cell>4.610</cell><cell>0.672 0.497</cell><cell>0.200</cell><cell>0.263</cell><cell>2.571</cell><cell>0.906 0.763</cell><cell>0.888</cell><cell>0.856</cell><cell>4.320</cell><cell cols="2">0.665 0.601</cell><cell>0.588</cell><cell>0.593</cell><cell>11.710</cell><cell cols="2">0.336 0.358</cell><cell>0.017</cell><cell>0.019</cell></row><row><cell>EGSC</cell><cell>1.676</cell><cell>0.888 0.723</cell><cell>0.604</cell><cell>0.708</cell><cell>0.214</cell><cell>0.984 0.897</cell><cell>0.987</cell><cell>0.989</cell><cell>0.573</cell><cell cols="2">0.939 0.829</cell><cell>0.872</cell><cell>0.883</cell><cell>9.356</cell><cell cols="2">0.545 0.414</cell><cell>0.055</cell><cell>0.078</cell></row><row><cell>MGMN</cell><cell>2.297</cell><cell>0.904 0.736</cell><cell>0.456</cell><cell>0.534</cell><cell>2.040</cell><cell>0.965 0.858</cell><cell>0.956</cell><cell>0.920</cell><cell>0.496</cell><cell cols="2">0.881 0.803</cell><cell>0.874</cell><cell>0.861</cell><cell>9.631</cell><cell cols="2">0.492 0.426</cell><cell>0.015</cell><cell>0.051</cell></row><row><cell>ERIC</cell><cell>1.383</cell><cell>0.906 0.740</cell><cell>0.679</cell><cell>0.746</cell><cell>0.113</cell><cell>0.988 0.908</cell><cell>0.994</cell><cell>0.996</cell><cell>0.385</cell><cell cols="2">0.890 0.791</cell><cell>0.882</cell><cell>0.891</cell><cell>7.127</cell><cell cols="2">0.591 0.525</cell><cell>0.118</cell><cell>0.080</cell></row><row><cell cols="3">5.1 Datasets</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="19">We conduct experiments on four widely used GSC datasets including AIDS700, LINUX, IMDB [1],</cell></row><row><cell cols="19">and NCI109 [2]. Following the same splits as [1-3], i.e., 60%, 20%, and 20% of all graphs as training</cell></row><row><cell cols="3">set, validation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Ablation study on the key components of ERIC on AIDS700 and LINUX.</figDesc><table><row><cell></cell><cell></cell><cell>AIDS700</cell><cell></cell><cell></cell><cell>LINUX</cell></row><row><cell></cell><cell>mse</cell><cell>?</cell><cell>p@10</cell><cell>mse</cell><cell>?</cell><cell>p@10</cell></row><row><cell>ERIC</cell><cell cols="6">1.383 0.906 0.679 0.113 0.988 0.994</cell></row><row><cell cols="7">ERIC (w/o AReg) 1.573 0.886 0.652 0.363 0.965 0.979</cell></row><row><cell cols="7">ERIC (w/o NTN) 1.687 0.854 0.633 0.302 0.951 0.969</cell></row><row><cell>ERIC (w/o ?2)</cell><cell cols="6">1.466 0.881 0.674 0.253 0.974 0.980</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Transferability study of AReg on AIDS700 and LINUX.</figDesc><table><row><cell></cell><cell></cell><cell>AIDS700</cell><cell></cell><cell></cell><cell>LINUX</cell></row><row><cell></cell><cell>mse</cell><cell>?</cell><cell>p@10</cell><cell>mse</cell><cell>?</cell><cell>p@10</cell></row><row><cell>SimGNN</cell><cell cols="6">1.573 0.835 0.417 2.479 0.912 0.635</cell></row><row><cell cols="7">SimGNN+AReg 1.439 0.858 0.506 1.974 0.945 0.658</cell></row><row><cell>EGSC</cell><cell cols="6">1.676 0.888 0.604 0.214 0.984 0.987</cell></row><row><cell>EGSC+AReg</cell><cell cols="6">1.478 0.904 0.643 0.142 0.989 0.992</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Inference time (sec).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.500</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.475</cell></row><row><cell>Dataset AIDS700 LINUX</cell><cell cols="2">SimGNN GraphSim 10.773 14.043 19.347 31.238</cell><cell>GMN 23.975 82.489</cell><cell>MGMN 11.337 22.574</cell><cell>EGSC 8.763 21.573</cell><cell>ERIC 6.662 18.969</cell><cell>MSE(1e-3)</cell><cell>1.400 1.425 1.450</cell></row><row><cell>IMDB NCI109</cell><cell>225.682 2913.178</cell><cell cols="5">379.480 1253.551 3463.620 &gt; 10 4 3726.834 2097.405 1763.356 357.933 133.437 48.750</cell><cell></cell><cell>1.375</cell><cell>w/o</cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>p</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>For node deletion, all edges connected to the deleted node are also deleted. Although editing of multiple edges is involved, it is a single-time effort. Thus, node deletion is treated as a single graph edit operation.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments and Disclosure of Funding</head><p>This work is supported in part by <rs type="funder">Shenzhen Baisc Research Fund</rs> under grant <rs type="grantNumber">JCYJ20200109142217397</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_qD3XbNd">
					<idno type="grant-number">JCYJ20200109142217397</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Simgnn: A neural network approach to fast graph similarity computation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Twelfth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="384" to="392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Unsupervised inductive graph-level representation learning via graph-graph proximity</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Marinovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.01098</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning-based efficient graph similarity computation via multi-scale convolutional set matching</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="3219" to="3226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On the exact computation of the graph edit distance</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Blumenthal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gamper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="page" from="46" to="57" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On a relation between graph edit distance and maximum common subgraph</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition letters</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="689" to="694" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A graph distance metric based on the maximal common subgraph</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shearer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition letters</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="255" to="259" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On the equivalence between graph isomorphism testing and function approximation with gnns</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Villar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Interpretable graph similarity computation via differentiable optimal alignment of node embeddings</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Doan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Manchanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mahapatra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Reddy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="665" to="674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Speeding up graph edit distance computation through fast bipartite matching</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fankhauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Riesen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Graph-Based Representations in Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="102" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A graph distance metric combining maximum common subgraph and minimum common supergraph</title>
		<author>
			<persName><forename type="first">M.-L</forename><surname>Fern?ndez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Valiente</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6-7</biblScope>
			<biblScope unit="page" from="753" to="758" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A hausdorff heuristic for efficient computation of graph edit distance</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Plamondon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Savaria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Riesen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint IAPR International Workshops on Statistical Techniques in Pattern Recognition (SPR) and Structural and Syntactic Pattern Recognition (SSPR)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="83" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Computers and intractability: a guide to the theory of np-completeness (michael r. garey and david s. johnson)</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hartmanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Siam Review</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">90</biblScope>
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multilayer feedforward networks are universal approximators</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stinchcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="359" to="366" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Similarity search in biological and engineering databases</title>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pfeifle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sch?nauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Data Eng. Bull</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="37" to="44" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval research logistics quarterly</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="83" to="97" />
			<date type="published" when="1955">1955</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Graph matching networks for learning the similarity of graph structured objects</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dullien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3835" to="3845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Similarity search in graph databases: A multi-layered indexing approach</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE 33rd International Conference on Data Engineering (ICDE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="783" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multilevel graph matching networks for deep graph similarity learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Weisfeiler and leman go neural: Higher-order graph neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grohe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4602" to="4609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fast suboptimal algorithms for the computation of graph edit distance</title>
		<author>
			<persName><forename type="first">M</forename><surname>Neuhaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Riesen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint IAPR International Workshops on Statistical Techniques in Pattern Recognition (SPR) and Structural and Syntactic Pattern Recognition (SSPR)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="163" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Discovering patterns in social networks with graph matching algorithms</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ogaard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sambhoos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sudit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Social Computing, Behavioral-Cultural Modeling, and Prediction</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="341" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Slow learning and fast inference: Efficient graph similarity computation via knowledge distillation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ghashing: semantic graph hashing for approximate similarity search in graph databases</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2062" to="2072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Probabilistic substructure mining from small-molecule screens</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ranu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">T</forename><surname>Calhoun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Swamidass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Molecular Informatics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="809" to="815" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A novel software toolkit for graph edit distance computation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Riesen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Emmenegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Graph-Based Representations in Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="142" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Community detection in a large real-world social network</title>
		<author>
			<persName><forename type="first">K</forename><surname>Steinhaeuser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Social computing, behavioral modeling, and prediction</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="168" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Saga: a subgraph matching tool for biological graphs</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Mceachin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>States</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="232" to="239" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Combinatorial learning of graph edit distance via dynamic embedding</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5241" to="5250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="4" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=ryGs6iA5Km" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Deep sets. Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Proximity enhanced graph neural networks with channel contrast</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22</title>
		<meeting>the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22</meeting>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Do the main claims made in the abstract and introduction accurately reflect the paper&apos;s contributions and scope? [Yes] (b) Did you describe the limitations of your work</title>
		<imprint>
			<publisher>For all authors</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
