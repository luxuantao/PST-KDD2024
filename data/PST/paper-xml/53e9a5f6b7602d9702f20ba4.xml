<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Gaussian Process Priors With Uncertain Inputs -Application to Multiple-Step Ahead Time Series Forecasting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Agathe</forename><surname>Girard</surname></persName>
							<email>agathe@dcs.gla.ac.uk</email>
						</author>
						<author>
							<persName><forename type="first">Carl</forename><forename type="middle">Edward</forename><surname>Rasmussen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Joaquin</forename><surname>Qui</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ñonero</forename><surname>Candela</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Roderick</forename><surname>Murray-Smith</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing Science</orgName>
								<orgName type="institution">University of Glasgow Glasgow</orgName>
								<address>
									<postCode>G12 8QQ</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Gatsby Unit University College London</orgName>
								<address>
									<postCode>WC1N 3AR</postCode>
									<settlement>London</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Informatics and Mathematical Modelling</orgName>
								<orgName type="institution">Technical University of Denmark Richard Petersens Plads</orgName>
								<address>
									<postBox>Building 321</postBox>
									<postCode>DK-2800</postCode>
									<settlement>Kongens, Lyngby</settlement>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Department of Computing Science</orgName>
								<orgName type="institution">University of Glasgow</orgName>
								<address>
									<postCode>G12 8QQ</postCode>
									<settlement>Glasgow</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Hamilton Institute National University of Ireland</orgName>
								<address>
									<settlement>Maynooth</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Gaussian Process Priors With Uncertain Inputs -Application to Multiple-Step Ahead Time Series Forecasting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2F4B5863F11833238387E93E1DC78313</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider the problem of multi-step ahead prediction in time series analysis using the non-parametric Gaussian process model. -step ahead forecasting of a discrete-time non-linear dynamic system can be performed by doing repeated one-step ahead predictions. For a state-space model of the form Ý Ø ´ÝØ ½ Ý Ø Ä µ, the prediction of Ý at time Ø • is based on the point estimates of the previous outputs. In this paper, we show how, using an analytical Gaussian approximation, we can formally incorporate the uncertainty about intermediate regressor values, thus updating the uncertainty on the current prediction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>One of the main objectives in time series analysis is forecasting and in many real life problems, one has to predict ahead in time, up to a certain time horizon (sometimes called lead time or prediction horizon). Furthermore, knowledge of the uncertainty of the prediction is important. Currently, the multiple-step ahead prediction task is achieved by either explic-itly training a direct model to predict steps ahead, or by doing repeated one-step ahead predictions up to the desired horizon, which we call the iterative method.</p><p>There are a number of reasons why the iterative method might be preferred to the 'direct' one. Firstly, the direct method makes predictions for a fixed horizon only, making it computationally demanding if one is interested in different horizons. Furthermore, the larger , the more training data we need in order to achieve a good predictive performance, because of the larger number of 'missing' data between Ø and Ø • . On the other hand, the iterated method provides any -step ahead forecast, up to the desired horizon, as well as the joint probability distribution of the predicted points.</p><p>In the Gaussian process modelling approach, one computes predictive distributions whose means serve as output estimates. Gaussian processes (GPs) for regression have historically been first introduced by O'Hagan <ref type="bibr" target="#b0">[1]</ref> but started being a popular non-parametric modelling approach after the publication of <ref type="bibr" target="#b6">[7]</ref>. In <ref type="bibr" target="#b9">[10]</ref>, it is shown that GPs can achieve a predictive performance comparable to (if not better than) other modelling approaches like neural networks or local learning methods. We will show that for a -step ahead prediction which ignores the accumulating prediction variance, the model is not conservative enough, with unrealistically small uncertainty attached to the forecast. An alternative solution is presented for iterative -step ahead prediction, with propagation of the prediction uncertainty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Gaussian Process modelling</head><p>We briefly recall some fundamentals of Gaussian processes. For a comprehensive introduction, please refer to <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b10">[11]</ref>, or the more recent review <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The GP prior model</head><p>Formally, the random function, or stochastic process, ´Üµ is a Gaussian process, with mean Ñ´Üµ and covariance function ´ÜÔ Ü Õ µ, if its values at a finite number of points,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>´Ü½ µ</head><p>´ÜÒ µ, are seen as the components of a normally distributed random vector. If we further assume that the process is stationary: it has a constant mean and a covariance function only depending on the distance between the inputs Ü. For any Ò, we have</p><formula xml:id="formula_0">´Ü½ µ ´ÜÒ µ AE´¼ ¦µ<label>(1)</label></formula><p>with ¦ ÔÕ ÓÚ´ ´ÜÔ µ ´ÜÕ µµ ´ÜÔ Ü Õ µ giving the covariance between the points ´ÜÔ µ and ´ÜÕ µ, which is a function of the inputs corresponding to the same cases Ô and Õ. A common choice of covariance function is the Gaussian kernel<ref type="foot" target="#foot_0">1</ref> </p><formula xml:id="formula_1">´ÜÔ Ü Õ µ ÜÔ ½ ¾ ½ ´ÜÔ Ü Õ µ ¾ Û ¾<label>(2)</label></formula><p>where is the input dimension. The Û parameters (correlation length) allow a different distance measure for each input dimension . For a given problem, these parameters will be adjusted to the data at hand and, for irrelevant inputs, the corresponding Û will tend to zero.</p><p>The role of the covariance function in the GP framework is similar to that of the kernels used in the Support Vector Machines community. This particular choice corresponds to a prior assumption that the underlying function is smooth and continuous. It accounts for a high correlation between the outputs of cases with nearby inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Predicting with Gaussian Processes</head><p>Given this prior on the function and a set of data Ý Ü AE ½ , our aim, in this Bayesian setting, is to get the predictive distribution of the function value ´Ü£ µ corresponding to a new (given) input Ü £ .</p><p>If we assume an additive uncorrelated Gaussian white noise, with variance Ú ¼ , relates the targets (observations) to the function outputs, the distribution over the targets is Gaussian, with zero mean and covariance matrix such that Ã ÔÕ ¦ ÔÕ • Ú ¼ AE ÔÕ . We then adjust the vector of hyperparameters ¢ Û ½ Û Ú ½ Ú ¼ ℄ Ì so as to maximise the log-likelihood</p><p>Ä´¢µ ÐÓ Ô´Ý ¢µ, where Ø is the vector of observations. In this framework, for a new Ü £ , the predictive distribution is simply obtained by conditioning on the training data. The joint distribution of the variables being Gaussian, this conditional distribution, Ô´ ´Üµ £ Ü £ µis also Gaussian with mean and variance</p><formula xml:id="formula_2">´Ü£ µ ´Ü£ µ Ì Ã ½ Ý (3) ¾ ´Ü£ µ ´Ü£ µ ´Ü£ µ Ì Ã ½ ´Ü£ µ<label>(4)</label></formula><p>where ´Ü£ µ ´Ü£ Ü ½ µ ´Ü£ Ü AE µ℄ Ì is the AE ¢ ½ vector of covariances between the new point and the training targets and ´Ü£ µ ´Ü£ Ü £ µ ½, with ´ µ as given by <ref type="bibr" target="#b1">(2)</ref>.</p><p>The predictive mean serves as a point estimate of the function output, ´Ü£ µ with uncer- tainty ´Ü£ µ. And it is also a point estimate for the target, Ý £ , with variance ¾ ´Ü£ µ • Ú ¼ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Prediction at a random input</head><p>If we now assume that the input distribution is Gaussian,</p><formula xml:id="formula_3">Ü £ AE´ Ü £ ¦ Ü £ µ, the predictive distribution is now obtain by integrating over Ü £ Ô´ ´Ü£ µ Ü £ ¦ Ü £ µ Ô´ ´Ü£ µ Ü £ µÔ´Ü £ µ Ü £ (5)</formula><p>where Ô´ ´Ü£ µ Ü £ µ is Normal, as specified by ( <ref type="formula">3</ref>) and (4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Gaussian approximation</head><p>Given that this integral is analytically intractable (Ô´ ´Ü£ µ Ü £ µ is a complicated function of Ü £ ), we opt for an analytical Gaussian approximation and only compute the mean and variance of Ô´ ´Ü£ µ Ü £ ¦ Ü £ µ. Using the law of iterated expectations and conditional variance, the 'new' mean and variance are given by</p><formula xml:id="formula_4">Ñ´ Ü £ ¦ Ü £ µ Ü £ ´Ü£ µ ´Ü£ µ Ü £ ℄℄ Ü £ ´Ü£ µ℄ (6) Ú´ Ü £ ¦ Ü £ µ Ü £ Ú Ö ´Ü£ µ ´ ´Ü£ µ Ü £ µ℄ • Ú Ö Ü £ ´ ´Ü£ µ ´Ü£ µ Ü £ ℄µ Ü £ ¾ ´Ü£ µ℄ • Ú Ö Ü £ ´ ´Ü£ µµ (7)</formula><p>where Ü £ indicates the expectation under Ü £ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>In our initial development, we made additional approximations ([2]</head><p>). A first and second order Taylor expansions of ´Ü£ µ and ¾ ´Ü£ µ respectively, around Ü £ , led to</p><formula xml:id="formula_5">Ñ´ Ü £ ¦ Ü £ µ ´ Ü £ µ (8) Ú´ Ü £ ¦ Ü £ µ ¾ ´ Ü £ µ • ½ ¾ ÌÖ ´¾ ¾ ´Ü£ µ Ü £ Ü £Ì ¬ ¬ ¬ ¬ Ü £ Ü £ ¦ Ü £ µ • ´Ü£ µ Ü £ ¬ ¬ ¬ ¬ Ì Ü £ Ü £ ¦ Ü £ ´Ü£ µ Ü £ ¬ ¬ ¬ ¬ Ü £ Ü £<label>(9)</label></formula><p>The detailed calculations can be found in <ref type="bibr" target="#b1">[2]</ref>.</p><p>In <ref type="bibr" target="#b7">[8]</ref>, we derived the exact expressions of the first and second moments. Rewriting the predictive mean ´Ü£ µ as a linear combination of the covariance between the new Ü £ and the training points (as suggested in <ref type="bibr" target="#b11">[12]</ref>), with our choice of covariance function, the calculation of Ñ´Ü £ µ then involves the product of two Gaussian functions:</p><formula xml:id="formula_6">Ñ´ Ü £ ¦ Ü £ µ ´Ü£ µÔ´Ü £ µ Ü £ ¬ ´Ü£ Ü µÔ´Ü £ µ Ü £<label>(10)</label></formula><p>with ¬ Ã ½ Ý. This leads to (refer to <ref type="bibr" target="#b8">[9]</ref> for details)</p><formula xml:id="formula_7">Ñ´ Ü £ ¦ Ü £ µ Õ Ì ¬ (11) with Õ Ï ½ ¦ Ü £ • Á ½ ¾ ÜÔ ½ ¾ ´ Ü £ Ü µ Ì ´¦Ü £ • Ïµ ½ ´ Ü £ Ü µ ¡</formula><p>, where Ï Û ¾ ½ Û ¾ ℄ and Á is the ¢ identity matrix.</p><p>In the same manner, we obtain for the variance</p><formula xml:id="formula_8">Ú´ Ü £ ¦ Ü £ µ ´ Ü £ Ü £ • ÌÖ ¢ ´¬¬ Ì Ã ½ µÉ £ ÌÖ´Õ Ì ¬µ ¾<label>(12)</label></formula><p>with</p><formula xml:id="formula_9">É ¾Ï ½ ¦ Ü £ • Á ½ ¾ ÜÔ ½ ¾ ´Ü Ü £ µ Ì ´½ ¾ Ï • ¦ Ü £ µ ½ ´Ü Ü £ µ ÜÔ ½ ¾ ´Ü Ü µ Ì ´¾Ïµ ½ ´Ü Ü µ<label>(13)</label></formula><p>where Ü ´Ü • Ü µ ¾.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Monte-Carlo alternative</head><p>Equation ( <ref type="formula">5</ref>) can be solved by performing a numerical approximation of the integral, using a simple Monte-Carlo approach:</p><formula xml:id="formula_10">Ô´ ´Ü£ µ Ü £ ¦ Ü £ µ Ô´ ´Ü£ µ Ü £ µÔ´Ü £ µ Ü £ ³ ½ Ì Ì Ø ½ Ô´ ´Ü£ µ Ü £Ø µ (14)</formula><p>where Ü £Ø are (independent) samples from Ô´Ü £ µ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Iterative -step ahead prediction of time series</head><p>For the multiple-step ahead prediction task of time series, the iterative method consists in making repeated one-step ahead predictions, up to the desired horizon. Consider the time series Ý Ø½ Ý Ø and the state-space model Ý Ø ´ÜØ µ • ¯Ø where</p><formula xml:id="formula_11">Ü Ø Ý Ø ½ Ý Ø Ä ℄ Ì</formula><p>is the state at time Ø (we assume that the lag Ä is known)</p><p>and the (white) noise has variance Ú ¼ .</p><p>Then, the"naive" iterative -step ahead prediction method works as follows: it predicts only one time step ahead, using the estimate of the output of the current prediction, as well as previous outputs (up to the lag Ä), as the input to the prediction of the next time step, until the prediction steps ahead is made. That way, only the output estimates are used and the uncertainty induced by each successive prediction is not accounted for.</p><p>Using the results derived in the previous section, we suggest to formally incorporate the uncertainty information about the intermediate regressor. That is, as we predict ahead in time, we now view the lagged outputs as random variables. In this framework, the input at time Ø is a random vector with mean formed by the predicted means of the lagged outputs Ý Ø• , ½ Ä, given by <ref type="bibr" target="#b10">(11)</ref>. The Ä ¢ Ä input covariance matrix has the different predicted variances on its diagonal (with the estimated noise variance Ú ¼ added to them), computed with <ref type="bibr" target="#b11">(12)</ref>, and the off-diagonal elements are given by, in the case of the exact solution, ÓÚ´Ý Ø Ü Ø µ È ¬ Õ ´ ÜØ µ, where Õ is as defined previously and</p><formula xml:id="formula_12">´Ï ½ Ü • ¦ ½ Ü £ Ü £ µ with ´Ï ½ • ¦ ½ Ü £ µ ½ .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Illustrative examples</head><p>The first example is intended to provide a basis for comparing the approximate and exact solutions, within the Gaussian approximation of ( <ref type="formula">5</ref>)), to the numerical solution (Monte-Carlo sampling from the true distribution), when the uncertainty is propagated as we predict ahead in time. We use the second example, inspired from real-life problems, to show that iteratively predicting ahead in time without taking account of the uncertainties induced by each succesive prediction leads to inaccurate results, with unrealistically small error bars.</p><p>We then assess the predictive performance of the different methods by computing the average absolute error (Ä ½ ), the average squared error (Ä ¾ ) and average minus log predictive density<ref type="foot" target="#foot_1">2</ref> (Ä ¿ ), which measures the density of the actual true test output under the Gaussian predictive distribution and use its negative log as a measure of loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Forecasting the Mackey-Glass time series</head><p>The Mackey-Glass chaotic time series constitutes a wellknown benchmark and a challenge for the multiple-step ahead prediction task, due to its strong non-linearity <ref type="bibr" target="#b3">[4]</ref>:</p><formula xml:id="formula_13">Þ´Øµ Ø Þ´Øµ • Þ´Ø µ ½•Þ´Ø µ ½¼ . We have ¼ ¾, ¼ ½ and ½ .</formula><p>The series is re-sampled with period ½ and normalized. We choose Ä ½ for the number of lagged outputs in the state vector, Ü Þ ½ Þ ¾ Þ Ä ℄ and the targets, Ø Þ , are corrupted by a white noise with variance ¼ ¼¼½.</p><p>We train a GP model with a Gaussian kernel such as (2) on ½¼¼ points, taken at random from a series of ¼¼¼ points. Figure <ref type="figure" target="#fig_0">1</ref> shows the mean predictions with their uncertainties, given by the exact and approximate methods, and ¼ samples from the Monte-Carlo numerical approximation, from ½ to ½¼¼ steps ahead, for different starting points. Figure <ref type="figure" target="#fig_1">2</ref> shows the plot of the ½¼¼-step ahead mean predictions (left) and their ¾ uncertainties (right), given by the exact and approximate methods, as well as the sample mean and sample variance obtained with the numerical solution (average over ¼ points).</p><p>These figures show the better performance of the exact method on the approximate one. Also, they allow us to validate the Gaussian approximation, noticing that the error bars encompass the samples from the true distribution. Table <ref type="table">1</ref> provides a quantitative confirmation.</p><p>Table <ref type="table">1</ref>: Average (over ¼¼ test points) absolute error (Ä ½ ), squared error (Ä ¾ ) and minus log predictive density (Ä ¿ ) of the ½¼¼-step ahead predictions obtained using the exact method (Å ½ ), the approximate one (Å ¾ ) and the sampling from the true distribution (Å ¿ ).  the exact method (dash), the approximate (dot) and the sample mean and variance of the numerical solution (dash-dot).</p><formula xml:id="formula_14">Ä ½ Ä ¾ Ä ¿ Å ½ ¼ ¼ ¿¿ ¼ ¿ Å ¾ ¼ ¼ ¼ ¼ ½ ½¿¼¼ Å ¿ ¼ ¿ ¼<label>0</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Prediction of a pH process simulation</head><p>We now compare the iterative -step ahead prediction results obtained when propagating the uncertainty (using the approximate method) and when using the output estimates only (the naive approach). For doing so, we use the pH neutralisation process benchmark presented in <ref type="bibr" target="#b2">[3]</ref>. The training and test data consist of pH values (outputs Ý of the process) and a control input signal (Ù).</p><p>With a model of the form Ý Ø ´ÝØ Ý Ø ½ Ù Ø Ù Ø ½ µ, we train our GP on ½¾¾ examples and consider a test set of points (all data have been normalized). Figure <ref type="figure" target="#fig_3">3</ref> shows the ½¼-step ahead predicted means and variances obtained when propagating the uncertainty and when using information on the past predicted means only. The losses calculated are the following: Ä ½ ¼ ½ ½ , Ä ¾ ¼ ¼ and Ä ¿ ¼ ¾¼ for the approximate method and Ä ¿ ½ ¼ ¾ for the naive one!  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We have presented a novel approach which allows us to use knowledge of the variance on inputs to Gaussian process models to achieve more realistic prediction variance in the case of noisy inputs.</p><p>Iterating this approach allows us to use it as a method for efficient propagation of uncertainty in the multi-step ahead prediction task of non-linear time-series. In experiments on simulated dynamic systems, comparing our Gaussian approximation to Monte Carlo simulations, we found that the propagation method is comparable to Monte Carlo simulations, and that both approaches achieved more realistic error bars than a naive approach which ignores the uncertainty on current state. This method can help understanding the underlying dynamics of a system, as well as being useful, for instance, in a model predictive control framework where knowledge of the accuracy of the model predictions over the whole prediction horizon is required (see <ref type="bibr" target="#b5">[6]</ref> for a model predictive control law based on Gaussian processes taking account of the prediction uncertainty). Note that this method is also useful in its own right in the case of noisy model inputs, assuming they have a Gaussian distribution.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Iterative method in action: simulation from ½ to ½¼¼ steps ahead for different starting points in the test series. Mean predictions with ¾ error bars given by the exact (dash) and approximate (dot) methods. Also plotted, ¼ samples obtained using the numerical approximation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: ½¼¼-step ahead mean predictions (left) and uncertainties (right.) obtained using</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: Predictions from ½ to ½¼ steps ahead (left). ½¼-step ahead mean predictions with the corresponding variances, when propagating the uncertainty (dot) and when using the previous point estimates only (dash).</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>This choice was motivated by the fact that, in<ref type="bibr" target="#b7">[8]</ref>, we were aiming at unified expressions for the GPs and the Relevance Vector Machines models which employ such a kernel. More discussion about possible covariance functions can be found in<ref type="bibr" target="#b4">[5]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>To evaluate these losses in the case of Monte-Carlo sampling, we use the sample mean and sample variance.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>Many thanks to Mike Titterington for his useful comments. The authors gratefully acknowledge the support of the Multi-Agent Control Research Training Network -EC TMR grant HPRN-CT-1999-00107 and RM-S is grateful for EPSRC grant Modern statistical approaches to off-equilibrium modelling for nonlinear system control GR/M76379/01.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Curve fitting and optimal design for prediction</title>
		<author>
			<persName><forename type="first">A</forename><surname>O'hagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society B</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Gaussian Process Priors With Uncertain Inputs: Multiple-Step Ahead Prediction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Girard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Murray-Smith</surname></persName>
		</author>
		<idno>TR-2002-119</idno>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
		<respStmt>
			<orgName>Dept. of Computing Science, University of Glasgow</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adaptive nonlinear control of a pH neutralisation process</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Henson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Seborg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Control System Technology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="169" to="183" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Oscillation and Chaos in Physiological Control Systems</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Mackey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">197</biblScope>
			<biblScope unit="page" from="287" to="289" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Gaussian Processes -A Replacement for Supervised Neural Networks</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J C</forename><surname>Mackay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lecture notes for a tutorial at NIPS</title>
		<imprint>
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Nonlinear adaptive control using non-parametric Gaussian process prior models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Murray-Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sbarbaro-Hofer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<pubPlace>Barcelona</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Bayesian Learning for Neural Networks PhD thesis</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
		<respStmt>
			<orgName>Dept. of Computer Science, University of Toronto</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Quiñonero</forename><surname>Candela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Girard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Larsen</surname></persName>
		</author>
		<title level="m">Propagation of Uncertainty in Bayesian Kernels Models -Application to Multiple-Step Ahead Forecasting Submitted to ICASSP</title>
		<imprint>
			<date type="published" when="2002">2002. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Prediction at an Uncertain Input for Gaussian Processes and Relevance Vector Machines -Application to Multiple</title>
		<author>
			<persName><forename type="first">J</forename><surname>Quiñonero Candela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Girard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Step Ahead Time-Series Forecasting</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
		<respStmt>
			<orgName>IMM, Danish Technical University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Evaluation of Gaussian Processes and other Methods for Non-Linear Regression PhD thesis</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
		<respStmt>
			<orgName>Dept. of Computer Science, University of Toronto</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<title level="m">Gaussian Processes for Regression Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<title level="m">Gaussian Processes To appear in The handbook of Brain Theory and Neural Networks</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note>Second edition</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
