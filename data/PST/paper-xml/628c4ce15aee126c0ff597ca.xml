<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HLATR: Enhance Multi-stage Text Retrieval with Hybrid List Aware Transformer Reranking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-05-21">21 May 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yanzhao</forename><surname>Zhang</surname></persName>
							<email>zhangyanzhao.zyz@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dingkun</forename><surname>Long</surname></persName>
							<email>dingkun.ldk@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guangwei</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pengjun</forename><surname>Xie</surname></persName>
							<email>pengjun.xpj@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">HLATR: Enhance Multi-stage Text Retrieval with Hybrid List Aware Transformer Reranking</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-05-21">21 May 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2205.10569v1[cs.IR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep pre-trained language models (e,g. BERT) are effective at large-scale text retrieval task. Existing text retrieval systems with state-of-the-art performance usually adopt a retrieve-then-reranking architecture due to the high computational cost of pretrained language models and the large corpus size. Under such a multi-stage architecture, previous studies mainly focused on optimizing single stage of the framework thus improving the overall retrieval performance. However, how to directly couple multi-stage features for optimization has not been well studied. In this paper, we design Hybrid List Aware Transformer Reranking (HLATR) as a subsequent reranking module to incorporate both retrieval and reranking stage features. HLATR is lightweight and can be easily parallelized with existing text retrieval systems so that the reranking process can be performed in a single yet efficient processing. Empirical experiments on two large-scale text retrieval datasets show that HLATR can efficiently improve the ranking performance of existing multi-stage text retrieval methods 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Text retrieval is a task which aims to search in a large corpus for texts that are most relevant given a query. The final retrieval result is informative and can benefit a wide range of down streaming applications, such as open domain question answering <ref type="bibr" target="#b10">(Karpukhin et al., 2020;</ref><ref type="bibr" target="#b12">Li and Lin, 2021)</ref>, machine reading comprehension <ref type="bibr" target="#b22">(Rajpurkar et al., 2016;</ref><ref type="bibr" target="#b19">Nishida et al., 2018)</ref>, and web search systems <ref type="bibr" target="#b9">(Huang et al., 2020;</ref><ref type="bibr" target="#b14">Liu et al., 2021)</ref>.</p><p>Since the corpus is massive in general, a practical text retrieval system usually leverages a retrievethen-reranking architecture due to the efficiency and effectiveness trade-off. Specifically, given a 1 Our code will be available at https://github.com/Alibaba-NLP/HLATR search query, a retriever is involved to efficiently recall a relatively small size of relevant texts from the large corpus. Then the query and the initial retrieval results will be passed through more delicate reranking stages to produce the final ranked results.</p><p>In recent years, the emergence of deep pre-trained language models (PTM) <ref type="bibr" target="#b11">(Kenton and Toutanova, 2019;</ref><ref type="bibr" target="#b15">Liu et al., 2019)</ref> has brought a storm in natural language processing (NLP) and information retrieval fields. Text retrieval systems armed with pre-trained language models have become a dominant paradigm to improve the overall performance compared to traditional statistical methods (e,g. BM25). Considering the large computational cost of transformer-based PTMs, the retrieval stage and reranking stage usually use different model architectures: representation-focused model and interaction-focused model respectively <ref type="bibr" target="#b3">(Fan et al., 2021)</ref>, as illustrated in Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>Based on the model structures presented in Figure <ref type="figure" target="#fig_0">1</ref>, previous approaches to multi-stage text retrieval roughly fall into two groups. The first group tend to separately optimize the retrieval or reranking stage by designing task-oriented PTMs <ref type="bibr">(Gao and Callan, 2021a,b;</ref><ref type="bibr" target="#b18">Ma et al., 2021)</ref> or better model training strategies <ref type="bibr" target="#b26">(Xiong et al., 2021;</ref><ref type="bibr" target="#b6">Gao et al., 2021)</ref>. Studies in the second group attend to jointly optimize the two stages by adversarial training <ref type="bibr" target="#b27">(Zhang et al., 2021)</ref> or knowledge distillation <ref type="bibr" target="#b23">(Ren et al., 2021;</ref><ref type="bibr" target="#b16">Lu et al., 2022)</ref>. However, this kind of joint learning method mainly focused on improving the retrieval stage performance by absorbing merits from the reranking stage.</p><p>In a global view, the retrieval stage and reranking stage are highly correlated. Intuitively, better retrieval results will definitely provide a more enlightening signal to the reranking stage <ref type="bibr" target="#b6">(Gao et al., 2021)</ref> for both the training and inference phase. Furthermore, since the two individual stages adopt different model architectures (representationfocused vs interaction-focused), we infer that the focus of the two stages is also different despite the sharing target to estimate the relevance of query and document. Empirically, we find that simply a weighted combination of the relevance scores produced by the two stages can further improve the overall performance, which also supports our hypothesis above. These observations inspire us whether it is possible to make an additional module that couples both coarse retrieval and fine reranking features to facilitate the final retrieval performance.</p><p>In this paper, we propose to add an additional reranking stage with a lightweight reranking model, which we noted as Hybrid List Aware Transformer Ranker (HLATR). As we tend to incorporate both the two-stage features and the reranking training objective is list aware, we adopt the transformer model structure <ref type="bibr" target="#b25">(Vaswani et al., 2017)</ref> for HLATR. As introduced in <ref type="bibr" target="#b25">(Vaswani et al., 2017)</ref>, the content for each position of the sequential inputs includes the embedding itself and the position embedding corresponded. Here, we carry out the feature coupling process by taking the logit produced by the PTM based reranking model as the embedding of each document, and we compute the position embedding based on the retrieval ranking order of each document in the retrieval stage. A stack of transformer layers will be used as the encoder part. In HLATR, we can effectively fuse the retrieval and reranking features and model the list aware text ranking process.</p><p>To verify the effectiveness and robustness of HLATR model, we conduct experiments on two commonly used large text retrieval benchmarks, namely the MS MARCO passage and document ranking datasets <ref type="bibr" target="#b1">(Campos et al., 2016)</ref>. Empirically experiment results demonstrate that HLATR can remarkable improve the final text retrieval performance with different combinations of retrieval and reranking architectures. Further we design sufficient subsidiary experiments to prove that HLATR is lightweight and easy to practical.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In order to balance efficiency and effectiveness, existing text retrieval systems usually leverage a retrieval-then-reranking framework.</p><p>For the retrieval stage, traditional sparse retrieval methods adopt the exact term matching signal for text relevance estimation (e,g. TF-IDF <ref type="bibr" target="#b17">(Luhn, 1957)</ref> and BM25 <ref type="bibr" target="#b24">(Robertson and Zaragoza, 2009)</ref> method). Recently, with the rapid development of the deep text representation method, many works shifted to training dense retrievers by taking BERT or other deep neural networks as text encoders <ref type="bibr" target="#b10">(Karpukhin et al., 2020;</ref><ref type="bibr" target="#b20">Nogueira and Cho, 2019)</ref>. These models utilize dual encoders to encode queries and documents separately, which allows pre-encoding and indexing of corpus. Under this paradigm, various optimization strategies have been proposed to improve the retrieval performance including designing task-specific pretrained model <ref type="bibr">(Gao and Callan, 2021b,a)</ref>, hard negatives mining strategy <ref type="bibr" target="#b26">(Xiong et al., 2021)</ref> and multi-view text representation <ref type="bibr" target="#b8">(Hu et al., 2021)</ref>.</p><p>For the reranking stage, previous studies use an interaction-focused model which takes the pair of query and document as input. By considering the token level similarity, these models always have a better ranking power <ref type="bibr" target="#b7">(Guo et al., 2016)</ref>. By such a full interaction computation strategy, the PTM based reranking model usually reranks a small number of documents produced by the retrieval stage. <ref type="bibr" target="#b18">(Ma et al., 2021)</ref> propose to enhance the pre-trained language model by constructing scale pseudo relevant query-document pairs. <ref type="bibr" target="#b6">(Gao et al., 2021)</ref> propose to replace the traditional binary cross-entropy loss <ref type="bibr" target="#b20">(Nogueira and Cho, 2019)</ref> with a contrastive loss for better list aware ranking. <ref type="bibr" target="#b21">(Pu et al., 2021)</ref> attend to dynamically select hard negative samples to construct high-quality semantic space the trained reranking model.</p><p>Apart from the separate optimization methods presented above, there are also studies seeking to jointly optimize the retrieval and reranking stage. In <ref type="bibr" target="#b23">(Ren et al., 2021)</ref>, they simultaneously optimize the retrieval model and the ranking model with a hybrid data augmentation technology. AR2 <ref type="bibr" target="#b27">(Zhang et al., 2021)</ref> uses a Generative adversarial network (GAN) <ref type="bibr" target="#b2">(Creswell et al., 2018)</ref> method which regards the retrieval model as the "generative mode" and the ranker model as the "discriminative model".</p><p>All of these jointly learning methods need a large extra training cost than the traditional two-stage training method. In contrast, HLTAR requires only a very small additional consumption cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Overview</head><p>The retrieve-then-reranking framework for text retrieval task is presented in Fig 2 . Given a query q, a text retrieval system aims to return a small size list of relevant documents from a large document set D = {d i } N i=1 . Existing retrieval systems usually consist of a retrieval stage and a reranking stage with different focuses on efficiency and effectiveness. Here, for simplicity, we only introduce the dense retrieval model and reranking model based on pre-trained language model (as illustrated in Figure <ref type="figure" target="#fig_0">1</ref>).</p><p>The retrieval stage takes q and D as input and aims to return all potentially relevant documents D r ? D refer to the relevance between query and document, where |D r | N . Dense retrieval model focuses on leaning to map text into quality continuous vector representation. By taking the pre-trained language model as the encoder, the retrieval model can be formulated as:</p><formula xml:id="formula_0">score(q, d) = f (E Q (q), E D (d)),<label>(1)</label></formula><p>where E Q and E D are query and document encoders, and f is the relevance estimation function.</p><p>Following the retrieval stage, a reranking stage takes q and D r as input to perform a more refined relevance evaluation for each query-document pair (q, d), where d ? D r . Commonly, the reranking model adopts the interaction-based model. Without loss of generality, the reranking method could be abstracted by the following formulation:</p><formula xml:id="formula_1">score(q, d) = f (E R (q, d)),<label>(2)</label></formula><p>where E R notes the PTM based encoder and f is the evaluation function that computes the relevance score.</p><p>In the retrieval stage, query and document are encoded independently. Therefore, the embedding representation of D can be pre-encoded. While in the reranking stage, the final representation of each query-document pair can not be pre-encoded due to the interaction-based architecture. As the result, we can only rerank a small set of documents as a result of the large computational cost of PTM.</p><p>Despite adopting different model architectures, the retrieval and reranking stage share the same target for evaluating the relevance of query and document. Certainly, the training of retrieval and reranking models relies on large-scale labeled data. Previous studies <ref type="bibr" target="#b10">(Karpukhin et al., 2020;</ref><ref type="bibr" target="#b6">Gao et al., 2021)</ref> proposed to optimize the PTM based retrieval and reranking model with a contrastive learning objective. Concretely, for each query q, we form a group G d with a single relevant positive document d + and multiple negatives. By taking the scoring function defined in equations (1)(2), the contrastive loss for each query q can be denoted as:</p><formula xml:id="formula_2">L q := -log exp(score(q, d + )) p?G d exp(score(q, d))</formula><p>.</p><p>(3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">HLATR</head><p>We target on building a lightweight reranking module that couples feature from both the retrieval and reranking stage. The overall architecture of our proposed model is illustrated in Figure <ref type="figure" target="#fig_2">3</ref>. Specifically, our model consists of a feature combination layer along with a transformer encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Multi-stage Feature Fusion</head><p>As the HLATR model is pipelined with the previous reranking model, HLATR takes the query q and D r as input, where D r is the ranking results produced by the PTM based reranking module. Obviously, |D r | = |D r | = Z, where Z denotes the reranking size. Since we adopt the transformer model architecture as the encoder, we carry out the multi-stage features fusion via coupling the retrieval ranking order in the retrieval stage and the final logit representation generated by the reranking model of each document.</p><p>For document d ? D r with retrieval order i (where 0 ? i ? Z), we map the i-th order into a learnable position embedding pe i ? R d with dimension d, and which will be jointly optimized during the learning process. The position embedding is randomly initialized. The strategy of introducing retrieval order as a feature makes our model not only practicable for systems that use dense retrieval models, but also for others based on traditional statistical retrieval methods (e.g, BM25).</p><p>For the reranking stage, we hire the final encoded representation E R (q, d), which is converted to an embedding representation sharing the same dimension as the position embedding: where W v is the trainable projection matrix. Then, for each d i in D r , we can directly add pe i and v i to build the input matrix H 0 ? R d?Z :</p><formula xml:id="formula_3">v i = W T v E R (q, d i ), (<label>4</label></formula><formula xml:id="formula_4">) Retrieval Stage Dense Model /BM25 Document Corpus ? ! ? " ? # ? Ranking Stage BERT HLATR Stage HLATR BERT ? ! q BERT ? " q B E R T ? # q HLATR ? ! ? " ? # ? ? ! ? " ? # ? " ? $ ? % ? ? ! ? " ? # ? ? " ? $ ? # ? Query q Retrieval Result Final Result Ranker Result</formula><formula xml:id="formula_5">H 0 = ? ? ? ? LN (pe 1 + v 1 ) LN (pe 2 + v 2 ) ... LN (pe Z + v Z ) ? ? ? ? ,</formula><p>where LN represents the LayerNorm (Ba et al., 2016) operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Transformer Encoder</head><p>The learning process of the reranking model is list aware. The key point of a reranking module is to model and learn the partial order relationship between different documents within the entire candidates pool. Thus, we adopt the transformer architecture <ref type="bibr" target="#b25">(Vaswani et al., 2017)</ref> as the encoder owing to its long-distance modeling ability and full attention mechanism. The self-attention mechanism in the transformer can directly model the mutual influences for any different query-documents pair sample.</p><p>The entire encoder module consists of L transformer layers. The hidden size of each Transformer layer is set equal to the input dimension d. For the first Transformer layer, it takes H 0 as input. While for the l-th Transformer layer T l , it takes the output of the previous layer as input:</p><formula xml:id="formula_6">H 1 = T 1 (H 0 ), H l = T l (H l-1 ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Learning Objective</head><p>Finally, we use a linear projection layer to predict the relevance score between the input query q and</p><formula xml:id="formula_7">? ! ? " ? $ + + + ? ! % ? " % ? &amp; % ?? ! ?? " ?? #</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reranker logists</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Retrieve order</head><p>Input Layer Encoder Layer</p><formula xml:id="formula_8">? ? ? ! ? " ? ' Re-rankded List ? " ? ! ? ' ? ! ? " ? '</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transformer Layers</head><p>Initial Rank List </p><formula xml:id="formula_9">score(q, d i ) = W T H l i .<label>(5)</label></formula><p>Similar to the previous stage, we optimize HLATR with a list-wise contrastive loss <ref type="bibr" target="#b6">(Gao et al., 2021)</ref>:</p><formula xml:id="formula_10">L {q,D r } = -log exp(score(q, d + )) d?D r exp(score(q, d)) .<label>(6)</label></formula><p>From the perspective of the learning objective, HLATR can also be classified as a reranking model, which takes a query q and a document list as input and then produces the relevance score of each query-document pair among all candidate documents. HLATR is different from the previous reranking model based on the pre-trained language model. We sort out the difference between the two modules from three aspects: 1) Superficially, the input of HLATR and the PTM based reranking model is the query q and its corresponding candidate document list. Indeed, the PTM based reranking model uses the raw content of query and document as input, but HLATR uses the combination of features generated by the previous two stages as input.</p><p>2) Moreover, the calculation paradigm of the final ranking score is also different. In the PTM based reranking model, the relevance score of each query-doc pair is calculated independently. In the HLATR model, the relevance score of the entire input document list is calculated at one time through the serialization computing mechanism of the transformer architecture. Such a list aware calculation method allows us to better model the mutual relationship between the different documents in the entire input document list.</p><p>3) During the training process, for PTM based reranking model, we have to control the data size of each mini-batch via a random sampling strategy when constructing G q for each query q due to the high computational cost of PTM. The lightweight nature of HLATR allows us to use the entire retrieval results to construct the set of negatives, thereby greatly expanding the number of negative samples in each mini-batch. A previous study has proven that the larger number of negative samples can effectively improve the reranking performance under the contrastive learning optimization objective <ref type="bibr" target="#b6">(Gao et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>Dataset In this paper, we conduct experiments on two widely-used large-scale text retrieval datasets: MS MARCO Passage (called "Passage" below for simplicity) dataset and MS MARCO Document (called "Document" below) dataset. The MS MARCO dataset is constructed from Bing? search query logs and web documents retrieved by Bing. The statistic information of MS MARCO dataset is presented in Table <ref type="table" target="#tab_0">1</ref>. Since the number of parameters of HLATR is much smaller compared with the PTM reranking model, we only need to use a small amount of data to train the HLATR model. For the passage dataset, there exists an extra dev dataset which contains 34, 000 queries which can be used as training dataset. For the document dataset, we split 50, 000 queries from the orinal training dataset as the final training dataset for HLATR.</p><p>Evaluation Metric Following the evaluation methodology used in previous work, we report MRR@10 and MRR@100 results for the MS Implementation Details To verify the efficiency and robustness of the HLATR module, we implement the experiments with different retrieval and reranking models. For the retrieval stage, we report results based on both sparse and dense retrieval methods. Specifically, we choose the BM25 method as the sparse retrieval model, and two state-of-the-art dense retrieval models coCondenser <ref type="bibr">(Gao and Callan, 2021b)</ref> and ANCE <ref type="bibr" target="#b26">(Xiong et al., 2021)</ref> for passage and document retrieval respectively. For the reranking stage, we leverage the commonly used reranking method based on the pre-trained language model as presented in <ref type="bibr" target="#b20">(Nogueira and Cho, 2019;</ref><ref type="bibr" target="#b6">Gao et al., 2021)</ref>. However, we conduct experiments based on different pre-trained language models (BERT base , BERT large , RoBERTa base , RoBERTa large ) to verify the robustness of HLATR. We implement the BM25 retrieval process by the pyserini toolkit <ref type="bibr" target="#b13">(Lin et al., 2021)</ref>. For ANCE<ref type="foot" target="#foot_0">2</ref> and coCondenser<ref type="foot" target="#foot_1">3</ref> methods, we use the publicly released checkpoint by authors. Following previous studies, for the passage dataset, we use the body field as the document text. For the document dataset, we combine the url, title and body field as the final document text. The maximum token length is set to 128 and 512 respectively. Detailed hype-parameters used in our experiments can be found in Table <ref type="table" target="#tab_1">3</ref>.</p><p>Baselines We compare HLATR with the conventional retrieve-then-reranking models and the simple Weighted Combination of two-stage Ranking models (WCR). For each query-document pair, the WCR method directly weights combined the rele-Table <ref type="table">2</ref>: MRR@10 and MRR@100 metrics for MS MARCO Passage and Document Dataset with different kinds of retrieval and reranking models. We bold the best performances of each combination of retrieval and reraking model. The results of HLATR are statistically significant difference (p &lt; 0.01) compared to other models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model/Dataset</head><p>Passage Document PTM Sparse Dense Sparse Dense MRR@10 MRR@100 MRR@10 MRR@100 MRR@10 MRR@100 MRR@10 MRR@100 </p><formula xml:id="formula_11">?f (E Q (q), E D (d)) + (1 -?)f (E R (q, d)),<label>(7)</label></formula><p>where ? ? (0, 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>Table <ref type="table">2</ref> presents the final results of our method for both passage and document datasets. We highlight the best performances of each metric in bold. We find that our approach achieves the best performance among all the baselines. Our model improves the text retrieval performances by a large margin compared to the conventional retrieve-then-reranking two-stage architecture. Specifically, the HLATR enhanced text retrieval systems improve the MRR@10 metric on passage and document datasets from 40.1 to 42.0 and 43.4 to 44.5 respectively under the dense retrieval and BERT-base reranking setting. This observation illustrates the effectiveness of combining features from the previous retrieval and reranking stage to improve the final ranking performance.</p><p>We can also observe that HLATR achieves consistent improvement whether using sparse or dense retrieval methods. We train the reranking models based on different pre-trained language models. Firstly, whether for the BERT or RoBERTa model, the performance of the large model is stably better than that of the base model, which is consistent with the results of previous studies <ref type="bibr" target="#b6">(Gao et al., 2021;</ref><ref type="bibr" target="#b18">Ma et al., 2021)</ref>. Moreover, based on different reranking models, the improvement of the HLATR model is consistent. The above experimental result strongly depicts the robustness of our HLATR model.</p><p>For each experimental setting, we also report the results of the WCR method. We can see that the WCR method can effectively improve the retrieval performance, which once again proves the effectiveness of the motivation in coupling retrieval and reranking features. However, we can also observe that the effect of the WCR model still lags behind the HLATR model, which further illustrates that HLATR is a model advisable architecture for large-scale text retrieval task from the perspective of coupling features.  <ref type="table" target="#tab_2">4</ref>. "w/o retrieval" denotes the setting that removes the retrieval feature and only uses the reranking stage feature as input for HLATR.</p><p>Similarly, "w/o reranking" notes the setting that we remove the reranking stage feature for HLATR. Unlike the "w/o retrieval" setting, only adopting "position embedding" as input for the transformer encoder is extra-conventional. Therefore, we replace the reranking stage feature vector with the query embedding and document embedding generated by the dense retrieval model. We tried several methods to concatenate these two embeddings and we find that the element-wise multiply performs best. So we only report the result based on the element-wise multiply concatenation method.</p><p>From Table <ref type="table" target="#tab_2">4</ref>, we can observe that when the input feature of HLATR only contains one of the retrieval or reranking features alone, the overall ranking performance is still slightly improved. For example, the "w/o reranking" setting improves the MRR@10 value from 38.3 to 39.0 compared to the simple retrieval performance. Notably, the coupled feature input will lead to an improvement with a larger margin. This experimental result shows that coupling multi-stage features can indeed improve the final reranking performance.</p><p>Architecture analysis of HLATR To better analyze the advantages of our model in tackling the list aware text reranking problem, we conduct experiments with two variants: 1) HLATR linear : In this model, we replace the multi-layer transformer with a two-layer MLP model with ReLU as the activation function.</p><p>2) HLATR bce : In this model, we replace the origi- nal contrastive loss with binary cross-entropy loss, which can be formulated as:</p><formula xml:id="formula_12">L bce (q, D r ) = 1 Z Z i (y i log(?(score(q, d i ))) + (1 -y i ) log(1 -?(score(q, d i ))),</formula><p>where y i is the {0, 1} label indicates whether this document is relevant to the query and ? denotes the sigmoid activation function.</p><p>The experiment result is presented in table 5, from which we can see that with just a 2-layer MLP encoder, HLATR can still outperform the WCR method significantly. However, compared with the HLATR model that uses the transformer structure as encoder, the performance of HLATR linear is degraded. We can infer that for such a list aware reranking task, the transformer model has a better ability in modeling the mutual associations between different documents in the input sequence.</p><p>Further, by comparing the performance of the HLATR bce model and the original model, we can conclude that the contrastive learning objective is more proper for the text ranking learning process. The essential reason is that the text reranking task is an optimization problem with imbalanced data. In the input sequence, the number of negative samples is much larger than the number of positive samples, and the loss of contrastive learning is more advantageous in dealing with such a data imbalance scenario.</p><p>Computational Cost of HLATR To test the efficiency of HLATR, here we report the inference time cost for different kinds of ranking models and the HLATR model. All experiments are conducted on NVIDIA Tesla 32G V100.</p><p>In Table <ref type="table" target="#tab_4">6</ref>, we can see that the time cost of HLATR is significantly smaller than the PTM based reranking models. On the passage reranking dataset, HLATR is 300/450 times more computa- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Passage Document BERT base 600ms 800ms</p><p>BERT large 900ms 1200ms HLATR 2ms 2ms tionally efficient. Therefore, although we add an extra reranking stage, it only adds a little time cost compared with the whole retrieve-then-reranking process due to its small size of parameters and effective way to calculate the relevance score.</p><p>Hyper-parameters of Transformer In this part, we wish to explore how the architecture of the transformer-based encoder affects the ranking performance. In Fig <ref type="figure" target="#fig_3">4</ref>, we compare the MRR@10 metric with different settings of number of transformer layers and its hidden dimension d on the Passage Ranking dataset. We can see that the performance increases first and drop later when the number of layer increases. The larger d is, the inflection point comes earlier. A small dimension size like 128 leads to a better performance than a large dimension size like 512. This can be attributed that the aim of HLATR is only designed to make use of the first two-stages features produced by the largescale pre-trained model. So that it does not need a big number of parameters. Consequently, we need to balance the "width" and the "depth" of HLATR to achieve the best performance.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we intuitively propose a unified hybrid list aware transformer reranking (HLATR) model via coupling retrieval and reranking features for multi-stage text retrieval task. The HLATR model is lightweight and can be easily applied to existing systems. Experiments on two large scale text retrieval datasets show our framework significantly outperforms previous baselines.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Model architectures of representationfocused model (left) and interaction based model (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An illusion of the conventional retrieve-then-reranking framework (solid line part) and our HLTAR ranking model (solid line and dashed line part).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>?Figure 3 :</head><label>3</label><figDesc>Figure 3: Thee detailed network structure of our HLATR (Hybrid List Aware Transformer Reranking) model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The impact of Transformer architecture with different dimensions of hidden states d and number of layers. The horizontal axis represents the number of transformer layers and the vertical axis represents the MRR@10 metric.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of MA MARCO dataset. "#Length" represents the average text length and "#Relvant" represents the number of the labeled relevant documents per query. We only count the query which at least has one relevant document.</figDesc><table><row><cell>Dataset</cell><cell>Document</cell><cell>Passage</cell></row><row><cell>#Query</cell><cell cols="2">367013/5193 502939/6980</cell></row><row><cell>#Document</cell><cell>3213835</cell><cell>8841823</cell></row><row><cell>#Length</cell><cell>56.8</cell><cell>1127.8</cell></row><row><cell>#Relevant</cell><cell>1.04</cell><cell>1.06</cell></row><row><cell cols="3">MARCO dev dataset due to the label of test data is</cell></row><row><cell>not available.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc>Hyperameters for HLATR, Ranker for Passage Dataset (Ranker(P)) and Document (Ranker(D)), lr represents learning rate, k is the number of hard negatives, n is the number of transformer layers, head is the attention head for each transformer layer, d is the dimension of v.</figDesc><table><row><cell cols="2">Retrieval Only</cell><cell>18.7</cell><cell>19.8</cell><cell>38.3</cell><cell>39.4</cell><cell>20.9</cell><cell>22.2</cell><cell>36.8</cell><cell>37.9</cell></row><row><cell></cell><cell>Reranking</cell><cell>34.4</cell><cell>35.5</cell><cell>40.1</cell><cell>41.1</cell><cell>40.4</cell><cell>40.9</cell><cell>43.4</cell><cell>44.2</cell></row><row><cell>BERT base</cell><cell>WCR</cell><cell>34.5</cell><cell>35.6</cell><cell>41.5</cell><cell>42.4</cell><cell>40.6</cell><cell>41.1</cell><cell>44.3</cell><cell>45.1</cell></row><row><cell></cell><cell>HLATR</cell><cell>35.0</cell><cell>36.0</cell><cell>42.0</cell><cell>42.9</cell><cell>42.0</cell><cell>42.5</cell><cell>44.5</cell><cell>45.2</cell></row><row><cell></cell><cell>Reranking</cell><cell>35.4</cell><cell>36.3</cell><cell>41.0</cell><cell>42.0</cell><cell>40.6</cell><cell>41.1</cell><cell>44.2</cell><cell>45.0</cell></row><row><cell>BERT large</cell><cell>WCR</cell><cell>35.6</cell><cell>36.6</cell><cell>42.0</cell><cell>42.9</cell><cell>40.8</cell><cell>41.3</cell><cell>45.2</cell><cell>45.9</cell></row><row><cell></cell><cell>HLATR</cell><cell>35.9</cell><cell>36.7</cell><cell>42.4</cell><cell>43.4</cell><cell>42.2</cell><cell>42.8</cell><cell>45.6</cell><cell>46.3</cell></row><row><cell></cell><cell>Reranking</cell><cell>34.9</cell><cell>36.1</cell><cell>40.8</cell><cell>41.7</cell><cell>40.6</cell><cell>41.0</cell><cell>44.3</cell><cell>45.1</cell></row><row><cell>RoBERTa base</cell><cell>WCR</cell><cell>35.2</cell><cell>36.3</cell><cell>42.0</cell><cell>42.9</cell><cell>41.0</cell><cell>41.6</cell><cell>45.1</cell><cell>45.8</cell></row><row><cell></cell><cell>HLATR</cell><cell>35.5</cell><cell>36.5</cell><cell>42.5</cell><cell>43.5</cell><cell>42.0</cell><cell>42.4</cell><cell>45.4</cell><cell>46.1</cell></row><row><cell></cell><cell>Reranking</cell><cell>36.2</cell><cell>37.2</cell><cell>41.6</cell><cell>42.6</cell><cell>40.8</cell><cell>41.2</cell><cell>45.2</cell><cell>45.9</cell></row><row><cell>RoBERTa large</cell><cell>WCR</cell><cell>36.4</cell><cell>37.4</cell><cell>42.9</cell><cell>43.9</cell><cell>41.2</cell><cell>41.6</cell><cell>45.7</cell><cell>46.4</cell></row><row><cell></cell><cell>HLATR</cell><cell>36.8</cell><cell>37.8</cell><cell>43.7</cell><cell>44.5</cell><cell>42.3</cell><cell>42.7</cell><cell>45.9</cell><cell>46.6</cell></row><row><cell></cell><cell cols="4">HLATR Rerenk(P) Ranker(D)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>lr</cell><cell>1e-3</cell><cell>1e-5</cell><cell cols="2">1e-5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>batch size</cell><cell>1024</cell><cell>256</cell><cell cols="2">128</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>epoch</cell><cell>40</cell><cell>3</cell><cell>3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>k</cell><cell>100</cell><cell>8</cell><cell>4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>n</cell><cell>4</cell><cell>12/24</cell><cell cols="2">12/24</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>head</cell><cell>2</cell><cell>12</cell><cell>12</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>d</cell><cell>128</cell><cell>768/1024</cell><cell cols="2">768/1024</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">vance scores produced by the retrieval stage rerank-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">ing stage as the final ranking score:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Ablation experiments result on MS MARCO Passage dataset. Dense model for retrieval stage and BERT base model for reranking stage.</figDesc><table><row><cell>Model</cell><cell>Passage</cell><cell></cell></row><row><cell></cell><cell cols="2">MRR@10 MRR@100</cell></row><row><cell>Retrieval</cell><cell>38.3</cell><cell>39.4</cell></row><row><cell>Reranking</cell><cell>40.8</cell><cell>41.8</cell></row><row><cell>HLATR</cell><cell>42.6</cell><cell>43.5</cell></row><row><cell>w/o retrieval</cell><cell>41.2</cell><cell>42.2</cell></row><row><cell>w/o ranker</cell><cell>39.0</cell><cell>39.9</cell></row><row><cell cols="2">4.3 Discussion and Analysis</cell><cell></cell></row><row><cell cols="3">Multi-stage Feature Coupling We further evalu-</cell></row><row><cell cols="3">ate the influence of the feature coupling mechanism</cell></row><row><cell cols="3">in HLATR. The experimental results are presented</cell></row><row><cell>in Table</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Experiment result on MS MARCO Passage dataset of HLATR and its two variants.</figDesc><table><row><cell>Model</cell><cell>Passage</cell><cell></cell></row><row><cell></cell><cell cols="2">MRR@10 MRR@100</cell></row><row><cell>HLATR</cell><cell>42.6</cell><cell>43.5</cell></row><row><cell>WCR</cell><cell>41.8</cell><cell>42.8</cell></row><row><cell>HLATR linear</cell><cell>42.4</cell><cell>43.3</cell></row><row><cell>HLATR bce</cell><cell>41.7</cell><cell>42.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>Time cost for different reranking models and HLATR model for per 1000 queries. The time cost for RoBERTa base and RoBERTa large is similar with the BERT base/large model. So we only report the result of BERT models.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>https://github.com/microsoft/ANCE</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>https://github.com/luyug/Condenser</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lei Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Ryan Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<title level="m">Layer normalization</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Ms marco: A human generated machine reading comprehension dataset</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Fernando Campos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tri</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mir</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rangan</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhaskar</forename><surname>Mitra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>ArXiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Generative adversarial networks: An overview</title>
		<author>
			<persName><forename type="first">Antonia</forename><surname>Creswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Arulkumaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Biswa</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anil</forename><forename type="middle">A</forename><surname>Bharath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Yixing</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohui</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinqiong</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqun</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.13853</idno>
		<title level="m">Pre-training methods in information retrieval</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">2021a. Condenser: a pretraining architecture for dense retrieval</title>
		<author>
			<persName><forename type="first">Luyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<biblScope unit="page" from="981" to="993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">2021b. Unsupervised corpus aware language model pre-training for dense passage retrieval</title>
		<author>
			<persName><forename type="first">Luyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.05540</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Rethink training of bert rerankers in multi-stage retrieval pipeline</title>
		<author>
			<persName><forename type="first">Luyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Information Retrieval</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A deep relevance matching model for ad-hoc retrieval</title>
		<author>
			<persName><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixing</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyao</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM international on conference on information and knowledge management</title>
		<meeting>the 25th ACM international on conference on information and knowledge management</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-view cross-lingual structured prediction with minimum supervision</title>
		<author>
			<persName><forename type="first">Zechuan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nguyen</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers). Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Embedding-based retrieval in facebook search</title>
		<author>
			<persName><forename type="first">Jui-Ting</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuying</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Pronin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janani</forename><surname>Padmanabhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giuseppe</forename><surname>Ottaviano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjun</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2553" to="2561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Dense passage retrieval for open-domain question answering</title>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.04906</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename><surname>Kristina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toutanova</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Encoder adaptation of dense passage retrieval for open-domain question answering</title>
		<author>
			<persName><forename type="first">Minghan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.01599</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pyserini: A python toolkit for reproducible information retrieval research with sparse and dense representations</title>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueguang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng-Chieh</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jheng-Hong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronak</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2356" to="2362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Pre-trained language model for web-scale retrieval in baidu search</title>
		<author>
			<persName><forename type="first">Yiding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weixue</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suqi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daiting</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuaiqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3365" to="3375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Ernie-search: Bridging cross-encoder with dual-encoder via self on-the-fly distillation for dense passage retrieval</title>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunsheng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengjie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shikun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuaiqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A statistical approach to mechanized encoding and searching of literary information</title>
		<author>
			<persName><forename type="first">Hans</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luhn</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Journal of research and development</title>
		<imprint>
			<date type="published" when="1957">1957</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">B-prop: bootstrapped pre-training with representative words prediction for ad-hoc retrieval</title>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixing</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1513" to="1522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Retrieve-andread: Multi-task learning of information retrieval and reading comprehension</title>
		<author>
			<persName><forename type="first">Kyosuke</forename><surname>Nishida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Itsumi</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atsushi</forename><surname>Otsuka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hisako</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junji</forename><surname>Tomita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 27th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.04085</idno>
		<title level="m">Passage re-ranking with bert</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Yes sir! optimizing semantic space of negatives with self-involvement ranker</title>
		<author>
			<persName><forename type="first">Ruizhi</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruofei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zikai</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinxia</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongkang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yantao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Cao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.06436</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rocketqav2: A joint training method for dense passage retrieval and passage re-ranking</title>
		<author>
			<persName><forename type="first">Ruiyang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingqi</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaoqiao</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2825" to="2835" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">The probabilistic relevance framework: BM25 and beyond</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Zaragoza</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Now Publishers Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Approximate nearest neighbor negative contrastive learning for dense text retrieval</title>
		<author>
			<persName><forename type="first">Lee</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kwok-Fung</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">N</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junaid</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnold</forename><surname>Overwijk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yeyun</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiancheng</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.03611</idno>
		<title level="m">Adversarial retriever-ranker for dense text retrieval</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
