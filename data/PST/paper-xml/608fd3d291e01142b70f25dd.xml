<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SRDiff: Single Image Super-Resolution with Diffusion Probabilistic Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-04-30">30 Apr 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Haoying</forename><surname>Li</surname></persName>
							<email>lhaoying@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yifan</forename><surname>Yang</surname></persName>
							<email>yangyifan@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Meng</forename><surname>Chang</surname></persName>
							<email>changm@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huajun</forename><surname>Feng</surname></persName>
							<email>fenghj@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhihai</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qi</forename><surname>Li</surname></persName>
							<email>liqi@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yueting</forename><surname>Chen</surname></persName>
							<email>chenyt@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SRDiff: Single Image Super-Resolution with Diffusion Probabilistic Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-04-30">30 Apr 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2104.14951v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Single image super-resolution (SISR) aims to reconstruct high-resolution (HR) images from the given low-resolution (LR) ones, which is an illposed problem because one LR image corresponds to multiple HR images. Recently, learning-based SISR methods have greatly outperformed traditional ones, while suffering from over-smoothing, mode collapse or large model footprint issues for PSNR-oriented, GAN-driven and flow-based methods respectively. To solve these problems, we propose a novel single image super-resolution diffusion probabilistic model (SRDiff), which is the first diffusion-based model for SISR. SRDiff is optimized with a variant of the variational bound on the data likelihood and can provide diverse and realistic SR predictions by gradually transforming the Gaussian noise into a super-resolution (SR) image conditioned on an LR input through a Markov chain. In addition, we introduce residual prediction to the whole framework to speed up convergence. Our extensive experiments on facial and general benchmarks (CelebA and DIV2K datasets) show that 1) SRDiff can generate diverse SR results in rich details with state-of-the-art performance, given only one LR input; 2) SRDiff is easy to train with a small footprint; and 3) SRDiff can perform flexible image manipulation including latent space interpolation and content fusion.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Over the years, single image super-resolution (SISR) has drawn active attention due to its wide applications in computer vision such as object recognition <ref type="bibr">[Fookes et al., 2012;</ref><ref type="bibr">Sajjadi et al., 2017]</ref>, remote sensing <ref type="bibr" target="#b4">[Li et al., 2009]</ref>, surveillance monitoring <ref type="bibr" target="#b0">[Fang et al., 2019;</ref><ref type="bibr" target="#b8">Park et al., 2020]</ref> and so on. SISR aims to recover high-resolution (HR) images from low-resolution (LR) ones, which is an ill-posed problem, for multiple HR images can be degenerated to one LR image as shown in Figure <ref type="figure" target="#fig_0">1</ref>. To establish the mapping between HR and LR images, lots of deep learning-based methods emerge and could be categorized into three main types: PSNR-oriented, GANdriven and flow-based methods. PSNR-oriented methods <ref type="bibr" target="#b0">[Dong et al., 2015;</ref><ref type="bibr" target="#b4">Lim et al., 2017;</ref><ref type="bibr" target="#b12">Zhang et al., 2018b;</ref><ref type="bibr">Qiu et al., 2019;</ref><ref type="bibr" target="#b0">Guo et al., 2020]</ref> are trained with simple distribution assumption-based losses (e.g., Laplacian for L 1 and Gaussian for L 2 ) and achieve excellent PSNR. However, these losses tend to drive the SR result to an average of several possible SR predictions, causing over-smoothing images with high-frequency information loss. One groundbreaking solution to tackle the over-smoothing problem is GAN-driven methods <ref type="bibr" target="#b0">[Cheon et al., 2018;</ref><ref type="bibr">Kim et al., 2019;</ref><ref type="bibr" target="#b3">Ledig et al., 2017;</ref><ref type="bibr" target="#b11">Wang et al., 2018]</ref>, which combine content losses (e.g., L 1 and L 2 ) and adversarial losses to obtain sharper SR images with better perceptual quality. However, GAN-driven methods are easy to fall into mode collapse, which leads to a single generated SR sample without diversity. Additionally, GAN-based training process is not easy to converge and requires an extra discriminator which is not used in inference. <ref type="bibr">Flow-based methods [Lugmayr et al., 2020]</ref> directly account for the ill-posed problem with an invertible encoder, which maps HR images to the flow-space latents conditioned on LR inputs. Trained with a negative loglikelihood loss, flow-based methods avoid training instability but suffer from extremely large footprint and high train-ing cost due to the strong architectural constraints to keep the bijection between latents and data.</p><p>Lately, the successful adoptions of diffusion probabilistic models (diffusion models for short) in image synthesis <ref type="bibr" target="#b1">[Ho et al., 2020]</ref> and speech synthesis <ref type="bibr" target="#b2">[Kong et al., 2020;</ref><ref type="bibr" target="#b0">Chen et al., 2021]</ref> witness the power of diffusion models in generative tasks. The diffusion models use a Markov chain to convert data x 0 to latent variable x T in simple distribution (e.g., Gaussian) by gradually adding noise in the diffusion process, and predict the noise in each diffusion step to recover the data x 0 through a learned reverse process. Diffusion models are trained by optimizing a variant of the variational lower bound, which is efficient and avoids the mode collapse encountered by GAN.</p><p>In this paper, we propose a novel single image superresolution diffusion probabilistic model (SRDiff) to tackle the over-smoothing, mode collapse and huge footprint problems in previous SISR models. Specifically, 1) to extract the image information in LR image, SRDiff exploits a pretrained lowresolution encoder to convert LR image into hidden condition. 2) To generate the HR image conditioned on LR image, SRDiff employs a conditional noise predictor to recover x 0 iteratively. 3) To speed up convergence and stabilize training, SRDiff introduces residual prediction by taking the difference between the HR and LR image as the input x 0 in the first diffusion step, making SRDiff focus on restoring highfrequency details. To the best of our knowledge, SRDiff is the first diffusion-based SR model and has several advantages:</p><p>• Diverse and high-quality outputs: SRDiff converts Gaussian white noise into an SR prediction through a Markov chain, which does not suffer from mode collapse and can generate diverse and high-quality SR results.</p><p>• Stable and efficient training with small footprint: Although the data distribution of HR image is hard to estimate, SRDiff admits a variant of the variational bound maximization and applies residual prediction.</p><p>Compared with GAN-driven methods, SRDiff is stably trained with a single loss and does not need any extra module (e.g., discriminator, which is only used in training). Compared with flow-based methods, SRDiff has no architectural constraints and thus enjoys benefits from small footprint and fast training.</p><p>• Flexible image manipulation: SRDiff can perform flexible image manipulation including latent space interpolation and content fusion using both diffusion process and reverse process, which shows broad application prospects.</p><p>Our extensive experiments on CelebA <ref type="bibr" target="#b5">[Liu et al., 2015a]</ref>   <ref type="bibr">Flow [Lugmayr et al., 2020]</ref>, which builds an invertible neural network to transform a Gaussian distribution into an HR image space instead of modeling one single output and inherently resolves the pathology of the original "one-to-many" SR problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Diffusion models</head><p>Diffusion probabilistic models <ref type="bibr" target="#b10">[Sohl-Dickstein et al., 2015;</ref><ref type="bibr" target="#b1">Ho et al., 2020]</ref> are a kind of generative models using a Markov chain to transform latent variables in simple distributions (e.g., Gaussian) to data in complex distributions. Researchers find it useful to tackle "one-to-many" problems and synthesize high-quality results in speech synthesis tasks <ref type="bibr" target="#b2">[Kong et al., 2020]</ref> and image synthesis fields <ref type="bibr" target="#b1">[Ho et al., 2020]</ref>. However, to the best of our knowledge, diffusion models have not yet been used in image reconstruction fields like super-resolution. In this paper, we propose our impressive work, SRDiff, building on diffusion models to generate diverse SR images with a single LR input, and solving oversmoothing, mode collapse and large footprint issues together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Diffusion Model</head><p>In this section, to provide a basic understanding of diffusion probabilistic models (diffusion model for short) <ref type="bibr" target="#b1">[Ho et al., 2020]</ref>, we first briefly review its formulation.</p><p>A diffusion model is a kind of generative model which adopts parameterized Markov chain trained using variational inference to gradually generate data x 0 in complex distribution from a latent variable x T in simple distribution, where T is the total diffusion step. We set x t ∈ R d to be the results of each diffusion timestep t ∈ {1, 2, ..., T } and x t shares the same dimension d as that of x 0 . As shown in Figure <ref type="figure" target="#fig_1">2</ref>, a diffusion model is composed of two processes: the diffusion process and the reverse process. The posterior q(x 1 , • • • , x T |x 0 ), named as the diffusion process, converts the data distribution q(x 0 ) to the latent variable distribution q(x T ), and is fixed to a Markov chain which gradually adds Gaussian noise to the data according to a variance schedule</p><formula xml:id="formula_0">β 1 , • • • , β T : q(x 1 , • • • , x T |x 0 ) := T t=1 q(x t |x t−1 ), q(x t |x t−1 ) := N (x t ; 1 − β t x t−1 , β t I),</formula><p>where β t is a small positive constant and could be regarded as constant hyperparameters. Setting α t := 1 − β t , ᾱt := t s=1 α s , the diffusion process allows sampling x t at an arbitrary timestep t in closed form:</p><formula xml:id="formula_1">q(x t |x 0 ) = N (x t ; √ ᾱt x 0 , (1 − ᾱt )I),<label>(1)</label></formula><p>which can be further reparameterized as</p><formula xml:id="formula_2">x t (x 0 , ) = √ ᾱt x 0 + √ 1 − ᾱt , ∼ N (0, I).<label>(2)</label></formula><p>The reverse process transforms the latent variable distribution p θ (x T ) to the data distribution p θ (x 0 ) parameterized by θ. It is defined by a Markov chain with learned Gaussian transitions beginning with p(x T ) = N (x T ; 0, I):</p><formula xml:id="formula_3">p θ (x 0 , • • • , x T −1 |x T ) := T t=1 p θ (x t−1 |x t ), p θ (x t−1 |x t ) := N (x t−1 ; µ θ (x t , t), σ θ (x t , t) 2 I),<label>(3)</label></formula><p>In training phase, we maximize the variational lower bound (ELBO) on negative log likelihood and introduce KL divergence and variance reduction <ref type="bibr" target="#b1">[Ho et al., 2020]</ref>:</p><formula xml:id="formula_4">E[− log p θ (x 0 )] ≤ L := E q D KL (q(x T |x 0 ) || p(x T ) L T ) + t&gt;1 D KL (q(x t−1 |x t , x 0 ) || p θ (x t−1 |x t )) Lt−1 − log p θ (x 0 |x 1 ) L0 . (4)</formula><p>This transformation requires a direct comparison between p θ (x t−1 |x t ) and its corresponding diffusion process posteriors. Setting μt (x t , x 0 ) :</p><formula xml:id="formula_5">= √ ᾱt−1βt 1− ᾱt x 0 + √ αt(1− ᾱt−1) 1− ᾱt x t , we have q(x t−1 |x t , x 0 ) = N (x t−1 ; μt (x t , x 0 ), βt I).</formula><p>(5)</p><p>Eq. ( <ref type="formula" target="#formula_1">1</ref>), ( <ref type="formula" target="#formula_3">3</ref>) and ( <ref type="formula">5</ref>) assure that all KL divergences in Eq. ( <ref type="formula">4</ref>) are comparisons between Gaussians. With σ 2 θ = βt = 1− ᾱt−1<ref type="foot" target="#foot_0">1</ref>− ᾱt β t for t &gt; 1, β1 = β 1 , and constant C, we have</p><formula xml:id="formula_6">L t−1 = E q 1 2σ 2 t μt (x t , x 0 ) − µ θ (x t , t) 2 + C.</formula><p>For simplicity, the training procedure minimizes the variant of the ELBO with x 0 and t as inputs:</p><formula xml:id="formula_7">min θ L t−1 (θ) = E x0, ,t − θ ( √ ᾱt x 0 + √ 1 − ᾱt , t) 2 ,</formula><p>(6) where θ is a noise predictor.</p><p>In inference, we first sample an x T ∼ N (x T ; 0, I), and then sample x t−1 ∼ p θ (x t−1 |x t ) according to Eq. ( <ref type="formula" target="#formula_3">3</ref>), where</p><formula xml:id="formula_8">µ θ (x t , t) := 1 √ α t x t − β t √ 1 − ᾱt θ (x t , t) , σ θ (x t , t) := β 1 2 t , t ∈ {T, T − 1, ..., 1}.<label>(7)</label></formula><p>4 SRDiff</p><p>As depicted in Figure <ref type="figure" target="#fig_1">2</ref>, SRDiff is built on a T-step diffusion model which contains two processes: diffusion process and reverse process. Instead of predicting the HR image directly, we apply residual prediction to predict the difference between the HR image x H and the upsampled LR image up(x L ) and denote the difference as input residual image x 0 . Diffusion process converts the x 0 into a latent x T in Gaussian distribution by gradually adding Gaussian noise as implied in Eq.</p><p>(2). According to Eq. ( <ref type="formula" target="#formula_3">3</ref>) and ( <ref type="formula" target="#formula_8">7</ref>), the reverse process is determined by θ , which is a conditional noise predictor with an RRDB-based <ref type="bibr" target="#b11">[Wang et al., 2018]</ref> low-resolution encoder (LR encoder for short) D, as shown in Figure <ref type="figure" target="#fig_2">3</ref>. The reverse process converts a latent variable x T to a residual image x r by iteratively denoising in finite step T using the conditional noise predictor θ , conditioned on the hidden states encoded from LR image by the LR encoder D. The SR image is reconstructed by adding the generated residual image x r to the upsampled LR image up(x L ). Therefore, the goal of θ is to predict the noise added at each diffusion timestep in the diffusion process 1 .</p><p>In the following subsections, we will introduce architectures of the conditional noise predictor, LR encoder, training and inference. Conditional Noise Predictor The conditional noise predictor θ predicts noise added in each timestep of the diffusion process conditioned on the LR image information, according to Eq. ( <ref type="formula">6</ref>) and ( <ref type="formula" target="#formula_8">7</ref>). As shown in Figure <ref type="figure" target="#fig_2">3</ref>, we use U-Net as the main body, taking 3-channel x t , the diffusion timestep t ∈ {1, 2, ..., T − 1, T } and the output of LR encoder as inputs. First, x t is transformed to hidden through a 2Dconvolution block which consists of one 2D-convolutional layer and Mish activation <ref type="bibr" target="#b7">[Misra, 2019]</ref>. Then the LR information is fused with the 2D-convolution block output hidden. Following <ref type="bibr" target="#b1">Ho et al. [2020]</ref>, we transform the timestep   </p><formula xml:id="formula_9">for t = T, T − 1, • • • , 1 do 7: Sample z ∼ N (0, I) if t &gt; 1, else z = 0 8:</formula><p>Compute xt−1 using Eq. ( <ref type="formula" target="#formula_8">7</ref>): t to the timestep embedding t e using the Transformer sinusoidal positional encoding <ref type="bibr">[Vaswani et al., 2017]</ref>. Then the last output hidden and t e are fed into the contracting path, one middle step and the expansive path successively. The contracting path and expansive path both consist of four steps, each of which successively applies two residual blocks and one downsampling/upsampling layer. To reduce the model size, we only double the channel size in the second and the fourth contracting steps and halve the spatial sizes of the feature map in each contracting step. The downsampling layer in contracting path is a two-stride 2D convolution and the upsampling layer in expansive path is 2D transposed convolution. The middle step consists of two residual blocks, which is inserted between the contracting and expansive paths. Besides, the inputs of each expansive step concatenate the corresponding feature map from the contracting path. Finally, a 2D-convolution block is applied to generate ˆ in timestep t−1 as the predicted noise, which is then used to recover x t−1 according to Eq. ( <ref type="formula" target="#formula_3">3</ref>) and ( <ref type="formula" target="#formula_8">7</ref>). Our conditional noise predictor is easy and stable to train due to the multi-scale skip connection. Moreover, it combines local and global information through the contracting and expansive path.</p><formula xml:id="formula_10">xt−1 = 1 √ α t xt − 1−α t √ 1− ᾱt θ (xt</formula><p>LR Encoder An LR encoder encodes the LR information x e , which is added to each reverse step to guide the generation to the corresponding HR space. In this paper, we choose the RRDB architecture following SRFlow <ref type="bibr" target="#b6">[Lugmayr et al., 2020]</ref>, which employs the residual-in-residual structure and multiple dense skip connections without batch normalization layers. In particular, we abandon the last convolution layer of the RRDB architecture because we do not aim at the concrete SR results but the hidden LR image information.</p><p>Training In the training phase, as illustrated in Algorithm 1, the input LR-HR image pairs in the training set are used to train SRDiff with the total diffusion step T (Line 1). We randomly initialize the conditional noise predictor θ and the RRDB based LR encoder D is pretrained by L 1 loss function (Line 2). We then sample a mini-batch of LR-HR image pairs from the training set (Line 4) and compute the residual image x r (Line 5). The LR images are encoded by the LR encoder as x e (Line 6), which is fed into the noise predictor θ together with t and x T . Then we sample from the standard Gaussian distribution and t from the integer set {1, • • • , T } (Line 7). We optimize the noise predictor by taking gradient step on Eq. ( <ref type="formula">6</ref>) (Line 8).</p><p>Inference A T-step SRDiff inference takes an LR image x L as input (Line 1), as illustrated in Algorithm 2. We sample a latent variable x T from the standard Gaussian distribution (Line 3) and upsample x L with bicubic kernel (Line 4). Different from the training procedure, we encode the LR image x L to x e by the LR encoder only once before the iteration begins (Line 5) and apply it in every iteration, which speeds up the inference. The iterations start from t = T (Line 6), and each iteration outputs a residual image with a different noise level, which gradually declines as t decreases. For t &gt; 1, we sample z from standard Gaussian distribution (Line 7) and compute x t−1 using the noise predictor θ with x t , x e and t as inputs (Line 8). Then for t = 1, we set z = 0 (Line 6) and x 0 is the final residual prediction (Line 10). An SR image is recovered by adding the residual image x 0 on the upsampled LR image up(x L ).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we first describe the experimental settings including datasets, model configurations and details in training and inference. Then we report experimental results and conduct some analyses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Settings</head><p>Datasets SRDiff is trained and evaluated on face SR (8×) and general SR (4×) tasks. For face SR, we use Celeb-Faces Attributes Dataset (CelebA) <ref type="bibr" target="#b6">[Liu et al., 2015b]</ref>, which is a large-scale face attributes dataset with more than 200K celebrity images. The images in this dataset cover large pose variations and background clutter. In this paper, we use the whole training set which consists of 162,770 images for training and evaluate on 5000 images from the test split following SRFlow <ref type="bibr" target="#b6">[Lugmayr et al., 2020]</ref>. We central-crop the aligned patches 2 and resize them to 160 × 160 as HR ground truth using standard MATLAB bicubic kernel. To obtain the corresponding LR images, we downsample the HR images with bicubic kernel. For ProgFSR <ref type="bibr">[Kim et al., 2019]</ref>, we use pro-2 https://drive.google.com/drive/folders/ 0B7EVK8r0v71pWEZsZE9oNnFzTm8 gressive bilinear kernel introduced in its original paper for a fair comparison.</p><p>For General SR, we use the DIV2K <ref type="bibr" target="#b10">[Timofte et al., 2018] and</ref><ref type="bibr">Flickr2K [Timofte et al., 2018]</ref>. These datasets consist of high-resolution RGB images with a large diversity of contents. In training, we use the whole training data (800 images) in DIV2K and whole images in Flickr2K (2650 images). Then we crop each image into patches with a size of 160 × 160 as HR ground truth following SRFlow. To obtain the corresponding LR images, we downsample the HR images with bicubic kernel. For evaluation, we use the whole validation data (100 images) in DIV2K. We downsample the HR images with bicubic kernel to obtain the LR images and directly apply SISR methods on the LR images to obtain the SR predictions without cropping.</p><p>Model Configuration Our SRDiff model consists of a 4step conditional noise predictor and an LR encoder with multiple RRDB blocks. The number of channels c in the first contracting step is set to 64. The numbers of RRDB blocks in the LR encoder are set to 8 and 15 for CelebA and DIV2K respectively and the channel size is set to 32. For diffusion process and reverse process, we set the diffusion step T to 100 and our noise schedule β 1 , ..., β T follows Nichol et al. <ref type="bibr">[2021]</ref>, which is proved to be beneficial for training. We also explore the model performance under different T and c in Section 5.3.</p><p>Training and Evaluation Firstly, we pretrain the LR encoder D using an L 1 loss for 100k iterations for the sake of efficiency. The training of the conditional noise predictor uses Eq. ( <ref type="formula">6</ref>) as loss term and Adam <ref type="bibr" target="#b1">[Kingma and Ba, 2014]</ref> as optimizer, with batch size 16 and learning rate 2 × 10 −<ref type="foot" target="#foot_2">4</ref> , which is halved every 100k steps. The entire SRDiff takes about 34/45 hours (300k/400k steps) to train on 1 GeForce RTX 2080Ti with 11GB memory for CelebA/DIV2K respectively.</p><p>Beside the well-known evaluation metrics PSNR and SSIM <ref type="bibr" target="#b11">[Wang et al., 2004]</ref>, we also evaluate our SRDiff on LPIPS <ref type="bibr" target="#b11">[Zhang et al., 2018a]</ref>, LR-PSNR <ref type="bibr" target="#b6">[Lugmayr et al., 2020]</ref> and the pixel standard deviation σ. LPIPS is recently introduced as a reference-based image quality evaluation metric, which computes the perceptual similarity between the ground truth and the SR images. LR-PSNR is computed as the PSNR between the downsampled SR image and the LR one indicating the consistency with the LR image. The pixel standard deviation σ indicates diversity in the SR output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Performance</head><p>In this subsection, we evaluate SRDiff by comparing with several state-of-the-art SR methods on face SR (8×) and general SR (4×) tasks. The detailed configurations of baseline models can be found in their original papers. General SR We also evaluate SRDiff on general SR (4×) compared with <ref type="bibr">EDSR [Lim et al., 2017]</ref>, RRDB, ESRGAN, RankSRGAN <ref type="bibr" target="#b12">[Zhang et al., 2019]</ref> and SRFlow (τ = 0.9) with their official released pretrained models 4 . As shown in Table <ref type="table" target="#tab_4">2</ref>, SRDiff achieves better quantitative results than the previous methods for most evaluation metrics (PSNR, SSIM and LR-PSNR) and comparable LPIPS, which reveals the effectiveness and great potential of our method. Figure <ref type="figure" target="#fig_5">5</ref> shows that SRDiff balances sharpness and naturalness well and produces strong consistency with the LR image. In contrast, PSNR-oriented methods (EDSR and RRDB) and SR-Flow, smear the edges of the objects, and GAN-driven methods (ESRGAN and RankSRGAN) introduce more artifacts. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Random SR (8x) predictions generated by our method. LR/HR image is shown on top top/bottom left. The right columns are diverse SR predictions and their facial regions, which are different from each other in expressions and attributes. For example, the first nose looks flat, while the third looks straight.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overview of two processes in SRDiff. The diffusion process is from right to left and the reverse process is from left to right. θ in p θ denotes the learnable components including conditional noise predictor and low-resolution encoder in SRDiff.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The architecture of conditional noise predictor in SRDiff. The content in parentheses (c,2c and 4c) after the block name means the channel size of each block. "Conv Block", "Res Block", "Downsample" and "Upsample" denote 2D-Convolution block, residual block, downsampling layer and upsampling layer respectively; "CS" and"ES" denote "Contracting Step" and "Expansive Step" respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Input: LR image xL, total diffusion step T 2: Load: conditional noise predictor θ and LR encoder D 3: Sample xT ∼ N (0, I) 4: Upsample xL to up(xL) 5: Encode LR image xL as xe = D(xL) 6:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Face SR 8× visual results. SRDiff generates rich details than RRDB and SRFlow, avoids artifacts (e.g., grids on the woman's hair and stripes on man's head) encountered by ESRGAN and ProgFSR and maintain consistency with the ground truth.</figDesc><graphic url="image-40.png" coords="5,54.00,255.25,53.86,53.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: General SR (4×) visual results. SRDiff produces more natural textures, e.g., animal fur, and grass texture, and also less unpleasant artifacts, e.g., speckles on the yellow grass by ESRGAN and RankSRGAN. Only SRDiff maintains the brown streak on the yellow grass, which is consistent with the ground truth.</figDesc><graphic url="image-49.png" coords="5,54.00,310.93,53.86,53.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Results for 8× SR of faces on CelebA. The first column indicates how LR images degenerate from HR ones and Prog. means the progressive linear kernel from ProgFSR.Face SR We compare SRDiff withRRDB [Wang et al.,  2018],ESRGAN [Wang et al., 2018],ProgFSR [Kim et al.,  2019]  and SRFlow (τ = 0.8)<ref type="bibr" target="#b6">[Lugmayr et al., 2020]</ref> 3 . RRDB is trained by only L 1 loss and can be regarded as a PSNRoriented method. The evaluation results are shown in Table1, which reveals that SRDiff outperforms previous works in term of most of the evaluation metrics, and can generate highquality and diverse SR images with strong LR-consistency. Specifically, 1) as shown in Figure4, compared with PSNRoriented methods, SRDiff reconstructs clearer textures, and compared with GAN-driven methods, SRDiff avoids artifacts and the results look more natural; and 2) as shown in Figure1, SRDiff provides diverse and realistic SR predictions given only one LR input. Every SR prediction is a complete portrait of a human face with rich details and maintains consistency with the input LR image. Moreover, SRDiff uses fewer model parameters (12M) than SRFlow (40M) and only takes about 30 hours until converge as described in Section 5.1, while SRFlow needs 5 days, which demonstrates that SRDiff is training-efficient and can achieve comparable performance with a small model footprint since SRDiff does not impose any architectural constraints to guarantee bijection. Compared with GAN-driven methods, SRDiff does not need any extra module (e.g., discriminator) in training.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Methods</cell><cell>↑PSNR ↑SSIM ↓LPIPS</cell><cell>↑LR-PSNR</cell><cell>↑ σ</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Bicubic</cell><cell>26.70</cell><cell>0.77</cell><cell>0.409</cell><cell>38.70</cell><cell>0.00</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>EDSR</cell><cell>28.98</cell><cell>0.83</cell><cell>0.270</cell><cell>54.89</cell><cell>0.00</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>RRDB</cell><cell>29.44</cell><cell>0.84</cell><cell>0.253</cell><cell>49.20</cell><cell>0.00</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>RankSRGAN</cell><cell>26.55</cell><cell>0.75</cell><cell>0.128</cell><cell>42.33</cell><cell>0.00</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ESRGAN</cell><cell>26.22</cell><cell>0.75</cell><cell>0.124</cell><cell>39.03</cell><cell>0.00</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SRFlow</cell><cell>27.09</cell><cell>0.76</cell><cell>0.120</cell><cell>49.96</cell><cell>5.14</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SRDiff</cell><cell>27.41</cell><cell>0.79</cell><cell>0.136</cell><cell>55.21</cell><cell>6.09</cell></row><row><cell></cell><cell>Methods</cell><cell cols="3">↑PSNR ↑SSIM ↓LPIPS</cell><cell>↑LR-PSNR</cell><cell>↑ σ</cell></row><row><cell></cell><cell>Bicubic</cell><cell>23.38</cell><cell>0.65</cell><cell>0.484</cell><cell>34.66</cell><cell>0.00</cell></row><row><cell>Bicubic</cell><cell>RRDB ESRGAN SRFlow</cell><cell>26.89 23.24 25.32</cell><cell>0.78 0.66 0.72</cell><cell>0.220 0.115 0.108</cell><cell>48.01 39.91 50.73</cell><cell>0.00 0.00 5.21</cell></row><row><cell></cell><cell>SRDiff</cell><cell>25.38</cell><cell>0.74</cell><cell>0.106</cell><cell>52.34</cell><cell>6.13</cell></row><row><cell>Prog.</cell><cell>ProgFSR SRFlow SRDiff</cell><cell>24.21 25.28 25.32</cell><cell>0.69 0.72 0.73</cell><cell>0.126 0.109 0.106</cell><cell>42.19 51.15 51.41</cell><cell>0.00 5.32 6.19</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Results for 4× SR of general images on DIV2K.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Ablations of SRDiff for faces SR (8×) on CelebA. T , c and Res. denote the total diffusion step, channel size of the noise predictor, and the residual prediction respectively.</figDesc><table><row><cell>T</cell><cell>c</cell><cell cols="4">Res. ↑PSNR ↑SSIM ↓LPIPS</cell><cell>↑LR-PSNR ↓Steps</cell></row><row><cell>100</cell><cell>64</cell><cell>√</cell><cell>25.38</cell><cell>0.74</cell><cell cols="2">0.106 52.34 300k</cell></row><row><cell>25 200 1000 100 100</cell><cell>64 64 64 32 128</cell><cell>√ √ √ √ √</cell><cell>25.12 25.41 25.43 25.15 25.40</cell><cell>0.71 0.74 0.75 0.72 0.74</cell><cell cols="2">0.109 52.17 300k 0.106 52.31 300k 0.105 52.35 300k 0.108 52.20 300k 0.106 52.37 300k</cell></row><row><cell>100</cell><cell>64</cell><cell>×</cell><cell>24.88</cell><cell>0.70</cell><cell cols="2">0.111 51.90 600k</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Instead of the original L2 in Eq. (6), we use L1 for better training stability following Chen et al.[2021].</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">Due to inconsistent patch size, we retrain all these baseline models from scratch on our pre-processed CelebA dataset with released codes. SRFlow uses the same patch size as our model, but we cannot obtain the same patch with its released image example, and therefore, we also have to re-train it.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2">Except RRDB which is trained from scratch with L1 loss as that in Face SR.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation Study</head><p>To probe the influences of the total diffusion step T , noise predictor channel size c and the effectiveness of residual prediction, we conduct some ablation studies as illustrated in Table 3. From row 1, 2, 3 and 4, we can see that the image quality is enhanced as total diffusion steps increases. From row 1, 5 and 6, we can see that larger model width results in better performance. However, larger total diffusion steps and model width both lead to slower inference, and therefore, we choose T = 100 and c = 64 as the default setting after trading off. Row 1 and 7 indicate that the residual prediction not only greatly enhances the image quality but also speeds up the training, which demonstrates the effectiveness of residual prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Extensions</head><p>In this subsection, we explore some extended applications including content fusion and latent space interpolation.</p><p>Content Fusion SRDiff is applicable in content fusion tasks, which aim to generate an image by fusing contents from two source images, e.g., an eye-source image and a face-source image providing the eye and face contents respectively. In this paragraph, we use SRDiff to conduct face content fusion by a demonstration of fusing one's eyes with another one's face. The procedure of content fusion is shown in Algorithm 3. First, we directly fuse a face image x f by replacing the eye region of the face-source image f with that of the eye-source image x eye and compute the differences between x f and the upsampled LR face-source image up(x L ) to get the residual x r . Second, x r goes through a t-step diffusion process, which outputs the xt in latent space. Then xt is denoised to an HR residual using the conditional noise predictor iterated from t to 0 with the LR face-source information encoded by the LR encoder, which ensures the compatibility of the two contents. Then we get the fused SR image by adding the SR residual to up(x L ). Finally, we replace the eye region of the face-source image with that of the SR face image and preserve the non-manipulated face. As shown in Figure <ref type="figure">6a</ref>, we set different timesteps t ∈ {30, 50, 70} and find that the eye region of the fusion result is more similar to the eye-source image when t is small, and is closer to the face-source image as t becomes larger.</p><p>Algorithm 3 Content Fusion Latent Space Interpolation Given an LR image, SRDiff can manipulate its prediction by latent space interpolation, which linearly interpolates the latents of two SR predictions and generate a new one. Let xt, x t ∼ q(xt|x 0 ) and we decode the latent xt = λxt + (1 − λ)x t by the reverse process, which feeds xt into the noise predictor with the LR information encoded by LR encoder iteratively. Then, we add the output residual result to the up(x L ) to obtain the interpolated SR prediction. We set t = 50 and λ ∈ {0.0, 0.4, 0.8, 1.0}. It could be observed in Figure <ref type="figure">6b</ref> that with λ approaching to 1.0, the woman's expression becomes closer to x t , which is the top right image holding a big laugh. In the same way, the man's mouth turns wider and bigger from λ = 0.0 to λ = 1.0. The trend of the interpolated images shows the effectiveness of SRDiff in latent space interpolation. The detailed algo-rithm of latent space interpolation is shown in Algorithm 4. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we proposed SRDiff, which is the first diffusionbased model for single image super-resolution to the best of our knowledge. Our work exploits a Markov chain to convert HR images to latents in simple distribution and then generate SR predictions in the reverse process which iteratively denoises the latents using a noise predictor conditioned on LR information encoded by the LR encoder. To speed up convergence and stabilize training, SRDiff introduces residual prediction. Our extensive experiments on both face and general datasets demonstrate that SRDiff can generate diverse and realistic SR images and over-smoothing and mode collapse issues that occurred in PSNR-oriented methods and GAN-driven methods respectively. Moreover, SRDiff is stable to train with small footprint and without an extra discriminator. Besides, SRDiff allows for flexible image manipulation including latent space interpolation and content fusion.</p><p>In the future, we will further improve the performance of the diffusion-based SISR model and speed up the inference. We will also extend our work to more image restoration tasks (e.g., image denoising, deblurring and dehazing) to verify the potential of diffusion models in the image restoration domain.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">De novo-designed near-infrared nanoaggregates for superresolution monitoring of lysosomes in cells, in whole organoids, and in vivo</title>
		<author>
			<persName><forename type="first">Chen</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
				<editor>
			<persName><surname>Fookes</surname></persName>
		</editor>
		<imprint>
			<publisher>Chao Dong</publisher>
			<date type="published" when="2012">2021. 2021. 2018. 2018. 2015. 2015. 2019. 2019. 2012. 2020</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="5407" to="5416" />
		</imprint>
	</monogr>
	<note>CVPRW</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName><surname>Ho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08239</idno>
		<idno>arXiv:1412.6980</idno>
	</analytic>
	<monogr>
		<title level="m">Adam: A method for stochastic optimization</title>
				<editor>
			<persName><forename type="first">Deokyun</forename><surname>Kim</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Minseon</forename><surname>Kim</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Gihyun</forename><surname>Kwon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Dae-Shik</forename><surname>Kim</surname></persName>
		</editor>
		<meeting><address><addrLine>Ba</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2020. 2020. 2016a. 2016. 2016b. 2016. 2019. 2019. 2014. 2014</date>
			<biblScope unit="page" from="1637" to="1645" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Diffwave: A versatile diffusion model for audio synthesis</title>
		<author>
			<persName><surname>Kong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.09761</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Photo-realistic single image superresolution using a generative adversarial network</title>
		<author>
			<persName><surname>Ledig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Super resolution for remote sensing images based on a universal hidden markov tree model</title>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
				<editor>
			<persName><forename type="first">Sanghyun</forename><surname>Lim</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Heewon</forename><surname>Son</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Seungjun</forename><surname>Kim</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kyoung</forename><surname>Nah</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Lee</forename><surname>Mu</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2009">2009. 2009. 2017. 2017</date>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="136" to="144" />
		</imprint>
	</monogr>
	<note>Enhanced deep residual networks for single image super-resolution</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2015">2015a. 2015</date>
			<biblScope unit="page" from="3730" to="3738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Srflow: Learning the super-resolution space with normalizing flow</title>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015-12">2015b. December 2015. 2020. 2020</date>
			<biblScope unit="page" from="715" to="732" />
		</imprint>
	</monogr>
	<note>ECCV</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><surname>Misra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08681</idno>
		<title level="m">Diganta Misra. Mish: A self regularized non-monotonic neural activation function</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Self-controllable superresolution deep learning framework for surveillance drones in security applications</title>
		<author>
			<persName><forename type="first">Nichol</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Dhariwal</forename><forename type="middle">;</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Quinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nichol</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EAI Endorsed Transactions on Security and Safety</title>
				<imprint>
			<date type="published" when="2019">2021. 2021. 2020. 2020. 2019</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="4180" to="4189" />
		</imprint>
	</monogr>
	<note>ICCV</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Enhancenet: Single image super-resolution through automated texture synthesis</title>
		<author>
			<persName><forename type="first">Rad</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<editor>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Mehdi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Bernhard</forename><surname>Sajjadi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Michael</forename><surname>Scholkopf</surname></persName>
		</editor>
		<editor>
			<persName><surname>Hirsch</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2017">2019. 2019. 2017</date>
			<biblScope unit="page" from="4491" to="4500" />
		</imprint>
	</monogr>
	<note>ICCV</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ntire 2018 challenge on single image super-resolution: methods and results</title>
		<author>
			<persName><surname>Sohl-Dickstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.03585</idno>
	</analytic>
	<monogr>
		<title level="m">Deep unsupervised learning using nonequilibrium thermodynamics</title>
				<editor>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2015">2015. 2015. 2018. 2018. 2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing</title>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004. 2004. 2018. 2018. 2018a. 2018</date>
			<publisher>ECCV</publisher>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="600" to="612" />
		</imprint>
	</monogr>
	<note>Esrgan: Enhanced superresolution generative adversarial networks. The unreasonable effectiveness of deep features as a perceptual metric. CVPR</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Ranksrgan: Generative adversarial networks with ranker for image super-resolution</title>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2018">2018b. 2018. 2019. 2019</date>
			<biblScope unit="page" from="286" to="301" />
		</imprint>
	</monogr>
	<note>ICCV</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
