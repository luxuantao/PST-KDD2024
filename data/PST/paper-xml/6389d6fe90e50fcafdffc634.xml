<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Merging Similar Patterns for Hardware Prefetching</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shizhi</forename><surname>Jiang</surname></persName>
							<email>shizhi@nfs.iscas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">University of Chinese Academy of Sciences * ? ? Institute of Software</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qiusong</forename><surname>Yang</surname></persName>
							<email>qiusong@iscas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">University of Chinese Academy of Sciences * ? ? Institute of Software</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yiwei</forename><surname>Ci</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">University of Chinese Academy of Sciences * ? ? Institute of Software</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Merging Similar Patterns for Hardware Prefetching</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/MICRO56248.2022.00071</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>One critical challenge of designing an efficient prefetcher is to strike a balance between performance and hardware overhead. Some state-of-the-art prefetchers achieve very high performance at the price of a very large storage requirement, which makes them not amenable to hardware implementations in commercial processors.</p><p>We argue that merging memory access patterns can be a feasible solution to reducing storage overhead while obtaining high performance, although no existing prefetchers, to the best of our knowledge, have succeeded in doing so because of the difficulty of designing an effective merging strategy. After analysis of a large number of patterns, we find that the address offset of the first access in a certain memory region is a good feature for clustering highly similar patterns. Based on this observation, we propose a novel hardware data prefetcher, named Pattern Merging Prefetcher (PMP), which achieves high performance at a low cost. The storage requirement for storing patterns is largely reduced and, at the same time, the prefetch accuracy is guaranteed by merging similar patterns in the training process. In the prefetching process, a strategy based on access frequencies of prefetch candidates is applied to accurately extract prefetch targets from merged patterns. According to the experimental results on a wide range of various workloads, PMP outperforms the enhanced Bingo by 2.6% with 30? lesser storage overhead and Pythia by 8.2% with 6? lesser storage overhead.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Prefetching is one of the well-known techniques to speed up long-latency memory accesses for decades. With the everincreasing memory consumption of modern applications, a cache hierarchy with limited capacity can be a bottleneck in performance improvements of processors <ref type="bibr" target="#b0">[1]</ref>. Because caches can be effectively utilized in reducing memory access latency for reusable data, a large cache might be used to improve performance further. However, larger caches can lead to longer cache access latency. The performance improvement is limited when enlarging the capacity of highlevel caches in the hierarchy (e.g., L1 Data Cache, L1D) due to their strict latency requirements. Prefetchers are often employed to reduce memory access latency by prefetching data in need without requiring a big on-chip area, leading to better cost performance than enlarging caches.</p><p>To accurately capture memory access patterns, some stateof-the-art prefetchers require more and more storage in recording memory access history <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b7">[8]</ref>. The large storage requirement comes from two aspects. First, unlike a simple memory access pattern that can be described by a constant stride, a complex memory access pattern requires varied strides for description. It can be a considerable cost in recording complex patterns through bit vectors <ref type="bibr" target="#b8">[9]</ref>, delta sequences <ref type="bibr" target="#b9">[10]</ref>, etc. Second, many features, including Program Counters (PCs), memory addresses, memory address offsets, memory address strides, etc, and their combinations can be leveraged to index patterns for improving prefetch accuracy. A prefetcher for high performance tends to apply finegrained features with large value ranges to index thousands of patterns, resulting in great storage consumption.</p><p>The art of designing an efficient prefetcher, dealing with complex memory access patterns, is to strike a balance between performance and storage overhead. We find that the major portion of the storage consumption of some prefetchers is caused by severe data redundancy. For instance, we find 82.9% of patterns are redundant in Bingo <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b10">[11]</ref>, while astonishingly 24.2% of valid entries are allocated to the same pattern. The low storage efficiency due to the high data redundancy is the main reason for Bingo using a large table with 16, 000 entries.</p><p>Through analyzing a large number of memory access patterns captured from 125 traces, we observe that the patterns are highly similar, if they are indexed by the same address offset of the first access (named Trigger Offset) in a certain memory region. The access is named Trigger Access in the region. Selecting trigger offsets as features offers a great promise of designing an efficient pattern merging strategy, because the information loss is relatively small when merging similar patterns. As result, a prefetcher can consume less storage and obtain high performance if the patterns with the same trigger offset are merged. Because the patterns indexed by a trigger offset are not necessarily identical, the strategy must be deliberately designed such that characteristics of patterns can be maintained after merging, and prefetch targets can be efficiently extracted from merged patterns.</p><p>In this paper, we design a pattern merging strategy that can efficiently quantify characteristics of patterns. Moreover, we design a strategy that can accurately extract prefetch targets from merged patterns based on access frequencies of prefetch candidates. In a nutshell, we propose a Through exhaustive experiments on 125 traces from different benchmarks, PMP outperforms the enhanced Bingo by 2.6% with 30? lesser storage overhead and Pythia <ref type="bibr" target="#b6">[7]</ref> by 8.2% with 6? lesser storage overhead in a single-core system.</p><p>We make the following contributions in this paper:</p><p>? We make a crucial observation that the memory access patterns are highly similar if they have the same trigger offset, after detailed analysis of a large number of memory access patterns captured from 125 traces. ? We propose a novel prefetcher, named Pattern Merging Prefetcher (PMP), including: a pattern merging strategy that quantifies characteristics of patterns and reduces the storage consumption; an extraction strategy based on access frequencies of prefetch candidates from merged patterns for accurate prefetching; an optimization using a dual pattern table structure to provide multi-feature-based pattern prediction. ? We show that PMP obtains better performance at a low storage cost compared to four state-of-the-art prefetchers over 125 traces by extensive experiments. The remaining sections are organized as follows. Section II provides the background. Section III presents our motivations, i.e., three key observations of memory access patterns. Section IV describes the innovative mechanisms of PMP. An exhaustive performance evaluation is presented in Section V. Section VI discusses related work about hardware data prefetching with different pattern forms. Finally, the paper is concluded in Section VII.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND</head><p>We first briefly introduce some typical prefetchers based on the bit vector pattern form. Next, we delve into the bit vector pattern capturing framework of Spatial Memory Streaming (SMS) <ref type="bibr" target="#b8">[9]</ref>, on which our prefetcher is based.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Prefetchers based on Bit Vectors</head><p>A bit vector describes the accessed positions in a memory region, each bit of which corresponds to an offset in the region. For example, the bit vector 1011 means that P , P +2, and P + 3 have been accessed in memory region P . The bit vector form is first leveraged in SMS, whose efficiency has been proven for addressing complex memory access patterns. Bulk Memory Access Prediction and Streaming (BuMP) <ref type="bibr" target="#b11">[12]</ref> improves SMS by reducing memory energy consumption. Bingo <ref type="bibr" target="#b1">[2]</ref> improves SMS by using multiple features, combining PCs with offsets or addresses, to accurately locate patterns to be prefetched. Dual Spatial Pattern Prefetcher (DSPatch) <ref type="bibr" target="#b12">[13]</ref> records memory access patterns with dual bit vectors, generated by an OR operation and an AND operation respectively, and uses different prefetch policies according to the bandwidth of environment.</p><p>The bit vector form has many advantages. First, any memory access distribution in a memory region can be represented with a bit vector. Second, bit vectors can represent dozens of offsets in a memory region at a very low cost. Benefiting from the high information density of bit vectors, a prefetcher can immediately generate many prefetch targets, leading to deep prefetching. Though bit vectors do not maintain temporal information of memory accesses, i.e., access order, a prefetcher can apply some heuristic methods to compensate for this drawback. For example, a prefetcher can prefetch the nearest target to the currently accessed address in advance. For a prefetcher based on bit vectors, the performance improvement, gained by attaching temporal information, is not significant <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Pattern Capturing Framework of SMS</head><p>SMS adopts a lightweight pattern capturing framework, which accounts for 2% of its total storage. It consists of two set-associative tables: the Filter Table (FT) and the Accumulation Table (AT). The FT records information of the first access to each memory region. The AT accumulates memory access patterns for each memory region. The pattern capturing procedure of SMS is shown in Fig. <ref type="figure" target="#fig_0">1</ref>. First, when the memory region of a new memory access misses in the AT and the FT, the FT will allocate a new entry to store the PC and the address of the access. The offset of its address is a trigger offset. Second, when another access to the same region comes with a different offset, a bit vector is assembled with the offsets of these two accesses and sent to the AT. Third, the offsets of the following accesses that belong to the region are used to update the bit vector in the AT. Finally, the accumulation process of the bit vector finishes when any cached data belonging to this region is evicted. The bit vector is sent to other components such as a pattern table.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. MOTIVATION</head><p>To analyze the characteristics of memory access patterns, we use the framework of SMS to capture patterns in 125 traces from SPEC CPU 2006 <ref type="bibr" target="#b14">[15]</ref>, SPEC CPU 2017 <ref type="bibr" target="#b15">[16]</ref>, PARSEC <ref type="bibr" target="#b16">[17]</ref>, and Ligra <ref type="bibr" target="#b17">[18]</ref>. The FT is 4 ? 16 setassociative, the AT is 8 ? 16 set-associative, and the pattern length is 64, matching with cachelines in 4KB pages. Observation 1: Only a tiny minority of memory access patterns occur with high frequency.</p><p>Because a bit vector consists of dozens of bits (64b), the huge status space (2 64 ) might introduce a large amount of data that no caches can afford. Fortunately, majority patterns occur infrequently. According to our experiments, 6.5?10 6 distinct patterns totally occur about 1.1?10 8 times among 125 traces, and 75.6% of distinct patterns appear only once. As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, the top 10 frequent patterns account for 33.1% of the total occurrences. Note that these patterns cover an extremely small portion (1.55 ? 10 -4 %) of distinct patterns. Moreover, the top 100 and the top 1000 frequent patterns account for 57.4% and 73.8% of the total occurrences respectively. Because only a tiny minority of patterns occur intensively, it is feasible to design a lightweight prefetcher with dozens of entries, in which only highly frequent patterns are maintained. Observation 2: Indexing memory access patterns with fine-grained features can lead to severe data redundancy.</p><p>The indexing schemes used by state-of-the-art prefetchers [2]- <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b18">[19]</ref> can hardly guarantee that the indexed memory access patterns are unique in their storage. As result, a large amount of data redundancy might be introduced.</p><p>To quantitatively analyze the data redundancy of various indexing schemes using different features, we define the Pattern Collision Rate (PCR) as the number of distinct  Intuitively, prefetchers tend to use fine-grained features with high bit widths, so that reduced PCRs can be obtained to effectively differentiate patterns. According to Table <ref type="table" target="#tab_0">I</ref>, the 80-bit PC+Address feature has a PCR of 1.7, providing the highest resolution in recognizing patterns compared to the other listed features. However, large storage could be wasted for storing redundant patterns, because the PDR of the PC+Address feature is high (608.7). By evaluating Bingo that uses the PC+Address feature, 82.9% of its patterns are redundant at the end of simulation, and 24.2% of valid entries are occupied by the same pattern.</p><p>For features containing addresses, their high PDRs indicate that many patterns are shared among different memory regions. To decrease duplicate patterns caused by this reason, the high bits of addresses that represent regions cannot be used as a feature, so that the same patterns of different regions can be indexed into the same one. Because it is difficult to find a feature that can eliminate the duplication, we try to reduce duplication with features of low PDRs.</p><p>A new problem appears if we attempt to leverage features with limited PDRs, such as the Trigger Offset feature, the PC feature, and the PC+Trigger Offset feature, to reduce data redundancy. According to their high PCRs, memory access patterns can be replaced frequently in a set-associative cache because thousands of distinct patterns inevitably collide with each other, resulting in poor prefetch accuracy. Even if we use an ideally large cache to save all the collided patterns, it is still difficult to select correct prefetch targets from them.</p><p>As result, those features with small PDRs can hardly be employed in a naive manner to reduce data redundancy. Observation 3: Patterns are highly similar if they have the same trigger offset.</p><p>To reduce overhead and, at the same time, keep the high performance of a prefetcher, a promising approach is to merge feature values or patterns. For example, data redundancy can be eliminated if feature values related to a pattern are merged into the same index. Challenges exist in implementing the idea of merging feature values or patterns for a prefetcher. On one side, we suppose that the feature values can be merged by a certain method such as an AND operation. The merged feature value keeps varying during the pattern capturing process, and so does the location of the related pattern in a cache. The requirement of frequently reallocating locations of patterns makes merging feature values infeasible. On the other side, we suppose that the patterns related to a feature value can be merged. Because the feature value is fixed, the location of the merged pattern in a cache is stable. But the merged pattern keeps varying during the pattern capturing process. In this case, how to guarantee prefetch accuracy for a prefetcher based on merged patterns becomes the key problem.</p><p>We hope to find a feature that can cluster similar patterns, thereby reducing the difficulty of designing an efficient  approach for merging patterns. For example, the common memory access distribution of patterns can be kept after merging with a bit-wise AND operation, and the information loss caused by merging patterns with high similarity is relatively small. In addition, the common distribution of similar patterns can be a good prefetch prediction target. Because memory access patterns are presented as (bit) vectors, we can use the Intracluster Centroid Diameter Distance (ICDD), the double average distance between all of the vectors and the center of a cluster, to measure their similarity. A smaller ICDD indicates higher similarity of patterns in a cluster. The calculation formulas are:</p><formula xml:id="formula_0">ICDD(S) = 2{ ?x?Sd(x, V ) |S| }. V = 1 |S| ?x?Sx. (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where S is a vector cluster, x is a vector belonging to S, and d(?, ?) is the Euclidean distance between two vectors. All the features have the same value range in our analysis. The Trigger Offset, hashed PC, hashed PC+Trigger Offset, hashed Address, and hashed PC+Address features all have a width of 6 bits, i.e., patterns are clustered into 64 sets (clusters). The average ICDD of 64 clusters is used to characterize the similarity of all clustered patterns. Fig. <ref type="figure" target="#fig_3">4</ref> illustrates the layouts of average ICDDs among 125 traces for each feature. The patterns with the same trigger offset have the highest similarity. Trigger Offset can be a good feature for implementing an efficient pattern merging method.</p><p>To show clustered patterns straightforwardly, we draw patterns in several typical traces as heat maps. A point in a heat map represents the magnitude of occurrences of patterns that contain the corresponding offset. Fig. <ref type="figure" target="#fig_5">5a</ref> is a heat map that shows the representative memory access distributions of a MCF trace. For most of memory accesses in the trace, the program tends to access a few positions around their current access addresses. These frequently visited positions form a blue dotted slash in Fig. <ref type="figure" target="#fig_5">5a</ref>. When certain big trigger offsets appear, the program tends to issue backward memory accesses, which form three horizontal dotted lines at the bottom. Fig. <ref type="figure" target="#fig_5">5b</ref> shows another memory access distribution from an Astar trace, in which the patterns can be described through three slashes. This indicates that the memory accesses of Astar obey a constant stride pattern.</p><p>However, the representative patterns cannot be observed when the hashed PC+Address feature is used. Fig. <ref type="figure" target="#fig_5">5c</ref> shows the memory access distributions of the MCF trace in this situation. As the figure illustrates, patterns are scattered into all 64 sets. We can not tell the common memory access distributions in the heat map. The common characteristics of memory access patterns are destroyed, probably resulting in inaccurate merging results.</p><p>Moreover, because the PDR of the PC feature is the smallest in Table <ref type="table" target="#tab_0">I</ref>, we would have thought that it could be a better choice compared to the other features. Surprisingly, we find the similarity of patterns indexed by PCs is lower than the Trigger Offset feature according to Fig. <ref type="figure" target="#fig_3">4</ref>. For most traces, patterns clustered by the PC feature present overlapped memory access distributions in each set, leading to limited recognition of patterns. In addition, PCs generally distribute patterns into several concentrated sets, which can be leveraged to predict whether or not patterns can be prefetched. As shown in Fig. <ref type="figure" target="#fig_5">5d</ref>, those PC-indexed patterns are allocated into several sets, forming horizontal lines. Finally, the PC feature is used to help predict prefetch levels as described later in Section IV-C.</p><p>Discussion. We think one major reason for Observation 3 is that Trigger Offset can be a more general feature relating to memory access behaviors of programs compared to PCs and addresses. For example, the backward memory accesses, as shown at the bottom of Fig. <ref type="figure" target="#fig_5">5a</ref>, can be generated by the two loops of the following codes in MCF <ref type="bibr" target="#b19">[20]</ref>: // File: pflowup.c // Method: MCF_primal_update_flow // data are stored in a big array for(;iplus!=w;iplus=iplus-&gt;pred) {...} for(;jplus!=w;jplus=jplus-&gt;pred) {...} Features containing addresses fail to cluster the same pat-terns because the accesses to a big array can cross many memory regions. The patterns can also hardly be clustered by the PC feature due to the different PCs of the two loops. In contrast, the trigger offsets can be a general and recognizable feature. First, the backward accesses to a big array will first load data from the end of a region, probably resulting in the same big trigger offset for the same pattern in different regions. Second, three major patterns generated by the loops can be differentiated by different big trigger offsets as Fig. <ref type="figure" target="#fig_5">5a</ref> shows. They could be mixed if the PCs were used as a feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DESIGN: PMP</head><p>The prefetching mechanism of PMP consists of two processes working in parallel: the training process and the prefetching process. In this section, we first introduce a Pattern Merging strategy in the training process, enabling a substantial reduction of storage requirements without sacrificing performance. Second, we present a Prefetch Pattern Extraction strategy that generates highly accurate prefetch targets from merged patterns in the prefetching process. Third, we propose an optimized structure, Dual Pattern Tables, to leverage multiple features for improving prefetch accuracy further. Arbitration rules are also proposed to decide the final prefetch targets based on the predictions of the dual pattern tables. Then, we put it together and summarize the main flows. Finally, we discuss the overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Pattern Merging</head><p>The observations in Section III inspire us to build a storage efficient prefetcher through merging memory access patterns clustered by trigger offsets. How to merge these patterns is a key problem that directly determines the effectiveness of our prefetcher. A bit-wise OR operation or a bit-wise AND operation could be an option for pattern merging, but the two operations are abandoned eventually. The OR operation creates a superset of patterns, e.g., the union of pattern 1111 and any other pattern is always 1111. The AND operation generates a common subset of patterns, e.g., the intersection of pattern 0000 and any other pattern is always 0000. As demonstrated in the examples above, a few outlier samples can obscure the differences in memory access patterns completely, leading to inaccurate records.</p><p>Based on extensive studies, we choose to merge bit vector patterns by counting the number of occurrences of each offset in bit vectors, instead of the two operations mentioned above. To achieve this goal, we apply a vector of counters (named Counter Vector), in which each element records the number of accesses to an offset, to merge patterns in a cluster. In the training process of PMP, the counters for offsets in a vector increase in parallel if the corresponding offsets are accessed in a bit vector pattern to be merged.</p><p>Fig. <ref type="figure" target="#fig_6">6a</ref> illustrates an example of merging the bit vector (0, 1, 1, 0, 1, 0, 0, 0), captured from the access sequence P + 2, P + 1, P + 4 in memory region P , into the counter vector (3, 0, 3, 0, 3, 0, 0, 0). First, the bit vector needs shifting into an anchored bit vector. The bit vector is converted into (1, 0, 1, 0, 0, 0, 0, 1) by left circular shifting 2 positions because the trigger offset is 2. Then, the first, the third, and the last elements of the counter vector increase by 1 respectively. Finally, the counter vector is updated to (4, 0, 4, 0, 3, 0, 0, 1). Note that the counter corresponding to the trigger offset will always be the first element of a shifted vector and increases in every merging operation, so it is called Time Counter.</p><p>Because all the patterns are merged, old records cannot be simply evicted during the training process. Instead, all the elements in a counter vector are halved when the time counter saturates. This mechanism aims at reducing but keeping the effects of old records in the prefetching process. Supposing that the maximum value of time counters is 3, the counter vector (4, 0, 4, 0, 3, 0, 0, 1) is saturated in the example above, then it is halved to (2, 0, 2, 0, 1, 0, 0, 0).</p><p>The pattern merging strategy is efficient for two reasons. First, because the patterns to be merged have high similarity, their common characteristics remain after merging, which lays the groundwork for high prediction accuracy in the prefetching process. Otherwise, the merged patterns would be inevitably ambiguous for prefetching, if the patterns being merged were irrelevant. Second, unlike the OR/AND operation, which accepts/rejects all the differences in patterns, our strategy quantifies characteristics of patterns and reduces the storage consumption. The statistical results after merging can be leveraged for precisely predicting prefetch targets, which is described in the next subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Prefetch Pattern Extraction</head><p>In the prefetching process, stored patterns can be triggered when a trigger access comes. Triggered patterns in the form of counter vectors cannot be simply replayed for prefetching like prior prefetchers do. Therefore, a conversion from triggered patterns to prefetch targets is required. Our strategy is to individually select target offsets by examining the corresponding elements in the triggered counter vector to form a Prefetch Pattern, which is a vector of target cache levels for each offset. We consider three different schemes.</p><p>Access-Number-based Extraction. A prefetch target for an offset is generated if the corresponding element of the triggered counter vector is equal to or greater than a prefetch threshold. We call this scheme Access-Number-based Extraction (ANE). For example, the counter vector (4, 2, 0, 1) can be converted to the prefetch pattern (0, L1, 0, L1) if the prefetch threshold for L1D is 1, then A+1 and A+3 are the prefetch targets for the current cacheline address A. Please note that the trigger offset (the first offset after shifting) itself will never be prefetched. This scheme is easy to implement, but the obvious drawback is that any target offset needs an inevitable cold start time to reach the prefetch threshold. For a threshold T , an offset will not be prefetched until it has been visited T times, losing many prefetch chances.</p><p>Access-Ratio-based Extraction. For every element of the triggered counter vector, a ratio of it to the sum of all counters can be compared to a threshold for generating prefetch targets. We briefly call this scheme Access-Ratiobased Extraction (ARE). For example, given a prefetch threshold 1/4 for L1D, the ratios of each element in the counter vector (4, 2, 0, 1) are (?, 2/3, 0, 1/3). Because the trigger offset is excluded, its ratio is ignored. Then, the counter vector can be converted to the prefetch pattern (0, L1, 0, L1). Then, A+1 and A+3 are the prefetch targets for the current cacheline address A. Though a counter vector is halved when the time counter saturates, the ratios can hardly be changed. <ref type="foot" target="#foot_1">1</ref> The ARE avoids retraining after halving if the pattern of following memory accesses does not vary, so the prefetching process continues. However, the ARE implicitly limits the maximum number of prefetches in one prediction, namely the prefetch depth. Because a prefetch target on an offset is generated only if its corresponding ratio is equal to or greater than a threshold, at most d offsets can be prefetched for the prefetch threshold 1/d. For a triggered vector containing 64 counters with the same value, e.g., a stream pattern, no addresses can be prefetched unless the threshold is lower than 1/63 (exclude the trigger offset). But a low threshold that is smaller than 1/63 may lead to inaccurate prefetching in most cases. The ARE introduces an unnecessary trade-off between the prefetch depth and the prefetch accuracy.</p><p>Access-Frequency-based Extraction. Access frequencies of offsets can be a good criterion for prefetch target selection. An access frequency is different from an access ratio that the former indicates how many times an offset occurs in a period, and it is not influenced by the occurrences of other offsets. Moreover, the access frequency of an offset can be easily calculated by dividing its counter by the time counter. The Access-Frequency-based Extraction (AFE) generates prefetch targets by comparing access frequencies of offsets to a prefetch threshold. For example, given an L1D prefetch threshold 1/4, the frequencies of each counter in the counter vector (4, 2, 0, 1) are (?, 2/4, 0, 1/4), so the counter vector can be converted to the prefetch pattern (0, L1, 0, L1), then A + 1 and A + 3 are the prefetch targets for the current cacheline address A. This scheme has several advantages. First, because the halving mechanism hardly affects the frequencies of offsets, the AFE avoids the extra retraining process when the following memory access patterns do not change. More importantly, the AFE has better adaptability to different kinds of patterns compared to the former two schemes. On one side, the AFE does not have the cold start problem. If an offset appears every time in the last T patterns, its frequency is 100% from the beginning of training which exceeds any threshold. This is friendly for patterns with a few repetitions. On the other side, no implicit restrictions are introduced by the AFE, since every offset that frequently occurs can be independently selected. This is friendly for stream-like patterns. Finally, we use the AFE as the default prefetch pattern extraction scheme.</p><p>To reduce the cache pollution in high-level caches without losing prefetch chances, PMP prefetches data into different cache levels depending on various thresholds. In the default configuration using the AFE, the confidence refers to frequencies. The threshold T l1d for prefetching data into L1D is 50% and the threshold T l2c for L2 Cache (L2C) is 15%. The target addresses, assembled using the current cacheline address and offsets with confidence greater than or equal to T l1d , are prefetched to L1D. The targets with confidence greater than or equal to T l2c but less than T l1d are prefetched into L2C for reducing the risk of cache pollution in L1D. Fig. <ref type="figure" target="#fig_6">6b</ref> shows an example that the prefetch pattern is (0, 0, L1, 0, L1, 0, 0, L2) extracted from the counter vector (4, 0, 4, 0, 3, 0, 0, 1).</p><p>As shown at the bottom of Fig. <ref type="figure" target="#fig_6">6c</ref>, a new prefetch pattern is stored in the Prefetch Buffer (PB) and indexed by the region address of the trigger access. There is no fixed prefetch degree for PMP. When free Prefetch Queue (PQ) entries exist, PMP first assembles addresses using valid offsets in the prefetch pattern that are near the cacheline address of the trigger access and issues them to the corresponding cache levels. If PQ is full, the prefetching process is suspended. When any load with the address of the same region reappears and free PQ entries exist, the process continues with the prefetch pattern in the PB. Please note that prefetch requests are prohibited from occupying all MSHRs, at least one MSHR is remained for normal load/store requests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Multi-Feature-based Prediction</head><p>Based on Observation 3, we notice that the PC feature of the trigger access can also be leveraged to help predict prefetch targets. The combination of PCs and trigger offsets can be used as a feature like prior research does, but it may not be the best option. First, the concatenated feature expands the index range greatly, necessitating a large table to store merged patterns. Second, patterns clustered by the combination of PCs and trigger offsets have lower similarity. The memory access patterns indexed by trigger offsets are separated into different sets again by PCs, bringing a greater divergence of patterns in a set than using the Trigger Offset feature or the PC feature alone.</p><p>Dual Pattern Tables. To enable multi-feature-based prediction and reduce the divergence of memory access patterns in a cluster, dual pattern tables are applied to maintain merged patterns indexed by trigger offsets and PCs respectively as Fig. <ref type="figure" target="#fig_6">6c</ref> illustrates. Because counter vectors collect all the patterns indexed by the same feature value without evicting, the vectors can be stored in tagless direct-mapped tables. The Offset Pattern Table (OPT) indexed with trigger offsets is the primary pattern table, and the PC Pattern Table (PPT) indexed with PCs is the supplement pattern table. During the training process, the dual pattern tables are updated simultaneously. In the prefetching process, the candidate prefetch patterns are independently predicted by the two tables when a trigger access comes. Two candidate prefetch patterns are given by the tables using the AFE described in the last subsection.</p><p>Arbitration. An arbiter is applied to decide the final prefetch pattern depending on the predictions from the two tables. Though both pattern tables can give prefetch targets individually, it is better to discard the targets given by the PPT that are not included in the targets given by the OPT. The predictions of the OPT are more accurate than the PPT according to the experimental result in Section V-E3. As shown in Fig. <ref type="figure" target="#fig_6">6e</ref>, the arbitration rules are as follows: 1) prefetches aiming at the same target offset are issued to L1D only if both pattern tables predict to prefetch data into L1D; 2) if the same target offset is predicted by both tables in which one table predicts to prefetch data into L2C, the target will be prefetched into L2C; 3) if the PPT has no predictions, the cache level of prefetches predicted by the OPT will be  downgraded (e.g., L2C to Last Level Cache, LLC); 4) no prefetches will be issued if there are no predictions from the OPT. Please note that the final prefetch pattern after arbitration is stored in the PB.</p><p>Coarse Counter Vector. Because the predictions from the PPT only affect prefetch cache levels, the storage costs can be reduced further by monitoring several offsets with one counter. The number of offsets monitored by a counter is called Monitoring Range. As Fig. <ref type="figure" target="#fig_6">6d</ref> depicts, the union of every several near bits of a bit vector is counted by a shorter counter vector, named Coarse Counter Vector. The 8-bit vector 10100001 is reduced to 1101 by joining every two bits, then 1101 is merged into the coarse counter vector (3, 1, 0, 1). Every element in a coarse counter vector controls whether/where to prefetch on adjacent offsets at the prefetch level arbitration step. The final prefetch pattern in the example of Fig. <ref type="figure" target="#fig_6">6</ref> is (0, 0, L1, 0, L2, 0, 0, L2) based on the two candidate prefetch patterns: (0, 0, L1, 0, L1, 0, 0, L2) from the OPT and (0, L1, 0, L2) from the PPT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Putting It Together</head><p>Fig. <ref type="figure" target="#fig_7">7</ref> shows the training and the prefetching flows of PMP. The training process performs on L1D loads. If the region of an L1D load misses in the AT and the FT, it is a trigger access in the region. The pattern capturing framework records the following accesses in the region as described in Section II-B and the captured pattern is merged in the OPT and the PPT. The prefetching process performs at the same time as the trigger access comes. The offset and the PC of the trigger access are used to trigger (index) the OPT and the PPT respectively. The final prefetch pattern is obtained after the extraction and the arbitration, and it is then stored in the PB for prefetching. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Overhead</head><p>Table II lists all the preset parameters of PMP. Table III lists the overhead details. In our default configuration, the OPT indexed with trigger offsets contains 64 entries, and the PPT indexed with PCs has 32 entries. The FT and the AT have 64 and 32 entries respectively. The PB can store 16 prefetch patterns. Two bits are enough for four states of every offset: No Prefetch, Prefetch to L1D, Prefetch to L2C, and Prefetch to LLC. Therefore, a prefetch pattern requires 126 bits for 63 targets. The size of memory regions that each pattern corresponds to is 4KB. Finally, the total hardware overhead of PMP is 4.3KB (30? lower than Bingo).</p><p>We use CACTI <ref type="bibr" target="#b20">[21]</ref> with its 22nm configuration to estimate the area consumption and the cache access time of our design. The area of the dual pattern table structure is 0.0069 mm 2 . It is 151? smaller than the large setassociative pattern table in Bingo, which costs 1.0372 mm 2 . The total access time (input and output) of the dual pattern table structure is 0.1ns, which is 11? shorter than the access time of Bingo's pattern table.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EVALUATION A. Methodology 1) Configuration:</head><p>We use the ChampSim <ref type="bibr" target="#b21">[22]</ref> simulator to evaluate prefetchers. Table IV illustrates the system configuration of ChampSim. We compare PMP to four state-of-the-art prefetchers: DSPatch <ref type="bibr" target="#b12">[13]</ref>, Bingo <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b10">[11]</ref>, SPP+PPF <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b22">[23]</ref>, and Pythia <ref type="bibr" target="#b6">[7]</ref>. For a fair comparison, all prefetchers are placed at L1D, and no helper prefetchers exist in the other cache levels. That is, the five prefetchers are all single-level in our evaluation. DSPatch is a lightweight bit-vector-based prefetcher that records patterns through an OR and an AND operations. The competition version of Bingo in the third Data Prefetching Championships (DPC) <ref type="bibr" target="#b10">[11]</ref> is the most powerful single-level prefetcher according to our experiments with a couple of state-of-the-art prefetchers. We enhance it by doubling the size of its pattern table to match its original configuration <ref type="bibr" target="#b1">[2]</ref>. SPP+PPF is a strong competitor in DPC-3 <ref type="bibr" target="#b22">[23]</ref>, leveraging nine different features.</p><p>Pythia is a new type of prefetcher built on machine learning in hardware. Table <ref type="table">V</ref> shows the storage overhead of these prefetchers.</p><p>2) Benchmarks: Over one hundred traces, captured from SPEC CPU 2006 <ref type="bibr" target="#b14">[15]</ref>, SPEC CPU 2017 <ref type="bibr" target="#b15">[16]</ref>, PARSEC <ref type="bibr" target="#b16">[17]</ref>, and Ligra <ref type="bibr" target="#b17">[18]</ref>, are used to evaluate the single-core performance of the five prefetchers. For SPEC CPU 2006 and SPEC CPU 2017, we use the instruction traces provided by DPC-2 and DPC-3 <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>. For PARSEC and Ligra, we use the traces provided by Pythia <ref type="bibr" target="#b6">[7]</ref>. All the traces have more than five LLC misses per kilo instructions (MPKI). Table VI lists the numbers of traces for each benchmark.</p><p>We evaluate the multi-core performance of the five prefetchers with homogeneous and heterogeneous multiprogrammed workloads in a 4-core processor. For homogeneous workloads, we examine prefetchers with 125 traces, each of which is simultaneously performed by different cores. For heterogeneous workloads, we first classify traces into three classes: Low MPKI (5 &lt; MPKI ? 10), Medium MPKI (10 &lt; MPKI ? 20), and High MPKI (MPKI &gt; 20). Then, we randomize traces according to their classes and generate workloads as Table <ref type="table" target="#tab_4">VII</ref> shows. We also examine prefetchers with CloudSuite traces <ref type="bibr" target="#b25">[26]</ref> but do not see many performance improvements, so the results are omitted.</p><p>In both single-core and multi-core systems, we use the first 50 million instructions of a trace to warm up microarchitectural structures and the next 200 million instructions to examine the performance of each core.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Single-core Performance</head><p>Fig. <ref type="figure" target="#fig_8">8</ref> illustrates the normalized IPCs (NIPCs) of the five prefetchers in the single-core system. We observe that PMP outperforms the other four prefetchers at a very low storage cost. PMP improves the performance of the nonprefetching baseline by 65.2% and outperforms DSPatch, Bingo, SPP+PPF and Pythia by 41.3% (up to 177.4%), 2.6% (up to 62.4%), 6.5% (up to 59.2%) and 8.2% (up to 183.1%). DSPatch requires the lowest storage among the five prefetchers. However, the low performance of DSPatch indicates that its pattern capturing strategies, an OR and an AND operations, are inefficient. Bingo is a heavy prefetcher that is 3? larger than a typical L1D, so it is more realistic for it to be placed at low-level caches, which brings lower performance. PMP (at L1) outperforms the original Bingo at LLC by 16.5%. SPP+PPF leverages nine features for filtering predictions generated by an aggressive Signature Path Prefetcher (SPP) <ref type="bibr" target="#b9">[10]</ref>, which performs much better than the original SPP. Though SPP+PPF applies more features compared to Bingo and PMP, its performance is lower, which indicates that the number of features is not the more the better. In contrast to PMP that can issue dozens of prefetches in one prediction, Pythia only generates one prefetch target per prediction, which limits its prefetch depth and performance. Pythia cannot deeply prefetch due to its poor prefetch accuracy, which will be described in the next subsection.</p><p>On the desktop or scientific workloads such as SPEC CPU 2006 and SPEC CPU 2017, the performance of PMP is better than that on the other workloads, since the memory access patterns of these workloads are regular. Though prefetchers usually require large hardware storage to deal with irregular memory accesses generated by Ligra and PARSEC, PMP still outperforms the heavy prefetchers (Bingo, SPP+PPF and Pythia) at a lower cost on these traces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Coverage &amp; Accuracy in Single-core</head><p>Prefetch coverage and prefetch accuracy are two metrics that explain a lot about performance of prefetchers. We define the coverage as the ratio of reduced load misses to the total load misses of the baseline, and the accuracy as the ratio of useful prefetches to the sum of useful and useless prefetches. PMP, SPP+PPF, and Bingo prefetch data into multiple levels of caches directly: PMP can fill data into L1D, L2C, and LLC; SPP+PPF and Bingo can fill data into  L1D and L2C based on their configuration. We examine coverage and accuracy on different cache levels separately.</p><p>Fig. <ref type="figure" target="#fig_9">9</ref> shows the coverage and the accuracy of these prefetchers. We observe that PMP obtains the highest L2C coverage and the highest LLC coverage among five prefetchers. Its L1D coverage is much higher than DSPatch by 121.1%, which is competitive compared to the other prefetchers. The L1D accuracy of PMP exceeds DSPatch, SPP+PPF and Pythia by 52.8%, 11.4% and 29.2%. The L2C accuracy of PMP exceeds DSPatch, Bingo, SPP+PPF, and Pythia by 118.1%, 66.8%, 302.3%, and 243.7%. The high accuracy indicates that the prediction strategy of PMP is accurate. Since the Trigger Offset and PC features allow patterns to be shared between different memory regions, prefetched data can be demanded even if it is not ever accessed before in a certain memory region, reducing compulsory misses. As result, the coverage of PMP is also high. In a nutshell, the high coverage and the high accuracy contribute much to the high performance of PMP.</p><p>The results show that the L2C accuracy and the LLC accuracy are much lower than the L1D accuracy for all the five prefetchers. This is because all the prefetchers train on L1D accesses. In the case of inclusive caches, prefetches for high-level caches will implicitly prefetch data to low-level caches for keeping inclusiveness. Since many load/store requests are invisible to low-level caches, the corresponding prefetch accuracy is lower. Fig. <ref type="figure" target="#fig_0">10</ref> shows the average valid prefetches that break down into useful and useless prefetches in different cache levels. We observe that PMP obtains high performance by suppressing the cache pollution in L1D and prefetching speculatively for low-level caches. First, PMP effectively restricts useless prefetches in the storage-limited L1D, while still offering many useful prefetches. DSPatch, SPP+PPF, and Pythia fail to limit the cache pollution in L1D compared to PMP. Though SPP+PPF and Pythia generate more useful prefetches than PMP, their large numbers of L1D Figure <ref type="figure" target="#fig_0">10</ref>: Average useful and useless prefetches. useless useful prefetches producing the least amount of useless prefetches in L1D, benefiting from applying the fine-grained feature (PC+Address) of the greatest pattern differentiation ability among the five prefetchers as Table <ref type="table" target="#tab_0">I</ref> shows. However, it also requires the largest storage space. Second, because PMP has a low L2C threshold (15%) and its first arbitration rule for prefetching data into L1D is strict, it speculatively prefetches data into low-level caches. Benefiting from the accurate prefetching strategy, the numbers of useful L2C and LLC prefetches of PMP are much larger than the other four prefetchers. Since L2C and LLC have larger storage space and are not latency-sensitive compared to L1D, it is affordable for them to contain more useless prefetches. The useless L2C/LLC prefetches of PMP do not hurt performance much.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Memory Traffic in Single-core</head><p>Prefetchers generally generate many memory access requests that consume additional memory bandwidths. To evaluate the memory traffic of the five prefetchers, we calculate the ratio of total memory access requests to the requests of the non-prefetching baseline as the Normalized Memory Traffic (NMT). Through evaluation, the NMTs of SPP+PPF and Pythia are 129.0% and 139.1%. Because triggered patterns in the bit vector form can generate dozens of prefetch targets in every prediction, the NMTs of DSPatch and Bingo are higher than the former two prefetchers, which are 159.8% and 164.2% respectively.</p><p>PMP produces the highest NMT (199.6%). It might be because PMP has the most aggressive prefetch policy compared to the other four prefetchers. First, counter vectors can generate up to 63 prefetch targets in one prediction, and prefetches will be issued if free PQ entries exist. Second, the prefetch condition of PMP is easy to meet because there are no tags for matching. After training PMP for a brief time, each trigger offset can issue a bundle of prefetches. PMP issues 1.46?10 7 prefetches per trace, which are 58.0% more than Bingo. This aggressive prefetch policy provides the deeper speculation through prefetching. Benefiting from the accurate prediction strategy, more useful prefetches are generated, so that PMP obtains the higher L2C/LLC prefetch coverage compared to other four prefetchers at the price of more memory traffic. Considering the high memory bandwidth consumption, high frequency memory (DDR4 <ref type="bibr" target="#b26">[27]</ref>, DDR5 <ref type="bibr" target="#b27">[28]</ref>, HBM <ref type="bibr" target="#b28">[29]</ref>, etc.) is appealing to PMP.  To limit the NMT of PMP without introducing extra overhead, a straightforward approach is to control the aggressiveness of prefetches for low-level caches (L2C and LLC). When the prefetch degree for low-level caches is 1, PMP still outperforms Bingo by 1.3% with a lower NMT (159.0%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Performance Analysis in Single-core 1) Pattern Merging Strategy:</head><p>In Section V-B, the comparison between PMP and DSPatch has indicated that it can be to merge patterns an an AND operation. Other than the merging strategy of PMP, it could be possible to obtain low storage consumption and good performance by merging the identical patterns only, considering spatial and temporal locality of workloads. A bit vector attached with a counter can be used to count repetitions of identical patterns, in which the counter can be used for target pattern selection, e.g., the ANE scheme can be used to determine the prefetch pattern according to the counter. Instead of individually selecting each offset, all the valid offsets in the bit vector with a counter that exceeds the threshold are the prefetch targets. The enhanced bit vectors can be stored in a setassociative cache indexed with trigger offsets. This design is named Design B, whose main structure is illustrated in Fig. <ref type="figure" target="#fig_10">11</ref>. We compare Design B to the pattern merging strategy of PMP. Table VIII depicts that the performance of Design B grows as the associativity increases. However, PMP outperforms Design B with 512 ways by 34.9%. Our pattern merging strategy is more efficient and eliminates the impact of a large number of evictions. Bingo does not suffer from severe evictions because of using features with low PCRs.</p><p>2) Prefetch Pattern Extraction Schemes: We consider three schemes for the prefetch pattern extraction in Section IV-B. For the ANE, the T l1d is 16 and the T l2c is 5 to scale close to the default thresholds of the AFE. The thresholds are the same between the ARE and the AFE.</p><p>Comparing to the AFE, the ARE performs poorly, which improves the baseline by 5.0%. We find that the poor adaptability of the ARE for prefetching severely limits the prefetch coverage. Because stream patterns can hardly be extracted by the ARE, a large number of prefetch opportu- nities are lost. The performance of the ARE is very low on many traces that contain a large number of stream patterns. Furthermore, PMP obtains no performance improvements by tuning the thresholds of the ARE. Though the similar methods of the ARE work well in many prefetchers <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b29">[30]</ref>, the ARE is not suitable for our prefetcher.</p><p>Because the ANE has the cold start time problem, and the prefetching process could be interrupted by the halving mechanism, the performance of the ANE is slightly impacted. PMP using the ANE achieves a 60.3% improvement beyond the baseline, which is 2.9% lower than the AFE. The ANE can be another feasible scheme with reduced hardware complexity compared to the AFE.</p><p>3) Multi-Feature-based Prediction: We compare the performance of the dual pattern table structure to the single table indexed with the combination of PCs and trigger offsets. Because PMP uses 6-bit trigger offsets and 5-bit PCs, the number of entries storing patterns increases from 96 (2 6 + 2 5 ) to 2048 (2 6+5 ) when the combined feature is used. However, the performance degrades by 3.1% compared to the dual pattern table structure.</p><p>We also evaluate the performance of PMP with a single feature. The performance reduces by 2.4% when using a single OPT. Moreover, we attempt to use a single PPT with the same size as the OPT, and the performance reduces by 3.5%. We find that the single-feature-based PMP prefetches more data into higher-level caches than the multi-featurebased PMP because of a lack of the prefetch level arbitration. As result, more useless prefetches are produced. For example, PMP with the single PPT generates 38.3% more useless prefetches at L1D compared to the default PMP. The large number of useless prefetches at L1D hurt its performance.</p><p>4) Preset Parameters: Pattern Length. The length of counter vectors depends on the size of tracked memory regions. Given a 4KB memory region, the accesses to each cacheline inside the region can be tracked using a vector containing 64 counters. PMP does not support cross-page prefetching, so we only discuss memory regions smaller than 4KB pages. As Table <ref type="table" target="#tab_6">IX</ref> shows, we evaluate the performance of PMP with different lengths of patterns at different scales (i.e., in 4KB, 2KB, and 1KB regions), namely PMP-64, PMP-32, and PMP-16. The results show that the performance degrades as the pattern length becomes shorter. Patterns can be folded in the case of shrinking the tracked memory regions, which impacts the statistical results of pattern merging, so that the accuracy of extracting the prefetch patterns is lower. According to our analysis, the L2C prefetch accuracy of PMP-32 decreases from 15.2% to 10.6% compared to PMP-64, and the L2C accuracy decreases to 8.0% after reducing the pattern length to 16.</p><p>Though the performance decreases, the overall overhead is greatly reduced when using short patterns. PMP-16 is still competitive in terms of performance improvements, which outperforms DSPatch by 34.5% at a 2? lesser storage cost. PMP-32 outperforms Bingo by 1.0% at a 51? lesser storage cost. We attempt to adjust the size of tracked memory regions of Bingo from 2KB to 4KB. But this adjustment does not affect the accuracy of Bingo because its prefetch accuracy relies on the fine-grained features. Trigger Offset Width. Table <ref type="table" target="#tab_7">X</ref> shows the NIPCs of PMP under different trigger offset widths. As the results show, the performance increases with the width of trigger offsets. To avoid hash collisions, the sizes of direct-mapped tables are equal to the value ranges of features. In this case, the storage overhead increases exponentially with the width of trigger offsets. Comparing to the default PMP, the overhead increases by 64? using 12-bit trigger offsets while the NIPC increases by only 0.4%. Since the gain of performance improvement is negligible, we finally choose to use 6-bit trigger offsets. Counter Size. The counter sizes of the OPT and the PPT can be different. We set the counter size of the PPT to 5 bits and only modify the counter size of the OPT. As described in Section IV-A, by leveraging the halving mechanism to reduce the effects of old records, a larger counter size allows maintaining history for a longer period. Table <ref type="table" target="#tab_7">X</ref> illustrates the performance of PMP under different counter sizes. With the increase of counter sizes, the performance can increase accordingly. The results show that the extraction strategy can make better predictions with longer history. Monitoring Range. As mentioned in Section IV-C, we reduce the storage overhead of the PPT by monitoring monitoring range 2 can be a good choice for reducing overhead while maintaining high performance. When the monitoring range is 8, the performance is almost the same as using a single OPT, which makes the dual pattern table structure useless.</p><p>F. Sensitivity in Single-core 1) Bandwidth: We examine the five prefetchers under various memory bandwidths as Fig. <ref type="figure" target="#fig_1">12a</ref> depicts. PMP performs better than the other prefetchers when the bandwidth is greater than or equal to 1600 MT/s. When the bandwidth reaches 3200 MT/s, the performance is close to peak. PMP slightly underperforms Bingo, SPP+PPF and Pythia at the 800 MT/s bandwidth due to the greater bandwidth requirements, but it still outperforms DSPatch, applying various prefetch policies based on bandwidths, by 18.2%. Because the number of prefetches hardly varies under different memory bandwidths, the performance of PMP is limited when the bandwidth is low.</p><p>2) LLC Size: Fig. <ref type="figure" target="#fig_1">12b</ref> shows the performance change of the five prefetchers under different LLC sizes. We enlarge the LLC by increasing the number of LLC sets. PMP outperforms the other four prefetchers in different LLC sizes. With the 8MB LLC setting, PMP outperforms Bingo by 3.3%. The performance gap between PMP and Bingo becomes larger when the LLC size increases because the impact of cache pollution caused by useless prefetches is reduced, which is friendly for aggressive prefetching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Multi-core Performance</head><p>Fig. <ref type="figure" target="#fig_2">13</ref> shows the multi-core performance of the prefetchers. PMP-Limit is the PMP whose prefetch degree for lowlevel caches is 1. PMP outperforms DSPatch by 39.6%, and the two heavy prefetchers SPP+PPF and Pythia by 7.3% and 6.9% respectively. PMP can match the performance of Bingo on homogeneous and heterogeneous workloads. Because the aggressive prefetching strategy consumes a larger amount of bandwidth resources in the 4-core system, the performance improvement is slightly reduced compared to PMP in the single-core system. After limiting prefetching, PMP-Limit obtains 1% higher performance compared to Bingo.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Prefetchers based on Constant Strides</head><p>Some memory access patterns can be described using constant strides. A stride is the difference between two continuous access addresses. This is a straightforward description of memory access patterns, which is widely used by prefetchers. For example, Next Line Prefetcher (NL) <ref type="bibr" target="#b30">[31]</ref> can be seen as a prefetcher that always prefetches data by one cacheline stride. Constant-stride-based prefetchers allow different strides for different regions <ref type="bibr" target="#b31">[32]</ref>.</p><p>periodically calculates the confidence of different strides and chooses the best stride for prefetching. Sandbox Prefetcher (SP) <ref type="bibr" target="#b18">[19]</ref> applies a similar method. The difference between BOP and SP is that the former records real memory accesses for calculating confidence and the latter uses a bloom filter to record fake prefetches instead. Though prefetchers built on constant strides can improve performance at very low storage costs, it can be hard to predict complex patterns. Because constant strides only assume that future accesses arrive at a certain pace, the patterns that consist of variable strides, like the address sequence (P + 1, P + 2, P + 4, P + 3, P + 1) in the memory region P , are beyond their capabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Prefetchers based on Delta Sequences</head><p>Delta sequences use several deltas to describe the variation of strides for access sequences, e.g., the delta sequence for the address sequence (P +1, P +2, P +4, P +3, P +1) in the memory region P is (1, 2, -1, -2). Different from the other pattern forms that require addition information for indexing, delta sequences of memory accesses can be utilized to index themselves, i.e., the prefixes of delta sequences can be used as a feature. For example, if the prior access history forms a delta sequence (1, 2, -1), we can predict the next delta is -2 in the example above. Signature Path Prefetcher (SPP) <ref type="bibr" target="#b9">[10]</ref> captures patterns using five-delta sequences. The first four deltas of a sequence are compressed as a signature to trigger the last delta. Variable Length Delta Prefetcher (VLDP) <ref type="bibr" target="#b32">[33]</ref> uses separated tables to maintain delta sequences with different lengths. Matryoshka <ref type="bibr" target="#b29">[30]</ref> supports variable-length sequence matching by coalescing variable-length sequences into a single table.</p><p>Delta sequences can record the spatial and temporal information of memory access patterns. An advantage of recording memory access patterns through delta sequences is that the common parts of sequences can be shared, resulting in a reduction of data redundancy. Moreover, no additional features are required for indexing the patterns described through delta sequences. However, prefetchers relying on delta sequences are not accurate enough. To address this obstacle, Perceptron-based Prefetch Filtering (SPP+PPF) <ref type="bibr" target="#b3">[4]</ref> designs a heavy perceptron-based filter with nine features to improve the accuracy of SPP. Moreover, prefetchers based on delta sequences have to yield prefetch addresses step by step using a recursive look-ahead strategy <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b29">[30]</ref>. It is difficult for prefetchers, relying on delta sequences, to issue dozens of prefetches immediately like Bingo does for deep prefetching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Other Hardware Data Prefetchers</head><p>Some other prefetchers pay attention to irregular memory access patterns such as Global History Buffer (GHB) <ref type="bibr" target="#b33">[34]</ref>, Irregular Stream Buffer (ISB) <ref type="bibr" target="#b34">[35]</ref>, Managed Irregular Stream Buffer (MISB) <ref type="bibr" target="#b5">[6]</ref>, and Triage <ref type="bibr" target="#b7">[8]</ref>. Because irregular memory access patterns usually cross many pages, which are difficult to be described through pattern forms such as constant strides, delta sequences, or bit vectors. GHB uses a large circular history buffer to record access history. ISB reconstructs physical addresses into structural addresses and stores them in memory. MISB improves ISB by filtering unnecessary memory requests with bloom filters. Triage organizes patterns as key-value pairs of addresses and requires up to the half storage of a LLC for eliminating requirements of off-chip storage. Most of them require too much storage and are only effective in irregular situations, which makes their design unaffordable in general processors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>In this paper, we design a low-overhead and powerful prefetcher, named Pattern Merging Prefetcher (PMP). By analyzing many features, we find that the Trigger Offset feature can be used to cluster memory access patterns with high similarity. By merging similar patterns, the storage overhead of our prefetcher is largely reduced, and an accurate prefetch pattern extraction strategy with various effective schemes is applied. Moreover, an optimized prediction mechanism based on multiple features is proposed. PMP outperforms the non-prefetching system by 65.2%, and exceeds the performance of enhanced Bingo by 2.6% with 30? lesser storage overhead and Pythia by 8.2% with 6? lesser storage overhead in a single-core system.</p><p>We believe that there is still room for further exploration in the design idea of pattern merging. In the future, we plan to do further research on fundamental reasons behind the three observations and try to apply the design idea to other predictors in processors for improving performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Pattern capturing procedure of SMS.</figDesc><graphic url="image-1.png" coords="2,65.46,73.00,159.96,121.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Percentages of top 10 frequent patterns.</figDesc><graphic url="image-4.png" coords="3,64.22,73.00,231.78,129.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Pattern collision and pattern duplicate.</figDesc><graphic url="image-5.png" coords="3,314.00,73.00,231.56,96.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Box plot of average ICDDs. White dotted lines refer to means. Points refer to outliers.</figDesc><graphic url="image-6.png" coords="4,64.44,73.00,231.44,185.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(a) Trigger Offset-indexed pattern heat map for a MCF trace. (b) Trigger Offset-indexed pattern heat map for an Astar trace. (c) PC+Address-indexed pattern heat map for a MCF trace. (d) PC-indexed pattern heat map for a MCF trace.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Heat maps of patterns. The x-axis represents the accessed offsets (from 0 to 63) in 4KB pages. The y-axis represents the indexes (from 0 to 63) of each feature. A darker point means more patterns containing the offset.</figDesc><graphic url="image-9.png" coords="4,314.00,194.64,104.33,104.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Architecture of PMP.</figDesc><graphic url="image-12.png" coords="6,192.78,73.00,128.04,132.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Training and prefetching flows of PMP.</figDesc><graphic url="image-15.png" coords="8,64.43,73.00,231.33,67.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Single-core performance of five prefetchers.</figDesc><graphic url="image-16.png" coords="10,64.21,73.00,231.78,66.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Coverage and accuracy of five prefetchers.</figDesc><graphic url="image-17.png" coords="10,64.21,164.31,231.78,66.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Main structure of Design B.</figDesc><graphic url="image-19.png" coords="11,64.67,73.00,231.33,54.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 :Figure 13 :</head><label>1213</label><figDesc>Figure 12: Performance of five prefetchers memory</figDesc><graphic url="image-20.png" coords="12,314.11,73.00,231.56,84.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table I :</head><label>I</label><figDesc>Average Pattern Collision/Duplicate Rates</figDesc><table><row><cell>Feature</cell><cell>Pattern Collision Rate</cell><cell>Pattern Duplicate Rate</cell></row><row><cell>PC (32b)</cell><cell>3823.6</cell><cell>2.2</cell></row><row><cell>Trigger Offset (6b)</cell><cell>2094.2</cell><cell>2.6</cell></row><row><cell>PC+Trigger Offset (38b)</cell><cell>269.0</cell><cell>6.3</cell></row><row><cell>Address (48b)</cell><cell>1.8</cell><cell>556.3</cell></row><row><cell>PC+Address (80b)</cell><cell>1.7</cell><cell>608.7</cell></row><row><cell cols="3">patterns related to a feature value and the Pattern Dupli-</cell></row><row><cell cols="3">cate Rate (PDR) as the number of feature values related</cell></row><row><cell cols="3">to a pattern. Fig. 3 shows examples of pattern collisions</cell></row><row><cell cols="3">and pattern duplicates. We say the pattern 1101 has two</cell></row><row><cell cols="3">duplicates because the feature values A and B both index</cell></row><row><cell cols="3">it, and the patterns 1101 and 0101 collide because they are</cell></row><row><cell cols="3">both indexed by the same feature value B. The PDR of</cell></row><row><cell cols="3">1101 is 2 and the PCR related to B is 2. A greater PDR</cell></row><row><cell cols="3">indicates higher data redundancy, since the same pattern</cell></row><row><cell cols="3">related to different feature values must be stored in multiple</cell></row><row><cell cols="3">entries in a classical set-associative cache. The averages of</cell></row><row><cell cols="3">the two metrics corresponding to various features are shown</cell></row><row><cell cols="2">in Table I after analyzing 125 traces.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table II :</head><label>II</label><figDesc>Preset Parameters</figDesc><table><row><cell>Parameter</cell><cell>Value</cell></row><row><cell>OPT Counter Size</cell><cell>5b</cell></row><row><cell>PPT Counter Size</cell><cell>5b</cell></row><row><cell>OPT Pattern Length</cell><cell>64</cell></row><row><cell>PPT Pattern Length</cell><cell>32</cell></row><row><cell>Region Size</cell><cell>4KB</cell></row><row><cell>Monitoring Range</cell><cell>2</cell></row><row><cell cols="2">L1D Prefetch Threshold (T l1d ) 50%</cell></row><row><cell>L2C Prefetch Threshold (T l2c )</cell><cell>15%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table III :</head><label>III</label><figDesc>Detailed Storage Overhead</figDesc><table><row><cell>Structure</cell><cell>Entry</cell><cell>Field</cell><cell>Storage</cell></row><row><cell></cell><cell></cell><cell>Region Tag (33b)</cell><cell></cell></row><row><cell>Filter Table</cell><cell>8 ? 8</cell><cell>Hashed PC (5b) Trigger offset (6b)</cell><cell>376Bytes</cell></row><row><cell></cell><cell></cell><cell>LRU (3b)</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Region Tag (35b)</cell><cell></cell></row><row><cell>Accumulation Table</cell><cell>2 ? 16</cell><cell>Hashed PC (5b) Bit Vector (64b) Trigger offset (6b)</cell><cell>456Bytes</cell></row><row><cell></cell><cell></cell><cell>LRU (4b)</cell><cell></cell></row><row><cell>Offset Pattern Table</cell><cell>64 ? 1</cell><cell>Counter Vector (320b)</cell><cell>2560Bytes</cell></row><row><cell>PC Pattern Table</cell><cell>32 ? 1</cell><cell>Coarse Counter Vector (160b)</cell><cell>640Bytes</cell></row><row><cell></cell><cell></cell><cell>Region Tag (36b)</cell><cell></cell></row><row><cell>Prefetch Buffer</cell><cell>1 ? 16</cell><cell>Prefetch Pattern (126 bits)</cell><cell>332Bytes</cell></row><row><cell></cell><cell></cell><cell>LRU (4b)</cell><cell></cell></row><row><cell></cell><cell cols="2">T otal ? 4.3KB</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table IV :</head><label>IV</label><figDesc>Simulated System Configuration</figDesc><table><row><cell>Core</cell><cell cols="4">One to four cores, 4GHz, 4-wide, 352-entry</cell></row><row><cell></cell><cell cols="4">ROB, 128-entry LQ, 72-entry SQ, 4KB Page</cell></row><row><cell>TLBs</cell><cell cols="4">64-entry ITLB, 64-entry DTLB, 1536-entry</cell></row><row><cell></cell><cell>L2DTLB</cell><cell></cell><cell></cell><cell></cell></row><row><cell>L1I</cell><cell cols="4">32KB, 8-way, 32-entry PQ, 8-entry MSHR,</cell></row><row><cell></cell><cell>4 cycles</cell><cell></cell><cell></cell><cell></cell></row><row><cell>L1D</cell><cell cols="4">48KB, 12-way, 8-entry PQ, 16-entry MSHR,</cell></row><row><cell></cell><cell>5 cycles</cell><cell></cell><cell></cell><cell></cell></row><row><cell>L2C</cell><cell cols="4">512KB, 8-way, 16-entry PQ, 32-entry MSHR,</cell></row><row><cell></cell><cell>10 cycles</cell><cell></cell><cell></cell><cell></cell></row><row><cell>LLC</cell><cell cols="4">2MB to 8MB, 16-way, 32 to 128-entry PQ,</cell></row><row><cell></cell><cell cols="4">64 to 256-entry MSHR, 20 cycles, Inclusive</cell></row><row><cell cols="4">DRAM 4GB 1 channel (1-core), 8GB 2</cell><cell></cell></row><row><cell></cell><cell cols="3">channels (4-core), 3200 MT/sec</cell><cell></cell></row><row><cell></cell><cell cols="3">Table V: Prefetcher Overhead</cell><cell></cell></row><row><cell>DSPatch</cell><cell>Bingo</cell><cell>SPP+PPF</cell><cell>Pythia</cell><cell>PMP</cell></row><row><cell>3.6KB</cell><cell cols="2">127.8KB 48.4KB</cell><cell cols="2">25.5KB 4.3KB</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table VI</head><label>VI</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="2">: Traces</cell><cell></cell></row><row><cell cols="4">SPEC 06 SPEC 17 Ligra PARSEC</cell><cell>Total</cell></row><row><cell>38</cell><cell>36</cell><cell>42</cell><cell>9</cell><cell>125</cell></row><row><cell cols="5">Table VII: Heterogeneous 4-core Workloads</cell></row><row><cell>MIX</cell><cell></cell><cell></cell><cell></cell><cell>#</cell></row><row><cell cols="2">All Low MPKI</cell><cell></cell><cell></cell><cell>10</cell></row><row><cell cols="2">All Medium MPKI</cell><cell></cell><cell></cell><cell>10</cell></row><row><cell cols="2">All High MPKI</cell><cell></cell><cell></cell><cell>10</cell></row><row><cell cols="4">Half Low and Half Medium MPKI</cell><cell>10</cell></row><row><cell cols="4">Half Low and Half High MPKI</cell><cell>10</cell></row><row><cell cols="5">Half Medium and Half High MPKI 10</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table VIII :</head><label>VIII</label><figDesc>Performance of Design B</figDesc><table><row><cell>Ways</cell><cell>8</cell><cell>32</cell><cell>128</cell><cell>512</cell></row><row><cell cols="2">NIPC 1.176</cell><cell>1.188</cell><cell>1.215</cell><cell>1.224</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table IX :</head><label>IX</label><figDesc>Performance and Overhead of PMP Under Different Pattern Lengths</figDesc><table><row><cell>Pattern Length</cell><cell>Region Size</cell><cell>Overhead</cell><cell>NIPC</cell></row><row><cell>64</cell><cell>4KB</cell><cell>4.3KB</cell><cell>1.652</cell></row><row><cell>32</cell><cell>2KB</cell><cell>2.5KB</cell><cell>1.626</cell></row><row><cell>16</cell><cell>1KB</cell><cell>1.6KB</cell><cell>1.572</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table X :</head><label>X</label><figDesc>Performance of PMP Under Different Trigger Offset Widths and Counter Sizes</figDesc><table><row><cell>Trigger Offset Width (b)</cell><cell>NIPC</cell><cell>Counter Size (b)</cell><cell>NIPC</cell></row><row><cell>6</cell><cell>1.652</cell><cell>2</cell><cell>1.624</cell></row><row><cell>7</cell><cell>1.654</cell><cell>3</cell><cell>1.634</cell></row><row><cell>8</cell><cell>1.655</cell><cell>4</cell><cell>1.648</cell></row><row><cell>9</cell><cell>1.657</cell><cell>5</cell><cell>1.652</cell></row><row><cell>10</cell><cell>1.657</cell><cell>6</cell><cell>1.653</cell></row><row><cell>11</cell><cell>1.657</cell><cell>7</cell><cell>1.654</cell></row><row><cell>12</cell><cell>1.658</cell><cell>8</cell><cell>1.655</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table XI :</head><label>XI</label><figDesc>Performance of PMP Under Different Pattern Monitoring Ranges</figDesc><table><row><cell>Monitoring Range</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell></row><row><cell>NIPC</cell><cell>1.650</cell><cell>1.652</cell><cell>1.630</cell><cell>1.615</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Authorized licensed use limited to: Tsinghua University. Downloaded on January 01,2024 at 07:40:01 UTC from IEEE Xplore. Restrictions apply.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>The values of counters in a counter vector decrease to half, so does their sum. Therefore, the ratios will not change theoretically. However, the calculation results may be slightly affected because of precision limitations in hardware.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>We thank the anonymous reviewers for their feedback.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Hitting the memory wall: implications of the obvious</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Wulf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Mckee</surname></persName>
		</author>
		<idno type="DOI">10.1145/216585.216588</idno>
		<ptr target="https://doi.org/10.1145/216585.216588" />
	</analytic>
	<monogr>
		<title level="j">SIGARCH Comput. Archit. News</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20" to="24" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bingo spatial data prefetcher</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bakhshalipour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shakerinava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lotfi-Kamran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sarbazi-Azad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="399" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Best-offset hardware prefetching</title>
		<author>
			<persName><forename type="first">P</forename><surname>Michaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="469" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Perceptron-based prefetch filtering</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chacon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pugsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Teran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Gratz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jim?nez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 ACM/IEEE 46th Annual International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pangloss: a novel markov chain prefetcher</title>
		<author>
			<persName><forename type="first">P</forename><surname>Papaphilippou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H J</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Luk</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1906.00877" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="1906">1906.00877, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Efficient metadata management for irregular data prefetching</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nathella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sunwoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.1145/3307650.3322225</idno>
		<ptr target="https://doi.org/10.1145/3307650.3322225" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th International Symposium on Computer Architecture, ISCA 2019</title>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Manne</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Hunter</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Altman</surname></persName>
		</editor>
		<meeting>the 46th International Symposium on Computer Architecture, ISCA 2019<address><addrLine>Phoenix, AZ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">June 22-26, 2019. 2019</date>
			<biblScope unit="page" from="449" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Pythia: A Customizable Hardware Prefetching Framework Using Online Reinforcement Learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kanellopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Shahroodi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Subramoney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3466752.3480114</idno>
		<ptr target="https://doi.org/10.1145/3466752.3480114" />
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Association for Computing Machinery</publisher>
			<biblScope unit="page" from="1121" to="1137" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Temporal prefetching without the off-chip metadata</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nathella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pusdesris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sunwoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.1145/3352460.3358300</idno>
		<ptr target="https://doi.org/10.1145/3352460.3358300" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 52nd Annual IEEE/ACM International Symposium on Microarchitecture<address><addrLine>MICRO; Columbus, OH, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019-10-12">2019. October 12-16, 2019. 2019</date>
			<biblScope unit="page" from="996" to="1008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Spatial memory streaming</title>
		<author>
			<persName><forename type="first">S</forename><surname>Somogyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moshovos</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISCA.2006.38</idno>
		<ptr target="https://doi.org/10.1109/ISCA.2006.38" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd Annual International Symposium on Computer Architecture, ser. ISCA &apos;06</title>
		<meeting>the 33rd Annual International Symposium on Computer Architecture, ser. ISCA &apos;06<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="252" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Path confidence based lookahead prefetching</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Pugsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Gratz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L N</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chishti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 49th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Accurately and maximally prefetching spatial data access patterns with bingo</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bakhshalipour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shakerinava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lotfi-Kamran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sarbazi-Azad</surname></persName>
		</author>
		<ptr target="https://dpc3.compas.cs.stonybrook.edu/pdfs/Accurately.pdf" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bump: Bulk memory access prediction and streaming</title>
		<author>
			<persName><forename type="first">S</forename><surname>Volos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Picorel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Grot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 47th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="545" to="557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dspatch: Dual spatial pattern prefetcher</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Nori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Subramoney</surname></persName>
		</author>
		<idno type="DOI">10.1145/3352460.3358325</idno>
		<ptr target="https://doi.org/10.1145/3352460.3358325" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture, ser. MICRO &apos;52</title>
		<meeting>the 52nd Annual IEEE/ACM International Symposium on Microarchitecture, ser. MICRO &apos;52<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="531" to="544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Not quite my temp: Matching prefetches to memory access times</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">Enright</forename><surname>Jerger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Prefetching Championship Workshop</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Spec cpu</title>
		<ptr target="https://www.spec.org/cpu" />
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Spec cpu</title>
		<ptr target="https://www.spec.org/cpu" />
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Parsec</title>
		<ptr target="http://parsec.cs.princeton.edu/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Ligra: A lightweight graph processing framework for shared memory</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Blelloch</surname></persName>
		</author>
		<idno type="DOI">10.1145/2442516.2442530</idno>
		<ptr target="https://doi.org/10.1145/2442516.2442530" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, ser. PPoPP &apos;13</title>
		<meeting>the 18th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, ser. PPoPP &apos;13<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="135" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sandbox prefetching: Safe run-time evaluation of aggressive prefetchers</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Pugsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chishti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Balasubramonian</surname></persName>
		</author>
		<idno type="DOI">10.1109/HPCA.2014.6835971</idno>
		<ptr target="https://doi.org/10.1109/HPCA.2014.6835971" />
	</analytic>
	<monogr>
		<title level="m">20th IEEE International Symposium on High Performance Computer Architecture, HPCA 2014</title>
		<meeting><address><addrLine>Orlando, FL, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2014">February 15-19, 2014. 2014</date>
			<biblScope unit="page" from="626" to="637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Mcf homepage</title>
		<ptr target="https://www.zib.de/opt-longprojects/Software/Mcf/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cacti 6.0: A tool to model large caches</title>
		<author>
			<persName><forename type="first">N</forename><surname>Muralimanohar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Balasubramonian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">HP laboratories</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page">28</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Champsim simulator</title>
		<ptr target="https://github.com/ChampSim/ChampSim" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Enhancing signature path prefetching with perceptron prefetch filtering</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chacon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Teran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Gratz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jim?nez</surname></persName>
		</author>
		<ptr target="https://dpc3.compas.cs.stonybrook.edu/pdfs/Enhancingsignature.pdf" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">The 2nd data prefetching championship</title>
		<ptr target="http://comparch-conf.gatech.edu/dpc2/" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">The 3rd data prefetching championship</title>
		<ptr target="https://dpc3.compas.cs.stonybrook.edu/" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Cloudsuite traces</title>
		<ptr target="https://www.dropbox.com/sh/pgmnzfr3hurlutq/AACciuebRwSAOzhJkmj5SEXBa/CRC2trace?dl=0&amp;subfoldernavtracking=1" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Jedec-ddr4</title>
		<ptr target="https://www.jedec.org/sites/default/files/docs/JESD79-4.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Jedec-ddr5</title>
		<ptr target="https://www.jedec.org/standards-documents/docs/jesd79-5a" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Hbm specification</title>
		<ptr target="https://www.amd.com/Documents/High-Bandwidth-Memory-HBM.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Matryoshka: A Coalesced Delta Sequence Prefetcher</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1145/3472456.3473510</idno>
		<ptr target="https://doi.org/10.1145/3472456.3473510" />
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Association for Computing Machinery</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sequential program prefetching in memory hierarchies</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.1109/C-M.1978.218016</idno>
		<ptr target="https://doi.org/10.1109/C-M.1978.218016" />
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="7" to="21" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Effective hardware based data prefetching for high-performance processors</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Baer</surname></persName>
		</author>
		<idno type="DOI">10.1109/12.381947</idno>
		<ptr target="https://doi.org/10.1109/12.381947" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Computers</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="609" to="623" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Efficiently prefetching complex address patterns</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shevgoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Koladiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Balasubramonian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Pugsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chishti</surname></persName>
		</author>
		<idno type="DOI">10.1145/2830772.2830793</idno>
		<ptr target="https://doi.org/10.1145/2830772.2830793" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th International Symposium on Microarchitecture</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Prvulovic</surname></persName>
		</editor>
		<meeting>the 48th International Symposium on Microarchitecture<address><addrLine>MICRO; Waikiki, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015-12-05">2015. December 5-9, 2015. 2015</date>
			<biblScope unit="page" from="141" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Data cache prefetching using a global history buffer</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Nesbit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.1109/MM.2005.6</idno>
		<ptr target="https://doi.org/10.1109/MM.2005.6" />
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="90" to="97" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Linearizing irregular memory accesses for improved correlated prefetching</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.1145/2540708.2540730</idno>
		<ptr target="https://doi.org/10.1145/2540708.2540730" />
	</analytic>
	<monogr>
		<title level="m">The 46th Annual IEEE/ACM International Symposium on Microarchitecture, MICRO-46</title>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Farrens</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</editor>
		<meeting><address><addrLine>Davis, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">December 7-11, 2013. 2013</date>
			<biblScope unit="page" from="247" to="259" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
