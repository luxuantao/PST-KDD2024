<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Gaussian Processes in Machine Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Carl</forename><forename type="middle">Edward</forename><surname>Rasmussen</surname></persName>
							<email>carl@tuebingen.mpg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Biological Cybernetics</orgName>
								<address>
									<postCode>72076</postCode>
									<settlement>Tübingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Gaussian Processes in Machine Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We give a basic introduction to Gaussian Process regression models. We focus on understanding the role of the stochastic process and how it is used to define a distribution over functions. We present the simple equations for incorporating training data and examine how to learn the hyperparameters using the marginal likelihood. We explain the practical advantages of Gaussian Process and end with conclusions and a look at the current trends in GP work.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Supervised learning in the form of regression (for continuous outputs) and classification (for discrete outputs) is an important constituent of statistics and machine learning, either for analysis of data sets, or as a subgoal of a more complex problem.</p><p>Traditionally parametric<ref type="foot" target="#foot_0">1</ref> models have been used for this purpose. These have a possible advantage in ease of interpretability, but for complex data sets, simple parametric models may lack expressive power, and their more complex counterparts (such as feed forward neural networks) may not be easy to work with in practice. The advent of kernel machines, such as Support Vector Machines and Gaussian Processes has opened the possibility of flexible models which are practical to work with.</p><p>In this short tutorial we present the basic idea on how Gaussian Process models can be used to formulate a Bayesian framework for regression. We will focus on understanding the stochastic process and how it is used in supervised learning. Secondly, we will discuss practical matters regarding the role of hyperparameters in the covariance function, the marginal likelihood and the automatic Occam's razor. For broader introductions to Gaussian processes, consult <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Gaussian Processes</head><p>In this section we define Gaussian Processes and show how they can very naturally be used to define distributions over functions. In the following section we continue to show how this distribution is updated in the light of training examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 1. A Gaussian Process is a collection of random variables, any finite number of which have (consistent) joint Gaussian distributions.</head><p>A Gaussian process is fully specified by its mean function m(x) and covariance function k(x, x ). This is a natural generalization of the Gaussian distribution whose mean and covariance is a vector and matrix, respectively. The Gaussian distribution is over vectors, whereas the Gaussian process is over functions. We will write:</p><formula xml:id="formula_0">f ∼ GP(m, k),<label>(1)</label></formula><p>meaning: "the function f is distributed as a GP with mean function m and covariance function k".</p><p>Although the generalization from distribution to process is straight forward, we will be a bit more explicit about the details, because it may be unfamiliar to some readers. The individual random variables in a vector from a Gaussian distribution are indexed by their position in the vector. For the Gaussian process it is the argument x (of the random function f (x)) which plays the role of index set: for every input x there is an associated random variable f (x), which is the value of the (stochastic) function f at that location. For reasons of notational convenience, we will enumerate the x values of interest by the natural numbers, and use these indexes as if they were the indexes of the process -don't let yourself be confused by this: the index to the process is x i , which we have chosen to index by i.</p><p>Although working with infinite dimensional objects may seem unwieldy at first, it turns out that the quantities that we are interested in computing, require only working with finite dimensional objects. In fact, answering questions about the process reduces to computing with the related distribution. This is the key to why Gaussian processes are feasible. Let us look at an example. Consider the Gaussian process given by:</p><formula xml:id="formula_1">f ∼ GP(m, k), where m(x) = 1 4 x 2 , and k(x, x ) = exp(− 1 2 (x − x ) 2 ).<label>(2)</label></formula><p>In order to understand this process we can draw samples from the function f . In order to work only with finite quantities, we request only the value of f at a distinct finite number n of locations. How do we generate such samples? Given the x-values we can evaluate the vector of means and a covariance matrix using Eq. ( <ref type="formula" target="#formula_1">2</ref>), which defines a regular Gaussian distribution:</p><formula xml:id="formula_2">µ i = m(x i ) = 1 4 x 2 i , i = 1, . . . , n and Σ ij = k(x i , x j ) = exp(− 1 2 (x i − x j ) 2 ), i,j = 1, . . . , n,<label>(3)</label></formula><p>where to clarify the distinction between process and distribution we use m and k for the former and µ and Σ for the latter. We can now generate a random vector from this distribution. This vector will have as coordinates the function values f (x) for the corresponding x's:  <ref type="formula" target="#formula_1">2</ref>). The dots are the values generated from Eq. ( <ref type="formula" target="#formula_3">4</ref>), the two other curves have (less correctly) been drawn by connecting sampled points. The function values suggest a smooth underlying function; this is in fact a property of GPs with the squared exponential covariance function. The shaded grey area represent the 95% confidence intervals</p><formula xml:id="formula_3">f ∼ N(µ, Σ).<label>(4)</label></formula><p>We could now plot the values of f as a function of x, see Figure <ref type="figure" target="#fig_0">1</ref>. How can we do this in practice? Below are a few lines of Matlab<ref type="foot" target="#foot_3">2</ref> used to create the plot: xs = (-5:0.2:5)'; ns = size(xs,1); keps = 1e-9; m = inline('0.25*x.^2'); K = inline('exp(-0.5*(repmat(p'',size(q))-repmat(q,size(p''))).^2)'); fs = m(xs) + chol(K(xs,xs)+keps*eye(ns))'*randn(ns,1); plot(xs,fs,'.')</p><p>In the above example, m and k are mean and covariances; chol is a function to compute the Cholesky decomposition<ref type="foot" target="#foot_4">3</ref> of a matrix.</p><p>This example has illustrated how we move from process to distribution and also shown that the Gaussian process defines a distribution over functions. Up until now, we have only been concerned with random functions -in the next section we will see how to use the GP framework in a very simple way to make inferences about functions given some training examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Posterior Gaussian Process</head><p>In the previous section we saw how to define distributions over functions using GPs. This GP will be used as a prior for Bayesian inference. The prior does not depend on the training data, but specifies some properties of the functions; for example, in Figure <ref type="figure" target="#fig_0">1</ref> the function is smooth, and close to a quadratic. The goal of this section is to derive the simple rules of how to update this prior in the light of the training data. The goal of the next section is to attempt to learn about some properties of the prior<ref type="foot" target="#foot_5">4</ref> in the the light of the data.</p><p>One of the primary goals computing the posterior is that it can be used to make predictions for unseen test cases. Let f be the known function values of the training cases, and let f * be a set of function values corresponding to the test set inputs, X * . Again, we write out the joint distribution of everything we are interested in:</p><formula xml:id="formula_4">f f * ∼ N µ µ * , Σ Σ * Σ * Σ * * , (<label>5</label></formula><formula xml:id="formula_5">)</formula><p>where we've introduced the following shorthand: </p><formula xml:id="formula_6">µ = m(x i ), i = 1, . . . ,</formula><formula xml:id="formula_7">f * |f ∼ N µ * + Σ * Σ −1 (f − µ), Σ * * − Σ * Σ −1 Σ * . (<label>6</label></formula><formula xml:id="formula_8">)</formula><p>This is the posterior distribution for a specific set of test cases. It is easy to verify (by inspection) that the corresponding posterior process is:</p><formula xml:id="formula_9">f |D ∼ GP(m D , k D ), m D (x) = m(x) + Σ(X, x) Σ −1 (f − m) k D (x, x ) = k(x, x ) − Σ(X, x) Σ −1 Σ(X, x ),<label>(7)</label></formula><p>where Σ(X, x) is a vector of covariances between every training case and x. These are the central equations for Gaussian process predictions. Let's examine these equations for the posterior mean and covariance. Notice that the posterior variance k D (x, x) is equal to the prior variance k(x, x) minus a positive term, which depends on the training inputs; thus the posterior variance is always smaller than the prior variance, since the data has given us some additional information.</p><p>We need to address one final issue: noise in the training outputs. It is common to many applications of regression that there is noise in the observations<ref type="foot" target="#foot_7">6</ref> . The most common assumption is that of additive i.i.d. Gaussian noise in the outputs.  <ref type="formula" target="#formula_2">3</ref>) and a noise level of σn = 0.7. The shaded area gives the 95% confidence region. Compare with Figure <ref type="figure" target="#fig_0">1</ref> and note that the uncertainty goes down close to the observations</p><formula xml:id="formula_10">x y ∼ N a b , A C C B =⇒ x|y ∼ N a + CB −1 (y − b), A − CB −1 C . −5 −4 −3 −2 −1 0 1 2<label>3</label></formula><p>In the Gaussian process models, such noise is easily taken into account; the effect is that every f (x) has a extra covariance with itself only (since the noise is assumed independent), with a magnitude equal to the noise variance:</p><formula xml:id="formula_11">y(x) = f (x) + ε, ε ∼ N(0, σ 2 n ), f ∼ GP(m, k), y ∼ GP(m, k + σ 2 n δ ii ),<label>(8)</label></formula><p>where δ ii = 1 iff i = i is the Kronecker's delta. Notice, that the indexes to the Kronecker's delta is the identify of the cases, i, and not the inputs x i ; you may have several cases with identical inputs, but the noise on these cases is assumed to be independent. Thus, the covariance function for a noisy process is the sum of the signal covariance and the noise covariance. Now, we can plug in the posterior covariance function into the little Matlab example on page 65 to draw samples from the posterior process, see Figure <ref type="figure" target="#fig_1">2</ref>. In this section we have shown how simple manipulations with mean and covariance functions allow updates of the prior to the posterior in the light of the training data. However, we left some questions unanswered: How do we come up with mean and covariance functions in the first place? How could we estimate the noise level? This is the topic of the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Training a Gaussian Process</head><p>In the previous section we saw how to update the prior Gaussian process in the light of training data. This is useful if we have enough prior information about a dataset at hand to confidently specify prior mean and covariance functions. However, the availability of such detailed prior information is not the typical case in machine learning applications. In order for the GP techniques to be of value in practice, we must be able to chose between different mean and covariance functions in the light of the data. This process will be referred to as training<ref type="foot" target="#foot_8">7</ref> the GP model.</p><p>In the light of typically vague prior information, we use a hierarchical prior, where the mean and covariance functions are parameterized in terms of hyperparameters. For example, we could use a generalization of Eq. ( <ref type="formula" target="#formula_1">2</ref>):</p><formula xml:id="formula_12">f ∼ GP(m, k), m(x) = ax 2 + bx + c, and k(x, x ) = σ 2 y exp − (x − x ) 2 2 2 + σ 2 n δ ii , (<label>9</label></formula><formula xml:id="formula_13">)</formula><p>where we have introduced hyperparameters θ = {a, b, c, σ y , σ n , }. The purpose of this hierarchical specification is that it allows us to specify vague prior information in a simple way. For example, we've stated that we believe the function to be close to a second order polynomial, but we haven't said exactly what the polynomial is, or exactly what is meant by "close". In fact the discrepancy between the polynomial and the data is a smooth function plus independent Gaussian noise, but again we're don't need exactly to specify the characteristic length scale or the magnitudes of the two contributions. We want to be able to make inferences about all of the hyperparameters in the light of the data.</p><p>In order to do this we compute the probability of the data given the hyperparameters. Fortunately, this is not difficult, since by assumption the distribution of the data is Gaussian:</p><formula xml:id="formula_14">L = log p(y|x, θ) = − 1 2 log |Σ| − 1 2 (y − µ) Σ −1 (y − µ) − n 2 log(2π). (<label>10</label></formula><formula xml:id="formula_15">)</formula><p>We will call this quantity the log marginal likelihood. We use the term "marginal" to emphasize that we are dealing with a non-parametric model. See e.g. <ref type="bibr" target="#b0">[1]</ref> for the weight-space view of Gaussian processes which equivalently leads to Eq. (10) after marginalization over the weights.</p><p>We can now find the values of the hyperparameters which optimizes the marginal likelihood based on its partial derivatives which are easily evaluated:</p><formula xml:id="formula_16">∂L ∂θ m = − (y − µ) Σ −1 ∂m ∂θ m , ∂L ∂θ k = 1 2 trace Σ −1 ∂Σ ∂θ k + 1 2 (y − µ) ∂Σ ∂θ k Σ −1 ∂Σ ∂θ k (y − µ),<label>(11)</label></formula><p>where θ m and θ k are used to indicate hyperparameters of the mean and covariance functions respectively. Eq. (11) can conveniently be used in conjunction</p><formula xml:id="formula_17">−5 −4 −3 −2 −1 0 1 2 3 4 5 −2 0 2 4 6 8</formula><p>Fig. <ref type="figure">3</ref>. Mean and 95% posterior confidence region with parameters learned by maximizing marginal likelihood, Eq. ( <ref type="formula" target="#formula_14">10</ref>), for the Gaussian process specification in Eq. ( <ref type="formula" target="#formula_12">9</ref>), for the same data as in Figure <ref type="figure" target="#fig_1">2</ref>. The hyperparameters found were a = 0.3, b = 0.03, c = −0.7, = 0.7, σy = 1.1, σn = 0.25. This example was constructed so that the approach without optimization of hyperparameters worked reasonably well (Figure <ref type="figure" target="#fig_1">2</ref>), but there is of course no guarantee of this in a typical application with a numerical optimization routine such as conjugate gradients to find good<ref type="foot" target="#foot_9">8</ref> hyperparameter settings.</p><p>Due to the fact that the Gaussian process is a non-parametric model, the marginal likelihood behaves somewhat differently to what one might expect from experience with parametric models. Note first, that it is in fact very easy for the model to fit the training data exactly: simply set the noise level σ 2 n to zero, and the model produce a mean predictive function which agrees exactly with the training points. However, this is not the typical behavior when optimizing the marginal likelihood. Indeed, the log marginal likelihood from Eq. (10) consists of three terms: The first term, − 1  2 log |Σ| is a complexity penalty term, which measures and penalizes the complexity of the model. The second term a negative quadratic, and plays the role of a data-fit measure (it is the only term which depends on the training set output values y). The third term is a log normalization term, independent of the data, and not very interesting. Figure <ref type="figure">3</ref> illustrates the predictions of a model trained by maximizing the marginal likelihood.</p><p>Note that the tradeoff between penalty and data-fit in the GP model is automatic. There is no weighting parameter which needs to be set by some external method such as cross validation. This is a feature of great practical importance, since it simplifies training. Figure <ref type="figure">4</ref> illustrates how the automatic tradeoff comes about.</p><p>We've seen in this section how we, via a hierarchical specification of the prior, can express prior knowledge in a convenient way, and how we can learn values of hyperparameters via optimization of the marginal likelihood. This can be done using some gradient based optimization. Also, we've seen how the marginal too simple too complex "just right" All possible data sets P(Y|M i ) Y Fig. <ref type="figure">4</ref>. Occam's razor is automatic. On the x-axis is an abstract representation of all possible datasets (of a particular size). On the y-axis the probability of the data given the model. Three different models are shown. A more complex model can account for many more data sets than a simple model, but since the probabilities have to integrate to unity, this means more complex models are automatically penalized more likelihood automatically incorporates Occam's razor; this property of of great practical importance, since it simplifies training a lot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions and Future Directions</head><p>We've seen how Gaussian processes can conveniently be used to specify very flexible non-linear regression. We only mentioned in passing one type of covariance function, but in fact any positive definite function<ref type="foot" target="#foot_10">9</ref> can be used as covariance function. Many such functions are known, and understanding the properties of functions drawn from GPs with particular covariance functions is an important ongoing research goal. When the properties of these functions are known, one will be able to chose covariance functions reflecting prior information, or alternatively, one will be able to interpret the covariance functions chosen by maximizing marginal likelihood, to get a better understanding of the data.</p><p>In this short tutorial, we have only treated the simplest possible case of regression with Gaussian noise. In the case of non-Gaussian likelihoods (such as e.g. needed for classification) training becomes more complicated. One can resort to approximations, such as the Laplace approximation <ref type="bibr" target="#b2">[3]</ref>, or approximations based on projecting the non-Gaussian posterior onto the closest Gaussian (in a KL sense) <ref type="bibr" target="#b3">[4]</ref> or sampling techniques <ref type="bibr" target="#b4">[5]</ref>.</p><p>Another issue is the computational limitations. A straightforward implementation of the simple techniques explained here, requires inversion of the covariance matrix Σ, with a memory complexity of O(n 2 ) and a computational complexity of O(n 3 ). This is feasible on a desktop computer for dataset sizes of n up to a few thousands. Although there are many interesting machine learning problems with such relatively small datasets, a lot of current work is going into the development of approximate methods for larger datasets. A number of these methods rely on sparse approximations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. Function values from three functions drawn at random from a GP as specified in Eq. (2). The dots are the values generated from Eq. (4), the two other curves have (less correctly) been drawn by connecting sampled points. The function values suggest a smooth underlying function; this is in fact a property of GPs with the squared exponential covariance function. The shaded grey area represent the 95% confidence intervals</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2.Three functions drawn at random from the posterior, given 20 training data points, the GP as specified in Eq. (3) and a noise level of σn = 0.7. The shaded area gives the 95% confidence region. Compare with Figure1and note that the uncertainty goes down close to the observations</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>n for the training means and analogously for the test means µ * ; for the covariance we use Σ for training set covariances, Σ * for training-test set covariances and Σ * * for test set covariances. Since we know the values for the training set f we are interested in the conditional distribution of f</figDesc><table /><note>* given f which is expressed as 5 :</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">By a parametric model, we here mean a model which during training "absorbs" the information from the training data into the parameters; after training the data can be discarded. O. Bousquet et al. (Eds.): Machine Learning</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2003" xml:id="foot_1">, LNAI 3176, pp. 63-71,</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2004" xml:id="foot_2">. c Springer-Verlag Berlin Heidelberg 2004</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_3">Matlab is a trademark of The MathWorks Inc.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_4">We've also added a tiny keps multiple of the identity to the covariance matrix for numerical stability (to bound the eigenvalues numerically away from zero); see comments around Eq. (8) for a interpretation of this term as a tiny amount of noise.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_5">By definition, the prior is independent of the data; here we'll be using a hierarchical prior with free parameters, and make inference about the parameters.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_6">the formula for conditioning a joint Gaussian distribution is:</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_7">However, it is perhaps interesting that the GP model works also in the noise-free case -this is in contrast to most parametric methods, since they often cannot model the data exactly.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_8">Training the GP model involves both model selection, or the discrete choice between different functional forms for mean and covariance functions as well as adaptation of the hyperparameters of these functions; for brevity we will only consider the latter here -the generalization is straightforward, in that marginal likelihoods can be compared.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_9">Note, that for most non-trivial Gaussian processes, optimization over hyperparameters is not a convex problem, so the usual precautions against bad local minima should be taken.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_10">The covariance function must be positive definite to ensure that the resulting covariance matrix is positive definite.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The author was supported by the German Research Council (DFG) through grant RA 1030/1.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Prediction with Gaussian processes: From linear regression to linear prediction and beyond</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning in Graphical Models</title>
				<editor>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="599" to="621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Gaussian processes -a replacement for supervised neural networks? Tutorial lecture notes for NIPS</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J C</forename><surname>Mackay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bayesian classification with Gaussian processes</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Barber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1342" to="1351" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sparse on-line Gaussian processes</title>
		<author>
			<persName><forename type="first">L</forename><surname>Csató</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Opper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="641" to="668" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Regression and classification using Gaussian process priors (with discussion)</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Bayesian statistics 6</title>
				<editor>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Bernardo</surname></persName>
		</editor>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="475" to="501" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
