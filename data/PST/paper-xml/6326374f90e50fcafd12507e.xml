<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SimNet: Accurate and High-Performance Computer Architecture Simulation using Deep Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-04-05">5 Apr 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lingda</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Thomas</forename><surname>Flynn</surname></persName>
							<email>tflynn@bnl.gov</email>
						</author>
						<author>
							<persName><forename type="first">Hang</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Noel</forename><surname>Wheeler</surname></persName>
							<email>nwheeler@lps.umd.edu</email>
						</author>
						<author>
							<persName><forename type="first">Santosh</forename><surname>Pandey</surname></persName>
							<email>spande1@stevens.edu</email>
						</author>
						<author>
							<persName><forename type="first">Adolfy</forename><surname>Hoisie</surname></persName>
							<email>ahoisie@bnl.gov</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Brookhaven National Laboratory</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">SANTOSH PANDEY</orgName>
								<orgName type="institution" key="instit2">Stevens Institute of Technology</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Brookhaven National Laboratory</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Stevens Institute of Technology</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="laboratory">Laboratory for Physical Sciences</orgName>
								<orgName type="institution">ADOLFY HOISIE</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Brookhaven National Laboratory</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">Brookhaven National Laboratory</orgName>
								<address>
									<addrLine>Upton</addrLine>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="institution">Stevens Institute of Technology</orgName>
								<address>
									<settlement>Hoboken</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff8">
								<orgName type="institution">Thomas Flynn</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff9">
								<orgName type="institution">Brookhaven National Laboratory</orgName>
								<address>
									<addrLine>Upton</addrLine>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff10">
								<orgName type="institution">Stevens Institute of Technology</orgName>
								<address>
									<settlement>Hoboken, Noel Wheeler</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff11">
								<orgName type="laboratory">Laboratory for Physical Sciences</orgName>
								<address>
									<settlement>College Park</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff12">
								<orgName type="institution">Brookhaven National Laboratory</orgName>
								<address>
									<addrLine>Upton</addrLine>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SimNet: Accurate and High-Performance Computer Architecture Simulation using Deep Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-04-05">5 Apr 2022</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/nnnnnnn.nnnnnnn</idno>
					<idno type="arXiv">arXiv:2105.05821v3[cs.AR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS Concepts:</term>
					<term>Computing methodologies ‚Üí Discrete-event simulation; Neural networks computer architecture simulation</term>
					<term>deep learning</term>
					<term>GPU</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While cycle-accurate simulators are essential tools for architecture research, design, and development, their practicality is limited by an extremely long time-to-solution for realistic applications under investigation. This work describes a concerted effort, where machine learning (ML) is used to accelerate microarchitecture simulation. First, an ML-based instruction latency prediction framework that accounts for both static instruction properties and dynamic processor states is constructed. Then, a GPU-accelerated parallel simulator is implemented based on the proposed instruction latency predictor, and its simulation accuracy and throughput are validated and evaluated against a state-of-the-art simulator. Leveraging modern GPUs, the ML-based simulator outperforms traditional CPU-based simulators significantly.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Adopted extensively in computer architecture research and engineering, cycle-accurate discreteevent simulators (DES) enable new architectural ideas, as well as design space exploration. DES is composed of distinct modules that mimic the behavior of different hardware components. On certain events (e.g., advancing a cycle), these individual components and their interactions are simulated to imitate the behavior of processors. Unfortunately, DES is extremely computationally demanding, markedly diminishing its practicality and applicability at full system and application scales. Typical simulations using the state-of-the-art gem5 simulator <ref type="bibr" target="#b6">[7]</ref> execute at speeds of hundreds of kilo instructions per second on modern CPUs, about four to five orders of magnitude slower than native execution. In this context, it would require weeks or months to simulate a realistic application our knowledge, the proposed framework is the first of its kind and could set the stage for developing alternative tools for architecture researchers and engineers.</p><p>‚Ä¢ We also prototype a GPU-accelerated ML-based simulator (Section 3.3). It improves the simulation throughput up to 76√ó compared to its traditional counterpart. It also achieves a higher throughput per watt thanks to GPU's power efficiency advantage.</p><p>Related Work. Ithemal <ref type="bibr" target="#b28">[28]</ref> represents the closest related work to this effort, proposing a long shortterm memory (LSTM) model to predict the execution latency of static basic blocks. However, Ithemal has three major limitations: 1) it targets a simplified processor model without branch prediction and cache/memory hierarchies; 2) it can only predict the performance of basic blocks with a handful of instructions, while real-world simulation executes billions or trillions of instructions; and 3) it simulates instructions at a pace of thousands of instructions per second, which is significantly slower than traditional simulators. As a result, unlike SimNet, Ithemal can neither simulate realworld processors nor applications, and it is infeasible for realistic computer architecture simulation. Section 2.5 will quantitatively compare Ithemal with SimNet, and Section 6 will discuss other related works. Scope of Work. This paper focuses on simulating out-of-order superscalar CPUs, which employ technologies such as multi-issue, out-of-order scheduling, and speculative execution to exploit instruction-level parallelism. We posit the proposed simulation methodology also is applicable to other processor architectures, which usually are less challenging to simulate. In addition, we constrain the scope to the prediction of program performance and single-thread program simulation, leaving multiple-thread/program simulation for future work. Traditional simulators also produce additional metrics other than performance, such as energy consumption. While it is reasonable to assume the proposed method is applicable to such prediction as well, these metrics are not considered in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">ML-BASED LATENCY PREDICTION</head><p>Figure <ref type="figure" target="#fig_0">1</ref> shows the workflow of SimNet. SimNet is built around an ML-based instruction latency prediction framework (the dashed box in Figure <ref type="figure" target="#fig_0">1</ref>), and this section will describe its design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Factors that Determine Instruction Latency</head><p>Successful instruction latency prediction by an ML model is contingent upon capturing all factors that impact latency in its design and implementation. These factors can be summarized into three categories. Static Instruction Properties. These properties describe the basics of an instruction, including the operation types, source/destination registers, etc. They guide how an instruction is executed in a processor. For instance, the type of instruction determines its computation resource (e.g., function units; register files) and synchronization requirements (e.g., memory barriers). Dynamic Processor States. Besides its static properties, the latency of an instruction largely depends on the states of all processor components (e.g., register files; caches) at its execution time.</p><p>We refer to these states as contexts and further distill them into two categories. Instruction Context: Many contexts relate to other concurrently running instructions in the processor, referred to as instruction context in this paper. For example, whether the desired execution unit is available depends on whether there is a co-running instruction of the same type using it currently, and if a source register can be read immediately depends on whether the previous instruction that writes the same register has finished. We argue that instruction context can be determined given all concurrently running instructions, named context instructions in this paper. The processor capacity decides the maximal number of context instructions.</p><p>History Context: The remaining hardware contexts depend on events that happened in the longterm execution history. Cache, translation lookaside buffer (TLB), and branch predictor states belong to this category. For example, whether a memory load hits in the L2 cache depends on when the same cache line was last accessed, and branch prediction results hinge on the branch execution history. Traditional simulators employ lookup tables (e.g., cache tag array; branch target predictor) to keep track of such states. We refer to them as history context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Framework Formulation</head><p>With these impact factors, we are ready to build an instruction latency prediction framework. The framework aims to balance between two competing goals: to predict instruction latency accurately and swiftly. An ML-based instruction latency predictor (the green box in Figure <ref type="figure" target="#fig_0">1</ref>) is the center of the framework, which captures the impact of input features. Its inputs (yellow boxes) take into account the aforementioned impact factors. Table <ref type="table" target="#tab_0">1</ref> summarizes the input features, which are divided into three categories based on which impact factor they model as introduced below.</p><p>Modeling Static Instruction Properties. The top row of Table <ref type="table" target="#tab_0">1</ref> lists the static instruction properties used as the input features of the ML model, including 13 operation features and 14 source and destination register indices. They are well known to computer architects and can be extracted from the instruction encoding directly. Modeling Instruction Context. To account for the impact of concurrently running instructions, the key is to model their relationships with the to-be-predicted instruction. Such relationships include resource competition, register dependency, and memory dependency. We call these concurrently running instructions, context instructions. Formally, the context instructions are those instructions present in the processor when a particular instruction is about to be fetched. In theory, instructions issued after the current instruction also can influence its execution. However, these cases are rare, and we only include previous instructions for practicality. The middle row of Table <ref type="table" target="#tab_0">1</ref> shows input features per context instruction.</p><p>For resource competition and register dependency, it is sufficient to provide the static properties of context instructions (i.e., their operation features and register indices). The ML model is responsible for deducing the resource competition using their operation features and the register dependency by comparing register indices.</p><p>To model the memory dependency (including instruction fetch and data access), one solution is to provide memory access addresses as parts of input features and leave the rest work to the ML model. Unlike register indices, memory access addresses spread across a much wider range in a typical 64-bit address space. Thus, having addresses as input would slow down the ML model prediction speed. Instead, we extract the memory dependency by explicitly comparing the memory access addresses of the current instruction with those of context instructions and generate several memory dependency flags as input features. For example, we compare their program counters (PCs) to identify if they fall into the same instruction cache line. This PC dependency flag presumably helps with fetch latency prediction as instructions that share the same instruction cache line can be fetched together. Similarly, there are dependency flags to indicate if data accesses share the same address, cache line, and page.</p><p>In addition, we introduce several features to capture the temporal relationship between instructions. Particularly, we include the number of cycles it has stayed in the processor (i.e., residence latency), how long it takes to complete execution (i.e., execution latency), and the memory store latency (i.e., store latency) if applicable. The latter two are provided by the ML model output, which will be introduced shortly. The latency of context instructions is useful to predict the latency of the current one. For example, when an instruction follows a mispredicted branch, its fetch latency is decided by the execution latency of the branch. Modeling History Context through Simplified Simulation. History context reflects the hardware states that depend on long-term historical events, and it includes caches, TLBs, and branch predictors. It is impractical to either capture the history context within the ML model or directly have it as the input because it includes a huge amount of information. Considering a 2MB cache with 64B cache lines as an example, we will need ‚àº5B per cache line to store its address tag, etc. Totally, a 2MB cache requires storing at least 2MB √∑ 64B √ó 5B = 160KB of information to simulate it accurately. The total history-context-related information is much larger given all history context. It is prohibitively expensive and inefficient to let the ML model memorize such large amounts of information.</p><p>Fortunately, the majority of history context impacts can be captured using a small number of intermediate results. For a memory access, the cache/TLB level in which it gets hit roughly determines its latency. Similarly, whether or not a branch target is predicted correctly determines the impact of a branch prediction.</p><p>Therefore, we propose to simulate history context components explicitly to obtain these intermediate results (i.e., history context simulation), which are passed to the ML model as input features. History context simulation greatly alleviates the burden on ML models. As shown in the last row of Table <ref type="table" target="#tab_0">1</ref>, a branch misprediction flag is obtained for a branch instruction. An access level feature is used for each memory access to indicate which level of the cache/TLB hierarchy satisfies the request. All instructions require fetch access and fetch table walking levels, and load/store instructions need data access and data table walking levels, e.g., a load request that hits in the L2 cache has a level of 2. The numbers of cache writebacks generated are also included in input features to capture their impacts.</p><p>Note that obtaining these intermediate results mostly involves table lookups (e.g., cache tag array; branch direction predictor). Detailed structures, such as pipeline and miss status history register (MSHR), are not needed in the history context simulation. The impacts of these structures are captured by the ML model in SimNet. Therefore, the history context simulation is lightweight and has negligible impact on the overall performance. ML Model Input Summary. In total, each context instruction has 50 input features, and the tobe-predicted instruction has 47 input features. For alignment purposes, we pad the to-be-predicted instruction features with three zeros, to have an equal number of 50 features. Together, the ML model takes 50 √ó (# context instructions + 1) features as input. While it is common to adopt one-hot encoding for individual input features, we choose not to do so to favor smaller input size and faster prediction speed. ML Model Output. In Figure <ref type="figure" target="#fig_0">1</ref>, the ML model is designed to predict three types of latencies per instruction: fetch, execution, and store. Fetch latency represents how long an instruction needs to wait to enter the processor after the previous instruction is fetched. It is affected by both its instruction fetch request and context instructions (e.g., when it follows a mispredicted branch). Execution latency represents the time interval from when an instruction is fetched to when it finishes execution and is ready to retire from the reorder buffer (ROB). Note it is different from the ROB retire latency because the ROB retires instructions in order. For store instructions, they write memory after being retired from the ROB. The store latency is used to represent the latency from when a store instruction is fetched to when it completes memory write (i.e., when it is ready to retire from the store queue (SQ)). Section 3.1 will introduce how these latencies are used in SimNet simulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Neural Network Architecture</head><p>Given the input and output, we train various ML models to learn their connections and capture the architectural impact. Sequence-Oriented Models. The ML model input includes a sequence of instructions (i.e., to-bepredicted instruction and context instructions), similar to word sequences in the case of natural language processing (NLP). Therefore, a natural option is to apply models designed to process sequences, such as recurrent neural networks <ref type="bibr" target="#b38">[38]</ref>, LSTM <ref type="bibr" target="#b14">[15]</ref>, and Transformer <ref type="bibr" target="#b52">[51]</ref>, for instruction latency prediction. Ithemal <ref type="bibr" target="#b28">[28]</ref> follows this strategy and adopts LSTM to predict basic block latency. The main drawback of these models is they are more computational intensive, resulting in low simulation throughput. Deep Convolutional Neural Network (CNN) Models. Deep CNN models have shown great success in computer vision <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b49">48]</ref>, where convolution kernels learn and recognize the spatial relationship between pixels. In our instruction latency prediction setting, convolution can help learn the relationship among input instructions. CNNs are less computational demanding than sequence-oriented models and fully connected networks. Another benefit of CNN is it eases the training of deeper networks because significantly less parameters need to be learned. As will be demonstrated in Section 2.5, we choose CNNs for SimNet's instruction latency predictors due to their prediction accuracy and computation overhead advantages.</p><p>Figure <ref type="figure" target="#fig_1">2</ref> illustrates the proposed CNN architecture. Inst 0 represents the instruction to be predicted, and the ML model outputs ùêπ 0 , ùê∏ 0 , and ùëÜ 0 , which are its predicted fetch, execution, and store latencies, respectively. Without loss of generality, Figure <ref type="figure" target="#fig_1">2</ref> shows three context instructions, Inst 1,2,3 .</p><p>We organize input instructions in a one-dimensional (1D) array by their execution order and have their features as channels, per CNN terminology. As introduced in Section 2.2, every instruction includes 50 features. Using computer vision as an analogy, instructions correspond to pixels, except  they are 1D instead of two-dimensional, and instruction features correspond to pixel color channels. This input organization facilitates convolutional operations to reason the relationship between instructions. Again, it is analogous to reasoning the shape composed by pixels in computer vision.</p><p>We organize the convolutional layers in a hierarchical way, where the first layer captures the relationship between temporally adjacent instructions, and subsequent layers integrate the impact of further away instructions. In Figure <ref type="figure" target="#fig_1">2</ref>, the impact of Inst 1 to Inst 0 is captured in the first layer, and the impact of Inst 2 and Inst 3 is incorporated in the second layer. This hierarchical design prioritizes the impact of temporally closer context instructions while penalizing the influence of more distant instructions, and real processors follow the same principle. For instance, if a source register of Inst 0 is the destination register of both Inst 1 and Inst 3 , Inst 0 only has to wait for Inst 1 where a true read after write dependency exists.</p><p>In our default design, each convolution layer includes a convolution operation followed by an activation operation. An alternative is to use a residual block as shown at the bottom of Figure <ref type="figure" target="#fig_1">2</ref>, which facilitates to increase the depth of CNNs <ref type="bibr" target="#b13">[14]</ref>. In this work, we design a residual block architecture inspired by the state-of-the-art image recognition model, EfficientNet <ref type="bibr" target="#b48">[47,</ref><ref type="bibr" target="#b49">48]</ref>.</p><p>The output of the last convolutional layer is flattened then used as the input of two fully connected layers. At the end, the model outputs the predicted fetch, execution, and store latencies of Inst 0 . We adopt the commonly used rectified linear unit (ReLU) as the activation function of both the convolutional and fully connected layers.</p><p>Empirically, we find the following CNN design principles work well for instruction latency prediction. First, the inputs of different convolutional operations have no overlap in contrast to computer vision CNNs. For example, we do not convolve Inst 1 and Inst 2 in Figure <ref type="figure" target="#fig_1">2</ref>. In this way, the impact of a context instruction is integrated only once. Second, a convolution kernel size of 2 is always used to account for only two adjacent inputs, which reduces the complexity. Combined with the first principle, it means all convolutional layers have the uniform kernel and stride size, 2. Our experiments demonstrate that these principles work well across different architecture configurations. While an extensive neural architecture search <ref type="bibr" target="#b10">[11]</ref> could potentially find better architectures, it also means significant searching overhead and we leave it for future work. From Output to Latency. There are two ways to convert the ML model output to the latency prediction results. In a regression model, the model output is directly used as the predicted latency. One inherited issue for the regression latency prediction model is its inability to distinguish between small latency differences. The impact may be minor when a latency of 1000 cycles is predicted to be 1001 cycles, but the error could be significant for small latencies (e.g., 0 cycle predicted to be 1). Because the fetch latency is 0 or 1 cycle in most cases, this drawback is particularly critical for its prediction.</p><p>A classification model could help to better distinguish between close latency values, where every latency value corresponds to a class, and the ML model predicts which class has the largest probability. However, because the latency could be up to several thousands of cycles, a pure classification scheme will significantly increase the output size and, thus, the computational overhead. Another problem of a pure classification scheme is it is difficult to train such a model because large latency samples appear less frequently in the training set.</p><p>As it is quite expensive to have a class for each possible latency value, we propose a hybrid scheme which uses classification for latency that appears frequently and regression for others. Naturally, small latencies appear more frequently. Taking the fetch latency prediction as an example, we classify them into 10 classes in the hybrid scheme. Cycles 0 to 8 have dedicated classes (ùëê 0 , ..., ùëê 8 ), while another class is used to represent cycles that are larger than 8 (ùëê &gt;8 ). The proposed model outputs the probability of each class. It also outputs a direct prediction result ùëô as in the regression model. On a prediction, we first check which class has the largest probability. If it is one among ùëê 0 , ..., ùëê 8 , the corresponding latency is predicted. Otherwise, ùëô is used as the predicted latency. Similar procedures are used to predict execution and store latencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Dataset and Training</head><p>Data Acquisition. Due to their data-driven nature, acquiring a sufficient training dataset is necessary for the success of ML-based approaches. Fortunately, it is convenient to engage existing simulator infrastructures to acquire a dataset for standard supervised training.</p><p>We modify gem5 <ref type="bibr" target="#b6">[7]</ref> to dump instruction execution traces, which then are used to generate ML training/validation/testing datasets. In the modified gem5, each instruction is assigned with three timestamps to record its respective fetch, execution, and store latencies. While the fetch latency stamp is updated in the instruction fetch unit, the execution and store latency stamps are updated in the ROB and SQ, respectively. After all latencies of an instruction are recorded, gem5 dumps it to a trace file.</p><p>The instruction traces output by gem5 require several steps of processing before they can be used for ML. First, for each instruction, we find and associate its context instructions based on the timestamps to form a sample. Second, many samples may be alike because the same scenarios can appear repeatedly during the execution of benchmarks. We eliminate such duplication to reduce the dataset. Finally, we convert the dataset to the format used by the ML framework.</p><p>Table <ref type="table" target="#tab_2">2</ref> shows the processor configurations that ML models learn from. The default O3CPU resembles a classic superscalar CPU. We also train models to learn the Fujitsu A64FX CPU deployed in the current top-ranked supercomputer, Fugaku <ref type="bibr" target="#b42">[41,</ref><ref type="bibr" target="#b57">56]</ref>, which represents a state-of-the-art CPU. We obtain the official gem5 configurations of A64FX at <ref type="bibr" target="#b23">[23]</ref>, which is verified to have an average simulation error of 6.6% against the real processor. Both simulated processors support the  ARMv8 instruction set architecture (ISA), and benchmarks are compiled using gcc 8.2.0 under the O3 optimization level. The full system simulation mode of gem5 is employed with Linux kernel 4.15. We use the default O3CPU configuration for most of our experiments, while Section 4.1 will present the results for the A64FX configuration. Under the default O3CPU configuration, there are, at most, 110 context instructions. Therefore, the ML model input has 50 √ó (1 + 110) = 5550 features.</p><p>ARMv8 is a representative 64-bit reduced instruction set computer (RISC). It includes various integer, floating-point, branch/jump, load/store, vectorized floating-point/integer, Boolean logic instructions, etc. A trained SimNet model can predict the latency of all these instructions. We expect SimNet will be able to support future ISA extensions.</p><p>SimNet can also support other ISAs including complex instruction set computers (CISCs) such as x86. To directly predict the performance of CISC instructions, more input features (e.g., multiple data access levels) are required because they show more complex behaviors such as multiple memory accesses. Another possible approach to support CISCs is to decompose CISC instructions into RISC like macro instructions, similar to what contemporary CISC CPUs do. In this way, SimNet can be used to predict the latency of simpler RISC instructions, similar to ARM ones. Benchmark. Theoretically, any program can be run on the modified gem5 to collect the ML dataset, and we can acquire an unlimited amount of data. We choose to use the SPEC CPU 2017 <ref type="bibr" target="#b7">[8]</ref> benchmark suite in this paper because it is widely used in computer architecture simulation and includes a wide range of applications, which should lead to a sufficient coverage of instruction execution scenarios. We select the first four SPEC CPU 2017 benchmarks to generate the ML training/validation/testing dataset, which are shown in Table <ref type="table" target="#tab_4">3</ref>. The default test workloads are used for these four benchmarks, and one billion instructions are simulated from the beginning for each benchmark to collect the ML dataset. Totally, we obtain a dataset with 71 million samples, among which roughly 90% of them are dedicated for training, 5% for validation, and 5% for testing.</p><p>As will be introduced in Section 4, we use the reference workloads to verify the simulation accuracy of all 25 SPEC CPU 2017 benchmarks. The facts that 21 benchmarks of them do not appear in the ML dataset and the simulation accuracy is evaluated on different input workloads, allow us to evaluate the generalizability of SimNet. Our training code is built upon PyTorch 1.7.0 <ref type="bibr" target="#b32">[32]</ref>. The objective function ùêΩ is minimized using the Adam optimizer <ref type="bibr" target="#b22">[22]</ref>. We use a learning rate of 0.001 and no weight decay or momentum. Every model is trained for 200 epochs, and the validation set is used to select the model with the lowest loss. No hyperparameter tuning is performed when training for different architecture configurations to avoid extensive hyperparameter search overhead. Our ML training hardware platform is an NVIDIA DGX A100 system <ref type="bibr" target="#b0">[1]</ref>. It includes eight NVIDIA A100 GPUs connected  through NVLink 3.0 and NVSwitch, and each is equipped with 40GB HBM that supports 1.5 TB/sec peak bandwidth. Tensor cores in an A100 GPU enable a peak performance of 156 TFlops for Tensor Float 32 operations. The DGX A100 system's high computing and memory throughputs make it ideal for ML training and inference. Depending on its complexity, training a model takes 18 ‚àº 75 hours on this machine. Section 4.3 will discuss the training overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Model Evaluation</head><p>We evaluate an array of ML models for instruction latency prediction, and the middle part of Table <ref type="table" target="#tab_6">4</ref> compares their prediction accuracy. We represent an ML model using a combination of letters and numbers, where the prefix denotes the basic building block type and the suffix denotes the number of layers. Particularly, FC, C, RB, LSTM, and TX represent the fully connected layer, the conventional convolutional layer, the residual block depicted at the bottom of Figure <ref type="figure" target="#fig_1">2</ref>, the standard LSTM block, and the Transformer encoder layer <ref type="bibr" target="#b52">[51]</ref>, respectively. For example, C3 is composed of three conventional convolutional layers.</p><p>The prediction error of each latency type is defined as follows for the ùëñth entry of testing dataset:</p><formula xml:id="formula_0">ùê∏ = |ùëì ùúÉ (ùë• ùëñ )‚àíùë¶ ùëñ | ùë¶ ùëñ +1</formula><p>, where ùë• ùëñ is the input and ùë¶ ùëñ is the expected output. Note that we use ùë¶ ùëñ + 1 as the denominator instead of ùë¶ ùëñ because the ùë¶ ùëñ of fetch and store latencies (e.g., non stores) is often 0.</p><p>Table <ref type="table" target="#tab_6">4</ref> affords several observations. First, we note that the prediction error of CNNs improves with the number of layers, which demonstrates the necessity of a deep neural network. Particularly, RB7 with residual blocks achieves the best accuracy, while the simplest FC2 model's prediction error is an order of magnitude larger.</p><p>Second, the hybrid scheme helps reduce prediction errors, from 35% to 2.7% for fetch latency's error under C3, while barely increasing the computation complexity. We notice that the hybrid C3 model makes correct fetch latency predictions in 95% of cases. In comparison, the regression C3 model predicts 65% of fetch latency correctly, which demonstrates that classification is helpful to predict latencies with small values.</p><p>Third, Table <ref type="table" target="#tab_6">4</ref> also compares the computational overhead of various models. CNN models require 4 ‚àº 93 millions of multiplications per inference/prediction. Different models represent different trade-off points between accuracy and computation overhead. Although they seem to be higher than that of traditional simulators, these computations are performed very efficiently on modern accelerators, such as GPUs and TPUs. As a result, SimNet achieves significantly higher simulation throughputs as well as better power efficiency, as will be shown in Section 4.2.</p><p>Compared with CNN models, both LSTM and Transformer models show lower prediction accuracy while incurring much larger computational overhead. Transformer models are especially expensive due to the attention computation <ref type="bibr" target="#b52">[51]</ref>. Although there are spaces to improve their accuracy through the adoption of deeper and wider networks, doing so requires larger computation overhead. This demonstrates that CNNs are more efficient at instruction latency prediction under a constrained computation budget.</p><p>Comparison with Ithemal. We also compare SimNet with Ithemal <ref type="bibr" target="#b28">[28]</ref>, a state-of-the-art MLbased latency prediction approach. It is designed to predict the latency of a basic block for processors with ideal caches and branch predictors, i.e., all memory accesses hit in L1 caches, and branch latency is not considered. To make a meanful comparison, we enhance it with SimNet input features so that it considers the impact of realistic caches and branch predictors and can predict the latencies of instruction sequences that are much longer than basic blocks. The key difference between Ithemal and SimNet is that the former uses a fix number of previous instructions as the input, while the latter explicitly selects instructions that are active in the processor (i.e., context instructions) as its input and excludes those that have retired. We train two LSTM models using the Ithemal approach, and the last two rows of Table <ref type="table" target="#tab_6">4</ref> show their results. The LSTM4 model is similar to what Ithemal originally uses, and we also include a 2-layer LSTM. The same training dataset and process as SimNet is adopted to ensure fairness. We find LSTM4 does not always perform better than LSTM2 due to the fading gradient problem that is common for deep LSTM training.</p><p>We observe that SimNet's prediction errors are one order of magnitude lower than those of Ithemal. SimNet models also incur lower computation overhead. These results demonstrate that explicitly constructing context instructions significantly improves instruction latency prediction accuracy, which is a key contribution of SimNet. This conclusion is intuitive because the input can better reflect the processor status when excluding retired instructions, which simplifies the job of ML models and results in higher accuracy. Note that the LSTM2 models of SimNet and Ithemal have the same architecture. SimNet's LSTM2 has a lower computation intensity because SimNet has less instructions as input. The fact that SimNet's LSTM2 has significantly better prediction accuracy than Ithemal's LSTM2 further demonstrates the effectiveness of SimNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ML-BASED SIMULATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">From Instruction Latency to Program Performance</head><p>SimNet simulates the program performance using the ML-based instruction latency predictor introduced in Section 2. Figure <ref type="figure" target="#fig_3">3</ref> illustrates how to calculate program execution time using instruction latencies, leveraging the fact that instruction fetch and instruction retire from ROB and SQ happen in order. Note that the fetch latency could be 0 in cases when multiple instructions are fetched together (e.g., Inst 4 ). We observe the execution time E of a program can be computed as</p><formula xml:id="formula_1">E = ( ùëõ ‚àëÔ∏Å ùëñ=1 ùêπ ùëñ ) + Œî, (<label>1</label></formula><formula xml:id="formula_2">)</formula><p>where ùëõ is the total number of simulated instructions, ùêπ ùëñ represents the fetch latency of the ùëñth instruction, and Œî is the amount of time from when the last instruction is fetched to when all instructions exit the processor. When ùëõ is large enough, the total execution time is dominated by the accumulated fetch latencies, and Œî is negligible. This equation lays down the foundation for the proposed instruction-centric simulator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Simulator Implementation</head><p>Based on Equation <ref type="formula" target="#formula_1">1</ref>, we develop a trace-driven simulator. The simulator goes through every executed instruction instance to predict its latency and outputs the program performance upon completion. The modified gem5 is used to generate input traces, which include instruction properties extracted by functional simulation, and history context simulation results. Context Management. As introduced in Section 2.2, the ML predictor requires the features of context instructions as part of its input. Therefore, SimNet needs to keep track of context instructions. For example, in Figure <ref type="figure" target="#fig_3">3</ref>, when Inst 6 is about to be fetched (vertical black-dotted line), Inst 1 has retired, and Inst 2‚àº5 are still in the processor based on their execution and store latencies. Therefore, Inst 2‚àº5 are the context instructions of Inst 6 . To this end, we employ two first-in-first-out (FIFO) queues, processor queue and memory write queue, to keep track of context instructions that stay in the processor and their features. They roughly correspond to the ROB and SQ in an outof-order processor but are not exactly the same. The two major differences are that the processor queue includes instructions in the frontend, while ROB does not, and a store instruction enters the memory write queue after it retires from the processor queue.</p><p>After the simulator reads one instruction from the input trace, the ML predictor is invoked to predict its latency. Then, it enters the processor queue with the residence latency initialized to 0. When it retires from the processor queue is determined based on its predicted execution latency and other simulation constraints (e.g., it must obey the in-order retirement and retire bandwidth). A non-store instruction exits the simulator when it retires from the processor queue. For a store instruction, it will enter the memory write queue. Similarly, when an instruction retires from the memory write queue is decided based on its predicted store latency, and it will exit the processor at that time. Much like real processors, the retire bandwidth of a processor queue is set according to that of the ROB, and the memory write queue can retire any number of instructions from its tail. Clock Management. The simulator employs curTick to record the total number of simulation cycles, which is updated whenever a prediction completes. When the predicted fetch latency is larger than 0, it is added to curTick so that the counter always points to the time when the current instruction enters the processor. In this case, we also increase the residence latency of all context instructions by the predicted fetch latency to update the time that they have remained in the processor. When the residence latency of an instruction is larger than its execution latency, it is ready to retire from the processor queue. Similarly, an instruction is ready to retire from the memory write queue when its residence latency exceeds its store latency.</p><p>After the last instruction in the input trace is predicted, we continue advancing curTick until all instructions retire from the simulator. The final value of curTick represents the total execution time of the program, which is exactly the same as Equation <ref type="formula" target="#formula_1">1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sub-Trace0</head><p>Instruction Trace</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Build ML Input</head><p>TensorRT Inference</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Clock management</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CPU GPU</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Batch</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Read instructions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sub-Trace1</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sub-Trace2</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Read instructions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Read instructions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-GPU CPU Multi-threading</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context management</head><p>Fig. <ref type="figure">4</ref>. Parallel simulation framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">GPU-accelerated Parallel Simulation</head><p>In our ML-based instruction latency predictor, the latency of an instruction depends on the predicted latencies of previous instructions, i.e., the latency prediction of adjacent instructions is inherently sequential. This restriction limits the sequential simulation speed and computational resource utilization. As a result, a sequential implementation of SimNet runs at a throughput of ‚àº 1k instructions per second, and it can only leverage a very small fraction of modern GPU's computing power. To improve the simulation throughput and resource utilization, we seek to extract parallelism. Parallel Simulation of Sub-traces. The primary idea is to break down the input instruction trace into multiple, equally sized continuous sub-traces and simulate sub-traces independently in parallel. The instructions within each sub-trace are simulated sequentially to preserve the instruction dependency within the specific sub-trace. The drawback of this approach is that extra simulation errors are introduced when simulating earlier instructions of a sub-trace due to inaccurate or missing contexts. Section 4.2 will show that such accuracy loss is negligible when each sub-trace is large enough. Figure <ref type="figure">4</ref> shows the overview of parallel simulation. The design leverages CPU multi-threading to partition the input trace and transfer sub-trace instructions to the GPU memory. The remaining work is done by GPUs to capitalize on their high computational capacity and reduce communications between CPUs and GPUs. GPU Acceleration. Both context and clock management are implemented on GPUs, and each sub-trace has separate copies of them. Particularly, each sub-trace has its own processor queue and memory write queue, as well as a curTick counter to record its number of simulated cycles. After the ML model input is built independently for each sub-trace, we combine them into a single input to allow GPU-batched inferences. This process repeats until all instructions in a sub-trace are simulated. After all sub-traces complete their simulation, we sum up their curTicks to get the total execution time. For ML model inferences, we use TensorRT <ref type="bibr" target="#b51">[50]</ref>, developed by NVIDIA for high-performance GPU deep learning inferences. It optimizes GPU memory allocations and supports reduced precision inferences. We adopt the TF32 and FP16 formats for ML inferences in this paper, and expect SimNet can benefit from the use of lower precisions when their supports become more mature in TensorRT. Compared with PyTorch, TensorRT provides roughly 3√ó speedup. In addition, this design can be scaled to multiple GPUs, where each GPU is responsible for a fraction of sub-traces. No inter-GPU communication is required during the simulation process. Section 4.2 will offer a detailed evaluation of simulation throughput.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EVALUATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Simulation Accuracy Validation</head><p>Benchmark Simulation Accuracy. We conduct simulation experiments on our training platform: the NVIDIA DGX A100 system equipped with eight A100 GPUs and an AMD EPYC 7742 64-core CPU. We simulate all 25 SPEC CPU 2017 SPECrate benchmarks using the reference workload. For each benchmark, SimPoint <ref type="bibr" target="#b46">[45]</ref> is used to select a representative sample of 100 million instructions. The right side of Table <ref type="table" target="#tab_6">4</ref> illustrates the simulation errors of various models compared with gem5. We use the absolute value of normalized cycle per instruction (CPI) difference to measure the simulation error for each benchmark: |ùê∂ùëÉùêº SimNet /ùê∂ùëÉùêº gem5 ‚àí 1| √ó 100%. Although models with lower instruction prediction errors have lower simulation errors in most cases, it is not always true. The reason is because previous prediction results are used to construct the input of latter predictions through the instruction context, which leads to a more complicated relationship between the predictor's and simulator's accuracy as will be discussed later.</p><p>Table <ref type="table" target="#tab_6">4</ref> shows the average simulation errors across three benchmark sets: benchmarks used in ML training (i.e., 4 ML benchmarks in Table <ref type="table" target="#tab_4">3</ref>), benchmarks not used in ML training (i.e., 21 simulation benchmarks in Table <ref type="table" target="#tab_4">3</ref>), and all of them. Note that for benchmarks used in training, different input workloads (test vs. reference) and simulation segments (beginning vs. SimPoint selected) are used in their simulation. We observe that the average errors of simulation workloads are not necessarily larger than those of training benchmarks, and the formers are smaller for several models. It demonstrates SimNet's ability to simulate unseen benchmarks. SimNet's generalizability roots in the fact that its predictor is trained at the instruction level.</p><p>Among SimNet models, the deepest CNN model RB7 achieves the lowest average simulation error of 5.6%. The shallower CNN model C3 also achieves good accuracy with a significantly low computation cost. Therefore, we focus on C3 and RB7 in the following experiments. On the other hand, LSTM and Transformer models achieve comparable simulation accuracy at a cost of one order of magnitude more computation overhead. Compared with Ithemal models, SimNet ones have significantly lower errors, which again demonstrates SimNet's effectiveness by constructing context explicitly.</p><p>Figure <ref type="figure" target="#fig_5">5</ref> further compares the simulated CPIs of gem5, the most accurate Ithemal model LSTM4, and representative SimNet models per benchmark. While Ithemal incurs significant errors for several benchmarks, SimNet models accurately simulates most benchmarks whose CPIs spread across a wide spectrum. Among them, RB7 achieves the best simulation accuracy where only 1 out of 25 benchmarks has an absolute error &gt; 10% (22% for imagick). Phase Level Accuracy. To verify the simulation accuracy with respect to execution phases, Figure <ref type="figure" target="#fig_6">6</ref> studies the CPI variation under C3 and RB7 models for all 25 benchmarks. Particularly, we calculate the average CPI every 1 million instructions and plot these CPIs over the total simulation length of 100 million instructions. As observed in Figure <ref type="figure" target="#fig_6">6</ref>, benchmarks have either steady curves (e.g., povray, leela), high CPI variations (e.g., perlbench, gcc), phased behaviors (e.g., bwaves, specrand), or mixes of them.</p><p>For most benchmarks, we observe that SimNet's CPI curves almost perfectly match those of gem5, especially those using RB7 (i.e., red dotted lines are always close to 0). This phenomenon happens to many highly variable benchmarks such as xalancbmk, which demonstrates SimNet's ability to capture small CPI variations during simulation. For cam4 where C3 has the largest simulation error (see Figure <ref type="figure" target="#fig_5">5</ref>), a consistent error persists across most simulation periods, while RB7 still has a CPI curve that resembles that of gem5. These results show that SimNet not only can predict the overall performance well, but it also generates insights, such as identifying execution phases and performance bottlenecks.</p><p>We also observe that a period of inaccurate simulation does not necessarily affect the simulation accuracy of the time periods that follow. For instance, C3's simulation errors reduce to 0 for 3 short periods when gem5's CPIs increase for cam4. Another example is cactuBSSN, where C3 fails to simulate it from 20M to 50M accurately, but has an almost identical CPI curve to that of gem5 after 50M.</p><p>This observation is counter-intuitive at first glance. Because previous prediction results are used to construct the input of latter predictions through the instruction context, it is reasonable to expect the prediction errors will propagate through the simulation. We discover two reasons behind it. First, the processor pipeline is emptied every once in a while due to events such as branch misprediciton during simulation. Upon these events, there are no context instructions, and the predicted latency does not rely on previous prediction results. As a result, the latency is easier to predict by SimNet models and thus the simulation accuracy gets calibrated on these events. Second, a well-trained SimNet model can self correct its errors throughout the simulation because such self-correction appears in the training data generated from real processors' behaviors. For example, assume one instruction ùêº takes longer than it should and prevents the next instruction ùêº ùëõ from entering the processor earlier. When ùêº ùëõ enters, the processor pipeline is emptier than it should be, which results in faster execution of ùêº ùëõ . In such a scenario, ùêº is executed slower, while ùêº ùëõ is executed faster. Together, the total execution time calibrates towards the right direction. These   reasons prevent the prediction error from propagating, and thus ensure SimNet's accuracy during long simulation. Accuracy Against Hardware. When there is an actual hardware that a simulator intends to simulate, the simulator accuracy can be validated against the hardware. For this purpose, we evaluate the accuracy of SimNet under the gem5 A64FX configuration. The gem5 simulaton of A64FX is verfied to have an average absolute error of 6.6% against the real A64FX processor across a set of benchmarks <ref type="bibr" target="#b23">[23]</ref>. Since their simulated benchmarks do not include SPEC CPU 2017 benchmarks, we cannot directly calculate the simulation accuracy of SimNet against the A64FX processor. Instead, we deduce the accuracy of SimNet against A64FX as follows. Under a reasonable assumption that the normalized CPI follows a normal/Gaussian distribution, we get the following distributions from SimNet results and <ref type="bibr" target="#b23">[23]</ref>: ùê∂ùëÉùêº SimNet /ùê∂ùëÉùêº gem5 ‚àº N (1.062, 0.016 2 ), ùê∂ùëÉùêº gem5 /ùê∂ùëÉùêº A64FX ‚àº N (1.013, 0.078 2 ). Their product, ùê∂ùëÉùêº SimNet /ùê∂ùëÉùêº A64FX , which represents the accuracy of SimNet against A64FX, has a mean of 1.060 and a standard variance of 0.016 <ref type="bibr" target="#b47">[46]</ref>. The expected average absoluate simulation error is 6.0% under this distribution, similar to that of gem5.</p><p>To give more contexts, a simulator is usually considered to be accurate if simulation errors are around 10%. For example, ZSim reports an average error of 9.7% against an Intel Westmere CPU <ref type="bibr" target="#b39">[39]</ref>, and <ref type="bibr" target="#b12">[13]</ref> reports a 13% error of gem5 against an ARM Cortex-A15 system. Although we cannot directly validate the accuracy of SimNet against A64FX, the deduced average absoluate simulation error is similar to that of gem5 that it learns from. We contend the low simulation error of SimNet is sufficient to gain confidence about its simulation results. Relative Accuracy. While the simulation accuracy against real hardware is a useful metrics, simulators are often applied in design space exploration where no corresponding hardware exists for verification. In these cases, computer architects care more about the "relative" simulation accuracy, which measures how accurate simulation results reflect the performance variance under certain architecture changes. For instance, how much the performance will improve with doubled cache sizes. Section 5 will demonstrate that SimNet achieves excellent relative accuracies using several case studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Parallel Simulation</head><p>Accuracy. When simulating a single benchmark, because the parallel simulator partitions the input trace into multiple sub-traces, there is simulation accuracy loss across sub-trace boundaries. Figure <ref type="figure" target="#fig_7">7</ref> studies how the overall simulation accuracy varies with the number of instructions per sub-trace. RB7 cannot have sub-traces that are smaller than 12k instructions because the GPU memory cannot accommodate too many sub-traces. As the results show, sub-traces of 3k instructions are sufficient to achieve parallel simulation errors that are similar to sequential ones. The parallel simulation errors vary in a small range with sub-traces of different sizes (around 8% for C3 and 5% for RB7), which demonstrates the reliability of parallel SimNet. Throughput. We evaluate the simulation throughput of parallel SimNet in terms of million instructions per second (MIPS). Figure <ref type="figure" target="#fig_8">8</ref> evaluates the average throughput across all benchmarks with various numbers of sub-traces using the same models. The x and y axes are on the logarithmic scale. Limited by the GPU memory capacity, we cannot evaluate C3 beyond 32k sub-traces or RB7 beyond 8k sub-traces. We observe that the simulation throughputs improve almost linearly when increasing the number of sub-traces, because more sub-traces allow SimNet to utilize both CPU and GPU resources more efficiently until it saturates them.</p><p>Figure <ref type="figure" target="#fig_9">9</ref> assesses the throughput scalability of SimNet with multiple GPUs, where the horizontal black-dotted line marks the gem5 simulation throughput. As ML inferences take a significant portion of time in SimNet, using multiple GPUs improves both the inference and simulation throughputs. Again, SimNet achieves near-linear speedup with the number of GPUs. With eight GPUs, it achieves 15.1 and 1.4 MIPS with the C3 and RB7 models. Correspondingly, this represents 76.2√ó and 7.4√ó improvement over gem5. We can further scale SimNet to distributed GPU systems easily for higher throughputs, because very limited communication is involved.</p><p>Note that we evaluate the throughput when simulating a single benchmark above. In practical simulation scenarios, computer architects usually need to simulate many benchmarks as well as different configurations. Our design of SimNet can naturally simulate different benchmarks and configurations in parallel, which provides even more opportunities to exploit parallelism. Comparison with CPU-based Parallel Simulation. Previous CPU-based simulators can make use of multi-core CPUs to simulate multiple programs/threads in parallel <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b39">39]</ref>. However, they cannot simulate a single program/thread in parallel and their parallelism is limited by the number of cores (dozens on modern CPUs). In comparison, GPU-based SimNet is able to simulate tens of thousands of traces in parallel on one GPU as shown in Figure <ref type="figure" target="#fig_8">8</ref>. These traces can come from a single or multiple programs/threads. As discussed below, such massive parallel simulation of SimNet benefits not only simulation performance, but also power efficiency. Power Efficiency. GPU-based SimNet can also achieve higher or similar simulation throughputs given a certain power/energy budget compared with traditional CPU-based simulators. On our experimental platform, SimNet has a simulation power efficiency of 4.7 and 0.44 KIPS/watt for C3 and RB7, while that of gem5 is 0.88 KIPS/watt. C3 is the most power efficient model while having acceptable simulation accuracy. While an A100 GPU has a TDP of 400 watts, we expect that SimNet's power efficiency can be further improved using consumer grade GPUs such as NVIDIA GeForce series or ASIC ML accelerators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Overhead Discussion</head><p>Training Overhead.   overhead amortizes with the increasing number of simulated instructions. The overall throughputs of SimNet exceed that of gem5 by 24 and 59 billion instructions for C3 and RB7, and approach their ideal throughputs with zero training overhead at trillions of instructions. To put it into context, a typical SPEC CPU 2017 benchmark executes more than one trillion instructions using the reference workload, and computer architects typically need to simulate dozens of benchmarks under hundreds of configurations, which means quadrillions of instructions. Even with the help of statistical simulation tools such as SimPoint, simulating trillions of instructions is still needed assuming the common practice of simulating at least 100 million ‚àº 1 billion instructions per benchmark. The training overhead of SimNet is negligible in these use cases. It is also worth noting that the training and simulation of SimNet can be trivially scaled to large distributed systems, which will further reduce the training overhead. Functional and History Context Simulation Overhead. Functional simulation can be accomplished using fast instruction set simulators/emulators such as QEMU <ref type="bibr" target="#b5">[6]</ref>. History context simulation is also fast because it only requires simplified results, such as cache access levels, where simulating address tag comparison and replacement is sufficient. Our initial experiments and previous research show such simulation can be done at ‚àº 100 MIPS on a single CPU core <ref type="bibr" target="#b50">[49]</ref>, which is much larger than SimNet's simulation throughputs. These overheads are therefore negligible. Further acceleration of functional and history context simulation is possible with GPUs, and we leave it for future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Impact of Features</head><p>Figure <ref type="figure" target="#fig_13">11</ref> evaluates the contribution of each input feature to the output for C3 using the SHapley Additive exPlanation (SHAP) method <ref type="bibr" target="#b27">[27]</ref>. SHAP's goal is to explain the prediction of an instance by computing the contribution of each feature to the prediction. It computes Shapley values <ref type="bibr" target="#b45">[44]</ref> using the coalitional game theory. Shapley value is the average marginal contribution of a feature value across all possible coalitions. We take the average of absolute Shapley values on training samples for each feature to produce feature attribution scores. Figure <ref type="figure" target="#fig_13">11a and 11b</ref> summarize the attribution scores of to-be-predicted instructions and context instructions separately. We categorize the 50 features into latency, operation, register, and memory. Memory and operation features generally have more impacts on the prediction results. The most influential feature of to-be-predicted instructions is the fetch access level because the fetch latency depends on it. For context instructions, the branch misprediction flag has the largest attribution score as mispredicted branches need to flush the processor pipeline.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Impact of Training Dataset Size</head><p>We also generate a large ML training dataset using 15 SPEC CPU 2017 benchmarks instead of four. Our results show that using the large dataset reduces the average simulation error by 33% at a cost of 3√ó training time. While larger training datasets further improve accuracy, we conclude that the smaller dataset is enough to train accurate models and also requires less training time.</p><p>The reason why a small training dataset is sufficient is because most benchmarks use a variety of instructions which provide ample samples to train the instruction latency predictor. We expect reasonable ML prediction accuracy as long as there are adequate samples to cover enough instruction and context scenarios, and the results show that 4 benchmarks are enough to obtain sufficient scenarios. Benchmark selection for the training set is also not critical.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">USE SCENARIOS</head><p>SimNet can be applied in many computer architecture research and engineering scenarios. First, many recent computer architecture efforts focus on caches or branch predictors while other microarchitecture components are more sophisticated and less likely to be subjects of change. In such scenarios, pre-trained SimNet models can be directly applied as caches and branch predictors are modeled in history context simulation, and no additional training is required (i.e., the training overhead discussed in Section 4.3 does not exist.). Second, when studying other microarchitecture parameters (e.g., ROB size) or novel components, different parameter/configuration choices can be included in the input of the model, so training a single model is sufficient to study all variations. We illustrate both use scenarios below, where the first two cases do not require training, and the last case requires a one time training. Branch Predictor Study. We compare the simulated performance of two branch predictors using gem5 and SimNet, including a large bi-mode branch predictor (BiMode_l) and the recently proposed TAGE-SC-L <ref type="bibr" target="#b44">[43]</ref>. Their implementation in gem5 is used in SimNet's history context simulation to generate branch misprediction flags for ML models' input. Table <ref type="table" target="#tab_8">5</ref> shows the simulated average speedups across SPEC benchmarks, where the speedup is calculated against the performance of a baseline bi-mode branch predictor. We observe that the average speedups obtained using SimNet are similar to those using gem5. Moreover, the right side of Table <ref type="table" target="#tab_8">5</ref> shows the speedup error ranges of individual benchmarks compared with gem5 results. We observe that SimNet also predicts the speedups of individual benchmarks well, especially under RB7. L2 Cache Size Exploration. We also simulate the performance impact of L2 cache sizes using gem5 and SimNet. Similar to the branch predictor case, SimNet accurately simulates the relative speedup under cache sizes from 256 kB to 4 MB, and the average error against gem5 is 0.8%. ROB Size Exploration. In this experiment, the ML model input includes the ROB size as an additional feature to account for its impact. The training data are generated by running the same four SPEC benchmarks in gem5 under various ROB sizes. We train a C3 model to study the impact of ROB sizes. Again, the simulation results of SimNet and gem5 agree with each other. For example, the average performance improvement when increasing the number of ROB entries from 40 to 80 and 120 is 1.2% and 1.4% under gem5. Using SimNet, the corresponding speedups are 1.1% and 1.5%, which are very similar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORK</head><p>ML for Latency Prediction. Ithemal <ref type="bibr" target="#b28">[28]</ref> uses LSTM models to predict the execution latency of static basic blocks. The instructions within a block are fed into the model in the form of assembly, such as words in NLP. On top of Ithemal, DiffTune <ref type="bibr" target="#b37">[37]</ref> trains a differentiable ML performance model to configure the simulator parameters to closely resemble a target architecture. These methods pose limits as they do not consider dynamic execution behaviors, such as memory accesses and branches, which have significant impacts on program performance. They also target basic blocks with a limited number of instructions. As a result, they are not applicable to a computer architecture simulator that needs to simulate realistic processors and billions/trillions of instructions. ML for Application Performance Prediction. Ipek et al. propose using neural networks for application performance prediction <ref type="bibr" target="#b15">[16]</ref>. Meanwhile, Lee et al. formulate nonlinear regression models for performance and power prediction <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b26">26]</ref>. Eyerman et al. propose inferring unknown parameters of mechanistic performance models using regression, to balance between model accuracy and interpretability <ref type="bibr" target="#b11">[12]</ref>. Mosmodel <ref type="bibr" target="#b2">[3]</ref> is a multi-input polynomial model used for virtual memory research that can predict the program execution time given the page table walking statistics. Wu et al. use performance counters as the input of ML models to predict GPU performance and power <ref type="bibr" target="#b54">[53]</ref>. Nemirovsky et al. schedule threads based on ML-based performance models <ref type="bibr" target="#b29">[29]</ref>. Some approaches are proposed to predict a processor's performance/power based on those obtained on different types of processors <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b31">31]</ref> or with different ISAs <ref type="bibr" target="#b58">[57,</ref><ref type="bibr" target="#b59">58]</ref>.</p><p>While these works build performance models on a per-program/input basis, SimNet works at the instruction level. Therefore, these application-centric approaches require generating training data and retraining models when target applications change, and the overhead of doing so is significant. On the other hand, SimNet can directly simulate any application, making it much more flexible. ML for Other Architecture Research. In addition to the aforementioned uses, ML has been widely applied to many other computer architecture aspects, including microarchitecture design and energy/power optimization. These applications are summarized in <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b55">54]</ref>. Simulation with Statistical Sampling. Instead of simulating the entire program, statistical simulation selectively simulates representative sampling units and infers the overall performance from these sample simulation results statistically <ref type="bibr" target="#b9">[10]</ref>. SMARTS <ref type="bibr" target="#b53">[52,</ref><ref type="bibr" target="#b56">55]</ref> periodically switches between detailed and functional simulation to obtain an accurate CPI estimation with minimal detailed simulation.</p><p>SimPoint records the basic block execution frequencies of individual sampling units and those of the whole program to select representative ones with the aim that the selected samples capture the overall execution behaviors well <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b46">45]</ref>. Similarly, PinPoints uses dynamic binary instrumentation to find representative samples for X86 programs <ref type="bibr" target="#b34">[34]</ref>, and BarrierPoint applies sampling to multithreaded simulation <ref type="bibr" target="#b8">[9]</ref>. These methods require pre-analyzing the simulated program with a certain input, while our ML-based simulator can be applied directly to any program and input combination because of its instruction-centric approach.</p><p>One key challenge in statistical simulation is to keep track of the microarchitecture state between detailed simulation fractions, especially cache states. To simulate the cache behavior accurately in statistical simulation, Nikoleris et al. propose using Linux KVM to monitor the reuse distance of selected cache lines <ref type="bibr" target="#b30">[30]</ref>. Similarly, Sandberg et al. leverage hardware virtualization to fast-forward between samples, so different samples can be simulated in parallel <ref type="bibr" target="#b41">[40]</ref>. These statistical simulation approaches can be used together with SimNet to further accelerate the detailed simulation portions. As an example, Section 4 uses SimPoint and SimNet together. Traditional Simulation Acceleration. ZSim is an X86 simulator that supports many-core system simulation <ref type="bibr" target="#b39">[39]</ref>. It decouples the simulation of individual cores and resources shared across cores, as well as adopts a simplified core model. As a result, it achieves ‚àº 10 MIPS for single-thread workload simulation on an Intel Sandy Bridge 16-core processor. SST <ref type="bibr" target="#b17">[18]</ref> distributes the simulation of different components across Message Passing Interface (MPI) ranks to achieve parallel simulation. Field programmable gate array (FPGA)-based emulators run significantly faster than simulation software but require a huge amount of effort to develop and validate register-transfer level models <ref type="bibr" target="#b21">[21]</ref>. In comparison, our work accelerates simulation from a different angle to make the most of widely available ML accelerators, such as GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSIONS</head><p>This work proposes a new computer architecture simulation paradigm using ML. To the best of our knowledge, this effort is the first to demonstrate ML's applicability to full-fledged architecture simulation. This new methodology significantly improves simulation performance without sacrificing accuracy. In addition to discrete-event, analytical, or other statistical approaches to architectural simulation/modeling, we maintain this new class of simulators will become a useful, valuable addition to the architect's "bag-of-tools." We recognize several advantages of this new approach.</p><p>1) We demonstrate that ML-based simulators can predict overall performance accurately, and they also qualitatively capture architecture and application behaviors. 2) ML is intrinsically easier to parallelize than discrete-event simulation. Moreover, ML-based simulators capitalize on modern computing technology that is tailored for boosting ML performance. 3) ML-based simulators generalize well to a large spectrum of application workloads. In our approach, this stems from building them around an instruction-level latency predictor. Hence, the focus is on learning instruction behaviors rather than high-level program behaviors that are much more difficult to capture. 4)</p><p>The training data are easy and fast to obtain. Potential sources of training data are multiple, including simplified models of simulators, actual execution of code on existing systems, or historical performance data. Future Directions. We plan to investigate ML-based approaches that support multi-thread/program simulation as our next step. The key to supporting multi-thread/program simulation is to model communications. We describe two possible strategies as follows, 1) extending context instructions to include concurrently executed instructions from other threads/programs, and 2) training ML models to model the impact of shared resources (e.g., caches, memory).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. ML-based simulation workflow. The ML-based instruction latency predictor is shown in green, and its input and output are in yellow and orange, respectively (Section 2). Other simulator components are in blue (Section 3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Convolutional neural network architecture illustration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Training. We use the standard gradient-based optimization to train various models. Let {(ùë• ùëñ , ùë¶ ùëñ )} ùëõ ùëñ=1 represent the set of input and output pairs in a training set of ùëõ samples. Let ùëì ùúÉ represent a to-betrained model with parameters ùúÉ , and our goal is to find a particular ùúÉ that minimizes the training loss ùêΩ (ùúÉ ) = 1 ùëõ ùëõ ùëñ=1 ùêø(ùëì ùúÉ (ùë• ùëñ ), ùë¶ ùëñ ). When training the regression output, ùêø is the squared-error loss function. When training the classification output, ùêø is the cross-entropy loss function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. From instruction latency to program execution time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Proc. ACM Meas. Anal. Comput. Syst., Vol. 1, No. 1, Article 1. Publication date: June 2022. SimNet: Accurate and High-Performance Computer Architecture Simulation using Deep Learning 1:13</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>n e t p p w r f x a la n c b m k x 2 6 4 b le n d e r c a m 4 d e e p s je n g im a g ic k le e la n a b e x c h a n g e 2 fo t o n ik 3 d r o m s x z s p e c r a n d f s p e c r a n dFig. 5 .</head><label>5</label><figDesc>Fig. 5. Simulated benchmark CPIs for various approaches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. CPI variation during the simulation of 100 million instructions. The solid lines show simulated CPI curves of gem5 and SimNet models. The dotted lines show the simulation errors of SimNet models, calculated by subtracting the CPIs of SimNet models by those of gem5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Average parallel simulation errors with various sub-trace sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Simulation throughput with different sub-trace numbers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Simulation throughput with multiple GPUs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Overall simulation throughput under different instruction numbers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Figure 10 shows the overall throughputs of various SimNet models that considers both simulation and training time. It is calculated as # simulated instructions training time+simulation time . The training To-be-predicted instruction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Feature attribution scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>Proc. ACM Meas. Anal. Comput. Syst., Vol. 1, No. 1, Article 1. Publication date: June 2022. SimNet: Accurate and High-Performance Computer Architecture Simulation using Deep Learning 1:21</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Input features for various instruction latency impact factors.</figDesc><table><row><cell>Impact Factor</cell><cell>Features</cell></row><row><cell>Static properties</cell><cell>13 operation features (function type, direct/indirect branch, memory barrier, etc.); 14 register</cell></row><row><cell></cell><cell>indices (8 sources and 6 destinations)</cell></row><row><cell cols="2">Instruction context 27 static properties; 14 history context features; 1 residence latency; 1 execution latency; 1 store</cell></row><row><cell></cell><cell>latency; 5 memory dependency flags to indicate if it shares the same instruction/data address/cache</cell></row><row><cell></cell><cell>line/page with the current instruction</cell></row><row><cell>History context</cell><cell>1 branch misprediction flag; 1 fetch level; 3 fetch table walking levels; 2 fetch caused writebacks;</cell></row><row><cell></cell><cell>1 data access level; 3 data access table walking levels; 3 data access caused writebacks</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Proc. ACM Meas. Anal. Comput. Syst., Vol. 1, No. 1, Article 1. Publication date: June 2022.</figDesc><table><row><cell cols="12">SimNet: Accurate and High-Performance Computer Architecture Simulation using Deep Learning</cell><cell>1:7</cell></row><row><cell>Inst 0 Inst 1 Inst 2 Inst 3</cell><cell></cell><cell cols="2">1D Conv 1D Conv</cell><cell></cell><cell>1D Conv</cell><cell></cell><cell>Flatten</cell><cell></cell><cell>Fully connected layer</cell><cell>0 F 0 E 0 S</cell><cell></cell></row><row><cell cols="8">A residual block inspired by EfficientNet</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Expand conv</cell><cell>Batch norm</cell><cell>Sigmoid</cell><cell>Depth conv</cell><cell>Batch norm</cell><cell>Sigmoid</cell><cell>Squeeze &amp;</cell><cell>excitation</cell><cell>Sigmoid</cell><cell>Point conv</cell><cell>Batch norm</cell><cell>+</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Core 3-wide fetch, 8-wide out-of-order issue/commit, bi-mode branch predictor, 32-entry IQ, 40-entry ROB, 16-entry LQ, 16-entry SQ 8-wide fetch, 4-wide out-of-order issue/commit, bi-mode branch predictor, 48-entry IQ, 128-entry ROB, 40-entry LQ, Simulated processor configurations.</figDesc><table><row><cell cols="2">Parameter Default O3CPU</cell><cell>A64FX</cell></row><row><cell></cell><cell></cell><cell>24-entry SQ</cell></row><row><cell>L1 ICache</cell><cell>48KB, 3-way, LRU, 4 MSHRs</cell><cell>64KB, 4-way, LRU, 8 MSHRs</cell></row><row><cell>L1 DCache</cell><cell>32KB, 2-way, LRU, 16 MSHRs, 5-cycle latency</cell><cell>64KB, 4-way, LRU, 21 MSHRs, 8-cycle latency, 8-degree</cell></row><row><cell></cell><cell></cell><cell>stride prefetcher</cell></row><row><cell>I/DMMU</cell><cell>2-stage TLBs, 1KB 8-way TLB caches with 6 MSHRs</cell><cell>2-stage TLBs, 1KB 4-way TLB caches with 6 MSHRs</cell></row><row><cell>L2 Cache</cell><cell>1MB, 16-way, LRU, 32 MSHRs, 29-cycle latency</cell><cell>8MB 16-way, LRU, 64 MSHRs, 111-cycle latency</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Benchmarks for ML and simulation.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .</head><label>4</label><figDesc>Instruction latency prediction and program simulation accuracy of various ML models. Output indicates if it is a regression model (reg) or hybrid model with classification (hyb). Computation intensity measures the number of million floating point multiplications (MFlops) required for one inference.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 .</head><label>5</label><figDesc>Simulated speedups of various branch predictors.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">Proc. ACM Meas. Anal. Comput. Syst., Vol. 1, No. 1, Article 1. Publication date: June 2022.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We would like to thank the anonymous reviewers for their helpful comments and Sergey Blagodurov for shepherding this paper. This research was conducted at the Brookhaven National Laboratory, supported by the U.S. Department of Energy's Office of Science under Contract No. DE-SC0012704.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="https://www.nvidia.com/en-us/data-center/dgx-a100/" />
		<title level="m">DGX A100: Universal System for AI Infrastructure</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName><forename type="first">Mart√≠n</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th {USENIX} symposium on operating systems design and implementation</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Predicting execution times with partial simulations in virtual memory research: why and how</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Agbarya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Idan</forename><surname>Yaniv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jayneel</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Tsafrir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/ACM International Symposium on Microarchitecture (MICRO)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Global Online Event. to appear</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cross-Architecture Performance Prediction (XAPP) Using CPU Code to Predict GPU Performance</title>
		<author>
			<persName><forename type="first">Newsha</forename><surname>Ardalani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clint</forename><surname>Lestourgeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthikeyan</forename><surname>Sankaralingam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1145/2830772.2830780</idno>
		<ptr target="https://doi.org/10.1145/2830772.2830780" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th International Proc</title>
				<meeting>the 48th International Proc<address><addrLine>Waikiki, Hawaii; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2015-06">2015. June 2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="725" to="737" />
		</imprint>
	</monogr>
	<note>Symposium on Microarchitecture</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Predicting GPU Performance from CPU Runs Using Machine Learning</title>
		<author>
			<persName><forename type="first">I</forename><surname>Baldini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Fink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Altman</surname></persName>
		</author>
		<idno type="DOI">10.1109/SBAC-PAD.2014.30</idno>
		<ptr target="https://doi.org/10.1109/SBAC-PAD.2014.30" />
	</analytic>
	<monogr>
		<title level="m">2014 IEEE 26th International Symposium on Computer Architecture and High Performance Computing</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="254" to="261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">QEMU, a fast and portable dynamic translator</title>
		<author>
			<persName><forename type="first">Fabrice</forename><surname>Bellard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX annual technical conference</title>
				<meeting><address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page">46</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Binkert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bradford</forename><surname>Beckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">K</forename><surname>Reinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Saidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arkaprava</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Hestness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><forename type="middle">R</forename><surname>Hower</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somayeh</forename><surname>Sardashti</surname></persName>
		</author>
		<idno type="DOI">10.1145/2024716.2024718</idno>
		<ptr target="https://doi.org/10.1145/2024716.2024718" />
	</analytic>
	<monogr>
		<title level="j">The Gem5 Simulator. SIGARCH Comput. Archit. News</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2011-08">2011. Aug. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">SPEC CPU2017: Next-generation compute benchmark</title>
		<author>
			<persName><forename type="first">James</forename><surname>Bucek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus-Dieter</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J√≥akim</forename><forename type="middle">V</forename><surname>Kistowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Companion of the 2018 ACM/SPEC International Conference on Performance Engineering</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="41" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">BarrierPoint: Sampled simulation of multi-threaded applications</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Heirman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Van Craeynest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Eeckhout</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISPASS.2014.6844456</idno>
		<ptr target="https://doi.org/10.1109/ISPASS.2014.6844456" />
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Reducing state loss for effective trace sampling of superscalar processors</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Conte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Menezes</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCD.1996.563595</idno>
		<ptr target="https://doi.org/10.1109/ICCD.1996.563595" />
	</analytic>
	<monogr>
		<title level="m">Proceedings International Conference on Computer Design. VLSI in Computers and Processors</title>
				<meeting>International Conference on Computer Design. VLSI in Computers and Processors</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="468" to="477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neural architecture search: A survey</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Hendrik Metzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1997" to="2017" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mechanistic-empirical processor performance modeling for constructing CPI stacks on real hardware</title>
		<author>
			<persName><forename type="first">Stijn</forename><surname>Eyerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Hoste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lieven</forename><surname>Eeckhout</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISPASS.2011.5762738</idno>
		<ptr target="https://doi.org/10.1109/ISPASS.2011.5762738" />
	</analytic>
	<monogr>
		<title level="m">IEEE ISPASS) IEEE International Symposium on Performance Analysis of Systems and Software</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="216" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sources of error in full-system simulation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gutierrez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pusdesris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Dreslinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mudge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sudanthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Emmons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hayenga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Paver</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISPASS.2014.6844457</idno>
		<ptr target="https://doi.org/10.1109/ISPASS.2014.6844457" />
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="13" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J√ºrgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Efficiently Exploring Architectural Design Spaces via Predictive Modeling</title>
		<author>
			<persName><forename type="first">Engin</forename><surname>√èpek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sally</forename><forename type="middle">A</forename><surname>Mckee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bronis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>De Supinski</surname></persName>
		</author>
		<author>
			<persName><surname>Schulz</surname></persName>
		</author>
		<idno type="DOI">10.1145/1168857.1168882</idno>
		<ptr target="https://doi.org/10.1145/1168857.1168882" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Architectural Support for Programming Languages and Operating Systems</title>
				<meeting>the 12th International Conference on Architectural Support for Programming Languages and Operating Systems<address><addrLine>San Jose, California, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="195" to="206" />
		</imprint>
	</monogr>
	<note>ASPLOS XII)</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">CMP$im: A Pin-based on-the-fly multi-core cache simulator</title>
		<author>
			<persName><forename type="first">Aamer</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi-Keung</forename><surname>Robert S Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruce</forename><surname>Luk</surname></persName>
		</author>
		<author>
			<persName><surname>Jacob</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Annual Workshop on Modeling, Benchmarking and Simulation (MoBS), co-located with ISCA</title>
				<meeting>the Fourth Annual Workshop on Modeling, Benchmarking and Simulation (MoBS), co-located with ISCA</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="28" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A simulator for large-scale parallel computer architectures</title>
		<author>
			<persName><forename type="first">Helgi</forename><surname>Curtis L Janssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Adalsteinsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">P</forename><surname>Cranford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Kenny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Pinar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jackson</forename><surname>Evensky</surname></persName>
		</author>
		<author>
			<persName><surname>Mayo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Distributed Systems and Technologies (IJDST)</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="57" to="73" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pushing the Limit of Molecular Dynamics with Ab Initio Accuracy to 100 Million Atoms with Machine Learning</title>
		<author>
			<persName><forename type="first">Weile</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denghui</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Car</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Weinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linfeng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis</title>
				<meeting>the International Conference for High Performance Computing, Networking, Storage and Analysis<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>SC &apos;20. 14 pages</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Norman</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cliff</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nishant</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raminder</forename><surname>Bajwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suresh</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Boden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Al</forename><surname>Borchers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rick</forename><surname>Boyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-Luc</forename><surname>Cantin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clifford</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Coriell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Daley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Dau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Gelb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tara</forename><surname>Vazir Ghaemmaghami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajendra</forename><surname>Gottipati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Gulland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Hagmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Richard</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Hogberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Hundt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hurt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Jaffey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alek</forename><surname>Jaworski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harshit</forename><surname>Khaitan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Killebrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naveen</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Lacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Laudon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diemthu</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Lundin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gordon</forename><surname>Mackean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Maggiore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maire</forename><surname>Mahony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kieran</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Narayanaswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ray</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathy</forename><surname>Nix ; Amir Salek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emad</forename><surname>Samadiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Severn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Sizikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Snelham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jed</forename><surname>Souter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Steinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Swing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mercedes</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Thorson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horia</forename><surname>Toma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erick</forename><surname>Tuttle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Walter</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wilcox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doe</forename><surname>Hyun Yoon</surname></persName>
		</author>
		<idno type="DOI">10.1145/3079856.3080246</idno>
		<ptr target="https://doi.org/10.1145/3079856.3080246" />
		<title level="m">Proceedings of the 44th Annual International Symposium on Computer Architecture</title>
				<meeting>the 44th Annual International Symposium on Computer Architecture<address><addrLine>Thomas Norrie, Mark Omernick, Narayana Penukonda, Andy Phelps, Jonathan Ross, Matt Ross,; Toronto, ON, Canada; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
	<note>ISCA &apos;17)</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">SimNet: Accurate and High-Performance Computer Architecture Simulation using Deep Learning</title>
	</analytic>
	<monogr>
		<title level="j">Proc. ACM Meas. Anal. Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">23</biblScope>
			<date type="published" when="2022-06">June 2022</date>
		</imprint>
	</monogr>
	<note>Publication date</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Firesim: FPGA-Accelerated Cycle-Exact Scale-out System Simulation in the Public Cloud</title>
		<author>
			<persName><forename type="first">Sagar</forename><surname>Karandikar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Howard</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donggyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Biancolin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alon</forename><surname>Amid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dayeol</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Pemberton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Amaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qijing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Kovacs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Borivoje</forename><surname>Nikolic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Randy</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Bachrach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krste</forename><surname>Asanoviƒá</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISCA.2018.00014</idno>
		<ptr target="https://doi.org/10.1109/ISCA.2018.00014" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual International Symposium on Computer Architecture</title>
				<meeting>the 45th Annual International Symposium on Computer Architecture<address><addrLine>Los Angeles, California</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="29" to="42" />
		</imprint>
	</monogr>
	<note>ISCA &apos;18)</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">P</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<title level="m">Adam: A Method for Stochastic Optimization. international conference on learning representations</title>
				<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Yuetsu</forename><surname>Kodama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tetsuya</forename><surname>Odajima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akira</forename><surname>Asato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitsuhisa</forename><surname>Sato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.06451</idno>
		<title level="m">Evaluation of the riken post-k processor simulator</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Illustrative Design Space Studies with Microarchitectural Regression Models</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Brooks</surname></persName>
		</author>
		<idno type="DOI">10.1109/HPCA.2007.346211</idno>
		<ptr target="https://doi.org/10.1109/HPCA.2007.346211" />
	</analytic>
	<monogr>
		<title level="m">2007 IEEE 13th International Symposium on High Performance Computer Architecture</title>
				<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="340" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Methods of Inference and Learning for Performance Modeling of Parallel Applications</title>
		<author>
			<persName><forename type="first">Benjamin</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bronis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>De Supinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sally</forename><forename type="middle">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><surname>Mckee</surname></persName>
		</author>
		<idno type="DOI">10.1145/1229428.1229479</idno>
		<ptr target="https://doi.org/10.1145/1229428.1229479" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming</title>
				<meeting>the 12th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming<address><addrLine>San Jose, California, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="249" to="258" />
		</imprint>
	</monogr>
	<note>PPoPP &apos;07)</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A Unified Approach to Interpreting Model Predictions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Su-In</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="4765" to="4774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ithemal: Accurate, portable and fast basic block throughput estimation using deep neural networks</title>
		<author>
			<persName><forename type="first">Charith</forename><surname>Mendis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Renda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saman</forename><surname>Amarasinghe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Carbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4505" to="4515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A Machine Learning Approach for Performance Prediction and Scheduling on Heterogeneous CPUs</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Nemirovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tugberk</forename><surname>Arkose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikola</forename><surname>Markovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Nemirovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Osman</forename><surname>Unsal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Cristal</surname></persName>
		</author>
		<idno type="DOI">10.1109/SBAC-PAD.2017.23</idno>
		<ptr target="https://doi.org/10.1109/SBAC-PAD.2017.23" />
	</analytic>
	<monogr>
		<title level="m">2017 29th International Symposium on Computer Architecture and High Performance Computing (SBAC-PAD)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="121" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Directed Statistical Warming through Time Traveling</title>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Nikoleris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lieven</forename><surname>Eeckhout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Hagersten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><forename type="middle">E</forename><surname>Carlson</surname></persName>
		</author>
		<idno type="DOI">10.1145/3352460.3358264</idno>
		<ptr target="https://doi.org/10.1145/3352460.3358264" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture</title>
				<meeting>the 52nd Annual IEEE/ACM International Symposium on Microarchitecture<address><addrLine>Columbus, OH, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1037" to="1049" />
		</imprint>
	</monogr>
	<note>MICRO &apos;52)</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">GPU Performance Estimation Using Software Rasterization and Machine Learning</title>
		<author>
			<persName><forename type="first">O'</forename><surname>Kenneth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Brisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zack</forename><surname>Abousamra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Waters</surname></persName>
		</author>
		<author>
			<persName><surname>Shriver</surname></persName>
		</author>
		<idno type="DOI">10.1145/3126557</idno>
		<ptr target="https://doi.org/10.1145/3126557" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Embed. Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">148</biblScope>
			<date type="published" when="2017-09">2017. Sept. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">PyTorch: An Imperative Style, High-Performance Deep Learning Library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2019/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Alch√©-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">MARSS: A full system simulator for multicore x86 CPUs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Afram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ghose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">48th ACM/EDAC/IEEE Design Automation Conference (DAC)</title>
				<imprint>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="page" from="1050" to="1055" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pinpointing Representative Portions of Large Intel ¬Æ Itanium ¬Æ Programs with Dynamic Instrumentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Charney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karunanidhi</surname></persName>
		</author>
		<idno type="DOI">10.1109/MICRO.2004.28</idno>
		<ptr target="https://doi.org/10.1109/MICRO.2004.28" />
	</analytic>
	<monogr>
		<title level="m">37th International Symposium on Microarchitecture (MICRO-37&apos;04)</title>
				<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="81" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">A survey of machine learning applied to computer architecture design</title>
		<author>
			<persName><forename type="first">D</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizhong</forename><surname>Penney</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.12373</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Using SimPoint for Accurate and Efficient Simulation. SIGMETRICS Perform</title>
		<author>
			<persName><forename type="first">Erez</forename><surname>Perelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Hamerly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Van Biesbrouck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Sherwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brad</forename><surname>Calder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eval. Rev</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="318" to="319" />
			<date type="published" when="2003-06">2003. June 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">DiffTune: Optimizing CPU Simulator Parameters with Learned Differentiable Surrogates</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Renda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yishen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charith</forename><surname>Mendis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Carbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/ACM International Symposium on Microarchitecture</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>David E Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986">1986. 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">ZSim: Fast and Accurate Microarchitectural Simulation of Thousand-Core Systems</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Kozyrakis</surname></persName>
		</author>
		<idno type="DOI">10.1145/2508148.2485963</idno>
		<ptr target="https://doi.org/10.1145/2508148.2485963" />
	</analytic>
	<monogr>
		<title level="j">SIGARCH Comput. Archit. News</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="475" to="486" />
			<date type="published" when="2013-06">2013. June 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Proc. ACM Meas. Anal. Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2022-06">June 2022</date>
		</imprint>
	</monogr>
	<note>Publication date</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Full Speed Ahead: Detailed Architectural Simulation at Near-Native Speed</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sandberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nikoleris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hagersten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kaxiras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Black-Schaffer</surname></persName>
		</author>
		<idno type="DOI">10.1109/IISWC.2015.29</idno>
		<ptr target="https://doi.org/10.1109/IISWC.2015.29" />
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Symposium on Workload Characterization. 183-192</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Co-Design for A64FX Manycore Processor and &quot;Fugaku</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ishikawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tomita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kodama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Odajima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tsuji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yashiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aoki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Miyoshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hirai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Furuya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Asato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Morita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Shimizu</surname></persName>
		</author>
		<idno type="DOI">10.1109/SC41405.2020.00051</idno>
		<ptr target="https://doi.org/10.1109/SC41405.2020.00051" />
	</analytic>
	<monogr>
		<title level="m">SC20: International Conference for High Performance Computing, Networking, Storage and Analysis</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Improved protein structure prediction using potentials from deep learning</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Andrew W Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Jumper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongli</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Augustin</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><surname>≈Ω√≠dek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Alexander Wr Nelson</surname></persName>
		</author>
		<author>
			<persName><surname>Bridgland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">577</biblScope>
			<biblScope unit="page" from="706" to="710" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">TAGE-SC-L Branch Predictors Again</title>
		<author>
			<persName><forename type="first">Andr√©</forename><surname>Seznec</surname></persName>
		</author>
		<ptr target="https://hal.inria.fr/hal-01354253" />
	</analytic>
	<monogr>
		<title level="m">5th JILP Workshop on Computer Architecture Competitions (JWAC-5): Championship Branch Prediction (CBP-5)</title>
				<meeting><address><addrLine>Seoul, South Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A value for n-person games</title>
		<author>
			<persName><surname>Lloyd S Shapley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Contributions to the Theory of Games</title>
				<imprint>
			<date type="published" when="1953">1953. 1953</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="307" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Automatically Characterizing Large Scale Program Behavior</title>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Sherwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erez</forename><surname>Perelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Hamerly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brad</forename><surname>Calder</surname></persName>
		</author>
		<idno type="DOI">10.1145/605397.605403</idno>
		<ptr target="https://doi.org/10.1145/605397.605403" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Architectural Support for Programming Languages and Operating Systems</title>
				<meeting>the 10th International Conference on Architectural Support for Programming Languages and Operating Systems<address><addrLine>San Jose, California; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="45" to="57" />
		</imprint>
	</monogr>
	<note>ASPLOS X)</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Julius</forename><forename type="middle">O</forename><surname>Smith</surname></persName>
		</author>
		<ptr target="https://ccrma.stanford.edu/~jos/sasponlinebook" />
		<title level="m">Spectral Audio Signal Processing</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>2011 edition</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">MnasNet: Platform-Aware Neural Architecture Search for Mobile</title>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v97/tan19a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning (Proceedings of Machine Learning Research</title>
				<editor>
			<persName><forename type="first">Kamalika</forename><surname>Chaudhuri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</editor>
		<meeting>the 36th International Conference on Machine Learning ( Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Cache Simulation for Instruction Set Simulator QEMU</title>
		<author>
			<persName><forename type="first">Tran</forename><surname>Van Dung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ittetsu</forename><surname>Taniguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroyuki</forename><surname>Tomiyama</surname></persName>
		</author>
		<idno type="DOI">10.1109/DASC.2014.85</idno>
		<ptr target="https://doi.org/10.1109/DASC.2014.85" />
	</analytic>
	<monogr>
		<title level="m">2014 IEEE 12th International Conference on Dependable, Autonomic and Secure Computing</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="441" to="446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Efficient inference with tensorrt</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Vanholder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">≈Åukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">SimFlex: Statistical Sampling of Computer System Simulation</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Wunderlich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Hoe</surname></persName>
		</author>
		<idno type="DOI">10.1109/MM.2006.79</idno>
		<ptr target="https://doi.org/10.1109/MM.2006.79" />
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="31" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">GPGPU performance and power estimation using machine learning</title>
		<author>
			<persName><forename type="first">Gene</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">L</forename><surname>Greathouse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Lyashevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nuwan</forename><surname>Jayasena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Chiou</surname></persName>
		</author>
		<idno type="DOI">10.1109/HPCA.2015.7056063</idno>
		<ptr target="https://doi.org/10.1109/HPCA.2015.7056063" />
	</analytic>
	<monogr>
		<title level="m">2015 IEEE 21st International Symposium on High Performance Computer Architecture (HPCA)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="564" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">Nan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.07952</idno>
		<title level="m">A Survey of Machine Learning for Computer Architecture and Systems</title>
				<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">SMARTS: Accelerating Microarchitecture Simulation via Rigorous Statistical Sampling</title>
		<author>
			<persName><forename type="first">Roland</forename><forename type="middle">E</forename><surname>Wunderlich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Babak</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">C</forename><surname>Hoe</surname></persName>
		</author>
		<idno type="DOI">10.1145/859618.859629</idno>
		<ptr target="https://doi.org/10.1145/859618.859629" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th Annual International Symposium on Computer Architecture</title>
				<meeting>the 30th Annual International Symposium on Computer Architecture<address><addrLine>San Diego, California; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="84" to="97" />
		</imprint>
	</monogr>
	<note>ISCA &apos;03)</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Fujitsu high performance CPU for the Post-K Computer</title>
		<author>
			<persName><forename type="first">Toshio</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Hot Chips</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Accurate Phase-Level Cross-Platform Power and Performance Estimation</title>
		<author>
			<persName><forename type="first">Xinnian</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizy</forename><forename type="middle">K</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Gerstlauer</surname></persName>
		</author>
		<idno type="DOI">10.1145/2897937.2897977</idno>
		<ptr target="https://doi.org/10.1145/2897937.2897977" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Design Automation Conference</title>
				<meeting>the 53rd Annual Design Automation Conference<address><addrLine>Austin, Texas; New York, NY, USA, Article</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>DAC &apos;16)</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Learning-based analytical crossplatform performance prediction</title>
		<author>
			<persName><forename type="first">Xinnian</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizy</forename><forename type="middle">K</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Gerstlauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 International Conference on Embedded Computer Systems: Architectures, Modeling, and Simulation (SAMOS)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="52" to="59" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
