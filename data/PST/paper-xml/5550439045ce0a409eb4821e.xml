<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Non-rigid visible and infrared face registration via regularized Gaussian fields criterion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2014-09-21">21 September 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jiayi</forename><surname>Ma</surname></persName>
							<email>jiayima@whu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Electronic Information School</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<postCode>430072</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ji</forename><surname>Zhao</surname></persName>
							<email>zhaoji84@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<postCode>430074</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yong</forename><surname>Ma</surname></persName>
							<email>mayong@whu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Electronic Information School</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<postCode>430072</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jinwen</forename><surname>Tian</surname></persName>
							<email>jwtian@hust.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<postCode>430074</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Non-rigid visible and infrared face registration via regularized Gaussian fields criterion</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2014-09-21">21 September 2014</date>
						</imprint>
					</monogr>
					<idno type="MD5">40EAB7E0B7A6620EDCD3782CFD596C69</idno>
					<idno type="DOI">10.1016/j.patcog.2014.09.005</idno>
					<note type="submission">Received 26 June 2014 Received in revised form 15 August 2014 Accepted 8 September 2014</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Registration Image fusion Infrared Non-rigid Face recognition Gaussian fields</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Registration of multi-sensor data (particularly visible color sensors and infrared sensors) is a prerequisite for multimodal image analysis such as image fusion. Typically, the relationships between image pairs are modeled by rigid or affine transformations. However, this cannot produce accurate alignments when the scenes are not planar, for example, face images. In this paper, we propose a regularized Gaussian fields criterion for non-rigid registration of visible and infrared face images. The key idea is to represent an image by its edge map and align the edge maps by a robust criterion with a non-rigid model. We model the transformation between images in a reproducing kernel Hilbert space and a sparse approximation is applied to the transformation to avoid high computational complexity. Moreover, a coarse-to-fine strategy by applying deterministic annealing is used to overcome local convergence problems. The qualitative and quantitative comparisons on two publicly available databases demonstrate that our method significantly outperforms the state-of-the-art method with an affine model. As a result, our method will be beneficial for fusion-based face recognition.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Multi-sensor data often provides complementary information about the region surveyed, and image fusion which aims to create new images from such data offering more complex and detailed scene representation has then emerged as a promising research strategy for the purposes of human visual perception, object detection, as well as target recognition <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref>. Particularly, for the face recognition problem, the use of multi-sensor data such as visible and thermal infrared (IR) images has been shown to be able to achieve higher recognition rate <ref type="bibr" target="#b3">[4]</ref>. For example, thermal IR images are not affected by illumination variation or face disguise, while visible information is better for establishing a discriminative face model. However, successful image fusion requires an essential and challenging step that the image pairs to be fused have to be correctly co-registered on a pixel-by-pixel basis. In this paper, we focus on registration of visible and thermal IR face images for fusion-based face recognition.</p><p>Registration of multi-sensor images typically can be implemented by either hardware or software. Due to elevated cost and low availability, a special-purpose imaging sensor assembly that generates co-registered image pairs may not be practical in many face recognition scenarios. In this context, software-based registration may be more appropriate for large-scale deployment, where off-the-shelf low cost visible and infrared cameras can be utilized and no additional hardware is needed. Although the pixel-to-pixel alignment accuracy could be reduced compared to the hardware-based registration, the salient structures within the images can still be matched sufficiently well in a software-based registration process. In this paper, we are interested in software-based registration techniques.</p><p>The first step of software-based registration is to choose the information type that should be extracted and used to represent the images. Most registration methods assume global statistical dependence of the images to be aligned <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b1">2]</ref>, and hence appearance features such as graylevels/colors, textures (e.g. Gabor filters <ref type="bibr" target="#b5">[6]</ref>) and gradient histograms (e.g. SIFT <ref type="bibr" target="#b6">[7]</ref> and HOG <ref type="bibr" target="#b7">[8]</ref>) are preferred for the purpose of matching. However, for visible and thermal IR face images which are manifestations of two different phenomena, these appearance features will not likely match; instead, certain features representing salient structures could be adopted, such as points of high curvature, line intersections, strong edges, structural contours and silhouettes within the images. Here we choose the edge map, which is a significant common feature that might be preserved in face images <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b3">4]</ref>.</p><p>To register edge maps of visible and IR face images, Kong et al. <ref type="bibr" target="#b3">[4]</ref> proposed to use a straightforward criterion, named the Gaussian fields criterion <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>, which consists of a Gaussian mixture depending on distances as well as point attributes such as image intensities and local shape descriptors. The criterion then maximizes the overlap between the salient structures such as edges that are present in both images. This method possesses several characteristics, for example, the criterion is differentiable and hence standard gradient-based optimization techniques could be employed. Besides, the method does not need any explicit set of point correspondences. However, the method is in general designed for rigid or affine registration, where the underlying relation between the data can be modeled by a parametric model. But a human face is a non-rigid shape and the deformation of the point set sampled from a face is restricted by physical constraints of bones and muscles. Therefore, the parametric model such as affine transformation used in <ref type="bibr" target="#b3">[4]</ref> cannot produce accurate alignment. To address this problem, in this paper we generalize the Gaussian fields criterion, and propose a regularized Gaussian fields criterion for the purpose of non-rigid registration.</p><p>More precisely, we model the non-rigid transformation in a functional space, called the reproducing kernel Hilbert space (RKHS) <ref type="bibr" target="#b11">[12]</ref>, in which the function has an explicit kernel representation. To ensure well-posedness of the Gaussian fields criterion, we introduce a regularization term to enforce smoothness of the transformation. This regularized Gaussian fields criterion keeps the property of differentiable, and, it is more promising in handling data that involves non-rigid motion. Feature descriptors such as shape context <ref type="bibr" target="#b12">[13]</ref> are used as attributes to help recover the point correspondence. Moreover, a sparse approximation based on a similar idea as the subset of regressors method <ref type="bibr" target="#b13">[14]</ref> is also introduced to improve the computational efficiency.</p><p>Our contribution in this paper includes the following three aspects. Firstly, we analyze the robustness of the Gaussian fields criterion both in theory and experiment, which provides support for why it can be successfully used to the point set registration problem. Secondly, we generalize the Gaussian fields criterion from rigid to the non-rigid case, and propose a regularized Gaussian fields criterion for non-rigid point set registration, which enables us to deal with more real-world matching problems. Thirdly, we customize and apply the proposed regularized Gaussian fields criterion to visible and thermal IR face image registration. Compared to previous methods, it can increase the registration accuracy, and hence is able to improve the reliability of fusion-based face recognition systems.</p><p>The rest of the paper is organized as follows. Section 2 describes background material and related work. Section 3 describes the Gaussian fields criterion for rigid point set registration, and discuss the robustness of the criterion as well. In Section 4, we present our regularized Gaussian fields criterion, and apply it to visible and thermal IR face image registration. Section 5 illustrates our method on face landmark set registration and then tests it for visible and thermal IR face image registration on a public available dataset with comparisons to other approaches, followed by some concluding remarks in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>There are in general two types of approaches for image registration: area-based methods and feature-based methods, as discussed in recent survey papers <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b14">15]</ref>. These approaches are typically developed for visible stereo, remote sensing, or medical image pairs. The application to visible/infrared camera configuration is not straightforward due to the different spectral sensitivity of the sensors. For instance, visible sensors capture reflected light, while IR sensors capture principally thermal radiations emitted by objects; and hence appearance features such as textures in a visible image often get lost in the corresponding IR image since texture seldom influences the heat emitted by an object. Next we briefly review the above mentioned two types of approaches, especially in the context of multimodal registration such as visible and thermal IR images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Area-based methods</head><p>Area-based methods deal directly with the image intensity values without attempting to detect salient structures. These methods can be broadly classified into three types: correlation-like methods, Fourier methods, and mutual information (MI) methods <ref type="bibr" target="#b1">[2]</ref>.</p><p>Correlation-like methods such as cross-correlation and its modifications are a classical representative of the area-based methods <ref type="bibr" target="#b15">[16]</ref>. The main idea of these methods is to compute the similarities of window pairs in two images, and consider the one with the largest similarity as a correspondence. To perform multimodal image registration, correlation ratio-based methods have been developed under the assumption that intensity dependence can be represented by some function <ref type="bibr" target="#b16">[17]</ref>. The correlation-like methods suffer from some drawbacks such as the flatness of the similarity measure in textureless regions and high computational complexity. However, due to their easy hardware implementation which is beneficial for real-time applications, these methods are still often in use.</p><p>Fourier methods are the second type of approaches. These methods exploit the Fourier representation of images in the frequency domain <ref type="bibr" target="#b17">[18]</ref>. Compared to correlation-like methods, they have some advantages in computational efficiency and are also robust to frequency-dependent noise. A representative application exploiting the Fourier transform for multimodal registration is described in <ref type="bibr" target="#b18">[19]</ref>. It computes the correlation in the frequency domain, which is able to handle multimodal images such as visible and IR images when applied to the edge representations rather than the original graylevel images.</p><p>Finally, area-based methods also include mutual information methods. The mutual information provides an attractive metric for maximizing the dependence between two images, and it is particularly suitable for multimodal registration <ref type="bibr" target="#b19">[20]</ref>. However, for visible/ infrared image pairs with quite different textures, the mutual information is good usually only on a small portion of the images. Therefore, mutual information is typically used only on a selected region of an image <ref type="bibr" target="#b20">[21]</ref>, on a detected foreground <ref type="bibr" target="#b21">[22]</ref>, or on region with similar edge density <ref type="bibr" target="#b22">[23]</ref>. Furthermore, the mutual information methods not only work directly with image intensities, but also with extracted features such as points of the area borders <ref type="bibr" target="#b23">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Feature-based methods</head><p>The second approach for image registration is based on the extraction of salient structuresfeaturesin the images, which can be represented as compact geometrical entities at different levels, such as points, line segments, curves, and surfaces <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b1">2]</ref>. The registration problem then reduces to determining the correct correspondence and to find the underlying spatial transformation between two sets of extracted features. In general, features at higher levels are more difficult to extract reliably; the point feature is the simplest form of feature, and it is also more general since lines and surfaces can be discretized as a set of points. In this sense, point matching serves as the basis for the feature-based methods.</p><p>To address the point registration problem, a variety of techniques have been developed in the fields of computer vision, remote sensing, medical imaging, etc. A popular strategy is to iteratively solve the two unknown variables, i.e., the correspondence and the transformation. This kind of algorithms includes iterated closest point (ICP) <ref type="bibr" target="#b24">[25]</ref>, robust point matching using thin-plate spline (TPS-RPM) <ref type="bibr" target="#b25">[26]</ref>, coherence point drift (CPD) <ref type="bibr" target="#b26">[27]</ref>, vector field consensus (VFC) <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref>, robust point matching using L 2 E estimator (PRM-L 2 E) <ref type="bibr" target="#b29">[30]</ref>, etc. Another interesting point matching approach is the kernel correlation (KC) based method <ref type="bibr" target="#b30">[31]</ref>, which was extended by using the L 2 distance between Gaussian mixture models representing the point set <ref type="bibr" target="#b31">[32]</ref>. Besides, Belongie et al. <ref type="bibr" target="#b12">[13]</ref> introduced a shape context descriptor incorporating the neighborhood structure of a point set to help establish point correspondence.</p><p>Specifically, for visible and thermal IR image registration, features like boundaries, edges or connected edges, are more preferred for common information capture <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref>. Here the edges can be extracted by using the Canny edge detector <ref type="bibr" target="#b3">[4]</ref>, discrete wavelet transform <ref type="bibr" target="#b32">[33]</ref>, or Gabor filters <ref type="bibr" target="#b33">[34]</ref>, and the quality of the transformation is evaluated using a certain distance metric (e.g. the Hausdorff distance) between the edge points. Methods matching other features such as corners, line segments and virtual line intersections have also been developed <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b8">9]</ref>. To register visible and thermal IR videos, recent methods such as trajectory computation <ref type="bibr" target="#b35">[36]</ref> and blob tracking <ref type="bibr" target="#b36">[37]</ref> have also been considered. Furthermore, Bilodeau et al. <ref type="bibr" target="#b37">[38]</ref> introduced the discrete curve evolution (DCE) for keypoints extraction on the boundary of silhouettes. It is essentially a polygonal approximation of the foreground blobs. Based on this technique, a fast registration method was presented in <ref type="bibr" target="#b38">[39]</ref>, which was later extended in <ref type="bibr" target="#b39">[40]</ref> by considering each target individually for better matching precision. However, the transformations in these methods are typically supposed to be parametric such as rigid or affine, and hence the matching performance could be degraded if the image data involves non-rigid motion. In this work, we introduce a novel registration method with the non-rigid model based on the regularized Gaussian fields criterion for visible and thermal IR face image registration. The edge map is chosen as the feature, and we discretize it into a set of points. Therefore, our method is essentially a non-rigid point set registration algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Gaussian fields criterion</head><p>Given a set of 'model' points fx n g N n ¼ 1 and a set of 'data' points fy m g M m ¼ 1 , the goal of registration is to align the model point set to the data point set. Typically, it requires recovery of both the correct correspondence and the underlying spatial transformation T . Intuitively, considering the noiseless case, the best alignment is the one resulting in the maximum point-to-point overlap between two point sets. This can be represented by a Boolean operator that 'count' the number of overlapping points for a given transformation:</p><formula xml:id="formula_0">EðT Þ ¼ ∑ M m ¼ 1 ∑ N n ¼ 1 δðdðT ðx n Þ; y m ÞÞ with δðtÞ ¼ 1 for t ¼ 0 0 otherwise ; &amp;<label>ð1Þ</label></formula><p>where dðx; yÞ is the distance (e.g. Euclidean) between points. Eq. ( <ref type="formula" target="#formula_0">1</ref>) is a discrete combinatorial function, which is not continuous with respect to the transformation T and it will be difficult to find the global maximum.</p><p>To regularize the ill-posed problem (1) with respect to differentiability, Boughorbel et al. <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> proposed a simple criterion based on Gaussian fields <ref type="bibr" target="#b40">[41]</ref>, which was for rigid or affine registration between 3D point clouds. By using the Gaussian kernel with a range parameter σ, the criterion maximizes a 'mollified' function as</p><formula xml:id="formula_1">E σ ðT Þ ¼ ∑ M m ¼ 1 ∑ N n ¼ 1 exp À d 2 ðT ðx n Þ; y m Þ σ 2 ( ) ;<label>ð2Þ</label></formula><p>where the transformation T is parameterized by a rotation matrix R and a translation vector t :</p><formula xml:id="formula_2">T ðx n Þ ¼ Rx n þt.</formula><p>The Gaussian fields criterion ( <ref type="formula" target="#formula_1">2</ref>) is a straightforward sum of Gaussian distances between all pairs of model and data points, which is robust to noise and outliers. On one hand, by relaxing the range parameter σ to values near that of noise variance, the criterion can account for the noise affecting the position of points. On the other hand, the criterion is robust to outliers. To get some intuition for this property, we compare it to a criterion with L 2 loss, e.g.,</p><formula xml:id="formula_3">E L 2 ðT Þ ¼ À∑ M m ¼ 1 ∑ N n ¼ 1 d 2 ðT ðx n Þ; y m Þ.</formula><p>The penalty curves of Gaussian distance (e.g. 1 À exp fÀd 2 ðx; yÞ=σ 2 g) and L 2 loss (e.g. we see that the quadratic penalty curve of L 2 loss is reluctant to assign large distance dðT ðx n Þ; y m Þ of any correspondence ðx n ; y m Þ, including outliers, and hence tends to be biased by outliers. By contrast, the penalty of Gaussian distance can be approximately seen as a truncated form of the L 2 loss penalty, and then it can bear large distances of many correspondences, hopefully including the outliers, without paying too high a penalty. Moreover, unlike a truncated penalty, the Gaussian fields criterion is derivable and it has the advantage of computational convenience.</p><p>To further demonstrate the robustness of the Gaussian fields criterion (2), we present a simple example of registration of 1D points which contrasts the behavior of L 2 loss criterion and Gaussian fields criterion, see Fig. <ref type="figure" target="#fig_0">2</ref>. The goal is to align the model points (i.e. ' þ') to the data points (i.e. '○'), where the data points suffer from Gaussian noise, outliers, as well as unknown translations. In the left figure, the data points do not contain outliers, and both the two criterions have global minima near the true value, i. e., t¼0. In the middle figure, some outliers are added to the data points; we see that the L 2 loss criterion is biased and the global maximum shifts to the right of the true value, while the Gaussian fields criterion is very resistant and still has a global maximum at approximately the true value. Observe, the Gaussian fields criterion also has a local maximum at about t ¼160; this is appropriate because, in this case, the model points are aligned to the cluster of outliers. In the right figure, to generate the data points, we first generate two copies of the model points contaminated by Gaussian noise, and then add translations t ¼ À100 and t¼100 to the two copies, respectively. We see that in this case the Gaussian fields criterion has two maxima near t ¼ À100 and t¼ 100, corresponding to alignments on two parts of the data points, respectively. However, the L 2 loss criterion has a global maximum at about t¼0, which makes no sense for the registration.</p><p>The definition of Gaussian fields criterion in Eq. ( <ref type="formula" target="#formula_1">2</ref>) uses a minimum amount of information about the datasets, e.g., just the position of points. To reduce the effect of outliers and at the same time increase the area of convergence, it is helpful to associate more information to the points such as local shape descriptors as well as intensity or color information. This could be achieved by extending the distance measure in Eq. ( <ref type="formula" target="#formula_1">2</ref>) as follows: where SðT ðx n ÞÞ and Sðy m Þ are the associated 1D attribute vectors, cðSðxÞ; SðyÞÞ is the distance (e.g. χ 2 test statistic) between attributes, ω mn ðT Þ ¼ expfÀη Á cðSðT ðx n ÞÞ; Sðy m ÞÞg with η being a positive value controlling the trade-off between the points' spatial and attribute distances. The Gaussian fields criterion here can be interpreted as a force field decaying with the Euclidean distance between the spatial positions and the χ 2 distance between the attribute vectors.</p><formula xml:id="formula_4">E σ ðT Þ ¼ ∑ M m ¼ 1 ∑ N n ¼ 1 exp À d 2 ðT ðx n Þ; y m Þ σ 2 À η Á cðSðT ðx n ÞÞ; Sðy m ÞÞ ( ) ¼ ∑ M m ¼ 1 ∑ N n ¼ 1 ω mn ðT Þ exp À d 2 ðT ðx n Þ; y m Þ σ 2 ( ) ;<label>ð3Þ</label></formula><p>If the attributes are invariant to the aligning transformation, then ω mn ðT Þ in Eq. ( <ref type="formula" target="#formula_4">3</ref>) will not depend on T . Typically, ω mn will be much larger for an inlier correspondence than for an outlier correspondence, and hence it plays a role of weight, which could be helpful for searching the optimal solution of the objective function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Regularized Gaussian fields criterion for non-rigid registration</head><p>The above Gaussian fields criterion (3) is created for rigid shape registration, which involves only a small number of parameters <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>. Rigid registration is relatively easy and has been widely studied <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b41">42]</ref>. By contrast, non-rigid registration is more difficult because the underlying non-rigid transformations are often unknown, complex, and hard to model <ref type="bibr" target="#b29">[30]</ref>. But non-rigid registration is very important because it is required for many real world tasks, for example, face image registration in our problem. In this paper, we introduce a regularization term and generalize the formulation (3) to the non-rigid case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Problem formulation</head><p>Given two point sets fx n ; Sðx n Þg N n ¼ 1 (i.e. the model) and fy m ; Sðy m Þg M m ¼ 1 (i.e. the data) with their associated attribute vectors, the goal is to align the model point set to the data point set. According to the Gaussian fields criterion (3), we solve for the best displacement function fðxÞ (i.e. T ðxÞ ¼ x þ fðxÞ) that minimizes the following regularized criterion:</p><formula xml:id="formula_5">E σ ðfÞ ¼ À ∑ M m ¼ 1 ∑ N n ¼ 1 ω mn ðfÞ exp À d 2 ðx n þ fðx n Þ; y m Þ σ 2 ( ) þλΦðfÞ;<label>ð4Þ</label></formula><p>where the first term enforces closeness of the model and data points, the second term is a stabilizer which enforces smoothness to the non-rigid displacement function f, and λ is a regularization constant controlling the trade-off between these two terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">Attributes</head><p>As mentioned above, it is suitable to adopt attributes that are invariant to the transformation, and then the weight ω mn ðfÞ in the regularized Gaussian field criterion (4) becomes ω mn , which is beneficial for numerical calculation. In the rigid case, moment invariants could be employed which has shown promising results <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b10">11]</ref>. Here we consider the non-rigid case.</p><p>In general, if the two point sets have similar shapes, the corresponding points have similar neighborhood structures which could be incorporated into a feature descriptor. Therefore, we can use such a shape descriptor as the attribute of a point. Inspired by these facts, in the 2D case, the shape context <ref type="bibr" target="#b12">[13]</ref> can be used as a feature descriptor, using the χ 2 test statistic as the cost measure. In the 3D case, the spin image <ref type="bibr" target="#b43">[44]</ref> can be used as a feature descriptor, where the local similarity is measured by an improved correlation coefficient.</p><p>In this paper, we mainly focus on 2D face image registration; thus we use shape context feature descriptor as the attribute. For two points x n and y m , their shape context descriptors are distributions represented as histograms, and the χ 2 test statistic used for cost measure between two points is defined as</p><formula xml:id="formula_6">cðSðx n Þ; Sðy m ÞÞ ¼ 1 2 ∑ K k ¼ 1 ðS k ðx n ÞÀS k ðy m ÞÞ 2 S k ðx n ÞþS k ðy m Þ ;<label>ð5Þ</label></formula><p>where S k ðx n Þ and S k ðy m Þ denote the K-bin normalized histogram at x n and y m , respectively. For face images, the relative position of the chin, nose, mouth, eyes, etc., cannot deform independently due to underlying constraints of bones and muscles; therefore, the degree of non-rigid deformation is usually relatively slight, and hence the attribute could be considered to be invariant to the non-rigid transformation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">Transformation models</head><p>The displacement function f is non-rigid, and we model it by requiring it to lie within a specific functional space, namely a vector-valued reproducing kernel Hilbert space (RKHS) <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b28">29]</ref>, as described in detail in Appendix A. It should be mentioned that other parameterized transformation models such as thin-plate splines (TPS) <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b46">47]</ref> can also be easily incorporated into our formulation.</p><p>We define a vector-valued RKHS H by a positive definite matrix-valued kernel Γ : R D Â R D -R DÂD with D being the dimension of the data. According to the representor theorem <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b28">29]</ref>, we have the following proposition:</p><p>Proposition 1. The optimal function f which minimizes the regularized Gaussian fields criterion (4) then takes the form:</p><formula xml:id="formula_7">fðxÞ ¼ ∑ N n ¼ 1 Γðx; x n Þc n ;<label>ð6Þ</label></formula><p>where the coefficient c n is a D Â 1 dimensional vector. The proof is given in Appendix B. Hence, the minimization over the infinite dimensional Hilbert space reduces to finding a finite set of N coefficients fc n g. Substituting the solution (6) into the regularized Gaussian criterion (4), it becomes</p><formula xml:id="formula_8">E σ ðfÞ ¼ À ∑ M m ¼ 1 ∑ N n ¼ 1 ω mn exp À ‖y m À x n À ∑ N k ¼ 1 Γðx n ; x k Þc k ‖ 2 σ 2 ( ) þ λ‖f‖ 2 Γ ;<label>ð7Þ</label></formula><p>where J Á J denotes the L 2 norm, and the stabilizer has the form</p><formula xml:id="formula_9">ΦðfÞ ¼ ‖f‖ 2</formula><p>Γ with ‖ Á ‖ Γ being the Hilbertian norm of the RKHS H defined by an inner product, e.g., ‖f‖ 2 Γ ¼ 〈f; f〉 Γ .</p><p>By choosing a diagonal Gaussian kernel <ref type="bibr" target="#b28">[29]</ref>:</p><formula xml:id="formula_10">Γðx i ; x j Þ ¼ κðx i ; x j Þ Á I ¼ expfÀβ‖x i À x j ‖ 2 g Á I with</formula><p>β determining the width of the range of interaction between points (i.e. neighborhood size), Eq. ( <ref type="formula" target="#formula_8">7</ref>) may be conveniently expressed in the following matrix form:</p><formula xml:id="formula_11">E σ ðCÞ ¼ À ∑ M m ¼ 1 ∑ N n ¼ 1 ω mn exp À ‖y T m Àx T n À Γ n;Á C‖ 2 σ 2 &amp; ' þ λ Á trðC T ΓCÞ;<label>ð8Þ</label></formula><p>where the kernel matrix ΓAR NÂN is called the Gram matrix with</p><formula xml:id="formula_12">Γ ij ¼ κðx i ; x j Þ ¼ expfÀβ‖x i À x j ‖ 2 g, Γ n;Á denotes the nth row of Γ, C ¼ ðc 1 ; …; c N Þ T</formula><p>is the coefficient matrix of size N Â D, and tr ðÁÞ denotes the trace.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Optimization</head><p>Consider the regularized Gaussian fields criterion in Eq. ( <ref type="formula" target="#formula_11">8</ref>). It is always continuously differentiable with respect to the transformation parameter C. The derivative is given by</p><formula xml:id="formula_13">∂E σ ðCÞ ∂C ¼ ∑ M m ¼ 1 ∑ N n ¼ 1 2ω mn σ 2 Γ T n;Á ðx T n þ Γ n;Á C À y T m Þ exp À ‖y T m À x T n À Γ n;Á C‖ 2 σ 2 &amp; ' þ 2λΓC:<label>ð9Þ</label></formula><p>By using the derivative in Eq. ( <ref type="formula" target="#formula_13">9</ref>), we can employ efficient gradient-based numerical optimization techniques such as the quasi-Newton method and the nonlinear conjugate gradient method to solve the optimization problem. However, the regularized Gaussian fields criterion ( <ref type="formula" target="#formula_11">8</ref>) is convex only in the neighborhood of the optimal solution so it is unlikely that any algorithm can find its global minimum. Therefore, to improve convergence we use a coarse-to-fine strategy by applying deterministic annealing <ref type="bibr" target="#b47">[48]</ref> on the range parameter σ. This starts with a large initial value for σ which is gradually reduced by σ↦γσ, where γ is the annealing rate. At large σ, the criterion will be convex in a large region surrounding the global minimum, and hence we are likely to find the global minimum. As σ decreases, the position of the global minimum will tend to change smoothly. The objective function will be convex in a small region around its minimum, which makes it likely that using the old global minimum as initial value could converge to the new global minimum. Therefore, as the iteration proceeds, we have a good chance of reaching the global minimum. The main idea is illustrated in Fig. <ref type="figure">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Fast implementation based on sparse approximation</head><p>The Gram matrix Γ is of size N Â N, and then the costs of updating the objective function ( <ref type="formula" target="#formula_11">8</ref>) and the derivative (9) are both OðMN 2 Þ. However, in point registration problems, the point set typically contains hundreds or thousands of points, which causes significant complexity problems (in both time and space), and, even when it is implementable, one may prefer a suboptimal but simpler method. Consequently, we adopt a sparse approximation, and randomly pick only a subset of size N 0 input points f xn g N 0 n ¼ 1 to have nonzero coefficients in the expansion of the solution (6). This follows <ref type="bibr" target="#b48">[49]</ref> who found that this approximation works well and that simply selecting a random subset of the input points in this manner, performs no worse than more sophisticated and time-consuming methods. Therefore, we seek a suboptimal solution of form</p><formula xml:id="formula_14">fðxÞ ¼ ∑ N 0 n ¼ 1 Γðx; xn Þc n :<label>ð10Þ</label></formula><p>Here the suboptimal solution space</p><formula xml:id="formula_15">H N 0 ¼ ∑ N 0 n ¼ 1 ΓðÁ; xn Þc n n o is a subset of the optimal solution space H N ¼ ∑ N n ¼ 1 ΓðÁ; x n Þc n È É , see</formula><p>Appendix B for details. The chosen point set f xn g N 0 n ¼ 1 is somewhat analogous to 'control points' <ref type="bibr" target="#b4">[5]</ref>. Thus the regularized Gaussian fields criterion (8) and the derivative ( <ref type="formula" target="#formula_13">9</ref>) become</p><formula xml:id="formula_16">E σ ðC s Þ ¼ À ∑ M m ¼ 1 ∑ N n ¼ 1 ω mn exp À ‖y T m Àx T n À U n;Á C s ‖ 2 σ 2 &amp; ' þ λ Á trðC sT Γ s C s Þ;<label>ð11Þ</label></formula><formula xml:id="formula_17">∂E σ ðC s Þ ∂C s ¼ ∑ M m ¼ 1 ∑ N n ¼ 1 2ω mn σ 2 U T n;Á ðx T n þ U n;Á C s À y T m Þ Â exp À ‖y T m À x T n ÀU n;Á C s ‖ 2 σ 2 &amp; ' þ 2λΓ s C s<label>ð12Þ</label></formula><p>where the Gram matrix</p><formula xml:id="formula_18">Γ s A R N 0 ÂN 0 with Γ s ij ¼ κð xi ; xj Þ ¼ expfÀβ‖ xi À xj ‖ 2 g, U A R NÂN 0 with U ij ¼ κðx i ; xj Þ ¼ expfÀβ‖x i À xj ‖ 2 g, U n;Á</formula><p>denotes the nth row of the matrix U, and the coefficient matrix Fig. <ref type="figure">3</ref>. Schematic illustration of optimizing a non-convex exponential function. In the left figure, we initialize the range parameter σ with a large value, and the objective function may be considered as a convex function, and hence we are able to obtain the global minimum x 1 . As σ decreases, see the middle figure, the new global minimum is x 2 . In general, x 1 is located around x 2 , and the new objective function is convex in a small interval around x 2 . Therefore, we can achieve the global minimum x 2 by using the old global minimum x 1 as an initial solution. As this iteration proceeds, see the right figure, we finally obtain the global minimum.</p><formula xml:id="formula_19">C s ¼ ðc 1 ; …; c N 0 Þ T is of size N 0 Â D. Our method is outlined in Algorithm 1.</formula><p>3 Compute the weights fω mn g M;N m;n ¼ 1 ; 4 Deterministic annealing: 5</p><p>Using the gradient <ref type="bibr" target="#b11">(12)</ref>, optimize the objective function ( <ref type="formula" target="#formula_16">11</ref>) by a numerical technique (e.g., the quasi-Newton algorithm with C s as the old value);</p><formula xml:id="formula_20">6 Update the coefficient matrix C s 'arg min C s E σ ðC s Þ; 7 Anneal σ ¼ γσ;</formula><p>8 The transformation T is determined by T ðxÞ ¼ x þ fðxÞ and Eq. ( <ref type="formula" target="#formula_14">10</ref>).</p><p>The fast Gauss transform: For a registration problem that attributes are not available or not necessary, the registration criterion in Eq. ( <ref type="formula" target="#formula_16">11</ref>) becomes</p><formula xml:id="formula_21">E σ ðC s Þ ¼ À ∑ M m ¼ 1 ∑ N n ¼ 1 exp À ‖y T m Àx T n À U n;Á C s ‖ 2 σ 2 &amp; ' þ λ Á trðC sT Γ s C s Þ:<label>ð13Þ</label></formula><p>The first term in the last equation is essentially a sum of N Gaussian at M points. The cost of naive evaluation will be O(MN), which is computationally expensive for large datasets. Greengard and Strain <ref type="bibr" target="#b40">[41]</ref> proposed the fast Gauss transform (FGT) for fast computation of sums of exponentials, e.g., Gðt i Þ ¼ ∑ N j ¼ 1 q j expfÀjt i Às j j 2 =σg for 1 r i r M, which can reduce the computational complexity to OðM þNÞ. The basic idea of FGT is to expand the Gaussians in terms of truncated Hermit expansion, and then approximate the sum up to a predefined accuracy. Here the FGT can be directly applied to our formulation by letting</p><formula xml:id="formula_22">x n ¼ x n þðU n;Á C s Þ T .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Computational complexity</head><p>By examining Eqs. ( <ref type="formula" target="#formula_11">8</ref>) and ( <ref type="formula" target="#formula_13">9</ref>), we see that the costs of updating the original objective function and gradient are both OðMN 2 Þ, where we omit the dimension D of the data which is typically 2 or 3 for vision applications. For the numerical optimization method, we choose the Matlab Optimization toolbox, which implicitly uses the BFGS Quasi-Newton method with a mixed quadratic and cubic line search procedure. Thus the total time complexity for solving the</p><formula xml:id="formula_23">N Â D coefficient matrix C is approximately OðMN 2 þN 3 Þ. The space complexity is OðMN þ N 2 Þ</formula><p>due to the requirements of storing the N Â N Gram matrix Γ and the weights fω mn g M;N m;n ¼ 1 . By using the sparse approximation, the size of the Gram matrix becomes N 0 Â N 0 , where N 0 is the number of control points required to construct the displacement function f. Then the time complexity is reduced to OðMNN 0 þN 3 0 Þ, and the space complexity is reduced to OðMN þ NN 0 Þ. Due to N 0 ≪N, the time and space complexities can both be written as O(MN). This is important for large scale problems.</p><p>For the FGT without using attributes, the time complexity of updating the objective function ( <ref type="formula" target="#formula_21">13</ref>) is further reduced to OðM þNÞ. Besides, the space complexity is also reduced to OðM þ NÞ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Application to visible and thermal infrared face image registration</head><p>In this section, we consider the problem of visible and thermal IR face image registration, which generally refers to aligning corresponding structures of faces from the given two images. To this end, we choose the binarized edge maps extracted by the Canny edge detectors to represent the images <ref type="bibr" target="#b3">[4]</ref>. After we obtain the edge maps, we discretize them into a set of points, for example, about 500 points, and compute the corresponding attribute of each point based on shape context. Thus the binary feature maps of the visible and IR data can be described as point sets fx n ; Sðx n Þg N n ¼ 1 and fy m ; Sðy m Þg M m ¼ 1 , respectively. Consequently, we can apply our regularized Gaussian fields criterion to register the two point sets, and hence to register the visible and thermal IR face images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Implementation details</head><p>The performance of point matching algorithms depends, typically, on the coordinate system in which points are expressed. We use data normalization to control for this. More specifically, we perform a linear re-scaling so that the positions in the model and data point sets both have zero mean and unit variance.</p><p>Parameter settings: There are mainly five parameters in our method: β, λ, γ, η and N 0 . Parameters β and λ control the influence of the smoothness constraint on the displacement function f.</p><p>Parameter β determines how wide the range of interaction between samples. Since we have used data normalization so that the points in the two sets both have zero mean and unit variance, the optimal value of parameter β in general will be similar on different samples. Parameter λ controls the trade-off between the closeness to the data and the smoothness of the solution. It is mainly influenced by the degrees of data degradation. Parameter γ controls the annealing rate, which is used to deal with the nonconvexity of the objective function. The deterministic annealing is more likely to work if we anneal slowly. Without considering the efficiency issue, we can always set it as large as possible (e.g., close to 1), and then use more iterations to produce satisfied results.</p><p>Parameter η controls the weight between the points' spatial and attribute information. For a specific type of data such as face contours, the spatial information and the attribute information are similar on different samples, and hence the optimal value of parameter η should also be similar. Parameter N 0 is the required number of control points. We will discuss it later in our experiments. In general, we find that our method is robust to parameter changes. We set β ¼ 0:2, λ ¼ 0:1, γ ¼ 0:93, η ¼ 1 and N 0 ¼ 15, throughout this paper. Finally, the parameter σ and C s in line 2 of Algorithm 1 were initialized to 0.05 and 0 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental results</head><p>To evaluate our regularized Gaussian fields criterion, we first perform face landmark registration to demonstrate the efficiency of our technique, and then focus on the registration problem for visible and thermal IR face images. The experiments are performed on a laptop with 2.5 GHz Intel Core CPU, 8 GB memory and Matlab Code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Registration on face landmark points</head><p>In this section, we test our regularized Gaussian fields criterion for face landmark registration. We choose the publicly available benchmark IMM Face Database<ref type="foot" target="#foot_1">1</ref>  <ref type="bibr" target="#b49">[50]</ref> for evaluation. It comprises 240 still images of 40 human faces with resolution 640 Â 480, and each face contains 6 samples involving different expressions, poses and illuminations. The facial structures such as eyebrows, eyes, nose, mouth and jaw are manually annotated using 58 landmarks. Here in our problem the goal is to align the landmark sets extracted from different face samples of the same individual. The ground truth correspondences between the landmark sets are supplied with the database. In our experiments, we choose the first eight faces for evaluation. And for each face, we construct five pairs of landmark sets for registration, where the landmark set in the first face sample is taken as the model point set and the landmark sets in the rest five samples are taken as the data point sets. Some examples of the face images and corresponding landmarks are shown in Fig. <ref type="figure" target="#fig_4">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1.">Qualitative experiments</head><p>The registration results on the eight groups of faces are shown in Fig. <ref type="figure" target="#fig_5">5</ref>. In each group, the first column is the model and data point sets, represented by blue pluses and red circles, respectively. Our goal is to align the blue pluses to the red circles. From the data, we see that all the landmark set pairs suffer from non-rigid deformation. The degrees of deformation in the second, third and fifth rows are quite large, where the images in these cases are taken under large viewpoint changes. The second and third columns of each group are the registration results of the Gaussian fields criterion with the affine model <ref type="bibr" target="#b3">[4]</ref> and our regularized Gaussian fields criterion with the nonrigid model, respectively. We see that our non-rigid model can generate alignments consistently better than those of the affine model, especially in the case of large non-rigid deformations, i.e., the second, third and fifth rows. This is no surprise, since an affine model is not enough to approximate the unknown and complex non-rigid deformation of a face. By contrast, using a non-rigid model with a regularization term controlling the smoothness of the transformation, our method can roughly align all the face landmark pairs. This guarantees that our regularized Gaussian fields criterion with the non-rigid model could achieve better performance on visible/thermal IR face image registration than the Gaussian fields criterion with the affine model.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2.">Quantitative evaluation</head><p>We now give quantitative comparisons of our method with other methods on the database. Besides the Gaussian fields criterion with the affine model, we also consider a state-of-the-art non-rigid registration method CPD <ref type="bibr" target="#b26">[27]</ref> for comparison. To have a quantitative evaluation, we compute the recall on all landmark set pairs of a face as the metric used in <ref type="bibr" target="#b50">[51]</ref>. Here the recall, or true positive rate, is defined as the proportion of true positive correspondences between the landmark pairs to the ground truth correspondences, and a true positive correspondence is counted when the pair falls within a given accuracy threshold in terms of pairwise distance, e.g., the Euclidean distance between a landmark in the warped model point set and the corresponding landmark in the data point set.</p><p>Fig. <ref type="figure" target="#fig_6">6</ref> reports the results of the three methods on eight faces, where each face contains five pairs of landmark sets. From the results, we see that the two methods with the non-rigid model consistently outperform the Gaussian fields criterion with the affine model. The total average matching errors of the affine model, CPD and our method on this database are about 7.2 pixels, 4.1 pixels and 2.9 pixels, respectively. This justifies the use of nonrigid model for face matching. Our method produces the best results, and our curves are mainly above the curves of CPD, which both use the non-rigid model. This could be attributed that CPD cannot integrate attribute information such as shape features as in our framework.</p><p>We also test the efficiency of our sparse approximation introduced in Section 4.3. For simplicity, we choose the first group of face for evaluation. The results are reported in Fig. <ref type="figure" target="#fig_8">7</ref>, where the left figure is the statistics of matching errors under different values of N 0 , and the right figure is the corresponding runtimes on different landmark set pairs. We see that for N 0 ¼ 15, the sparse approximation works quite well, which can produce results almost the same as using all the input points to construct the transformation, i.e., N 0 ¼ N. Meanwhile, the average runtime is significantly reduced, e.g., 5.9 s compared to 20.6 s. Moreover, this advantage will be magnified as the number of input points N grows. We set N 0 ¼ 15 in the rest of the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Registration on real face images</head><p>Next, we evaluate our regularized Gaussian fields criterion for registration of visible and thermal IR face images. The publicly available benchmark UTK-IRIS (Imaging, Robotics and Intelligent System) Thermal/Visible Face Database 2 is used for evaluation. It contains a number of individuals with various poses, facial expressions, as well as illumination changes. The images are all of resolution 320 Â 240. Some examples of an individual are given in Fig. <ref type="figure" target="#fig_9">8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1.">Qualitative experiments</head><p>To get an intuitive impression of our method's performance, we demonstrates the registration results between several typical frame pairs involving different poses, facial expressions, and illumination changes in the database, as shown in Fig. <ref type="figure" target="#fig_10">9</ref>. This task is typically performed based on a rigid or affine model. Therefore, for the purpose of performance comparison, we choose the Gaussian fields criterion with the affine model which has been considered in <ref type="bibr" target="#b3">[4]</ref>. The original visible images and thermal IR images are given in Fig. <ref type="figure" target="#fig_10">9a</ref> and<ref type="figure">b</ref>, respectively. We see that there exist slight non-rigid pose changes between the image pairs, and hence a non-rigid registration is more preferable. Fig. <ref type="figure" target="#fig_10">9c</ref> shows the edge maps extracted by the Canny edge detector, and we use a sampling method introduced in <ref type="bibr" target="#b12">[13]</ref> to discretize the edges into a set of points, where the red circles correspond to the edge maps of the visible images, and the blue pluses correspond to the edge maps of the thermal IR images. The goal is then to align the red circles to the blue pluses, and subsequently register the visible and thermal IR images accordingly. Note that the edges inside the faces are quite different and may not be aligned accurately, since the two images are manifestations of two different phenomena. Fig. <ref type="figure" target="#fig_10">9d</ref> shows the registration results of the edge map pairs by using the Gaussian fields criterion with the affine model <ref type="bibr" target="#b3">[4]</ref>. We see that the prominent features of the face (i.e., eyes, nose, mouth) have been roughly aligned; but the matching accuracy is not high,   2 Available at: http://www.cse.ohio-state.edu/otcbvs-bench/ which can be seen from the face contour. The corresponding image registration results are given in Fig. <ref type="figure" target="#fig_10">9e</ref>, where we superimpose the warped visible images to the original thermal IR images on a checkerboard. Clearly, the image pairs are not accurately aligned; this can be seen from the seams marked by the red rectangles. By contrast, the matching accuracy of our regularized Gaussian fields criterion with non-rigid model is much higher, and the seams are more natural, as shown in Fig. <ref type="figure" target="#fig_10">9f</ref> and<ref type="figure">g</ref>. This demonstrates that for image pairs involving non-rigid motion such as face images, our method is able to generate more accurate matching results compared to the Gaussian fields criterion with the rigid (or affine) model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2.">Quantitative evaluation</head><p>We next give quantitative comparisons of the two methods on the database. In addition, we also use the non-rigid registration method CPD for further comparison. For each visible/infrared image pair, we manually construct a set of point correspondences as ground truth, for example, the point correspondences located on the contours of the face, eyes, nose, mouth, ears and glasses which have little ambiguity. To have a quantitative evaluation, we also compute the recall on all images of an individual as the metric.</p><p>Fig. <ref type="figure" target="#fig_11">10</ref> reports the results of the three methods on four individuals, where each individual contains about two hundred images. We again see that the two methods with the non-rigid model significantly outperform the Gaussian fields criterion with the affine model, and the curves of our non-rigid model are mainly above the curves of the affine model. Moreover, our method can achieve consistently better performance compared to CPD. The average matching errors of the affine model are about 4.2, 3.3, 3.8 and 4.2 pixels on the four individuals, respectively. By contrast, the average matching errors of our non-rigid model are reduced to about 3.2, 2.4, 2.8 and 3.0 pixels, respectively. The CPD method gives average performance, where the average matching errors are 3.4, 2.6, 3.1 and 3.6 pixels, respectively.</p><p>Note that the curves of our regularized Gaussian fields criterion with the non-rigid model have long 'tails'. We give an explanation as follows. Since the textures of visible and thermal IR images are quite different, the extracted canny edges typically suffer from lots of outliers, as can be seen from the edges on the eyes (first and last columns), mouth (second columns), and eyeglasses (third column) in Fig. <ref type="figure" target="#fig_10">9</ref>. However, the external face contours are preserved very well. By using an affine model, if the face contours are rough aligned, then the matching error of each pixel will not be very large due to the physical constraints on a face. By contrast, a nonrigid model may encourage to match some outliers to achieve the maximum point-to-point overlap, which leads to over-fitting on the outliers, and hence the matching accuracy on the corresponding parts may be badly degraded. Nevertheless, if the data does not contain outliers, as can be seen from Fig. <ref type="figure" target="#fig_6">6</ref>, the curves of our non-rigid model will be consistently above the curves of the affine model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>Within this paper, we have generalized the Gaussian fields criterion from rigid to the non-rigid case, and introduced a regularized Gaussian fields criterion for non-rigid registration of visible and thermal IR face images. The method uses edge map as the feature to represent an image, and the edge maps are registered via the proposed criterion with a non-rigid transformation lying in a reproducing kernel Hilbert space. Experimental results on a Face Landmark Database and a Visible/Infrared Image Database demonstrate that the proposed method with the nonrigid model is able to achieve much more accurate alignments compared to other state-of-the-art methods, especially the method with the affine model. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>d 2</head><label>2</label><figDesc>ðx; yÞ) for a correspondence are shown in Fig. 1. From the figure,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The penalty curves of L 2 loss (left) and Gaussian distance (right) for a correspondence (x, y).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Comparison between L 2 loss criterion and Gaussian fields criterion for 1D point registration. The model and data points are represented by 'þ ' and '○', respectively. Left: 25 model points and 25 data points, where the data points are generated by adding Gaussian noise to the model points. Middle: 25 model points and 30 data points, where the data points are generated by first adding Gaussian noise to the model points and then adding 5 outliers. Right: 25 model points and 50 data points, where the data points consist of two parts: one is generated by adding Gaussian noise to the model points combining with translation t ¼ À100; the other is generated by adding Gaussian noise to the model points combining with translation t¼ 100. The range parameter used in the Gaussian fields criterion is σ ¼ 30.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 1 .</head><label>1</label><figDesc>Regularized Gaussian fields criterion for non-rigid registration. Input: Two point sets with attributes fx n ; Sðx n Þg N n ¼ 1 and fy m ; Sðy m Þg M m ¼ 1 , parameters β, λ, η, γ, N 0 Output: Optimal transformation T 1 Construct the Gram matrix Γ s and matrix U; 2 Initialize parameter σ and the coefficient matrix C s ;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Examples of images in the IMM Face Database. Each row is an individual with different expressions, poses and illuminations.</figDesc><graphic coords="7,40.65,58.64,504.24,125.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Face landmark registration results on the IMM Face Database. There are eight groups, as shown in (a)-(h); each group contains five pairs of face landmarks, as shown by five rows. In each row of a group, the first figure is the two face landmark sets, and the goal is to align the blue ' ' to the red ' '. The second figure is the registration result of the Gaussian fields criterion with the affine model [4]. The third figure is the registration result of our regularized Gaussian fields criterion with the non-rigid model. (For interpretation of the references to color in this figure caption, the reader is referred to the web version of this paper.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Quantitative comparisons of the Gaussian fields criterion with the affine model [4], CPD [27] and our regularized Gaussian fields criterion with the non-rigid model on the IMM Face Database. The eight figures indicate results on eight different individuals in the database. The numbers in the figures are the average matching errors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Registration results of our method on the first face group under different values of N 0 for sparse approximation. Left: statistics of matching errors; right: runtimes on different landmark set pairs. The numbers in the figures are the average matching errors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Examples of images in the UTK-IRIS Thermal/Visible Face Database. Each column is a pair of visible (top) and thermal IR (bottom) face images.</figDesc><graphic coords="9,52.61,237.99,480.24,116.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Registration results on four typical frame pairs with different poses, facial expressions, and illumination changes in the UTK-IRIS Database. (a) and (b) are the original visible images and thermal IR images. (c) is the Canny edge maps, where ' ' corresponds to the visible image, and ' ' corresponds to the thermal IR image. (d) and (e) are the registration results of the edge map pairs and image pairs by using the Gaussian fields criterion with the affine model [4]. (f) and (g) are the registration results of the edge map pairs and image pairs by using our regularized Gaussian fields criterion with the non-rigid model. (e) and (g) are checkerboards of the warped visible image and the original thermal IR image. (For interpretation of the references to color in this figure caption, the reader is referred to the web version of this paper.)</figDesc><graphic coords="10,87.55,58.64,430.08,531.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Quantitative comparisons of the Gaussian fields criterion with the affine model [4], CPD [27] and our regularized Gaussian fields criterion with the non-rigid model on the UTK-IRIS Database. The four figures indicate results on four different individuals in the database. The numbers in the figures are the average matching errors.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>J.Ma  et al. / Pattern Recognition 48 (2015) 772-784</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>Available at: http://www.imm.dtu.dk/ $ aam/datasets/datasets.html J. Ma et al. / Pattern Recognition 48 (2015) 772-784</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors gratefully acknowledge the financial supports from the National Natural Science Foundation of China (Nos. 61273279 and 61275098), and the Natural Science Foundation of Hubei Province of China (No. 2011CDB027).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflict of interest</head><p>None declared.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Vector-valued RKHS</head><p>We review the basic theory of vector-valued reproducing kernel Hilbert space, and for further details and references we refer to <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b51">52]</ref>.</p><p>Let X be a set, for example, X D R P , Y a real Hilbert space with inner product (norm) 〈Á; Á〉, (J Á J ), for example, Y D R D , and H a Hilbert space with inner product (norm) 〈Á; Á〉 H , (‖ Á ‖ H ), where P ¼ D ¼ 2 or 3 for point matching problem. Note that a norm can be induced by an inner product, for example, 8 f A H; ‖f‖ H ¼ ffiffiffiffiffiffiffiffiffiffiffiffi ffi 〈f; f〉 H p . And a Hilbert space is a real or complex inner product space that is also a complete metric space with respect to the distance function induced by the inner product. Thus a vector-valued RKHS can be defined as follows.</p><p>Definition 1. A Hilbert space H is an RKHS if the evaluation maps ev x : H-Y (i.e., ev x ðfÞ ¼ fðxÞ) are bounded, i.e., if 8 x A X there exists a positive constant C x such that</p><p>A reproducing kernel Γ : X Â X -BðYÞ is then defined as Γðx; x 0 Þ≔ev x ev n x 0 , where BðYÞ is the Banach space of bounded linear operators (i.e., Γðx; x 0 Þ, 8 x; x 0 A X ) on Y, for example, BðYÞ DR DÂD , and ev n</p><p>x is the adjoint of ev x . We have the following two properties about the RKHS and kernel. </p><p>with respect to the norm induced by the inner product</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B. Proof of Proposition 1</head><p>For any given reproducing kernel Γ, we can define a unique RKHS H N as in Eq. <ref type="bibr" target="#b14">(15)</ref>. Let H ? N be a subspace of H, for example,</p><p>According to the reproducing property, i.e. Remark 1 in Appendix A, 8 f A H ? N , we have</p><p>Thus H ? N is the orthogonal complement of H N . Consequently, every f A H can be uniquely decomposed in components along and perpendicular to</p><p>H , the regularized Gaussian fields criterion (4) then satisfies</p><p>Therefore, the optimal solution of the regularized Gaussian fields criterion (4) comes from the space H N , and hence has the form <ref type="bibr" target="#b5">(6)</ref>.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multisensor image fusion using the wavelet transform</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Manjunath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Graph. Models Image Process</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="235" to="245" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Image registration methods: a survey</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zitová</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Flusser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="977" to="1000" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multisource image fusion method using support value transform</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-W</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1831" to="1839" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multiscale fusion of visible and thermal IR images for illuminationinvariant face recognition</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Boughorbel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Abidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Koschan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Abidi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="215" to="233" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A survey of image registration techniques</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">G</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="325" to="376" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Comparison of texture features based on Gabor filters</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Grigorescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Petkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kruizinga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1160" to="1167" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Segment-based registration technique for visual-infrared images</title>
		<author>
			<persName><forename type="first">E</forename><surname>Coiras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Santamarı</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Miravet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Opt. Eng</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="282" to="289" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<author>
			<persName><forename type="first">F</forename><surname>Boughorbel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Koschan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Abidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abidi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Gaussian fields: a new criterion for 3D rigid registration</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1567" to="1571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A new method for the registration of three-dimensional point-sets: the Gaussian fields framework</title>
		<author>
			<persName><forename type="first">F</forename><surname>Boughorbel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mercimek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Koschan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abidi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="124" to="137" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Theory of reproducing kernels</title>
		<author>
			<persName><forename type="first">N</forename><surname>Aronszajn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Am. Math. Soc</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="337" to="404" />
			<date type="published" when="1950">1950</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Shape matching and object recognition using shape contexts</title>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Puzicha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="509" to="522" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Networks for approximation and learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Girosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="1481" to="1497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Medical image registration: a review</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">P</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M R</forename><surname>Tavares</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Methods Biomech. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="73" to="93" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wintz</surname></persName>
		</author>
		<title level="m">Digital Image Processing</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Addison-Wesley</publisher>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The correlation ratio as a new similarity measure for multimodal image registration</title>
		<author>
			<persName><forename type="first">A</forename><surname>Roche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Malandain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Pennec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ayache</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Computing and Computer-Assisted Interventation</title>
		<imprint>
			<biblScope unit="page" from="1115" to="1124" />
			<date type="published" when="1998">1998</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin, Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N</forename><surname>Bracewell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bracewell</surname></persName>
		</author>
		<title level="m">The Fourier Transform and Its Applications</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>McGraw-Hill</publisher>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Spatial registration of multispectral and multitemporal digital imagery using fast fourier transform techniques</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Anuta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Electron</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="353" to="368" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Alignment by maximization of mutual information</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On registration of regions of interest (ROI) in video sequences</title>
		<author>
			<persName><forename type="first">H.-M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Varshney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-A</forename><surname>Slamani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Advanced Video and Signal Based Surveillance</title>
		<meeting>IEEE Conference on Advanced Video and Signal Based Surveillance</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="313" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Mutual information based registration of multimodal stereo videos for person tracking</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Krotosky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Underst</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="270" to="287" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Robust multi-sensor image registration by enhancing statistical correlation</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Ra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Information Fusion</title>
		<meeting>IEEE International Conference on Information Fusion</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rigid point feature registration using mutual information</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rangarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Duncan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="425" to="440" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A method for registration of 3-D shapes</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Besl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Mckay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="239" to="256" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A new point matching algorithm for non-rigid registration</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rangarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Underst</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="114" to="141" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Point set registration: coherent point drift</title>
		<author>
			<persName><forename type="first">A</forename><surname>Myronenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="2262" to="2275" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A robust method for vector field learning with application to mismatch removing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2977" to="2984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Robust point matching via vector field consensus</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1706" to="1721" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Robust estimation of nonrigid transformation for point set registration</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2147" to="2154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A correlation-based approach to robust point set registration</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tsin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="page" from="558" to="569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Robust point set registration using gaussian mixture models</title>
		<author>
			<persName><forename type="first">B</forename><surname>Jian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Vemuri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1633" to="1645" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A wavelet-based multisensor image registration algorithm</title>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Signal Processing</title>
		<meeting>IEEE International Conference on Signal Processing</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="773" to="776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multi-modal image registration using local frequency representation and computer-aided design (CAD) models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Elbakary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Sundareshan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="663" to="670" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Infrared-visual image registration based on corners and hausdorff distance</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hrkać</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Kalafatić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krapac</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Analysis</title>
		<imprint>
			<biblScope unit="page" from="383" to="392" />
			<date type="published" when="2007">2007</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin, Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Visible and infrared image registration using trajectories and composite foreground images</title>
		<author>
			<persName><forename type="first">G.-A</forename><surname>Bilodeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Morin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="41" to="50" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Human segmentation by fusing visible-light and thermal imaginary</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cheung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision Workshops</title>
		<meeting>IEEE Conference on Computer Vision Workshops</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1185" to="1192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Silhouette-based features for visibleinfrared registration</title>
		<author>
			<persName><forename type="first">G.-A</forename><surname>Bilodeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>St-Onge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Garnier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="68" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Fast and accurate registration of visible and infrared videos</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sonn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-A</forename><surname>Bilodeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Galinier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>IEEE conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="308" to="313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Chakravorty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-A</forename><surname>Bilodeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Granger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1403.4232</idno>
		<title level="m">Automatic image registration in infrared-visible videos using polygon vertices</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The fast gauss transform</title>
		<author>
			<persName><forename type="first">L</forename><surname>Greengard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Strain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Stat. Comput</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="79" to="94" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Robust registration of 2D and 3D point sets</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1145" to="1153" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">ICP registration using invariant features</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Wehe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="90" to="102" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Using spin-images for efficient object recognition in cluttered 3-D scenes</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="433" to="449" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">On learning vector-valued functions</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Micchelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="177" to="204" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Spline Models for Observational Data</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wahba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<pubPlace>SIAM, Philadelphia, PA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Mismatch removal via coherent spatial mapping</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Image Processing</title>
		<meeting>IEEE International Conference on Image Processing</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Generalized deformable models and matching problems</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Regularized vector field learning with sparse approximation for mismatch removal</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="3519" to="3532" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">FAME-a flexible appearance modelling environment</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Stegmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Ersbøll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Larsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1319" to="1331" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A mixture model for robust point matching under multi-layer motion</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">92282</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Vector valued reproducing kernel hilbert spaces of integrable functions and mercer theorem</title>
		<author>
			<persName><forename type="first">C</forename><surname>Carmeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vito</surname></persName>
		</author>
		<author>
			<persName><surname>Toigo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Anal. Appl</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="377" to="408" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">From 2012 to 2013, he was with the Department of Statistics, University of California at Los Angeles. He is now a Post-Doctoral with the Electronic Information School, Wuhan University. His current research interests include in the areas of computer vision, machine learning, and pattern recognition. Ji Zhao received the B.S. degree in automation from the Nanjing University of Posts and Telecommunication and the Ph.D. degree in control science and engineering from Huazhong University of Science and Technology</title>
		<imprint>
			<date type="published" when="2005">2008 and 2014. 2005. 2012. 2012</date>
			<pubPlace>Wuhan, China</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Huazhong University of Science and Technology ; Doctoral Research Associate with the Robotics Institute, Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note>Jiayi Ma received the B.S. degree from the Department of Mathematics, and the Ph.D. Degree from the School of Automation. respectively. His current research interests include image classification, image segmentation, and kernel-based learning</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">His general field of research is in signal and systems. His current research projects include remote sensing of the Lidar and infrared, as well as Infrared image processing, pattern recognition, interface circuits to sensors and actuators</title>
		<imprint>
			<date type="published" when="1997">1997. 2003. 2004 and 2006. 2006 and 2014</date>
			<pubPlace>Beijing, China; Wuhan, China; Between; Bristol, U.K. Between; Wuhan; Wuhan University</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Yong Ma graduated from the Department of Automatic Control, Beijing Institute of Technology ; Huazhong University of Science and Technology (HUST) ; Wuhan National Laboratory for Optoelectronics, HUST</orgName>
		</respStmt>
	</monogr>
	<note>He received the Ph. he was a Lecturer at the University of the West of England. where he was a Professor of electronics. He is now a Professor with the Electronic Information School</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">He is a Professor and Ph.D. Supervisor of pattern recognition and artificial intelligence with HUST. His current research interests include remote sensing image analysis</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
	<note>Jinwen Tian received the Ph.D. degree in pattern recognition and intelligent systems from Huazhong University of Science and Technology (HUST), China. wavelet analysis, image compression, computer vision, and fractal geometry</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
