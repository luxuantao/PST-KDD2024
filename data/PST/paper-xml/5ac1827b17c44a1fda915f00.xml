<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Local and Global Feature Learning for Blind Quality Evaluation of Screen Content and Natural Scene Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wujie</forename><surname>Zhou</surname></persName>
							<email>wujiezhou@163.com</email>
							<idno type="ORCID">0000-0002-3055-2493</idno>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Lu</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><surname>Zhou</surname></persName>
							<email>zhouyang@zust.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Weiwei</forename><surname>Qiu</surname></persName>
							<email>qiuweiwei@zust.edu.cn</email>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Ming-Wei</forename><surname>Wu</surname></persName>
							<email>wumingwei@zust.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Ting</forename><surname>Luo</surname></persName>
						</author>
						<author>
							<persName><surname>Zhou</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Electronic Engineering</orgName>
								<orgName type="institution">Zhejiang University of Science and Technology</orgName>
								<address>
									<postCode>310023</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Institute of Information and Communication Engineering</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<postCode>310027</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Institute of Information and Communication Engineering</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<postCode>310027</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">School of Information and Electronic Engineering</orgName>
								<orgName type="institution">Zhejiang University of Science and Technol-ogy</orgName>
								<address>
									<postCode>310023</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Faculty of Information Science and Engineering</orgName>
								<orgName type="institution">Ningbo University</orgName>
								<address>
									<postCode>315211</postCode>
									<settlement>Ningbo</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Local and Global Feature Learning for Blind Quality Evaluation of Screen Content and Natural Scene Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2135B6FF2248B71F98E48B8297855FB4</idno>
					<idno type="DOI">10.1109/TIP.2018.2794207</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Screen content images (SCIs)</term>
					<term>natural scene images (NSIs)</term>
					<term>image quality assessment</term>
					<term>local and global features</term>
					<term>locality-constrained linear coding</term>
					<term>collaborative representation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The blind quality evaluation of screen content images (SCIs) and natural scene images (NSIs) has become an important, yet very challenging issue. In this paper, we present an effective blind quality evaluation technique for SCIs and NSIs based on a dictionary of learned local and global quality features. First, a local dictionary is constructed using local normalized image patches and conventional K -means clustering. With this local dictionary, the learned local quality features can be obtained using a locality-constrained linear coding with max pooling. To extract the learned global quality features, the histogram representations of binary patterns are concatenated to form a global dictionary. The collaborative representation algorithm is used to efficiently code the learned global quality features of the distorted images using this dictionary. Finally, kernel-based support vector regression is used to integrate these features into an overall quality score. Extensive experiments involving the proposed evaluation technique demonstrate that in comparison with most relevant metrics, the proposed blind metric yields significantly higher consistency in line with subjective fidelity ratings.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>more frequently and closely employed in daily life. In addition, the use of SCIs and NSIs in various multimedia applications for computers and other electronic devices is increasing, e.g., for visual screen sharing, remote education, remote computing, cloud gaming, immediate communication, and snapshots <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref>. During acquisition, compression, storage, and transmission in multiclient communication systems, SCIs and NSIs are inevitably degraded by various distortion artifacts, such as blurring, noising, compression artifacts, contrast change, quantization, and transmission loss <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b6">[7]</ref>. Therefore, accurate techniques for evaluating the perceptual quality of SCIs and NSIs are required in order to design, monitor, and optimize the performance of each processing stage. That is, a quality evaluation model for SCIs and NSIs can be used as a performance index for further improving compression efficiency. Moreover, it can be used by the sender to control SCI and NSI quality according to user requirements. Thus, this topic is attracting considerable research interest <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b15">[16]</ref>.</p><p>In general, image quality assessment (IQA) methods can be divided into two types: subjective and objective methods <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b9">[10]</ref>. Subjective methods are based on subjective judgments made by human beings, whereas objective methods provide an objective metric to quantify the perceptual quality of distorted images. Although subjective IQA using human subjects is the most natural and reliable method of quantifying perceptual quality, this approach has a number of limitations; it is time consuming, complex, and cumbersome. Most notably, subjective IQA cannot be implemented in realtime or automated systems <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b9">[10]</ref>. Therefore, objective IQA metrics that can accurately and automatically predict image quality are highly desirable.</p><p>In recent years, various objective IQA metrics for SCIs or NSIs have been designed <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>. Depending on the availability of reference information, objective IQA metrics are typically classified into three categories: full-reference (FR), reduced-reference (RR), and blind/ no-reference (NR) metrics. At present, the most commonly used IQA metrics for SCIs or NSIs are FR metrics, which assume that the reference information is completely known. The well-known structural similarity index (SSIM) proposed by Wang et al. <ref type="bibr" target="#b10">[11]</ref> can be considered as a milestone study of FR-IQA metrics. Other relevant studies can be found in <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b16">[17]</ref>.</p><p>The FR-IQA metric provides a useful and effective means of predicting visual quality differences. However, the practical application of FR-IQA metric is very limited, as reference information is rarely accessible on the client side. In such cases, if some reference information can be made available, RR techniques can then be implemented; this approach has attracted considerable interest and yielded reasonably good performance for different distortion types <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b17">[18]</ref>. However, the RR-IQA metric still requires reference information in practice; therefore, this approach is incompatible with most relevant image processing systems, which do not provide additional reference information. In comparison, blind metrics can predict the visual quality of distorted images with no prior knowledge of the reference information. This study focuses on blind quality metrics.</p><p>In recent years, numerous blind IQA metrics for NSIs have been studied extensively <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b27">[28]</ref>. For instance, Ye and Doermann <ref type="bibr" target="#b18">[19]</ref> proposed a visual codebook-based blind IQA metric to measure NSIs quality using histograms of codeword occurrences, but the codebook size is very large. Xue et al. <ref type="bibr" target="#b19">[20]</ref> proposed a quality clustering (QAC) metric that learns a set of quality-predictive centroids. These centroids are then used as a codebook to calculate the quality of an NSI patch; hence, the final quality value of the overall NSI is inferred. Further, Mittal et al. <ref type="bibr" target="#b20">[21]</ref> presented the NSI quality evaluator (NIQE), which does not require training with humanscored distorted NSIs. Inspired by NIQE, Zhang et al. <ref type="bibr" target="#b21">[22]</ref> developed an integrated-local NIQE (IL_NIQE) by integrating more NSI feature statistics. Further, Xue et al. <ref type="bibr" target="#b22">[23]</ref> presented a blind natural NSI IQA metric that uses joint statistics of the gradient magnitude and Laplacian of Gaussian features (GM_LOG). In addition, Zhou et al. <ref type="bibr" target="#b23">[24]</ref> developed a natural IQA metric by analyzing the usefulness and effectiveness of two complementary image components, i.e., the gradient phase and magnitude. Xu et al. <ref type="bibr" target="#b24">[25]</ref> presented a blind IQA metric by aggregating soft-weighted high-order statistical differences between a small codebook and normalized NSI patches. From the image content perspective, SCIs can be regarded as a mixture of natural images, computer graphics, document images, and other components. Obviously, these conventional blind metrics may not perform well on SCIs, as the statistical properties of the pictorial and textual regions in such images are distinct from those of natural images. Furthermore, the same levels of distortion in distinct regions may yield differences in perceptual quality <ref type="bibr" target="#b9">[10]</ref>. In other words, the application of blind IQA to SCIs is significantly more complex than it is for NSIs, because it is affected by the quality of the natural images, computer graphics, document images, and other content <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>.</p><p>Blind quality assessment of SCIs has received little research attention; thus, only a small number of studies have focused on this area <ref type="bibr" target="#b28">[29]</ref>- <ref type="bibr" target="#b31">[32]</ref>. For instance, Gu et al. <ref type="bibr" target="#b28">[29]</ref> presented a blind quality metric (BQMS) for SCIs based on a new SCI statistical model. Further, Qian et al. <ref type="bibr" target="#b29">[30]</ref> devised a blind SCI quality IQA metric utilizing an edge-preserving filter-based free energy and structural degradation model. Gu et al. <ref type="bibr" target="#b31">[32]</ref> proposed a blind SCI IQA metric through bag data learning. However, the performance improvement yielded by these methods is limited by insufficient consideration of the statistical properties of the SCIs.</p><p>Thus, the efficacy of blind IQA metrics for SCIs can be improved significantly.</p><p>In the current state of the research described above, the existing blind IQA metrics are either developed for NSIs or designed for SCIs. Only a very few quality metrics work for both simultaneously <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b30">[31]</ref>. In practical multimedia application systems, we may encounter cross-content-type images (e.g., NSIs, SCIs, and other image types). Efficient general blind IQA metrics that do not depend on image types are required in such circumstances.</p><p>To further advance the development of blind IQA metrics for SCIs and NSIs, in this study, we proposed an effective blind quality metric for SCIs and NSIs that fuses learned local and global quality features to efficiently represent both the local fine details and global statistical structures of the images. The key contributions of our work are summed up as follows:</p><p>(1) The local and global dictionaries are preconstructed based on an image statistical model. With these local dictionaries, the learned local and global quality features can comprehensively characterize the features of the images (e.g., spots, lines, and corners, which are the basic elements of SCIs and NSIs).</p><p>(2) The learned local quality features can be obtained using a strategy of locality-constrained linear coding (LLC) with max pooling. Meanwhile, the collaborative representation (CR) algorithm is used to efficiently code the learned global quality features of the distorted images using global dictionary.</p><p>(3) To the best of our knowledge, this is the first attempt to combine LLC-based quality local features and CR-based global quality features to achieve a fused representation for distorted SCIs and NSIs. The combined use of two types of features can effectively mitigate the respective shortcomings of the individual local and global features.</p><p>The remainder of this study is organized as follows. Section II describes in detail the proposed blind quality assessment metric, including the local and global feature learning process and the support vector regression (SVR)-based quality feature mapping process. The experimental results are presented and discussed in Section III. Finally, we conclude this study in Section IV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. METHODOLOGY</head><p>A flow chart of the proposed blind metric is depicted in Fig. <ref type="figure" target="#fig_0">1</ref>. The process is composed of two stages: i) local and global feature learning and ii) perceived quality prediction. In the feature learning stage, the local and global dictionaries are preconstructed in advance based on an image statistical model. The local and global quality features are learned using these dictionaries and able to express the micro-and macrostructures of the distorted images. In the perceptual quality prediction stage, SVR is implemented to determine the overall quality score.</p><p>The final human perception of visual signals is a synthesis of local and global visual perception <ref type="bibr" target="#b31">[32]</ref>- <ref type="bibr" target="#b34">[35]</ref>. The average distortion in an image influences the overall human visual perception to some degree, whereas an extremely distorted local region lowers the overall visual quality severely. Because of the fundamental difference in the computational methods for the local and global quality features, we expect the two feature representations to provide different types of information. That is, most local quality features convey texture information for a given image patch. However, global quality features include contour representations, texture features, and shape descriptors. Global and local texture features provide different types of information about an image because the support over which the texture is calculated varies. Note that it is necessary to consider both the local and global statistical properties of the images when designing the proposed metric. Therefore, in this work, we combine metrics for patch-based local artifacts and global quality degradation to obtain the overall prediction score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Local Quality Feature Learning</head><p>The local quality feature learning methods implemented in this work incorporate local feature extraction, local dictionary learning, and local feature encoding. These processes are individually described in more detail below.</p><p>1) Local Feature Extraction: For a given image, local features</p><formula xml:id="formula_0">X = [x 1 , x 2 , . . . , x N ] ∈ R d (d = B × B)</formula><p>are extracted from a given gallery set of B × B image patches <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, where N is the total number of local features. To reduce redundancies in the local features and to simulate early visual nonlinear processing (the masking phenomenon) in the human visual system (HVS), each patch is normalized by subtracting the local mean value from the patch and dividing the result by the standard deviation of its elements <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>. In addition, we perform a zero-phase component analysis whitening process on the normalized patches to further reduce the linear dependence relations between the local features <ref type="bibr" target="#b39">[40]</ref>.</p><p>2) Local Dictionary Learning: Local dictionary learning is based on the concept of mapping the patch-based local feature space into more meaningful informative structures. The resultant structures are usually called "visual codewords" <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b40">[41]</ref>. In this work, we follow the dictionary learning protocol presented in <ref type="bibr" target="#b24">[25]</ref>. In particular, a 100-codeword local dictionary is constructed using K -means clustering <ref type="bibr" target="#b41">[42]</ref>. The final generated local dictionary is described using vectors of the form</p><formula xml:id="formula_1">D Local d×K = [D 1 , D 2 , D 3 , . . . , D K ],</formula><p>which captures various distortion characteristics. Hence, the statistical differences between the local dictionary and local features indicate the perceptual quality of the examined images.</p><p>3) Local Feature Encoding: In this approach, for each individual local feature x i , the k nearest codewords k N N( x i ) are selected based on their Euclidean distances. The softweighted mean of the patch based local features assigned to cluster c is expressed as:</p><formula xml:id="formula_2">⎧ ⎪ ⎨ ⎪ ⎩ X c = i:c∈kN N( x i ) [w ic x i ], c = 1, 2, 3 . . . C, w ic = e -β x i -D c (1)</formula><p>where w ic is the Gaussian kernel similarity between feature x i and one of its k nearest codewords D c . Then, all the softweighted means of the patch based local features are concatenated into the patch-based local feature vector</p><formula xml:id="formula_3">X Local d×C = [ X 1 , X 2 , X 3 , . . . , X C ] ∈ R d×C .</formula><p>Here, we do not utilize sparse coding but LLC coding instead, because sparse coding requires a greater computational effort. Furthermore, LLC coding not only ensures sparsity, but also obtains the feature representation by solving a constrained least-squares data-fitting problem <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b42">[43]</ref>. Thus, both efficiency and accuracy are obtained. The objective function of LLC coding is formulated as</p><formula xml:id="formula_4">arg min α X i -D Local d×C • α i 2 + λ l i α i 2 s.t. lα i = 1,<label>(2)</label></formula><p>where α = [α 1 , α 2 , α 3 , . . . , α C ] is a coding coefficient vector for X Local d×C , denotes element-wise multiplication, λ is a small constant that acts as a regularization parameter to adjust the weight decay speed, and l i denotes the i th locality adaptor that assigns weights to each element according to its distance from the local feature vector. We define l i = f ( X i , D Local d×C ), where f (•) measures the distance between the local dictionary and local feature vector <ref type="bibr" target="#b39">[40]</ref>. Finally, the learned local quality features can be obtained from</p><formula xml:id="formula_5">F local = [max(α 1 ), max(α 2 ), . . . , max(α C )].</formula><p>(3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Global Quality Feature Learning</head><p>Semantic structural alteration can generally reflect degradations in visual quality. Recent work indicated that structure descriptors (e.g., local binary patterns (LBPs) <ref type="bibr" target="#b43">[44]</ref>) can effectively and efficiently represent the semantic structural information of visual signals and can be considered as the binary approximation of the semantic structural information primitives in the primary visual cortex <ref type="bibr" target="#b23">[24]</ref>.</p><p>The global quality feature learning method implemented in this work incorporates global dictionary learning and global feature representation. These processes are discussed in detail in the following subsections.</p><p>1) Magnitude and Phase of the Log-Gabor: The HVS is highly sensitive to the edge profile representation that is often encountered in images <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>. Because cortical and retinal neurons in the primary stage of vision respond to stimulus frequency and orientation, multiscale orientation filter responses are similar to the orientation and scale sensitivities of the visual receptive fields of HVS <ref type="bibr" target="#b44">[45]</ref>. In this work, perceptually similar log-Gabor filters (kernels) are utilized for global feature extraction <ref type="bibr" target="#b45">[46]</ref>. The frequency and orientation responses of the log-Gabor filters with orientation j and scale n can be obtained by</p><formula xml:id="formula_6">G n, j (ω, θ ) = exp - (log(ω/ω 0 )) 2 2σ 2 r • exp - θ -θ j 2 2σ 2 θ ,<label>(4)</label></formula><p>where J denotes the number of orientations, θ j = j π/J ( j = {0, 1, . . . , J -1}) denotes the orientation angle, σ r controls the filter radial bandwidth, σ θ determines the angular bandwidth of the filter, ω is the frequency, and parameter ω 0 denotes the center frequency.</p><p>For every log-Gabor kernel, at each point x at a particular scale and orientation, a complex number containing two log-Gabor components (i.e., the real component η n, j (x) and imaginary component ζ n, j (x)) is generated. Using these two components, the magnitude ρ at x at scale n for orientation j is calculated from</p><formula xml:id="formula_7">ρ(n, j, x) = η n, j (x) 2 + ζ n, j (x) 2 1/2 . (<label>5</label></formula><formula xml:id="formula_8">)</formula><p>Similarly, the phase ϕ at location x at scale n for orientation j is calculated from</p><formula xml:id="formula_9">ϕ(n, j, x) = arg tan ζ n, j (x) η n, j (x) . (<label>6</label></formula><formula xml:id="formula_10">)</formula><p>2) Binarization of ρ(n, j, x): Given a magnitude value ρ(n, j, x) of the central pixel z c in a 3 × 3 pattern, the binarization of ρ(n, j, x) is calculated by comparing its magnitude with its P-neighbor values. Then, the binary quantification of ρ(n, j, x) can be expressed as</p><formula xml:id="formula_11">C ρ (z c -z i ) = ⎧ ⎨ ⎩ 1, if ρ(n, j, z i ) ≥ ρ(n, j, z c ), 0, else. (<label>7</label></formula><formula xml:id="formula_12">)</formula><p>3) Binarization of ϕ(n, j, x): A phase change generally induces significant visual distortion. In this study, we first readjust the phase range to [0, 360). Motivated by the local binary pattern (LBP) strategy <ref type="bibr" target="#b43">[44]</ref>, we consider two cases when comparing feature types: different feature types or identical/similar feature types. To improve the stability of the binary quantification, the normalized phase range is divided into K intervals (here, we set K = 4). The local feature types are considered similar when they belong to the same interval; otherwise, they are considered different. The binarization of ϕ(n, j, x) can be expressed as</p><formula xml:id="formula_13">ξ ϕ(n, j,z c ) (z c -z i ) = 1, if Q(ϕ(n, j, z c )) = Q(ϕ(n, j, z i )), 0, else,<label>(8)</label></formula><p>where Q(θ ) is the phase quantification operator, defined as</p><formula xml:id="formula_14">Q(ϕ) = q if 360 • (q -1) K ≤ ϕ &lt; 360 • q K . (<label>9</label></formula><formula xml:id="formula_15">)</formula><p>4) Global Dictionary Learning: Similar to the principle of the LBP operator <ref type="bibr" target="#b43">[44]</ref>, the rotational invariance of the magnitude pattern can be expressed as <ref type="bibr" target="#b10">(11)</ref> where P, R, and U denote the total number of neighbors for each location, neighborhood radius, and frequency of the 1-0 and 0-1 transformations in a circular representation of the magnitude pattern. There are P +2 different values of uniform magnitude patterns representing the structural distribution of the magnitude pattern features. Then, the global statistical normalized histograms from P P riu2 P,R are represented as:</p><formula xml:id="formula_16">M P riu2 P,R = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ P-1 i=0 C ρ (z c -z i ), if U (M P P,R ) ≤ 2, P + 1, else,<label>(10)</label></formula><formula xml:id="formula_17">U (M P P,R ) = C ρ (z P-1 -z c ) -C ρ (z 0 -z c ) + P-1 i=1 C ρ (z i -z c ) -C ρ (z i-1 -z c ) ,</formula><formula xml:id="formula_18">H M P riu2 P,R (k) = 1 x f (M P riu2 P,R (x), k), (<label>12</label></formula><formula xml:id="formula_19">)</formula><p>where f (i, j ) = 1, if i = j 0, otherwise, and k ∈ [0, K ], where K denotes the maximum value of P P riu2 P,R . Thus, K = P + 1. Further, is the position of P P riu2 P,R . For the phase response, we can similarly obtain the global statistical normalized histograms of the phase pattern, i.e., H P P riu2 P,R (k), using Eqs. ( <ref type="formula" target="#formula_16">10</ref>)- <ref type="bibr" target="#b11">(12)</ref>. Finally, by concatenating the above global statistical normalized histograms, the quality feature vectors can be described as</p><formula xml:id="formula_20">b = [H M P riu2 P,R , H P P riu2 P,R ].<label>(13)</label></formula><p>We calculate the global quality features for each sample in a given image gallery set <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref> using the proposed global quality features extraction method. We then construct a global dictionary B for the entire image gallery set by combining all the global quality features that represent the images, such that</p><formula xml:id="formula_21">B = [b 1 , • • • , b n ] ∈ R α×n , (<label>14</label></formula><formula xml:id="formula_22">)</formula><p>where n denotes the total number of samples in the image gallery set and α denotes the feature dimension. 5) CR-Based Global Feature Representation: Given a query image, let y ∈ R m×3 denote its global quality features. Then, y can be expressed by a linear combination of all the global quality features in the gallery, and can be formulated as</p><formula xml:id="formula_23">y = BF global . (<label>15</label></formula><formula xml:id="formula_24">)</formula><p>Note that because B is a redundant dictionary, the solution given by Eq. ( <ref type="formula" target="#formula_21">14</ref>) is not unique. To render the sparse coefficient matrix F global informative and suitable for IQA, some regularization terms should be added to this matrix. In <ref type="bibr" target="#b45">[46]</ref>, the l 2 -minimization-based algorithm is shown to be less complex than the l 1 -minimization-based algorithm. Hence, in this work, we utilize the l 2 -norm regularization term to solve the optimization problem, leading to the following regularized least-squares instantiation of CR. The resultant model is expressed as</p><formula xml:id="formula_25">F global = arg min x { y -Bx 2 2 + λ 1 x 2 2 },<label>(16)</label></formula><p>where λ 1 is a scalar weight. Fortunately, it is analytically and easily derived that Eq. ( <ref type="formula" target="#formula_23">15</ref>) has the closed-form solution</p><formula xml:id="formula_26">F global = (B T B + λ 2 I) -1 B T y, (<label>17</label></formula><formula xml:id="formula_27">)</formula><p>where I is the identity matrix. Let P = (B T B + λ 2 I) -1 B T . Clearly, matrix P is independent of matrix y, allowing it to be precalculated based on the image gallery set alone. Once a query image matrix y is considered, matrix y can be projected ontoP via Py.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Prediction Model Based on SVR</head><p>As discussed in <ref type="bibr" target="#b47">[48]</ref> and <ref type="bibr" target="#b48">[49]</ref>, visual perception and visual working memory retrieval are highly correlated because they share a certain brain cognitive mechanism. From a practical perspective, we imitate this visual working memory module using the kernel-based SVR. The final objective quality score Q of an SCI can be predicted using {F local , F global } and a nonlinear prediction function f . That is, the objective quality value representing the objective quality of the SCI is given by Q = f (F local , F global ), where f is pre-estimated using kernelbased SVR. Here, f : R 2 → R takes the learned local and global quality features {F local , F global } as input and outputs a difference mean opinion score. In kernel-based SVR, the unknown f can be learned as</p><formula xml:id="formula_28">f (x) = l i=1 (α i -α * i )K (x i , x) + b, (<label>18</label></formula><formula xml:id="formula_29">)</formula><p>where α and α * are Lagrange multipliers, b is a bias parameter, and K (x i , x) is a radial basis kernel function <ref type="bibr" target="#b49">[50]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTAL RESULTS AND ANALYSES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Protocol</head><p>We conducted several experiments using three IQA datasets: the newly released SIQAD dataset <ref type="bibr" target="#b10">[11]</ref>, categorical subjective image quality (CSIQ) dataset <ref type="bibr" target="#b50">[51]</ref>, and cross-contenttype (CCT) dataset <ref type="bibr" target="#b30">[31]</ref> to validate the effectiveness of the proposed blind metric.</p><p>The SIQAD dataset contains 20 reference SCIs altered by seven types of distortion, each with seven levels of degradation. Thus, this dataset consists of 140 distorted SCIs, comprising 20 reference SCIs distorted by Gaussian noise (GN), motion blur (MB), Gaussian blur (GB), contrast change (CC), JPEG2000 compression (JP2K), JPEG compression (JPEG), and layer-segmentation-based coding (LSC). For further details on this dataset, see <ref type="bibr" target="#b9">[10]</ref>.</p><p>The CSIQ dataset consists of 866 NISs and six types of distortion: white noise (WN), JPEG, JP2K, additive Gaussian pinknoise (PN), GB, and global contrast decrements (GCD). The difference mean opinion score (DMOS) of each NSI is included.</p><p>The CCT dataset, contains 1,320 distorted images (e.g., SCIs, NSIs, and others) and associated DMOS values generated from 72 pristine images distorted with two distortion types, at various levels of distortion.</p><p>Two typical performance criteria were employed to evaluate the IQA metrics tested in these experiments: i) Pearson's linear correlation coefficient (PLCC), which evaluates prediction accuracy, and ii) Spearman's rank order correlation coefficient (SROCC), which reflects the prediction monotonic of IQA metrics. Higher PLCC and SROCC values values indicate superior correlation performance. Thus, a high-performance objective model has the PLCC and SROCC values are close to 1.</p><p>As the proposed blind metric requires training and testing, a cross-validation test was implemented by randomly splitting each dataset into two non-overlapping subsets: training (80%) and test (20%). To offset the performance bias as much as possible, this 80:20 split of the data was iterated 1,000 times and the median PLCC and SROCC values of the 1,000 trials are reported. When deploying SVR <ref type="bibr" target="#b48">[49]</ref> to train the prediction module, the parameters (C = 2 a , γ = 2 b ) of SVR must be set.</p><p>To select the values of (a, b), a grid search was used. Some of the experimental results on the SIQAD dataset are listed in Table <ref type="table">I</ref>. The parameters (a = 4, b = -5) obtained the best results and were used in the following experiments.</p><p>The parameters of the log-Gabor filter were set as: σ θ = 0.71, σ r = 0.61, ω 1 0 = 0.417, ω 2 0 = 0.318, and ω 3 0 = 0.243, where ω 1 0 , ω 2 0 , and ω 3 0 denote the three center frequencies on three scales. In this work, the scale and orientation of the filter were set to n = 3 and j = 4, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Performance Comparison on the SIQAD Dataset</head><p>To comprehensively evaluate the improvement in prediction of the proposed blind metric, the performance in terms of the SROCC and PLCC was compared with that of the following previous objective metrics: five FR IQA metrics (peak signal to noise ratio (PSNR), structural similarity index (SSIM) <ref type="bibr" target="#b10">[11]</ref>, SPQA <ref type="bibr" target="#b9">[10]</ref>, SFUW <ref type="bibr" target="#b11">[12]</ref>, and ESIM <ref type="bibr" target="#b16">[17]</ref>) and six blind IQA metrics (QAC <ref type="bibr" target="#b19">[20]</ref>, NIQE <ref type="bibr" target="#b20">[21]</ref>, IL_NIQE <ref type="bibr" target="#b21">[22]</ref>, GM_LOG <ref type="bibr" target="#b22">[23]</ref>, local gradient patterns (LGP) <ref type="bibr" target="#b23">[24]</ref>, DIIVINE <ref type="bibr" target="#b51">[52]</ref>, and BQMS <ref type="bibr" target="#b28">[29]</ref>). The PLCC and SROCC values for the given SIQAD are summarized in Table <ref type="table" target="#tab_1">II</ref>. The best results in each case are highlighted in bold font. Hence, these experimental results indicate that the proposed blind metric can achieved comparative and reasonably encouraging quality predictions for distorted SCIs compared to other blind metrics. Specifically, it is well known that GM_LOG and LGP perform well for natural images. However, these metrics do not perform well when applied to SCIs. Moreover, compared with FR metrics, the proposed blind metric achieves competitive performance with the top-performing FR metric because it considers the local and global statistical properties of the SCIs. Furthermore, Table <ref type="table" target="#tab_1">III</ref> shows the mean and standard deviation of the SROCC values across the 1,000 trials; higher means with lower standard deviations indicate outstanding prediction performance. In summary, the proposed blind metric quantifies and predicts perceptual distortions in SCIs stably.</p><p>To more comprehensively evaluate an IQA metric's ability to predict the perceptual quality of SCIs caused by different types of distortions, we examined the prediction performance of our blind metric against the competing metrics (PSNR, SSIM, SPQA, SFUW, ESIM, QAC, NIQE, IL_NIQE, GM_LOG, DIIVINE, and LGP) on specific types of distortions. The PLCC and SROCC results are presented (Tables <ref type="table" target="#tab_3">IV</ref> and<ref type="table" target="#tab_3">V</ref>, respectively). From the tables, it can be see that, compared with existing IQA metrics, the proposed blind metric can better handle GN, GB, MB, JPEG, and JP2k distortions. Possibly because of the quality features, which cannot efficiently and effectively represent the visual quality distortion of contrast or shape changes, the proposed blind metric is not good at handling CC and LSC distortions. However, although some blind metrics are good at handling certain individual distortions (CC and LSC), the proposed blind metric is clearly competitive with the blind metric that obtain very promising performance on CC and LSC distortions. In general, the proposed blind metric performs better than, or has comparable prediction performance to, the classical FR metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Performance Comparison on the CSIQ and CCT Datasets</head><p>In this section, we tested our proposed blind metric on the CSIQ and CCT datasets to demonstrate that the proposed model has a strong ability to handle cross-content-type images. Table <ref type="table" target="#tab_4">VI</ref> summarizes the test results on these two datasets, which lead to several useful findings. First, on the CSIQ dataset, which contains NISs only, the results indicate that the learned local and global quality features are efficient for NSIs. Second, on the CCT dataset, which contains cross-content-type images (e.g., NSIs, SCIs, and others), it is interesting to note that the conventional handcrafted features cannot represent cross-content-type images, which contain not only natural scenes but also text, tables, icons, and graphics. However, the learning-based features are able to catch the specific information with various degrees of distortion for text, tables, icons, and graphics. It is efficient for the analysis of NSIs and SCIs under these circumstance not to include any prior knowledge about the image type. In summary, the results show that the proposed blind metric has a generalization ability that can be applied to cross-content-type images, including SCIs, NSIs, and other types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Contribution of Learned Local and Global Features to the Proposed Blind Metric</head><p>To better understanding the individual contributions of the learned local and global features in the proposed blind metric, we designed two test metrics, A and B, in which only the learned local or global features, respectively, were used to estimate the visual quality. Tables VII, VIII, and IX show the performance of metrics A and B, along with that of the proposed blind metric on the SIQAD dataset. For all distortions, it can be see that the performance can be promoted by correctly and properly integrating the learned local and global features. For each type of distortion, we were surprised to see that metric B obtained promising performances for       were conducted at the 5% significance level. The validation results on the SIQAD dataset are summarized in Table <ref type="table" target="#tab_8">X</ref>.</p><p>Symbol "1" denotes that the row metric is significantly better than the column metric, "-1" means that the row metric is significantly worse than the column metric, and "0" indicates that the row metric is significantly indistinguishable with the column metric. It is clear that outstanding performance was achieved by our proposed metric, whose results are "1" for all comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Complexity Analysis</head><p>The computational complexity is another important consideration when evaluating the computational performance of </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Discussions</head><p>1) The current computational complexity of the proposed metric is not very low. The most time-consuming part of the metric is the extraction of the log-Gabor base features. We could replace these features with less computationally expensive features (e.g., gradient based features) in future work. Furthermore, because the local and global feature methods can be executed simultaneously, and parallel computing could be used to improve the speed of the proposed metric.</p><p>2) Blind quality evaluation for NSIs and SCIs is still in a preliminary stage. Hence, we have sufficient space for improvement. In this work, only local and global visual characteristics for NSIs and SCIs were investigated, while the visual characteristics of NSIs and SCIs such as visual saliency and the visual statistical model are different from those of natural images. Further exploration of human visual physiology and psychology may bring more inspiration for establishing a model with more specific characteristics.</p><p>3) Table <ref type="table" target="#tab_5">VII</ref> shows that the adding global features only improves the performance a little for all distortions; however, Tables VIII and IX show that the main contribution of the performances on GN, MB JPEG, and LSC distortions may come from the global feature. Metric B is marginally inferior to metric A for all distortions, perhaps because the global feature cannot efficiently and effectively represent the visual quality distortion for contrast change. Further, how to construct a more effective global feature for CC distortion should be considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CONCLUSION</head><p>In this paper, we have put forward an effective blind quality predictor for distorted images by incorporating the learning of both local and global quality features. The novelty of our research resides in combining the complementary behaviors of the locality-constrained linear coding (LLC)-based local quality features and collaborative representation (CR)-based global quality features to achieve a fused representation for images.</p><p>To extract the learned local features, we use a strategy of LLC with max pooling. Further, the learned global features of the distorted images can be obtained using the CR method. Compared with competing IQA metrics, experimental results show that the proposed blind metric can obtain significantly higher statistical consistency with evaluations from human subjects, confirming that the devised metric is a very robust blind IQA metric for NSIs and SCIs.</p><p>Various aspects of the present research merit further investigation and will, therefore, be considered in future work.</p><p>In the feature extraction stage, we will focus on mining the special characteristics of the NSIs and SCIs more deeply. In the perceptual-quality prediction stage, we intend to design blind IQA metrics for NSIs and SCIs based on deep learning approaches.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. A flow chart of the proposed blind metric.</figDesc><graphic coords="3,77.99,58.25,455.78,156.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II PERFORMANCE</head><label>II</label><figDesc>OF THE PROPOSED BLIND METRIC AND THE OTHER TWELVE METRICS USING THE SIQAD DATASET</figDesc><table><row><cell>TABLE III</cell></row><row><cell>MEAN AND STANDARD DEVIATION OF THE SROCC</cell></row><row><cell>VALUES ACROSS 1,000 TRIALS</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE IV OVERALL</head><label>IV</label><figDesc>PERFORMANCE OF ELEVEN METRICS FOR EACH TYPE OF DISTORTION (PLCC)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE V PERFORMANCE</head><label>V</label><figDesc>OF ELEVEN METRICS FOR EACH TYPE OF DISTORTION (SROCC)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE VI PERFORMANCE</head><label>VI</label><figDesc>COMPARISON ON THE CSIQ AND CCT DATASETS</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VII PERFORMANCE</head><label>VII</label><figDesc>OF EACH FEATURE IN THE PROPOSED BLIND METRIC FOR ALL DISTORTIONS ON THE SIQAD DATASET</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE IX PERFORMANCE</head><label>IX</label><figDesc>OF EACH FEATURE IN THE PROPOSED METRIC FOR EACH DISTORTION TYPES (SROCC) ON THE SIQAD DATASET</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE X RESULTS</head><label>X</label><figDesc>OF F-TEST COMPARING SROCC VALUES OF VARIOUS METRICS ON THE SIQAD DATASET</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE XI RUNNING</head><label>XI</label><figDesc>TIMES OF THE COMPARED BLIND IQA METRICS ON THE SIQAD DATASET our proposed blind metric. The testing environment consisted of a 2.70 GHz Intel Core i5 CPU processor, 8 GB RAM, and the MATLAB R2010 platform. The experimental results on the SIQAD dataset are listed in Table XI, where we list the average computation time per SCI image. As Table XI clearly demonstrates, our proposed blind metric has medium computational complexity.</figDesc><table /></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the National Natural Science Foundation of China under Grant 61502429, Grant 61505176, Grant 61501270, and Grant 61431015, in part by the Zhejiang Provincial Natural Science Foundation of China under Grant LY18F020012 and Grant LQ15F020010, and in part by the China Postdoctoral Science Foundation under Grant 2015M581932. The associate editor coordinating the review of this manuscript and approving it for publication was Prof. Wen Gao.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Just noticeable difference estimation for screen content images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3838" to="3851" />
			<date type="published" when="2016-08">Aug. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Overview of the emerging HEVC screen content coding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="50" to="62" />
			<date type="published" when="2016-01">Jan. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Joint chroma downsampling and upsampling for screen content image</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1595" to="1609" />
			<date type="published" when="2016-09">Sep. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Advanced screen content coding using color table and index map</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4399" to="4412" />
			<date type="published" when="2014-10">Oct. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hash-based block matching for screen content coding</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="935" to="944" />
			<date type="published" when="2015-07">Jul. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Utilitydriven adaptive preprocessing for screen content video compression</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="660" to="667" />
			<date type="published" when="2017-03">Mar. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Intra line copy for HEVC screen content intra-picture prediction</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-H</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1568" to="1579" />
			<date type="published" when="2017-07">Jul. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Subjective and objective quality assessment of compressed screen content images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Emerg. Sel. Topics Circuits Syst</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="532" to="543" />
			<date type="published" when="2016-12">Dec. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Subjective quality evaluation of compressed digital compound images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vis. Commun. Image Represent</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="105" to="114" />
			<date type="published" when="2015-01">Jan. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Perceptual quality assessment of screen content images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4408" to="4421" />
			<date type="published" when="2015-11">Nov. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image quality assessment: From error visibility to structural similarity</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004-04">Apr. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Objective quality assessment of screen content images by uncertainty weighting</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2016" to="2027" />
			<date type="published" when="2017-04">Apr. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Saliency-guided quality assessment of screen content images</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1098" to="1110" />
			<date type="published" when="2016-06">Jun. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Objective quality assessment and perceptual compression of screen content images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Comput. Graph. Appl</title>
		<imprint/>
	</monogr>
	<note>to be published</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Gradient direction for screen content image quality assessment</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-K</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1394" to="1398" />
			<date type="published" when="2016-10">Oct. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Naturalization of screen content images for enhanced quality evaluation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEICE Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="574" to="577" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">ESIM: Edge similarity for screen content image quality assessment</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-K</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4818" to="4831" />
			<date type="published" when="2017-10">Oct. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Reducedreference quality assessment of screen content images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2018-01">Jan. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">No-reference image quality assessment using visual codebooks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3129" to="3138" />
			<date type="published" when="2012-07">Jul. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning without human scores for blind image quality assessment</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Mou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2013-06">Jun. 2013</date>
			<biblScope unit="page" from="995" to="1002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Making a &apos;completely blind&apos; image quality analyzer</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="209" to="212" />
			<date type="published" when="2013-03">Mar. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A feature-enriched completely blind image quality evaluator</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2579" to="2591" />
			<date type="published" when="2015-08">Aug. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Blind image quality assessment using joint statistics of gradient magnitude and Laplacian features</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4850" to="4862" />
			<date type="published" when="2014-11">Nov. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Local gradient patterns (LGP): An effective local-statistical-feature extraction scheme for no-reference image quality assessment</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2017-08">Aug. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Blind image quality assessment based on high order statistics aggregation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4444" to="4457" />
			<date type="published" when="2016-09">Sep. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Utilizing image scales towards totally training free blind image quality assessment</title>
		<author>
			<persName><forename type="first">A</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">M J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1879" to="1892" />
			<date type="published" when="2015-06">Jun. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Toward a no-reference image quality assessment using statistics of perceptual color descriptors</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Plataniotis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3875" to="3889" />
			<date type="published" when="2016-08">Aug. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">CID2013: A database for evaluating no-reference image quality assessment algorithms</title>
		<author>
			<persName><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nuutinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vaahteranoksa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Oittinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Häkkinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="390" to="402" />
			<date type="published" when="2015-01">Jan. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning a blind quality evaluation engine of screen content images</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">196</biblScope>
			<biblScope unit="page" from="140" to="149" />
			<date type="published" when="2016-07">Jul. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Towards efficient blind quality evaluation of screen content images based on edgepreserving filter</title>
		<author>
			<persName><forename type="first">J</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Jakhetiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electron. Lett</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="592" to="594" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unified blind quality assessment of compressed natural, graphic, and screen content images</title>
		<author>
			<persName><forename type="first">X</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5462" to="5474" />
			<date type="published" when="2017-11">Nov. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">No-reference quality assessment of screen content pictures</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-F</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="4005" to="4018" />
			<date type="published" when="2017-08">Aug. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Exploiting global and local information for image quality assessment with contrast change</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Symp. Broadband Multimedia Syst. Broadcast. (BMSB)</title>
		<meeting>IEEE Int. Symp. Broadband Multimedia Syst. Broadcast. (BMSB)</meeting>
		<imprint>
			<date type="published" when="2015-06">Jun. 2015</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning to integrate local and global features for a blind image quality measure</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Smart Comput. (ICSC)</title>
		<meeting>IEEE Int. Conf. Smart Comput. (ICSC)</meeting>
		<imprint>
			<date type="published" when="2014-11">Nov. 2014</date>
			<biblScope unit="page" from="49" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">An efficient color image quality metric with local-tuned-global model</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Process. (ICIP)</title>
		<meeting>IEEE Int. Conf. Image ess. (ICIP)</meeting>
		<imprint>
			<date type="published" when="2014-10">Oct. 2014</date>
			<biblScope unit="page" from="506" to="510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Study on subjective quality assessment of screen content images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Picture Coding Symp. (PCS)</title>
		<meeting>Picture Coding Symp. (PCS)</meeting>
		<imprint>
			<date type="published" when="2015-06">May/Jun. 2015</date>
			<biblScope unit="page" from="75" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Webpage saliency</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis. (ECCV)</title>
		<meeting>Eur. Conf. Comput. Vis. (ECCV)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="33" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">No-reference image quality assessment in the spatial domain</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4695" to="4708" />
			<date type="published" when="2012-12">Dec. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning framework for no-reference image quality assessment</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2012-06">Jun. 2012</date>
			<biblScope unit="page" from="1098" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Independent component analysis: Algorithms and applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Oja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4-5</biblScope>
			<biblScope unit="page" from="411" to="430" />
			<date type="published" when="2000-06">Jun. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The devil is in the details: An evaluation of recent feature encoding methods</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Brit. Mach. Vis. Conf. (BMVC)</title>
		<meeting>Brit. Mach. Vis. Conf. (BMVC)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Least squares quantization in PCM</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lloyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="137" />
			<date type="published" when="1982-03">Mar. 1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Localityconstrained linear coding for image classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2010-06">Jun. 2010</date>
			<biblScope unit="page" from="3360" to="3367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Multiresolution gray-scale and rotation invariant texture classification with local binary patterns</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mäenpää</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="971" to="987" />
			<date type="published" when="2002-07">Jul. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Neural encoding of binocular disparity: Energy models, position shifts and phase shifts</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Heeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vis. Res</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1839" to="1857" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Relations between the statistics of natural images and the response properties of cortical cells</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Field</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Opt. Soc. Amer. A, Opt. Image Sci</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2379" to="2394" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Sparse representation or collaborative representation: Which helps face recognition?</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. (ICCV)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. (ICCV)</meeting>
		<imprint>
			<date type="published" when="2011-11">Nov. 2011</date>
			<biblScope unit="page" from="471" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Polymorphisms in the dopamine receptor 2 gene region influence improvements during working memory training in children and adolescents</title>
		<author>
			<persName><forename type="first">S</forename><surname>Söderqvist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Matsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Peyrard-Janvid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Klingberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Cognit. Neurosci</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="54" to="62" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Increased prefrontal and parietal activity after training of working memory</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Olesen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Westerberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Klingberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Neurosci</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="75" to="79" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A tutorial on support vector regression</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statist. Comput</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="199" to="222" />
			<date type="published" when="2004-08">Aug. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Most apparent distortion: Full-reference image quality assessment and the role of strategy</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Chandler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Electron. Imag</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">11006</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Blind image quality assessment: From natural scene statistics to perceptual quality</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3350" to="3364" />
			<date type="published" when="2011-12">Dec. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
