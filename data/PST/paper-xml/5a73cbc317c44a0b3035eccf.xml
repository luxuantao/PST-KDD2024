<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">&quot;Zero-Shot&quot; Super-Resolution using Deep Internal Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Assaf</forename><surname>Shocher</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science and Applied Math</orgName>
								<orgName type="institution">The Weizmann Institute of Science</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nadav</forename><surname>Cohen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">School of Mathematics</orgName>
								<orgName type="department" key="dep2">Institute for Advanced Study</orgName>
								<address>
									<settlement>Princeton</settlement>
									<country>New Jersey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michal</forename><surname>Irani</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science and Applied Math</orgName>
								<orgName type="institution">The Weizmann Institute of Science</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">&quot;Zero-Shot&quot; Super-Resolution using Deep Internal Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep Learning has led to a dramatic leap in Super-Resolution (SR) performance in the past few years. However, being supervised, these SR methods are restricted to specific training data, where the acquisition of the lowresolution (LR) images from their high-resolution (HR) counterparts is predetermined (e.g., bicubic downscaling), without any distracting artifacts (e.g., sensor noise, image  compression, non-ideal PSF, etc). Real LR images, however, rarely obey these restrictions, resulting in poor SR results by SotA (State of the Art) methods. In this paper we introduce "Zero-Shot" SR, which exploits the power of Deep Learning, but does not rely on prior training. We exploit the internal recurrence of information inside a single image, and train a small image-specific CNN at test time, on examples extracted solely from the input image itself. As such, it can adapt itself to different settings per image. This allows to perform SR of real old photos, noisy images, biological data, and other images where the acquisition process is unknown or non-ideal. On such images, our method outperforms SotA CNN-based SR methods, as well as previous unsupervised SR methods. To the best of our knowledge, this is the first unsupervised CNN-based SR method.   </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Super-Resolution (SR) from a single image has recently received a huge boost in performance using Deep-Learning based methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>. The recent SotA (State of the Art) method <ref type="bibr" target="#b12">[13]</ref> exceeds previous non-Deep SR methods (supervised <ref type="bibr" target="#b21">[22]</ref> or unsupervised <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>) by a few dBs -a huge margin! This boost in performance was obtained with very deep and well engineered CNNs, which were trained exhaustively on external databases, for lengthy periods of time (days or weeks). However, while these externally supervised <ref type="foot" target="#foot_0">1</ref> methods perform extremely well on data satisfying the conditions they were trained on, their performance deteriorates significantly once these conditions are not satisfied.</p><p>For example, SR CNNs are typically trained on highquality natural images, from which the low-resolution (LR) images were generated with a specific predefined downscaling kernel (usually a Bicubic kernel with antialiasing -MATLAB's default imresize command), without any distracting artifacts (sensor noise, non-ideal PSF, image compression, etc.), and for a predefined SR scaling-factor (usually ×2, ×3 or ×4; assumed equal in both dimensions). Fig. <ref type="figure">2</ref> shows what happens when these conditions are not satisfied, e.g., when the LR image is generated with a nonideal (non-bicubic) downscaling kernel, or contains aliasing effects, or simply contains sensor noise or compression artifacts. Fig. <ref type="figure">1</ref> further shows that these are not contrived cases, but rather occur often when dealing with real LR imagesimages downloaded from the internet, images taken by an iPhone, old historic images, etc. In those 'non-ideal' cases, SotA SR methods often produce poor results.</p><p>In this paper we introduce "Zero-Shot" SR (ZSSR), which exploits the power of Deep Learning, without relying on any prior image examples or prior training. We exploit the internal recurrence of information within a single image and train a small image-specific CNN at test time, on examples extracted solely from the LR input image itself (i.e., internal self-supervision). As such, the CNN can be adapted to different settings per image. This allows to perform SR on real images where the acquisition process is unknown and non-ideal (see example results in Figs. <ref type="figure">1 and 2</ref>). On 'non-ideal' images, our method outperforms externallytrained SotA SR methods by a large margin.</p><p>The recurrence of small pieces of information (e.g., small image patches) across scales of a single image, was shown to be a very strong property of natural images <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b23">24]</ref>. This formed the basis for many unsupervised image enhancement methods, including unsupervised SR <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>, Blind-SR <ref type="bibr" target="#b14">[15]</ref> (when the downscaling kernel is unknown), Blind-Deblurring <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b1">2]</ref>, Blind-Dehazing <ref type="bibr" target="#b2">[3]</ref>, and more. While such unsupervised methods can exploit image-specific information (hence are less subject to the above-mentioned supervised restrictions), they typically rely on simple Eucledian similarity of small image patches, of predefined size (typically 5 × 5), using K-  The unknown image-specific kernel is estimated directly from the LR test image using <ref type="bibr" target="#b14">[15]</ref>, and fed into our image-specific CNN as the downscaling kernel (note that externally-trained networks cannot make use of such image-specific information at test-time). Full sized images can be found on our project website. Quantitative evaluation on hundreds of 'non-ideal' LR images can be found in Sec. 4.2.</p><p>nearest-neighbours search. As such, they do not generalize well to patches that do not exist in the LR image, nor to new implicitly learned similarity measures, nor can they adapt to non-uniform sizes of repeating structures inside the image. Our image-specific CNN leverages on the power of the cross-scale internal recurrence of image-specific information, without being restricted by the above-mentioned limitations of patch-based methods. We train a CNN to infer complex image-specific HR-LR relations from the LR image and its downscaled versions (self-supervision). We then apply those learned relations on the LR input image to produce the HR output. This outperforms unsupervised patchbased SR by a large margin.</p><p>Since the visual entropy inside a single image is much smaller than in a general external collection of images <ref type="bibr" target="#b23">[24]</ref>, a small and simple CNN suffices for this image-specific task. Hence, even though our network is trained at test time, its train+test runtime is comparable to the test runtime of SotA supervised CNNs. Interestingly, our image-specific CNN produces impressive results (although not SotA) on the 'ideal' benchmark datasets used by the SotA supervised methods (even though our CNN is small and has not been pretrained), and surpasses SotA supervised SR by a large margin on 'non-ideal' images. We provide both visual and empirical evidence of these statements.</p><p>The term "Zero-Shot" used here, is borrowed from the domains of recognition/classification. Note however, that unlike these approaches for Zero-Shot Learning <ref type="bibr" target="#b22">[23]</ref> or One-shot Learning <ref type="bibr" target="#b18">[19]</ref>, our approach does not require any side information/attributes or any additional images. We may only have a single test image at hand, one of a kind, and nothing else. Nevertheless, when additional information is available and provided (e.g., the downscaling kernel can be estimated directly from the test image using <ref type="bibr" target="#b14">[15]</ref>), our image-specific CNN can make good use of this at test time, to further improve the results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The Power of Internal Image Statistics</head><p>Fundamental to our approach is the fact that natural images have strong internal data repetition. For example, small image patches (e.g., 5×5, 7×7) were shown to repeat many times inside a single image, both within the same scale, as well as across different image scales. This observation was empirically verified by <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b23">24]</ref> using hundreds of natural images, and was shown to be true for almost any small patch in almost any natural image.</p><p>Fig. <ref type="figure" target="#fig_2">3</ref> shows an example of a simple single-image SR based on internal patch recurrence (courtesy of <ref type="bibr" target="#b4">[5]</ref>). Note that it is able to recover the tiny handrails in the tiny balconies, since evidence to their existence is found elsewhere inside this image, in one of the larger balconies. In fact, the only evidence to the existence of these tiny handrails exists internally, inside this image, at a different location and different scale. It cannot be found in any external database of examples, no matter how large this dataset is! As can be seen, SotA SR methods fail to recover this imagespecific information when relying on externally trained images. While the strong internal predictive-power is exemplified here using a 'fractal-like' image, the internal predictivepower was analyzed and shown to be strong for almost any natural image <ref type="bibr" target="#b4">[5]</ref>.</p><p>In fact, it was empirically shown by <ref type="bibr" target="#b23">[24]</ref> that the internal entropy of patches inside a single image is much smaller than the external entropy of patches in a general collection of natural images. This further gave rise to the observation that internal image statistics often provides stronger predictive-power than external statistics obtained from a general image collection. This preference was further shown to be particularly strong under growing uncertainty and image degradations (see <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b16">17]</ref> for details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Image-Specific CNN</head><p>Our image-specific CNN combines the predictive power and low entropy of internal image-specific information, with the generalization capabilities of Deep-Learning. Given a test image I, with no external examples available to train on, we construct an Image-Specific CNN tailored to solve the SR task for this specific image. We train our CNN on examples extracted from the test image itself. Such examples are obtained by downscaling the LR image I, to generate a lower-resolution version of itself, I ↓ s (where s is the desired SR scale factor). We use a relatively light CNN, and train it to reconstruct the test image I from its lower-resolution version I ↓ s (top part of Fig. <ref type="figure" target="#fig_3">4(b)</ref>). We then apply the resulting trained CNN to the test image I, now using I as the LR input to the network, in order to construct the desired HR output I ↑ s (bottom of Fig. <ref type="figure" target="#fig_3">4(b)</ref>). Note that the trained CNN is fully convolutional, hence can be applied to images of different sizes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evidence to the existence of those tiny handrails exists only internally, inside this image, at a different location and scale (in one of the larger balconies). Such evidence is not found in any external database of images (no matter how large it is).</head><p>Since our "training set" consists of one instance only (the test image), we employ data augmentation on I to extract more LR-HR example-pairs to train on. The augmentation is done by downscaling the test image I to many smaller versions of itself (I = I 0 , I 1 , I 2 , ..., I n ). These play the role of the HR supervision and are called "HR fathers". Each of the HR fathers is then downscaled by the desired SR scale-factor s to obtain the "LR sons", which form the input training instances. The resulting training set consists of many image-specific LR-HR example pairs. The network can then stochastically train over these pairs.</p><p>We further enrich the training set by transforming each LR-HR pair using 4 rotations (0 • , 90 • , 180 • , 270 • ) and their mirror reflections in the vertical and horizontal directions. This adds ×8 more image-specific training examples.</p><p>For the sake of robustness, as well as to allow large SR scale factors s even from very small LR images, the SR is performed gradually <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b20">21]</ref>. Our algorithm is applied for several intermediate scale-factors (s 1 , s 2 , ..., s m = s). At each intemediate scale s i , we add the generated SR image HR i and its downscaled/rotated versions to our gradually growing training-set, as new HR fathers. We downscale those (as well as the previous smaller 'HR examples') by the next gradual scale factor s i+1 , to generate the new LR-HR training example pairs. This is repeated until reaching the full desired resolution increase s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Architecture &amp; Optimization</head><p>Supervised CNNs, which train on a large and diverse external collection of LR-HR image examples, must capture in their learned weights the large diversity of all possible LR-HR relations. As such, these networks tend to be ex- We use a simple, fully convolutional network, with 8 hidden layers, each has 64 channels. We use ReLU activations on each layer. The network input is interpolated to the output size. As done in previous CNN-based SR methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b3">4]</ref>, we only learn the residual between the interpolated LR and its HR parent. We use L 1 loss with ADAM optimizer <ref type="bibr" target="#b10">[11]</ref>. We start with a learning rate of 0.001. We periodically take a linear fit of the reconstruction error and if the standard deviation is greater by a factor than the slope of the linear fit we divide the learning rate by 10. We stop when we get to a learning rate of 10 −6 .</p><p>Note that despite its limited receptive field, ZSSR is able to capture non-local recurrence of information inside the test image. E.g., when ZSSR is applied to the LR image of Fig. <ref type="figure" target="#fig_2">3</ref>, it trains a CNN to recover the handrail in the LR test image from its lower-res versions, even if no other handrail appears in its receptive field. When this CNN is then applied to the test image itself, it can recover new handrails elsewhere, due to using the same image-specific filters.</p><p>To accelerate the training stage and make the runtime independent of the size of the test image I, at each iteration we take a random crop of fixed size from a randomly-selected father-son example pair. The crop is typically 128×128 (unless the sampled image-pair is smaller). The probability of sampling a LR-HR example pair at each training iteration is set to be non-uniform and proportional to the size of the HR-father. The closer the size-ratio (between the HR-father and the test image I) is to 1, the higher its probability to be sampled. This reflects the higher reliability of non-synthesized HR examples over synthesize ones.</p><p>Lastly, we use a method similar to the geometric self-ensemble proposed in <ref type="bibr" target="#b12">[13]</ref> (which generates 8 different outputs for the 8 rotations+flips of the test image I, and then combines them). We take the median of these 8 outputs rather than their mean. We further combine it with the back-projection technique of <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b4">5]</ref>, so that each of the 8 output images undergoes several iterations of back-projection and finally the median image is corrected by back-projection as well.</p><p>Runtime: Although training is done at test time, the average runtime for SRx2 is only 9 sec on Tesla V100 GPU or 54 sec on K-80 (average taken on BSD100 dataset). This runtime is almost independent of the image size or the relative SR scale-factor s (this is a result of the equally sized random crops used in training; the final test runtime is negligible with respect to training iterations).</p><p>For the ideal case we use a gradual increase in resolution. For example, a gradual increase using 6 intermediate scale-factors typically improves the PSNR by ∼0.2dB, but increases the runtime to ∼1 min per image (on V100). There is therefore a tradeoff between runtime and the output quality, which is up to the user to choose.</p><p>For compariosn, the test-time of leading EDSR+ <ref type="bibr" target="#b12">[13]</ref> grows quadratically with the image size. While it is fast on small images, for a 800×800 image it performs <ref type="bibr" target="#b4">5</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Adapting to the Test Image</head><p>When the acquisition parameters of the LR images from their HR ones are fixed for all images (e.g., same downscaling kernel, high-quality imaging conditions), current supervised SR methods achieve an incredible performance <ref type="bibr" target="#b19">[20]</ref>. In practice, however, the acquisition process tends to change from image to image, since cameras/sensors differ (e.g., different lens types and PSFs), as well as the individual imaging conditions (e.g., subtle involuntary camera shake when taking the photo, poor visibility conditions, etc). This results in different downscaling kernels, different noise characteristics, various compression artifacts, etc. One could not practically train for all possible image acquisition configurations/settings. Moreover, a single supervised CNN is unlikely to perform well for all possible types of degradations/settings. To obtain good performance, one would need many different specialized SR networks, each trained (for days or weeks) on different types of degradations/settings. This is where the advantage of an image-specific network comes in. Our network can be adapted to the specific degradations/settings of the test image at hand, at test time. Our network can receive from the user, at test time, any of the following parameters: (i) The desired downscaling kernel (when no kernel is provided, the bicubic kernel serves as a default). (ii) The desired SR scale-factor s. (iii) The desired number of gradual scale increases (a tradeoff between speed and quality -the default is 6). (iv) Whether to enforce Backprojection between the LR and HR image (the default is 'Yes'). (v) Whether to add 'noise' to the LR sons in each LR-HR example pair extracted from the test image (default is 'No').</p><p>The last 2 parameters (cancelling the Backprojection and adding noise) allow to handle SR of poor-quality LR images (whether due to sensor noise in the image, JPEG compression artifacts, etc.) We found that adding a small amount of Gaussian noise (with zero mean and a small standarddeviation of ∼5 grayscales), improves the performance for a wide variety of degradations (Gaussian noise, speckle noise, JPEG artifacts, and more). We attribute this phenomenon to the fact that image-specific information tends to repeat across scales, whereas noise artifacts do not <ref type="bibr" target="#b24">[25]</ref>. Adding a bit of synthetic noise to the LR sons (but not to their HR fathers) teaches the network to ignore uncorrelated crossscale information (the noise), while learning to increase the resolution of correlated information (the signal details).</p><p>Indeed, our experiments show that for low-quality LR images, and for a wide variety of degradation types, the image-specific CNN obtains significantly better SR results than SotA EDSR+ <ref type="bibr" target="#b12">[13]</ref> (see Sec. 4). Similarly, in the case of non-ideal downscaling kernels, the image-specific CNN obtains a significant improvement over SotA (even in the absence of any noise). When the downscaling kernel is known (e.g., a sensor with a known PSF), it can be provided to our network. When the downscaling kernel is unknown (which is usually the case), a rough estimate of the kernel can be computed directly from the test image itself (e.g., using the method of <ref type="bibr" target="#b14">[15]</ref>). Such rough kernel estimations suffice to obtain +1dB improvement over EDSR+ on nonideal kernels (see examples in Figs. <ref type="figure">1 and 2</ref>, and empirical evaluations in Sec. 4).</p><p>Note that providing the estimated downscaling kernel to externally-supervised SotA SR methods at test time, would be of no use. They would need to exhaustively re-train a new network on a new collection of LR-HR pairs, generated with this specific (non-parametric) downscaling kernel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments &amp; Results</head><p>Our method (ZSSR -'Zero-Shot SR') is primarily aimed at real LR images obtained with realistic (unknown and varying) acquisition settings. Real LR images have no HR ground truth, hence are evaluated visually (as in Fig. <ref type="figure">1</ref>). In order to quantitatively evaluate ZSSR's performance, we ran several controlled experiments on a variety of settings. Interestingly, ZSSR produces competitive results (although VDSR <ref type="bibr" target="#b8">[9]</ref> EDSR+ <ref type="bibr" target="#b12">[13]</ref> Blind-SR <ref type="bibr" target="#b14">[15]</ref> ZSSR [estimated kernel] (ours) ZSSR [true kernel] (ours) 27.7212 / 0.7635 27.7826 / 0.7660 28.420 / 0.7834 28.8118 / 0.8306 29.6814 / 0.8414 Table <ref type="table">2</ref>: SR in the presence of unknown downscaling kernels. LR images were generated from the BSD100 dataset using random downscaling kernels (of reasonable size). SR×2 was then applied to those images. Please see text for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground Truth VDSR [9]</head><p>EDSR+ <ref type="bibr" target="#b12">[13]</ref> ZSSR (ours) (PSNR, SSIM) (20.11, 0.9136) (25.29 / 0.9627) (25.68 / 0.9546) Figure <ref type="figure">5</ref>: In images with strong internal repetitive structures, ZSSR tends to surpass VDSR, and sometimes also EDSR+, even though the LR image was generated using the 'ideal' supervised setting (i.e., bicubic downscaling).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bicubic interpolation</head><p>EDSR+ <ref type="bibr" target="#b12">[13]</ref> ZSSR (ours) 27.9216 / 0.7504 27.5600 / 0.7135 28.6148 / 0.7809 Table <ref type="table">3</ref>: SR in the presence of unknown image degradation. Each LR image from the BSD100 dataset was randomly degraded using one of 3 types of degradations: (i) Gaussian noise, (ii) Speckle noise, (iii) JPEG compression. SR×2 was then applied to those images, without knowing the type of degradation. ZSSR shows robustness to unknown degradations, whereas SotA SR methods are not. In fact, under such conditions, bicubic interpolation outperforms current SotA SR methods.</p><p>not SotA) on the 'ideal' benchmark datasets for which the SotA supervised methods train and specialize (even though our CNN is small, and has not been pretrained). However, on 'non-ideal' datasets, ZSSR surpasses SotA SR by a large margin. All reported numerical were produced using the evaluation script of <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">The 'Ideal' Case</head><p>While this is not the aim of ZSSR, we tested it also on the standard SR benchmarks of 'ideal' LR images. In these benchmarks, the LR images are ideally downscaled from their HR versions using MATLAB's imresize command (a bicubic kernel downsampling with antialiasing). Table <ref type="table" target="#tab_1">1</ref> shows that our image-specific ZSSR achieves competitive results against externally-supervised methods that were exhaustively trained for these conditions. In fact, ZSSR is significantly better than the older SRCNN <ref type="bibr" target="#b3">[4]</ref>, and in some cases achieves comparable or better results than VDSR <ref type="bibr" target="#b8">[9]</ref> (which was the SotA until a year ago). Within the unsupervised-SR regime, ZSSR outperforms the leading method SelfExSR <ref type="bibr" target="#b6">[7]</ref> by a large margin.</p><p>Moreover, in images with very strong internal repetitive structures, ZSSR tends to surpass VDSR, and sometimes also EDSR+, even though these LR images were generated using the 'ideal' supervised setting. One such example is shown in Fig. <ref type="figure">5</ref>. Although this image is not a typ-ical natural image, further analysis shows that the preference for internal learning (via ZSSR) exhibited in Fig. <ref type="figure">5</ref> exists not only in 'fractal-like' images, but is also found in general natural images. Several such examples are shown in Fig. <ref type="figure" target="#fig_4">6</ref>. As can be seen, some of the pixels in the image (those marked in green) benefit more from exploiting internally learned data recurrence (ZSSR) over deeply learned external information, whereas other pixels (those marked in red) benefit more from externally learned data (EDSR+). As expected, the internal approach (ZSSR) is mostly advantageous in image area with high recurrence of information, especially in areas where these patterns are extremely small (of extremely low resolution), like the small windows in the top of the building. Such tiny patters find larger (highres) examples of themselves elsewhere inside the same image (at a different location/scale). This indicates that there may be potential for further SR improvement (even in the 'ideal' bicubic case), by combining the power of Internal-Learning with External-Learning in a single computational framework. This remains part of our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">The 'Non-ideal' Case</head><p>Real LR images do not tend to be ideally generated. We have experimented with non-ideal cases that result from either: (i) non-ideal downscaling kernels (that deviate from the bicubic kernel), and (ii) low-quality LR images (e.g., due to noise, compression artifacts, etc.) In such non-ideal cases, the image-specific ZSSR provides significantly better results than SotA SR methods (by 1 − 2dB). These quantities experiments are described next. Fig. <ref type="figure">2</ref> shows a few such visual results. Additional visual results and full images can be found in our project website.</p><p>(A) Non-ideal downscaling kernels:</p><p>The purpose of this experiment is to test more realistic blur kernels with the ability to numerically evaluate the results. For this purpose we created a new dataset from BSD100 <ref type="bibr" target="#b13">[14]</ref> by downscaling the HR images using random (but reasonably sized) Gaussian kernels. For each image, the covariance matrix Σ of its downscaling kernel was chosen to have a random angle θ and random lengths λ 1 , λ 2 in each axis:</p><formula xml:id="formula_0">λ 1 , λ 2 ∽ U [0, s 2 ], θ ∽ U [0, π], Λ = diag(λ 1 , λ 2 ) U = cos(θ) − sin(θ) sin(θ) cos(θ)</formula><p>, Σ = UΛU t where s is the HR-LR downscaling factor. Thus, each LR image was subsampled by a different random kernel. Table <ref type="table">2</ref> compares our against the leading externallysupervised SR methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b8">9]</ref>. We also compared our performance to the unsupervised Blind-SR method of <ref type="bibr" target="#b14">[15]</ref>. We considered two cases for applying ZSSR: (i) The more realistic scenario of unknown downscaling kernel. For this mode we used <ref type="bibr" target="#b14">[15]</ref> to evaluate the kernel directly from the test image and fed it to ZSSR. The unknown SR kernel is estimated in <ref type="bibr" target="#b14">[15]</ref> by seeking a non-parametric downscaling kernel which maximizes the similarity of patches across scales in the LR test image. (ii) We applied ZSSR with the true downscaling kernel used to create the LR image. Such a scenario is potentially useful for images obtained by sensors with known specs. Note that externally-supervised methods are unable to benefit from knowing the blur kernel of the test image (estimated or real), since they were trained and optimized exhaustively for a specific kernel. Table <ref type="table">2</ref> shows that ZSSR outperforms SotA methods by a large margin: +1db for unknown (estimated) kernels; +2db when provided the true kernels. Visually, the images generated by SotA SR methods are very blurry (see Fig. <ref type="figure">2</ref>, and project website). Interestingly, the unsupervised Blind-SR method of <ref type="bibr" target="#b14">[15]</ref>, which does not use deep learning, also outperforms SotA SR methods. This supports the analysis and observations of <ref type="bibr" target="#b17">[18]</ref>, that (i) an accurate downscaling model is more important than sophisticated image priors, and (ii) using the wrong donwscaling kernel leads to oversmoothed SR results. (using the code of <ref type="bibr" target="#b0">[1]</ref>). SRGAN obtains poor visual quality on 'non-ideal' LR images -Please zoom-in on screen.</p><p>A special case of a non-ideal kernel is the δ kernel, which results in aliasing. This case too, is not handled well by SotA methods (see example in Fig. <ref type="figure">2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(B) Poor-quality LR images:</head><p>In this experiment, we tested images with different types of quality degradation. To test the robustness of ZSSR in coping with unknown damage, we chose for each image from BSD100 <ref type="bibr" target="#b13">[14]</ref> a random type of degradation out of 3 degradations: (i) Gaussian noise [σ = 0.05], (ii) Speckle noise [σ = 0.05], (iii) JPEG compression [quality = 45 (By MATLAB standard)]. Table <ref type="table">3</ref> shows that ZSSR is robust to unknown degradation types, while these typically damage SR supervised methods to the point where bicubic interpolation outperforms current SotA SR methods! Comparison to SRGAN <ref type="bibr" target="#b11">[12]</ref>: SRGAN is also trained for the ideal case. In those cases, SRGAN methods tend to hallucinate visually pleasing information, hence score numerically worse than ZSSR (see Table <ref type="table" target="#tab_1">1</ref>). In the non-ideal case they further obtain very poor visual quality (see Fig. <ref type="figure" target="#fig_5">7</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We introduce the concept of "Zero-Shot" SR, which exploits the power of Deep Learning, without relying on any external examples or prior training. This is obtained via a small image-specific CNN, which is trained at test time on internal examples extracted solely from the LR test image. This yields SR of real-world images, whose acquisition process is non-ideal, unknown, and changes from image to image (i.e., image-specific settings). In such real-world 'nonideal' settings, our method substantially outperforms SotA SR methods, both qualitatively and quantitatively. To our best knowledge, this is the first unsupervised CNN-based SR method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Work funded in part by the Israel Science Foundation (Grant No. 931/14).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>2 EDSRFigure 1 :Figure 2 :</head><label>212</label><figDesc>Figure 1: SR of real images (unknown LR acquisition process). Real-world images rarely obey the 'ideal conditions' assumed by supervised SR methods. For example, old historic photos (a,c), images taken by smartphones (b), random images on the Internet (d), etc. Since ZSSR trains at test time on examples extracted from the test image, it is better at performing SR 'In-the-Wild' (i.e., in unconstrained and unknown settings). Full sized images can be found on our project website.</figDesc><graphic url="image-4.png" coords="2,240.97,421.47,266.31,224.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Internal predictive power of image-specific information. Simple unsupervised internal-SR<ref type="bibr" target="#b4">[5]</ref> is able to reconstruct the tiny handrail in the tiny balconies, whereas externally-trained SotA SR methods fail to do so. Evidence to the existence of those tiny handrails exists only internally, inside this image, at a different location and scale (in one of the larger balconies). Such evidence is not found in any external database of images (no matter how large it is).</figDesc><graphic url="image-7.png" coords="4,305.09,66.33,259.86,148.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Image-Specific CNN -"Zero-Shot" SR. (a) Externally-supervised CNNs are pre-trained on large external databases of images. The resulting very deep network is then applied to the test image I. (b) Our proposed method (ZSSR): a small image-specific CNN is trained on examples extracted internally, from the test image itself. It learns how to recover the test image I from its coarser resolutions. The resulting self-supervised CNN is then applied to the LR image I to produce its HR output.</figDesc><graphic url="image-8.png" coords="5,99.19,72.00,396.85,195.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Internal vs. External preference. Green: pixels that favor Internal-SR (i.e., pixels where ZSSR obtains lower error with respect to the ground-truth HR image); Red: pixels that favour External-SR (EDSR+).</figDesc><graphic url="image-10.png" coords="8,50.11,72.00,226.78,132.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Visual comparison of ZSSR to SRGAN [12](using the code of<ref type="bibr" target="#b0">[1]</ref>). SRGAN obtains poor visual quality on 'non-ideal' LR images -Please zoom-in on screen.</figDesc><graphic url="image-11.png" coords="8,312.36,72.00,226.78,146.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison of SR results for the 'ideal' case (bicubic downscaling).</figDesc><table><row><cell>times</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">We use the term "supervised" for any method that trains on externally supplied examples (even if their generation does not require manual labelling).</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="https://github.com/brade31919/srgan-tensorflow.8" />
		<title level="m">SRGAN-tensorflow code</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Non-uniform blind deblurring by reblurring</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bahat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Efrat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Blind dehazing using internal patch recurrence</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bahat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCP</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H X T</forename><surname>Chao Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
				<imprint>
			<date type="published" when="2007">2014. 1, 5, 6, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Super-resolution from a single image</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B D</forename><surname>Glasner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2005">2009. 1, 4, 5</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Image and video upscaling from local self-examples</title>
		<author>
			<persName><forename type="first">G</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Single image superresolution from transformed self-exemplars</title>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Improving resolution by image registration</title>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Peleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Graphical Model and Image Processing</title>
				<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="231" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Accurate image superresolution using very deep convolutional networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
				<imprint>
			<date type="published" when="2008">06 2016. 1, 5, 6, 7, 8</date>
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deeply-recursive convolutional network for image super-resolution</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR Oral)</title>
				<imprint>
			<date type="published" when="2007">June 2016. 1, 5, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>CoRR, abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.04802</idno>
		<imprint>
			<date type="published" when="2008">2016. 1, 6, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
				<imprint>
			<date type="published" when="2008">July 2017. 1, 2, 3, 5, 6, 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 8th Int&apos;l Conf. Computer Vision</title>
				<meeting>8th Int&apos;l Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2001-07">July 2001</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="416" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Nonparametric blind superresolution</title>
		<author>
			<persName><forename type="first">T</forename><surname>Michaeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2008">2013. 1, 3, 6, 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Blind deblurring using internal patch recurrence</title>
		<author>
			<persName><forename type="first">T</forename><surname>Michaeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Combining the power of internal and external denoising</title>
		<author>
			<persName><forename type="first">I</forename><surname>Mosseri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zontak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCP</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Accurate blur models vs. image priors in single image super-resolution</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A B N A L</forename><surname>Netalee Efrat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Glasner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Meta-learning with memory-augmented neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Ntire 2017 challenge on single image superresolution: Methods and results</title>
		<author>
			<persName><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
				<imprint>
			<date type="published" when="2017-07">July 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Seven ways to improve example-based single image super resolution</title>
		<author>
			<persName><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2004">June 2016. 4</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A+: Adjusted anchored neighborhood regression for fast super-resolution</title>
		<author>
			<persName><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">D</forename><surname>Smet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Zero-shot learning -the Good, the Bad and the Ugly</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Internal statistics of a single natural image</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zontak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2004">June 2011. 1, 3, 4</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Separating signal from noise using patch recurrence across scales</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zontak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mosseri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
