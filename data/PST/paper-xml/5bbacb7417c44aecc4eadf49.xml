<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Monocular Vision Sensor-Based Efficient SLAM Method for Indoor Service Robots</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Member, IEEE O</roleName><forename type="first">Dan</forename><forename type="middle">"</forename><surname>Cho</surname></persName>
							<email>dicho@snu.ac.kr</email>
						</author>
						<author role="corresp">
							<persName><forename type="first">Tae-Jae</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Chul-Hong</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<postCode>151-742</postCode>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Automation and Systems Research Institute</orgName>
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<postCode>151-744</postCode>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<postCode>151-742</postCode>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Interuniversity Semiconductor Research Center and Automation and Systems Research Institute</orgName>
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<postCode>151-744</postCode>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Monocular Vision Sensor-Based Efficient SLAM Method for Indoor Service Robots</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DA833734E7BA6F8CAD1E052A7ABFD664</idno>
					<idno type="DOI">10.1109/TIE.2018.2826471</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIE.2018.2826471, IEEE Transactions on Industrial Electronics</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T04:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Efficient simultaneous localization and mapping (SLAM)</term>
					<term>Embedded system</term>
					<term>Indoor service robot</term>
					<term>Monocular vision</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a new implementation method for efficient simultaneous localization and mapping using a forward-viewing monocular vision sensor. The method is developed to be applicable in real time on a low-cost embedded system for indoor service robots. In this study, the orientation of a robot is directly estimated using the direction of the vanishing point. Then, the estimation models for the robot position and the line landmark are derived as simple linear equations. Using these models, the camera poses and landmark positions are efficiently corrected by a local map correction method. The performance of the proposed method is demonstrated under various challenging environments using datasetbased experiments using a desktop computer and real-time experiments using a low-cost embedded system. The experimental environments include a real home-like setting. These conditions contain low-textured areas, moving people, or changing environments. The proposed method is also tested using the robotics advancement through web-publishing of sensorial and elaborated extensive data sets (RAWSEEDS) benchmark dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>NE of the goals in robotics is to develop a mobile robot that can act autonomously in the real world. Localization is the most important prerequisite for this goal. Without using a global positioning system (GPS) or pre-constructed ad-hoc infrastructures, localization can be performed using the sensors on board the robot <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. Among the various methods, the simultaneous localization and mapping (SLAM) technique provides an attractive solution because it does not need user-built maps. Within various sensors for SLAM, monocular vision is highly attractive because of its low cost, light weight, and low power consumption, especially for resource-constrained robotic platforms. In addition, it can capture rich visual information from the environment.</p><p>During the last two decades, the vision-based SLAM problem has been intensively researched. However, in order for these SLAM solutions to be practical for real applications such as consumer robots, some important factors need to be considered. First, the computational requirement of the SLAM algorithm should be low enough to be executed on a low-cost microprocessor. A microprocessor on a robot should execute SLAM in parallel with other algorithms such as path and motion planning in real-time to conduct various missions. The computational requirements of conventional SLAM algorithms are too high to be used on a low-cost processor. Second, the environment where a robot is used should be considered in developing and applying a SLAM method.</p><p>From the perspective of vision-based SLAM, the home environment is quite challenging. It contains plenty of less-textured areas. When the robot with a camera moves close to the obstacles or objects, all tracked features are easily lost because of occlusion and severe scale changes in the image domain. This situation occurs frequently for home-service robots, such as robotic vacuums, because they must move through every inch of the environment. Input images from robotic vacuums with forward-viewing monocamera that moves through every inch of the house contain only an average of 151 corner features at 320 × 240 image resolution per image (see Section IV-A for a detailed explanation of the home dataset). When compared with popular benchmark datasets, this is a very challenging situation for SLAM. Kitti (outdoor, sequence 00) <ref type="bibr" target="#b2">[3]</ref> and RAWSEEDS (indoor, sequence 25b Frontal) <ref type="bibr" target="#b3">[4]</ref>, which are popular benchmark datasets, contain an average of 4443 corners at 1241 × 376 image resolution per image (731 for 320 × 240 image size ratio) and 657 corners at 320 × 240 image resolution per image, respectively. For corner detection, algorithm <ref type="bibr" target="#b4">[5]</ref> with the threshold 20 is used. Another challenge is that the home environment is highly dynamic because of human activities. There are moving people and objects that affect the performance of the SLAM. In addition, the environment can be changed during the SLAM process. For example, the locations of the objects or the illumination can be changed. Fig. <ref type="figure" target="#fig_0">1</ref> shows these challenging situations, which are present in the home dataset in this study.</p><p>This paper presents an efficient implementation method for forward-viewing monocular vision-based SLAM. The method is applicable on a low-cost embedded system for indoor service robots, such as home service robots. We have assumed that the robotic motions are planar, and the images captured by a forward-viewing monocamera and odometer are used as sensory inputs. The proposed method is quite robust in challenging indoor environments, which contain low-textured areas, moving people, or changing environments. For robust performance in challenging indoor environments, the proposed method adopts the vanishing point (VP) and orthogonal structure assumptions, which are mainly inspired by the work of <ref type="bibr" target="#b5">[6]</ref>.</p><p>The main contribution of the paper is proposing a new implementation method using line features and VP for efficient SLAM. The details of the contribution are as follows. Firstly, VP is utilized in both line landmark parametrization and direct estimation of the robot orientation which improves the accuracy. Secondly, new estimation models for robot's translation and landmarks are newly derived in a simple form for efficient implementation. Finally, using these models, robot poses and landmarks are separately corrected during the local map correction process, which significantly reduces the computational requirement of the proposed method such that it can be installed on a low-cost embedded system and integrated in a real-time autonomous robot navigation system.</p><p>The rest of the paper is organized as follows. In Section II, we present studies related to this topic. The proposed method is described in Section III. In Section IV, we present the experimental result. This is followed by conducting remarks and plans for future research in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head><p>In the fields of robotics and computer vision, visual SLAM has been studied intensively. Researchers have mainly used feature points as a landmark for SLAM. Current SLAM methods can be classified into two categories: filtering-based methods <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> and optimization-based methods <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. In filtering-based methods, every frame is processed by the filter to jointly estimate the camera pose and landmark positions in a probabilistic way. On the other hand, optimization-based methods estimate the camera pose and landmark positions in a deterministic way, usually through numerical optimizations, called bundle adjustments. The representative work of this kind is parallel tracking and mapping (PTAM) <ref type="bibr" target="#b8">[9]</ref>. The PTAM method proposed the concept of tracking the camera pose and mapping the environment in two simultaneous threads. Recently, Mur-Artal et al. <ref type="bibr" target="#b9">[10]</ref> proposed an oriented FAST and rotated BRIEF (ORB) point-feature-based monocular SLAM called ORB-SLAM. This method used the same ORB features for all SLAM tasks: tracking, mapping, relocalization, and loop closing.</p><p>Various graph optimization methods in a least square sense for optimization-based SLAM methods are proposed. The g2o or iSAM2 are widely used methods <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>. Recently, Khosoussi et al. <ref type="bibr" target="#b12">[13]</ref> proposed an efficient graph optimization algorithm that take advantage of the separable structure of SLAM for reliable and faster convergence. To deal with false constraints problem from loop closure detections or data association in graph optimization Vertigo, DCS and Max-mixture methods were proposed <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b15">[16]</ref>.</p><p>Several studies have been presented to cope with dynamic environment. To deal with moving objects while performing SLAM, simultaneous localization, mapping and moving object tracking (SLAMMOT) was proposed <ref type="bibr" target="#b16">[17]</ref>. The method distinguishes stationary and moving points based on the difference of hypotheses, and augments those points into SLAM state vector. Tan et al. <ref type="bibr" target="#b17">[18]</ref> proposed a monocular vision sensor-based online keyframe representation and updating method to adaptively model the dynamic environments, where the appearance or structure changes can be effectively detected. Lee and Myung <ref type="bibr" target="#b18">[19]</ref> proposed an error metric and node grouping rules to detect low dynamic situations for RGB-D SLAM. Li and Lee <ref type="bibr" target="#b19">[20]</ref> proposed a static weighting method for edge points in RGB-D SLAM to reduce the influence of dynamic objects. Apart from these point methods, line feature-based SLAM is generally more robust for dynamic environments, because they are not steadily extracted in moving people or objects <ref type="bibr" target="#b5">[6]</ref>. Several studies about image analysis can be used to remove the moving objects from images in a dynamic environment. Li et al. <ref type="bibr" target="#b20">[21]</ref> proposed a foreground extraction algorithm by using superpixels. Li et al. <ref type="bibr" target="#b21">[22]</ref> proposed a foreground extraction algorithm by using path alignment manifold matting. Moving Objects can be excluded by removing the extracted foreground from the images.</p><p>Numerous studies have been presented on arbitrary 3D line feature-based SLAM <ref type="bibr" target="#b22">[23]</ref>- <ref type="bibr" target="#b24">[25]</ref>. These methods used the filtering method to formulate the SLAM problem where line segments are parameterized with two end points <ref type="bibr" target="#b22">[23]</ref> or as an infinite line <ref type="bibr" target="#b23">[24]</ref> in a small 3D space. Recently, Zhang et al. <ref type="bibr" target="#b24">[25]</ref> proposed a 3D line-based stereo SLAM system. The method used the ̈ line coordinates for line parameterization.</p><p>In the indoor environment, several studies have used ceiling line features as landmarks for SLAM. They usually require upward-viewing camera because data association, geometrical modeling, and implementation are much easier. In addition, they use corner features as additional landmarks for SLAM <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>. On the other hand, the proposed method uses a forward-viewing camera, which has much technical difficulties in feature extraction and tracking. Nevertheless, a forward- viewing method has significant commercial advantages in that the same camera can be used for obstacle avoidance, home monitoring, or as a user interface.</p><p>Several studies have applied VP information to attitude estimation and SLAM as is done in this study. These studies usually used the orthogonal structure assumption. Huttunen and Robert <ref type="bibr" target="#b27">[28]</ref> proposed a monocular camera-based orientation estimation method using the VPs. The method detects orthogonal VPs and estimates the three-axis orientation of the camera by using the assumption of structural regularity. Camposeco and Marc <ref type="bibr" target="#b28">[29]</ref> proposed a visual-inertial odometry method that used VPs to reduce angular drift in camera pose estimations. The method used direction of VP observations to update the orientation of the camera and the directions of the VP. Zhou et al. <ref type="bibr" target="#b5">[6]</ref> proposed visual SLAM using building structure lines called StructSLAM. In this method, the orthogonal structures and the directions of VPs are used for parameterizing the line landmarks. Ji et al. <ref type="bibr" target="#b29">[30]</ref> proposed a RGB-D SLAM using VP and door plate in an indoor environment. The method used VP and door plate as landmarks to increase the stability of the SLAM process. Lee et al. <ref type="bibr" target="#b30">[31]</ref> proposed an algorithmic compass which uses VP to estimate the heading information of a mobile robot in an indoor environment. Zhang et al. <ref type="bibr" target="#b31">[32]</ref> proposed a VP based loop closure method in line-based mono-SLAM. The method used VP to correct the heading angle and line landmark to correct poses.</p><p>In this study, we have utilized the VP to reduce the orientation error by assuming the structural regularity of the indoor environment. In addition, line landmarks are aligned with the directions of the VPs as in <ref type="bibr" target="#b5">[6]</ref>. Unlike the previous studies, the robot poses and landmarks are separately corrected during the local map correction process. In order to implement this method, the estimation model for the robot's orientation, robot's translation and landmarks are newly derived in a simple form. In addition, the robot angle measurements which are directly calculated from the VP are utilized in the local map correction method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head><p>In this work, the robot is assumed to move around the flat ground. As sensory inputs, images captured from a forward-viewing monocamera and odometry are used. The camera is slightly tilted 8.7° upward to provide various consumer services. The overall architecture of the proposed SLAM algorithm is shown in Fig. <ref type="figure">2</ref>. The proposed method runs three threads in parallel as in <ref type="bibr" target="#b9">[10]</ref> (i.e., tracking, mapping, and loop closing). To avoid redundant computation, new images are only captured when the robot moved more than a predefined distance or rotated more than a predefined angle from the previous frame based on the odometry data. The tracking thread extracts lines and VPs and estimates the robot's orientation from the VPs. From the data association of lines, line landmark observations are made, and the position of new landmarks are estimated. By using the VP and line landmarks, the proposed method can be made more robust for dynamic environments, because they are not steadily extracted in moving people or objects. The mapping thread creates new line landmarks using the estimated positions in the tracking thread and inserts them in a map database. Afterwards, local map correction is conducted on an active window of selected frames. The loop-closing thread finds large loops for every input frame. After a loop is detected, a pose graph is optimized using relative pose constraints between frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Manhattan Frame and System Initialization</head><p>Most indoor environments can be abstracted as blocks that are stacked together in three dominant directions, which are referred to as a Manhattan grid <ref type="bibr" target="#b32">[33]</ref>. This grid gives a natural reference frame for the viewer. The advantage of adopting the Manhattan world assumption is that both the robot's orientation error and the line landmark's orientation error can be eliminated. Under this assumption, the extracted VPs in the image plane correspond to the three dominant directions of the Manhattan grid.</p><p>From the first pose of the robot, the world frame is set according to the initial pose of the robot. At the system initialization step, the orientation of the Manhattan frame with respect to the world frame is estimated. To do this, the estimated angle of the Manhattan frame with respect to the world frame is averaged up to some predefined number. The initialization is conducted only when the variance of the orientation of the Manhattan frame with respect to the world frame is smaller than some predefined angle. In the initialization step, the robot poses are estimated using the odometry data only. The detailed method for estimating the angle between the robot and the Manhattan frame is explained in the next section. Usually, typical indoor environments can be modeled using one Manhattan frame, but sometimes multiple Manhattan frames are required (known as "Atlanta world"). The proposed method resolves this situation by setting up several Manhattan frames. The additional Manhattan frame is configured when a dominant direction of the VP is changed in a partitioned grid area.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Vanishing Point-Based Robot Orientation Estimation</head><p>To extract a VP, we extract the line segments using the line segment detector <ref type="bibr" target="#b33">[34]</ref>. For a robust estimation of the VP, we eliminate line segments with lengths less than 15 pixels because short line segments tend to be noisy observations. Then, we extract the VP using the algorithm proposed by Zhang et al. <ref type="bibr" target="#b34">[35]</ref>. Examples of VP extraction results in a typical home environment are illustrated in Fig. <ref type="figure">3</ref>. The images shown in the third row are failure cases when the robot moved under a sofa or the robot is too close to a wall. Although the VP extraction fails in these situations, the local map correction method automatically handles the situation (detailed explanation in Section III-E). In this study, VPs of horizontal lines in the image are used to estimate the orientation between the robot and the Manhattan frame. These VPs are reliable when there are several lines appearing at the top of the image. Hence, images without any horizontal lines at the top region of the image are skipped.</p><p>From the VP extraction, we obtain three dominant orthogonal line direction vectors with respect to the camera frame. Then the three direction vectors are transformed with respect to the robot frame. The robot rotates only with respect to its z-axis; therefore, the orientation of the robot with respect to the Manhattan frame can be calculated using the estimated VP direction vectors. First, from the three VP direction vectors, we find the one VP whose direction is closest to the y-axis of the robot frame. We call this vp is parallel or orthogonal to the Manhattan frame, which can be considered as a wall in front of the robot. Second, we compute the robot's orientation with respect to the Manhattan frame structure by projecting the direction vector ' R y vp to the x-y plane of the robot frame. Finally, we calculate the robot's orientation with respect to the world frame. The angle between the Manhattan frame and the world frame is known from the initialization; therefore, this angle is added to the robot's orientation in the Manhattan frame, resulting in the robot's orientation in the world frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Line Landmark Estimation</head><p>Considering the tilting angle t q of the camera, the camera matrix for point projection can be expressed as: q q q q q q q q q q q q ae ö ç</p><formula xml:id="formula_0">÷ = ç ÷ ç ÷ è ø - - ae ö ç ÷ × - - + ç ÷ ç ÷ - - è ø P ,<label>(1)</label></formula><p>where ( , )   x y f f , ( , )   x y c c , ( , )   c c</p><p>x y , c q , t s and t c are focal length of the camera, principal point of the camera in image domain, the x-y position of the camera, the orientation of the camera, sin t q , and cos t q , respectively. Using the camera matrix, the line projection model can be expressed as follows <ref type="bibr" target="#b35">[36]</ref>: </p><formula xml:id="formula_1">2 3 2 2 1 3 1 3 1 2 1 2 1 2 3 ( ) ( ) ( )( ) ( ) ( ) ( )( ) ( ) ( ) ( )( ) T T T T T T T T T T T T l l l ae ö × × × - × × ae ö ç ÷ ç ÷ = = × × × - × × ç ÷ ç ÷ ç ÷ ç ÷ × × × - × × è ø è ø</formula><formula xml:id="formula_2">% % % % % % % % % % % % % , (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>where l % is the projected line in an image plane; ( ) ( )</p><formula xml:id="formula_4">1 2</formula><p>1 and 1 ( ) ( )</p><formula xml:id="formula_5">T T v v v v v v x y z x y z = = a b % % , (<label>3</label></formula><formula xml:id="formula_6">1 2</formula><p>1 and 1</p><formula xml:id="formula_7">T T hx hx hx hx hx hx x y z x y z = = a b % % ,<label>(4)</label></formula><p>where hx y and hx z are variables that we estimate during the SLAM. Similarly, the end points hy a % and hy b % of the y-axis horizontal line can be expressed as follows:</p><formula xml:id="formula_8">( ) (<label>)</label></formula><formula xml:id="formula_9">1 2</formula><p>1 and 1 x and hy z are variables that we estimate during the SLAM.</p><p>Using the line projection model of ( <ref type="formula" target="#formula_2">2</ref>) and the previously defined landmark parameterization from (3) to (5), the equation for the position estimation of landmarks can be derived as a simple linear model. The projection equation of a vertical line is as follows: </p><formula xml:id="formula_10">( ) ( ) ( ) v v v v v v v v l a l a x l b l b y l c l c - + - = - ,<label>(6) where</label></formula><p>Similarly, the projection equation of the x-axis horizontal line is as follows:</p><formula xml:id="formula_12">2 ,<label>1 3 ,2 2 ,1 3 ,2 3 ,1 2 ,2 ( ) ( ) (</label></formula><p>)  </p><formula xml:id="formula_13">hx hx hx hx hx hx hx hx l a l a y l b l b z l c l c - + - = - ,<label>(8) where</label></formula><p>Finally, the projection equation of the y-axis horizontal line is as follows:  </p><formula xml:id="formula_15">2 ,<label>1 3 ,2 2 ,1 3 ,2 3 ,1 2 ,2 ( ) ( ) (</label></formula><p>The position of the landmarks can be easily estimated from the line matching results obtained from different locations of the robot because ( <ref type="formula" target="#formula_10">6</ref>), ( <ref type="formula" target="#formula_13">8</ref>) and ( <ref type="formula">10</ref>) are linear. For data association of line features, the normalized image patch of size 11 × 11 pixels around the midpoint of the extracted line is used for computational efficiency. When the same line is matched over several images, the position of the line can be calculated using the linear least-squares method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Camera Position Estimation</head><p>From the extracted line landmark with a known position, the camera position ( )</p><formula xml:id="formula_17">T c c</formula><p>x y = x can be estimated. These estimates of the camera x-y positions are used in the local map correction step in the mapping thread. Using the previously defined landmark parameterization and the line projection model, we can obtain the equations for estimating the camera position. Using the vertical line projection, the camera position can be expressed as follows: q q q q q q q q q q q q = + -</p><formula xml:id="formula_18">1 3 1 3 3 1 ( ) ( ) v v c v v c v</formula><formula xml:id="formula_19">= = - - - = = - + = + + + - - .<label>(13)</label></formula><p>Similarly, using the x-axis horizontal line projection, the camera position can be expressed as follows: </p><formula xml:id="formula_20">3 2 2 3 ( ) hx hx c hx hx a l b l y c l d l × -× = × -× ,<label>(14) where</label></formula><formula xml:id="formula_21">f f c f c s c f c s f f c y f f s f c c f c z d f s y f c z q q q q = = - = - + + - = - - .<label>(15)</label></formula><p>Finally, using the y-axis horizontal line projection, the camera position can be expressed as follows:  </p><formula xml:id="formula_23">a f s b f c s f f c c f f c f c s x f c f f s f c c z d f s x f c z q q q q = - = - = - - + + = + . (<label>17</label></formula><formula xml:id="formula_24">)</formula><p>Using these three types of extracted line landmarks, the camera position can be estimated using the linear least squares method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Local Map Correction</head><p>The local map correction process corrects the estimates of a bundle of camera poses and landmark positions. It is only conducted for the recent N frames and the observed line landmarks at the corresponding frames. When measurements from the camera (i.e., robot orientation from the VP and robot x-y position measurement) are not valid because of low-textured areas, occlusions, or changing environments, the local map correction is skipped. Subsequently, when valid measurements are available, local map correction is conducted, which includes the skipped frames. The flowchart of the local map correction process is shown in Fig. <ref type="figure" target="#fig_11">5</ref>.</p><p>The motion-only bundle adjustment 1 corrects the camera poses using the odometry measurements and the VP-based orientation measurements obtained in the tracking thread. The cost function to be minimized is formulated as follows:  x x y y</p><formula xml:id="formula_25">2 2 , , , ,<label>, 1 , , ( , , ) || (</label></formula><formula xml:id="formula_26">( )) || ( ( )) odo c s c k o i c i c i i VP s i i s i i E R z q q - S = - + -- å å x x z x x L ! ,<label>(18)</label></formula><p>x x y y</p><formula xml:id="formula_27">q q q q q q é ù - + - ê ú = = -- + - ê ú ê ú - ë û z x x ! . (<label>19</label></formula><formula xml:id="formula_28">)</formula><p>The Levenberg-Marquart algorithm using a g2o framework <ref type="bibr" target="#b10">[11]</ref> is executed to minimize <ref type="bibr" target="#b17">(18)</ref>. We can obtain the estimates of the bundle of camera poses from the motion-only bundle adjustment 1 step; therefore, the accuracy of the corresponding landmark position estimation can be improved. In this regard, line position is re-estimated using the method proposed in Section III-C. The next step is to calculate the x-y position of cameras using the method given in Section III-D followed by the motion-only bundle adjustment 2. The cost function to be minimized is formulated as follows: is the covariance matrix of the x-y position measurement. xy å is determined proportional to the inverse of the residual error for estimating the x-y position. Finally, the line position is again estimated using the method proposed in Section III-C.</p><formula xml:id="formula_29">2 , , , ,<label>, 1 2 2 , , , , ( , , ) || (</label></formula><formula xml:id="formula_30">( )) || ( ( )) || ( ) || odo xy c s c k o i c i c i i i VP s i i s xy i xy i i i E R z q q - S S = - + - - + - å å å x x z x x z x L ! ,<label>(20) where 3</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Whole Image Descriptor-Based Loop Detection</head><p>The loop-closing thread finds large loops for every input frame. The state-of-the-art SLAM methods usually adopt the loop-detection technique based on bag-of-visual-words (BoVW) <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b24">[25]</ref>. To convert an input image into BoVW representations for loop detection, the system has to load a huge vocabulary tree (e.g. 250 Mb memory for DBoW2 <ref type="bibr" target="#b36">[37]</ref>). As an alternative, the BRIEF-Gist <ref type="bibr" target="#b37">[38]</ref> method is used in this work considering the applicability to a low-cost embedded system. To robustly detect the revisited place even under the viewpoint change, we extract three BRIEF-Gist descriptors from a single image, that is, the left 80% of the image, the right 80% of the image, and the original image as in <ref type="bibr" target="#b38">[39]</ref>. When comparing two images captured from the two places, three descriptors are compared with one another. For fast comparison, we also adopt data structure as proposed in <ref type="bibr" target="#b38">[39]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Global Pose Correction</head><p>To close the loop, a pose graph optimization is performed after the loop detection. Before the pose optimization, the relative position is computed between the matched frame from the loop detection and the current frame using the method proposed in Section III-D. The pose graph optimization is conducted for all poses between the matched frame from the loop detection and the current frame. The cost function to be minimized is as follows:  ; incre S is the covariance matrix of the incremental pose estimation; and LD å is the covariance matrix of the relative pose from the loop detection. The covariance matrices are determined proportional to the inverse of the co-visibility of line landmarks. In case there is low co-visibility between incremental poses, we set the upper bound to determine the covariance matrix. After the optimization, each map line is transformed according to the correction of each corresponding frame that observes it.</p><formula xml:id="formula_31">E - S = S = - + - å x x z x x z x x ! ! ,<label>(21) where 3</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>Datasets-based experiments on a desktop computer and real-time experiments on an embedded system are performed. Experiments are conducted using the home datasets which we acquired and the public RAWSEEDS benchmark dataset <ref type="bibr" target="#b3">[4]</ref>. For real-time embedded experiments, the proposed algorithm is implemented in NXP4330Q embedded board (NEXELL Co., Republic of Korea), and experiments are conducted in a home environment. In our comparative experiments, we compare the proposed method with three approaches as given below.</p><p>First, a 2D version of ORB-SLAM <ref type="bibr" target="#b9">[10]</ref> method is implemented. There are several differences between the original ORB-SLAM and the implemented 2D version of ORB-SLAM. First, the camera poses are parameterized in the 2D space. Second, the odometry data is used to calculate the initial pose of the robot in the tracking thread. The odometry data are used as an edge in the local bundle adjustment step. This algorithm is denoted as ORB-SLAM 2D in the following.</p><p>Second, the VP-based line SLAM with the standard bundle adjustment method is implemented. The overall algorithm is very similar to the proposed method. The line extraction, VP extraction, data association, line parameterization, line observation model, line initialization, and loop closing methods are the same as those in the proposed method. In the tracking thread, the VP-based robot orientation estimation procedure is skipped. In the mapping thread, the standard local bundle adjustment is conducted. The cost function to be minimized is formulated as follows:</p><formula xml:id="formula_32">2 2 , ,<label>1 , ( , , , ) || ( ( )) || ( ,</label></formula><formula xml:id="formula_33">)</formula><formula xml:id="formula_34">odo line c s l o i i i i j i j i i j E h - S S = - + - å åå x l z x x z x l L L ! ,<label>(22)</label></formula><p>where 3 j Î l ¡ is the position of the landmark, which is one of the three lines (i.e., the vertical line, the x-axis horizontal line, or the y-axis horizontal line); 3 , i j Î z ¡ is the measurement of line landmark in the image plane; and ( ) h × is the line projection model. This version of implementation is intended to examine the effect of the proposed estimation method for VP-based robot orientation and the local map correction method compared with the standard optimization-based method. This algorithm is denoted as VP standard BA in the following. Third, the VP-based orientation correction method without any other process is implemented. This version estimates VP-based robot orientation only in the tracking thread and conducts motion-only bundle adjustment 1 in the mapping thread with no loop-closing thread. This version is intended to examine the performance of the proposed VP-based orientation estimation method only. This method is denoted as VP-only in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Home Dataset</head><p>Home datasets are acquired in a typical home environment, as shown in Fig. <ref type="figure">6</ref>. The robot explored the experimental environment in a trajectory greater than 400 m while collecting images from a forward-viewing monocamera, along with the robot odometry data. These steps are performed four times with different conditions. The characteristics of each home datasets are summarized in Table <ref type="table" target="#tab_8">I</ref>. The sample images of the dynamic environment situations are illustrated in Figs. <ref type="figure" target="#fig_0">1(c</ref>)-1(g). For our robotic platform, a robotic vacuum is used as shown in Fig. <ref type="figure">7</ref>. An upward-viewing monocamera is used for autonomous navigation <ref type="bibr" target="#b26">[27]</ref> while acquiring the datasets. As a pathplanning strategy, the Boustrophedon method <ref type="bibr" target="#b39">[40]</ref> is used to cover the whole environment, and ultrasonic sensors are used to avoid obstacles. At the end of the drive, we manually returned the robot to the starting point using the remote controller. Ideally, the estimated final pose of the robot must be the origin, that (0, 0, 0 ) T °. We use the distance between the estimated final position and the origin to quantitatively measure the accuracy of the SLAM algorithm, and we call this error as closed-loop error.</p><p>The algorithms are tested on a desktop computer with Intel     Core i7-2600. Fig. <ref type="figure">8</ref> shows the result of the proposed SLAM from the home dataset 3 (dynamic environment). Fig. <ref type="figure" target="#fig_16">9</ref> shows the estimated robot trajectories for various methods for the home dataset 3. For the ORB-SLAM 2D case, a large error drift occurred. This is mainly due to the error drift in the low-textured areas of the environment and the difficulty in feature tracking. Furthermore, no large loop closure between the early stage and the last stage occurred. The loop closure occurred only twice between the 1035 frame and the 1206 frame, and between the 3060 frame and the 6320 frame. On the other hand, in the proposed method, the loop closure occurred six times. The examples of the matched frames from the loop closure detection are illustrated in Fig. <ref type="figure" target="#fig_15">10</ref>. Clearly, the orientation estimation of the proposed method is more accurate than that of the VP Standard BA method. The difference lies in the fact that the robot's orientation is corrected directly from the VP in the proposed method. The robot's orientation is corrected by extracting the landmarks in the VP Standard BA method. Although the orientation error in the line landmark estimation can be eliminated, the accuracy of the VP Standard BA method reduces when the extraction of the line landmark is limited in low-textured areas. In case of VP-only, the orientation error is effectively eliminated, but the integrated translation error exists. Table <ref type="table" target="#tab_8">II</ref> shows the measured closed-loop error of the various methods for the four home datasets. The proposed method shows the lowest closed-loop error compared with other methods. Especially, the average of the closed-loop errors of the proposed method is 6.1 times lower than that of VP Standard BA method. With respect to the computational speed, the average running time for processing a single frame of each thread is measured and summarized in Table <ref type="table" target="#tab_10">III</ref>. The difference between the proposed method and VP Standard BA is mainly in the mapping thread. The proposed local map correction method is 3.6 times faster than the standard bundle adjustment while showing better accuracy. The ORB-SLAM 2D shows the slowest result.</p><p>After finishing the SLAM, the total memory usages are measured. The average memory usages are summarized in Table <ref type="table" target="#tab_11">IV</ref> for the four methods. The memory usage for the VP-only method is the lowest because it does not estimate the landmark. For ORB-SLAM 2D case, it requires more than 250 MB to load the vocabulary tree at the start of the SLAM. In the experiments, the ORB-SLAM 2D method have reconstructed an average of 25,892 point features, whereas the proposed method have reconstructed an average of 1,072 line features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. RAWSEEDS Benchmark Dataset</head><p>To evaluate the accuracy in large-scale indoor environments, experiments are conducted using the RAWSEEDS benchmark dataset (Biccoca25b) <ref type="bibr" target="#b3">[4]</ref>. The dataset is collected in an office building by a wheeled robot with multiple sensors, including laser range finders, cameras, an inertial measurement unit, and wheel encoders. The dataset also provides ground truth data of the robot's pose for evaluation. In this experiment, the image sequences from the frontal camera and odometry data are used. The dataset consists of 52,695 images, and the whole path is approximately 774 m long. Even though this dataset contains low-textured areas and dark corridors in some areas, it has sufficient features in comparison with the home dataset. In addition, the path contains several loops and revisited regions. Absolute position errors for whole robot trajectories are compared in Table <ref type="table" target="#tab_12">V</ref>. The average of the absolute position error for the whole trajectory of the proposed method ORB-SLAM 2D, VP Standard BA, and VP-only are 0.71, 2.51, 0.88, and 2.43m, respectively. Interestingly, the VP-only method is quite accurate with no effort required to execute complicated landmark estimation procedures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Embedded Real-Time SLAM in Home Environment</head><p>The proposed SLAM algorithm is implemented in a   low-cost embedded board of NXP4330Q, which is equipped with a Cortex A9 processor and a 512 MB memory. Fig. <ref type="figure" target="#fig_17">11</ref> illustrates the robot platform equipped with the NXP4330Q board. The proposed method is integrated in an autonomous robot navigation system. When the robot explores the environment, the obstacle grid map is constructed using the localization result from the proposed SLAM and the detected obstacles from the ultrasonic sensor. This obstacle grid map is used in path and motion planning for autonomous robot navigation. The Boustrophedon method <ref type="bibr" target="#b39">[40]</ref> is used for the path planning strategy. All services, including data acquisition, the proposed SLAM, navigation, and motion planning are simultaneously executed in the NXP4330Q board in real time.</p><p>Considering the limited computational resources, images are captured when the robot is moved more than 30 cm or rotated more than 30° compared with the previous frame based on the odometry data. The driving and rotation velocities of the mobile robot are 0.35 m/s and 30°/s, respectively. At the end of the drive, we manually returned the robot to the starting point along with the remote controller for measuring the closed-loop error. Similar to the home dataset-based experiments in the previous section, we have performed real-time SLAM experiments four times. The scenarios of the experiments are the same as those for acquiring home datasets. Fig. <ref type="figure" target="#fig_18">12</ref> illustrates the generated obstacle grid map in the real-time experiment sequence 3, which is a dynamic environment. The yellow grid indicates the area where the robot has driven. The blue and pink grids indicate the detected wall and obstacle from the ultrasonic sensor, respectively. Compared with the blueprint of the environment in Fig. <ref type="figure">6</ref>, the map is accurately built using the proposed method. Table <ref type="table" target="#tab_13">VI</ref> shows the measured closed-loop error of the real-time SLAM experiments for the four sequences. The accuracy is similar to the dataset-based experimental results where the algorithm is executed in the desktop PC. For the computation time, the average time for the embedded processor to process one frame for tracking, mapping, and loop-closing thread are 243.2, 211.8, and 138.1 ms, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS AND FUTURE WORK</head><p>This paper presented a new implementation method for efficient SLAM for low-cost indoor service robots. The estimation model for the robot's orientation and translation were separately derived in simple equations using VP and the line landmark, respectively. Using these models, the camera poses and landmark positions can be efficiently corrected by a local map correction process. When the robot revisits the previously mapped areas, a loop-detection procedure and a pose correction procedure are performed to obtain more accurate SLAM results. The performance of the proposed method was demonstrated under various challenging environments, which contained low-textured areas, moving people, and changing environments.</p><p>Although the proposed method was demonstrated using a production home service robot in various environments, additional studies may be necessary for actual implementation. First, we plan to investigate the case where the three dominant directions are not perpendicular to one another. Second, we also plan to conduct research on the applicability of the developed method to large-scale indoor environments such as hospitals, schools or museums.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Challenging situations from the home dataset in this work. Images are captured from a robotic vacuum with a forward-viewing monocular vision sensor. (a)-(b) Less-textured areas. (c)-(d) Moving people and object. (e) Moving person who hang out the wash. (f) Changing illumination when a person turns off the light. (g) Changing environment where a person pull down a roller blind. (h) No visual information when robot is too close to a wall. (i) No visual information when robot moved under a sofa.</figDesc><graphic coords="2,78.08,87.38,176.67,156.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .Fig. 3 .</head><label>23</label><figDesc>Fig. 2. Flowchart of the proposed SLAM algorithm.</figDesc><graphic coords="3,336.06,420.38,179.54,168.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>is shown by a red arrow in Fig. 4. The direction vector ' R y</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>two end points of the line; and iT p is the ith row vector of camera matrix 3 4 Î P ¡ . As landmarks for SLAM, three types of line features are used in this work. These are composed of the vertical line, the x-axis horizontal line, and the y-axis horizontal line with respect to the Manhattan frame. To parameterize the line landmarks, both end points are used. For the vertical line, the end points v a % and v b % can be expressed as follows:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Schematic of mobile robot coordinate system in Manhattan grid</figDesc><graphic coords="4,317.35,87.39,216.24,137.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>y f c c y f c x f c y f f s x f c c x</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>¡Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Flowchart of the proposed local map correction process.</figDesc><graphic coords="5,346.73,597.19,155.08,136.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>,</head><label></label><figDesc>xy i Î z ¡ is the x-y position measurement at the ith camera obtained from the line matching results; of the x-y position at ith camera; and xy å</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 6 .Fig. 7 .Fig. 8 .</head><label>678</label><figDesc>Fig. 6. Blueprint of home environment and example images</figDesc><graphic coords="7,88.75,320.65,155.32,109.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Examples of matched frames from loop closure in home dataset 3.</figDesc><graphic coords="8,88.99,218.28,154.84,175.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Estimated robot trajectories of various methods using the home dataset 3.</figDesc><graphic coords="8,52.17,85.47,489.82,119.51" type="vector_box" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Robot platform equipped with NXP4330Q board for real time SLAM experiment.</figDesc><graphic coords="9,343.02,243.21,164.91,117.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Fig. 12 .</head><label>12</label><figDesc>Fig.12. Generated obstacle grid map for autonomous navigation using the proposed SLAM on NXP4330Q board in home environment (sequence 3).TABLE VI CLOSED LOOP ERROR OF REAL-TIME SLAM EXPERIMENTS ON EMBEDDED</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>)</figDesc><table><row><cell>hy l a</cell><cell>-</cell><cell>hy l a</cell><cell>hy x</cell><cell>+</cell><cell>hy l b</cell><cell>-</cell><cell cols="2">hy l b</cell><cell>z</cell><cell>hy</cell><cell>=</cell><cell>hy l c</cell><cell>-</cell><cell>hy l c</cell><cell>, (10)</cell></row><row><cell>where</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>,1</cell><cell cols="2">11 22</cell><cell cols="3">12 21</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>,2</cell><cell cols="2">12 31</cell><cell></cell><cell cols="2">11 32</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>,1</cell><cell cols="2">13 22</cell><cell cols="2">12 23</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>,2</cell><cell cols="2">12 33</cell><cell cols="3">13 32</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>,1</cell><cell cols="2">12 34</cell><cell cols="2">14 32</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>,2</cell><cell cols="2">14 22</cell><cell></cell><cell cols="2">12 24</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIE.2018.2826471, IEEE Transactions on Industrial Electronics R is the covariance of VP-based orientation measurement. The variable s indicates the start index of the local map correction. The covariance of the VP-based orientation is inversely proportional to the ratio of inliers in estimating the VPs. The inverse pose composition operator is a relative transformation between the two camera poses i</figDesc><table><row><cell cols="14">IEEE TRANSACTIONS ON INDUSTRIAL ELECTRONICS</cell></row><row><cell cols="2">estimation; odo S</cell><cell cols="12">is the covariance matrix of odometry</cell></row><row><cell cols="14">measurement; and x and</cell></row><row><cell cols="3">j x defined as follows:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>(</cell><cell>i</cell><cell>j</cell><cell cols="2">) cos</cell><cell>j</cell><cell>(</cell><cell>i</cell><cell>j</cell><cell cols="2">) sin</cell><cell>j</cell></row><row><cell>ij</cell><cell>i</cell><cell>j</cell><cell>(</cell><cell>i</cell><cell></cell><cell>j</cell><cell>) sin</cell><cell>j</cell><cell>(</cell><cell>i</cell><cell cols="2">j</cell><cell>) cos</cell><cell>j</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>i</cell><cell>j</cell><cell></cell><cell></cell><cell></cell></row></table><note><p>0278-0046 (c) 2018 IEEE. Translations and content mining are permitted for academic research only. Personal use is also permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. i</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE I</head><label>I</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="2">HOME DATASET CHARACTERISTICS</cell><cell></cell></row><row><cell></cell><cell>START</cell><cell></cell><cell></cell><cell># of images</cell></row><row><cell>Dataset</cell><cell>POSITIO</cell><cell cols="2">Environment # of images</cell><cell>containing moving</cell></row><row><cell></cell><cell>N</cell><cell></cell><cell></cell><cell>people or objects</cell></row><row><cell>1</cell><cell>1</cell><cell>Static</cell><cell>8393</cell><cell>None</cell></row><row><cell>2</cell><cell>2</cell><cell>Static</cell><cell>8780</cell><cell>None</cell></row><row><cell>3</cell><cell>1</cell><cell>Dynamic</cell><cell>7734</cell><cell>1859</cell></row><row><cell>4</cell><cell>2</cell><cell>Dynamic</cell><cell>8431</cell><cell>1466</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE III TIMING</head><label>III</label><figDesc>RESULTS OF VARIOUS METHODS PER EACH THREADS</figDesc><table><row><cell>Version</cell><cell>THREAD</cell><cell>Avg. (ms)</cell><cell>Std. (ms)</cell></row><row><cell>Proposed Method</cell><cell>Tracking</cell><cell>6.96</cell><cell>5.03</cell></row><row><cell></cell><cell>Mapping</cell><cell>5.71</cell><cell>4.19</cell></row><row><cell></cell><cell>Loop Closing</cell><cell>4.72</cell><cell>1.38</cell></row><row><cell>ORB-SLAM 2D</cell><cell>Tracking</cell><cell>16.41</cell><cell>7.67</cell></row><row><cell></cell><cell>Local Mapping</cell><cell>41.92</cell><cell>34.22</cell></row><row><cell></cell><cell>Loop Closing</cell><cell>3.08</cell><cell>26.37</cell></row><row><cell>VP Standard BA</cell><cell>Tracking</cell><cell>6.92</cell><cell>5.01</cell></row><row><cell></cell><cell>Mapping</cell><cell>20.31</cell><cell>11.20</cell></row><row><cell></cell><cell>Loop Closing</cell><cell>4.81</cell><cell>1.72</cell></row><row><cell>VP-only</cell><cell>Tracking</cell><cell>3.02</cell><cell>0.96</cell></row><row><cell></cell><cell>Mapping</cell><cell>0.3</cell><cell>0.14</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE IV AVERAGE</head><label>IV</label><figDesc>MEMORY USAGE OF VARIOUS METHODS IN HOME DATASET PROPOSED</figDesc><table><row><cell>METHOD</cell><cell cols="2">ORB-SLAM 2D VP Standard BA</cell><cell>VP-only</cell></row><row><cell>114.1 MB</cell><cell>1160.5 MB</cell><cell>113.9 MB</cell><cell>2.9 MB</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE V ABSOLUTE</head><label>V</label><figDesc>POSITION ERROR OF VARIOUS METHODS IN RAWSEEDS</figDesc><table><row><cell></cell><cell></cell><cell>DATASET</cell><cell></cell><cell></cell></row><row><cell></cell><cell>PROPOSED METHOD</cell><cell>ORB-SLAM 2D</cell><cell>VP Standard BA</cell><cell>VP-only</cell></row><row><cell>Avg. (m)</cell><cell>0.71</cell><cell>2.51</cell><cell>0.88</cell><cell>2.43</cell></row><row><cell>Max. (m)</cell><cell>1.36</cell><cell>5.14</cell><cell>2.24</cell><cell>3.61</cell></row><row><cell>Std. (m)</cell><cell>0.31</cell><cell>1.21</cell><cell>0.54</cell><cell>0.73</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE VI CLOSED</head><label>VI</label><figDesc>LOOP ERROR OF REAL-TIME SLAM EXPERIMENTS ON EMBEDDED SYSTEM IN HOME ENVIRONMENT</figDesc><table><row><cell>Sequence</cell><cell>START POSITION</cell><cell>Environment</cell><cell>Closed loop error (cm)</cell></row><row><cell>1</cell><cell>1</cell><cell>Static</cell><cell>9.6</cell></row><row><cell>2</cell><cell>2</cell><cell>Static</cell><cell>6.0</cell></row><row><cell>3</cell><cell>1</cell><cell>Dynamic</cell><cell>8.4</cell></row><row><cell>4</cell><cell>2</cell><cell>Dynamic</cell><cell>10.9</cell></row><row><cell></cell><cell>Avg.</cell><cell></cell><cell>8.7</cell></row><row><cell></cell><cell>Std.</cell><cell></cell><cell>1.8</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>,1 3 ,2 1 ,1 3 ,2 3 ,1 1 ,2</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIE.2018.2826471, IEEE</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported by LG Electronics Inc., Seoul, Korea and Brain Korea 21 Plus Project funded by National Research Foundation of Korea (NRF).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Stereo Visual-Inertial Odometry with Multiple Kalman Filters Ensemble</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Ind. Electron</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="6205" to="6216" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Localization of a Mobile Robot Using a Laser Range Finder in a Glass-Walled Environment</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Ind. Electron</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3616" to="3627" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? The KITTI vision benchmark suite</title>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conf. on Comput. Vis.Pattern Recog</title>
		<meeting>of IEEE Conf. on Comput. Vis.Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="3355" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">RAWSEEDS: Robotics Advancement through Web-publishing of Sensorial and Elaborated Extensive Data Sets</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bonarini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. IEEE Int. Conf. Intell. Robot. Syst.</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Machine Learning for High Speed Corner Detection</title>
		<author>
			<persName><forename type="first">E</forename><surname>Rosten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Drummond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Eur. Conf. Comput. Vis</title>
		<meeting>of Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="page" from="430" to="443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">StructSLAM: Visual SLAM with building structure lines</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Veh. Technol</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1364" to="1375" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Real time simultaneous localisation and mapping with a single camera</title>
		<author>
			<persName><forename type="first">A</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1403" to="1410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Inverse depth parametrization for monocular SLAM</title>
		<author>
			<persName><forename type="first">J</forename><surname>Civera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M M</forename><surname>Montiel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Robot</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="932" to="945" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Parallel tracking and mapping for small AR workspaces</title>
		<author>
			<persName><forename type="first">G</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Orb-slam: a versatile and accurate monocular slam system</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mur-Artal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M M</forename><surname>Montiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Tardos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Robot</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1147" to="1163" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">G2o: A general framework for graph optimization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kümmerle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Robot. Autom</title>
		<meeting>IEEE Int. Conf. Robot. Autom</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="3607" to="3613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">iSAM2: incremental smoothing and mapping using the Bayes tree</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kaess</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. of Rob. Res</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="216" to="235" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A sparse separable SLAM back-end</title>
		<author>
			<persName><forename type="first">K</forename><surname>Khosoussi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dissanayake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Robot</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1536" to="1549" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Switchable constraints for robust pose graph SLAM</title>
		<author>
			<persName><forename type="first">N</forename><surname>Sünderhauf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Protzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int.Conf. Intell. Robot. Syst</title>
		<meeting>IEEE Int.Conf. Intell. Robot. Syst</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1879" to="1884" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Robust map optimization using dynamic covariance scaling</title>
		<author>
			<persName><forename type="first">P</forename><surname>Agarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Robot. Autom</title>
		<meeting>IEEE Int. Conf. Robot. Autom</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="62" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Inference on networks of mixtures for robust robot mapping</title>
		<author>
			<persName><forename type="first">E</forename><surname>Olson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Agarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. of Rob. Res</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="826" to="840" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Stereo-based simultaneous localization, mapping and moving object tracking</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int.Conf. Intell. Robot. Syst</title>
		<meeting>IEEE Int.Conf. Intell. Robot. Syst</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="3975" to="3980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Robust monocular SLAM in dynamic environments</title>
		<author>
			<persName><forename type="first">W</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Symp. Mixed Augmented Reality</title>
		<meeting>IEEE Int. Symp. Mixed Augmented Reality</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="209" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Solution to the SLAM problem in low dynamic environments using a pose graph and an RGB-D sensor</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Myung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="12467" to="12496" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">RGB-D SLAM in dynamic environments using static point weighting</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Rob. Autom. Let</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2263" to="2270" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Superpixel-Based Foreground Extraction With Fast Adaptive Trimaps</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<idno type="DOI">10.1109/TCYB.2017.2747143</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Patch Alignment Manifold Matting</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNNLS.2017.2727140</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Real-time Monocular SLAM with Straight Lines</title>
		<author>
			<persName><forename type="first">P</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. British Machine Vis</title>
		<meeting>British Machine Vis</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="17" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Undelayed initialization of line segments in monocular SLAM</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sol` A</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vidal-Calleja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int.Conf. Intell. Robot. Syst</title>
		<meeting>IEEE Int.Conf. Intell. Robot. Syst</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1553" to="1558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Building a 3-D Line-Based Map Using Stereo SLAM</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Robot</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1364" to="1377" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Visual SLAM with Line and Corner Features</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Intell. Robot</title>
		<meeting>IEEE Int. Conf. Intell. Robot</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="2570" to="2575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Embedded visual SLAM: Applications for low-cost consumer robots</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics &amp; Automation Magazine</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="83" to="95" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A monocular camera gyroscope</title>
		<author>
			<persName><forename type="first">V</forename><surname>Huttunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Robert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Gyroscopy and Navigation</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="124" to="131" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Using vanishing points to improve visual-inertial odometry</title>
		<author>
			<persName><forename type="first">F</forename><surname>Camposeco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Robot. Autom</title>
		<meeting>IEEE Int. Conf. Robot. Autom</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="5219" to="5225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">RGB-D SLAM using vanishing point and door plate information in corridor environment</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yamashita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Asama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intelligent Service Robotics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="105" to="114" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">VPass: Algorithmic Compass using Vanishing Points in Indoor Environments</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Yeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">L</forename><surname>Doh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Intelligent Robots and Systems</title>
		<meeting>IEEE Int. Conf. Intelligent Robots and Systems</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="936" to="941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Loop Closure Through Vanishing Points in a Line-based Monocular SLAM</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Suh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Robot. Autom</title>
		<meeting>IEEE Int. Conf. Robot. Autom</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="4565" to="4570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Manhattan world: Compass direction from a single image by Bayesian inference</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Coughlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="941" to="947" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">LSD: A fast line segment detector with a false detection control</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">G R</forename><surname>Grompone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jakubowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Morel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Randall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="722" to="732" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Vanishing Point Estimation and Line Classification in a Manhattan World with a Unifying Camera Model</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="111" to="130" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Multiple View Geometry in Computer Vision</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
	<note>second edition</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Bags of Binary Words for Fast Place Recognition in Image Sequences</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gálvez-López</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Tardos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Robot</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1188" to="1197" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">BRIEF-Gist -closing the loop by simple means</title>
		<author>
			<persName><forename type="first">N</forename><surname>Sünderhauf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Protzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int.Conf. Intell. Robot. Syst</title>
		<meeting>IEEE Int.Conf. Intell. Robot. Syst</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1234" to="1241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A BRIEF-Gist Based Efficient Place Recognition for Indoor Home Service Robots</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Int. Conf. Control, Autom. Sys</title>
		<meeting>of Int. Conf. Control, Autom. Sys</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1526" to="1530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Coverage path planning: The boustrophedon cellular decomposition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Choset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Philippe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Field and Service Robotics</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="203" to="209" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
