<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LEARNING LATENT REPRESENTATIONS FOR STYLE CONTROL AND TRANSFER IN END-TO-END SPEECH SYNTHESIS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ya-Jie</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">National Engineering Laboratory for Speech and Language Information Processing</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">P.R.China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shifeng</forename><surname>Pan</surname></persName>
							<affiliation key="aff1">
								<address>
									<country>Microsoft China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lei</forename><surname>He</surname></persName>
							<email>helei@microsoft.com</email>
							<affiliation key="aff1">
								<address>
									<country>Microsoft China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
							<email>zhling@ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">National Engineering Laboratory for Speech and Language Information Processing</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">P.R.China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">LEARNING LATENT REPRESENTATIONS FOR STYLE CONTROL AND TRANSFER IN END-TO-END SPEECH SYNTHESIS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>unsupervised learning</term>
					<term>variational autoencoder</term>
					<term>style transfer</term>
					<term>speech synthesis</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we introduce the Variational Autoencoder (VAE) to an end-to-end speech synthesis model, to learn the latent representation of speaking styles in an unsupervised manner. The style representation learned through VAE shows good properties such as disentangling, scaling, and combination, which makes it easy for style control. Style transfer can be achieved in this framework by first inferring style representation through the recognition network of VAE, then feeding it into TTS network to guide the style in synthesizing speech. To avoid Kullback-Leibler (KL) divergence collapse in training, several techniques are adopted. Finally, the proposed model shows good performance of style control and outperforms Global Style Token (GST) model in ABX preference tests on style transfer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>End-to-end text-to-speech (TTS) models which generate speech directly from characters have made rapid progress in recent years, and achieved very high voice quality <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref>. While the single style TTS, usually neutral speaking style, is approaching the extreme quality close to human expert recording <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3]</ref>, the interests in expressive speech synthesis also keep rising. Recently, there also published many promising works in this topic, such as transferring prosody and speaking style within or cross speakers based on end-toend TTS model <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref>.</p><p>Deep generative models, such as Variational Autoencoder (VAE) <ref type="bibr" target="#b6">[7]</ref> and Generative Adversarial Network (GAN) <ref type="bibr" target="#b7">[8]</ref>, are powerful architectures which can learn complicated distribution in an unsupervised manner. Particularly, VAE, which explicitly models latent variables, have become one of the most popular approaches and achieved significant success on Paper accepted by <ref type="bibr">IEEE ICASSP 2019</ref> Work done during internship at Microsoft STC Asia text generation <ref type="bibr" target="#b8">[9]</ref>, image generation <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> and speech generation <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref> tasks. VAE has many merits, such as learning disentangled factors, smoothly interpolating or continuously sampling between latent representations which can obtain interpretable homotopies <ref type="bibr" target="#b8">[9]</ref>.</p><p>Intuitively, in speech generation, the latent state of speaker, such as affect and intent, contributes to the prosody, emotion, or speaking style. For simplicity, we'll hereafter use speaking style to represent these prosody related expressions. The latent state plays a pretty similar role as the latent variable does in VAE. Therefore, in this paper we intend to introduce VAE to Tacotron2 <ref type="bibr" target="#b0">[1]</ref>, a state-of-the-art end-to-end speech synthesis model, to learn the latent representation of speaker state in a continuous space, and further to control the speaking style in speech synthesis. To be specific, direct manipulation can be easily imposed on the disentangled latent variable, so as to control the speaking style. On the other hand, with variational inference the latent representation of speaking style can be inferred from a reference audio, which then controls the style of synthesized speech. Style transfer, from reference audio to synthesized speech, is thus achieved. Last but not least, directly sampling on prior of latent distribution can generate a lot of speech with various speaking style, which is very useful for data augmentation. Comprehensive evaluation shows the good performance of this method.</p><p>We have become aware of recent work by Akuzawa et al. <ref type="bibr" target="#b11">[12]</ref> which combines an autoregressive speech synthesis model with VAE for expressive speech synthesis. The proposed work differs from Akuzawa's as follows: 1) their goal is to synthesize expressive speech, which is achieved by direct sampling from prior of latent distribution at inference stage, while our goal is to control the speaking style of synthesized speech through direct manipulate latent variable or variational inference from a reference audio; 2) the proposed work is on end-to-end TTS model while Akuzawa's not.</p><p>The rest of the paper is organized as follows: Section 2 introduces VAE model, our proposed model architecture and tricks for solving KL-divergence collapse problem. Section 3 presents the experimental results. Finally, the paper will be arXiv:1812.04342v2 [cs.CL] 14 Feb 2019 concluded in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">MODEL</head><p>In this section, we first review Variational Autoencoder. We then show the details of our proposed style transfer model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Variational Autoencoder</head><p>Variational Autoencoder was first defined by Kingma et al. <ref type="bibr" target="#b6">[7]</ref> which constructs a relationship between unobserved continuous random latent variables z and observed dataset x. The true posterior density p θ (z|x) is intractable, which results in an indifferentiable marginal likelihood p θ (x). To address this, a recognition model q φ (z|x) is introduced as an approximation to the intractable posterior. Following the variational principle, log p θ (x) can be rewritten as shown in equation ( <ref type="formula">1</ref>), where L(θ, φ; x) is the variational lower bound to optimize.</p><formula xml:id="formula_0">log p θ (x) = KL[q φ (z|x)||p θ (z|x)] + L(θ, φ; x) ≥ L(θ, φ; x) = E q φ (z|x) [log p θ (x|z)] − KL[q φ (z|x)||p θ (z)]</formula><p>(1) Generally, the prior over latent variables p θ (z) is assumed to be centered isotropic multivariate Gaussian N (z; 0, I), where I is the identity matrix. The usual choice of q φ (z|x) is N (z; µ(x), σ 2 (x)I), so that KL[q φ (z|x)||p θ (z)] can be calculated in closed form. In practice, µ(x) and σ 2 (x) are learned from observed dataset via neural networks which can be viewed as an encoder. The expectation term in equation ( <ref type="formula">1</ref>) plays the role of decoder which decodes latent variables z to reconstruct x. The decoder may produce the expected reconstruction if the output of decoder is averaged over many samples of x and z <ref type="bibr" target="#b13">[14]</ref>. In the rest of the paper, −E q φ (z|x) [log p θ (x|z)] is referred to as reconstruction loss and KL[q φ (z|x)||p θ (z)] is referred to as KL loss.</p><p>Stochastic inputs can be processed by stochastic gradient descent via backpropagation, but stochastic units within the network cannot be processed by backpropagation. Thus, in practice, "reparameterization trick" is introduced to VAE framework. Sampling z from distribution N (µ, σ 2 I) is decomposed to first sampling ∼ N (0, I) and then computing z = µ + σ</p><p>, where denotes an element-wise product.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Proposed Model Architecture</head><p>In this work, we introduce VAE into end-to-end TTS model and propose a flexible model for style control and style transfer. The whole network consists of two components, as shown in Fig. <ref type="figure" target="#fig_0">1:</ref> (1) A recognition model or inference network which encodes reference audio into a fixed-length short vector of latent representation (or latent variables z which stand for style representation), and (2) an end-to-end TTS model based on Tacotron 2, which converts the combined encoder states (including latent representations and text encoder states) to generated target sentence with specific style. The input texts are character sequences and the acoustic features are mel-frequency spectrograms. One may use various powerful and complex neural networks for the recognition model. Here, we only adopt a recurrent reference encoder followed by two fully connected layers. We use the same architecture and hyperparameters for reference encoder as Wang et al. <ref type="bibr" target="#b3">[4]</ref> which consists of six 2-D convolutional layers followed by a GRU layer. The output, which denotes some embedding of the reference audio, is then passed through two separate fully connected (FC) layers with linear activation function to generate the mean and standard deviation of latent variables z. The prior and approximative posterior are Gaussian distribution mentioned Section 2.1. Then z is derived by reparameterization trick. The encoder which deals with character inputs consists of three 1-D convolutional layers with 5 width and 512 channels followed by a bidirectional <ref type="bibr" target="#b14">[15]</ref> LSTM <ref type="bibr" target="#b15">[16]</ref> layer using zoneout <ref type="bibr" target="#b16">[17]</ref> with probability 0.1. The output text encoder state is simply added by z and then is consumed by a location-sensitive attention network <ref type="bibr" target="#b17">[18]</ref> which converts encoded sequence to a fixed-length context vector for each decoder output step. In addition, z should be first passed through a FC layer to make sure the dimension equal to text encoder state before add operation. The attention module and decoder have the same architecture as Tacotron 2 <ref type="bibr" target="#b0">[1]</ref>. Then, WaveNet <ref type="bibr" target="#b18">[19]</ref> vocoder is utilized to reconstruct waveform.</p><p>The total loss of proposed model is shown in equation <ref type="bibr" target="#b1">(2)</ref>.</p><formula xml:id="formula_1">Loss = KL[q φ (z|x)||p θ (z)] − E q φ (z|x) [log p θ (x|z, t)] + l stop<label>(2</label></formula><p>) Compared with the lower bound in equation ( <ref type="formula">1</ref>), the reconstruction loss term is conditioned on both latent variable z and input text t and a stop token loss l stop is added. It is worth mentioning that, after comparing L2-loss with negative log likelihood of Gaussian distribution, we finally choose L2loss of mel spectrograms as reconstruction loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Resolve KL collapse problem</head><p>During training, we observe that the KL loss KL[q φ (z|x)||p θ (z)] is always found collapsed before they learned a distinguishable representation, which is a common phenomenon but a crucial issue in training VAE models. In other words, the convergence speed of KL loss far surpasses that of the reconstruction loss and the KL loss quickly drops to nearly zero and never rises again, which means the encoder doesn't work. Thus, KL annealing <ref type="bibr" target="#b8">[9]</ref> is introduced to our task to solve this problem. That is, during training, add a variable weight to the KL term. The weight is close to zero at the beginning of training and then gradually increase. In addition, KL loss is taken into account once every K steps. By combining these two tricks, the KL loss keeps nonzero and avoids to collapse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTS AND ANALYSIS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">experimental setup</head><p>An 105-hour audiobook recordings dataset read with various storytelling styles by a single English speaker (Blizzard Challenge 2013) was used in our experiments. The dataset contains 58453 utterances for training and 200 for test. 80dimensional mel spectrograms were extracted with frame shift 12.5 ms and frame length 50 ms. GST model <ref type="bibr" target="#b3">[4]</ref> with character inputs was used as our baseline model. The hyperparameters are set according to <ref type="bibr" target="#b3">[4]</ref>. As for our proposed model, the dimension of latent variables is 32. The parameter K mentioned in 2.3 is 100 before 15000 training steps and 400 after the threshold. At inference stage, in evaluation of style control, we directly manipulate z without going through the whole recognition model. With regard to evaluation of style transfer, we feed audio clips as reference and go through the recognition model. Both parallel and non-parallel style transfer audios are Fig. <ref type="figure">3</ref>. Spectrograms generated to demonstrate disentangled factors. The first row exhibits the control of pitch height only by adjusting latent dimension 6 to be -0.9, -0.1, 0.7. The second row shows that the local pitch variation is gradually magnified by increasing the value of dimension 10, which is 0.1, 0.5, 0.9, respectively. generated and evaluated <ref type="foot" target="#foot_0">1</ref> . Parallel transfer means the target text information is the same as reference audio's, vice versa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Style control</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Interpolation of latent variables</head><p>As mentioned in <ref type="bibr" target="#b8">[9]</ref>, VAE supports smoothly interpolation and continuous sampling between latent representations, which obtains interpretable homotopies. Thus, we did interpolation operation between two z<ref type="foot" target="#foot_1">2</ref> . One can generate speech with high speaking rate and high-pitch, the other with low speaking rate and low-pitch. The mel spectrograms of generated speech are shown in Fig. <ref type="figure" target="#fig_1">2</ref>. As we can see, both pitch and speaking rate of generated speech gradually decrease along with the interpolating. The result shows that the learnt latent space is continuous in controlling the trend of spectrograms which will further reflect in the change of style.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Disentangled factors</head><p>A disentangled representation means that a latent variable completely controls a concept alone and is invariant to changes from other factors <ref type="bibr" target="#b9">[10]</ref>. In experiments, we found that several dimensions of z could independently control different style attributes, such as pitch-height, local pitch variation, speaking rate. Fig. <ref type="figure">3</ref> shows the alteration of spectrograms by manipulating single dimension while fixing others. Adjusting one of these dimensions, only one attribute of generated speech changes. This shows that, in our model, VAE has the ability of learning disentangled latent factors.</p><p>Next, we combined two disentangled dimensions to verify the additivity of latent variables. Fig. <ref type="figure" target="#fig_2">4</ref> illustrates the combination results of pitch height and local pitch variation attributes. It shows that the audio generated with combined z inherits the characteristics of both disentangled dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Style transfer</head><p>Fig. <ref type="figure">5</ref> shows mel spectrograms of the style transferred synthetic speech aligned with their corresponding references. The reference audios are chosen from test set with certain styles. The synthesized audios share the same input text. As we can see in Fig. <ref type="figure">5</ref>, the mel spectrograms of generated speech and their reference audio have pattern similarities, such as in pitch-height, pause time, speaking rate and pitch variation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Subjective test</head><p>To subjectively evaluate the performance of style transfer, crowd-sourcing ABX preference tests on parallel and nonparallel transfer were conducted. For parallel transfer, 60 audio clips with their texts are randomly selected from test set. For non-parallel transfer, 60 sentences of text and 60 other reference audio clips are selected to generate speech. The baseline voice is generated from the best GST model we have built. Each case in ABX test is judged by 25 native judgers. The total number of judger is 56 for parallel test and 57 for non-parallel test. The criterion in rating is "which one's speaking style is closer to the reference style" with three choices: (1) 1st is better; (2) 2nd is better; (3) neutral.</p><p>Fig. <ref type="figure">6</ref> shows the ABX results. As we can see, the proposed model outperforms GST model on both parallel and non-parallel style transfer (at p-value &lt; 10 −5 ). It shows that VAE can better model the latent style representations, which results in better style transfer. What's more, the performance of the proposed model on non-parallel transfer is much better than that on parallel transfer, which shows the better generalization capability of the proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSION</head><p>A VAE module is introduced to end-to-end TTS model, to learn the latent representation of speaking style in a continuous space in an unsupervised manner, which then can control the speaking style in synthesized speech. We have demonstrated that the latent space is continuous and explored the disentangled factors in learned latent variables. The proposed model shows good performance in style transfer, which outperforms GST model via ABX test, especially in non-parallel transfer.</p><p>Future work will keep focusing on getting better disentan-gled and interpretable latent representations. In addition, the scope of style transfer research will further extend to multispeakers, instead of single speaker.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. An architecture of the proposed style transfer TTS model. The dashed lines denote sampling z from parametric distribution.</figDesc><graphic url="image-1.png" coords="2,330.80,72.00,212.59,160.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Spectrograms generated by interpolation between two z. The interpolation coefficient is : (a) z a , (b) 1 3 z a + 2 3 z d , (c) 2 3 z a + 1 3 z d , (d) z d .</figDesc><graphic url="image-2.png" coords="3,62.93,72.00,226.77,96.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Audio (a) and (b) are generated with z which setting a single dimension to be non-zero with other dimensions to be zero. The valued dimension in (a) controls pitch height, while in (b) controls pitch variation. (c) is generated with the summation of z from (a) and (b).</figDesc><graphic url="image-4.png" coords="4,62.93,72.00,226.78,98.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .Fig. 6 .</head><label>56</label><figDesc>Fig. 5. The first row exhibits the mel spectrograms of three recordings with different styles, while the second row exhibits the synthesised audios referenced on those recordings separately. The synthesised audios have the same text "She went into the shop . It was warm and smelled deliciously."</figDesc><graphic url="image-6.png" coords="4,323.72,356.58,226.78,96.84" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">The audio samples can be found at http://home.ustc.edu.cn/ ˜zyj008/ICASSP2019.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">These two z are derived by feeding two audios to the recognition model, one with high speaking rate and high-pitch, the other with low speaking rate and low-pitch.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Natural TTS synthesis by conditioning WaveNet on mel spectrogram predictions</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rj</forename><surname>Skerrv-Ryan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4779" to="4783" />
		</imprint>
	</monogr>
	<note>ICASSP</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Deep voice 3: Scaling text-to-speech with convolutional sequence learning</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kainan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Gibiansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Sercan O Arik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Raiman</surname></persName>
		</author>
		<author>
			<persName><surname>Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Close to human quality TTS with transformer</title>
		<author>
			<persName><forename type="first">Naihan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shujie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.08895</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Style tokens: Unsupervised style modeling, control and transfer in end-to-end speech synthesis</title>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daisy</forename><surname>Stanton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Skerry-Ryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Shor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rif</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning, ICML 2018</title>
				<meeting>the 35th International Conference on Machine Learning, ICML 2018</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5167" to="5176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Towards end-to-end prosody transfer for expressive speech synthesis with Tacotron</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Rj Skerry-Ryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daisy</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Stanton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Shor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rif</forename><forename type="middle">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><surname>Saurous</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
				<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="4700" to="4709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Predicting expressive speaking style from text in end-to-end speech synthesis</title>
		<author>
			<persName><forename type="first">Daisy</forename><surname>Stanton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Skerry-Ryan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.01410</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Autoencoding variational bayes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2nd International Conference on Learning Representations</title>
				<meeting>2nd International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generating sentences from a continuous space</title>
		<author>
			<persName><forename type="first">Luke</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafal</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning</title>
				<meeting>the 20th SIGNLL Conference on Computational Natural Language Learning<address><addrLine>CoNLL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">β-VAE: Learning basic visual concepts with a constrained variational framework</title>
		<author>
			<persName><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loic</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arka</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Lerchner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Understanding disentangling in β-VAE</title>
		<author>
			<persName><forename type="first">Irina</forename><surname>Christopher P Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arka</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loic</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Watters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName><surname>Lerchner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03599</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Expressive speech synthesis via modeling expressions with variational autoencoder</title>
		<author>
			<persName><forename type="first">Kei</forename><surname>Akuzawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusuke</forename><surname>Iwasawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutaka</forename><surname>Matsuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="3067" to="3071" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning latent representations for speech generation and transformation</title>
		<author>
			<persName><forename type="first">Wei-Ning</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2017">2017, 2017</date>
			<biblScope unit="page" from="1273" to="1277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05908</idno>
		<title level="m">Tutorial on variational autoencoders</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuldip K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Long shortterm memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Zoneout: Regularizing rnns by randomly preserving hidden activations</title>
		<author>
			<persName><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tegan</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">János</forename><surname>Kramár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Pezeshki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><forename type="middle">Rosemary</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
				<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Attention-based models for speech recognition</title>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">K</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitriy</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="577" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">WaveNet: A generative model for raw audio</title>
		<author>
			<persName><forename type="first">Aäron</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sander</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiga</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nal</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SSW</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">125</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
