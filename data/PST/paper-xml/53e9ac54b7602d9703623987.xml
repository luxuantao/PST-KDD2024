<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Comparison of Multiple Cloud Frameworks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Gregor</forename><surname>Von Laszewski</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Pervasive Technology Institute</orgName>
								<orgName type="institution">Indiana University Bloomington</orgName>
								<address>
									<postCode>47408</postCode>
									<region>IN</region>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Javier</forename><surname>Diaz</surname></persName>
							<email>javier.diazmontes@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Pervasive Technology Institute</orgName>
								<orgName type="institution">Indiana University Bloomington</orgName>
								<address>
									<postCode>47408</postCode>
									<region>IN</region>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fugang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Pervasive Technology Institute</orgName>
								<orgName type="institution">Indiana University Bloomington</orgName>
								<address>
									<postCode>47408</postCode>
									<region>IN</region>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Geoffrey</forename><forename type="middle">C</forename><surname>Fox</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Pervasive Technology Institute</orgName>
								<orgName type="institution">Indiana University Bloomington</orgName>
								<address>
									<postCode>47408</postCode>
									<region>IN</region>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Comparison of Multiple Cloud Frameworks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F6D27F433EC3C6C54D1F8CD1043DA413</idno>
					<idno type="DOI">10.1109/CLOUD.2012.104</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Cloud, Grid</term>
					<term>Nimbus</term>
					<term>Eucalyptus</term>
					<term>OpenStack</term>
					<term>OpenNebula</term>
					<term>RAIN</term>
					<term>FutureGrid</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Today, many cloud Infrastructure as a Service (IaaS) frameworks exist.</p><p>Users, developers, and administrators have to make a decision about which environment is best suited for them. Unfortunately, the comparison of such frameworks is difficult because either users do not have access to all of them or they are comparing the performance of such systems on different resources, which make it difficult to obtain objective comparisons. Hence, the community benefits from the availability of a testbed on which comparisons between the IaaS frameworks can be conducted. FutureGrid aims to offer a number of IaaS including Nimbus, Eucalyptus, OpenStack, and OpenNebula. One of the important features that FutureGrid provides is not only the ability to compare between IaaS frameworks, but also to compare them in regards to bare-metal and traditional highperformance computing services. In this paper, we outline some of our initial findings by providing such a testbed. As one of our conclusions, we also present our work on making access to the various infrastructures on FutureGrid easier.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Cloud computing has become an important tool to deliver infrastructure as a service (IaaS) to users that require a great deal of customization and management of their own software stacks. In addition, we observe that users demand further abstraction and expect platforms as a service (PaaS) to be readily available to conduct higher-level development efforts. Together IaaS and PaaS can provide potent solutions not only to business users, but also to the educational and scientific computing communities. We observe that in the scientific community we can distinguish a number of typical user categories based on their usage in regards to scale, virtual machines used, computational tasks, and the number of users served by the services. In particular we are interested in the following use cases.</p><p>First, we distinguish users that demand very few images but want to run them for a long period of time with guarantees on high-availability. Such environments are targeted to support what is today termed the "long tail of science". In this case, many millions of scientific users with moderate computing needs benefit from the delivery of less compute intense services.</p><p>Second, we distinguish scientists requiring a large number of resources to conduct actual calculations and analyses of data to be exposed to their community. This is the traditional high-performance computing use case.</p><p>Third, we identified a class of users with requirements inbetween the two previous cases. These users have modest but still significant demand of compute resources. Hence our use cases motivate three classes of infrastructures: 1. Traditional high performance computing (HPC) services offered as part of traditional HPC centers. 2. Hosting services for production such as services that cater to a community, including gateways, Web servers and others. Such services may interface with services that run on traditional HPC resources. 3. Access to moderate compute resources to support many moderate calculations to its many users that demand it.</p><p>Within this paper we focus on the third class of users. We will identify the rational for some of the choices that we offer in FutureGrid and identify how to simplify access to an environment that provides so many choices.</p><p>The paper is structured as follows. In Section II, we present an overview of FutureGrid and explain why FutureGrid provides the current services it offers. We also outline some future directions motivated by simple usage patterns observed in FutureGrid. One of the main objectives of this paper is to contrast the different IaaS frameworks we offer and project first results of our qualitative comparison described in Sections III and IV, respectively. Based on our qualitative analysis we strive to provide answers to the following questions: 1. Which IaaS framework is most suited for me? 2. How can I compare these frameworks not just between each other, but also to bare-metal? The later is of special interest as at this time many of the Cloud frameworks are still under heavy development and pathways to utilize multiple of them are of current interest.</p><p>In Section V, we impart our thoughts on providing PaaS offerings attractive for our user communities. Next, in Section VI, we discuss what implications this multitude of service offerings has for the user community. In Section VII we introduce rain and in Section VIII we detail the scalability tests performed in different FG infrastructures. Finally, Section IX collects the conclusions of this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. FUTUREGRID</head><p>The FutureGrid project is sponsored by N partners from Indiana University, Chicago, University of Florida, San Diego Center, Texas Advanced Computing Cente Virginia, University of Tennessee, Univer California, Dresden, Purdue University, and Among its sites Futuregrid has a se resources totaling about 5000 compute c include a variety of different platforms a access heterogeneous distributed computin storage resources. This variety of resourc allow interesting interoperability and scalabi Additionally, FG provides a fertile environm variety of IaaS and PaaS frameworks.</p><p>Early on we were faced with two elem (1) Does a user need access to more framework, and (2) if so, which framew offer? To answer these questions, we analyz from the user community of FutureGrid as p registration process. Each project requestor h feedback on the technologies that were execution of their projects. The result of th depicted in Figure <ref type="figure" target="#fig_1">1</ref>. Figure <ref type="figure" target="#fig_0">2</ref> shows the dem projects we host on FutureGrid classifi disciplines and goals. We observe the follow 1. Nimbus and Eucalyptus were requested not surprising, as we made most advert systems and recommended them for e projects on FG. Originally we had m Eucalyptus but scalability issues of install on FutureGrid resulted in a re Nimbus requests. 2. High Performance Computing (HPC requested as third highest category. Thi our affiliation with traditional HPC com as the strong ties to XSEDE. 3. Hadoop and map/reduce were requ 36.78% of the projects. This number i one reported in Figure <ref type="figure" target="#fig_1">1</ref>, as we combine Hadoop and MapReduce while only co entries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">We saw recently an increase in request</head><p>It has just become one of the prefer solutions for cloud computing within a companies, but also within the research 5. We have seen an increased demand fo OpenNebula. It is quite popular as part Cloud efforts and has gained substantia US projects such as Clemson Unive Laboratory as part of their cloud strateg 6. Not surprisingly the largest contingent o group of technology experts.</p><p>Based on this analysis we spent our effo services within FutureGrid. As a result w providing the following partitioning betw listed in Figure <ref type="figure" target="#fig_3">3</ref>. However, the number of between these services can be chang At present, we are integratin resources between the IaaS frame could decide to assign some reso tomorrow the same resources could IaaS farmework Hence, the differe testbed can be resized on-deman experiments or scientific problem requirements.  is chart because we have Nevertheless, we have (see Section IV) and d motivate a possible shift For this decision we are rs, such as what our users servation we made is that st on the FG portal is one Figure <ref type="figure" target="#fig_3">3</ref>) although we do Grid at this time. This also offer OpenNebula. ng mechanisms to float eworks. Thus, today we ources to Nimbus, while d be assigned to another ent infrastructures of our d to support scalability ms with high resource fic areas that we identified ote that 20 project have yet his data). rt of the project application entries could be selected so the III. OVERVIEW OF CLOUD IAAS FRA One fundamental concept in cloud comp providing Infrastructure as a Service (I resources to customers and users instead o maintaining compute, storage, and network. achieved by offering virtual machines to th to establish such a service, a number of available including Eucalyptus, Nimbu OpenStack (see Figure <ref type="figure" target="#fig_2">4</ref>). Next, we p discussion about these frameworks and out qualitative differences between them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Nimbus</head><p>The Nimbus project <ref type="bibr" target="#b0">[1]</ref> is working on t they term "Nimbus Infrastructure" and "Nim Nimbus Infrastructure: The Nimbus pr Nimbus Infrastructure to be "an open compatible Infrastructure-as-a-Service specifically targeting features of interest community such as support for proxy c schedulers, best-effort allocations and oth this mission, Nimbus is providing its own im a storage cloud compatible with S3 compatib by quota management, EC2 compatible clou convenient cloud client which uses internally Nimbus Platform: The Nimbus platform provide additional tools to simplify the ma infrastructure services and to facilitate the other existing clouds (OpenStack and Ama  m is targeting to anagement of the integration with azon). Currently, this includes the following tools a launching, controlling, and monit b) a context broker service that cluster launches automatically and</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. OpenNebula</head><p>OpenNebula <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> is an open design is flexible and modular to different storage and network infra and hypervisor technologies <ref type="bibr" target="#b4">[5]</ref>. I resource needs, resource add snapshotting, and failure of ph Furthermore, OpenNebula suppor interface with external clouds, wh isolation, and multiple-site support supplement the local infrastructure from public clouds to meet peak high availability strategies. Thus, a centralized management system c multiple deployments of OpenNebu OpenNebula supports differ including REST-based interfaces interfaces, and the emerging cloud the de-facto AWS EC2 API standard</p><p>The authorization is based keypairs, X.509 certificates or LDA integrates fine-grained role -based authorization capabilities.</p><p>The authorization is based keypairs, X.509 certificates or LDA supports fine-grained ACLs to man roles as part of its authorization capa</p><p>The storage subsystem su configuration, from non-shared fi transferring via SSH to shared file s Lustre) or LVM with CoW (copy-o server, from using commodity hard solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. OpenStack</head><p>OpenStack <ref type="bibr" target="#b6">[7]</ref> is a collection of to deliver public and private clo currently include OpenStack Co OpenStack Object Storage (called Image Service (called Glance). Op and has received considerable openness and the support of compan Nova is designed to provisi networks of virtual machines, cr scalable cloud computing platform. redundant, scalable object stora standardized servers in order to stor data. It is not a file system or real-t but rather a long-term storage system static data. Glance provides disc delivery services for virtual disk ima urces by IaaS a) cloudinit.d coordinates oring cloud applications, coordinates large virtual repeatedly <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> n source IaaS toolkit. Its o allow integration with astructure configurations, It can deal with changing ditions, live migration, ysical resources <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6]</ref>. rts cloud federation to hich provides scalability, t. This lets organizations with computing capacity demands, or implement a single access point and can be used to control la. rent access interfaces s, OGF OCCI service API standard, as well as d. on passwords, ssh rsa AP. This framework also ACLs that as part of its on passwords, ssh rsa AP. This framework also nage users with multiple abilities. upports any backend ile systems with image systems (NFS, GlusterFS, on-write), and any storage dware to enterprise-grade f open source components ouds. These components ompute (called Nova), d Swift), and OpenStack penStack is a new effort momentum due to its nies. ion and manage large eating a redundant and Swift is used to create a age using clusters of re petabytes of accessible time data storage system, m for more permanent or covery, registration, and ages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Eucalyptus</head><p>Eucalyptus <ref type="bibr" target="#b7">[8]</ref> promises the creation of on-premise private clouds, with no requirements for retooling the organization's existing IT infrastructure or need to introduce specialized hardware. Eucalyptus implements an IaaS (Infrastructure as a Service) private cloud that is accessible via an API compatible with Amazon EC2 and Amazon S3. It has five high-level components: Cloud Controller (CLC) that manages the virtualized resources; Cluster Controller (CC) controls the execution of VMs; Walrus is the storage system, Storage Controller (SC) provides block-level network storage including support for Amazon Elastic Block Storage (EBS) semantics; and Node Controller (NC) is installed in each compute node to control VM activities, including the execution, inspection, and termination of VM instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. QUALITATIVE FEATURE COMPARISON OF THE IAAS FRAMEWORKS</head><p>All these IaaS frameworks have been designed to allow users to create and manage their own virtual infrastructures. However, these frameworks have differences that need to be considered when choosing a framework. Some qualitative features to consider as part of the selection are summarized in Table <ref type="table" target="#tab_2">1</ref> and enhance findings from others <ref type="bibr" target="#b8">[9]</ref>.</p><p>Software deployment. An important feature is the ease and frequency of the software deployments. From our own experience the easiest to deploy is OpenNebula because we only have to install a single service in the frontend for a basic configuration while no OpenNebula software is installed in the compute nodes. Nimbus is also relatively easy to install, as only two services have to be configured in the frontend plus the software installation in each compute node. On the other hand, the deployment of Eucalyptus and OpenStack is more difficult due to the number of different components to configure and the different configuration possibilities that they provide. The learning curve for OpenStack is still steep, as the documentation needs some improvements</p><p>In addition to a single install we also have to consider update and new release frequencies. From the release notes and announcements of the framework we observe that major updates happen on a four or six months schedule, with many release candidates and upgrades that fix intermediate issues. Furthermore, we observed that the installation deployment depends on scalability requirements. As such, an OpenStack deployment of four nodes may look quite different from one that utilizes 60 nodes. Hence, it is important that integration with configuration management toolkits exist in order to minimize the effort to repeatedly adapt to configuration requirements for a production system. Tools such as chef and puppet provide a considerable value add in this regards. They serve as a repeatable "template" to install the services in case version dependent performance comparisons or feature comparisons are conducted by the users.</p><p>Interfaces. Since Amazon EC2 is a de-facto standard, all of them support the basic functionality of this interface, namely image upload and registration, instantiate VMs, as well as describe and terminate operations. However, the OpenStack project noticed disadvantages due to features that EC2 is not exposing. Thus, OpenStack is considering to provide interfaces that diverge from the original EC2.</p><p>Storage. Storage is very important in cloud because we have to manage many images and they must be available for users anytime. Therefore, most of the IaaS frameworks decided to provide cloud storage. In the case of Nimbus, it is called Cumulus and it is based on the POSIX filesystem. Cumulus also provides a plugin supporting various storage systems including PVFS, GFS, and HDFS (under a FUSE module). Cumulus uses http/s as a communication protocol.</p><p>OpenStack and Eucalyptus provide more sophisticated storage systems. In OpenStack it is called called Swift, and in Eucalyptus it is called Walrus. Both of them are designed to provide fault tolerance and scalability. OpenStack can store the images in either the POSIX filesystem or Swift. In the first case, images are transferred using ssh while in the second one are transferred using http/s. Finally, OpenNebula does not provide a cloud storage product, but its internal storage system can be configured in different ways. Thus, we can have a shared filesystem between frontend and compute nodes; we can transfer the images using ssh; or we can use LVM with CoW to copy the images to the compute nodes.</p><p>Networking. The network is managed differently for each IaaS framework while providing various options in each of them:</p><p>• Eucalyptus offers four different networking modes: managed, managed-noLAN, system, and static [10]. In the two first modes, Eucalyptus manages the network of the VMs. The modes differ in the network isolation provided by vLAN. In the system mode, Eucalyptus assumes that IPs are obtained by an external DHCP server. In the static mode, Eucalyptus manages VM IP address assignment by maintaining its own DHCP server with one static entry per VM. • Nimbus assigns IPs using a DHCP server that can be configured in two ways: centralized and local. In the first case, a DHCP service is used that one configures with Nimbus-specific MAC to IP mappings. In the second case, a DHCP server is installed on every compute node and automatically configured with the appropriate addresses just before a VM boots. Authentication. All of the IaaS frameworks support X.509 credentials as authentication method for users. OpenStack and OpenNebula also support authentication via LDAP, although it is quite basic. OpenNebula also support ssh rsa keypair and password authentication. OpenStack focuses recently on the development of a keystore. Nimbus allows interfacing with existing Grid infrastructure authentication frameworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. OVERVIEW OF SELECTED PAAS FRAMEWORKS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Platform as a Service</head><p>Platform as a Service (PaaS) offers higher-level services to the users that focus on the delivery of a "platform" to develop and reuse advanced services.. Hence, developers can build applications without installing any tools on their computer and deploy those applications without worrying about system administration tasks. On FutureGrid we provide Hadoop and Twister to our users as they provide popular choices in the community. Twister is a product developed at Indiana University (IU) and used as part of courses taught within the university. This also explains why it is so frequently requested as part of the project registration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. SUPPORTED IAAS AND PAAS FRAMEWORKS IN FUTUREGRID</head><p>As already outlined in Section II FutureGrid provides at this time Nimbus, OpenStack, and Eucalyptus on various resources. However, We also experimented with an OpenStack installation with great success. At this time Nimbus is our preferred IaaS framework due to its easy install, the stability, and the support that is provided while including the authors of the Nimbus project as funded partners into the FutureGrid Project. This has a positive impact in support questions, but also in the development of features motivated by FutureGrid. As part of our PaaS offerings, we provide various ways of running Hadoop on FG. This is achieved by either using Hadoop as part of the virtualized environment or exposing it through the queuing system via myHadoop <ref type="bibr" target="#b9">[11]</ref>. Twister <ref type="bibr" target="#b10">[12]</ref> is contributed through community efforts by IU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Unicore and Genesis II</head><p>Additionally, services such as Unicore and Genesis II are available as part of the HPC services. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Globus</head><p>Originally we expected that the demand for Globus <ref type="bibr" target="#b11">[13]</ref> would be higher than what we currently see. However, this is explainable due to the following factors: a) Globus demand has been shrinking because many new projects investigate cloud technologies instead of using Grid toolkits; and b) Grid services have been offered in stable form as part of OSG and XSEDE and thus the demand within a testbed is small as sufficient resources are available to experiment with such systems in production environments. Projects that requested Globus include the SAGA team to investigate interoperability, the EMI project, and the Pegasus Project. Nevertheless, only the later has actively requested a real permanent Globus infrastructure on bare metal resources. The other projects have indicated that their needs can be fulfilled by raining Globus onto a server based on images that are managed by the projects themselves. Furthermore, we observe that the Globus project has moved over the last year to deliver package based installs that make such deployments easy. We had only one additional user that requested not the installation of Globus, but the installation of GridFTP to more easily move files along. This request could be satisfied while using the CoG Kit [14] and abstracting the need for moving data away from a protocol dependency. Interoperability projects would be entirely served by a virtual Globus installation on images via rain (see Section VII). In many of these cases even bare-metal access is not needed. In fact such use cases benefit from the ability to start up many Globus services to as part of the development of advanced workflows that are researched by the community.</p><p>At this time we are not supporting any other PaaS such as messaging queues or hosted databases. However, we intend to adapt our strategies based on evaluating user requests with available support capabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. RAINING</head><p>Due to the variety of services and limited resources provided in FG, it is necessary to enable a mechanism to provision needed services onto resources. This includes also the assignment of resources to different IaaS or PaaS frameworks. We have developed, as first step to address this challenge, a sophisticated image management toolkit that allows us to not only provision virtual machines, but also provision directly onto bare-metal. Thus, we use the term raining to indicate that we can place arbitrary software stack onto a resource. The toolkit to do so is called rain.</p><p>Rain makes it possible to compare the benefits of IaaS, PaaS performance issues, as well as evaluating which applications can benefit from such environments and how they must be efficiently configured. As part of this process, we allow the generation of abstract images and universal image registration with the various infrastructures including Nimbus, Eucalyptus, OpenNebula, OpenStack, but also baremetal via the HPC services. This complex process is described in more detail in <ref type="bibr" target="#b13">[15]</ref>. It is one of the unique features about FutureGrid to provide an essential component to make comparisons between the different infrastructures more easily possible. Our toolkit rain is tasked with simplifying the full life cycle of the image management process in FG. It involves the process of creating, customizing, storing, sharing and deploying images for different FG environments. One important feature in our image management design is that we are not simply storing an image but rather focus on the way an image is created through templating. Thus it will be possible at any time to regenerate an image based on the template that is used to install the software stack onto a bare operating system. In this way, we can optimize the use of the storage resources. Now that we have an elementary design of managing images, we can dynamically provision them to create baremetal and virtualized environments while raining them onto our resources. Hence, rain will offers four main features:</p><p>• Create customized environments on demand.</p><p>• Compare different infrastructures.</p><p>• Move resources from one infrastructure to another by changing the image they are running plus doing needed changes in the framework. • Ease the system administrator burden for creating deployable images. This basic functionality has been tested as part of a scalability experiment motivated by our use case introduced in the introduction. Here, a user likes to start many virtual machines and compare the performance of this task between the different frameworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. INFRASTRUCTURE SCALABILITY STUDY</head><p>We have performed several tests to study the scalability of the infrastructures installed in our cluster at Indiana University called India. The India cluster is composed by Intel Xeon X5570 with 24GB of memory, a single drive of 500GB with 7200RPMm 3Gb/s and an interconnection network of 1Gbps Ethernet.</p><p>The idea of these tests is to provision as many physical machines (PM) or virtual machines (VM) at the same time as possible. Obviously this is an important test that is of especial interest to our traditional HPC community that needs many compute resources to operate in parallel. Tests succeed if all the machines have ssh access and we can successfully connect and run commands on all the machines. We measure the time that takes since the request is first issued until we have access to all the machines.</p><p>For that purpose we have used our image generator and registration tools and created a CentOS 5 image. We registered it in the different infrastructures: HPC, OpenStack, Eucalyptus and OpenNebula. This process gives us an identical image template. Therefore, the only difference in the image is the version of the 2.6 kernel/ramdisk used. In HPC we use the ramdisk modified by xCAT, in Eucalyptus we use a XEN kernel and in OpenStack or OpenNebula we use a generic kernel with KVM support. The total size of the image without compression is 1.5 GB. In the case of netboot it is compressed to about 300MB. Figure <ref type="figure" target="#fig_4">5</ref> shows the results of the performed tests. In the following sections we describe the software used in each infrastructure, the results obtained and the problems we found.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. HPC (Moab/xCAT)</head><p>We used Moab 6.0.3 and xCAT 2.6.9 provision images to the compute nodes. W 111 nodes provisioned with the same image we did not have any problems and observed scalability. This behavior is based on the g execution the provisioning step in pa bottleneck in our experiments was introduc retrieving the image from the server. Sin compressed and relatively small, the scalab is preserved. However, provisioning even takes more time in comparison to its virtuali This easily explained by the fact that the pro the physically machine must be called whic needed in case of using virtualized mac process of rebooting takes place in memory consuming.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. OpenStack</head><p>In our experiments, we have used the C OpenStack. In order to make an efficient ima OpenStack caches images in the computes does not need to transfer the image over th time it is requested and the instantiation of a Due to scalability issues by provisionin of VMs, we modified our strategy to submi batches of 10 VMs at a time (maximum). T the case of provisioning 32 VMs, we have r 10 VMs and once 2 VMs. This is a OpenStack and is going to be solved in fu through the method we chose for Cactus <ref type="bibr" target="#b14">[16</ref> Furthermore, we observed that if the ima in the compute nodes, the scalability of th very limited. In fact, we started to have pr boot <ref type="bibr" target="#b14">16</ref> VMs and observed a failure rate o our test. We observed that VMs got stuck status (this is the status where the image is compute node).</p><p>On the other hand, once we had the imag of the compute nodes, we were able to boo simultaneously. However, placing the imag was not an easy task, as we had to deal wi  The tests with this infrastructure because we could only get 16 VMs Even though, the failure rate was ve OpenStack, configures the pub forwarding and creates the bridges this creates problems, like in the ca similar errors like missing bridg Additionally, we had problems assignment of public IPs to the VM VMs did not get public IPs and accessible for users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. OpenNebula</head><p>We used OpenNebula versi OpenNebula does not cache images supports three basic transfer plugins NFS has a terrible performance beca the image through the network. SS used because is still easy to con performance. The last one seems mo but it should provide the best perfo one selected by the OpenNebula experiments <ref type="bibr" target="#b15">[17,</ref><ref type="bibr" target="#b16">18]</ref>. We were abl at the same time with almost a 100 only got one error in the case o configuration does not use an image ks on FutureGrid image got stuck in the d that other images were them via ssh. The later t that OpenStack does not uration inside the VM. d conducts IP forwarding private IP. At times we bles entries or both were ar, the iptables issue is at sometimes OpenStack in the iptables using old leaned up. This erroneous essible. We observed that needs to manually modify rong entries or restart m of OpenStack Cactus is assign public IPs to the ool and assign one to each een added in the Diablo to rerun our experiments on FutureGrid. yptus, which is the latest lso caches the images in OpenStack, we observed while requesting larger ed our initial experiment equests (10 at a time) but ue to an annoying feature from executing the same eriod of time.</p><p>were quite disappointing running at the same time. ery high. Eucalyptus, like blic network with IP s at running time. Thus, ase of OpenStack, we got es and iptables entries. s with the automatic Ms. This means that some therefore they were not ion 3.0.0. By default s in the compute nodes. It s named nfs, ssh and lvm. ause the VMs are reading SH was the one that we nfigure and has a better ore difficult to configure, ormance, because it is the team to perform their e to instantiate 148 VMs % of success. In fact, we of 148 VMs. Since our e cache, it was very slow and limited by the network, as each image must be copied to each VM. This is done in a sequential process. However, this can be improved by introducing caches as others have showcased <ref type="bibr" target="#b17">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IX. CONCLUSIONS</head><p>This paper provides evidence that offering not one but many IaaS frameworks is needed to allow users to answer the question which IaaS framework is right for them. We have to be aware that users requirements may be very different from each other and the answer to this question cannot be universally cast.</p><p>We identified an experiment to be executed on FutureGrid to provide us with feedback on a question many of our users have: Which infrastructure framework should I use under the assumption I have to start many instances. Our close ties to the HPC community and researchers utilizing parallel services and programming methodologies naturally motivate this experiment. However, such users tend not to be experts in IaaS frameworks and a tool is needed to allow the comparison of their use cases in the different frameworks. In this paper, we have demonstrated that we achieved a major milestone towards this goal and proven that our efforts can lead to significant comparisons between deployed frameworks. This also includes the ability to identify a set of benchmarks for such infrastructures in order to further identify which framework is best suited for a particular application. Our test case must be part of such a benchmark.</p><p>Concretely, we found challenges in our scalability experiments while raining images on them. This was especially evident for Eucalyptus and even OpenStack. As many components are involved in the deployment they are also not that easy to deploy. Tools provided as part of developments such as Chef <ref type="bibr" target="#b18">[20]</ref> and Puppet <ref type="bibr" target="#b19">[21]</ref> can simplify deployments especially if they have to be done repeatedly or require modifications to improve scalability. We claim that the environment to conduct an OpenStack experiment with just a handful VMs may look quite different from a deployment that uses many hundreds of servers on which Openstack may be hosted. This is also documented nicely as part of the OpenStack Web pages that recommends more complex service hierarchies in case of larger deployments.</p><p>On the other hand, we have seen that OpenNebula is very reliable and easy to deploy. Although in our experiments we found it is quite slow due to the lack of caching images. Nevertheless, we think that this problem is easy to solve. In fact, after we finished the tests we also found other community members having the same problem. A plugin to provide caching support is already available, which can significantly improve managing images across many nodes in OpenNebula.</p><p>Through the ability of rain it will become easier for us to deploy PaaS on the IaaS offerings, as we will be able to create "templates" that facilitate their installation and potentially their upgrade. Due to this ability it is possible to replicate the environments and introduce reproducible environment.</p><p>The answer to the question, which IaaS question should one use, depends on the requirements of the applications and the user community. OpenNebula and Nimbus were easier to install, but we observed that OpenStack and Eucalyptus have considerably worked on these issues in newer versions to be released shortly. Nimbus has turned out to be very reliable, but our OpenStack and Eucalyptus Clouds have been extremely popular resulting in resource starvation. We will address this issue by assigning dynamically resources between the clouds. From our own observations working with applications it is clear that we do need a testbed, such as FutureGrid, to allow users to cast their own evaluations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The distribution of the scientif while reviewing the project requests. (No to be integrated into th</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Technology choices made as pa process in FutureGrid. Note that multiple e total will be more than 100%.</figDesc><graphic coords="2,330.86,73.99,123.62,218.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Typical partitioning of FG compute resou framework</figDesc><graphic coords="3,67.68,194.63,162.43,150.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Number of hits on our tutorial pages</figDesc><graphic coords="3,227.97,194.63,66.82,150.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Scalability Experiment of IaaS Framework</figDesc><graphic coords="7,228.86,73.49,58.35,135.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Feature comparison of IaaS frameworks. indicates</figDesc><table><row><cell></cell><cell>OpenStack</cell><cell>Eucalyptus 2.0</cell><cell>Nimbus</cell><cell>OpenNebula</cell></row><row><cell>Interfaces</cell><cell>EC2 and S3, Rest Interface.</cell><cell>EC2 and S3, Rest Interface.</cell><cell>EC2 and S3, Rest</cell><cell>Native XML/RPC, EC2 and S3,</cell></row><row><cell></cell><cell>Working on OCCI</cell><cell>Working on OCCI</cell><cell>Interface</cell><cell>OCCI, Rest Interface</cell></row><row><cell>Hypervisor</cell><cell>KVM, XEN, VMware Vsphere,</cell><cell>KVM and XEN. VMWare in the</cell><cell>KVM and XEN</cell><cell>KVM, XEN and VMWare</cell></row><row><cell></cell><cell>LXC, UML and MS HyperV</cell><cell>enterprise edition.</cell><cell></cell><cell></cell></row><row><cell>Networking</cell><cell>-Two modes:</cell><cell>-Four modes: (a) managed; (b)</cell><cell>-IP assigned using a</cell><cell>-Networks can be defined to</cell></row><row><cell></cell><cell>(a) Flat networking</cell><cell>managed-novLAN; (c) system;</cell><cell>DHCP server that can be</cell><cell>support Ebtable, Open vSwitch</cell></row><row><cell></cell><cell>(b) VLAN networking</cell><cell>and (d) static</cell><cell>configured in two ways.</cell><cell>and 802.1Q tagging</cell></row><row><cell></cell><cell>-Creates Bridges automatically</cell><cell>-In (a) &amp; (b) bridges are created</cell><cell>-Bridges must exists in</cell><cell>-Bridges must exists in the</cell></row><row><cell></cell><cell>-Uses IP forwarding for public IP -VMs only have private IPs</cell><cell>-VMs only have private IPs automatically -IP forwarding for public IP</cell><cell>the compute nodes</cell><cell>compute nodes -IP are setup inside VM</cell></row><row><cell>Software</cell><cell>-Software is composed by</cell><cell>-Software is composed by</cell><cell>Software is installed in</cell><cell>Software is installed in frontend</cell></row><row><cell>deployment</cell><cell>component that can be placed in</cell><cell>component that can be placed in</cell><cell>frontend and compute</cell><cell></cell></row><row><cell></cell><cell>different machines.</cell><cell>different machines.</cell><cell>nodes</cell><cell></cell></row><row><cell></cell><cell>-Compute nodes need to install</cell><cell>-Compute nodes need to install</cell><cell></cell><cell></cell></row><row><cell></cell><cell>OpenStack software</cell><cell>OpenStack software</cell><cell></cell><cell></cell></row><row><cell>DevOps</cell><cell>Chef, Crowbar, Puppet</cell><cell>Chef*, Puppet*</cell><cell>no</cell><cell>Chef, Puppet</cell></row><row><cell>deployment</cell><cell></cell><cell>(*according to vendor)</cell><cell></cell><cell></cell></row><row><cell>Storage (Image</cell><cell>-Swift (http/s)</cell><cell>Walrus (http/s)</cell><cell>Cumulus (http/https)</cell><cell>Unix Filesystem (ssh, shared</cell></row><row><cell>Transference)</cell><cell>-Unix filesystem (ssh)</cell><cell></cell><cell></cell><cell>filesystem or LVM with CoW)</cell></row><row><cell>Authentication</cell><cell>X509 credentials, LDAP</cell><cell>X509 credentials</cell><cell>X509 credentials,</cell><cell>X509 credential, ssh rsa keypair,</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Grids</cell><cell>password, LDAP</cell></row><row><cell>Avg. Release</cell><cell>&lt;4month</cell><cell>&gt;4 month</cell><cell>&lt;4 month</cell><cell>&gt;6 month</cell></row><row><cell>Frequency</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>License</cell><cell>OpenSource -Apache</cell><cell>OpenSource Commercial</cell><cell>OpenSource Apache</cell><cell>OpenSource Apache</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>a positive evaluation. The more checkmarks the better.</head><label></label><figDesc></figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>We would like to acknowledge the rest of the FG team for its support in the preparation and execution of these tests. This material is based upon work supported in part by the National Science Foundation under Grant No. 0910812.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="http://www.nimbusproject.org" />
		<title level="m">Nimbus Project Web Page</title>
		<imprint>
			<date type="published" when="2012-05">May 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Virtual Workspaces: Achieving Quality of Service and Quality of Life in the Grid</title>
		<author>
			<persName><forename type="first">K</forename><surname>Keahey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Programming Journal</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="265" to="276" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Web</forename><surname>Opennebula</surname></persName>
		</author>
		<author>
			<persName><surname>Page</surname></persName>
		</author>
		<ptr target="http://www.opennebula.org" />
		<imprint>
			<date type="published" when="2012-05">May 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cloud computing for on-demand grid resource provisioning</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M</forename><surname>Llorente</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Moreno-Vozmediano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Montero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Parallel Computing</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="177" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An Elasticity Model for High Throughput Computing Clusters</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Montero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Moreno-Vozmediano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M</forename><surname>Llorente</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Parallel and Distributed Computing</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Virtual Infrastructure Management in Private and Hybrid Clouds</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sotomayor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Montero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M</forename><surname>Llorente</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Foster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet Computing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="14" to="22" />
			<date type="published" when="2009-10">Sep.-Oct 2009 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Web</forename><surname>Openstack</surname></persName>
		</author>
		<author>
			<persName><surname>Page</surname></persName>
		</author>
		<ptr target="http://www.openstack.org" />
		<imprint>
			<date type="published" when="2012-05">May 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<ptr target="http://open.eucalyptus.com/" />
		<title level="m">Eucalyptus Web Pages (Open Source)</title>
		<imprint>
			<date type="published" when="2012-05">May 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A Comparison and Critique of Eucalyptus, OpenNebula and Nimbus</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sempolinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Thain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 2nd Int. Conf. on Cloud Computing Technology and Science</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<ptr target="http://sourceforge.net/projects/myhadoop/" />
		<title level="m">myHadoop</title>
		<imprint>
			<date type="published" when="2012-05">May 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<ptr target="http://www.iterativemapreduce.org/" />
		<title level="m">Twister</title>
		<imprint>
			<date type="published" when="2012-05">May 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<ptr target="http://www.globus.org/" />
		<title level="m">Globus Project</title>
		<imprint>
			<date type="published" when="2012-05">May 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A Java Commodity Grid Kit</title>
		<author>
			<persName><forename type="first">G</forename><surname>Laszewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gawor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Concurrency and Computation: Practice and Experience</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="643" to="662" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Abstract Image Management and Universal Image Registration for Cloud and HPC Infrastructures</title>
		<author>
			<persName><forename type="first">J</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">V</forename><surname>Laszewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>IEEE Cloud</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<ptr target="http://blog.griddynamics.com/2011/05/running-200-vm-instances-on-openstack.html" />
		<title level="m">Running 200 VM instances on OpenStack</title>
		<imprint>
			<date type="published" when="2012-05">May 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">CERN Cloud Computing Infrastructure</title>
		<author>
			<persName><forename type="first">U</forename><surname>Schwickerath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICS Cloud Computing Conference</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<ptr target="http://blog.opennebula.org/?p=620" />
		<title level="m">CERN Scaling up to 16000 VMs</title>
		<imprint>
			<date type="published" when="2012-05">May 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<ptr target="http://dev.opennebula.org/issues/553" />
		<title level="m">OpenNebula</title>
		<imprint>
			<date type="published" when="2012-05">May 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<ptr target="http://www.opscode.com/chef/" />
		<title level="m">Chef</title>
		<imprint>
			<date type="published" when="2012-05">May 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<ptr target="http://puppetlabs.com/" />
		<title level="m">Puppet</title>
		<imprint>
			<date type="published" when="2012-05">May 2012</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
