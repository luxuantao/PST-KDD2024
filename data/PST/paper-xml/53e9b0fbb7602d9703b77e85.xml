<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Science of Computer Programming</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2011-07-07">7 July 2011</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Guillermo</forename><forename type="middle">L</forename><surname>Taboada</surname></persName>
							<email>taboada@udc.es</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Architecture Group</orgName>
								<orgName type="institution">University of A Coruña</orgName>
								<address>
									<settlement>Coruña</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sabela</forename><surname>Ramos</surname></persName>
							<email>sramos@udc.es</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Architecture Group</orgName>
								<orgName type="institution">University of A Coruña</orgName>
								<address>
									<settlement>Coruña</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Roberto</forename><forename type="middle">R</forename><surname>Expósito</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Architecture Group</orgName>
								<orgName type="institution">University of A Coruña</orgName>
								<address>
									<settlement>Coruña</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Juan</forename><surname>Touriño</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Architecture Group</orgName>
								<orgName type="institution">University of A Coruña</orgName>
								<address>
									<settlement>Coruña</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ramón</forename><surname>Doallo</surname></persName>
							<email>doallo@udc.es</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Architecture Group</orgName>
								<orgName type="institution">University of A Coruña</orgName>
								<address>
									<settlement>Coruña</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Science of Computer Programming</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2011-07-07">7 July 2011</date>
						</imprint>
					</monogr>
					<idno type="MD5">CD066A63A1184759F5FF618F8AD7143B</idno>
					<idno type="DOI">10.1016/j.scico.2011.06.002</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T07:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Java High Performance Computing Performance evaluation Multi-core architectures Message-passing Threads Cluster InfiniBand</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The rising interest in Java for High Performance Computing (HPC) is based on the appealing features of this language for programming multi-core cluster architectures, particularly the built-in networking and multithreading support, and the continuous increase in Java Virtual Machine (JVM) performance. However, its adoption in this area is being delayed by the lack of analysis of the existing programming options in Java for HPC and thorough and up-to-date evaluations of their performance, as well as the unawareness on current research projects in this field, whose solutions are needed in order to boost the embracement of Java in HPC.</p><p>This paper analyzes the current state of Java for HPC, both for shared and distributed memory programming, presents related research projects, and finally, evaluates the performance of current Java HPC solutions and research developments on two shared memory environments and two InfiniBand multi-core clusters. The main conclusions are that: (1) the significant interest in Java for HPC has led to the development of numerous projects, although usually quite modest, which may have prevented a higher development of Java in this field; (2) Java can achieve almost similar performance to natively compiled languages, both for sequential and parallel applications, being an alternative for HPC programming; (3) the recent advances in the efficient support of Java communications on shared memory and low-latency networks are bridging the gap between Java and natively compiled applications in HPC. Thus, the good prospects of Java in this area are attracting the attention of both industry and academia, which can take significant advantage of Java adoption in HPC.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Java has become a leading programming language soon after its release, especially in web-based and distributed computing environments, and it is an emerging option for High Performance Computing (HPC) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. The increasing interest in Java for parallel computing is based on its appealing characteristics: built-in networking and multithreading support, object orientation, platform independence, portability, type safety, security, it has an extensive API and a wide community of developers, and finally, it is the main training language for computer science students. Moreover, performance is no longer an obstacle. The performance gap between Java and native languages (e.g., C and Fortran) has been narrowing for the past years, thanks to the Just-in-Time (JIT) compiler of the Java Virtual Machine (JVM) that obtains native performance from Java bytecode. However, the use of Java in HPC is being delayed by the lack of analysis of the existing programming options in this area and thorough and up-to-date evaluations of their performance, as well as the unawareness on current research projects in Java for HPC, whose solutions are needed in order to boost its adoption.</p><p>Regarding HPC platforms, new deployments are increasing significantly the number of cores installed in order to meet the ever growing computational power demand. This current trend to multi-core clusters underscores the importance of parallelism and multithreading capabilities <ref type="bibr" target="#b2">[3]</ref>. In this scenario Java represents an attractive choice for the development of parallel applications as it is a multithreaded language and provides built-in networking support, key features for taking full advantage of hybrid shared/distributed memory architectures. Thus, Java can use threads in shared memory (intra-node) and its networking support for distributed memory (inter-node) communication. Nevertheless, although the performance gap between Java and native languages is usually small for sequential applications, it can be particularly high for parallel applications when depending on inefficient communication libraries, which has hindered Java adoption for HPC. Therefore, current research efforts are focused on providing scalable Java communication middleware, especially on high-speed networks commonly used in HPC systems, such as InfiniBand or Myrinet.</p><p>The remainder of this paper is organized as follows. Section 2 analyzes the existing programming options in Java for HPC. Section 3 describes current research efforts in this area, with special emphasis on providing scalable communication middleware for HPC. A comprehensive performance evaluation of representative solutions in Java for HPC is presented in Section 4. Finally, Section 5 summarizes our concluding remarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Java for High Performance Computing</head><p>This section analyzes the existing programming options in Java for HPC, which can be classified into: (1) shared memory programming; (2) Java sockets; (3) Remote Method Invocation (RMI); and (4) message-passing in Java. These programming options allow the development of both high-level libraries and Java parallel applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Java shared memory programming</head><p>There are several options for shared memory programming in Java for HPC, such as the use of Java threads, OpenMP-like implementations, and Titanium.</p><p>As Java has built-in multithreading support, the use of Java threads for parallel programming is quite extended due to its high performance, although it is a rather low-level option for HPC (work parallelization and shared data access synchronization are usually hard to implement). Moreover, this option is limited to shared memory systems, which provide less scalability than distributed memory machines. Nevertheless, its combination with distributed memory programming models can overcome this restriction. Finally, in order to partially relieve programmers from the low-level details of threads programming, Java has incorporated from the 1.5 specification the concurrency utilities, such as thread pools, tasks, blocking queues, and low-level high performance primitives for advanced concurrent programming like CyclicBarrier.</p><p>The project Parallel Java (PJ) <ref type="bibr" target="#b3">[4]</ref> has implemented several high-level abstractions over these concurrency utilities, such as ParallelRegion (code to be executed in parallel), ParallelTeam (group of threads that execute a ParallelRegion) and ParallelForLoop (work parallelization among threads), allowing an easy thread-base shared memory programming. Moreover, PJ also implements the message-passing paradigm as it is intended for programming hybrid shared/distributed memory systems such as multi-core clusters.</p><p>There are two main OpenMP-like implementations in Java, JOMP <ref type="bibr" target="#b4">[5]</ref> and JaMP <ref type="bibr" target="#b5">[6]</ref>. JOMP consists of a compiler (written in Java, and built using the JavaCC tool) and a runtime library. The compiler translates Java source code with OpenMP-like directives to Java source code with calls to the runtime library, which in turn uses Java threads to implement parallelism. The whole system is ''pure'' Java (100% Java), and thus can be run on any JVM. Although the development of this implementation stopped in 2000, it has been used recently to provide nested parallelism on multi-core HPC systems <ref type="bibr" target="#b6">[7]</ref>. Nevertheless, JOMP had to be optimized with some of the utilities of the concurrency framework, such as the replacement of the busy-wait implementation of the JOMP barrier by the more efficient java.util.concurrent.CyclicBarrier. The experimental evaluation of the hybrid Java message-passing + JOMP configuration (being the message-passing library thread-safe) showed up to 3 times higher performance than the equivalent pure message-passing scenario. Although JOMP scalability is limited to shared memory systems, its combination with distributed memory communication libraries (e.g., message-passing libraries) can overcome this issue. JaMP is the Java OpenMP-like implementation for Jackal <ref type="bibr" target="#b7">[8]</ref>, a software-based Java Distributed Shared Memory (DSM) implementation. Thus, this project is limited to this environment. JaMP has followed the JOMP approach, but taking advantage of the concurrency utilities, such as tasks, as it is a more recent project.</p><p>The OpenMP-like approach has several advantages over the use of Java threads, such as the higher-level programming model with a code much closer to the sequential version and the exploitation of the familiarity with OpenMP, thus increasing programmability. However, current OpenMP-like implementations are still preliminary works and lack efficiency (busywait JOMP barrier) and portability (JaMP).</p><p>Titanium <ref type="bibr" target="#b8">[9]</ref> is an explicitly parallel dialect of Java developed at UC Berkeley which provides the Partitioned Global Address Space (PGAS) programming model, like UPC and Co-array Fortran, thus achieving higher programmability. Besides the features of Java, Titanium adds flexible and efficient multi-dimensional arrays and an explicitly parallel SPMD control model with lightweight synchronization. Moreover, it has been reported that it outperforms Fortran MPI code <ref type="bibr" target="#b9">[10]</ref>, thanks to its source-to-source compilation to C code and the use of native libraries, such as numerical and high-speed network communication libraries. However, Titanium presents several limitations, such as the avoidance of the use of Java threads and the lack of portability as it relies on Titanium and C compilers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Java sockets</head><p>Sockets are a low-level programming interface for network communication, which allows sending streams of data between applications. The socket API is widely extended and can be considered the standard low-level communication layer as there are socket implementations on almost every network protocol. Thus, sockets have been the choice for implementing in Java the lowest level of network communication. However, Java sockets usually lack efficient high-speed networks support <ref type="bibr" target="#b10">[11]</ref>, so it has to resort to inefficient TCP/IP emulations for full networking support. Examples of TCP/IP emulations are IP over InfiniBand (IPoIB), IPoMX on top of the Myrinet low-level library MX (Myrinet eXpress), and SCIP on SCI.</p><p>Java has two main sockets implementations, the widely extended Java IO sockets, and Java NIO (New I/O) sockets which provide scalable non-blocking communication support. However, both implementations do not provide high-speed network support nor HPC tailoring. Ibis sockets partly solve these issues adding Myrinet support and being the base of Ibis <ref type="bibr" target="#b11">[12]</ref>, a parallel and distributed Java computing framework. However, their implementation on top of the JVM sockets library limits their performance benefits.</p><p>Java Fast Sockets (JFS) <ref type="bibr" target="#b10">[11]</ref> is our high performance Java socket implementation for HPC. As JVM IO/NIO sockets do not provide high-speed network support nor HPC tailoring, JFS overcomes these constraints by: (1) reimplementing the protocol for boosting shared memory (intra-node) communication; (2) supporting high performance native sockets communication over SCI Sockets, Sockets-MX, and Socket Direct Protocol (SDP), on SCI, Myrinet and InfiniBand, respectively; (3) avoiding the need of primitive data type array serialization; and (4) reducing buffering and unnecessary copies. Thus, JFS is able to reduce significantly JVM sockets communication overhead. Furthermore, its interoperability and user and application transparency through reflection allow for its immediate applicability on a wide range of parallel and distributed target applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Java Remote Method Invocation</head><p>The Java Remote Method Invocation (RMI) protocol allows an object running in one JVM to invoke methods on an object running in another JVM, providing Java with remote communication between programs equivalent to Remote Procedure Calls (RPCs). The main advantage of this approach is its simplicity, although the main drawback is the poor performance shown by the RMI protocol.</p><p>ProActive <ref type="bibr" target="#b12">[13]</ref> is an RMI-based middleware for parallel, multithreaded and distributed computing focused on Grid applications. ProActive is a fully portable ''pure'' Java (100% Java) middleware whose programming model is based on a Meta-Object protocol. With a reduced set of simple primitives, this middleware simplifies the programming of Grid computing applications: distributed on Local Area Network (LAN), on clusters of workstations, or for the Grid. Moreover, ProActive supports fault-tolerance, load-balancing, mobility, and security. Nevertheless, the use of RMI as its default transport layer adds significant overhead to the operation of this middleware.</p><p>The optimization of the RMI protocol has been the goal of several projects, such as KaRMI <ref type="bibr" target="#b13">[14]</ref>, RMIX <ref type="bibr" target="#b14">[15]</ref>, Manta <ref type="bibr" target="#b15">[16]</ref>, Ibis RMI <ref type="bibr" target="#b11">[12]</ref>, and Opt RMI <ref type="bibr" target="#b16">[17]</ref>. However, the use of non-standard APIs, the lack of portability, and the insufficient overhead reductions, still significantly larger than socket latencies, have restricted their applicability. Therefore, although Java communication middleware (e.g., message-passing libraries) used to be based on RMI, current Java communication libraries use sockets due to their lower overhead. In this case, the higher programming effort required by the lower-level API allows for higher throughput, key in HPC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Message-passing in Java</head><p>Message-passing is the most widely used parallel programming paradigm as it is highly portable, scalable and usually provides good performance. It is the preferred choice for parallel programming distributed memory systems such as clusters, which can provide higher computational power than shared memory systems. Regarding the languages compiled to native code (e.g., C and Fortran), MPI is the standard interface for message-passing libraries.</p><p>Soon after the introduction of Java, there have been several implementations of Java message-passing libraries (eleven projects are cited in <ref type="bibr" target="#b17">[18]</ref>). However, most of them have developed their own MPI-like binding for the Java language. The two main proposed APIs are the mpiJava 1.2 API <ref type="bibr" target="#b18">[19]</ref>, which tries to adhere to the MPI C++ interface defined in the MPI standard version 2.0, but restricted to the support of the MPI 1.1 subset, and the JGF MPJ (message-passing interface for Java) API <ref type="bibr" target="#b19">[20]</ref>, which is the proposal of the Java Grande Forum (JGF) <ref type="bibr" target="#b20">[21]</ref> to standardize the MPI-like Java API. The main differences among these two APIs lie on naming conventions of variables and methods.</p><p>The message-passing in Java (MPJ) libraries can be implemented: (1) using Java RMI; <ref type="bibr" target="#b1">(2)</ref> wrapping an underlying native messaging library like MPI through Java Native Interface (JNI); or (3) using Java sockets. Each solution fits with specific situations, but presents associated trade-offs. The use of Java RMI, a ''pure'' Java (100% Java) approach, as base for MPJ libraries, ensures portability, but it might not be the most efficient solution, especially in the presence of high-speed  <ref type="bibr" target="#b22">[23]</ref> Jcluster <ref type="bibr" target="#b23">[24]</ref> Parallel Java <ref type="bibr" target="#b3">[4]</ref> mpiJava <ref type="bibr" target="#b21">[22]</ref> P2P-MPI <ref type="bibr" target="#b24">[25]</ref> MPJ Express <ref type="bibr" target="#b6">[7]</ref> MPJ/Ibis <ref type="bibr" target="#b25">[26]</ref> JMPI <ref type="bibr" target="#b26">[27]</ref> F-MPJ <ref type="bibr" target="#b27">[28]</ref> communication hardware. The use of JNI has portability problems, although usually in exchange for higher performance. The use of a low-level API, Java sockets, requires an important programming effort, especially in order to provide scalable solutions, but it significantly outperforms RMI-based communication libraries. Although most of the Java communication middleware is based on RMI, MPJ libraries looking for efficient communication have followed the latter two approaches.</p><p>The mpiJava library <ref type="bibr" target="#b21">[22]</ref> consists of a collection of wrapper classes that call a native MPI implementation (e.g., MPICH2 or OpenMPI) through JNI. This wrapper-based approach provides efficient communication relying on native libraries, adding a reduced JNI overhead. However, although its performance is usually high, mpiJava currently only supports some native MPI implementations, as wrapping a wide number of functions and heterogeneous runtime environments entails an important maintaining effort. Additionally, this implementation presents instability problems, derived from the native code wrapping, and it is not thread-safe, being unable to take advantage of multi-core systems through multithreading.</p><p>As a result of these drawbacks, the mpiJava maintenance has been superseded by the development of MPJ Express <ref type="bibr" target="#b6">[7]</ref>, a ''pure'' Java message-passing implementation of the mpiJava 1.2 API specification. MPJ Express is thread-safe and presents a modular design which includes a pluggable architecture of communication devices that allows to combine the portability of the ''pure'' Java shared memory (smpdev device) and New I/O package (Java NIO) communications (niodev device) with the high performance Myrinet support (through the native Myrinet eXpress (MX) communication library in mxdev device).</p><p>Currently, MPJ Express is the most active project in terms of uptake by the HPC community, presence on academia and production environments, and available documentation. This project is also stable and publicly available along with its source code.</p><p>In order to update the compilation of Java message-passing implementations presented in <ref type="bibr" target="#b17">[18]</ref>, this paper presents the projects developed since 2003, in chronological order:</p><p>• MPJava <ref type="bibr" target="#b22">[23]</ref> is the first Java message-passing library implemented on Java NIO sockets, taking advantage of their scalability and high performance communications.</p><p>• Jcluster <ref type="bibr" target="#b23">[24]</ref> is a message-passing library which provides both PVM-like and MPI-like APIs and is focused on automatic task load balance across large-scale heterogeneous clusters. However, its communications are based on UDP and it lacks high-speed networks support.</p><p>• Parallel Java (PJ) <ref type="bibr" target="#b3">[4]</ref> is a ''pure'' Java parallel programming middleware that supports both shared memory programming (see Section 2.1) and an MPI-like message-passing paradigm, allowing applications to take advantage of hybrid shared/distributed memory architectures. However, the use of its own API makes its adoption difficult.</p><p>• P2P-MPI <ref type="bibr" target="#b24">[25]</ref> is a peer-to-peer framework for the execution of MPJ applications on the Grid. Among its features are:</p><p>(1) self-configuration of peers (through JXTA peer-to-peer technology); (2) fault-tolerance, based on process replication;</p><p>(3) a data management protocol for file transfers on the Grid; and (4) an MPJ implementation that can use either Java NIO or Java IO sockets for communications, although it lacks high-speed networks support. In fact, this project is tailored to grid computing systems, disregarding the performance aspects.</p><p>• MPJ/Ibis <ref type="bibr" target="#b25">[26]</ref> is the only JGF MPJ API implementation up to now. This library can use either ''pure'' Java communications, or native communications on Myrinet. Moreover, there are two low-level communication devices available in Ibis for MPJ/Ibis communications: TCPIbis, based on Java IO sockets (TCP), and NIOIbis, which provides blocking and non-blocking communication through Java NIO sockets. Nevertheless, MPJ/Ibis is not thread-safe, and its Myrinet support is based on the GM library, which shows poorer performance than the MX library.</p><p>• JMPI <ref type="bibr" target="#b26">[27]</ref> is an implementation which can use either Java RMI or Java sockets for communications. However, the reported performance is quite low (it only scales up to two nodes).</p><p>• Fast MPJ (F-MPJ) <ref type="bibr" target="#b27">[28]</ref> is our Java message-passing implementation which provides high-speed networks support, both direct and through Java Fast Sockets (see Section 3.1). F-MPJ implements the mpiJava 1.2 API, the most widely extended, and includes a scalable MPJ collectives library <ref type="bibr" target="#b28">[29]</ref>.</p><p>Table <ref type="table" target="#tab_0">1</ref> serves as a summary of the Java message-passing projects discussed in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Java for HPC: current research</head><p>This section describes current research efforts in Java for HPC, which can be classified into: (1) design and implementation of low-level Java message-passing devices; (2) improvement of the scalability of Java message-passing collective primitives;</p><p>(3) automatic selection of MPJ collective algorithms; (4) implementation and evaluation of efficient MPJ benchmarks;</p><p>(5) language extensions in Java for parallel programming paradigms; and (6) Java libraries to support data parallelism. These ongoing projects are providing Java with several evaluations of their suitability for HPC, as well as solutions for increasing their performance and scalability in HPC systems with high-speed networks and hardware accelerators such as Graphics Processing Units (GPUs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Low-level Java message-passing communication devices</head><p>The use of pluggable low-level communication devices for high performance communication support is widely extended in native message-passing libraries. Both MPICH2 and OpenMPI include several devices on Myrinet, InfiniBand and shared memory. Regarding MPJ libraries, in MPJ Express the low-level xdev layer <ref type="bibr" target="#b6">[7]</ref> provides communication devices for different interconnection technologies. The three implementations of the xdev API currently available are niodev (over Java NIO sockets), mxdev (over Myrinet MX), and smpdev (shared memory communication), which has been introduced recently <ref type="bibr" target="#b29">[30]</ref>. This latter communication device has two implementations, one thread-based (pure Java) and the other based on native IPC resources.</p><p>F-MPJ communication devices conform with the xxdev API <ref type="bibr" target="#b27">[28]</ref>, which supports the direct communication of any serializable object without data buffering, whereas xdev, the API that xxdev is extending, does not support this direct communication, relying on a buffering layer (mpjbuf layer). Additional benefits of the use of this API are its flexibility, portability and modularity thanks to its encapsulated design.</p><p>The xxdev API (see Listing 1) has been designed with the goal of being simple and small, providing only basic communication methods in order to ease the development of xxdev devices. In fact, this API is composed of 13 simple methods, which implement basic message-passing operations, such as point-to-point communication, both blocking (send and recv, like MPI_Send and MPI_Recv) and non-blocking (isend and irecv, like MPI_Isend and MPI_Irecv). Moreover, synchronous communications are also embraced (ssend and issend). However, these communication methods use ProcessID objects instead of using ranks as arguments to send and receive primitives. In fact, the xxdev layer is focused on providing basic communication methods and it does not deal with high-level message-passing abstractions such as groups and communicators. Therefore, a ProcessID object unequivocally identifies a device object.  The initial implementation of F-MPJ included only one communication device, iodev, implemented on top of Java IO sockets, which therefore can rely on top of JFS and hence obtain high performance on shared memory and Gigabit Ethernet, SCI, Myrinet, and InfiniBand networks. However, the use of sockets in a communication device, despite the high performance provided by JFS, still represents an important source of overhead in Java communications. Thus, F-MPJ is including the direct support of communications on high performance native communication layers, such as MX and IBV. This direct access of Java to InfiniBand network was somewhat restricted so far to MPI libraries. Like mxdev, this device has to deal with the Java Objects communication and the JNI transfers, and additionally with the communication protocols operation. Finally, both mxdev and ibvdev, although they have been primarily designed for network communication, support shared memory intra-node communication. However, smpdev device is the thread-based communication device that should support more efficiently shared memory transfers. This device isolates a naming space for each running thread (relying on custom class loaders) and allocates shared message queues in order to implement the communications as regular data copies between threads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">MPJ collectives scalability</head><p>MPJ application developers use collective primitives for performing standard data movements (e.g., Broadcast, Scatter, Gather and Alltoall or total exchange) and basic computations among several processes (reductions). This greatly simplifies code development, enhancing programmers productivity together with MPJ programmability. Moreover, it relieves developers from communication optimization. Thus, collective algorithms, which generally consist of multiple pointto-point communications, must provide scalable performance, usually through overlapping communications in order to maximize the number of operations carried out in parallel. An unscalable algorithm can easily waste the performance provided by an efficient communication middleware.</p><p>The design, implementation and runtime selection of efficient collective communication operations have been extensively discussed in the context of native message-passing libraries <ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref>, while there is little discussion in MPJ, except for F-MPJ, which provides a scalable and efficient MPJ collective communication library <ref type="bibr" target="#b28">[29]</ref> for parallel computing on multicore architectures. This library provides multi-core aware primitives, implements several algorithms per collective operation, and explores thread-based communications, obtaining significant performance benefits in communication-intensive MPJ applications.</p><p>The collective algorithms present in MPJ libraries can be classified in six types, namely Flat Tree (FT) or linear, Minimum-Spanning Tree (MST), Binomial Tree (BT), Four-ary Tree (FaT), Bucket (BKT) or cyclic, and BiDirectional Exchange (BDE) or recursive doubling, which are extensively described in <ref type="bibr" target="#b31">[32]</ref>. Table <ref type="table" target="#tab_2">2</ref> presents a complete list of the collective algorithms used in MPJ Express and F-MPJ (the prefix ''b'' means that only blocking point-to-point communication is used, whereas ''nb'' refers to the use of non-blocking primitives). It can be seen that F-MPJ implements up to six algorithms per collective primitive, allowing their selection at runtime, as well as it takes more advantage of communications overlapping, achieving higher performance scalability. Regarding the memory requirements of the collective primitives, some algorithms require more memory than others (e.g., the MST algorithm for the Scatter and Gather demands more memory than the FT algorithm). Thus, when experiencing memory limitations the algorithms with less memory requirements must be selected in order to overcome the limitation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Automatic selection of MPJ collective algorithms</head><p>The F-MPJ collectives library allows the runtime selection of the collective algorithm that provides the highest performance in a given multi-core system, among the several algorithms available, based on the message size and the number of processes. The definition of a threshold for each of these two parameters allows the selection of up to four algorithms per collective primitive. Moreover, these thresholds can be configured for a particular system by means of an autotuning process, which obtains an optimal selection of algorithms, based on the particular performance results on a specific system and taking into account the particularities of the Java execution model.</p><p>The information of the selected algorithms is stored in a configuration file that, if available in the system, is loaded at MPJ initialization, otherwise the default algorithms are selected, thus implementing a portable and user transparent approach. The autotuning process consists of the execution of our own MPJ collectives micro-benchmark suite <ref type="bibr" target="#b17">[18]</ref>, the gathering of their experimental results, and finally the generation of the configuration file that contains the algorithms that maximize performance. The performance results have been obtained on a number of processes power of two, up to the total number of cores of the system, and for message sizes power of two. The parameter thresholds, which are independently configured for each collective, are those that maximize the performance measured by the micro-benchmark suite. Moreover, this autotuning process is required to be executed only once per system configuration in order to generate the configuration file. After that MPJ applications would take advantage of this information.</p><p>Table <ref type="table" target="#tab_3">3</ref> presents the information contained in the optimum configuration file for the x86-64 multi-core cluster used in the experimental evaluation presented in this paper (Section 4). The thresholds between short and long messages, and between small and large number of processes are specific for each collective, although in the evaluated testbeds their values are generally 32 Kbytes and 16 processes, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Implementation and evaluation of efficient HPC benchmarks</head><p>Java lacks efficient HPC benchmarking suites for characterizing its performance, although the development of efficient Java benchmarks and the assessment of their performance is highly important. The JGF benchmark suite <ref type="bibr" target="#b34">[35]</ref>, the most widely used Java HPC benchmarking suite, presents quite inefficient codes, as well as it does not provide the native language counterparts of the Java parallel codes, preventing their comparative evaluation. Therefore, we have implemented the NAS Parallel Benchmarks (NPB) suite for MPJ (NPB-MPJ) <ref type="bibr" target="#b35">[36]</ref>, selected as this suite is the most extended in HPC evaluations, with implementations for MPI (NPB-MPI), OpenMP (NPB-OMP), Java threads (NPB-JAV) and ProActive (NPB-PA).</p><p>NPB-MPJ allows, as main contributions: (1) the comparative evaluation of MPJ libraries; (2) the analysis of MPJ performance against other Java parallel approaches (e.g., Java threads); (3) the assessment of MPJ versus native MPI scalability; (4) the study of the impact on performance of the optimization techniques used in NPB-MPJ, from which Java HPC applications can potentially benefit. The description of the NPB-MPJ benchmarks implemented is next shown in Table <ref type="table" target="#tab_4">4</ref>.</p><p>In order to maximize NPB-MPJ performance, the ''plain objects'' design has been chosen as it reduces the overhead of the ''pure'' object-oriented design (up to 95% overhead reduction). Thus, each benchmark uses only one object instead of defining an object per each element of the problem domain. Thus, complex numbers are implemented as two-element arrays instead of complex numbers objects.</p><p>The inefficient multi-dimensional array support in Java (an n-dimensional array is defined as an array of n-1-dimensional arrays, so data is not guaranteed to be contiguous in memory) imposed a significant performance penalty in NPB-MPJ, which handles arrays of up to five dimensions. This overhead was reduced through the array flattening optimization, which consists of the mapping of a multi-dimensional array in a one-dimensional array. Thus, adjacent elements in the C/Fortran versions are also contiguous in Java, allowing the data locality exploitation.</p><p>Finally, the implementation of the NPB-MPJ takes advantage of the JVM JIT (Just-in-Time) compiler-based optimizations. The JIT compilation of the bytecode (or even its recompilation in order to apply further optimizations) is reserved to heavily used methods, as it is an expensive operation that increases significantly the runtime. Thus, the NPB-MPJ codes have been refactored toward simpler and independent methods, such as methods for mapping elements from multi-dimensional to one-dimensional arrays, and complex number operations. As these methods are invoked more frequently, the JVM gathers more runtime information about them, allowing a more effective optimization of the target bytecode.</p><p>The performance of NPB-MPJ significantly improved using these techniques, achieving up to 2800% throughput increase (on SP benchmark). Furthermore, we believe that other Java HPC codes can potentially benefit from these optimization techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Language extensions in Java for parallel programming paradigms</head><p>Regarding language extensions in Java to support various parallel programming paradigms, X10 and Habanero Java deserve to be mentioned. X10 <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38]</ref> is an emerging Java-based programming language developed in the DARPA program on High Productivity Computer Systems (HPCS). Moreover, it is an APGAS (Asynchronous Partitioned Global Address Space) language implementation focused on programmability which supports locality exploitation, lightweight synchronization, and productive parallel programming. Additionally, an ongoing project based on X10 is Habanero Java <ref type="bibr" target="#b38">[39]</ref>, focused on supporting productive parallel programming on extreme scale homogeneous and heterogeneous multi-core platforms. It allows to take advantage of X10 features in shared memory systems together with the Java Concurrency framework. Both X10 and Habanero Java applications can be compiled with C++ or Java backends, although looking for performance the use of the C++ one is recommended. Nevertheless, these are still experimental projects with limited performance, especially for X10 arrays handling, although X10 has been reported to rival Java threads performance on shared memory <ref type="bibr" target="#b39">[40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Java libraries to support data parallelism</head><p>There are several ongoing efforts in the support in Java of data parallelism using hardware accelerators, such as GPUs, once they have emerged as a viable alternative for significantly improving the performance of appropriate applications. On the one hand this support can be implemented in the compiler, at language level such as for JCUDA <ref type="bibr" target="#b40">[41]</ref>. On the other hand, the interface to these accelerators can be library-based, such as the following Java bindings of CUDA: jcuda.org [42], jCUDA <ref type="bibr" target="#b41">[43]</ref>, JaCuda <ref type="bibr" target="#b42">[44]</ref>, Jacuzzi <ref type="bibr" target="#b43">[45]</ref>, and java-gpu <ref type="bibr" target="#b44">[46]</ref>.</p><p>Furthermore, the bindings are not restricted to CUDA as there are several Java bindings for OpenCL: jocl.org [47], JavaCL <ref type="bibr" target="#b45">[48]</ref>, and JogAmp <ref type="bibr" target="#b46">[49]</ref>.</p><p>This important number of projects is an example of the interest of the research community in supporting data parallelism in Java, although their efficiency is lower than using directly CUDA/OpenCL due to the overhead associated to the Java data movements to and from the GPU, the support of the execution of user-written CUDA code from Java programs and the automatic support for data transfer of primitives and multi-dimensional arrays of primitives. An additional project that targets these sources of inefficiency is JCudaMP <ref type="bibr" target="#b47">[50]</ref>, an OpenMP framework that exploits more efficiently GPUs. Finally, another approach for Java performance optimization on GPUs is the direct generation of GPU-executable code (without JNI access to CUDA/OpenCL) by a research Java compiler, Jikes, which is able to automatically parallelize loops <ref type="bibr" target="#b48">[51]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Performance evaluation</head><p>This paper presents an up-to-date comparative performance evaluation of representative MPJ libraries, F-MPJ and MPJ Express, on two shared memory environments and two InfiniBand multi-core clusters. First, the performance of pointto-point MPJ primitives on InfiniBand, 10 Gigabit Ethernet and shared memory is presented. Next, this section evaluates the results gathered from a micro-benchmarking of MPJ collective primitives. Finally, the impact of MPJ libraries on the scalability of representative parallel codes, both NPB-MPJ kernels and the Gadget2 application <ref type="bibr" target="#b49">[52]</ref>, has been assessed comparatively with MPI, Java threads and OpenMP performance results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental configuration</head><p>Two systems have been used in this performance evaluation, a multi-core x86-64 Infiniband cluster and the Finis Terrae supercomputer <ref type="bibr" target="#b50">[53]</ref>. The first system (from now on x86-64 cluster) is a 16-node cluster with 16 Gbytes of memory and 2 x86-64 Xeon E5620 quad-core Nehalem-based ''Gulftown'' processors at 2.40 GHz per node (hence 128 physical cores in the cluster). The interconnection network is InfiniBand (QLogic IBA7220 4x DDR, 16 Gbps), although 2 of the nodes have additionally a 10 Gigabit Ethernet NIC (Intel PRO/10GbE NIC). As each node has 8 physical cores, and 16 logical cores when hyperthreading is enabled, shared memory performance has been also evaluated on one node of the cluster, using up to 16 processes/threads. The performance results on this system have been obtained using one core per node, except for 32, 64 and 128 processes, for which 2, 4 and 8 cores per node, respectively, have been used.</p><p>The OS is Linux CentOS 5. The second system used is the Finis Terrae supercomputer (14 TFlops), an InfiniBand cluster which consists of 142 HP Integrity rx7640 nodes, each of them with 16 Montvale Itanium2 (IA64) cores at 1.6 GHz and 128 Gbytes of memory. The InfiniBand NIC is a 4X DDR Mellanox MT25208 (16 Gbps). Additionally an HP Integrity Superdome system with 64 Montvale Itanium 2 dual-core processors (total 128 cores) at 1.6 GHz and 1 TB of memory has also been used for the shared memory evaluation. The OS of the Finis Terrae is SUSE Linux Enterprise Server 10 with Intel compiler 10.1.074 (used with thefast flag) and GNU compiler (used with the -O3 flag) version 4.1.2. Regarding native message-passing libraries, HP-MPI 2.2.5.1 has been selected as it achieves the highest performance on InfiniBand and shared memory on the Finis Terrae. The InfiniBand drivers are OFED version 1.4. The JVM is Oracle JDK 1.6.0_20 for IA64. The poor performance of Java on IA64 architectures, due to the lack of mature support for this processor in the Java Just-In-Time compiler, has motivated the selection of this system only for the analysis of the performance scalability of MPJ applications, due to its high number of cores. The performance results on this system have been obtained using 8 cores per node, the recommended configuration for maximizing performance. In fact, the use of a higher number of cores per node increases significantly network contention and memory access bottlenecks.</p><p>Regarding the benchmarks, Intel MPI Benchmarks (IMB, formerly Pallas) and our own MPJ micro-benchmark suite, which tries to adhere to IMB measurement methodology, have been used for the message-passing primitives evaluation. Moreover, the NPB-MPI/NPB-OMP version 3.3 and the NPB-JAV version 3.0 have been used together with our own NPB-MPJ implementation <ref type="bibr" target="#b35">[36]</ref>. The metrics that have been considered for the NPB evaluation are the speedup and MOPS (Millions of Operations Per Second), which measures the operations performed in the benchmark, that differ from the CPU operations issued. Moreover, NPB Class C workloads have been selected as they are the largest workloads that can be executed in a single node, which imposes the restriction of using workloads with memory requirements below 16 Gbytes (the amount of memory available in a node of the x86-64 cluster).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Performance evaluation methodology</head><p>All performance results presented in this paper are the median of 5 measurements in case of the kernels and applications and the median of up to the 1000 samples measured for the collective operations. The selection of the most appropriate performance evaluation methodology in Java has been thoroughly addressed in <ref type="bibr" target="#b51">[54]</ref>, concluding that the median is considered one of the best measures as its accuracy seems to improve with the number of measurements, which is in tune with the results reported in this paper.</p><p>Regarding the influence of JIT compilation in HPC performance results, the use of long-running codes (with runtimes of several hours and days) generally involves the use of a high percentage of JIT compiled code, which eventually improves performance. Moreover, the JVM execution mode selected for the performance evaluation is the default one (mixed mode) which compiles dynamically at runtime, based on profiling information, the bytecode of costly methods to native code, while interprets inexpensive pieces of code without incurring in runtime compilation overheads. Thus, this mode is able to provide higher performance than the use of the interpreted and even the compiled (an initial static compilation) execution modes. In fact, we have experimentally assessed the higher performance of the use of the mixed mode for the evaluated codes, whose percentage of runtime of natively compiled code is generally higher than 95% (hence, less than 5% of the runtime is generally devoted to interpreted code).</p><p>Furthermore, the non-determinism of JVM executions leads to oscillations in the time measures of Java applications. The main sources of variation are the JIT compilation and optimization in the JVM driven by a timer-based method sampling, thread scheduling, and garbage collection. However, the exclusive access to HPC resources and the characteristics of HPC applications (e.g., numerical intensive computation and a restricted use of object-oriented features such as extensions and handling numerous objects) limit the variations in the experimental results of Java. In order to assess the variability of representative Java codes in HPC, the NPB kernels evaluated in this paper (CG, FT, IS and MG with Class C problem size) have been executed 40 times, both using F-MPJ and MPI, on 64 and 128 cores of the x86-64 cluster. Regarding message-passing primitives, both point-to-point and collectives include calls to native methods, which provide efficient communications on high-speed networks, thus obtaining performance results close to the theoretical limits of the network hardware. Moreover, their performance measures, when relying on native methods, provide results with little variation among iterations. Only message-passing transfers on shared memory present a high variability due to the scheduling of the threads on different cores within a node. In this scenario the performance results depend significantly on the scheduling of the threads on cores that belong to the same processor and that even can share some cache levels. Nevertheless, due to space restrictions a detailed analysis of the impact of thread scheduling on Java communications performance cannot be included in this paper. Thus, only the NPB kernels have been selected for the analysis of the performance variability of Java in HPC due to their balance in the combination of computation and communication as well as for their representativeness in HPC evaluation.</p><p>Fig. <ref type="figure" target="#fig_3">2</ref> presents speedup graphs with box and whisker diagrams for the evaluated benchmarks, showing the measure of the minimum sample, the lower quartile (Q1), the median (Q2), upper quartile (Q3), and the maximum sample. The selected metric, speedup, has been selected for clarity purposes, as it allows a straightforward analysis of F-MPJ and MPI results, especially for the comparison of their range of values, which lie closer using speedups than other metrics such as execution times.</p><p>The analysis of the variability of the performance of these NPB kernels shows that F-MPJ results present similar variability as MPI codes, although for CG and FT on 128 cores the NPB-MPJ measures present higher variations than their natively compiled counterparts (MPI kernels). However, even in this scenario the variability of the Java codes is less than 10% of the speedup value (the measured speedups fall in the range of 90% and 110% of the median value), whereas the average variation is less than 5% of the speedup value. Furthermore, there is no clear evidence of the increase of the variability with the number of cores, except for NPB-MPJ CG and FT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experimental performance results on one core</head><p>Fig. <ref type="figure" target="#fig_4">3</ref> shows a performance comparison of several NPB implementations on one core from the x86-64 cluster (left graph) and on one core from the Finis Terrae (right graph). The results are shown in terms of speedup relative to the MPI library (using the GNU C/Fortran compiler), Runtime(NPB-MPI benchmark) / Runtime(NPB benchmark). Thus, a value higher than 1 means than the evaluated benchmark achieves higher performance (shorter runtime) than the NPB-MPI benchmark, whereas a value lower than 1 means than the evaluated code shows poorer performance (longer runtime) than the NPB-MPI benchmark. The NPB implementations and NPB kernels evaluated are those that will be next used in this section for the performance analysis of Java kernels (Section 4.6.1). Moreover, only F-MPJ results are shown for NPB-MPJ performance for clarity purposes, as other MPJ libraries (e.g., MPJ Express) obtain quite similar results on one core.</p><p>The differences in performance that can be noted in the graphs are explained by the different implementations of the NPB benchmarks, the use of Java or native code (C/Fortran), and for native code the compiler being used (Intel or GNU compiler). Regarding Java performance, as the JVM used in this performance evaluation, the Oracle JVM for Linux, has been built with the GNU compiler, Java performance is limited by the throughput achieved with this compiler. Thus, Java codes (MPJ and Threads) cannot generally outperform their equivalent GNU-built benchmarks. This fact is of special relevance on the Finis Terrae, where the GNU compiler is not able to take advantage of the Montvale Itanium2 (IA64) processor, whereas the Intel compiler does. As a consequence of this, the performance of Java kernels on the Finis Terrae is significantly lower, even an order of magnitude lower, than the performance of the kernels built with the Intel compiler. The performance of Java kernels on the x86-64 cluster is close to the natively compiled kernels for CG and IS, whereas for FT and MG Java performance is approximately 55% of the performance of MPI kernels built with the GNU compiler.</p><p>This analysis of the performance of Java and natively compiled codes on the x86-64 cluster and the Finis Terrae has also verified that the use of the Intel compiler shows better performance results than the use of the GNU compiler, especially on the Finis Terrae. Thus, from now on only the Intel compiler has been used in the performance evaluation included in this paper, although a fair comparison with Java would have considered the GNU compiler (both Oracle JVM and the GNU compiler are freely available software). However, the use of the compiler provided by the processor vendor is the most generally adopted solution in HPC. Furthermore, a wider availability of JVMs built with commercial compilers would improve this scenario, especially on Itanium platforms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Message-passing point-to-point micro-benchmarking</head><p>The performance of message-passing point-to-point primitives has been measured on the x86-64 cluster using our own MPJ micro-benchmark suite and IMB. Regarding Finis Terrae, its results are not considered for clarity purposes, as well as due to the poor performance of Java on this system. Moreover, Finis Terrae communication mechanisms, InfiniBand and shared memory, are already covered in the x86-64 cluster evaluation.</p><p>Fig. <ref type="figure" target="#fig_5">4</ref> presents message-passing point-to-point latencies (for short messages) and bandwidths (for long messages) on InfiniBand (top graph), 10 Gigabit Ethernet (middle graph) and shared memory (bottom graph). Here, the results shown are the half of the round-trip time of a pingpong test or its corresponding bandwidth.</p><p>On the one hand these results show that F-MPJ is quite close to MPI performance, which means that F-MPJ is able to take advantage of the low latency and high throughput provided by shared memory and these high-speed networks. In fact, F-MPJ obtains start-up latencies as low as 2 µs on shared memory, 10 µs on InfiniBand and 12 µs on 10 Gigabit Ethernet. Regarding throughput, F-MPJ significantly outperforms MPI for 4 Kbytes and larger messages on shared memory when using smpdev communication device, achieving up to 51 Gbps thanks to the exploitation of the thread-based intraprocess communication mechanism, whereas the inter-process communication protocols implemented in MPI and the F-MPJ network-based communication devices (ibvdev and mxdev) are limited to less than 31 Gbps.</p><p>On the other hand, MPJ Express point-to-point performance suffers from the lack of specialized support on InfiniBand, having to rely on NIO sockets over IP emulation IPoIB, and the use of a buffering layer, which adds noticeable overhead for long messages. Moreover, the communication protocols implemented in this library show a significant start-up latency. In fact, MPJ Express and F-MPJ rely on the same communication layer on shared memory (intra-process transfers) and 10 Gigabit Ethernet (Open-MX library), but MPJ Express adds an additional overhead of 8 µs and 11 µs, respectively, over F-MPJ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Message-passing collective primitives micro-benchmarking</head><p>Fig. <ref type="figure" target="#fig_6">5</ref> presents the performance of representative message-passing data movement operations (Broadcast and Allgather), and computational operations (Reduce and Allreduce double precision sum operations), as well as their associated scalability using a representative message size (32 Kbytes). The results, obtained using 128 processes on the x86-64 cluster, are represented using aggregated bandwidth metric as this metric takes into account the global amount of data transferred, generally message size * number of processes.</p><p>The original MPJ Express collective primitives use the algorithms listed in Table <ref type="table" target="#tab_2">2</ref> (column MPJ Express), whereas F-MPJ collectives library uses the algorithms that maximize the performance on this cluster according to the automatic performance tunning process. The selected algorithms are presented in Table <ref type="table" target="#tab_5">5</ref>, which extracts from the configuration file the most relevant information about the evaluated primitives.</p><p>The results confirm that F-MPJ is bridging the gap between MPJ and MPI collectives performance, but there is still room for improvement, especially when using several processes per node as F-MPJ collectives are not taking full advantage of the   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Java HPC kernel/application performance analysis</head><p>The scalability of Java for HPC has been analyzed using the NAS Parallel Benchmarks (NPB) implementation for MPJ (NPB-MPJ) <ref type="bibr" target="#b35">[36]</ref>. The selection of the NPB has been motivated by its widespread adoption in the evaluation of languages, libraries and middleware for HPC. In fact, there are implementations of this benchmarking suite for MPI (NPB-MPI), Java Threads (NPB-JAV), OpenMP (NPB-OMP) and hybrid MPI/OpenMP (NPB-MZ). Four representative NPB codes, those with medium/high communication intensiveness (see Table <ref type="table" target="#tab_4">4</ref>), have been evaluated: CG (Conjugate Gradient), FT (Fourier Transform), IS (Integer Sort) and MG (Multi-Grid). Furthermore, the jGadget <ref type="bibr" target="#b52">[55]</ref> cosmology simulation application has also been analyzed.</p><p>These MPJ codes have been selected for showing poor scalability in the related literature <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b49">52]</ref>. Hence, these are target codes for the analysis of the scalability of current MPJ libraries, which have been evaluated using up to 128 processes on the x86-64 cluster, and up to 256 processes on the Finis Terrae.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.1.">Java NAS parallel benchmarks performance analysis</head><p>Figs. 6 and 7 present the NPB CG, IS, FT and MG kernel results on the x86-64 cluster and Finis Terrae, respectively, for the Class C workload in terms of MOPS (Millions of Operations Per Second) (left graphs) and their corresponding scalability, in terms of speedup (right graphs). These four kernels (CG, IS, FT and MG) have been selected as they present medium or high communication intensiveness (see Table <ref type="table" target="#tab_4">4</ref>). The two remaining kernels, EP and SP, were discarded due to their low communication intensiveness (see Table <ref type="table" target="#tab_4">4</ref>) so their results show high scalability, having limited abilities to assess the impact of multithreading and MPJ libraries on the scalability of parallel codes. The NPB implementations used are NPB-MPI and NPB-MPJ for the message-passing scalability evaluation on distributed memory and NPB-OMP and NPB-JAV for the evaluation of shared memory performance.</p><p>Although the configuration of the shared and the distributed memory scenarios are different, they share essential features such as the processor and the architecture of the system, so their results are shown together in order to ease their comparison. Thus, Fig. <ref type="figure" target="#fig_7">6</ref> presents NPB results of shared and distributed memory implementations measured in the x86-64 cluster. The selected NPB kernels (CG, IS, FT and MG) are implemented in the four NPB implementations evaluated, in fact the lack of some of these kernels has prevented the use of additional benchmark suites, such as the hybrid MPI/OpenMP NPB Multi-Zone (NPB-MZ), which does not implement any of these kernels.</p><p>NPB-MPI results have been obtained using the MPI library that achieves the highest performance on each system, OpenMPI on the x86-64 cluster and HP-MPI on the Finis Terrae, in both cases in combination with the Intel C/Fortran compiler. Regarding NPB-MPJ, both F-MPJ and MPJ Express have been benchmarked using the communication device that shows the best performance on InfiniBand, the interconnection network of both systems. Thus, F-MPJ has been run using its ibvdev device whereas MPJ Express relies on niodev over the IP emulation IPoIB. NPB-OMP benchmarks have been compiled with the OpenMP support included in the Intel C/Fortran compiler. Finally, NPB-JAV codes only require a standard JVM for running.</p><p>The analysis of the x86-64 cluster results (Fig. <ref type="figure" target="#fig_7">6</ref>) first reveals that F-MPJ achieves similar performance to OpenMPI for CG when using 32 and higher number of cores, showing higher speedups than the MPI library in this case. As this kernel only includes point-to-point communication primitives, F-MPJ takes advantage of obtaining similar point-to-point performance to MPI. However, MPJ Express and the Java threads implementations present poor scalability from 8 cores. On the one hand, the poor speedups of MPJ Express are direct consequence of the use of sockets and IPoIB in its communication layer. On the other hand, the poor performance of the NPB-JAV kernels is motivated by their inefficient implementation. In fact, the evaluated codes obtain lower performance on a single core than the MPI, OpenMP and MPJ kernels, except for NPB-JAV MG, which outperforms NPB-MPJ MG (see in Section 4.3 the left graph in Fig. <ref type="figure" target="#fig_4">3</ref>). The reduced performance of NPB-JAV kernels on a single core, which can incur up to 50% performance overhead compared to NPB-MPJ codes, determines the lower overall performance in terms of MOPS.</p><p>Additionally, the NPB shared memory implementations, using OpenMP and Java Threads, present poorer scalability on the x86_64 cluster than distributed memory (message-passing) implementations, except for NPB-OMP IS. The main reason behind this behavior is the memory access overhead when running 8 and even 16 threads on 8 physical cores, which thanks to hyperthreading are able to run up to 16 threads simultaneously. Thus, the main performance bottleneck for these shared memory implementations is the access to memory, which limits their scalability and prevents taking advantage of enabling hyperthreading.</p><p>Regarding FT results, although F-MPJ scalability is higher than MPI (F-MPJ speedup is about 50 on 128 cores whereas the MPI one is below 36), this is not enough for achieving similar performance in terms of MOPS. In this case MPJ performance is limited by its poor performance on one core, which is 54% of the MPI performance (see in Section 4.3 the left graph in Fig. <ref type="figure" target="#fig_4">3</ref>). Moreover, the scalability of this kernel relies on the performance of the Alltoall collective, which has not prevented F-MPJ scalability. As for CG, MPJ Express and the shared memory NPB codes show poor performance, although NPB-JAV FT presents a slightly performance benefit when resorting to hyperthreading, probably due to its poor performance on one core, which is below 30% of the NPB-MPI FT result. In fact, a longer runtime reduces the impact of communications and memory bottlenecks in the scalability of parallel codes.</p><p>The significant communication intensiveness of IS, the highest among the evaluated kernels, reduces the observed speedups, which are below 20 on 128 cores. On the one hand, the message-passing implementations of this kernel rely heavily on Alltoall and Allreduce primitives, whose overhead is the main performance penalty. In fact, F-MPJ scalability drops from 64 cores (MPJ Express from 32 cores), whereas MPI shows poor scalability from 64 cores (the performance comparison between 64 and 128 cores shows that the use of the additional 64 cores only increases the speedup in 3 units, from 16 to 19). On the other hand, OpenMP IS obtains the best results on 8 cores, showing a high parallel efficiency, and even takes advantage of the use of hyperthreading. However, the implementation of IS using Java threads shows very poor scalability, with speedups below 2.</p><p>The highest MG performance in terms of MOPS has been obtained with MPI, followed at a significant distance by F-MPJ although this Java library shows higher speedups, especially on 128 cores. The reason, as for FT, is that MPJ performance is limited by its poor performance on one core, which is 55% of the MPI performance (see in Section 4.3 the left graph in Fig. <ref type="figure" target="#fig_4">3</ref>). The longer MPJ runtime contributes to achieve high speedups in MG, trading off the bottleneck that represents the extensive use by this kernel of Allreduce, a collective whose performance is lower for MPJ than for MPI. In fact, the message-passing implementations of this kernel, both MPI and MPJ, present relatively good scalability, even for MPJ Express which achieves speedups around 30 on 64 and 128 cores. Nevertheless, the shared memory codes show little speedups, below 4 on 8 cores. Fig. <ref type="figure" target="#fig_9">7</ref> shows the Finis Terrae results, where the message-passing kernel implementations, NPB-MPI and NPB-MPJ, have been run on the rx7640 nodes of this supercomputer, using 8 cores per node and up to 32 nodes (hence up to 256 cores), whereas the shared memory results (NPB-OMP and NPB-JAV) have been obtained from the HP Integrity Superdome using up to 128 cores. Although the results have been obtained using two different hardware configurations, both subsystems share the same features but the memory architecture, which is distributed in rx7640 nodes and shared in the Integrity Superdome, as presented in Section 4.1.</p><p>The analysis of the Finis Terrae results (Fig. <ref type="figure" target="#fig_9">7</ref>) shows that the best performer is OpenMP, showing significantly higher MOPS than the other implementations, except for MG where it is outperformed by MPI. Nevertheless, OpenMP suffers scalability losses from 64 cores due to the access to remote cells and the relative poor bidirectional traffic performance in the cell controller (the Integrity Superdome is a ccNUMA system which consists of 16 cells, each one with 4 dual-core processors and 64 Gbytes memory, interconnected through a crossbar network) <ref type="bibr" target="#b53">[56]</ref>. The high performance of OpenMP contrasts with the poor results in terms of MOPS of NPB-JAV, although this is motivated by its poor performance on one core, which is usually an order of magnitude lower than MPI (Intel Compiler) performance (see in Section 4.3 the right graph in Fig. <ref type="figure" target="#fig_4">3</ref>). Although this poor runtime favors the obtaining of high scalability, in fact NPB-JAV obtains speedups above 30 for CG and FT, this is not enough to bridge the gap with OpenMP results as NPB-OMP codes achieves even higher speedups, except for FT. Furthermore, NPB-JAV results are significantly poorer than those of NPB-MPJ (around 2-3 times lower), except for MG, which confirms the inefficiency of this Java threads implementation.</p><p>The performance results of the message-passing codes, NPB-MPI and NPB-MPJ, are between NPB-OMP kernels and the shared memory implementations, except for NPB-MPI MG, which is the best performer for MG kernel. Nevertheless, there are significant differences among the libraries been used. Thus, MPJ Express presents modest speedups, below 30, due to the use of a sockets-based (niodev) communication device over the IP emulation IPoIB. This limitation is overcome in F-MPJ, relying more directly on IBV. Thus, F-MPJ is able to achieve the highest speedups, motivated in part by the longer runtimes on one core (see in Section 4.3 the right graph in Fig. <ref type="figure" target="#fig_4">3</ref>) which favor this scalability (a heavy workload reduces the impact of communications on the overall performance scalability). The high speedups of F-MPJ, which are significantly higher than those of MPI (e.g., up to 7 times higher in CG), allow F-MPJ to bridge the gap between Java and natively compiled languages in HPC. In fact, F-MPJ performance results for CG and FT on 256 are close to those of MPI, although their performance on one core is around 7 and 4 times lower than MPI results for CG and FT, respectively.</p><p>The analysis of these NPB experimental results show that the performance of MPJ libraries heavily depends on their InfiniBand support. Thus, F-MPJ, which relies directly on IBV, outperforms significantly MPJ Express, whose socketbased communication device runs on IPoIB, obtaining relatively low performance, especially in terms of start-up latency. Furthermore, NPB-MPJ kernels have revealed to be the most efficient Java implementation, significantly outperforming Java threads implementations, both in terms of performance on one core and scalability. Moreover, the comparative evaluation of NPB-MPJ and NPB-MPI results reveals that efficient MPJ libraries can help to bridge the gap between Java and native code performance in HPC. Finally, the evaluated libraries have shown higher speedups on Finis Terrae than on the x86-64 cluster. The reason behind this behavior is that the obtaining of poorer performance on one core allows for higher scalability given the same interconnection technology (both systems use 16 Gbps InfiniBand DDR networks). Thus, NPB-MPJ kernels on the Finis Terrae, showing some of the poorest performance on one core, are able to achieve speedups of up to 175 on 256 cores, whereas NPB-MPI scalability on the x86-64 cluster is always below a speedup of 50. Nevertheless, NPB-MPI on the x86-64 cluster shows the highest performance in terms of MOPS, outperforming NPB-MPI results on the Finis Terrae, which has double the number of available cores (256 cores available on the Finis Terrae vs. 128 cores available on the x86-64 cluster).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.2.">Performance analysis of the jGadget application</head><p>The jGadget <ref type="bibr" target="#b52">[55]</ref> application is the MPJ implementation of Gadget <ref type="bibr" target="#b54">[57]</ref>, a popular cosmology simulation code initially implemented in C and parallelized using MPI that is used to study a large variety of problems like colliding and merging galaxies or the formation of large-scale structures. The parallelization strategy, both with MPI and MPJ, is an irregular and dynamically adjusted domain decomposition, with copious communication between processes. jGadget has been selected as representative Java HPC application as its performance has been previously analyzed <ref type="bibr" target="#b49">[52]</ref> for their Java (MPJ) and C (MPI) implementations, as well as for its communication intensiveness and its popularity.  bundle). As Gadget is a communication-intensive application, with significant collective operations overhead, its scalability is modest, obtaining speedups of up to 48 on 128 cores of the x86-64 cluster and speedups of up to 57 on 256 cores of the Finis Terrae. Here F-MPJ achieves generally the highest speedups, followed closely by MPI, except from 64 cores on the Finis Terrae where MPI loses performance. This slowdown is shared with MPJ Express, which shows its highest performance on 64 cores for both systems. Nevertheless, MPJ Express speedups on the Finis Terrae are much higher (up to 37) than on the x86-64 cluster (only up to 16), something motivated by the different runtime of the application on the x86-64 cluster and the Finis Terrae. In fact, MPI Gadget presents numerous library dependences, such as FFTW-MPI, Hierarchical Data Format (HDF) support, and the numerical GNU Scientific Library (GSL), which are not fully optimized for this system, thus increasing significantly its runtime. An example of inefficiency is that GSL shows poor performance on the Finis Terrae. Here the use of Intel Math Kernel Library (MKL) would show higher performance but the support for this numerical library is not implemented in Gadget. As a consequence of this jGadget performs better, compared in relative terms with MPI, the Finis Terrae (only 2 times slower than MPI) than on the x86-64 cluster (3 times slower than MPI), although the performance of Java on IA64 architectures is quite poor.</p><p>Moreover, the performance gap between Gadget and jGadget is motivated by the poor performance of the numerical methods included in jGadget, which consist of a translation of the GSL functions invoked in the Gadget source code, without relying on external numerical libraries. The use of an efficient Java numerical library <ref type="bibr" target="#b55">[58]</ref>, comparable in performance to Fortran numerical codes, would have improved the performance of jGadget. The development of such a library is still an ongoing effort, although it started a decade ago when it was demonstrated that Java was able to compete with Fortran in high performance numerical computing <ref type="bibr" target="#b56">[59,</ref><ref type="bibr" target="#b57">60]</ref>. In the last years a few projects are being actively developed <ref type="bibr" target="#b58">[61]</ref>, such as Universal Java Matrix Package (UJMP) [62], Efficient Java Matrix Library (EJML) <ref type="bibr" target="#b59">[63]</ref>, Matrix Toolkit Java (MTJ) <ref type="bibr" target="#b60">[64]</ref> and jblas <ref type="bibr" target="#b61">[65]</ref>, which are replacing more traditional frameworks such as JAMA <ref type="bibr" target="#b62">[66]</ref>. Furthermore, a recent evaluation of Java for numerical computing <ref type="bibr" target="#b63">[67]</ref> has shown that the performance of Java applications can be significantly enhanced by delegating numerically intensive tasks to native libraries (e.g., Intel MKL) which supports the development of efficient high performance numerical applications in Java.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>This paper has analyzed the current state of Java for HPC, both for shared and distributed memory programming, showing an important number of past and present projects which are the result of the sustained interest in the use of Java for HPC. Nevertheless, most of these projects are restricted to experimental environments, which prevents their general adoption in this field. However, the analysis of the existing programming options and available libraries in Java for HPC, together with the presentation in this paper of our current research efforts in the improvement of the scalability of our Java message-passing library, F-MPJ, would definitively contribute to boost the embracement of Java in HPC.</p><p>Additionally, Java lacks thorough and up-to-date evaluations of their performance in HPC. In order to overcome this issue this paper presents the performance evaluation of current Java HPC solutions and research developments on two shared memory environments and two InfiniBand multi-core clusters. The main conclusion of the analysis of these results is that Java can achieve almost similar performance to natively compiled languages, both for sequential and parallel applications, being an alternative for HPC programming. In fact, the performance overhead that Java may impose is a reasonable trade-off for the appealing features that this language provides for parallel programming multi-core architectures. Furthermore, the recent advances in the efficient support of Java communications on shared memory and low-latency networks are bridging the performance gap between Java and more traditional HPC languages.</p><p>Finally, the active research efforts in this area are expected to bring in the next future new developments that will continue rising the interest of both industry and academia and increasing the benefits of the adoption of Java for HPC.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1</head><label>1</label><figDesc>Fig.1presents an overview of the F-MPJ communication devices on shared memory and cluster networks. From top to bottom, the communication support of MPJ applications run with F-MPJ is implemented in the device layer. Current F-MPJ communication devices are implemented either on JVM threads (smpdev, a thread-based device), on sockets over the TCP/IP stack (iodev on Java IO sockets), or on native communication layers such as Myrinet eXpress (mxdev) and InfiniBand Verbs (IBV) (ibvdev), which are accessed through JNI.The initial implementation of F-MPJ included only one communication device, iodev, implemented on top of Java IO sockets, which therefore can rely on top of JFS and hence obtain high performance on shared memory and Gigabit Ethernet, SCI, Myrinet, and InfiniBand networks. However, the use of sockets in a communication device, despite the high performance provided by JFS, still represents an important source of overhead in Java communications. Thus, F-MPJ is including the direct support of communications on high performance native communication layers, such as MX and IBV.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. F-MPJ communication devices on shared memory and cluster networks.The mxdev device implements the xxdev API on MX, which runs natively on Myrinet and high-speed Ethernet networks, such as 10 Gigabit Ethernet, relying on MXoE (MX over Ethernet) stack. As MX already provides a low-level messaging API, mxdev deals with the Java Objects marshaling and communication, the JNI transfers and the MX parameters handling. The ibvdev device implements the xxdev API on IBV, the low-level InfiniBand communication driver, in order to take full advantage of the InfiniBand network. Unlike mxdev, ibvdev has to implement its own communication protocols, as IBV API is quite close to the InfiniBand Network Interface Card (NIC) operation. Thus, this communication device has implemented two communication protocols, eager and rendezvous, on RDMA (Remote Direct Memory Access) Write/Send operations. This direct access of Java to InfiniBand network was somewhat restricted so far to MPI libraries. Like mxdev, this device has to deal with the Java Objects communication and the JNI transfers, and additionally with the communication protocols operation. Finally, both mxdev and ibvdev, although they have been primarily designed for network communication, support shared memory intra-node communication. However, smpdev device is the thread-based communication device that should support more efficiently shared memory transfers. This device isolates a naming space for each running thread (relying on custom class loaders) and allocates shared message queues in order to implement the communications as regular data copies between threads.</figDesc><graphic coords="6,137.99,52.13,270.00,82.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>3, the C/Fortran compilers are the Intel compiler (used with -fast flag) version 11.1.073 and the GNU compiler (used with -O3 flag) version 4.1.2, both with OpenMP support, the native communication libraries are OFED (OpenFabrics Enterprise Distribution) 1.5 and Open-MX 1.3.4, for InfiniBand and 10 Gigabit Ethernet, respectively, and the JVM is Oracle JDK 1.6.0_23. Finally, the evaluated message-passing libraries are F-MPJ with JFS 0.3.1, MPJ Express 0.35, and OpenMPI 1.4.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. NPB performance variability on the x86-64 cluster.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. NPB relative performance on one core.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Message-passing point-to-point performance on InfiniBand, 10 Gigabit Ethernet and shared memory.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Collective primitives performance on the InfiniBand multi-core cluster.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. NPB Class C results on the x86-64 cluster.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8</head><label>8</label><figDesc>presents jGadget and Gadget performance results on the x86-64 cluster and the Finis Terrae for a galaxy cluster formation simulation with 2 million particles in the system (simulation available within the examples of Gadget software</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. NPB Class C results on Finis Terrae.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Gadget runtime and scalability on the x86-64 cluster and the Finis Terrae supercomputer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Java message-passing projects overview.</figDesc><table><row><cell>Pure Java Impl.</cell><cell cols="2">Socket impl.</cell><cell cols="3">High-speed network support</cell><cell></cell><cell>API</cell></row><row><cell></cell><cell>Java IO</cell><cell>Java NIO</cell><cell>Myrinet</cell><cell>InfiniBand</cell><cell>SCI</cell><cell>mpiJava 1.2</cell><cell>JGF MPJ</cell><cell>Other APIs</cell></row><row><cell>MPJava</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>Algorithms implemented in MPJ collectives libraries.</figDesc><table><row><cell>Primitive</cell><cell>MPJ Express collectives library</cell><cell>F-MPJ collectives library</cell></row><row><cell>Barrier</cell><cell>Gather+Bcast</cell><cell>nbFTGather+bFaTBcast, Gather+Bcast, BT</cell></row><row><cell>Bcast</cell><cell>bFaTBcast</cell><cell>bFT, nbFT, bFaTBcast, MST</cell></row><row><cell>Scatter</cell><cell>nbFT</cell><cell>nbFT, MST</cell></row><row><cell>Scatterv</cell><cell>nbFT</cell><cell>nbFT, MST</cell></row><row><cell>Gather</cell><cell>nbFT</cell><cell>bFT, nbFT, nb1FT, MST</cell></row><row><cell>Gatherv</cell><cell>nbFT</cell><cell>bFT, nbFT, nb1FT, MST</cell></row><row><cell>Allgather</cell><cell>nbFT, BT</cell><cell>nbFT, BT, nbBDE, bBKT, nbBKT, Gather+Bcast</cell></row><row><cell>Allgatherv</cell><cell>nbFT, BT</cell><cell>nbFT, BT, nbBDE, bBKT, nbBKT, Gather+Bcast</cell></row><row><cell>Alltoall</cell><cell>nbFT</cell><cell>bFT, nbFT, nb1FT, nb2FT</cell></row><row><cell>Alltoallv</cell><cell>nbFT</cell><cell>bFT, nbFT, nb1FT, nb2FT</cell></row><row><cell>Reduce</cell><cell>bFT</cell><cell>bFT, nbFT, MST</cell></row><row><cell>Allreduce</cell><cell>nbFT, BT</cell><cell>nbFT, BT, bBDE, nbBDE, Reduce+Bcast</cell></row><row><cell>Reduce-Scatter</cell><cell>Reduce+Scatterv</cell><cell>bBDE, nbBDE, bBKT, nbBKT, Reduce+Scatterv</cell></row><row><cell>Scan</cell><cell>nbFT</cell><cell>nbFT, linear</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc>Example of configuration file for the selection of collective algorithms.</figDesc><table><row><cell>Primitive</cell><cell>Short message/</cell><cell>Short message/</cell><cell>Long message/</cell><cell>Long message/</cell></row><row><cell></cell><cell>small number of processes</cell><cell>large number of processes</cell><cell>small number of processes</cell><cell>large number of processes</cell></row><row><cell>Barrier</cell><cell>nbFTGather+bFatBcast</cell><cell>nbFTGather+bFatBcast</cell><cell>Gather+Bcast</cell><cell>Gather+Bcast</cell></row><row><cell>Bcast</cell><cell>nbFT</cell><cell>MST</cell><cell>MST</cell><cell>MST</cell></row><row><cell>Scatter</cell><cell>nbFT</cell><cell>nbFT</cell><cell>nbFT</cell><cell>nbFT</cell></row><row><cell>Gather</cell><cell>nbFT</cell><cell>nbFT</cell><cell>MST</cell><cell>MST</cell></row><row><cell>Allgather</cell><cell>Gather+Bcast</cell><cell>Gather+Bcast</cell><cell>Gather+Bcast</cell><cell>Gather+Bcast</cell></row><row><cell>Alltoall</cell><cell>nb2FT</cell><cell>nb2FT</cell><cell>nb2FT</cell><cell>nb2FT</cell></row><row><cell>Reduce</cell><cell>nbFT</cell><cell>nbFT</cell><cell>MST</cell><cell>MST</cell></row><row><cell>Allreduce</cell><cell>Reduce+Bcast</cell><cell>Reduce+Bcast</cell><cell>Reduce+Bcast</cell><cell>Reduce+Bcast</cell></row><row><cell>Reduce-Scatter</cell><cell>bFTReduce+nbFTScatterv</cell><cell>bFTReduce+nbFTScatterv</cell><cell>BDE</cell><cell>BDE</cell></row><row><cell>Scan</cell><cell>Linear</cell><cell>Linear</cell><cell>Linear</cell><cell>Linear</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc>NPB-MPJ benchmarks description.</figDesc><table><row><cell>Name</cell><cell>Operation</cell><cell>Communicat. intensiveness</cell><cell>Kernel</cell><cell>Applic.</cell></row><row><cell>CG</cell><cell>Conjugate Gradient</cell><cell>Medium</cell><cell></cell><cell></cell></row><row><cell>EP</cell><cell>Embarrassingly Parallel</cell><cell>Low</cell><cell></cell><cell></cell></row><row><cell>FT</cell><cell>Fourier Transformation</cell><cell>High</cell><cell></cell><cell></cell></row><row><cell>IS</cell><cell>Integer Sort</cell><cell>High</cell><cell></cell><cell></cell></row><row><cell>MG</cell><cell>Multi-Grid</cell><cell>High</cell><cell></cell><cell></cell></row><row><cell>SP</cell><cell>Scalar Pentadiagonal</cell><cell>Low</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5</head><label>5</label><figDesc>Algorithms that maximize performance on the x86-64 cluster.</figDesc><table><row><cell>Primitive</cell><cell>Short message/small</cell><cell>Short message/large</cell><cell>Long message/small</cell><cell>Long message/large</cell></row><row><cell></cell><cell>number of processes</cell><cell>number of processes</cell><cell>number of processes</cell><cell>number of processes</cell></row><row><cell>Bcast</cell><cell>nbFT</cell><cell>MST</cell><cell>MST</cell><cell>MST</cell></row><row><cell>Allgather</cell><cell>nbFTGather+nbFTBcast</cell><cell>nbFTGather+MSTBcast</cell><cell>MSTGather+MSTBcast</cell><cell>MSTGather+MSTBcast</cell></row><row><cell>Reduce</cell><cell>bFT</cell><cell>bFT</cell><cell>MST</cell><cell>MST</cell></row><row><cell>Allreduce</cell><cell>bFTReduce+nbFTBcast</cell><cell>bFTReduce+MSTBcast</cell><cell>MSTReduce+MSTBcast</cell><cell>MSTReduce+MSTBcast</cell></row></table><note><p>cores available within each node. The scalability graphs (right graphs) confirm this analysis, especially for the Broadcast and the Reduce operations.</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was funded by the Ministry of Science and Innovation of Spain under Project TIN2010-16735 and an FPU grant AP2009-2112. We gratefully thank CESGA (Galicia Supercomputing Center, Santiago de Compostela, Spain) for providing access to the Finis Terrae supercomputer.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Java for high performance computing: assessment of current research and practice</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Taboada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Touriño</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Doallo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 7th Intl. Conference on the Principles and Practice of Programming in Java, PPPJ&apos;09</title>
		<meeting>7th Intl. Conference on the Principles and Practice of Programming in Java, PPPJ&apos;09<address><addrLine>Calgary, Alberta, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="30" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">ProActive: using a Java middleware for HPC design, implementation and benchmarks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Amedro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Caromel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bodnartchouk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Delbé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Taboada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computers and Communications</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="49" to="57" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The impact of multicore on computational science software</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Dongarra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gannon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kennedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CTWatch Quarterly</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Parallel Java: a unified API for shared memory and cluster parallel programming in 100% Java</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kaminsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 9th Intl. Workshop on Java and Components for Parallelism, Distribution and Concurrency, IWJacPDC&apos;07</title>
		<meeting>9th Intl. Workshop on Java and Components for Parallelism, Distribution and Concurrency, IWJacPDC&apos;07<address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page">196</biblScope>
		</imprint>
	</monogr>
	<note>8 pages</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An OpenMP-like interface for parallel programming in Java</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Kambites</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Obdrzálek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Bull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Concurrency and Computation: Practice and Experience</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">8-9</biblScope>
			<biblScope unit="page" from="793" to="814" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">JaMP: an implementation of OpenMP for a Java DSM</title>
		<author>
			<persName><forename type="first">M</forename><surname>Klemm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bezold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Veldema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Philippsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Concurrency and Computation: Practice and Experience</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="2333" to="2352" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Nested parallelism for multi-core HPC systems using Java</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Carpenter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Parallel and Distributed Computing</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="532" to="545" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Run-time optimizations for a Java DSM implementation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Veldema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F H</forename><surname>Hofman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bhoedjang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">E</forename><surname>Bal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Concurrency and Computation: Practice and Experience</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3-5</biblScope>
			<biblScope unit="page" from="299" to="316" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Titanium: a high-performance Java dialect</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Yelick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Concurrency -Practice and Experience</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">11-13</biblScope>
			<biblScope unit="page" from="825" to="836" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Titanium performance and potential: an NPB experimental study</title>
		<author>
			<persName><forename type="first">K</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bonachea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Yelick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 18th Intl. Workshop on Languages and Compilers for Parallel Computing, LCPC&apos;05</title>
		<meeting>18th Intl. Workshop on Languages and Compilers for Parallel Computing, LCPC&apos;05<address><addrLine>Hawthorne, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">4339</biblScope>
			<biblScope unit="page" from="200" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Java Fast Sockets: enabling high-speed Java communications on high performance clusters</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Taboada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Touriño</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Doallo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Communications</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page" from="4049" to="4059" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ibis: a flexible and efficient Java-based Grid programming environment</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V V</forename><surname>Nieuwpoort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Maassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wrzesinska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hofman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kielmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">E</forename><surname>Bal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Concurrency and Computation: Practice and Experience</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">7-8</biblScope>
			<biblScope unit="page" from="1079" to="1107" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Object-oriented SPMD</title>
		<author>
			<persName><forename type="first">L</forename><surname>Baduel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Baude</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Caromel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th IEEE Intl. Symposium on Cluster Computing and the Grid, CCGrid&apos;05</title>
		<meeting>5th IEEE Intl. Symposium on Cluster Computing and the Grid, CCGrid&apos;05<address><addrLine>Cardiff, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="824" to="831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">More efficient serialization and RMI for Java</title>
		<author>
			<persName><forename type="first">M</forename><surname>Philippsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Haumacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Concurrency: Practice and Experience</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="495" to="518" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">RMIX: a multiprotocol RMI framework for Java</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kurzyniec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wrzosek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sunderam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Slominski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th Intl. Workshop on Java for Parallel and Distributed Computing, IWJPDC&apos;03</title>
		<meeting>5th Intl. Workshop on Java for Parallel and Distributed Computing, IWJPDC&apos;03<address><addrLine>Nice, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page">140</biblScope>
		</imprint>
	</monogr>
	<note>7 pages</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Efficient Java RMI for parallel programming</title>
		<author>
			<persName><forename type="first">J</forename><surname>Maassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V V</forename><surname>Nieuwpoort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Veldema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">E</forename><surname>Bal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kielmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hofman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Programming Languages and Systems</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="747" to="775" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">High performance Java remote method invocation for parallel computing on clusters</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Taboada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Teijeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Touriño</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 12th IEEE Symposium on Computers and Communications, ISCC&apos;07</title>
		<meeting>12th IEEE Symposium on Computers and Communications, ISCC&apos;07<address><addrLine>Aveiro, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="233" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Performance analysis of Java message-passing libraries on Fast Ethernet, Myrinet and SCI clusters</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Taboada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Touriño</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Doallo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th IEEE Intl. Conf. on Cluster Computing, CLUSTER&apos;03</title>
		<meeting>5th IEEE Intl. Conf. on Cluster Computing, CLUSTER&apos;03<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="118" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Carpenter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-H</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lim</surname></persName>
		</author>
		<ptr target="http://www.hpjava.org/reports/mpiJava-spec/mpiJava-spec/mpiJava-spec.html" />
		<title level="m">mpiJava 1.2: API Specification</title>
		<imprint>
			<date type="published" when="2011-05">May 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">MPJ: MPI-like message passing for Java</title>
		<author>
			<persName><forename type="first">B</forename><surname>Carpenter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Getov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Skjellum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Concurrency: Practice and Experience</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1019" to="1038" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<ptr target="http://www.javagrande.org" />
		<title level="m">Java grande forum</title>
		<imprint>
			<date type="published" when="2011-05">May 2011</date>
		</imprint>
	</monogr>
	<note>Last visited</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">mpiJava: an object-oriented Java interface to MPI</title>
		<author>
			<persName><forename type="first">M</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Carpenter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1st Intl. Workshop on Java for Parallel and Distributed Computing, IWJPDC&apos;99</title>
		<meeting>1st Intl. Workshop on Java for Parallel and Distributed Computing, IWJPDC&apos;99<address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">1586</biblScope>
			<biblScope unit="page" from="748" to="762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">MPJava: high-performance message passing in Java using Java.nio</title>
		<author>
			<persName><forename type="first">B</forename><surname>Pugh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Spacco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 16th Intl. Workshop on Languages and Compilers for Parallel Computing, LCPC&apos;03</title>
		<meeting>16th Intl. Workshop on Languages and Compilers for Parallel Computing, LCPC&apos;03<address><addrLine>College Station, TX, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">2958</biblScope>
			<biblScope unit="page" from="323" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Jcluster: an efficient Java parallel environment on a large-scale heterogeneous cluster</title>
		<author>
			<persName><forename type="first">B.-Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-M</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Concurrency and Computation: Practice and Experience</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1541" to="1557" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">P2P-MPI: a peer-to-peer framework for robust execution of message passing parallel programs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Genaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rattanapoka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Grid Computing</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="27" to="42" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">MPJ/Ibis: a flexible and efficient message passing platform for Java</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bornemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V V</forename><surname>Nieuwpoort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kielmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 12th European PVM/MPI Users&apos; Group Meeting, EuroPVM/MPI&apos;05</title>
		<meeting>12th European PVM/MPI Users&apos; Group Meeting, EuroPVM/MPI&apos;05<address><addrLine>Sorrento, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">3666</biblScope>
			<biblScope unit="page" from="217" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Implementation and performance evaluation of socket and RMI based Java message passing systems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th ACIS Intl. Conf. on Software Engineering Research, Management and Applications, SERA&apos;07</title>
		<meeting>5th ACIS Intl. Conf. on Software Engineering Research, Management and Applications, SERA&apos;07<address><addrLine>Busan, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="153" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">F-MPJ: scalable Java message-passing communications on parallel systems</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Taboada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Touriño</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Doallo</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11227-009-0270-0</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Supercomputing</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Design of efficient Java message-passing collectives on multi-core clusters</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Taboada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Touriño</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Doallo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Supercomputing</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="126" to="154" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multicore-enabling the MPJ Express messaging library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Manzoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hameed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Carpenter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 8th Intl. Conference on the Principles and Practice of Programming in Java, PPPJ&apos;10</title>
		<meeting>8th Intl. Conference on the Principles and Practice of Programming in Java, PPPJ&apos;10<address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="49" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fast tuning of intra-cluster collective communications</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Barchet-Estefanel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mounié</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th European PVM/MPI Users&apos; Group Meeting, EuroPVM/MPI&apos;04</title>
		<meeting>11th European PVM/MPI Users&apos; Group Meeting, EuroPVM/MPI&apos;04<address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">3241</biblScope>
			<biblScope unit="page" from="28" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Collective communication: theory, practice, and experience</title>
		<author>
			<persName><forename type="first">E</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Heimlich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Purkayastha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Van De Geijn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Concurrency and Computation: Practice and Experience</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="1749" to="1783" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Optimization of collective communication operations in MPICH</title>
		<author>
			<persName><forename type="first">R</forename><surname>Thakur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rabenseifner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gropp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of High Performance Computing Applications</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="66" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Towards an accurate model for collective communications</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Vadhiyar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Fagg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Dongarra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of High Performance Computing Applications</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="159" to="167" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A benchmark suite for high performance Java</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Bull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Westhead</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Henty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Davey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Concurrency: Practice and Experience</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="375" to="388" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">NPB-MPJ: NAS parallel benchmarks implementation for message-passing in Java</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Mallón</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Taboada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Touriño</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Doallo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 17th Euromicro Intl. Conf. on Parallel, Distributed, and Network-Based Processing (PDP&apos;09)</title>
		<meeting>17th Euromicro Intl. Conf. on Parallel, Distributed, and Network-Based essing (PDP&apos;09)<address><addrLine>Weimar, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="181" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">an object-oriented approach to nonuniform cluster computing</title>
		<author>
			<persName><forename type="first">P</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Grothoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">A</forename><surname>Saraswat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Donawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kielstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ebcioglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Von Praun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 20th Annual ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications, OOPSLA&apos;05</title>
		<meeting>20th Annual ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications, OOPSLA&apos;05<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="519" to="538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Performance and productivity at scale</title>
		<author>
			<persName><surname>X10</surname></persName>
		</author>
		<ptr target="http://x10plus.cloudaccess.net" />
		<imprint>
			<date type="published" when="2011-05">May 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Habanero</forename><surname>Java</surname></persName>
		</author>
		<ptr target="http://habanero.rice.edu/hj.html" />
		<imprint>
			<date type="published" when="2011-05">May 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Language extensions in support of compiler parallelization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shirako</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kasahara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 20th Intl. Workshop on Languages and Compilers for Parallel Computing, LCPC&apos;07</title>
		<meeting>20th Intl. Workshop on Languages and Compilers for Parallel Computing, LCPC&apos;07<address><addrLine>Urbana, IL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="78" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">JCUDA: a programmer-friendly interface for accelerating Java programs with CUDA</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 15th Intl. European Conference on Parallel and Distributed Computing, Euro-Par&apos;09</title>
		<meeting>15th Intl. European Conference on Parallel and Distributed Computing, Euro-Par&apos;09<address><addrLine>Delft, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="887" to="899" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<ptr target="http://hoopoe-cloud.com/Solutions/jCUDA/Default.aspx" />
		<title level="m">jCUDA</title>
		<imprint>
			<date type="published" when="2011-05">May 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<ptr target="http://jacuda.sourceforge.net" />
		<title level="m">JaCuda</title>
		<imprint>
			<date type="published" when="2011-05">May 2011</date>
		</imprint>
	</monogr>
	<note>Last visited</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName><surname>Jacuzzi</surname></persName>
		</author>
		<ptr target="http://sourceforge.net/apps/wordpress/jacuzzi" />
		<imprint>
			<date type="published" when="2011-05">May 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName><surname>Java-Gpu</surname></persName>
		</author>
		<ptr target="http://code.google.com/p/java-gpu" />
		<imprint>
			<date type="published" when="2011-05">May 2011</date>
		</imprint>
	</monogr>
	<note>Last visited</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<ptr target="http://code.google.com/p/javacl" />
		<title level="m">JavaCL</title>
		<imprint>
			<date type="published" when="2011-05">May 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<ptr target="http://jogamp.org" />
		<title level="m">JogAmp</title>
		<imprint>
			<date type="published" when="2011-05">May 2011</date>
		</imprint>
	</monogr>
	<note>Last visited</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Dotzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Veldema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Klemm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jcudamp</forename></persName>
		</author>
		<title level="m">Proc. 3rd Intl. Workshop on Multicore Software Engineering, IWMSE&apos;10</title>
		<meeting>3rd Intl. Workshop on Multicore Software Engineering, IWMSE&apos;10<address><addrLine>Cape Town, South Africa</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="10" to="17" />
		</imprint>
	</monogr>
	<note>OpenMP/Java on CUDA</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Parallel execution of Java loops on graphics processing units</title>
		<author>
			<persName><forename type="first">A</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Lhoták</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lashari</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.scico.2011.06.004</idno>
	</analytic>
	<monogr>
		<title level="j">Science of Computer Programming</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A comparative study of Java and C performance in two large-scale parallel applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Carpenter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hussain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Concurrency and Computation: Practice and Experience</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="1882" to="1906" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<author>
			<persName><forename type="first">Terrae</forename><surname>Finis</surname></persName>
		</author>
		<author>
			<persName><surname>Supercomputer</surname></persName>
		</author>
		<ptr target="http://www.top500.org/system/9156" />
	</analytic>
	<monogr>
		<title level="m">Galicia Supercomputing Center</title>
		<imprint>
			<date type="published" when="2011-05">May 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Statistically rigorous Java performance evaluation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Georges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Buytaert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Eeckhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 22nd Annual ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications, OOPSLA&apos;07</title>
		<meeting>22nd Annual ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications, OOPSLA&apos;07<address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="57" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">MPJ Express meets Gadget: towards a Java code for cosmological simulations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Carpenter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 13th European PVM/MPI Users&apos; Group Meeting, EuroPVM/MPI&apos;06</title>
		<meeting>13th European PVM/MPI Users&apos; Group Meeting, EuroPVM/MPI&apos;06<address><addrLine>Bonn, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="358" to="365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Performance evaluation of MPI, UPC and OpenMP on multicore architectures</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Mallón</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Taboada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Teijeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Touriño</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fraguela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gómez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Doallo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mouriño</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 16th European PVM/MPI Users&apos; Group Meeting, EuroPVM/MPI&apos;09</title>
		<meeting>16th European PVM/MPI Users&apos; Group Meeting, EuroPVM/MPI&apos;09<address><addrLine>Espoo, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="174" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">The cosmological simulation code GADGET-2</title>
		<author>
			<persName><forename type="first">V</forename><surname>Springel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Monthly Notices of the Royal Astronomical Society</title>
		<imprint>
			<biblScope unit="volume">364</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1105" to="1134" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Javagrande</forename><surname>Javanumerics</surname></persName>
		</author>
		<ptr target="http://math.nist.gov/javanumerics/" />
		<imprint>
			<date type="published" when="2011-05">May 2011</date>
		</imprint>
	</monogr>
	<note>Last visited</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Developing numerical libraries in Java</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Boisvert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Dongarra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pozo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Remington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Stewart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Concurrency: Practice and Experience</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">11-13</biblScope>
			<biblScope unit="page" from="1117" to="1129" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Java programming for high-performance numerical computing</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Moreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Midkiff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Artigas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Snir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Systems Journal</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="56" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Towards a next-generation matrix library for Java</title>
		<author>
			<persName><forename type="first">H</forename><surname>Arndt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bundschus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Naegele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 33rd Annual IEEE Intl. Computer Software and Applications Conference, COMPSAC&apos;09</title>
		<meeting>33rd Annual IEEE Intl. Computer Software and Applications Conference, COMPSAC&apos;09<address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="460" to="467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<ptr target="http://code.google.com/p/efficient-java-matrix-library/" />
		<title level="m">Efficient Java Matrix Library (EJML)</title>
		<imprint>
			<date type="published" when="2011-05">May 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<ptr target="http://code.google.com/p/matrix-toolkits-java/" />
		<title level="m">Matrix Toolkits Java (MTJ)</title>
		<imprint>
			<date type="published" when="2011-05">May 2011</date>
		</imprint>
	</monogr>
	<note>Last visited</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<ptr target="http://jblas.org/" />
		<title level="m">Linear Algebra for Java (jblas)</title>
		<imprint>
			<date type="published" when="2011-05">May 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<ptr target="http://math.nist.gov/javanumerics/jama" />
		<title level="m">JAMA: A Java Matrix Package</title>
		<imprint>
			<date type="published" when="2011-05">May 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">A toolkit for efficient numerical applications in Java</title>
		<author>
			<persName><forename type="first">M</forename><surname>Baitsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hartmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Engineering Software</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="75" to="83" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
