<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GitHub Copilot AI pair programmer: Asset or Liability?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-06-30">30 Jun 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Arghavan</forename><forename type="middle">Moradi</forename><surname>Dakhel</surname></persName>
							<email>arghavan.moradi-dakhel@polymtl.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">York University</orgName>
								<address>
									<settlement>Toronto</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vahid</forename><surname>Majdinasab</surname></persName>
							<email>vahid.majdinasab@polymtl.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">York University</orgName>
								<address>
									<settlement>Toronto</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Amin</forename><surname>Nikanjam</surname></persName>
							<email>amin.nikanjam@polymtl.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">York University</orgName>
								<address>
									<settlement>Toronto</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Foutse</forename><surname>Khomh</surname></persName>
							<email>foutse.khomh@polymtl.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">York University</orgName>
								<address>
									<settlement>Toronto</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michel</forename><forename type="middle">C</forename><surname>Desmarais</surname></persName>
							<email>michel.desmarais@polymtl.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">York University</orgName>
								<address>
									<settlement>Toronto</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Montreal</roleName><forename type="first">Polytechnique</forename><surname>Montreal</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">York University</orgName>
								<address>
									<settlement>Toronto</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Canada</forename><surname>Zhen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">York University</orgName>
								<address>
									<settlement>Toronto</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jack</forename><forename type="middle">)</forename><surname>Jiang</surname></persName>
							<email>zmjiang@cse.yorku.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">York University</orgName>
								<address>
									<settlement>Toronto</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GitHub Copilot AI pair programmer: Asset or Liability?</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-06-30">30 Jun 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2206.15331v1[cs.SE]</idno>
					<note type="submission">Preprint submitted to Journal of Software and System</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Code Completion</term>
					<term>Language Model</term>
					<term>GitHub Copilot</term>
					<term>Testing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automatic program synthesis is a long-lasting dream in software engineering. Recently, a promising Deep Learning (DL) based solution, called Copilot, has been proposed by Open AI and Microsoft as an industrial product. Although some studies evaluate the correctness of Copilot solutions and report its issues, more empirical evaluations are necessary to understand how developers can benefit from it effectively. In this paper, we study the capabilities of Copilot in two different programming tasks: (i) generating (and reproducing) correct and efficient solutions for fundamental algorithmic problems, and (ii) comparing Copilot's proposed solutions with those of human programmers on a set of programming tasks. For the former, we assess the performance and functionality of Copilot in solving selected fundamental problems in computer science, like sorting and implementing basic data structures. In the latter, a dataset of programming problems with human-provided solutions is used. The results show that Copilot is capable of providing solutions for almost all fundamental algorithmic problems, however, some solutions are buggy and non-reproducible. Moreover, Copilot has some difficulties in combining multiple methods to generate a solution. Comparing Copilot to humans, our results show that the correct ratio of human solutions is greater than Copilot's correct ratio, while the buggy solutions generated by Copilot require less effort to be repaired. While Copilot shows limitations as an assistant for developers especially in advanced programming tasks, as highlighted in this study and previous ones, it can generate preliminary solutions for basic programming tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent breakthroughs in Deep Learning (DL), in particular the Transformer architecture, have revived the Software Engineering (SE) decades-long dream of automating code generation that can speed up programming activities. The aim of program generation is to deliver a program that meets a user's intentions in the form of input-output examples, natural language descriptions, or partial programs <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b16">17]</ref>.</p><p>Program synthesis is useful for different purposes such as teaching, programmer assistance or the discovery of new algorithmic solutions for a problem <ref type="bibr" target="#b16">[17]</ref>. One finds different approaches to automatic code generation in the literature, from natural language programming <ref type="bibr" target="#b22">[23]</ref> and formal models <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b18">19]</ref> to Evolutionary Algorithms <ref type="bibr" target="#b32">[33]</ref> and machine-learned translation <ref type="bibr" target="#b27">[28]</ref>.</p><p>Novel large language models with the transformer architecture recently achieved good performance in automatic program synthesis <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b13">14]</ref>. One such model is Codex <ref type="bibr" target="#b4">[5]</ref>; a GPT-3 <ref type="bibr" target="#b3">[4]</ref> language model with up to 12 billion parameters which has been fine-tuned with 159 GB of code samples from 54 million GitHub repositories. Codex shows a good performance in solving a set of hand-written programming problems (i.e., not in the training dataset) using Python, named HumanEval dataset <ref type="bibr" target="#b4">[5]</ref>. This dataset includes simple programming problems with test cases to assess the functional correctness of codes. A production version of Codex is available as an extension on Visual Studio Code development environment, named GitHub Copilot 1 . Copilot, as an industrial "AI pair programmer", can generate code in different programming languages when provided with some context (called prompt), such as comments, methods names, and surrounding code. There are already studies that evaluated the correctness of Copilot's solutions on different tasks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b33">34]</ref>. However, they did not investigate the reproducibility or complexity of the generated solutions. Nguyen and Nadi <ref type="bibr" target="#b24">[25]</ref> compared the complexity of Copilot's solutions in different programming languages, besides their correctness. However, they did not analyze the efficiency of the solutions proposed by Copilot. They also did not examine what it would take to repair the buggy solutions. Authors in <ref type="bibr" target="#b34">[35]</ref> conducted a user study to understand how Copilot can help programmers complete a task. They studied how much time participants needed to complete a task using Copilot. They neither examined the quality or complexity of the codes produced by Copilot nor did they compare them with the codes generated by their participants (without using any code completion tool).</p><p>Therefore, despite all these previous studies, we still do not know if-how Copilot, as an industrial component, can be leveraged by developers efficiently. We need to go beyond evaluating the correctness of Copilot's suggestions for fundamental programming problems and examine how despite its limitations, it can be used as an effective pair programming tool. In this paper, we conduct an empirical evaluation of Copilot's strengths and weaknesses with two different strategies and formulate guidelines for its effective adoption as well as recommendations for its improvement. First, we assess Copilot in solving fundamental algorithmic problems (i.e., searching and sorting) in programming. We study the correctness and reproducibility of Copilot's solutions to these problems. Secondly, we compare Copilot's solutions with human solutions in solving programming tasks, to assess the extent to which it can mimic the work of a human pair programmer. We use a dataset of different programming tasks containing up to 4000 human-provided solutions (correct and buggy). The results of our study show that Copilot is capable of providing efficient solutions for the majority of fundamental problems, however some solutions are buggy or non-reproducible. We also observed that Copilot has some difficulties in combining multiple methods to generate a solution. Compared to human programmers, Copilot's solutions to programming tasks have lower correct ratio and diversity. While the buggy codes generated by Copilot can be repaired easily, the results highlight the limitation of Copilot in understanding some details in the context of the problems, which are easily understandable by humans.</p><p>To summarize, this paper makes the following contributions:</p><p>? We present an empirical study on the performance and functionality of Copilot's suggestions for fundamental algorithmic problems.</p><p>? We empirically compare Copilot's solutions with human solutions on a recent dataset of programming problems.</p><p>? We indicate and discuss reproducibility of the results when experimenting with Copilot.</p><p>? We make the dataset used and the detailed results obtained in this study publicly available online <ref type="bibr" target="#b12">[13]</ref> for other researchers and-or practitioners to replicate our results or build on our work.</p><p>The rest of this paper is organized as follows. We briefly review the related works in Section 2. Section 3 presents the design of our study to evaluate Copilot as an assistant to developers. We report our experiments to assess Copilot's suggestions for fundamental algorithmic problems and compare generated suggestions with what programmers really do on specific programming tasks in Section 4. We discuss our results and potential limitations in Section 5. Threats to validity are reviewed in Section 6. Finally, we conclude the paper in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>A few studies empirically investigate different capabilities of Copilot. Sobania et al. <ref type="bibr" target="#b31">[32]</ref> compared Copilot with a genetic programming (GP)-based approach that achieved good performance in program synthesis. Their findings show that GP-based approaches need more time to generate a solution. Moreover, training GP-based models is expensive due to the high cost of data labeling. Also, these approaches are not suitable to support developers in practice as GP usually generates codes that are bloated and difficult to understand by humans <ref type="bibr" target="#b31">[32]</ref>. Vaithilingam et al. <ref type="bibr" target="#b34">[35]</ref> conducted a human study involving 24 participant to understand how Copilot can help programmers to complete a task. They focused on 3 Python programming tasks: "1. edit csv, 2. web scraping" and "3. graph plotting". These tasks involve less problem solving effort compared to the typical programming tasks in our study. They are mostly related to using programming language libraries. Also, they did not compare the Copilot's suggestions with their participants' suggestions when working without the help of Copilot. Their finding shows that while Copilot did not necessarily improve the task completion time and success rate, programmers prefer to use Copilot for their daily tasks because it suggests good starting points to address the task. In Drori and Verma <ref type="bibr" target="#b10">[11]</ref>, authors studied the capability of Copilot in solving linear algebra problems for the MIT linear algebra course. In the same line of work, Tang et al. examined the capability of Copilot in solving university level probability and statistical problems <ref type="bibr" target="#b33">[34]</ref>. These two studies only focused on the correctness ratio of Copilot's solutions and did not examine its performance on programming tasks. Nguyen and Nadi <ref type="bibr" target="#b24">[25]</ref> evaluated Copilot on 33 LeetCode questions in 4 different programming languages. They used the LeetCode platform to test the correctness of Copilot's solutions. The questions in their study included different levels of difficulty. Although they evaluated the correctness of Copilot's solutions and compared their understandability, they did not assess whether Copilot successfully found the optimal solution for each task.</p><p>Another group of studies focuses on vulnerability issues to evaluate Copilot solutions. As mentioned before, Copilot is trained on a large volume of publicly available code repositories on GitHub which may contain bug or vulnerability problems. Pearce et al. <ref type="bibr" target="#b26">[27]</ref> conducted different scenarios on high-risk cybersecurity problems and investigated if Copilot learns from buggy code to generate insecure code. Another study investigates how Copilot can reproduce vulnerabilities in human programs <ref type="bibr" target="#b2">[3]</ref>. To do so, they first used a dataset of vulnerabilities generated by humans, then they rebuilt the whole code before the bug and asked Copilot to complete the code. The completed section was manually inspected by three coders to determine if Copilot reproduced the bug or fixed it. Moroz et al. <ref type="bibr" target="#b23">[24]</ref> examined the challenges and the potential of Copilot to improve the productivity of developers. They highlighted the copyright problems and the safety issues of its solutions. They discussed about the non-deterministic nature of such models and the harmful content that could be generated by them. Authors in <ref type="bibr" target="#b36">[37]</ref> surveyed 2631 developers about the impact of Copilot on their productivity and highlighted different metrics of users' interaction with Copilot that help predict their productivity. They relied on the SPACE <ref type="bibr" target="#b14">[15]</ref> framework to generate 11 Likert-style questions in their survey. Also, they analyzed the usage data of Copilot of the participants who responded to this survey. They extracted different metrics from this data such as acceptance rate of solutions, persistence rate, unchanged and mostly unchanged accepted solutions, etc. They found that the acceptance rate of solutions by developers is the most relevant metric that shows the impact of Copilot on the productivity of developers.</p><p>To the best of our knowledge, none of these studies compared Copilot with humans for solving programming tasks. The majority of these studies focused on assessing the correctness of Copilot's solutions and highlighted its issues; e.g., the presence of vulnerabilities in generated solutions. In this study, we focus on fundamental algorithmic problems and compare Copilot with humans in solving real world programming tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Study Design</head><p>In this section, we present our methodology to assess Copilot as an AI-based assistant for developers and detail the experimental design to achieve our goals. To assess Copilot as an "AI pair programmer", we need to see how it can effectively assist developers during the development process. To solve a programming task, a developer usually builds on the top of fundamental data structures (e.g., queues, trees, graphs) and algorithms (e.g., search, sorting) in computer science. Moreover, the developer needs to come up with creative ideas to achieve the goal(s) of the programming task efficiently. As the development is an iterative process and a developer may examine her code several times for refinement/debugging, the bug-proneness and creativity of automatically generated solutions should be assessed. Therefore, an ideal automatic code generator should be able to recommend proper structure/algorithms and innovative clues to the developer, so that the developer can build on them. The recommended code is expected to be competitive with what humans may generate during the development.</p><p>As none of the previous studies examined the effectiveness of Copilot as a programming assistant, we evaluate Copilot on: 1) the adequacy of recommended code for fundamental algorithmic problems, and 2) how the recommendation compares to human-provided solutions. Specifically, we address the following research questions (RQs): RQ1: Can Copilot suggest correct and efficient solutions for fundamental algorithmic problems?</p><p>-Are Copilot's suggestions for these problems reproducible?</p><p>RQ2: Are Copilot's solutions competitive with human solutions for solving programming problems?</p><p>-How difficult is it to repair Copilot's buggy suggestion compared to human solutions?</p><p>In the rest of this section, we describe the methodology we followed to answer each of our RQs as illustrated in Figure <ref type="figure">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">RQ1: Copilot on Algorithm Design</head><p>Our goal in RQ1 is to observe if Copilot can generate solutions for basic algorithmic problems given clear descriptions of the problem (the same way developers are assessed in their coding ability). The algorithms we have chosen are fundamental in software development. Therefore, it is imperative to study Copilot's ability in generating code for them since it claims to take the position of a pair programmer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Data Collection</head><p>We selected the problems and their descriptions from <ref type="bibr" target="#b7">[8]</ref>. We choose this resource because it is a highly cited and prestigious textbook that is widely used for teaching algorithmic design fundamentals to computer science students <ref type="bibr" target="#b8">[9]</ref>. In this book, the authors explain the principal algorithms that computer engineers must be knowledgeable about by breaking them into categories. Since our problems were selected from this book, we followed its categorization, such that our tests on Copilot were conducted on 4 categories:</p><p>? Sorting Algorithms: Sorting algorithms are among the first algorithmic concepts that are taught to computer science students. These algorithms introduce the concept of time complexity and how inefficient code can make differences in more complex programs. Sorting algorithms are used in databases to segment large amounts of data that cannot be loaded entirely into memory or in numerical operations to determine which numbers should undergo operations first in order for the results to be produced as quickly as possible. From this section, we selected some wellknown sorting algorithms which students are asked </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Correct Solution Found</head><p>Figure <ref type="figure">1</ref>: The Workflow of proposed methods. The study includes two different methods to test Copilot in recommending codes to solve programming problems. The first pipeline focuses on algorithmic problems collected from a well-known algorithm design book <ref type="bibr" target="#b7">[8]</ref>. The second pipeline focuses on the assignments of a Python programming course <ref type="bibr" target="#b19">[20]</ref>. It compares Copilot with students in solving programming problems in different aspects.</p><p>to learn and then implement. These algorithms are methods for sorting an array of numbers (integers or floats) in a descending or ascending manner. In these problems, time complexity, a measure of an algorithm's run-time as a function of input length, is an important factor to be considered.</p><p>From the algorithms described in the book, we selected bubble sort, bucket sort, heap sort, insertion sort, merge sort, quick sort, radix sort, and selection sort. We selected these algorithms based on their implementation complexity, from easy to hard, based on <ref type="bibr" target="#b7">[8]</ref>'s descriptions. Copilot's ability to understand and produce code for these algorithms, will allow the programmer to use the generated code in their code bases and instead focus on more complex parts of the program.</p><p>? Data Structures. From this section, we selected the Binary Search Tree (BST). BST is a basic data structure which is taught in algorithm design. Each node of the tree contains a value (called key) that is greater than all the values in the left sub-tree and smaller than all the values in its right sub-tree. The implementation of BST involves multiple steps, namely:</p><p>-Finding the minimum and maximum values in the tree before inserting any new value. -In-order tree walk to extract all the values in the tree in a sorted manner. -Finding the successor node. Given a node x, the successor of x is the node that has the smallest value which is greater than x.</p><p>? Graph Algorithms. From this section, we selected the Elementary Graph Algorithms. These algorithms are used to perform some elementary operations on a graph. Since graphs store information about how each node is connected to others, they can be used in implementing applications such as maps and social media user connections. We tested Copilot on the following graph algorithms problems:</p><p>-Generating code for a simple graph data structure.</p><p>-Breadth First Search (BFS) on a graph.</p><p>-Depth First Search (DFS) on a graph.</p><p>-Implementing Directed Acyclic Graphs (DAG). DAGs require a more complex implementation logic compared to simple graphs, since during initialization, based on the directions of the edges, we need to check if a cycle exists in the graph or not.</p><p>-Finding reachable vertices. A pair of vertices are defined as reachable, if both vertices can be reached from each other in a directed graph.</p><p>? Advanced Design and Analysis Techniques. We selected the greedy algorithms from this section. Unlike the algorithms described above, greedy algorithms are methods for solving optimization problems by breaking them down into multiple smaller problems and selecting the best solution at the given time. We selected the "activity selection", an introductory problem to greedy algorithms from <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Prompt Engineering</head><p>Alongside code completion, Copilot can generate code from natural language descriptions in the form of comments. However, as noted by <ref type="bibr" target="#b20">[21]</ref>, if the description becomes too long or detailed, Copilot's performance degrades. Authors, in <ref type="bibr" target="#b7">[8]</ref>, assumed that the reader has no experience in coding or algorithm design. For this reason, each problem is described by building upon concepts that were explained in the previous chapters. As a result, some problem descriptions span multiple paragraphs and sometimes, pages. Therefore, our prompt engineering was done in two ways:</p><p>1. Describing the problem: We needed to summarize each problem's description to feed them to Copilot, while staying as faithful as possible to the books.</p><p>To make sure that our descriptions were understandable and did contain enough information about the algorithm being described, we cross-checked each of them with those on popular coding websites such as W3SCHOOLS <ref type="bibr" target="#b35">[36]</ref> and GEEKSFORGEEKS <ref type="bibr" target="#b15">[16]</ref> as well. For cross-checking, the second author summarized <ref type="bibr" target="#b7">[8]</ref>'s algorithm descriptions while keeping Copilot's limits in mind. If there were differences in the descriptions (i.e., the description was missing some key elements in explaining the problem), the descriptions were revised. This approach was taken for sorting algorithms, binary search trees, and the "activity selection" problem. The second author created the input descriptions as explained above. Then, these descriptions where checked with the first author to make sure that they were correct, understandable, and contained enough information about the problem being described. The first two authors both have more than 5 years of experience in coding and program design. To assess the inter-rater agreement, we have calculated Cohen's Kappa score <ref type="bibr" target="#b6">[7]</ref>. While the score was 0.95 implying an almost perfect agreement, for cases where there were conflicts about the descriptions, we met and discussed the conflicts to resolve them. At the end, the descriptions were checked with the third author who has more than 10 years of experience in teaching algorithm design. Therefore, the final input descriptions were what all three authors agreed on.</p><p>Excluding sorting algorithms, other problems require code to be generated using a previous code as it is very common practice in both functional and object oriented programming For these problems, we followed exactly the book's example by asking Copilot to generate code for the underlying concept and then for the succeeding problems, we asked it to implement the solution using the code it had generated before. We have recorded all our descriptions and Copilot's responses in our replication package <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3.">Query Execution and Evaluation Metrics</head><p>Below, we briefly explain the 4 different markers which we have used to evaluate Copilot and explain them in detail in the rest of this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Successful code generation (Response Received).</head><p>Whether Copilot was able to understand the problem and output code for solving it.</p><p>2. Code correctness. Whether the code which was suggested has syntax errors or bugs.</p><p>3. Code optimality. Whether Copilot was able to generate solutions with optimal time complexity.</p><p>4. Code reproducibility. Whether the code generated by Copilot on another independent run has the same structure as another run's given the same description.</p><p>Testing Successful Code Generation:. For testing successful code generation, we gave the description of the problem in the form of comments. Then, Copilot was asked to generate code for the input description. Given that Copilot tries to understand the general purpose of the code from the script's filename, to make sure that solutions were generated from our descriptions, we gave the scripts unrelated names. For example, if we were testing on bubble sort, we named the corresponding script as "script 1".</p><p>Testing Code Correctness and Optimality:. For testing code correctness and optimality, we took 2 steps:</p><p>? In order to analyze generated codes' time complexity, the codes were inspected manually by the first two authors.</p><p>? In order to make sure that the generated codes were correct and functioned as intended, we wrote unit tests for each script and ran them to make sure that the generated solution runs without any syntax errors and/or bugs. All scripts are accessible in our replication package.</p><p>Testing Code Reproducibility:. For testing code reproducibility, first we need to ensure that our results are as independent from each other as possible. So, we terminated our internet connection and closed the editor after each experiment. After which, we re-connected to the internet, opened the editor, and asked Copilot to generate solutions by giving it the exact same description. Another step in testing reproducibility was repeating the steps above, 6 times for each problem. In order to make sure our results stay consistent over time, we performed 2 trials within a 30 day time window. Each trial, consists of 3 experiments for each algorithm and each experiment contains up to 10 suggestions provided by Copilot. The experiments done during the first trial are named tests 1 through 3 in our dataset. The results of the second trial which was conducted 30 days later are named tests 4 through 6 in our dataset. For quantifying the reproducibility between trials, we have compared the Abstract Syntax Trees (AST) of each of Copilot's suggestions between each test. To do this, we have used the code from <ref type="bibr" target="#b29">[30]</ref>. We measure reproducibility in 2 ways:</p><p>1. Inter-set reproducibility: The inter-set reproducibility is calculated as the average of AST similarity between the suggestions in each trial. Each trial consists of 3 separate experiments where we ask Copilot to generate a solution for the given problem. The inter-set reproducibility, measures how similar these suggestions (in a trial) are to each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Intra-set reproducibility:</head><p>The intra-set reproducibility is calculated as the average of AST similarity between the suggestions of different trials. We have conducted 2 different sets of trials within a 30 day time window. The intra-set reproducibility measures how similar the suggestions between 2 trials, are to each other after this time window.</p><p>To measure AST similarity, the codes that are to be compared to each other need to have no syntax errors. Even though some of Copilot's suggestions are similar to each other, since they had syntax errors, we could not calculate AST similarity between them without editing the generated code. Doing so, meant fixing these syntax errors by hand which would have been against our goal to test Copilot solely on code generation. As a result, for measuring AST similarity, we did not include suggestions which had errors and could not be compiled.</p><p>The second author, executed all the steps above in order to collect experiment results. Then, the first author examined and evaluated the results obtained by the second author in order to validate them on the markers described earlier in this section. This, resulted in a Cohen's Kappa coefficient of 0.61 which indicates a moderate agreement between the first and second authors. For resolving conflicts of opinion, the results were discussed with the third author in a meeting to reach a consensus. After doing so, for each algorithm category, a Cohen's Kappa coefficient to asses the inter-rater agreements between the authors were calculated as follows:</p><p>? Sorting Algorithms: 0.89</p><p>? Binary Search Trees: 1</p><p>? Elementary Graph Algorithms: 0.83</p><formula xml:id="formula_0">? Greedy Algorithms: 1</formula><p>Which results in an average Kappa coefficient of 0.93 which indicates an almost perfect agreement on the obtained results between the authors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">RQ2: Copilot vs. Human</head><p>In this subsection, we aim to describe our methodology for RQ2, on how to compare Copilot codes with human written codes. First we illustrate the dataset of programming tasks that we used in our experiments and explain why select this dataset. Then, we explain how we employ Copilot to generate solutions for each task in this dataset. After that, we present how we selected students' solutions for this comparison. Finally we discuss the criteria to compare Copilot with students in solving Python programming tasks from different aspects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Dataset of Programming Tasks</head><p>To conduct this part of the study, we require a dataset of some programming problems plus human-provided solutions. To address this requirement, we use a dataset of a Python repairing bug benchmark<ref type="foot" target="#foot_0">2</ref> that includes students' submissions for 5 Python programming assignments in a Python course. Since Copilot is still in initial steps and shows good performance on simple programming tasks, we choose this dataset of simple programming assignments with the novice programmer level codes (submissions) for a Python course. These tasks are not included in the programming tasks in HumanEval <ref type="bibr" target="#b4">[5]</ref> evaluation set. Also, this dataset includes different test cases for each task and a tool that gives us the possibility to automatically check the functional correctness of Copilot's solutions against test cases. In addition, the description of problems are humanwritten. It reduces the chance for Copilot to memorize the solutions from its trainset.</p><p>This dataset includes 2442 "Correct" and 1783 "Buggy" student submissions for 5 Python programming assignments in a Python course. Another study also used this dataset for characterizing the benefit of adaptive feedback for errors generated by novice developers <ref type="bibr" target="#b0">[1]</ref>. Table <ref type="table" target="#tab_2">1</ref> shows the description of each programming task. Each task includes a description of the problem, one or more reference solutions, a different number of submissions by students that includes "Correct" and "Buggy" solutions, and different unit tests for each task, with an average of 9 tests per problem, to evaluate the functional correctness of solutions. This dataset also contains a tool named "Refactory" to automatically repair the buggy submissions of students if applicable <ref type="bibr" target="#b19">[20]</ref>. In our study, we use this tool to repair buggy solutions generated by Copilot and students to evaluate the complexity of fixing bugs in codes generated by Copilot compared to those of novice programmers. This tool matches each buggy program with the closest correct solution based on its AST structure. Then, it modifies different blocks of the incorrect program to repair its bug(s) and convert it to a correct solution if possible. This tool shows better performance than other state-of-the-art methods in repairing buggy programs such as Clara <ref type="bibr" target="#b17">[18]</ref>. Despite others that need a large and diverse range of correct solutions, this tool can repair buggy codes even with one or two references (i.e., correct solutions).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Solving Programming Problems with Copilot</head><p>To generate solutions with Copilot, we feed the description of each programming task in Table <ref type="table" target="#tab_2">1</ref>, called prompt, to Copilot. At each attempt, Copilot only returns the Top-10 solutions for a prompt. Thus, we do not have access to the rest of the potential suggestions. To inquire about the Copilot's consistency in generating solutions, similar to the previous experiments, we repeat the process 5 times and each time collect its top 10 suggested solutions. Expressly, we ask Copilot to solve each programming problem in 5 different attempts and collect the top 10 suggested solutions in each one. Thus in total, we collect 50 solutions by Copilot for each problem.</p><p>As we already explained in subsection 3.2.1, there are different test cases per task. To evaluate the functional correctness of Copilot's solutions, a solution is considered "'Correct" if it passes all the unit tests related to its problem. Otherwise, it is considered as "Buggy".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Downsampling Student Solutions</head><p>The average number of student submissions for these tasks is 689.8. In each attempt on Copilot, we only have access to its top 10 suggestions. One solution to have more suggestions could be to increase the number of attempts on Copilot. But, increasing the number of attempts to more than 5 will increase the number of duplicate answers in Copilot's solutions. We discuss the duplicate solutions in Section 3.2.4 with more details. Thus, instead of increasing the number of attempts on Copilot, we downsample the students' submissions to the same size of Copilot solutions (50 in total) to have a fair comparison between students and Copilot in the number of solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4.">Evaluation Criteria</head><p>For this part of our study, we consider different criteria to compare solutions suggested by Copilot and students to solve these programming tasks. We investigate the solutions on the following markers. In the rest of this section, we explain each metric in more details.</p><p>1. The correct ratio of solutions (pass@Topk), 2. The repairing costs of buggy solutions, 3. The diversity of solutions, 4. The cyclomatic complexity of codes.</p><p>(1) The correct ratio of solutions (pass@Topk)</p><p>A very common metric to evaluate programming language models is pass@k metric <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b4">5]</ref>. For example, to calculate pass@100, we need to generate n ? 100 sample solutions, count the number of solutions that passed all test cases, and then calculate the fraction of 100 out of all correct solutions. However, since Copilot returns only the Top10 solutions in each attempt, we cannot accurately use this metric in our study.</p><p>In this study, what attracts our interest is the pass@Topk of all the attempts. It means that if we call Copilot n times for the same problem (the same prompt), n equals the number of attempts, and collect the Topk solutions of each attempt, then pass@Topk equals the fraction of these solutions that passed all the test units. As an example for pass@Top2, we collect all the Top2 suggested solutions for a problem in n = 5 different attempts (#solutions = k * n = 2 * 5 = 10). Then pass@Top2 reports the fraction of these Top2 solutions that passed all test units. However, we cannot calculate the pass@TopK for students.</p><p>Another evaluation that comes to our attention is the Correct Ratio (CR) of solutions. Here by CR, we mean the fraction of correct solutions out of all solutions suggested by Copilot or human for each problem. We calculate this fraction for each problem while collecting Topk suggestions of Copilot in different attempts. For students, we calculate the fraction of correct submissions out of all students' submissions for each problem. Also, we calculate the distribution of the CR and its average in independent attempts on Copilot. We like to study how increasing the number of attempts (giving different chances to Copilot to solve the same problem) impacts the CR.</p><p>(</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) The Repairing Costs of Buggy Solutions</head><p>After computing the CR for Copilot and students, we aim to compare Copilot's buggy solutions with students' buggy submissions. Our observation shows that several buggy solutions generated by Copilot can be easily converted into a correct solution by applying small changes. We discuss this observation in detail in Section 4.2.2. This observation brings us to the idea of repairing buggy solutions generated by Copilot and students and then comparing them in repairing costs. We use the repairing tool that we explained in Section 3.2.1, and report three different metrics to evaluate the repairing cost of buggy solutions <ref type="bibr" target="#b19">[20]</ref> including:</p><p>? Repair Rate: This metric shows the fraction of buggy codes that passed all test cases after the repair process.</p><p>? Avg. Repair Time: This metric shows the average time taken to repair a buggy program in seconds. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Top k Elements</head><p>Write a function top k that accepts a list of integers as the input and returns the greatest k number of values as a list, with its elements sorted in descending order. You may use any sorting algorithm you wish, but you are not allowed to use sort and sorted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="418">108</head><p>Total 2442 1783</p><p>? Relative Patch Size (RPS): This metric defines as the Tree-Edit-Distance (TED) between the AST of a buggy code and the AST of its repaired code, normalized by the AST size of the buggy code.</p><p>(3) The Diversity of Solutions It is already shown in language models such as Codex that increasing the number of sample solutions for a programming task can increase the number of correct solutions that pass all test units <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b20">21]</ref>. However, they did not study if this increment is due to the increasing the diversity of solutions or if the new correct solutions are just a duplication of previous ones.</p><p>Copilot claims that it removes duplicate solutions among the Top10 suggested solutions in a single attempt. However, our observations show the appearance of duplicate solutions in Top10 suggestions of a single attempt. Figure <ref type="figure" target="#fig_0">2</ref> shows three different solutions generated by Copilot for the task "Duplicate Elimination" at a single attempt. As we can see, the structure of all three codes is the same. The only difference between Figure <ref type="figure" target="#fig_0">2a</ref> and Figure <ref type="figure" target="#fig_0">2b</ref> is in the variable name, "item" and "i". Also, the solution in Figure <ref type="figure" target="#fig_0">2c</ref> is the same as the solution in Figure <ref type="figure" target="#fig_0">2a</ref> alongside comments. Since Copilot compares the sequence of characters to eliminate duplicates, it considers these three solutions as three unique suggestions in Top10 solutions of a single attempt.</p><p>In this study, we use a new method to remove the duplicate solutions in each attempt. We investigate if increasing the number of attempts and consequently increasing the total number of solutions will increase the number of unique solutions. Also, we compare the diversity of solutions (correct and buggy) provided by Copilot and students. This metric compares Copilot's novelty in generating solutions to that of students in solving a programming task.</p><p>To remark on the duplicate solutions, we compare the AST of two codes, similar to what we did in Section 2. We eliminate the last level leaves in AST which are variable or function names. Also, we ignore comments or any natural language text in each solution. Then, we calculate a similarity between the AST of every two solutions for a problem by method introduced in <ref type="bibr" target="#b29">[30]</ref>. If the similarity between two ASTs is equal to 1, then they are assumed to be duplicate. We keep just one of the solutions. Any value less than 1 represents a difference between the functionality of two solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(4) The Cyclomatic Complexity of Codes</head><p>Finally, we compare the complexity of codes generated by Copilot and students for each task. Cyclomatic Com- shows the number of independent paths. Specifically, the number of decisions that can be made in a source code <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b30">31]</ref>.</p><p>When comparing solutions for a problem, the lower a code's C.C. value is, the more optimized that code is considered to be. We use a Python package, RADON 3 , to calculate it. C.C. close or above 10 is interpreted as not an optimized code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Empirical results</head><p>In this section, we present the results we obtained to answer our RQs, one by one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">RQ1: Copilot on Algorithm Design</head><p>In this section, we assess the capability of Copilot to solve algorithmic problems. First, we describe our methodology to answer RQ1. Then we present our empirical results.</p><p>Tables 2 -9 show our results on testing Copilot's ability for code generation. In the following sections, we discuss our results in detail. To highlight the difference between our 2 trials which have been conducted 30 days apart from each other, for each marker, we have indicated the results of the trials separately from each other. To showcase our results from each round of testing in each trial, we count the number of successful tests. For example, if the generated code passes 2 of the 3 tests for a given marker, we display it as "2/3". We have also used "0" and "-" to differentiate between cases where we obtained no results and not applicable cases. For example, if we did not receive a response for a solution, we display it as "0/3" and since we had no responses, finding a correct solution was not applicable, hence, we use "-" to indicate this circumstance. Our results show that Copilot is relatively capable at understanding the problem and generating reproducible code which executes at optimal time complexity. However, if the algorithm requires helper functions (i.e., functions which require to be coded outside of the main function in 3 https://radon.readthedocs.io/en/latest order for the algorithm to be implemented) it will struggle to generate solutions. For the Bubble, Radix, and Heap sort algorithms, Copilot generated different codes during our second trial.</p><p>? Bubble Sort. This sorting algorithm is one of the easiest sorting algorithms and Copilot was able to generate correct, optimal code for it. As shown in Table <ref type="table" target="#tab_3">2</ref>, for the first trial, out of all 3 experiments, Copilot generated code that was related to bubble sort and all 3 contained correct solutions. However, during our second trial, Copilot was starting to generate code for insertion and selection sort (instead of bubble sort), as shown in Table <ref type="table" target="#tab_3">2</ref> as "0/3", when we gave it the same description as before.</p><p>? Radix Sort. This algorithm is more difficult to implement. Copilot struggled to understand what the problem was in our first trial ("1/3" in Table <ref type="table" target="#tab_3">2</ref>). However, it was able to generate correct, optimal code for it. In the second trial however, Copilot showed improvement in understanding the problem as it generated codes for solving the problem ("3/3" in Table <ref type="table" target="#tab_3">2</ref>) but none of the generated codes were correct ("0/3" in Table <ref type="table" target="#tab_3">2</ref>).</p><p>? Heap Sort. Since implementing heap sort requires implementing a max heap, and then writing a sorting function, this algorithm is one of the harder algorithms to implement. During our first set of tests, Copilot was unable to generate a solution for this problem until we explicitly asked it to "generate a function that implements the heap sort algorithm" and even then the generated code was faulty. However, during our second trial, Copilot generated the correct solution from our indirect description of the problem where we explained the algorithm in detail without mentioning its name.</p><p>Tables <ref type="table" target="#tab_3">2</ref> and<ref type="table" target="#tab_4">3</ref> show our results on Copilot's code generation ability and the suggested codes' reproducibility, respectively. As our results show, there is no clear pattern between algorithm complexity and Copilot's output. For example, out of the sorting algorithms we tested, Bubble sort is the easiest to implement and Copilot was able to output optimal code for it during the first set of experiments. However, during our second trial, given the same description, Copilot was unable to generate a single code snippet for this problem and instead generated solutions for other sorting algorithms. On the other hand, for the Heap sort algorithm which is much harder, during the first trial Copilot did not generate any correct solutions but did so during the second trial.</p><p>The authors disagreed on the result of generated codes for selection sort. Our source for describing the problems was <ref type="bibr" target="#b7">[8]</ref> and therefore, the input prompt was summarized from the description in the book. The given prompt for this algorithm was "create a function that accepts a list as input. the function should create two lists named sorted and unsorted. the function sorts an array by repeatedly finding the minimum element (considering ascending order) from unsorted list and putting it at the beginning of the sorted list. finally it should return the sorted array". Given this description, the second author only accepted solutions that followed this exact description, mainly those which created the two empty sorted and unsorted lists. Upon review however, the first and third authors mentioned that some solutions followed the selection sort algorithm, which can be found on the web, without following the exact steps mentioned in the description. After discussions, these solutions were considered as correct as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">Binary Search Trees (Kappa Score: 1)</head><p>For this problem, we first asked Copilot to generate the BST data structure which should comprise of a class with parent, right, and left nodes alongside the node's value. After that, we asked Copilot to generate a method which handles insertion per BST insertion algorithm for the class. Then, we asked Copilot to create a method for deleting a node. These operations require the BST to be re-built in order to conform to the BST property. We also asked Copilot to implement a search method for finding if a value is present in the BST. These 3 methods, comprise the base BST data structure. In the next steps, we asked Copilot to generate functions for finding the maximum and minimum value in a tree, performing an in-order tree walk, and finding the successor node of a child node. Tables <ref type="table" target="#tab_5">4</ref> and<ref type="table" target="#tab_6">5</ref> show our results.</p><p>As our results show, Copilot is adept at producing algorithms for BSTs. It should be noted that Copilot was able to generate code which conformed to the optimal time complexities that are required for an efficient BST. In addition, Copilot was able to generate multiple different versions (with iterative and recursive programming techniques) for "Finding maximum and minimum values in the tree", "In-order tree walk", and "Finding successor nodes" problems. As Table <ref type="table" target="#tab_5">4</ref> shows, unlike sorting algorithms, reproducibility was not an issue as Copilot generated the same code at least once throughout different experiments during different trials. However, as Table <ref type="table" target="#tab_6">5</ref> shows, there are differences between the codes that are suggested during each experiment which means that Copilot was not simply repeating itself.</p><p>For the "In-order Tree Walk" problem, Copilot generated functions inside the main function responsible for executing the walk. These functions were duplicate functions of those generated for finding minimum and maximum values in the tree. This is bad programming practice as it over-complicates the code. However, since these functions were not used by the original function at all, the generated code was still optimal. 4.1.3. Elementary Graph Algorithms (Kappa Score: 0.83) As our algorithms are becoming more complex, it is required for Copilot to generate code that uses the previous codes that it has generated. For example, checking if a graph is cyclic, requires using a BFS or DFS approach. If Copilot does not use the codes that it has generated for BFS and DFS during checking if a graph is cyclic, we will be left with code pieces that repeat the same operation over and over which is a bad practice in programming. Tables <ref type="table" target="#tab_7">6</ref> and<ref type="table" target="#tab_8">7</ref> show our results.</p><p>Our results show that like BSTs, Copilot is adept at generating code for elementary graph algorithms. As mentioned, up until now, we described the problem to Copilot in detail without mentioning the name of what we wanted explicitly. However, now that we are asking Copilot the name of the algorithm, the generated codes have little to no differences with each other even through different sets of trials. We observed that some of the generated code is using an advanced Python feature called "operator overloading" in which a native Python function is re-written by the programmer to behave differently depending on the arguments that it receives as input. During our testing for BFS and DFS, Copilot generated code for both algorithms regardless of us asking it to do so only for one of them.</p><p>Even though Copilot was able to recognize and generate code for our description, some of the generated codes had one flaw and since successor methods use the previous methods, this bug was present in every piece of generated code. This snow-balling effect, has affected our Kappa score as well. This bug, was a result of Copilot considering the nodes being named by integer numbers. As a result, if a node is created with a name that is not integer (e.g. "A" or "Node1" instead of "1" or "2"), the code will fail to iterate through the list of nodes and generate a syntax error. However, since the code functioned correctly given the normal usage, we labeled them as correct. The dataset in our replication package contains an in-depth explanation of this bug <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4.">Greedy Algorithms (Kappa Score: 1)</head><p>The "activity selection" problem requires the programmer to define a class for "activities". Each activity has a start and end time. The goal of this problem is: given a set of activities where each activity has its own start and ending time, return a set that contain the maximum number of activities that can be performed as long as they do not overlap. Overlapping is defined as:</p><p>? An activity's start time must be after a previous activity's end time.</p><p>? An activity should not happen during another activity.</p><p>Our results are outlined in Tables <ref type="table" target="#tab_9">8</ref> and<ref type="table" target="#tab_10">9</ref>.</p><p>Copilot was able to implement an activity class given the description "implement a class called activity. Each instance of this class has two attributes: start-time and end-time. Both should be integer numbers between 0 and 24". However, it failed to check for the class's input types to be integers and place a limit on the range of input numbers.</p><p>Next, we asked Copilot to implement a method for comparing activities. We gave it the following description: "implement a function for comparing two activities. the function should return True if the first activity ends before the second activity starts. if the inputs have overlapping start-times, return False". Here, Copilot implemented the description correctly. However, since this method is dependent on its inputs being instances of the activity class, this code will fail if the input is anything else. Type checking is important and a basic operation to do which Copilot fails to do here.</p><p>For adding activities to a set of activities, Copilot was asked to create a method which accepts a set of activities alongside a start time and end time of an activity. The method should first create a new activity instance with the given start and end time and then check if this new activity does not overlap with the activities in the set. Copilot was unable to generate the necessary code for this no matter how detailed the description was. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Findings:</head><p>Copilot is able to recognize fundamental algorithms by their names and generate correct, optimal code for them as long as the descriptions are short and concise. In some cases the developers may need to invoke Copilot multiple times in order to receive solutions that are correct and tailored to their descriptions. Challenges: Copilot is unable to generate code for type-checking variables. It also generates needlessly complicated code for some simple descriptions. Hence, Copilot still needs to be improved to truly be considered as a pair programmer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">RQ2: Copilot vs. Human in Solving Programming Problems</head><p>In this section, we discuss our findings to answer RQ2. We discuss the results for each criterion of our evaluation separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">The correct ratio for solutions of Copilot and</head><p>students' submissions As explained in section 3.2.4, we calculate the pass@Topk for solutions generated by Copilot for each programming task. The pass@Topk shows the fraction of correct solutions among the Topk solutions, collected from 5 different attempts. We normalized the values of this metric for the programming tasks.</p><p>Figure <ref type="figure" target="#fig_2">3a</ref> shows the normalized values for pass@Topk of each programming task for Copilot. TopK solutions range between Top1 to Top10 because each attempt on Copilot includes only the Top10 suggestions. Based on this result, Copilot cannot find correct solutions for "q2: Unique Dates Months". This task asks for "...solve the problem by implementing 3 different functions...". Also, test units of this task are based on implementing 3 different functions, but Copilot could not understand this point within the task description and tried to solve the problem in one function. Thus, all Copilot's solutions for this task failed the test cases.</p><p>There is no correct solutions in Copilot's Top3 suggestions for "q4: Sorting Tuples" in 5 different attempts. It increases to 0.02 in the set of Top4 solutions. For "q1", "q3" and "q5", the pass@Top1 is equal to 0.08, 0.13 and 0.13 , respectively. For some questions, the pass@Topk, at different values of k, shows greater values than the other questions. For example, "q5" has the greatest values for pass@Top4 and above. Also, "q4" has the lowest pass@Topk, for different values of k, after "q2".</p><p>In general, pass@Topk increases by increasing the value of k. It means collecting more number of solutions suggested by Copilot, increases the number of correct solutions and this growth can be different for different programming tasks.</p><p>In addition, fig <ref type="figure" target="#fig_2">3b</ref> shows the Correct Ratio (CR) of solutions in each attempt independently. However, the distribution of CRs in different attempts is varied, but adding new attempts can increase the average CR of solutions. For example, the average CR in the first attempt (atp1) is equal to 0.32 while it increases to 0.44 in the last attempts (atp5). It shows if we ask Copilot to solve the same problem multiple times (here 5 attempts), there is a chance to increase the CR among new Top10 suggested solutions on average. However, this is not correct for all questions. For example for "q1", the CR in "atp4" is 0.7 but it decreases to 0.4 in "atp5". But, for "q5", the CR in the first attempt is equal to 0.7 and it increases to 0.9 in the last attempt.</p><p>Since we cannot calculate pass@Topk for students, in Table <ref type="table" target="#tab_11">10</ref>, we compare the CR of solutions generated by Copilot with the CR of students' submissions. For this comparison, we calculate three different CRs for Copilot. The first, CR@Top1, reports the number of correct solutions out of all Top1 solutions in 5 different attempts for each programming task. CR@Top5 calculates the fraction of correct solutions out of all Top5 solutions suggested by Copilot in 5 different attempts. Finally, CR@Top10   represents the number of correct solutions generated by Copilot out of all its 50 solutions for a programming task.</p><p>Collecting more solutions decreases the CR of Copilot since it increases the fraction of wrong solutions. For some of the questions, CR@Top1 and CR@Top5 of Copilot are greater than students' CR. For all questions, the CR of students' submissions is greater than CR@Top10 for Copilot's suggestions. On average for all the programming tasks, the Correct Ratio (CR) of students' submissions is greater than the CR of Copilot's suggestions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">The repairing costs of Buggy solutions generated by</head><p>Copilot compare to students In this part, we compare the repair cost of buggy solutions for Copilot with students. As we already discussed, our observation shows there are buggy solutions that are generated by Copilot and are very similar to correct solutions. A small change can convert them into a correct solution. Therefore, we attempt to justify our observation by calculating the intersection between correct and buggy solutions of Copilot for each problem using BLEU score <ref type="bibr" target="#b25">[26]</ref>. BLEU is used in evaluating program synthesis approaches such as text-to-code, code summarization, and code prediction. Ren et al. <ref type="bibr" target="#b28">[29]</ref> introduces a new metric, called CodeBLEU, that measures the BLEU score on syntax and semantic codes. As a part of this new metric, they measure CodeBLEU between AST of codes.</p><p>To measure the overlap between correct and buggy solutions, we measure the BLEU score between AST of the buggy and correct. We omit those buggy codes which have syntax errors and cannot be converted into AST. Figure <ref type="figure" target="#fig_3">4</ref> shows the distribution of BLEU score between buggy and correct solutions of Copilot for different questions. As we can see in this figure, there are buggy solutions whose BLEU scores with correct solutions are greater than 0.75 which proves our observations about the intersection between buggy and correct solutions. Now that some of the buggy solutions generated by Copilot are very similar to the correct solutions, we are  interested into comparing the repairing cost of Copilot's buggy solutions with students' buggy submissions. As we have explained in Section 3.2.3, for this comparison, we need to downsample students' submissions to the same size of Copilot's suggestions. Figure <ref type="figure" target="#fig_4">5</ref> shows the distribution of repairing time for repairing students' buggy submissions. There are a high number of submissions with low repairing time and few with high repairing time. Thus, to keep the distribution of repairing costs in the sampleset close to the entire populations, we repeat the downsampling process 5 times and report all repairing metrics for students submissions based on the average of all 5 sampleset.</p><p>As we can find in Table11, the average repair rate for Copilot's buggy solutions is greater than students', which are 0.95 and 0.89 respectively. This means that on average, 95% of buggy solutions generated by Copilot have been fixed after the repair process. For example, for "q4: Sorting Tuples" and "q5: Top-k Elements", all buggy solutions of Copilot (100%) have been fixed while the repairing rate of students' submissions for these two tasks is equal to 85%.</p><p>In addition, the average repair time for Copilot's buggy solutions is less than students'. This means that not only the repairing tool can fix the majority of Copilot's buggy solutions but also it can fix them faster than students' buggy submissions. The average repairing time for Copilot's buggy solutions is 4.94 seconds while it is equal to 6.48 seconds for the students. The reason is that on average, the Relative Patch Size (RPS) of Copilot's buggy solutions that need to be repaired is smaller than students'. As we can find in Table <ref type="table" target="#tab_12">11</ref>, the average RPS for Copilot and students are 0.33 and 0.35, respectively.</p><p>We can conclude that however on average, the CR of students' submissions is greater than Copilots' solutions, but the repairing costs of buggy solutions of Copilot is less than students. With a repairing tool, we can repair majority of buggy solutions generated by Copilot and increase its CR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3.">The diversity of solutions between Copilot and students</head><p>The diversity of solutions shows the novelty of Copilot and students in solving different problems. Also, it shows that while increasing the number of sample codes increases the fraction of correct solutions, this increamnet is due to the diversity of solutions or new correct solutions are duplicates. As we discussed in Section 3.2.4, we observe duplicate solutions in a single attempt and across multiple attempts on Copilot to solve a problem. On the other hand, we observe duplicate solutions among students' submissions as well. For example, for "q1", and "Sequential Seach", after comparing the ASTs of students' correct submissions, 54.32% of their submissions are identified to be duplicated.</p><p>To compare the diversity among students' submissions and Copilot's solutions, we randomly downsample 10 students' submissions in 5 different samplesets and considers them as 5 different attempts. Then, in each attempt on Copilot and for each sampleset of students' submissions, we eliminate duplicate correct and buggy solutions. There are a few buggy solutions for Copilot and students with syntax errors which cannot be converted into AST (3 solutions). We consider them as non-duplicate buggy solutions.</p><p>Figure <ref type="figure" target="#fig_5">6</ref> shows the cumulative distribution of Correct (C) solutions, None Duplicate Correct (NDC) solutions, Buggy (B) solutions, and None Duplicate Buggy (NDB) solutions by Copilot and students across different tasks. Increasing the number of attempts on Copilot leads to a jump in the number of correct solutions for "'q1" and "q5" from 2 to 18 and 7 to 38 respectively. However, for "q3" and "q4", this growth is smaller. The number of None Duplicate Correct (NDC) solutions of Copilot is less than or equal to the number of Correct (C) solutions in each attempt for each task. This is the same story for Buggy solutions. It shows, however Copilot claims it removes the duplicate solutions, but there are still duplicates in Top10 solutions of each attempt.</p><p>The difference between C and NDC in students' submissions is less than Copilot. For example, in "q3", the cumulative number of C solutions generated by Copilot in different attempts is greater than students' submissions in different samplesets. However, it is the opposite for NDC solutions. In "atp5" the cumulative number of C solutions generated by Copilot equals 28 and it equals 22 after 5 sampleset on students' submissions. However, the cumulative NDC solutions at these attempts equal 2 (out of 28) for Copilot and it equals 21 (out of 22) for students. It shows more diversity between correct and even buggy submissions of students compare to Copilot's solutions. As another example for Copilot, there is no more NDC solution after "atp3" for "q3" and "q5". This means that by increasing the number of solutions generated by Copilot for these two questions, the CR increases due to the duplication of correct solutions not generating new ones.</p><p>In general, the diversity of correct and buggy submissions for students is more than Copilot's. While there is no guarantee that all non-duplicate solutions are optimized, students solved these 5 tasks with more diverse and novel solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4.">The Cyclomatic Complexity of Codes</head><p>In this section, we calculate the Cyclomatic Complexity (C.C.) of codes generated by Copilot and students. Table <ref type="table" target="#tab_13">12</ref> shows the average and the standard deviation of C.C. for the correct solutions generated by Copilot and students. It is worth mentioning that we use the sampling method explained in section 3.2.3 to collect students' correct solutions. On average, the correct solutions suggested by Copilot are found to be more optimized than students' solutions. However, we should consider that for example, for "q2", Copilot has no correct solutions; or the CR of Copilot for "q4" is only 8%. However, in general, Copilot recommends less complex solutions than students for the same questions except for "q1". But, for "q1", the C.C. of Copilot's correct solutions have a lower standard deviation.  It means that its C.C. is less spread around the average. Also, for "q5", Copilot used Python built-in functions "Sort" and "Sorted", however it was asked in the description to not use them. Similar to previous one, we can conclude that however based on Section 4.2.3, the diversity of correct solutions of students is greater than Copilot's, but Copilot suggests more optimize solutions for the same problem.</p><p>Findings: The correct ratio and diversity of students' submissions is greater than Copilot's. However, the cost of repairing buggy solutions generated by Copilot is less than students'. In addition, the complexity of Copilot's generated codes is less than students'. Challenges: Copilot has difficulty understanding some requirements in the description of tasks. This affects the correct ratio of its solutions. However, students understand those details and consider them in their submissions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion and Limitations</head><p>Our results show that Copilot cannot understand some details in the description of problems which are understandable by humans. For example, when asking Copilot to implement the "activity" class in Section 4.1.4, Copilot cannot understand putting limits on variables even though it was asked to do so explicitly. As another example, in q5, "Top-k Elements", it is asked in the description to "... not use Python's built-in functions sort and sorted ...". Copilot cannot understand this detail and uses these two built-in functions in all of the correct solutions. However, the majority of students avoided using these built-in functions. Instead, they wrote a sorting algorithm and then called it for sorting tasks or used other built-in functions such as "append", "remove" and "max". As our results in Section 4.1 shows, Copilot suggests correct solutions for different sorting algorithms (meaning that Copilot is familiar with different sorting algorithms such as "Bubble Sort" or "Merge Sort"), but it did not use them in q5 because it could not figure out the requirements of the problem. On the other side, students apply their knowledge about sorting algorithms to solve this problem. Also, in q4, "Sorting Tuple", it is asked to return the list of tuples in an order that "... older people are at the front ...". Copilot annot understand this part. In 92% of suggestions, it returned the sorted tuples in the default order: ascending. However, students considered this point in their submission. We even checked some of the buggy submissions by students. Our observations show that even in buggy submission, students considered the correct order of sorting. It means that they fully understood what the point of sorting tuples is in a way that "...older people are at the front...".</p><p>Furthermore, for more exploration, we performed some experiments by applying different scenarios and report their impacts on the results:</p><p>Scenario#1: In this scenario, we changed "...older people are at the front..." to "...descending order..." in the description of q4 and repeated the process with Copilot to generate solutions. This small change improves the CR from 14% to 79%. This improvement shows there are some details/keywords in the description of problems that seem obvious to humans, but Copilot cannot understand those details in natural language. If we change those details into programming specific/technical keywords such as "descending", it can help Copilot recommend relevant solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scenario#2:</head><p>We have a similar observation for The q2, "Unique Birthday", where the Copilot cannot understand the requirements mentioned in the description, however, all students considered it. In this question, it is mentioned that "...implement 3 different functions unique day, unique month and contains unique day...", to address the problem. Copilot could not understand this condition. Despite q5 that it is not testing its requirement with its test cases, test cases for q2 are testing all 3 functions. Thus, the CR of Copilot for q2 equals zero because all 50 solutions in different attempts have failed on some of the test units. So, in this scenario, we gave 3 separate descriptions to Copilot for unique day, unique month, and contains unique day functions in the same source file. Here is the revised description that we used:</p><p>? unique day: Given a day and a list of possible birthday dates return True if there is only one possible birthday with that day, and False otherwise.</p><p>? unique month: Given a month and a list of possible birthday dates, return True if there is only one possible birthday within that month, and False otherwise.</p><p>? contains unique day: Given a month and a list of possible birthday dates, return True if there is only one possible birthday with that month and day, and False otherwise.</p><p>We start with the description of unique day at the first line of the source file. Then, we accepted the first solution suggested by Copilot. We continued with the description of unique month in the next line and accepted the first suggested solution and followed the same instruction for contains unique day. We repeat the process 50 times to generate 50 solutions that contain 3 separate functions. Copilot even calls unique day function in some of its suggestions for contains unique day function. We mentioned our sample solutions in our replication package. Since there are separate unit tests to test each function separately, we run related tests against each function. In this scenario, the CR of unique day, unique month and contains unique day are 88%, 0% and 40% respectively.</p><p>While the original description was clear to students, Copilot could not understand it. Instead of asking Copilot to solve the problem with different functions, we divide a problem into 3 different problems. It increases the CR for unique day and contains unique day. However, the CR of unique month is still zero. In the following, we investigate this result with a different scenario.</p><p>Scenario#3: Since Copilot could not find any correct solutions for unique month, we manually checked its suggested solutions. We found that in all buggy solutions, Copilot refers to the second item of "birthday" tuple in the list of birthday dates as the month of birthday. However, test cases consider month as the first item of tuples. For example, consider below test case:</p><p>? unique month (Month = "January", Birthdays = [( "January","1" ), ( "January", "2" )]).</p><p>In each tuple in the list of birthdays, for example, ("January","1"), Copilot collected the second item as a month, however the first item refers to the month of birthday.</p><p>In the description of "unique month", we added the above test case as a sample input, at the end of the description. It improves the CR of "unique month" from 0% to 91%. It shows that adding sample input or sample test cases in the description of problems can help Copilot to generate more correct solutions. In addition, we randomly checked 20% of students' submissions (both correct and buggy). Our observation shows that none of them assumed any wrong structure for the input data, while the structure of input is not clear in the description of the question. Thus, we assume that there is some extra clarification between students and professors about the structure of input.</p><p>Another limitation that Copilot has and is also observed by <ref type="bibr" target="#b20">[21]</ref>, is its difficulties in understanding long descriptions. Throughout our testing in Section 4.1 and 4.2, we observed that Copilot might misunderstand the problem entirely if the description contains multiple sentences (whether short or long).</p><p>We also observed that Copilot tries to mimic developer's naming, commenting, and coding style. As explained and laid out in our replication package <ref type="bibr" target="#b12">[13]</ref>, we used comments and doc-strings to separate Copilot's output from our own notes. We also wrote some unit testing methods for each script to make sure of the functionality of each script. After some time, we saw that Copilot was generating similar testing snippets and comments at the end of some suggestions for the same problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Threats to Validity</head><p>The first threat to construct validity comes from the fact that Copilot is closed-source. We cannot analyze our results based on the characteristics (and expected behavior) of Copilot's trained model. This is also the case for Copilot's training data, hence we are not able to indicate whether it memorized the solutions to these inquiries from its training set or whether it generates a unique solution. Similar to other researchers, we can only investigate Copilot's functionality in suggesting code for provided questions/descriptions.</p><p>The second threat to construct validity concerns the limited number of questions to compare Copilot's suggestions with humans'. The dataset of the Python programming course includes five assignments for Python beginners. These five questions may not be enough to uncover all the strengths and weaknesses of Copilot. Despite this limitation, this dataset provided us an opportunity to compare Copilot with human programmers, because it includes the students' submissions. The other advantage of this dataset is that the description of the questions is human written and decreases the chance of memorizing solutions from Copilot's training dataset. Future works can explore more diverse questions in a human-centered study to more comprehensively compare Copilot with humans in solving problems.</p><p>All Copilot's suggestions collected during our experiments are publicly available online in our replication package <ref type="bibr" target="#b12">[13]</ref> However, as our experiments have shown, Copilot's suggestions change over time and are not always consistent. The reason is that the development team is improving the Copilot engine in an ongoing manner, perhaps by feeding new code samples, or learning from new queries submitted to Copilots. As a result, we cannot guarantee that other researchers will receive the same suggestions and results that we did by performing the same experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper, we have studied Copilot's ability on code generation and compared its generated codes with those of humans. Our results show that Copilot is able to generate correct and optimal solutions for some fundamental problems in algorithm design. However, the quality of the generated codes depend greatly on the conciseness and depth of the prompt that is provided by the developer. Furthermore, our results indicate that Copilot still needs more development in fully understanding natural language utterances in order to be able fill-in the position of a pairprogrammer. Moreover, our results show that even though Copilot may be unable to generate codes that satisfy all the criteria described in the prompt, the generated codes can be incorporated by the developer with little to moderate change to the provided prompt or the generated codes. Given that recently Copilot has been released as a commercial product, a new wave of developers will have access to it. This will undoubtedly increase Copilot's training dataset and will also expose more of its shortcomings. Both outcomes however, may result in Copilot's ability as pair-programmer to be improved over time. Therefore, for future work, we suggest investigating the current RQs further, by exploring more diverse programming tasks, in a human-centered study, to indicate the capabilities of Copilot as an effective programming assistant for humans.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Three different solutions were generated by Copilot for "Duplicate Eliminatation" Task in one attempt.There is no difference between the approach of these 3 solutions in solving the task. The only difference between (a) and (b) is in variable names, "i" and "item". The difference between (c) and (b) is the additional comment in (c). The differences between (c) and (a) are in variable names and comments.plexity (C.C.) (McCabe's Cyclomatic Complexity)shows the number of independent paths. Specifically, the number of decisions that can be made in a source code<ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b30">31]</ref>. When comparing solutions for a problem, the lower a code's C.C. value is, the more optimized that code is considered to be. We use a Python package, RADON 3 , to calculate it. C.C. close or above 10 is interpreted as not an optimized code.</figDesc><graphic url="image-5.png" coords="9,215.37,97.54,161.22,55.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>4. 1 . 1 .</head><label>11</label><figDesc>Sorting Algorithms (Kappa Score: 0.89)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Evaluation of correct solutions generated by Copilot. Plot (a) shows the normalized values for pass@Topk metrics against different values of k. It shows the fraction of correct solutions between Topk solutions of 5 different attempts. Plot (b) shows the distribution, average and standard deviation of correct ratio in each attempt for different programming tasks.</figDesc><graphic url="image-7.png" coords="13,37.61,201.49,260.03,154.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The distribution of BLEU score for ASTs of Copilot's buggy solutions for each programming task. The BLEU score represents the similarity between buggy and correct solutions generated by Copilot. The BLEU score between several buggy and correct solutions are greater than 0.75 in different programming tasks. Thus, a small change in several buggy solutions can convert them into correct solutions.</figDesc><graphic url="image-9.png" coords="14,89.62,127.34,416.05,205.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The distribution of repairing time for students' buggy submissions. It shows that the frequency of submissions with low repairing time is much more than submissions with high repairing time. Thus, we repeat the downsampling process on students' submissions 5 times to observe the same distribution in samplesets</figDesc><graphic url="image-10.png" coords="14,89.62,475.78,416.05,205.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The cumulative distribution of solutions by Copilot and students. It shows the cumulative distribution of Correct (C), None Duplicate Correct (NDC), Buggy (B) and None Duplicate Buggy (NDB) solutions for Copilot and students. Attempts (atp) for students equals to the sampleset of randomly selection of their submission. The growth of NDC solutions for Copilot's solutions decreases or stops for some programming tasks while the number of its Correct (C) solutions increases. The diversity of submissions for students is greater than Copilot's solutions.</figDesc><graphic url="image-11.png" coords="16,37.61,413.36,520.06,225.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Solution Found Optimize Solution Found Reproduced Correct solution Same Day After 30 Days Students' Submissions Evaluation Repairing Cost of Buggy Solutions Diversity of Correct Solutions Quality of Solutions Ratio of Correct Solutions Students GitHub Copilot Assignments Python Programming Course Recommened Solution Evaluation Repairing Tool Prompt Engineering Algorithmic Problems def search(x, seq): for i in range(len(seq)): if x &lt;= seq[i]: return i return len(seq)</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>A summary of the dataset used to compare Copilot with the human in solving simple programming tasks. The Dataset includes the assignments and submissions of a Python programming course. It includes students' submissions for 5 Python programming tasks in two categories of "Correct" and "Buggy" solutions<ref type="bibr" target="#b19">[20]</ref>.We represent a person using a tuple (gender, age). Given a list of people, write a function sort age that sorts the people and returns a list in an order such that the older people are at the front of the list. You may assume that no two members in the list of people are of the same age.</figDesc><table><row><cell></cell><cell>Task</cell><cell>Description</cell><cell>#Correct Solution</cell><cell>#Buggy Solution</cell></row><row><cell></cell><cell></cell><cell>Takes in a value "x" and a sorted sequence "seq",</cell><cell></cell><cell></cell></row><row><cell>q1</cell><cell>Sequential Search</cell><cell>and returns the position that "x" should go to such that the sequence remains sorted. Otherwise, return</cell><cell>768</cell><cell>575</cell></row><row><cell></cell><cell></cell><cell>the length of the sequence.</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Given a month and a list of possible birthdays, re-</cell><cell></cell><cell></cell></row><row><cell>q2</cell><cell>Unique Dates Months</cell><cell>turns True if there is only one possible birthday wise. Implement 3 different functions: unique day, with that month and unique day, and False other-</cell><cell>291</cell><cell>435</cell></row><row><cell></cell><cell></cell><cell>unique month and contains unique day.</cell><cell></cell><cell></cell></row><row><cell>q3</cell><cell>Duplicate Elimination</cell><cell>Write a function remove extras(lst) that takes in a rences of any element removed. list and returns a new list with all repeated occur-</cell><cell>546</cell><cell>308</cell></row><row><cell>q4</cell><cell>Sorting Tuples</cell><cell></cell><cell>419</cell><cell>357</cell></row><row><cell>q5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Results of Copilot's code generation ability on sorting algorithms.</figDesc><table><row><cell>Algorithm</cell><cell cols="8">Response Received First Trial Second Trial First Trial Second Trial First Trial Second Trial First Trial Second Trial Correct Solution Reproduced Optimal</cell></row><row><cell>Bubble Sort</cell><cell>3/3</cell><cell>0/3</cell><cell>3/3</cell><cell>-</cell><cell>3/3</cell><cell>-</cell><cell>3/3</cell><cell>-</cell></row><row><cell>Bucket Sort</cell><cell>3/3</cell><cell>3/3</cell><cell>3/3</cell><cell>1/3</cell><cell>3/3</cell><cell>3/3</cell><cell>3/3</cell><cell>1/3</cell></row><row><cell>Heap Sort</cell><cell>1/3</cell><cell>3/3</cell><cell>0/3</cell><cell>2/3</cell><cell>-</cell><cell>2/3</cell><cell>-</cell><cell>2/3</cell></row><row><cell>Insertion Sort</cell><cell>3/3</cell><cell>3/3</cell><cell>3/3</cell><cell>3/3</cell><cell>3/3</cell><cell>3/3</cell><cell>3/3</cell><cell>3/3</cell></row><row><cell>Merge Sort</cell><cell>3/3</cell><cell>2/3</cell><cell>2/3</cell><cell>0/3</cell><cell>0/3</cell><cell>-</cell><cell>2/3</cell><cell>-</cell></row><row><cell>Quick Sort</cell><cell>3/3</cell><cell>3/3</cell><cell>1/3</cell><cell>1/3</cell><cell>2/3</cell><cell>2/3</cell><cell>1/3</cell><cell>1/3</cell></row><row><cell>Radix Sort</cell><cell>1/3</cell><cell>3/3</cell><cell>1/3</cell><cell>0/3</cell><cell>-</cell><cell>2/3</cell><cell>1/3</cell><cell>-</cell></row><row><cell>Selection Sort</cell><cell>3/3</cell><cell>3/3</cell><cell>3/3</cell><cell>2/3</cell><cell>2/3</cell><cell>-</cell><cell>3/3</cell><cell>2/3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Similarity ratios of the AST of Copilot's suggestions on sorting algorithms.</figDesc><table><row><cell>Algorithm</cell><cell cols="2">Inter-set First Trial Inter-set Second Trial</cell><cell>Intra-set</cell></row><row><cell>Bubble Sort</cell><cell>0.93</cell><cell>-</cell><cell>-</cell></row><row><cell>Bucket Sort</cell><cell>0.76</cell><cell>0.38</cell><cell>0.49</cell></row><row><cell>Heap Sort</cell><cell>-</cell><cell>0.39</cell><cell>-</cell></row><row><cell>Insertion Sort</cell><cell>0.93</cell><cell>1</cell><cell>0.96</cell></row><row><cell>Merge Sort</cell><cell>0.007</cell><cell>-</cell><cell>-</cell></row><row><cell>Quick Sort</cell><cell>0.54</cell><cell>0.4</cell><cell>0.008</cell></row><row><cell>Radix Sort</cell><cell>-</cell><cell>0.43</cell><cell>-</cell></row><row><cell>Selection Sort</cell><cell>0.29</cell><cell>-</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Results of Copilot's code generation ability on BSTs.</figDesc><table><row><cell>Algorithm</cell><cell cols="8">Response Received First Trial Second Trial First Trial Second Trial First Trial Second Trial First Trial Second Trial Correct Solution Reproduced Optimal</cell></row><row><cell>Binary Search Tree</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Data Structure</cell><cell>3/3</cell><cell>1/3</cell><cell>3/3</cell><cell>1/3</cell><cell>3/3</cell><cell>-</cell><cell>3/3</cell><cell>1/3</cell></row><row><cell>Finding Minimum and</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Maximum Values in Tree</cell><cell>3/3</cell><cell>3/3</cell><cell>1/3</cell><cell>2/3</cell><cell>3/3</cell><cell>3/3</cell><cell>1/3</cell><cell>2/3</cell></row><row><cell>In-order Tree Walk</cell><cell>3/3</cell><cell>3/3</cell><cell>3/3</cell><cell>3/3</cell><cell>3/3</cell><cell>3/3</cell><cell>3/3</cell><cell>3/3</cell></row><row><cell>Finding The Successor Node</cell><cell>3/3</cell><cell>3/3</cell><cell>2/3</cell><cell>3/3</cell><cell>3/3</cell><cell>3/3</cell><cell>2/3</cell><cell>3/3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Similarity ratios of the AST of Copilot's suggestions on BSTs.</figDesc><table><row><cell>Algorithm</cell><cell cols="3">Inter-set First Trial Inter-set Second Trial Intra-set</cell></row><row><cell>Binary Search Tree</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Data Structure</cell><cell>0.17</cell><cell>-</cell><cell>-</cell></row><row><cell>Finding Minimum and</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Maximum Values in Tree</cell><cell>1</cell><cell>0.96</cell><cell>0.91</cell></row><row><cell>In-order Tree Walk</cell><cell>0.72</cell><cell>0</cell><cell>0.49</cell></row><row><cell>Finding The Successor Node</cell><cell>-</cell><cell>0.1</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Results of Copilot's code generation ability on elementary graph algorithms.</figDesc><table><row><cell>Algorithm</cell><cell cols="8">Response Received First Trial Second Trial First Trial Second Trial First Trial Second Trial First Trial Second Trial Correct Solution Reproduced Optimal</cell></row><row><cell>Simple Graph</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Data Structure</cell><cell>2/3</cell><cell>2/3</cell><cell>2/3</cell><cell>0/3</cell><cell>2/3</cell><cell>2/3</cell><cell>1/3</cell><cell>0/3</cell></row><row><cell>Breadth First Search</cell><cell>3/3</cell><cell>3/3</cell><cell>3/3</cell><cell>3/3</cell><cell>3/3</cell><cell>3/3</cell><cell>3/3</cell><cell>3/3</cell></row><row><cell>Depth First Search</cell><cell>3/3</cell><cell>3/3</cell><cell>3/3</cell><cell>3/3</cell><cell>3/3</cell><cell>3/3</cell><cell>3/3</cell><cell>3/3</cell></row><row><cell>Directed Acyclic</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Graph Data Structure</cell><cell>2/3</cell><cell>3/3</cell><cell>2/3</cell><cell>0/3</cell><cell>2/3</cell><cell>2/3</cell><cell>2/3</cell><cell>0/3</cell></row><row><cell>Finding Reachable</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Vertices</cell><cell>3/3</cell><cell>3/3</cell><cell>3/3</cell><cell>3/3</cell><cell>3/3</cell><cell>2/3</cell><cell>3/3</cell><cell>3/3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Similarity ratios of the AST of Copilot's suggestions on elementary graph algorithms.</figDesc><table><row><cell>Algorithm</cell><cell cols="3">Inter-set First Trial Inter-set Second Trial Intra-set</cell></row><row><cell>Simple Graph</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Data Structure</cell><cell>0.09</cell><cell>0</cell><cell>0</cell></row><row><cell>Breadth First Search</cell><cell>-</cell><cell>0.47</cell><cell>-</cell></row><row><cell>Depth First Search</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Directed Acyclic</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Graph Data Structure</cell><cell>0</cell><cell>0.27</cell><cell>0</cell></row><row><cell>Finding Reachable</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Vertices</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Results of Copilot's code generation ability on greedy algorithms, Second Trial First Trial Second Trial First Trial Second Trial First Trial Second Trial Implement The Activity Class</figDesc><table><row><cell>Algorithm</cell><cell>Solution Found First Trial</cell><cell>Correct Solution</cell><cell>Reproduced</cell><cell>Optimal</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Similarity ratios of the AST of Copilot's suggestions on greedy algorithms</figDesc><table><row><cell>Implement The Activity Class</cell><cell>0.11</cell><cell>0.16</cell><cell>0.13</cell></row><row><cell>Comparing Activities</cell><cell>0.16</cell><cell>0.35</cell><cell>0.25</cell></row><row><cell>Adding Activities to A Set of</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Activities</cell><cell>0.12</cell><cell>0.15</cell><cell>0.14</cell></row><row><cell>Generate All from comment</cell><cell>0</cell><cell>0</cell><cell>0</cell></row></table><note><p><p>Algorithm</p>Inter-set: First Trial Inter-set: Second Trial Intra-set</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10</head><label>10</label><figDesc></figDesc><table><row><cell cols="5">: The Correct Ratio (CR) of Copilot's solutions while col-</cell></row><row><cell cols="5">lecting Top1, Top5 and Top10 solutions in all 5 attempts compare to</cell></row><row><cell cols="3">the Correct Ratio (CR) of students's submissions</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Copilot</cell><cell></cell><cell>Students</cell></row><row><cell>Task</cell><cell cols="3">CR@Top1 CR@Top5 CR@Top10</cell><cell>CR</cell></row><row><cell>q1 Sequential Search</cell><cell>0.6</cell><cell>0.44</cell><cell>0.36</cell><cell>0.57</cell></row><row><cell>q2 Unique Dates Months</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.40</cell></row><row><cell>q3 Duplicate Elimination</cell><cell>1</cell><cell>0.72</cell><cell>0.56</cell><cell>0.64</cell></row><row><cell>q4 Sorting Tuples</cell><cell>0.00</cell><cell>0.08</cell><cell>0.14</cell><cell>0.54</cell></row><row><cell>q5 Top-k Elements</cell><cell>1</cell><cell>0.92</cell><cell>0.76</cell><cell>0.79</cell></row><row><cell>Total</cell><cell>0.52</cell><cell>0.43</cell><cell>0.35</cell><cell>0.59</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11 :</head><label>11</label><figDesc>Comparing the Repairing Cost of Copilot's suggestions with students's submissions</figDesc><table><row><cell></cell><cell></cell><cell>Copilot</cell><cell></cell><cell></cell><cell>Students</cell><cell></cell></row><row><cell>Task</cell><cell>Rep Rate</cell><cell>Avg Rep Time(sec)</cell><cell>Avg rps</cell><cell>Rep Rate</cell><cell>Avg Rep Time</cell><cell>Avg RPS</cell></row><row><cell>q1 sequential search</cell><cell>0.94</cell><cell>9.61</cell><cell>0.48</cell><cell>0.98</cell><cell>2.58</cell><cell>0.40</cell></row><row><cell>q2 unique dates months</cell><cell>0.92</cell><cell>3.26</cell><cell>0.28</cell><cell>0.82</cell><cell>3.81</cell><cell>0.44</cell></row><row><cell>q3 duplicate elimination</cell><cell>0.91</cell><cell>0.64</cell><cell>0.26</cell><cell>0.96</cell><cell>4.35</cell><cell>0.30</cell></row><row><cell>q4 sorting tuples</cell><cell>1.00</cell><cell>0.78</cell><cell>0.15</cell><cell>0.85</cell><cell>8.82</cell><cell>0.29</cell></row><row><cell>q5 top-k elements</cell><cell>1.00</cell><cell>10.40</cell><cell>0.50</cell><cell>0.85</cell><cell>12.84</cell><cell>0.30</cell></row><row><cell>Total</cell><cell>0.95</cell><cell>4.94</cell><cell>0.33</cell><cell>0.89</cell><cell>6.48</cell><cell>0.35</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 12 :</head><label>12</label><figDesc>The Cyclomatic Complexity (C.C.) of Copilot's solutions compare to students' submissions</figDesc><table><row><cell>Question</cell><cell cols="2">C.C. Copilot C.C. Students</cell></row><row><cell>Sequential Search</cell><cell>5.8 ? 1.94</cell><cell>4.63 ? 2.1</cell></row><row><cell>unique dates Months</cell><cell>-</cell><cell>4.18 ? 1.03</cell></row><row><cell>Duplicate Elimination</cell><cell>3 ? 0.01</cell><cell>3.12 ? 0.5</cell></row><row><cell>Sorting Tuples</cell><cell>1 ? 0</cell><cell>4.13 ? 1.03</cell></row><row><cell>Top k Elements</cell><cell>1.44 ? 0.69</cell><cell>3.3 ? 1.46</cell></row><row><cell>Total</cell><cell>2.81</cell><cell>3.87</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>https://github.com/githubhuyang/refactory</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Characterizing the pedagogical benefits of adaptive feedback for compilation errors by novice programmers</title>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">Z</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sindhgatta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karkare</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Software Engineering Education and Training</title>
		<meeting>the ACM/IEEE 42nd International Conference on Software Engineering: Software Engineering Education and Training</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="139" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Syntax-guided synthesis</title>
		<author>
			<persName><forename type="first">R</forename><surname>Alur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bodik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Juniwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Raghothaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Seshia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Solar-Lezama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Torlak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Udupa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">O</forename><surname>Asare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nagappan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Asokan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.04741</idno>
		<title level="m">Is github&apos;s copilot as bad as humans at introducing vulnerabilities in code? arXiv preprint</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P D O</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Brockman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.03374</idno>
		<title level="m">Evaluating large language models trained on code</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Clement</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Drain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Timcheck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Svyatkovskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sundaresan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.03150</idno>
	</analytic>
	<monogr>
		<title level="m">multi-mode translation of natural language and python code with transformers</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A coefficient of agreement for nominal scales</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Educational and psychological measurement</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="37" to="46" />
			<date type="published" when="1960">1960</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Cormen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Leiserson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Rivest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Introduction to algorithms. MIT press, 4th edition</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Cormen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Leiserson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName><surname>Rivest</surname></persName>
		</author>
		<ptr target="https://www.goodreads.com/book/show/58064696-introduction-to-algorithms" />
		<title level="m">Introduction to algorithms reviews</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generating formal system models from natural language descriptions</title>
		<author>
			<persName><forename type="first">R</forename><surname>Drechsler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">G</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE International High Level Design Validation and Test Workshop (HLDVT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="164" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Solving linear algebra by program synthesis</title>
		<author>
			<persName><forename type="first">I</forename><surname>Drori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Verma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.08171</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cyclomatic complexity</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Antoniol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Counsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Laplante</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE software</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="27" to="29" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
		<ptr target="https://github.com/Copilot-Eval-Replication-Package/CopilotEvaluation" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Replication package</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08155</idno>
		<title level="m">A pre-trained model for programming and natural languages</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The space of developer productivity: There&apos;s more to it than you think</title>
		<author>
			<persName><forename type="first">N</forename><surname>Forsgren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-A</forename><surname>Storey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Maddila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Houck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Butler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Queue</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20" to="48" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Geeksforgeeks</forename><surname>Team</surname></persName>
		</author>
		<author>
			<persName><surname>Geeksforgeeks</surname></persName>
		</author>
		<ptr target="https://www.geeksforgeeks.org" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dimensions in program synthesis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gulwani</surname></persName>
		</author>
		<idno type="DOI">10.1145/1836089.1836091</idno>
		<ptr target="https://doi.org/10.1145/1836089.1836091" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International ACM SIGPLAN Symposium on Principles and Practice of Declarative Programming, PPDP &apos;10</title>
		<meeting>the 12th International ACM SIGPLAN Symposium on Principles and Practice of Declarative Programming, PPDP &apos;10<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="13" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Automated clustering and program repair for introductory programming assignments</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gulwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Radi?ek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zuleger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGPLAN Notices</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="465" to="480" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning formal grammars to translate natural language specifications into hardware assertions</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">G</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><surname>Glast</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 Design, Automation &amp; Test in Europe Conference &amp; Exhibition (DATE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="966" to="971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Re-factoring based program repair applied to programming assignments</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">Z</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mechtaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Leong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roychoudhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="388" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Competition-level code generation with alphacode</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kushman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Leblond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Eccles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Keeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gimeno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Lago</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.07814</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A deductive approach to program synthesis</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Manna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Waldinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Programming Languages and Systems (TOPLAS)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="90" to="121" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Nlp (natural language processing) for nlp (natural language programming)</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lieberman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on intelligent text processing and computational linguistics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="319" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The potential of artificial intelligence as a method of software developer&apos;s productivity improvement</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Moroz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">O</forename><surname>Grizkevich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M</forename><surname>Novozhilov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 Conference of Russian Young Researchers in Electrical and Electronic Engineering (El-ConRus)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="386" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An empirical evaluation of GitHub Copilot&apos;s code suggestions</title>
		<author>
			<persName><forename type="first">N</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Accepted for publication Proceedings of the 19th ACM International Conference on Mining Software Repositories (MSR)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Asleep at the keyboard? assessing the security of github copilot&apos;s code contributions</title>
		<author>
			<persName><forename type="first">H</forename><surname>Pearce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dolan-Gavitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Karri</surname></persName>
		</author>
		<idno type="DOI">10.1109/SP46214.2022.00057</idno>
		<ptr target="https://doi.ieeecomputersociety.org/10.1109/SP46214.2022.00057" />
	</analytic>
	<monogr>
		<title level="m">2022 2022 IEEE Symposium on Security and Privacy (SP) (SP)</title>
		<meeting><address><addrLine>Los Alamitos, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-05">may 2022</date>
			<biblScope unit="page" from="980" to="994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Machine translation from natural language to code using long-short term memory</title>
		<author>
			<persName><forename type="first">K</forename><surname>Rahit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Nabil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Huq</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Future Technologies Conference</title>
		<meeting>the Future Technologies Conference</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="56" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sundaresan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blanco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.10297</idno>
		<title level="m">Codebleu: a method for automatic evaluation of code synthesis</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Comparing python programs using abstract syntax trees</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Salazar</forename><surname>Paredes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Uniandes</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Cyclomatic complexity: The nesting problem</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M S</forename><surname>Sarwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shahzad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ahmad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth International Conference on Digital Information Management (ICDIM 2013)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="274" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Choose your programming copilot: A comparison of the program synthesis performance of github copilot and genetic programming</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sobania</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Briesch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Rothlauf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.07875</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Recent developments in program synthesis with evolutionary algorithms</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sobania</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schweim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Rothlauf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.12227</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Solving probability and statistics problems by program synthesis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Drori</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.08267</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Expectation vs. experience: Evaluating the usability of code generation tools powered by large language models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Vaithilingam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Glassman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI Conference on Human Factors in Computing Systems Extended Abstracts</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<ptr target="https://www.w3schools.com" />
		<title level="m">W3schools Team. W3schools</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kalliamvakou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Simister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sittampalam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rifkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Aftandilian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.06537</idno>
		<title level="m">Productivity assessment of neural code completion</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
