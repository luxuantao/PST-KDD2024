<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PatchGT: Transformer over Non-trainable Clusters for Learning Graph Representations</title>
				<funder>
					<orgName type="full">NSF1908617</orgName>
				</funder>
				<funder ref="#_X5wbree">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_JaJy3EN #_rUGH8uR">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-11-26">26 Nov 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Han</forename><surname>Gao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
							<email>xu.han@tufts.edu</email>
						</author>
						<author>
							<persName><forename type="first">Jiaoyang</forename><surname>Huang</surname></persName>
							<email>huangjy@wharton.upenn.edu</email>
						</author>
						<author>
							<persName><forename type="first">Jian-Xun</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Li-Ping</forename><surname>Liu</surname></persName>
							<email>liping.liu@tufts.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Notre Dame</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Tufts University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of Notre Dame</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Tufts University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">PatchGT: Transformer over Non-trainable Clusters for Learning Graph Representations</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-11-26">26 Nov 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2211.14425v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently the Transformer structure has shown good performances in graph learning tasks. However, these Transformer models directly work on graph nodes and may have difficulties learning high-level information. Inspired by the vision transformer, which applies to image patches, we propose a new Transformerbased graph neural network: Patch Graph Transformer (PatchGT). Unlike previous transformer-based models for learning graph representations, PatchGT learns from non-trainable graph patches, not from nodes directly. It can help save computation and improve the model performance. The key idea is to segment a graph into patches based on spectral clustering without any trainable parameters, with which the model can first use GNN layers to learn patch-level representations and then use Transformer to obtain graph-level representations. The architecture leverages the spectral information of graphs and combines the strengths of GNNs and Transformers. Further, we show the limitations of previous hierarchical trainable clusters theoretically and empirically. We also prove the proposed non-trainable spectral clustering method is permutation invariant and can help address the information bottlenecks in the graph. PatchGT achieves higher expressiveness than 1-WL-type GNNs, and the empirical study shows that PatchGT achieves competitive performances on benchmark datasets and provides interpretability to its predictions. The implementation of our algorithm is released at our Github repo: https://github.com/tufts-ml/PatchGT.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Learning from graph data is ubiquitous in applications such as drug design <ref type="bibr" target="#b14">[15]</ref> and social network analysis <ref type="bibr" target="#b36">[37]</ref>. The success of a graph learning task hinges on effective extraction of information from graph structures, which often contain combinatorial structures and are highly complex. Early works <ref type="bibr" target="#b6">[7]</ref> often need to manually extract features from graphs before applying learning models. In the era of deep learning, Graph Neural Networks (GNNs) <ref type="bibr" target="#b34">[35]</ref> are developed to automatically extract information from graphs. Through passing learnable messages between nodes, they are able to encode graph information into vector representations of graph nodes. GNNs have become the standard tool for learning tasks on graph data.</p><p>While they have achieved good performances in a wide range of tasks, GNNs still have a few limitations. For example, GNNs <ref type="bibr" target="#b35">[36]</ref> suffer from issues such as inadequate expressiveness <ref type="bibr" target="#b35">[36]</ref>, oversmoothing <ref type="bibr" target="#b27">[28]</ref>, and over-squashing <ref type="bibr" target="#b1">[2]</ref>. These issues have been partially addressed by techniques such as improving message-passing functions and expanding node features <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b20">21]</ref>.</p><p>Another important progress is to replace the message-passing network with the Transformer architecture <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b37">38]</ref>. These models treat graph nodes as tokens and apply the Transformer architecture to nodes directly. The main focus of these models is how to encode node information and how to incorporate adjacency matrices into network calculations. Without the message-passing structure, these models may overcome some associated issues and have shown premium performances in various graph learning tasks. However, these models suffer from computation complexity because of the global attention on all nodes. It is hard to capture the topological information of graphs.</p><p>As a comparison, the Transformer for image data works on image patches instead of pixels <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22]</ref>. While this model choice is justified by reduction of computation cost, recent work <ref type="bibr" target="#b30">[31]</ref> shows that "patch representation itself may be a critical component to the 'superior' performance of newer architectures like Vision Transformers". One intriguing question is whether patch representation can also improve learning models on graphs. With this question, we consider patches on graphs. Patches over graphs are justified by a "mid-level" understanding of graphs: for example, a molecule graph's property is often decided by some function groups, each of which is a subgraph formed by locally-connected atoms. Therefore, patch representations are able to capture such mid-level concepts and bridge the gap between low-level structures to high-level semantics.</p><p>Motivated by our question, we propose a new framework, Patch Graph Transformer (PatchGT). It first segments a graph into patches based on spectral clustering, which is a non-trainable segmentation method, then applies GNN layers to learn patch representations, and finally uses Transformer layers to learn a graph-level representation from patch representations. This framework combines the strengths of two types of learning architectures: GNN layers can extract information with message passing, while Transformer layers can aggregate information using the attention mechanism. To our best knowledge, we firstly show several limitations of previous trainable clustering method based on GNN. We also show that the proposed non-trainable clustering can provide more reasonable patches and help overcoming information bottleneck in graphs.</p><p>We justify our model architecture with theoretical analysis. We show that our patch structure derived from spectral clustering is superior to patch structures learned by GNNs <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b38">39]</ref>. We also propose a new mathematical description of the information bottleneck in vanilla GNNs and further show that our architecture has the ability of mitigating this issue when graphs have small graph cuts. The contributions of this paper are summarized as follows.</p><p>-We develop a general framework to overcome the information bottleneck in traditional GNNs by applying a Transformer on graph patches in Section 3. The graph patches are from an unlearnable spectral clustering process. -We prove several new theorems for the limitations of previous pooling methods from the 1-WL algorithm in Theorem 1 and Theorem 2. And we theoretically prove that PatchGT is strictly beyond 1-WL and hence has better expressiveness in Theorem 3. Also, in Section 4. <ref type="bibr" target="#b3">4</ref>, we show that the segmentations from hierarchical learnable clustering methods may aggregate disconnected nodes, which will definitely hurt the performance of the transformer model. -We demonstrate the existence of information bottleneck in GNNs in Section 4.3. When a graph consists of loosely-connected clusters, we make the first attempt to characterize such information bottleneck. And it indicates when there is a small graph cut between two clusters, the GNNs need to use more layers to pass signals from one group to another. And we further demonstrate with direct attention between groups, PatchGT could overcome such limitations.</p><p>We run an extensive empirical study and demonstrate that the proposed model outperforms competing methods on a list of graph learning tasks. The ablation study shows that our PatchGT is able to combine the strengths of GNN layers and Transformer layers. The attention weights in Transformer layers also provide explanations for model predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Transformer models have gained remarkable successes in NLP applications <ref type="bibr" target="#b15">[16]</ref>. Recently, they have also been introduced to vision tasks <ref type="bibr" target="#b8">[9]</ref> and graph tasks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b39">40]</ref>. These models all treat nodes as tokens. Particularly, Memory-based graph networks <ref type="bibr" target="#b0">[1]</ref> apply a hierarchical attention pooling methods on the nodes. GraphTrans <ref type="bibr" target="#b33">[34]</ref> directly applies a GNN on all nodes, followed by a transformer. Therefore, they are hard to be applied to large graphs because of huge computation complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input graph</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Laplacian matrix n</head><p>Eigen decomposition At the same time, image patches have been shown to be useful for Transformer models on image data <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b30">31]</ref>, so it is not surprising if graph patches are also helpful to Transformer models on graph data. Graph multiset pooling <ref type="bibr" target="#b2">[3]</ref> applies trainable pooling methods on the nodes based on GNN. And then adopt a global attention layer on learned clusters. We will show that such trainable clustering has several limitations for attention mechanism in this work.</p><p>Hierarchical pooling models <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b38">39]</ref> are relevant to our work in that they also aggregate information from node representations in middle layers of networks. However, these methods all form their pooling structures based on representations learned from GNNs. As a result, these pooling structures inherit drawbacks from GNNs <ref type="bibr" target="#b35">[36]</ref>. They may also aggregate nodes that are far apart on the graph and thus cannot preserve the global structure of the input graph. Also such trainable clustering methods need much computation for training. Furthermore, our main purpose is to use non-trainable patches on graphs as tokens for a Transformer model, which is different from these models.</p><p>3 Patch Graph Transformer</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Background</head><p>In this work, we consider graph-level learning problems. Let G = (V, E) denote a graph with node set V and edge set E. Let A denote its adjacency matrix. The graph has both node features</p><formula xml:id="formula_0">X = (x i ? R d : i ? V ) and edge features E = (e i,j ? R d : (i, j) ? E).</formula><p>Let y denote the label of graph. This work aims to learn a model that maps (A, X, E) to a vector representation g, which is then used to predict the graph label y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GNN layers.</head><p>A GNN uses node vectors to represent structural information of the graph. It consists of multiple GNN layers. Each GNN layer passes learnable messages and updates node vectors. Suppose H = (h i ? R d : i ? V ) are node vectors, a typical GNN layer updates H as follows.</p><formula xml:id="formula_1">h i = ?(W 1 h i + j:(i,j)?E W 2 h j + W 3 e i,j )<label>(1)</label></formula><p>Here matrices (W 1 , W 2 , W 3 ) are all learnable parameters; and ? is the activation function. We denote the layer function by H = GNN(A, E, H). If there are no edge features, then the calculation can be written in matrix form.</p><formula xml:id="formula_2">H = ?(HW 1 + AHW 2 )<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model design</head><p>PatchGT has three components: segmenting the input graph into patches, learning patch representations, and aggregating patch representations into a single graph vector. The overall architecture is shown in Figure <ref type="figure" target="#fig_0">1</ref>. The second and third steps are in an end-to-end learning model. Graph segmentation is outside of the learning model, which will be justified by our theoretical analysis later.</p><p>Forming patches over the graph. We first discuss how to form patches on a graph. One consideration is to include an informative subgraph (e.g., a function group, a motif) into a single patch instead of segmenting it into pieces. A reasonable approach is to run node clustering on the input graph and treat each cluster as a graph patch. If a meaningful subgraph is densely connected, it has a good chance of being contained in a single cluster.</p><p>In this work, we consider spectral clustering <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b40">41]</ref> for graph segmentation. Let L = I -D -1/2 AD -1/2 be the normalized Laplacian matrix of G, and its eigen-decomposition is L = U?U , where the eigen-values ? = diag(? 1 , . . . , ? |V | ) is sorted in the ascending order. By thresholding eigen-values with a small threshold ?, we get k = arg max k ? k ? ? eigen-vectors U 1:k , then we run k-means to get k clusters (denoted by P) of graph nodes. Here P = {C k ? V : k = 1, . . . , k} with each C k representing a cluster/patch. Note that the threshold ? is a hyper-parameter, and k varies depending on the underlying graph's topology.</p><p>Computing patch representations. When we learn representations of patches in P, we consider both node connections within the patch and also connections between patches. Patches form a coarse graph, which is also referred as a patch-level graph, by treating patches as nodes and their connections as edges. We first learn node representations using GNN layers. Let H 0 = X denote the initial representations of all nodes. Then we apply L 1 GNN layers to get node representations H L1 .</p><formula xml:id="formula_3">H = GNN(A, E, H -1 ), = 1, . . . , L 1<label>(3)</label></formula><p>Here for easier discussion, we apply GNN layers to the entire graph. We have also tried to apply GNN layers within each patch only and found that the performance is similar.</p><p>Then we read out the initial patch representation by summarizing representations of nodes within this patch. Let z 0 k denote the initial patch representation, then</p><formula xml:id="formula_4">z 0 k = |C k | |V | ? readout(h L1 i : i ? C k ), k = 1, . . . , k<label>(4)</label></formula><p>Here h L1 i is node i's representation in H L1 . We collectively denote these patch representations in a matrix Z 0 = (z 0 k : k = 1, . . . , k). The readout function readout(?) is a function aggregating information from a set of vectors. Our implementation uses the max pooling. We use the factor</p><formula xml:id="formula_5">|C k | |V |</formula><p>to assign proper weights to patch representations.</p><p>To further refine patch representations and encode structural information of the entire graph, we apply further GNN layers to the patch-level formed by patches. We first compute the adjacency matrix ? of the patch-level graph. If we convert the partition P to an assignment matrix S = (S i,k : i ?</p><formula xml:id="formula_6">V, k = 1, . . . k) such that S i,k = 1[i ? C k ], then the adjacency matrix over patches is ? = 1 (S AS) &gt; 0 .<label>(5)</label></formula><p>Note that ? only has connections between patches and does not maintain connection strength.</p><p>We then compute use L 2 GNN layers to refine patch representations.</p><formula xml:id="formula_7">Z = GNN( ?, 0, Z -1 ), = 1, . . . , L 2<label>(6)</label></formula><p>GNN layers here do not have edge features. From the last layer, we get patch representations in Z L2</p><p>Graph representation via Transformer layers. Then we use L 3 Transformer layers to extract the representation of the entire graph. Here we use a learnable query vector q 0 to "retrieve" the global representation g of the graph from patch representations Z L2 .</p><formula xml:id="formula_8">q = MHA (q -1 , Z L2 , Z L2 ) , = 1, . . . , L 3 (7) q = MLP(q ) + q -1 , = 1, . . . , L 3 (8) g = LN(q L3 )<label>(9)</label></formula><p>Here MHA(?, ?, ?) is the function of a multi-head attention layer (please refer to Chp. 10 of <ref type="bibr" target="#b41">[42]</ref>). Its three arguments are the query, key, and value. The two functions MLP(?) and LN(?) are respectively a multi-layer perceptron and a linear layer. Note that patch representations Z L2 are carried through without being updated. Only the query token is updated to query information from patch representations. The final learned graph representation is g, from which we can perform various graph level tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Theoretical Analysis</head><p>In this section, we study the theoretical properties of the proposed model. To save space, we put all proofs in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Enhancing model expressiveness with patches</head><p>On purpose we form graph patches using a clustering method that is not part of the neural network. An alternative consideration is to learn such cluster assignments with GNNs (e.g. DiffPool <ref type="bibr" target="#b38">[39]</ref> and MinCutPool <ref type="bibr" target="#b3">[4]</ref>. However, cluster assignment learned by GNNs inherits the limitation of GNNs and hinders the expessiveness of the entire model. Theorem 1. Suppose two graphs receive the same coloring by 1-WL algorithm, then DiffPool will compute the same vector representation for them.</p><p>Although DiffPool and MinCutPool claims to cluster "similar" graph nodes into clusters during pooling, but these nodes may not be connected. Because of the limitation of GNNs, they may aggregate nodes that are far apart in the graph. For example, nodes in the same orbit always get the same color by the 1-WL algorithm and also the same representations from a GNN, then these nodes always have the same cluster assignment. Merging these nodes into the same cluster does not seem capture the high-level structure of a graph.</p><p>Another prominent pooling method is the Graph U-Net <ref type="bibr" target="#b11">[12]</ref>, which has similar issues. We briefly introduce its calculation here. Suppose the layer input is (A, H), the model's pooling layer projects H with a unit vector p and gets values v = Hp for all nodes, then it chooses the top k nodes that have largest values in v and keep their representations only. We will show that this approach is NOT invariant to node orders.</p><p>We also consider a small variant of Graph U-Net for analysis convenience. Instead of choosing k nodes with top values in v, the variant uses a threshold ? (either learnable or a hyper-parameter) to choose nodes: b = v ? ?. Then the output of the layer is</p><formula xml:id="formula_9">(A[b, b], H[b]</formula><p>). We call the model with the variant with thresholding as Graph U-Net-th. We show that the variant of Graph U-Net-th is also bounded by the 1-WL algorithm. Theorem 2. Suppose two graphs receive the same coloring by 1-WL algorithm, then Graph U-Net-th will compute the same vector representation for them.</p><p>The two theorems strongly indicate that pooling structures learned by GNNs have the same drawback.</p><p>We provide detailed analysis for Graph U-Net in Appendix A.3.</p><p>In contrast, a small variant of PatchGT is more expressive than the 1-WL algorithm. Figure <ref type="figure" target="#fig_7">7</ref> in Appendix shows two graph pairs that can be distinguished by PatchGT but not the 1-WL algorithm.</p><p>In this PatchGT variant, we only need to choose the summation operation to aggregate node representations in the same patch and multiply a scalar to the MHA output. We put the result in the following Theorem. Theorem 3. Suppose a PatchGT uses GIN layers, uses sum-pooling as the readout function in Equation (4), z 0 k = i?C k h L1 i , and multiplies the MHA output in Equation <ref type="bibr" target="#b6">(7)</ref> with the number k of patches, q = k ? MHA (q -1 , Z L2 , Z L2 ). Let g 1 and g 2 be outputs computed from two graphs G 1 and G 2 by a PatchGT model. There exists a PatchGT such that g 1 = g 2 if G 1 and G 2 can be distinguished by the 1-WL algorithm. Furthermore, there are graph pairs G 1 = G 2 that cannot be distinguished by the 1-WL algorithm, but g 1 = g 2 from this PatchGT model.</p><p>The first part of the conclusion is true because the patch aggregation, patch-level GNN, and the MHA pooling can all be bijective mapping. According to Corollary 6 of <ref type="bibr" target="#b35">[36]</ref>, the outputs of GIN layers have the same expressive power as the 1-WL algorithm. Such expressive power is maintained in the model output. However, when GIN layers on patches use extra structural information on patches, the model can distinguish graphs that cannot be distinguished by the 1-WL algorithm. We put the formal proof in Appendix A.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Permutation invariance</head><p>Our model depends on the patch structure formed by the clustering algorithm, which further depends on the spectral decomposition of the normalized Laplacian. Note that the spectral decomposition is not unique, but we show that the clustering result is not affected by sign variant and multiplicities associated with decomposition, and our model is still invariant to node permutations. Theorem 4. The network function of PatchGT is invariant to node permutations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Addressing information bottleneck with patch representations</head><p>Alon et al. <ref type="bibr" target="#b1">[2]</ref> recently characterize the issue of information bottleneck in GNNs through empirical methods. Here we consider this issue on a special case when a graph consists of loosely-connected node clusters. Note that molecule graphs often have this property. Here we make the first attempt to characterize the information bottleneck through theoretical analysis. We further show that our PatchGT can partially address this issue.</p><p>For convenient analysis, we consider a regular graph with degree ? . Suppose the node set V of G forms two clusters S and T : V = S ? T, S ? T = ?, and there are only m edges between S and T .</p><p>We consider the difficulty of passing signal from S to T . Let f GNN (?) denote the network function of a GNN of L layers with ReLU activation ? as in (2), and input</p><formula xml:id="formula_10">X = (x i ? R d : i ? V ) ? R |V |?d , which contains d-dimensional feature inputs to nodes in G. Let f GNN i (?)</formula><p>be the output at node i. We can ask this question: if we perturb the input to nodes in S, how much impact we can observe at the output at nodes in T . We need to avoid the case that the impact is amplified by scaling up network parameters. In real applications, scaling up network parameters also amplifies signals within T itself, and the signal from S still cannot be well received. Here we consider relative impact: the ratio between the impact on T from S over that from T itself.</p><p>Let ? ? R |V |?d be some perturbation on S such that ? ij ? if i ? S and ? ij = 0 otherwise. Here is the scale of the perturbation. Similarly let ? ? R |V |?d be some perturbation on T :</p><formula xml:id="formula_11">? ij ? if i ? T and ? ij = 0 otherwise. Then the impacts on node representations f GNN i , i ? T from ? and ? are respectively ? S?T = max ? i?T f GNN i (X + ?) -f GNN i (X) 1 (<label>10</label></formula><formula xml:id="formula_12">)</formula><formula xml:id="formula_13">? T ?T = max ? i?T f GNN i (X + ?) -f GNN i (X) 1 (<label>11</label></formula><formula xml:id="formula_14">)</formula><p>where the maximum is also over all possible learnable parameters W 1 L1?L1 , W 2 L1?L1 ? 1 as in <ref type="bibr" target="#b1">(2)</ref>. Then we have the following proposition to bound the ratio ? S?T /? T ?T . Proposition 1. Given a ? -regular graph G, a node subset S with its complement T such that there are only m edges between S and T , and a L-layer GNN, it holds that</p><formula xml:id="formula_15">? S?T ? T ?T ? 2mL |T |<label>(12)</label></formula><p>The proposition indicates that when there is a small graph cut between two clusters, then it forms an information bottleneck in a GNN -the network needs to use more layers to pass signal from one Molecules Ours Trainable Molecules Ours Trainable group to another. The bound is still conservative: if the signal is extracted in middle layers of the network, then passing the signal is even harder. The proposition is illustrated in Figure <ref type="figure">3</ref>.</p><p>In our PatchGT model, communication can happen at the coarse graph and thus can partially address this issue. The coarse graph ? consists of two nodes (we still denote them by S, T ), and there is an edge between S and T . From the output f GNN , we construct the patch representations</p><formula xml:id="formula_16">(z S , z T ) = ( 1 |V | i?S f GNN i (X), 1 |V | i?T f GNN i (X)) ? R 2?d .</formula><p>Then we apply a GNN layer to get node represents on the coarse graph (g</p><formula xml:id="formula_17">GNN S (X), g GNN T (X)) ? R 2?d : g GNN S (X) = ?(z S W 1 + z T W 2 ), g GNN T (X) = ?(z T W 1 + z S W 2 ),<label>(13)</label></formula><p>where W 1 , W 2 ? R d?d are learnable parameters. We consider the impact of ? on our patch GT, let</p><formula xml:id="formula_18">? S?T = max ? g GNN T (X + ?) -g GNN T (X) 1<label>(14)</label></formula><formula xml:id="formula_19">? T ?T = max ? g GNN T (X + ?) -g GNN T (X) 1 ,<label>(15)</label></formula><p>Then we have the following proposition on the ratio ? S?T /? T ?T . Theorem 5. The ratio ? S?T ? T ?T can be arbitrarily close to 1 in a PatchGT model, under the assumption of regular graphs. This is because S and T are direct neighbors in the coarse graph, then ? S can directly impact z S , which can impact g GNN T through messages passed by GNN layers or the attention mechanism of Transformer layers. The right part of fig. <ref type="figure">3</ref> shows that patch representation can include signals from the other node cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison for different Segmentation methods</head><p>In the previous researches, there exist many hierarchical pooling models <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b38">39]</ref>. The most obvious difference from the proposed method is that the pooling/segmentation is trainable. Particularly, the pooling is from the node respresentations learned by GNNs. In the Theorem 1 and Theorem 2, we prove such trainable clustering methods will compute the same representations to the nodes if 1-WL algorithm can not differentiate them. This takes two serious problems for the graph segmentation: First, the nodes with the same representations will be assigned to the same cluster even if they are not connected to each other; Second, too many nodes could be assigned to one cluster to make sure that the nodes far away from each other are in the same cluster.</p><p>Here we compare the two segmentation results: one is from spectral clustering and another is from Memory-based graph networks <ref type="bibr" target="#b0">[1]</ref> which is a typical trainable clustering method. In the first case, we find that nodes in the blue cluster from trainable clustering are not connected. If we adopt such patch representations by aggregating the disconnected nodes, it will definitely hurt the performance. This can also be applied to other hierarchical pooling methods such as Diffpool, Eigenpool, and MinCutpool.</p><p>In the second case, the spectral clustering methods segment the graph by minimum cuts. This is helpful to solve the information bottleneck between patches. However, the Memory-based graph networks cluster the two benzene rings together. It will be difficult for the model to detect the existence of these two benzene rings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Empirical Study</head><p>In this section, we evaluate the effectiveness of PatchGT through experiments.</p><p>Datasets. We benchmark the performances of PatchGT on several commonly studied graph-level prediction datasets. The first four are from the Open Graph Benchmark (OGB) datasets <ref type="bibr" target="#b13">[14]</ref> (ogbgmolhiv, ogbg-molbace, ogbg-molclintox, and ogbg-molsider). These tasks are predicting molecular attributes. The evaluation metric for these four datasets is ROC-AUC (%). The second group of six datasets are from the TU datasets <ref type="bibr" target="#b24">[25]</ref>, and they are DD, MUTAG, PROTEINS, PTC-MR, ENZYMES, and Mutagenicity. Each dataset contains one classification task for molecules. The evaluation metric is accuracy (%) over all six datasets. The statistics for the datasets is summarized in Appendix A.11. Baselines. In this section, we compare the performance of PatchGT against several baselines including GCN <ref type="bibr" target="#b16">[17]</ref>, GIN <ref type="bibr" target="#b35">[36]</ref>, as well as recent works Nested Graph Neural Networks <ref type="bibr" target="#b43">[44]</ref> and GraphSNN <ref type="bibr" target="#b32">[33]</ref>. To compare with learnable pooling methods, we also include DiffPool <ref type="bibr" target="#b38">[39]</ref>, MinCutPool <ref type="bibr" target="#b3">[4]</ref> Graph U-Nets <ref type="bibr" target="#b11">[12]</ref>, and EigenGCN <ref type="bibr" target="#b22">[23]</ref> as baselines for TU datasets. We also include the Graphormer model, but note that Graphormer needs a large-scale pre-training and cannot be easily applied to a wider range of datasets. We also compare our model with other transformerbased models such as U2GNN <ref type="bibr" target="#b25">[26]</ref> and SEG-BERT <ref type="bibr" target="#b42">[43]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Quantitative evaluation</head><p>Settings. We search model hyper-parameters such as the eigenvalue threshold, the learning rate, and the number of graph neural network layers on the validation set. Each OGB dataset has its own data split of training, validation, and test sets. We run ten fold cross-validation on each TU dataset. In each fold, one-tenth of the data is used as the test set, one-tenth is used as the validation set, and the rest is used as training. For the detailed search space, please refer to Appendix A.12.   Results. Table <ref type="table" target="#tab_0">1</ref> and Table <ref type="table" target="#tab_1">2</ref> summarize the performance of PatchGT and other baselines on OGB datasets and TU datasets. We take values from the original papers and the OGB website; EXCEPT the performance values of Nested GIN on the last three OGB datasets -we obtain the three values by running Nested GIN. We also tried to run the contemporary method GRAPHSNN+VN on the other three OGB datasets, but we did not find the official implementation at the submission of this work.</p><p>From the results, we see that the proposed method gets good performances on almost all datasets and often outperforms competing methods with a large margin. On the ogbg-molhiv dataset, the performance of PatchGT with GCN is only slightly worse than Graphormer, but note that Graphormer needs large-scale pre-training, which limits its applications.</p><p>PatchGT with GCN outperforms three baselines on the other three OGB datasets. The improvements on these three OGB datasets are significant. PatchGT with GCN outperforms baselines on four out of six TU datasets. When it does not outperform all baselines, its performances are only slightly worse than the best performance. Similarly, two other configurations, PatchGT-GIN and PatchGT-DeeperGCN, also perform very well on these two datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ablation study</head><p>We perform ablation studies to check how different configurations of our model affect its performance. The results are shown in Figure <ref type="figure" target="#fig_4">5</ref>.</p><p>Effect of eigenvalue threshold. The eigenvalue threshold ? influences how many patches for a graph after the segmentation. Generally speaking, larger ? introduces more patches and patches with smaller sizes. When ? is large enough, the number of patches k equals the number of nodes |V | in the graph, and the Transformer actually works at the node level. When the ? is 0, then the whole graph is treated as one patch, and the model is reduced to a GNN with pooling. The left figure shows that there is a sweet point (depending on the dataset) for the threshold, which means that using patches is a better choice than not using patches.</p><p>Effect of GNN layer on the coarse graph and Transformer layers. This ablation study removes either patch-level GNN layers or Transformer layers to check which part of the architecture is important for the model performance. From the middle plot in Figure <ref type="figure" target="#fig_4">5</ref>, we see that both types of layers are useful, and Transformer layers are more useful. This is another piece of evidence that PatchGT can combine the strengths of different models.</p><p>Comparison of readout functions. We compare the performance of PatchGT model using different readout functions when aggregating node representations at each patch in Equation ( <ref type="formula" target="#formula_4">4</ref>). In the right figure, we observe the remarkable influence of the readout function on the performance. Empirical studies indicate max-pooling is the optimal choice under most circumstances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Understanding the attention</head><p>Besides improving learning performances, we are also interested in understanding how the attention mechanism helps the model identify the graph property. We train the PatchGT model on the ogbgmolhiv dataset and visualize the attention weights between query tokens and each patch. Interestingly, the attention only concentrates on some chemical motifs such as Cl O 3 and CON 2 but ignores other very common motifs such as benzene rings. It can be noticed that for the molecule in the first figure, the two benzene rings are connected to each other by -C-C-. However, the model does not pay any attention to this part. The two rings in the molecule of the second molecule are connected by -S-S-; differently, the model pays attention to this part this time. It indicates that Transformer can identify which motifs are informative and which motifs are common. Such property offers better model interpretability compared to the traditional global pooling. It not only makes accurate predictions but also provides some insight into why decisions are made. In the two examples shown above, we can start from motifs SO 3 and -S-S-to look for structures meaningful for the classification problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Limitations</head><p>In this work, we show that graph learning models benefit from modeling patches on graphs, particularly when it is combined with Transformer layers. We propose PatchGT, a new learning model that uses non-trainable clustering to get graph patches and learn graph representations based on patch representations. It combines the strengths of GNN layers and Transformer layers and we theoretically prove that it helps mitigate the bottleneck of graphs and limitations of trainable clustering. It shows superior performances on a list of graph learning tasks. Based on graph patches, Transformer layers also provides a good level of interpretability of model predictions.</p><p>However, the work tested our model mostly on chemical datasets. It is unclear whether the model still performs well when input graphs do not have clear cluster structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Proof of Theorem 1</head><p>The proof that DiffPooling cannot distinguish graphs that are colored in the same way by the 1-WL algorithm.</p><p>Proof. The function form of a pooling layer in DiffPooling is</p><formula xml:id="formula_20">H = S ASS H, S = gnn c (A, X), H = gnn r (A, X)<label>(16)</label></formula><p>Here gnn c (?, ?) learns a cluster assignment S of all nodes in the graph, and gnn r (?, ?) learns node representations.</p><p>Note that gnn r has at most the ability of 1-WL algorithm <ref type="bibr" target="#b35">[36]</ref>. Two nodes must get the same representation when they have the same color in the 1-WL coloring result. We use an indicator matrix C to represent the 1-WL coloring of the graph, that is, the node i is colored as j if C i,j = 1, then we can write</p><formula xml:id="formula_21">S = CB<label>(17)</label></formula><p>Here the j-th row of B denote the vector representation learned for color j.</p><p>If two graphs represented by A and ? cannot be distinguished by the 1-WL algorithm, then they get the same coloring matrix C (subject to some node permutation that does not affect our analysis here). Now we show that:</p><formula xml:id="formula_22">C AC = C ?C<label>(18)</label></formula><p>Let's compare the two matrices on both sides of the equation at an arbitrary entry (k, t). Let ? k and ? t represent nodes colored in k and t, then the entry at (k, t) is i?? k j??t A i,j , which is the count of edges that have one incident node colored in k and the other incident node colored in t. Since the coloring is obtained by 1-WL algorithm, each node i ? ? k has exactly the same number of neighbors colored as t. The number of nodes in color k and the number of neighbors in color t are exactly the same for ? because ? receives the same coloring as A. Therefore,</p><formula xml:id="formula_23">i?? k j??t A i,j = i?? k j??t ? i,j</formula><p>, and (18) holds. At the same time, if two graphs cannot be distinguished by 1-WL, they have the same node representations H, then they have the same H .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Proof of Theorem 2</head><p>We first prove a lemma. Lemma 1. Suppose two graphs represented by A and ? obtain the same coloring from the 1-WL algorithm, then i) the resultant two graphs from removal of nodes in the same color still get the same coloring by the 1-WL algorithm; and</p><p>ii) the two multigraphs represented by A and ? still get the same coloring by the 1-WL algorithm.</p><p>Here A and ? are the -th power of the two adjacency matrices, and they represent multigraphs that may have self-loops and parallel edges. The 1-WL algorithm is still valid over graphs with self-loops and multi-edges. A 1-WL style GNN defined in Section 3.1 or <ref type="bibr" target="#b11">[12]</ref> is still bounded by the 1-WL algorithm on such multigraphs.</p><p>Proof. i) We first consider updating of 1-WL coloring when nodes in a color is removed. Suppose we have stable coloring of graphs represented by A. Let ? t and ? r denote two groups of nodes in color t and r respectively. We also assume each node in r has t in its color set -if there are not such cases, then we can simply remove nodes in a color and obtain a stable 1-WL coloring.</p><p>Suppose we remove nodes in color t from both graphs. Note that all nodes ? r have the same number of neighbors in color t. We update the color set of each i ? ? r by removing color t from it. Then all nodes in ? r still get the same color. Therefore, removing the color t from nodes in all relevant color groups gives at least a stable coloring, which, however, might not be the coarsest.</p><p>Then we merge some colors when nodes share the same color set. If a node in color r has the same color set as a node in color r , then we assign the same color to both nodes in colors r and r . We run merging steps until no nodes in different colors share the same color set, then the coloring is a stable coloring of the graph, and the resultant coloring of the graph can be viewed as the 1-WL coloring of the graph.</p><p>In the procedure above, the step of removing a color, and the steps of merging colors directly operate on nodes' color sets. Since nodes in A and nodes in ? have the same color sets, therefore, they will have the same color sets after color updates.</p><p>The update procedure above purely runs on color relations between different colors. Since A and ? have exactly the same color relations because they receive the same 1-WL coloring. Therefore, the update procedure above still gives the same stable coloring to A and ?.</p><p>ii) For the second part of the lemma, we first check the coloring of A . We show that the coloring of A is a stable coloring of A . Suppose each node i has a color set C i . In the graph A , i's -th neighbors become direct neighbors of i. The color set of i becomes</p><formula xml:id="formula_24">C i ? ? j1?N (i) C j ? . . . ? ? j1?N (i) . . . ? j ?N (j -1 ) C j<label>(19)</label></formula><p>We know that if two nodes i and i have the same color if and only if their color sets are the same. By using the relation recursively, i and i have the same color set in A . Therefore, the stable coloring of A is also a stable coloring of A . If necessary, we can also run the merging procedure above and eventually get 1-WL coloring of A . With the same argument as above, the operations only run on color sets, therefore, A and ? have the same coloring. Now we are ready to prove the main theorem that the Graph U-Net variant cannot distinguish graphs colored in the same way by the 1-WL algorithm.</p><p>Proof. In the calculation of Graph U-Net-th, the indicator b for removing nodes is obtained by thresholding v, which is computed by a 1-WL GNN. Therefore, nodes in the same color are always kept or removed all together in b. By using the argument above recursively, the network cannot distinguish the graph at the final outputs if network inputs (A, X) and (?, X) cannot be distinguished by the 1-WL algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Suppose the inputs to a</head><p>Remark 1. For graphs with noise or low homophily ratios, the aforementioned issue may not be severe and long-distance aggregation is helpful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Analysis for expressiveness of Graph U-Nets</head><p>In this section we use an example in Fig. <ref type="figure" target="#fig_7">7</ref> to understand how to maintain a graph's global structure with pooling operations. In a pooling step, DiffPool and MinCutPool will assign nodes in the same color to the same cluster and merge them as one node. Clearly it does not maintain the global structure of the graph and cannot distinguish the two graphs.</p><p>Graph U-Net always ranks nodes in one color above nodes of the other color. It is not always permutation invariant: for example, it may get different structures when it breaks tie to take two green nodes. In many cases, it cannot distinguish the two graphs: when it takes three nodes, either three green nodes or two blue and one green nodes, it cannot distinguish the two graphs. The Graph U-Net variant considered above always remove blue or green nodes, thus it cannot distinguish the two graphs. One important observation is Graph U-Net cannot preserve the global graph structure in its pooling steps. For example, when it removes three nodes, the structure left is vastly different from the original graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original graphs Graph segmentation Patch graph</head><formula xml:id="formula_25">G 1 G 2 G1 G2</formula><p>Original graphs Graph segmentation Patch graph Proof. From the proof of GIN, we know that the two multi-sets {h L1 i : i ? G 1 } and {h L1 i : i ? G 2 } are already different if the two graphs can be distinguished by the L 1 -round 1-WL algorithm.</p><formula xml:id="formula_26">G 3 G 4 G3 G4</formula><p>Then we show that the rest of a learned network from Equation (4) to Equation ( <ref type="formula" target="#formula_8">9</ref>) is a bijective operation. We first consider the patch aggregation by the sum-pooling is bijective. According to Corollary 6 of <ref type="bibr" target="#b35">[36]</ref>, and assuming the GIN layers are properly trained, then there is an inverse inv(?) of sum-pooling such that {h L1 i : i ? C k } = inv(z 0 ). Then the inverse of patch aggregation is:</p><formula xml:id="formula_27">{h L1 i : i ? G 1 } = ? k k =1 inv(z 0 k )<label>(20)</label></formula><p>If the L 2 GNN layers on patches are also properly trained, then the mapping from Z 0 to Z L2 is also bijective. At the same time, we assume vectors in Z L2 are properly transformed, which will be useful in the following MHA operation.</p><p>Finally, we consider MHA layers. We first analyze the case with only one layer with one attention head. Note that q 1 = k ? softmax q 0 Z L2 / ? d Z L2 with d being the dimension of row vectors in Z 1 . Suppose PatchGT learns the query q 0 to be a zero vector, and the linear transformation in Equation ( <ref type="formula" target="#formula_8">9</ref>) is the identity operation, then g 1 = q 1 = 1 Z L2 , which is the summation of patch vectors Z L2 . Combining the last GIN layer, this summation is a bijective operation according to Corollary 6 of <ref type="bibr" target="#b35">[36]</ref>. If there are multiple MHA layers, then we only need the MLP in Equation ( <ref type="formula">8</ref>) to zero out the input, and the layer is equivalent to no operation. If there are multiple attention heads, the network can always take the first attention head. Therefore, a general case of MHA layers can also be a summation of input vectors.</p><p>Putting these steps together, there is an inverse mapping g 1 to {h L1 i : i ? G 1 } and mapping g 2 to {h L1 i : i ? G 2 }. Then g 1 and g 2 must be different. We further show that there are cases that cannot be distinguished by the 1-WL algorithm but can be distinguished by PatchGT. Consider two examples in Figure <ref type="figure" target="#fig_7">7</ref>. The two original graphs G 1 and G 2 , or G 3 and G 4 , are non-isomorphic. However, both the 1-WL algorithm cannot differentiate them. In comparison, by segmenting these graphs into patches, PatchGT can discriminate G 1 from G2. After segmentation, the two patches from G 1 and the pacthes G 2 can be distinguished by the 1-WL algorithm and also PatchGT. Note that node degrees of G 1 patches are already different from node degrees of G 2 patches. It is the same for G 3 and G 4 . These two examples indicate that the expressiveness of PatchGT is beyond 1-WL algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Proof of Theorem 4</head><p>We prove the theorem 4 through three lemmas below. Lemma 2. The patches split via k-means are invariant to column vectors in U from the spans of eigenvectors associated with the multiplicities of eigenvalues.</p><formula xml:id="formula_28">kmeans(V) = kmeans(VQ)<label>(21)</label></formula><p>where Q is a standard block-diagonal rotation matrix.</p><p>Proof. If we use N u eigenvectors for the graph patch splitting, corresponding to the first N u smallest eigenvalues, we can write them as (? 1 , u 1 ), ..., (? Nu , u Nu ). If we have multiplicities in these eigenvalues, we can rotate the eigenvectors by a block-diagonal rotation matrix Q ? R Nu?Nu to obtain another set of eigenvectors,</p><formula xml:id="formula_29">U = [u 1 , ..., u k ] = [u 1 , ..., u k ]Q = UQ<label>(22)</label></formula><p>where u i , u i ? R |V |?1 . If we perform k-means on the row vectors of [(u 1 ) i , ..., (u k ) Nu ], we can write the nodes' coordinates as</p><formula xml:id="formula_30">[x 1 ; ...; x |V | ] = [u 1 , ..., u Nu ].<label>(23)</label></formula><p>Similarly, we can write down the new coordinates after rotation as</p><formula xml:id="formula_31">[x 1 ; ...; x |V | ] = [u 1 , ..., u Nu ].<label>(24)</label></formula><p>From the above three equations, it holds that</p><formula xml:id="formula_32">[x 1 ; ...; x |V| ] = [x 1 ; ...; x |V| ]Q. (<label>25</label></formula><formula xml:id="formula_33">)</formula><p>So for i, j ? {1, ..., |V |}, we have</p><formula xml:id="formula_34">x i = x i Q x j = x j Q. (<label>26</label></formula><formula xml:id="formula_35">)</formula><p>The relative distance of new coordinates can be calculated as</p><formula xml:id="formula_36">(x i -x j )(x i -x j ) = (x i Q -x j Q)(x i Q -x j Q) = (x i -x j )QQ (x i -x j ) .<label>(27)</label></formula><p>From the property of the rotational matrix, we have</p><formula xml:id="formula_37">I = QQ .<label>(28)</label></formula><p>So it holds that</p><formula xml:id="formula_38">(x i -x j )(x i -x j ) = (x i -x j )(x i -x j ) .<label>(29</label></formula><p>) So for any two node pair, the relative distance is preserved, thus it will not affect the k-means results.</p><p>Using the property of undirected ? -graph, it holds that</p><formula xml:id="formula_39">AI S = ? I S - (i,j)?E,i?S,j?T (E i -E j ) = ? I S + B S ,<label>(57)</label></formula><p>where we denote B S = -(i,j)?E,i?S,j?T </p><formula xml:id="formula_40">(E i -E j ),<label>(58)</label></formula><formula xml:id="formula_41">| l+1 | ? a l ? I S V l |W 2l | + a l B S V l |W 2l | + Ar l |W 2l | + a l I S V l |W 1l | + r l |W 1l |.<label>(60)</label></formula><p>Let</p><formula xml:id="formula_42">a l+1 = (1 + ? )a l , V l+1 = ? ? + 1 V l |W 2l | + 1 ? + 1 V l |W T 1l |, r l+1 = a l B S V l |W 2l | + Ar l |W 2l | + r l |W 1l |,<label>(61)</label></formula><p>then we rewrite equation 60 as</p><formula xml:id="formula_43">| l+1 | ? a l+1 I S V l+1 + r l+1<label>(62)</label></formula><p>From the assumption that </p><formula xml:id="formula_44">||W 1l || 1 ? 1, ||W 2l || 1 ? 1,<label>(63) we</label></formula><formula xml:id="formula_45">have ||(|W 1l |)|| 1 = ||W 1l || 1 ? 1, ||(|W 2l |)|| 1 = ||W 2l || 1 ? 1.<label>(64)</label></formula><formula xml:id="formula_46">a l+1 = (? + 1)a l ? (? + 1) l+1 ||V l+1 || ? ? ? + 1 ||V l || 1 + 1 ? + 1 ||V l || ? d<label>(65)</label></formula><p>and</p><formula xml:id="formula_47">||r l+1 || 1 ? a l ||B S || 1 ||V l || 1 + ||A|| 1 ||r l || 1 + ||r l || 1 ? 2a l md + (? + 1)||r l || 1 ? 2md (? + 1) l + (? + 1)||r l || 1 ? 2md (? + 1) l + 2md (l + 1)(? + 1) l+1 ? 2md (? + 1) l+1 + 2md (l + 1)(? + 1) l+1 = 2md (l + 2)(? + 1) l+1 .<label>(66)</label></formula><p>This finishes the induction.</p><p>The above lemma gives max</p><formula xml:id="formula_48">||W 1l ||1 ||W 2l ||1 ? | l | ? (? + 1) l I S V l + r l<label>(67)</label></formula><p>where ||V l || ? d and ||r l || 1 ? 2d m(l + 1)(? + 1) l . So when only looking at indices ij with i ? T , the first term vanishes and it holds that</p><formula xml:id="formula_49">max ||W 1l ||1 ||W 2l ||1 ? i?T | l | ij ? 2d m(l + 1)(? + 1) l<label>(68)</label></formula><p>For the denominator, we simply construct W 1l = W 2l as both identity matrix and take 0 = ?. Then it simply holds that</p><formula xml:id="formula_50">| 0 | = (1 + ? ) 0 I T I d<label>(69)</label></formula><p>where I T is the indicator vector on set T . Assume it holds,</p><formula xml:id="formula_51">l = (1 + ? ) l I T I d<label>(70)</label></formula><p>then from the Lipschitz continuity (ReLU) of ? and standard ? -graph, it holds that</p><formula xml:id="formula_52">l+1 = ?((I + A)(H l + l )) -?((I + A)(H l )) = (1 + d) l (I + A)I T I d = (1 + ? ) l+1 I T I d (71) So we can get i?T |( l ) ij | = (1 + ? ) l i?T (I T I d ) ij = (1 + ? ) l |T |d<label>(72)</label></formula><p>So that it holds that max</p><formula xml:id="formula_53">||W 1l ||1 ||W 2l ||1 ? i?T | l | ij ? (1 + ? ) l |T |d<label>(73)</label></formula><p>Combine equation 68 and equation 73, and substitute the last layer number as L -1, we have max</p><formula xml:id="formula_54">||W 1l ||1 ||W 2l ||1 ? i?T | l | ij max ||W 1l ||1 ||W 2l ||1 ? i?T | l | ij ? 2mL |T | .<label>(74)</label></formula><p>A.8 Proof of Theorem 5</p><p>From the proof of proposition 1 in appendix A.7, by simply constructing W 1l , W 2l in the node-level GNN as identity matrix, we have</p><formula xml:id="formula_55">i?S |( S ) ij | = (1 + ? ) L |S|d if 0 = ?, i?T |( S ) ij | = (1 + ? ) L |T |d if 0 = ?.<label>(75)</label></formula><p>Then from Lipschitz continuity (ReLU) we have</p><formula xml:id="formula_56">? S?T = g GNN T (X + ?) -g GNN T (X) = ?(z T W 1 + (z S + 1 |V | i?S ( S ) ij )W 2 ) -?(z T W 1 + z S W 2 ) = ( 1 |V | i?S ( S ) ij )W 2 if 0 = ?<label>(76)</label></formula><p>and</p><formula xml:id="formula_57">||? S?T || 1 = ||( 1 |V | i?S ( S ) ij )W 2 || 1 = ||W 2 || 1 (1 + ? ) L |S| |V | d if 0 = ?<label>(77)</label></formula><p>Similarly, we can get</p><formula xml:id="formula_58">? T ?T = ( 1 |V | i?T ( T ) ij )W 1 . if 0 = ?,<label>(78)</label></formula><p>and</p><formula xml:id="formula_59">||? T ?T || 1 = ||W 1 || 1 (1 + ? ) L |T | |V | d if 0 = ?,<label>(79)</label></formula><p>Then we can simply make ||W1||1 ||W2||1 = |S| |T | , so that the ratio is 1. Remark 2. The assumption of output norm unification can be achieved by standard normalization, such as batch and layer normalizations. Lipschitz continuity exists widely in the activation functions such as ReLU. And most molecules can be modeled as qusi standard graphs. These assumptions are fair assumptions in graph learning. Although it is difficult to universally obtain a precise and tight bound, the existence of such bounds is still helpful for GNN structure design.</p><p>Remark 3. The ratio may become informative if there are information bottlenecks within a cluster. We can mitigate the problem by having an appropriate, sufficient number of clusters. However, the number of clusters can not be too large, so there is a tradeoff between avoiding bottlenecks and computational cost.</p><p>Here, we also introduce a heuristic example for possibly extending to a non-standard graph. Let subgraph S be an cycle (2-graph) and subgraph T be a clique (n-graph), approximately. And we assume |S| = |T | = n, and node values are all units with perturbation . After one propagation of node level, each node in S has the value 3(1 + ), each node in T has the value n(1 + ). Then at patch level, equations (75), (77), (79) are modified accordingly as ? S?T = 3 W 2 , ? T ?T = n W 1 , then ? S?T ? T ?T = 3 n ? W2 W1 , which indicates the unevenness may affect the performance. However, if at patch level W2 W1 ? O(n) can be learned, we can still reach a sub-optimal balance. Actually, if W2 W1 &gt; 1 can be learned, it will help mitigate the bottleneck anyway.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.9 Graph segmentation</head><p>As a graph has an irregular structure and contains rich structural information, forming patches on a graph is not as straightforward as segmenting images. The previous works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22]</ref> generally split an image in the euclidean space. However, graphs are segmented through spectral clustering based on its topology. Figure <ref type="figure" target="#fig_9">8</ref> shows the second eigenvector and patch segmentations based on the algorithm described in Section 3.2. It can be seen that the eigenvectors change along with the graph structures, and the graphs are splitted into several function groups. Such patches are useful for discriminating the property of the given molecule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.10 More results</head><p>Table <ref type="table" target="#tab_5">7</ref> provides the performance of PatchGT on ogbg-moltox21 and ogbg-moltoxcast.  <ref type="table" target="#tab_4">4</ref> contains the statistics for the six datasets from Open Graph Bechmark (OGB) <ref type="bibr" target="#b13">[14]</ref>, and Table <ref type="table">5</ref> contains the statistics for the six datasets from TU datasets <ref type="bibr" target="#b24">[25]</ref>.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.15 Frequencies of motifs</head><p>There are two classes in ogbg-molhiv, and we record the frequencies of motifs PatchGT pay attention to. There are an apparent difference between the two classes. It indicates the model has a better interpretability.</p><p>A. <ref type="bibr" target="#b15">16</ref> Ablation study for patch level GNN</p><p>In PatchGT, we apply patch level GNN to the entire graph. We can also apply it to each patch so that there would not be any connection between subgraphs. Here we test the difference of these two designs.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Model review. We segment a graph into several patch subgraphs by non-trainable clustering. We first extract local information through a GNN, and the initial patch representations are summarized by the aggregation of nodes within the corresponding patches. To further encode structure information, we apply another patch-level GNN to update the representations of patches. Finally, we use Transformer to extract the representation of the entire graph based on patch representations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: Pooling methods on a pair of graphs that cannot be distinguished by the 1-WL algorithm (nodes are colored by the 1-WL algorithm).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Segmentation results from spectral clustering and trainable clustering.</figDesc><graphic url="image-4.png" coords="7,118.40,90.83,63.35,63.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Analysis of the key design for the proposed PatchGT. All results are based on PatchGT GCN. In the left figure, we show how changing the threshold for eigenvalues affects performance on the ogbg-molclintox and PROTEINS datasets; The middle figure shows the model performances with the removal of patch-GNN or Transformer (replaced by mean pool) on DD and ogbg-molhiv datasets; The right figure shows the effect of the different readout functions for patch representations.</figDesc><graphic url="image-11.png" coords="9,200.66,259.09,98.14,73.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Attention visualization of PatchGT on ogbg-molhiv molecules. The second and fourth figures show the attention weights of query tokens on the node patches for the corresponding molecules, which are in the first and third figures. The molecule in the first figure does not inhibit HIV virus, yet the molecule in the third figure does.</figDesc><graphic url="image-13.png" coords="9,405.87,259.11,98.14,73.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Graph U-Net layer are (A, X) and (?, X) respectively, and A and ? cannot be distinguished by the 1-WL algorithm. The inputs to next layer are (A [b, b], X[b]) and (? [b, b], X[b]) respectively. By the lemma above, the 1-WL algorithm cannot distinguish A and ? , and it cannot be distinguish A [b, b] and ? [b, b] either. Therefore, it still cannot distinguish the inputs (A [b, b], X[b]) to the next layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Two graphs that cannot be distinguished by the 1-WL algorithm. The colors illustrate the coloring of graph nodes. In comparison, PatchGT can differentiate them through the patch-level graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>So substitute equation 64, equation 59 and equation 54 into equation 61,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Examples of eigenvectors, and graph patches for molecules.</figDesc><graphic url="image-22.png" coords="22,121.38,544.93,95.04,95.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Attention visualization of PatchGT on ogbg-molhiv molecules.</figDesc><graphic url="image-36.png" coords="24,206.07,315.75,108.33,81.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Frequency of motifs PatchGT pay attention to in two classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Results (%) on OGB datasets</figDesc><table><row><cell></cell><cell cols="4">ogbg-molhiv ogbg-molbace ogbg-molclintox ogbg-molsider</cell></row><row><cell>GCN +VN</cell><cell>75.99 ?1.19</cell><cell>71.44 ? 4.01</cell><cell>88.55?2.09</cell><cell>59.84?1.54</cell></row><row><cell>GIN + VN</cell><cell>77.07?1.49</cell><cell>76.41?2.68</cell><cell>84.06?3.84</cell><cell>57.75 ?1.14</cell></row><row><cell>Deep LRP</cell><cell>77.19?1.40</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PNA</cell><cell>79.05?1.32</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Nested GIN</cell><cell>78.34?1.86</cell><cell>74.33?1.89</cell><cell>86.35?1.27</cell><cell>61.2?1.15</cell></row><row><cell>GRAPHSNN +VN</cell><cell>79.72?1.83</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Graphormer (pre-trained)</cell><cell>80.51?0.53</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PatchGT-GCN</cell><cell>80.22?0.84</cell><cell>86.44?1.92</cell><cell>92.21 ?1.35</cell><cell>65.21 ? 0.87</cell></row><row><cell>PatchGT-GIN</cell><cell>79.99?1.21</cell><cell>84.08?2.03</cell><cell>86.75 ?1.04</cell><cell>64.90 ?0.92</cell></row><row><cell>PatchGT-DeeperGCN</cell><cell>78.13 ? 1.89</cell><cell>88.31?1.87</cell><cell>89.02? 1.21</cell><cell>65.46?1.03</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results (%) on TU datasets</figDesc><table><row><cell></cell><cell>DD</cell><cell cols="5">MUTAG PROTEINS PTC-MR ENZYMES Mutagenicity</cell></row><row><cell>GCN</cell><cell cols="2">71.6?2.8 73.4?10.8</cell><cell>71.7?4.7</cell><cell>56.4?7.1</cell><cell>50.17</cell><cell>-</cell></row><row><cell>GraphSAGE</cell><cell cols="2">71.6?3.0 74.0?8.8</cell><cell>71.2?5.2</cell><cell>57.0?5.5</cell><cell>54.25</cell><cell>-</cell></row><row><cell>GIN</cell><cell cols="2">70.5?3.9 84.5?8.9</cell><cell>70.6?4.3</cell><cell>51.2?9.2</cell><cell>59.6</cell><cell>-</cell></row><row><cell>GAT</cell><cell cols="2">71.0?4.4 73.9?10.7</cell><cell>72.0?3.3</cell><cell>57.0?7.3</cell><cell>58.45</cell><cell>-</cell></row><row><cell>DiffPool</cell><cell>79.3?2.4</cell><cell>-</cell><cell>72.7?3.8</cell><cell>-</cell><cell>62.53</cell><cell>77.6?2.7</cell></row><row><cell>MinCutPool</cell><cell>80.8?2.3</cell><cell>-</cell><cell>76.5?2.6</cell><cell>-</cell><cell>-</cell><cell>79.9?2.1</cell></row><row><cell>Nested GCN</cell><cell cols="2">76.3?3.8 82.9?11.1</cell><cell>73.3?4.0</cell><cell>57.3?7.7</cell><cell>31.2?6.7</cell><cell>-</cell></row><row><cell>Nested GIN</cell><cell cols="2">77.8?3.9 87.9?8.2</cell><cell>73.9?5.1</cell><cell>54.1?7.7</cell><cell>29.0?8.0</cell><cell>-</cell></row><row><cell>DiffPool-NOLP</cell><cell>79.98</cell><cell>-</cell><cell>76.22</cell><cell>-</cell><cell>61.95</cell><cell>-</cell></row><row><cell>SEG-BERT</cell><cell>-</cell><cell>90.8 ?6.5</cell><cell>77.1?4.2</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>U2GNN</cell><cell cols="2">80.2?1.5 89.9?3.6</cell><cell>78.5?4.07</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>EigenGCN</cell><cell>78.6</cell><cell>-</cell><cell>76.6</cell><cell>-</cell><cell>64.5</cell><cell>-</cell></row><row><cell>Graph U-Nets</cell><cell>82.43</cell><cell>-</cell><cell>77.68</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PatchGT-GCN</cell><cell cols="2">83.3?3.1 94.7?3.5</cell><cell>80.3?2.5</cell><cell>62.5?4.1</cell><cell>73.3?3.3</cell><cell>78.3?2.2</cell></row><row><cell>PatchGT-GIN</cell><cell cols="2">79.6?3.3 89.4?3.2</cell><cell>79.5?3.1</cell><cell>58.4?2.9</cell><cell>70.0?3.5</cell><cell>80.4?1.4</cell></row><row><cell cols="3">PatchGT-DeeperGCN 76.1?2.8 89.4?3.7</cell><cell>77.5?3.4</cell><cell>60.0?2.6</cell><cell>56.6?3.1</cell><cell>80.6?1.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>and E i , E j ? R |V |?1 are unit vectors with ith and jth entry equal to 1 respectively. Then it is trivial to show that ||B S || 1 ? 2m.</figDesc><table><row><cell>(59)</cell></row><row><cell>Substitute equation 57 into equation 56, we have</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Results (%) on OGB datasets</figDesc><table><row><cell></cell><cell cols="2">ogbg-moltox21 ogbg-moltoxcast</cell></row><row><cell>GCN +VN</cell><cell>75.51 ? 0.86</cell><cell>66.33?0.35</cell></row><row><cell>GIN + VN</cell><cell>76.21 ? 0.82</cell><cell>66.18 ?0.68</cell></row><row><cell>GRAPHSNN +VN</cell><cell>76.78? 1.27</cell><cell>67.68 ? 0.92</cell></row><row><cell>PatchGT-GCN</cell><cell>76.49 ?0.93</cell><cell>66.58 ?0.47</cell></row><row><cell>PatchGT-GIN</cell><cell>77.26 ? 0.80</cell><cell>67.95 ?0.55</cell></row><row><cell>A.11 Datasets</cell><cell></cell><cell></cell></row><row><cell>Table</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Statistics of OGB datasetsWe report the detailed hyper-parameter settings used for training PatchGT in Table6. The search space for ? is {0.1, 0.2, 0.4, 0.5, 0.8}.</figDesc><table><row><cell>Name</cell><cell cols="4">#Graphs #Nodes per graphs #Edges per graph #Tasks</cell></row><row><cell>molhiv</cell><cell>41,127</cell><cell>25.5</cell><cell>27.5</cell><cell>1</cell></row><row><cell>molbace</cell><cell>1,513</cell><cell>34.1</cell><cell>36.9</cell><cell>1</cell></row><row><cell>molclintox</cell><cell>1,477</cell><cell>26.2</cell><cell>27.9</cell><cell>2</cell></row><row><cell>molsider</cell><cell>1,427</cell><cell>33.6</cell><cell>35.4</cell><cell>27</cell></row><row><cell>ogbg-moltox21</cell><cell>7,831</cell><cell>18.6</cell><cell>19.3</cell><cell>12</cell></row><row><cell>ogbg-moltoxcast</cell><cell>8,576</cell><cell>18.8</cell><cell>19.3</cell><cell>617</cell></row><row><cell cols="2">A.12 Hyper-parameters selection</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 :</head><label>7</label><figDesc>Results (%) on ogbg-molhiv</figDesc><table><row><cell></cell><cell>single GNN</cell><cell>multiple GNNs</cell></row><row><cell cols="2">PatchGT-GCN 80.22 ?0.0.84</cell><cell>79.13 ?0.47</cell></row><row><cell>PatchGT-GIN</cell><cell>79.99 ? 1.21</cell><cell>78.96 ?0.55</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p><rs type="person">Li-Ping Liu</rs> was supported by <rs type="funder">NSF1908617</rs>. <rs type="person">Xu Han</rs> was supported by Tufts RA support. The research of J.H. is supported by <rs type="funder">NSF</rs> grant <rs type="grantNumber">DMS-2054835</rs>. J.W and H.G would like to acknowledge the funds from <rs type="funder">National Science Foundation</rs> under Award Nos. <rs type="grantNumber">CMMI-1934300</rs> and <rs type="grantNumber">OAC-2047127</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_JaJy3EN">
					<idno type="grant-number">DMS-2054835</idno>
				</org>
				<org type="funding" xml:id="_rUGH8uR">
					<idno type="grant-number">CMMI-1934300</idno>
				</org>
				<org type="funding" xml:id="_X5wbree">
					<idno type="grant-number">OAC-2047127</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Lemma 3. The patches split via k-means are invariant to column vectors in U with different signs.</p><p>Proof. The sign invariance is a special case of rotation invariance by taking Q as a diagonal matrix with entry (Q) ii ? {-1, 1} Lemma 4. The patches split via k-means are invariant to the permutations of nodes kmeans(U) = kmeans(PU) <ref type="bibr" target="#b29">(30)</ref> where P is a permutation matrix.</p><p>Proof. We denote</p><p>For a permutation matrix P of A, we have the corresponding permutation matrix P such that</p><p>where A and A are adjacency matrices of G and G respectively. And the for the degree matrix of</p><p>) Substitute equation31 into equation 32</p><p>From the symmetry of the permutation matrix, it holds that</p><p>Combine the above three equations, we can get</p><p>So the permuted Laplacian matrix is</p><p>Substitute into the Laplacian eigen decomposition, we have the equation</p><p>and its algebraic form</p><p>so the eigenvalues are remaining invariant.</p><p>Next we look at the eigenvector. For a eigenvector of bL , (?, u ), we have</p><p>Combine with equation 36, we can get</p><p>So we have the relation of two corresponding eigenvectors as</p><p>So we have the relation for the node coordinate [x 1 ; ...;</p><p>Thus there is a bijective mapping B : n ? m such that (P) nB(n) = 1 and x n = x B(n) . Then for any node pair (i, j), we can find (i , j ) = (B(i), B(j)) such that</p><p>then it clearly holds that</p><p>So for any two node pair, the relative distance is preserved, thus it will not affect the k-means results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 Multi-head attention</head><p>Transformer <ref type="bibr" target="#b31">[32]</ref> has been proved successful in the NLP and CV fields. The design of multi-head attention (MHA) layer is based on attention mechanism with Query-Key-Value (QKV). Given the packed matrix representations of queries Q, keys K, and values V, the scaled dot-product attention used by Transformer is given by:</p><p>where D k represents the dimensions of queries and keys.</p><p>The multi-head attention applies H heads of attention, allowing a model to attend to different types of information.</p><p>A.7 Proof of proposition 1</p><p>Given a L layer GNN with uniform hidden feature and initial feature H 0 = X, for l = 0, ..., L, the recurrent output of a GNN layer H l+1 follows</p><p>where</p><p>And then we introduce another recurrent relationship to track the output change of each layers propagated from an initial perturbation 0 ? R |V |?d on H 0 ,</p><p>We denote | ? | as an operator to replace a matrix's (?) elements with absolute values and we write</p><p>We firstly prove a lemma below. Lemma 5. Given 0 = ?, it holds that</p><p>where</p><p>Proof. We prove by induction. For l = 0, we can take</p><p>(50) From the recurrent relation in equation 48, it holds that</p><p>From the Lipschitz continuity of ?, it holds that</p><p>From the triangle inequality, we have</p><p>From the assumption the statement holds at lth layer, we have</p><p>Substitute equation 54 into equation 53, we have,</p><p>Expand the above equation, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.13 Visualization of attention on nodes</head><p>Figure <ref type="figure">9</ref> shows more attention on graphs. We notice that some patches the model concentrates on are far away from each other. This can help address information bottleneck in the graph. Also, it provides more model interpretation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.14 Analysis of the computational complexity</head><p>We compare our computational complexity with the node-level Transformer, Graphormer <ref type="bibr" target="#b37">[38]</ref>. The comptutational complexity for both framework can be classified into two parts. The first part is extracting graph structure infromation. For PatchGT, the complexity is O(|V | 3 ) for calculating the eigenvectors and perform kmeans for k patches. For Graphormer, the complexity is O(|V | 4 ) due to node pairwise shortest path computation.</p><p>Remark 4. The software and algorithms of eigen-decomposition are being widely developed in many disciplines <ref type="bibr" target="#b9">[10]</ref>. The complexity can be reduced to O(|V | 2 ) if a partial query and approximation of eigenvectors and eigenvalues are allowed <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b28">29]</ref>. And spectral clustering does not require all eigenvectors with exact values. However, we admit that for graphs with eigenvalues that are too close to each other, the complexity of computing the eigenvectors takes O(N 3 ).</p><p>The second part is neural network computation. For PatchGT, the complexity is O(|E|) for GNN if the adjacency matrix is sparse and O(k 2 ) for Transformer. And for Graphormer, the complexity of Transformer is O(|V | 2 ). It shoud be noticed that for a large graph, k &lt;&lt; |V |. Overall, the complexity of patch-level transformer is significantly less than that of applying transformer directly on the node level.</p><p>For other hierarchical pooling methods, they also need O(L|E|) to learn the segmentation (L is the number of layers used in GNN), which is comparable to spectral clustering. And spectral clustering is easier for parallel computation. Specifically, for a N pool -level hierarchical pooling, it needs O( </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Memory-based graph networks</title>
		<author>
			<persName><forename type="first">Amir</forename><surname>Hosein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khas</forename><surname>Ahmadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2020. 2, 7</date>
		</imprint>
		<respStmt>
			<orgName>University of Toronto (Canada)</orgName>
		</respStmt>
	</monogr>
	<note>PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">On the bottleneck of graph neural networks and its practical implications</title>
		<author>
			<persName><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eran</forename><surname>Yahav</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.05205</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Accurate learning of graph representations with graph multiset pooling</title>
		<author>
			<persName><forename type="first">Jinheon</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minki</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.11533</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Spectral Clustering with Graph Neural Networks for Graph Pooling</title>
		<author>
			<persName><forename type="first">Maria</forename><surname>Filippo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniele</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cesare</forename><surname>Grattarola</surname></persName>
		</author>
		<author>
			<persName><surname>Alippi</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<title level="s">Hal Daum? III and Aarti Singh</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020-07-18">18 Jul 2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="874" to="883" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research. 2, 3, 5, 7, 8</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Weisfeiler and Lehman go cellular: CW networks</title>
		<author>
			<persName><forename type="first">Cristian</forename><surname>Bodnar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="2625" to="2640" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Structure-Aware Transformer for Graph Representation Learning</title>
		<author>
			<persName><forename type="first">Dexiong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'</forename><surname>Leslie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><surname>Bray</surname></persName>
		</author>
		<author>
			<persName><surname>Borgwardt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.03036</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Mining Graph Data</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">B</forename><surname>Holder</surname></persName>
		</author>
		<ptr target="https://books.google.com/books?id=bHGy0%5C_H0g8QC.1" />
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast linear algebra is stable</title>
		<author>
			<persName><forename type="first">James</forename><surname>Demmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioana</forename><surname>Dumitriu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Holtz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Numerische Mathematik</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="59" to="91" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Dimensional Reduction of Highly Nonlinear Multiscale Models Using Most Appropriate Local Reduced-Order Bases</title>
		<author>
			<persName><forename type="first">Charbel</forename><surname>Farhat</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">23</biblScope>
			<pubPlace>LELAND STANFORD JUNIOR UNIV CA STANFORD United States</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Tech. rep</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Graph Attention Retrospective</title>
		<author>
			<persName><forename type="first">Kimon</forename><surname>Fountoulakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.13060</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Graph u-nets</title>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<idno>PMLR. 2019</idno>
		<imprint>
			<date type="published" when="2083">2083-2092. 3, 5, 7, 8, 13</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Understanding Pooling in Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Daniele</forename><surname>Grattarola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.05292</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">21</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Could graph neural networks learn better molecular representation for drug discovery? A comparison study of descriptor-based and graph-based models</title>
		<author>
			<persName><forename type="first">Dejun</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of cheminformatics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Ammus: A survey of transformer-based pretrained models in natural language processing</title>
		<author>
			<persName><forename type="first">Katikapalli</forename><surname>Subramanyam Kalyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajit</forename><surname>Rajasekharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sivanesan</forename><surname>Sangeetha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.05542</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rethinking graph transformers with spectral attention</title>
		<author>
			<persName><forename type="first">Devin</forename><surname>Kreuzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Self-attention graph pooling</title>
		<author>
			<persName><forename type="first">Junhyun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inyeop</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
		<idno>PMLR. 2019</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05493</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Sign and Basis Invariant Networks for Spectral Graph Representation Learning</title>
		<author>
			<persName><forename type="first">Derek</forename><surname>Lim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.13013</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision. 2021</meeting>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Graph convolutional networks with eigenpooling</title>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="723" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Graphit: Encoding graph structure in transformers</title>
		<author>
			<persName><forename type="first">Gr?goire</forename><surname>Mialon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.05667</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">TUDataset: A collection of benchmark datasets for learning with graphs</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08663</idno>
		<ptr target="www.graphlearning.io" />
	</analytic>
	<monogr>
		<title level="m">ICML 2020 Workshop on Graph Representation Learning and Beyond (GRL+ 2020)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Universal graph transformer selfattention networks</title>
		<author>
			<persName><forename type="first">Tu</forename><surname>Dai Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinh</forename><surname>Dinh Nguyen</surname></persName>
		</author>
		<author>
			<persName><surname>Phung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11855</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Towards interpretable sparse graph representation learning with laplacian pooling</title>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Noutahi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11577</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Revisiting graph neural networks: All we have is low-pass filters</title>
		<author>
			<persName><forename type="first">Hoang</forename><surname>Nt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takanori</forename><surname>Maehara</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09550</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Iterative methods for sparse linear systems</title>
		<author>
			<persName><forename type="first">Yousef</forename><surname>Saad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM</title>
		<imprint>
			<biblScope unit="page">23</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Patches Are All You Need?</title>
		<author>
			<persName><forename type="first">Asher</forename><surname>Trockman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zico</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kolter</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2201.09792</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">How Graph Neural Networks Go Beyond Weisfeiler-Lehman?</title>
		<author>
			<persName><forename type="first">Asiri</forename><surname>Wijesinghe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
	<note>A New Perspective on</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Representing long-range context for graph neural networks with global attention</title>
		<author>
			<persName><forename type="first">Zhanghao</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="13266" to="13279" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="4" to="24" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<imprint>
			<date type="published" when="2018">2018. 1, 3, 5, 8, 13</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep graph kernels</title>
		<author>
			<persName><forename type="first">Pinar</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the 21th ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Do Transformers Really Perform Badly for Graph Representation?</title>
		<author>
			<persName><forename type="first">Chengxuan</forename><surname>Ying</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">23</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
	<note>2, 3, 5, 7, 8</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Graph transformer networks</title>
		<author>
			<persName><forename type="first">Seongjun</forename><surname>Yun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Data reduction for spectral clustering to analyze high throughput flow cytometry data</title>
		<author>
			<persName><forename type="first">Habil</forename><surname>Zare</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Dive into Deep Learning</title>
		<author>
			<persName><forename type="first">Aston</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.11342</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Segmented graph-bert for graph instance modeling</title>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.03283</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Nested Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
