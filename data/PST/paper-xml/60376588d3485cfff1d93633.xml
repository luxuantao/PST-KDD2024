<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep learning for monocular depth estimation: A review q</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-01-05">5 January 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yue</forename><surname>Ming</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Beijing Key Laboratory of Work Safety and Intelligent Monitoring</orgName>
								<orgName type="department" key="dep2">School of Electronic Engineering</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<postCode>100876</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xuyang</forename><surname>Meng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Beijing Key Laboratory of Work Safety and Intelligent Monitoring</orgName>
								<orgName type="department" key="dep2">School of Electronic Engineering</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<postCode>100876</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chunxiao</forename><surname>Fan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Beijing Key Laboratory of Work Safety and Intelligent Monitoring</orgName>
								<orgName type="department" key="dep2">School of Electronic Engineering</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<postCode>100876</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Hui</forename><surname>Yu</surname></persName>
							<email>hui.yu@port.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="department">School of Creative Technologies</orgName>
								<orgName type="institution">University of Portsmouth</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep learning for monocular depth estimation: A review q</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-01-05">5 January 2021</date>
						</imprint>
					</monogr>
					<idno type="MD5">1938A69E9E94FE48DFB566D9B32E76DA</idno>
					<idno type="DOI">10.1016/j.neucom.2020.12.089</idno>
					<note type="submission">Received 20 October 2020 Revised 19 December 2020 Accepted 19 December 2020</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Monocular depth estimation Deep learning Supervised learning Unsupervised learning Multi-task learning</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Depth estimation is a classic task in computer vision, which is of great significance for many applications such as augmented reality, target tracking and autonomous driving. Traditional monocular depth estimation methods are based on depth cues for depth prediction with strict requirements, e.g. shape-fromfocus/ defocus methods require low depth of field on the scenes and images. Recently, a large body of deep learning methods have been proposed and has shown great promise in handling the traditional ill-posed problem. This paper aims to review the state-of-the-art development in deep learning-based monocular depth estimation. We give an overview of published papers between 2014 and 2020 in terms of training manners and task types. We firstly summarize the deep learning models for monocular depth estimation. Secondly, we categorize various deep learning-based methods in monocular depth estimation. Thirdly, we introduce the publicly available dataset and the evaluation metrics. And we also analysis the properties of these methods and compare their performance. Finally, we highlight the challenges in order to inform the future research directions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Scene depth estimation plays an important role in computer vision, which enhances the perception and understanding of real three-dimensional scenes leading to a wide range of applications such as robotic navigation, autonomous driving, and virtual reality <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b139">139,</ref><ref type="bibr" target="#b145">145,</ref><ref type="bibr" target="#b167">166]</ref>. Active depth estimation methods usually utilize lasers, structured light and other reflections on the object surface to obtain depth point clouds, complete surface modeling and estimate scene depth maps <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b183">182]</ref>. However, obtaining dense and accurate depth maps usually requires extremely heavy costs of manpower and computing resources <ref type="bibr" target="#b101">[101,</ref><ref type="bibr" target="#b179">178]</ref>. Therefore, image-based depth estimation has become the mainstream of research, and can be applied in a wide range of applications <ref type="bibr" target="#b88">[89,</ref><ref type="bibr" target="#b135">135]</ref>.</p><p>The evolution of image-based depth estimation is shown in Fig. <ref type="figure" target="#fig_0">1</ref>. In the early period, researchers estimated depth maps depending on depth cues, such as vanishing points <ref type="bibr" target="#b142">[142]</ref>, focus and defocus <ref type="bibr" target="#b138">[138]</ref>, and shadow <ref type="bibr" target="#b182">[181]</ref>. However, most of these methods were applied in constraint scenes <ref type="bibr" target="#b138">[138,</ref><ref type="bibr" target="#b142">142,</ref><ref type="bibr" target="#b182">181]</ref>. With the development of computer vision, many hand-made features and probabilistic graph models have been proposed, such as scale-invariant feature transform (SIFT) <ref type="bibr" target="#b87">[88]</ref>, speeded up robust features (SURF) <ref type="bibr" target="#b6">[7]</ref>, pyramid histogram of oriented gradient (PHOG) <ref type="bibr" target="#b8">[9]</ref>, Conditional Random Field (CRF) <ref type="bibr" target="#b65">[66]</ref>, and Markov Random Field (MRF) <ref type="bibr" target="#b24">[25]</ref>, which were adopted to predict monocular depth maps with parameter and non-parameter learning in the machine learning process <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b80">81]</ref>. The advent of deep learning technologies has brought great advantages to image processing <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b148">148,</ref><ref type="bibr" target="#b173">172]</ref> especially depth estimation.</p><p>Traditional depth estimation methods of image-based depth estimation are usually based on binocular camera, which calculates the disparity of two 2D images (taken by a binocular camera) through stereo matching and triangulation to obtain a depth map <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b81">82,</ref><ref type="bibr" target="#b117">117,</ref><ref type="bibr" target="#b171">170,</ref><ref type="bibr" target="#b181">180]</ref>. However, the binocular depth estimation method requires at least two fixed cameras <ref type="bibr" target="#b186">[185]</ref>, and it is difficult to capture enough features in the image to match when the scene has less or no texture <ref type="bibr" target="#b83">[84]</ref>. Therefore, researchers turn their attention to monocular depth estimation. Monocular depth estimation uses only one camera to obtain an image or video sequence, which does not require additional complicated equipments and professional techniques. It has vast application demands due to the availability of only one single camera in most application scenarios. Thus,there is an increasing demand for monocular depth estimation in recent years. Since monocular images lack a reliable stereo-scopic visual relationship, it is essentially an ill-posed problem to regress depth in 3D space <ref type="bibr" target="#b102">[102]</ref>. Therefore, researchers propose various methods for monocular depth estimation <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b66">67]</ref>.</p><p>Monocular images adopt a two-dimensional form to reflect the three-dimensional world. However, one dimension of the scene, namely depth, has missed in the imaging process, which makes it impossible to judge the size and distance of the object, nor to judge whether the object is occluded by another object. Therefore, we need to recover the depth of the monocular image. Based on the depth map, we can judge the size and distance of the object to meet the needs of scene understanding. When the estimated depth map can reflect the three-dimensional structure of the scene, we can consider that the depth estimation method is effectiveness.</p><p>This paper focuses on the research of monocular depth estimation, which surveys deep learning-based methods in recent years, details their remarks, and compares their performances. Furthermore, this paper describes the limitations of these existing methods and briefly introduces the future trends. The remainder of this paper is as follows: Section 2 introduces some deep learning models for monocular depth estimation; Section 3 summarizes deep learning-based methods of monocular depth estimation, from training manners and task types; Section 4 introduces the common datasets and evaluation metrics of depth estimation, and then analysis their properties and compares their performance; Section 5 discusses the challenges and trends of monocular depth estimation; Conclusions are drawn in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Deep Learning models for monocular depth estimation</head><p>This section mainly introduces common deep learning models for monocular depth estimation: Convolutional Neural Network (CNN) <ref type="bibr" target="#b62">[63]</ref>, Recurrent Neural Network (RNN) <ref type="bibr" target="#b122">[122]</ref>, and Generative Adversarial Network (GAN) <ref type="bibr" target="#b38">[39]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">CNN</head><p>CNN can automatically extract spatial features representing depth in a scene. It is a type of feed-forward neural network, which extracts depth features and reconstructs depth maps at the same time with fewer parameters compared to traditional methods <ref type="bibr" target="#b166">[165,</ref><ref type="bibr" target="#b160">159,</ref><ref type="bibr" target="#b85">86]</ref>. CNN mainly includes convolutional layer, pooling layer, fully connected layer and activation function, which enable CNN to learn the two-dimensional spatial features of the input image. The convolutional layer transforms the input into depth features; the pooling layer reduces the size of the input feature map in max-pooling or average-pooling manner; the fully connected layer is usually located at the end of the CNN to output the results; and the activation function is generally a continuously differentiable nonlinear function to avoid pure linear combinations. Representative CNNs include AlexNet <ref type="bibr" target="#b62">[63]</ref>, VGG <ref type="bibr" target="#b131">[131]</ref>, GoogLeNet <ref type="bibr" target="#b137">[137]</ref>, ResNet <ref type="bibr" target="#b47">[48]</ref>, DenseNet <ref type="bibr" target="#b51">[52]</ref>, and some lightweight network, such as MobileNet <ref type="bibr" target="#b50">[51]</ref>, ShuffleNet <ref type="bibr" target="#b184">[183]</ref>, and GhostNet <ref type="bibr" target="#b45">[46]</ref>, each of which is used as the backbone of the existing CNN-based depth estimation network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">RNN</head><p>RNN is a sequence-to-sequence model with memory capabilities <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b40">41]</ref> as shown in Fig. <ref type="figure" target="#fig_1">2</ref>(a), which is introduced into monocular depth estimation so as to learn temporal features from video sequences. RNN includes three parts: input unit, hidden unit, and output unit, where the input of the hidden unit consists of the outputs of both current input unit and previous hidden unit. Furthermore, Hochreiter et al. <ref type="bibr" target="#b49">[50]</ref> proposed a Long Short-Term Memory (LSTM) unit as shown in Fig. <ref type="figure" target="#fig_1">2</ref>(b), which could learn long-term dependences with a three-gate structure: input gate layer, forget gate layer, and output gate layer. Representative RNNs including BiRNN <ref type="bibr" target="#b126">[126]</ref>, GRU <ref type="bibr" target="#b21">[22]</ref>, ConvLSTM <ref type="bibr" target="#b163">[162]</ref>, G 2 -LSTM <ref type="bibr" target="#b77">[78]</ref>, ON-LSTM Neurocomputing 438 (2021) 14-33 <ref type="bibr" target="#b127">[127]</ref>, Mogrifier LSTM <ref type="bibr" target="#b96">[96]</ref> and others are introduced into deep learning models for monocular depth estimation, which are usually combined with CNNs to extract spatial-temporal features to recover depth <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b149">149]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">GAN</head><p>The supervised depth estimation model needs to learn the 3D mapping and scale information from the ground truth (GT) depth maps. However, it is difficult to obtain GT depth maps in real scenes so that researchers introduced GAN <ref type="bibr" target="#b38">[39]</ref> to generate clearer and more realistic depth maps compared to other models <ref type="bibr" target="#b178">[177]</ref>. GAN includes two modules: the generator predicts the depth map as a depth estimation network, and the discriminator determines whether the input depth map is true or false, as shown in Fig. <ref type="figure" target="#fig_3">3</ref>. Representative GANs are introduced into depth estimation, including conditional GAN <ref type="bibr" target="#b99">[99]</ref>, DCGAN <ref type="bibr" target="#b111">[111]</ref>, WGAN <ref type="bibr" target="#b3">[4]</ref>, stacked GAN <ref type="bibr" target="#b178">[177]</ref>, SimGAN <ref type="bibr" target="#b128">[128]</ref>, and Cycle GAN <ref type="bibr" target="#b197">[196]</ref>. Depth estimation models with GANs can provide generation adversarial constraints for the estimated depth maps and the GT depth maps <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b57">58]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Deep learning methods for monocular depth estimation</head><p>Deep neural networks have played an important role in various areas with their powerful feature learning ability. Monocular depth estimation based deep learning is a task of learning depth maps from a single 2D color image through a deep neural network, which was firstly proposed by Eigen et al. <ref type="bibr" target="#b28">[29]</ref> in 2014. It was a coarse-to-fine framework, where the coarse network learned the global depth on the entire image to obtain a rough depth map and the fine network learned the local features to refine the depth map, as shown in Fig. <ref type="figure" target="#fig_4">4</ref>. Since then, many researchers have carried out deep learning methods for monocular depth estimation <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b82">83,</ref><ref type="bibr" target="#b170">169,</ref><ref type="bibr" target="#b175">174,</ref><ref type="bibr" target="#b190">189]</ref>.</p><p>The framework of monocular depth estimation based on deep learning is an encoder-decoder network, with the RGB image input and depth map output, as shown in Fig. <ref type="figure" target="#fig_5">5</ref>. The encoder network consists of convolution and pooling layers to capture the depth features, and the decoder network includes deconvolution layers to regress the estimated pixel-level depth map, with the same size as the input. Additionally, in order to preserve the features of each scale, the corresponding layers of encoder and decoder are concatenated with skip-connections. The entire network is constrained and trained by the depth loss functions and converges when the desired depth map is generated.</p><p>Deep learning methods for monocular depth estimation often utilize gradient descent to train deep neural networks, and obtain a local minimum finally. The best local minimum depends on initialization and specific parameter settings. In the initialization process, it is generally necessary to resize the image to meet the needs of network learning. In addition, it also need to set the initial learning rate, optimizer parameters, batchsize and mini-batchsize, to learn and save image features. The commonly used learning method is stochastic gradient descent, and the optimizer is Adam. When the gradient no longer changes and the loss function becomes stable, the network converges.</p><p>Compared with traditional methods, deep learning methods for monocular depth estimation construct the multi-layer neural network to learn deep features, which has higher accuracy. When there is small occlusion in the monocular image or part of the ground-truth depth is missing, the deep learning methods can still estimate the depth of the scene, and have low errors; when there is large occlusion in presence in the scene or there is no ground-truth depth, deep learning methods can learn the depth of the scene by adding network constraints. In short, deep learning methods for monocular depth estimation have shown strong robustness.</p><p>This section reviews and summarizes deep learning methods for monocular depth estimation from 2014 to 2020, which was classified into two different perspectives: the training manners with supervised, unsupervised and semi-supervised manner, and the tasks with single-task and multi-task learning of depth estimation models. The overall diagram of monocular depth estimation based on deep learning is drawn in Fig. <ref type="figure">6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Training manners</head><p>The supervised monocular depth estimation network estimates the depth maps by learning the scene structure information from the GT depth maps. The cost of obtaining the GT depth maps is very high, so that some monocular depth estimation networks need to be trained with less or no GT to reconstruct depth maps, which are the semi-supervised or unsupervised learning methods. This section will review and classify deep learning methods from the perspective of training manners: supervised, unsupervised, and semi-supervised models for monocular depth estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Supervised learning methods</head><p>Supervised learning networks for monocular depth estimation are trained with the GT depth maps as shown in Fig. <ref type="figure">7</ref>. The purpose of learning is to penalize the errors between the predictions and GT depth maps constrained by the loss functions formulated in Table <ref type="table">1</ref>, where the log(d) as Eq. ( <ref type="formula" target="#formula_0">1</ref>) is based on log depth <ref type="bibr" target="#b27">[28]</ref>, and the reverse Huber (Berhu) function as Eq. (1) combines the L 1 and L 2 norms at the same time to reduce the influence of error changes on the range of weights proposed by Laina et al. <ref type="bibr" target="#b68">[69]</ref>. That is, the depth model converges when the predicted depth value is as close to GT as possible, and other loss functions are variants of the functions mentioned in Table <ref type="table">1</ref>. For absolute depth learning, Li et al. <ref type="bibr" target="#b75">[76]</ref> proposed a twostreamed framework based on VGG-16 <ref type="bibr" target="#b131">[131]</ref> for monocular depth estimation: one stream for depth regression and other for depth gradients, which were combined through a depth-gradient fusion module to obtain a coherent depth map. The entire model was con-   strained by the depth loss and the gradient loss functions, enhancing the generalization abilities of each stream mutually for richer 3D projections. Furthermore, there are many monocular depth estimation methods based on more complex CNNs to learn pixellevel depth, such as VGG-based models <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b189">188]</ref>, ResNet-based models <ref type="bibr" target="#b68">[69,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b189">188]</ref>, and DenseNet-based models <ref type="bibr" target="#b70">[71]</ref>.</p><p>For relative depth estimation, Zoran et al. <ref type="bibr" target="#b198">[197]</ref> proposed a method adopting the relative relationship between point-pairs in the image to infer depth information. They output the relative relationship between the point-pairs and utilized the numerical optimization method to obtain the dense depth maps. Chen et al. <ref type="bibr" target="#b18">[19]</ref> proposed a multi-scale network that predicted pixel-level depth by learning relative depth. The network was trained with the relative depth loss function and performed depth recovery on monocular images in an unconstrained environment, whose root mean square error (RMSE) was 1.10 comparable to the absolute depth estimation model <ref type="bibr" target="#b82">[83]</ref>. Lee et al. <ref type="bibr" target="#b69">[70]</ref> designed a CNN to estimate the relative depth at different scales, which was optimally reorganized to reconstruct the final depth map. Their RMSE was better than most absolute depth methods mentioned above. The absolute depth learning has higher accuracy, and the relative depth learning models are more robust which aren't affected by the data homography. 3.1.1.1.1. Combined with CRF. Conditional Random Field (CRF) is a conditional probability distribution model under the condition of a given input sequences <ref type="bibr" target="#b65">[66]</ref>. CRF can establish a structured connection between input and output, where the key is to construct a reasonable and correct feature for monocular image depth estimation. In order to regress continuous depth, depth estimation networks with fixed and shared weights are constructed to learn different patches firstly. Then, these estimations are propagated to the CRF module to obtain the final depth, as shown in Fig. <ref type="figure" target="#fig_7">8</ref>.</p><p>Based on CRF, Xu et al. <ref type="bibr" target="#b164">[163]</ref> proposed an attention model to automatically learn robust multi-scale features through an integrated attention mechanism <ref type="bibr" target="#b84">[85,</ref><ref type="bibr" target="#b146">146,</ref><ref type="bibr" target="#b156">155]</ref>, where the cascade-CRFs module reduced the RMSE of 0.088 compared to the baseline based on ResNet-50. Ricci et al. <ref type="bibr" target="#b119">[119]</ref> proposed two deep models for monocular depth estimation, one was based on multiple CRF cascading, and the other was based on a unified graph model. Multi-scale features were merged through CRF integration multilevel cascade. Additionally, there are lots of CNNs combined with continuous CRF <ref type="bibr" target="#b74">[75,</ref><ref type="bibr" target="#b82">83]</ref>, hierarchical CRF <ref type="bibr" target="#b151">[151]</ref>, FC-CRF <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b100">100]</ref>, to predict monocular depth in a supervised manner.</p><p>CNN has made great progress in monocular depth estimation recently. On the one hand, it learns and fits deep features to reconstruct the scene depth maps by designing deeper and more complex networks; on the other hand, it combines with CRF to analyze and optimize the predictions of the deep networks, to obtain refined depth map. How to reconstruct the novel networks to adapt to monocular depth estimation is an important research direction.</p><p>3.1.1.2. RNNs-based methods. RNN-based supervised learning networks for monocular depth estimation capture the spatial features and temporal information from monocular image sequences <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b149">149]</ref>. Different from CNN-based models, the encoder of RNNbased network is designed with all LSTM (or ConvLSTM) layers or consists of convolution and LSTM (ConvLSTM) layers to extract and reserve spatial-temporal features for monocular depth estimation, as shown in Fig. <ref type="figure" target="#fig_8">9</ref>.</p><p>Kumar et al. <ref type="bibr" target="#b63">[64]</ref> proposed the DepthNet with ConvLSTM [162] layers to predict monocular depth maps and implicitly learned the smooth temporal variation. The encoder of DepthNet only consisted of eight ConvLSTM layers likes Fig. <ref type="figure" target="#fig_8">9</ref>(a), which made the network fully use the temporal information in sequences, and the convolution operation helped to maintain the spatial geometric relationships between the cells. Furthermore, Mancini et al. <ref type="bibr" target="#b92">[93]</ref> adopted LSTM units to exploit the input stream sequentiality and predict scene depth, where the LSTM layers followed the convolution layers in the encoder network, illustrated in Fig. <ref type="figure" target="#fig_8">9</ref>(b).</p><p>3.1.1.3. GANs-based methods. GAN-based supervised networks can generate depth maps close to the GT <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b57">58]</ref>, as shown in Fig. <ref type="figure" target="#fig_3">3</ref>. Specially, Jung et al. <ref type="bibr" target="#b57">[58]</ref> introduced GANs to the monocular depth estimation, where the generator consisted of a GlobalNet to extract global features and a RefinementNet to estimate local structures from the input image. The entire model was trained with an adversarial loss built on the estimated depth map and the GT depth map:</p><formula xml:id="formula_0">min G max D E x$P GT ½logDðxÞ þ E x Ã $P G ½logð1 À Dðx Ã ÞÞ<label>ð1Þ</label></formula><p>where G is the generator function, D is the discriminator function, x is the depth estimated by the generator, x Ã is the GT depth map, and P represents the domain of pixel. Summary. Supervised deep learning methods have been widely studied and applied in monocular depth estimation, mainly including CNN-based, RNN-based and GAN-based models, where the CNN mainly learns the spatial features of the scene, the RNN learns the temporal information from the video sequences, and GAN is introduced to generate and discriminate depth maps. Because the supervised learning methods need plenty of GT depth maps as the supervision, the accuracy rate is high when scale of the Table <ref type="table">1</ref> The loss functions commonly used in supervised learning for monocular depth estimation, where d respects the estimated depth, d Ã is the GT depth, y 2 i ¼ logðdÞ À logðd Ã Þ; k is a balance factor, and c is a threshold.</p><p>Name Formulation predicted depth map is close to the GT depth map. They can effectively map the 3D structure of the scene. However, GT depth maps are difficult to obtain. Therefore, depth estimation methods based on virtual images have attracted many researchers, and many unsupervised learning methods have emerged, which do not require GT and reduce the requirements for datasets with GT.</p><formula xml:id="formula_1">L 1 ðd; d Ã Þ L 1 ðd; d Ã Þ ¼ 1 N P N i¼1 jjd i À d Ã i jj 1 L 2 ðd; d Ã Þ L 2 ðd; d Ã Þ ¼ 1 N P N i¼1 jjd i À d Ã i jj 2 2 LðlogdÞ Lðd; d Ã Þ ¼ 1 N X N i¼1 y 2 i À k N X N i¼1 y i ! 2 Berhu L Berhu ðd; d Ã Þ ¼ jd À d Ã j if jd À d Ã j 6 c; jdÀd Ã j 2 þc 2 2c if jd À d Ã j &gt; c:</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Unsupervised learning methods</head><p>Supervised learning methods need to input a large number of images with GT depth maps during the training stage. However, high-resolution publicly labeled datasets still need numerous equipments and intensive labor work. Therefore, researchers explore unsupervised deep learning methods for monocular depth estimation without GT depth maps. Unsupervised monocular depth estimation are usually trained with stereo pair-wise images or monocular image sequences, and tested on monocular images or sequences, which are trained with scene geometric constraints.</p><p>3.1.2.1. Stereo matching. Unsupervised learning methods are inspired by traditional stereo matching methods as shown in Fig. <ref type="figure" target="#fig_18">10</ref>, which usually utilize left and right images to calculate depth value <ref type="bibr" target="#b136">[136]</ref>. The learning model is trained with stereo pair-wise images and tested on single image, as shown in Fig. <ref type="figure" target="#fig_18">11</ref>. The depth network estimates the disparity map between the left and right images, where the new image can be constructed with image warping based on the disparity map and the right image. The pixel p(s) can be obtained through pðsÞ $ KTðt ! sÞDðtÞK À1 pðtÞ ð 2Þ</p><p>where K is the camera intrinsics matrix, Tðt ! sÞ is the transformation between left and right images, DðtÞ is the estimated depth map, and p(t) is the homogeneous coordinate of a pixel in the reconstructed image. Therefore, the depth network is constrained by the difference, a reconstruction error, between the source and the reconstructed image. Common image reconstruction loss functions are L 1 and SSIM <ref type="bibr" target="#b157">[156]</ref> as follow:</p><formula xml:id="formula_2">L rec ¼ X p jIðpÞ À I w ðpÞj 1<label>ð3Þ</label></formula><formula xml:id="formula_3">L rec ¼ a 1 À SSIMðIðpÞ À I w ðpÞÞ 2 þ ð1 À aÞjIðpÞ À I w ðpÞj 1<label>ð4Þ</label></formula><p>where IðpÞ and I w ðpÞ represents the source image and the warped image reconstructed from the source image, respectively. a is a weight between L 1 norm and SSIM term. Unsupervised learning methods based on stereo matching usually adopt CNNs for monocular depth estimation. Garg et al. <ref type="bibr" target="#b33">[34]</ref> adopted the general model as shown in Fig. <ref type="figure" target="#fig_18">11</ref> to learn monocular depth maps in an unsupervised manner with the reconstruction loss in L 1 norm as Eq. ( <ref type="formula" target="#formula_2">3</ref>) in 2016. On this basis, a number of researchers began to utilize the left and right views to train networks with stereo matching based on 2D CNNs and 3D CNNs.</p><p>For 2D CNNs, Godard et al. <ref type="bibr" target="#b35">[36]</ref> proposed the left-right consistency constraints to train the unsupervised network, where they reconstructed the left and right view simultaneously. Their model was constrained by the reconstruction loss, the disparity smoothness loss, and the left-right disparity consistency. Experiments proved that the addition of the new loss functions enhanced the accuracy of the predicted depth map from each view. Moreover, Xie et al. <ref type="bibr" target="#b162">[161]</ref> added a selection layer in image reconstruction, Wong et al. <ref type="bibr" target="#b159">[158]</ref> designed a global-to-local network for feature extraction, Goldman et al. <ref type="bibr" target="#b37">[38]</ref> constructed a Siamese network to learn stereo images, Andraghetti et al. <ref type="bibr" target="#b2">[3]</ref> enhanced the depth estimation with traditional visual odometry. Watson et al. <ref type="bibr" target="#b158">[157]</ref> strengthened stereo matching with depth hints. Ur et al. <ref type="bibr" target="#b115">[115]</ref> applied unsupervised pre-trained filter method.</p><p>For 3D CNNs, some researchers adopted context information to constrain unsupervised networks in 3D convolution blocks for monocular depth estimation <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b59">60]</ref>, as shown in Fig. <ref type="figure" target="#fig_18">12</ref>. During training, two 2D CNNs with shared weights learn feature maps from left and right images, respectively. And then, these two groups of feature maps are concatenated to the 3D convolution network in a cost volume module <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b143">143]</ref> to estimate the final depth map combined with context information <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b176">175]</ref>. Specially, Chang et al. <ref type="bibr" target="#b13">[14]</ref> proposed the PSMNet, trained in a top-down/ bottom-up manner to perform unsupervised monocular depth estimation, where a spatial pyramid pooling module was used as a matching cost volume by aggregating semi-global environment information and a 3D convolution module adjusted the matching Unsupervised learning models based on stereo matching is mainly constrained by the projection and mapping relationship between the left and right pair-wise images, which still require the datasets containing stereo images. Therefore, how to utilize only a single camera in the training stage for unsupervised monocular depth estimation has attracted the attention of researchers.</p><p>3.1.2.2. Monocular sequences. Unsupervised learning models trained with monocular sequences consider the scene structure and camera motion at the same time, where camera pose estimation is similar to the images transformation estimation and has a positive impact on monocular depth estimation <ref type="bibr" target="#b169">[168,</ref><ref type="bibr" target="#b191">190,</ref><ref type="bibr" target="#b196">195]</ref>. Recently, researchers have introduced the visual odometry <ref type="bibr" target="#b105">[105,</ref><ref type="bibr" target="#b125">125]</ref> into the depth estimation based on monocular sequences, where the scene depth can be learned by predicting the camera motion.</p><p>The general model of unsupervised learning based on monocular sequences for depth estimation is shown in Fig. <ref type="figure" target="#fig_18">13</ref>, which consists of two sub-networks, depth network for depth estimation and pose network for visual odometry, respectively. During the training stage, these two networks are trained jointly, and the entire model is constrained by image reconstruction loss similar to stereo matching methods. The difference is that the image warping is built on adjacent frames of the monocular sequence. For loss functions, the smoothness loss and the photometric consistency loss in stereo matching methods are adopted in the unsupervised methods based on monocular sequences apart from the reconstruction loss.</p><p>Zhou et al. <ref type="bibr" target="#b195">[194]</ref> designed two networks to estimate depth maps and camera motion in the monocular video independently, which could be trained jointly or separately with reconstruction loss and photometric consistency loss functions <ref type="bibr" target="#b144">[144,</ref><ref type="bibr" target="#b155">154]</ref> and tested on one image or monocular sequence. Their work provided many useful references for subsequent works, such as, models trained with 3D geometric constraints <ref type="bibr" target="#b90">[91,</ref><ref type="bibr" target="#b168">167,</ref><ref type="bibr" target="#b194">193]</ref>, estimation with uncertainty or confidence maps <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b107">107]</ref>, networks designed with self-attention <ref type="bibr" target="#b56">[57]</ref>, and others <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b165">164,</ref><ref type="bibr" target="#b177">176]</ref>.</p><p>Summary. Unsupervised learning methods for monocular depth estimation directly learn depth information from geometric constraints. It mainly includes two types: one is based on the stereo matching, where the geometric constraints are built on the left and right images; the other is based on monocular sequences, where the geometric constraints are built on adjacent frames. Compared with the supervised learning methods, unsupervised learning methods don't need GT depth maps, which reduces the cost of building depth labels yet suffer from lower accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3.">Semi-supervised learning methods</head><p>In order to effectively utilize a large amount of relatively cheap unlabeled data to improve learning performance, researchers have proposed the semi-supervised learning methods, which introduces other information, such as synthetic data, surface normals, and LIDAR, as the semi-supervised learning manners to reduce the model's dependence on GT depth maps, which enhance the scale consistency and improve estimated accuracy of depth maps.</p><p>3.1.3.1. Combined with synthetic data. The synthetic data generated by the graphics engine provides a possible solution for collecting a large amount of depth data. Thus, researchers introduce synthetic datasets with depth labels to monocular depth estimation. How to overcome the domain gap between synthetic and real data is a challenge during training <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b118">118]</ref>.</p><p>With the development of image style transfer and its connection with domain adaptation, researchers adopted the style transfer and adversarial training to estimate depth maps in real scenes <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b103">103]</ref>, which relied on the models trained with a large amounts of synthetic data, as shown in Fig. <ref type="figure" target="#fig_11">14</ref>. The depth estimation network is trained with synthetic images and corresponding GT depth maps. During the test stage, the trained network is applied directly to predict the depth maps from real RGB images with transfer learning to minimize the gap between the real and synthetic domain.</p><p>DispNet <ref type="bibr" target="#b93">[94]</ref> was the first network that introduced image style transfer for depth estimation. It utilized a large comprehensive synthetic dataset to train, and fine-tuned the model on the less available GT data. Based on the DispNet, Zheng et al. <ref type="bibr" target="#b193">[192]</ref> proposed a two-module domain adaptive network, T 2 Net, where one module was trained with synthetic and real images and reconstructed each other with the reconstruction loss and generative adversarial loss <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b38">39]</ref>, and these outputs were input into the other module to predict the real depth maps. Besides, there are more models with self-attention <ref type="bibr" target="#b192">[191]</ref>, cycle consistency <ref type="bibr" target="#b190">[189]</ref>, cross-domain <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b140">140,</ref><ref type="bibr" target="#b141">141]</ref>, and others for domain adaptation to predict monocular depth maps.</p><p>Domain adaptation methods can successfully solve the domain difference of the deep end-to-end disparity estimation network. However, when the illumination or the saturation of the style transfer changes suddenly, the accuracy of the estimated depth map will decrease accordingly.</p><p>3.1.3.2. Combined with LIDAR. Researchers also adopt auxiliary depth sensors to capture GT information, such as LIDAR, for monocular depth estimation <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b110">110]</ref>. Auxiliary depth sensors cause some noises and the measured depth values are usually sparser than GT depth maps. The general model for monocular depth estimation with LIDAR is shown in Fig. <ref type="figure" target="#fig_18">15</ref>. The depth network learns not only structure features but also depth and noise from sparse data captured by LIDAR, where the entire mode needs to add the depth consistency constraint built on the sparse data and estimated depth map as follow:</p><formula xml:id="formula_4">L depth ðpÞ ¼ X p jjDðpÞ À ZðpÞjj 1<label>ð5Þ</label></formula><p>where p is the depth pixel, DðpÞ is the estimated depth map, and ZðpÞ is the sparse data from LIDAR. Kuznietsov et al. <ref type="bibr" target="#b64">[65]</ref> proposed a semi-supervised learning network for monocular depth estimation with sparse data, which input left and right images to the model and built a stereo alignment as a geometric constraint. Thus, the depth consistency losses include two parts: one is the error between the left estimated depth map and sparse data, and the other is the error between the right estimated depth map and sparse data. Experiments proved that the added sparse data did improve the performance than supervised and unsupervised methods <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b82">83]</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3.3.">Combined with surface normal.</head><p>There are still some features with similar information to depth extracted from the input RGB image, which contribute to predict the depth maps more accurately and conveniently, e.g. surface normal.</p><p>There is a strong correlation between the surface normal and the depth: the surface normal is determined by the local tangent plane of the 3D point, which can be estimated from the depth; the depth is constrained by the local tangent plane determined by the surface normal. The general model for monocular depth estimation combined with surface normal estimation is shown in Fig. <ref type="figure" target="#fig_13">16</ref>. Qi et al. <ref type="bibr" target="#b109">[109]</ref> proposed the GeoNet, which consists of a depth-to-normal network exploiting the least square solution of the surface normal from depth and a normal-to-depth network refining the initial depth map in a kernel regression module. They took the advantage of the theory that surface normals change less in local plane to refine monocular depth estimation, where the specific derivation process could be found in Reference <ref type="bibr" target="#b109">[109]</ref>. Furthermore, there are some models with depth-normal consistency <ref type="bibr" target="#b110">[110,</ref><ref type="bibr" target="#b168">167]</ref>, surface regularized constraints <ref type="bibr" target="#b153">[152,</ref><ref type="bibr" target="#b188">187]</ref>, and depth completion <ref type="bibr" target="#b185">[184]</ref>, for monocular depth estimation combined with surface normal estimation.</p><p>Summary. Semi-supervised learning methods for monocular depth estimation relies on auxiliary information, such as virtual data, sparse depth, and surface normals, apart from learning the depth features from the RGB image, which makes the depth map more accurate than that estimated in unsupervised learning methods. Although auxiliary information is easier to obtain than GT depth maps, it still increases the amount of input data and the dependence of depth estimation on it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4.">Summary</head><p>This section mainly reviews and summarizes the deep learning methods for monocular depth estimation from the networks training manners, including: supervised, unsupervised, and semisupervised learning methods. Supervised learning methods for monocular depth estimation have the highest accuracy, yet strong dependence on GT depth maps; unsupervised learning methods build geometric constraints on the input images to predict depth maps without supervision, but its accuracy is slightly inferior to supervised learning and semi-supervised learning methods, where scale ambiguity, occlusion, and other problems need to be overcome; semi-supervised learning methods depend on auxiliary information, which are easier to obtain than GT depth maps. The summaries for different learning manners are concluded in Table <ref type="table" target="#tab_0">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Tasks</head><p>From the perspective of task types, deep learning methods for monocular depth estimation can be divided into two categories. On the one hand, we can train a single network only for depth estimation, that is single-task learning; on the other hand, we can combine depth estimation with other related tasks to learn together for the features projection and improve the depth estimation performance, that is multi-task learning. This section will review the two aspects of single-task learning and multi-task learning methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Single-task learning methods</head><p>The core of the single-task learning methods is to construct an association model between the RGB image and the depth map, that is, the model is learned from the RGB image, and recover the depth value. According to whether the depth value returned by the network is continuous or not, single-task learning methods can be divided into regression methods and classification methods.   According to the deep learning model used, it can be divided into CNN-based <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b75">76]</ref>, RNN-based <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b163">162,</ref><ref type="bibr" target="#b177">176]</ref>, and GANbased regression methods <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b192">191]</ref>. Zhang et al. <ref type="bibr" target="#b189">[188]</ref> proposed an end-to-end progressive hard mining network (PHN) to regress depth maps, in which an intra-scale module restored the depth information, an inter-scale module fused the depth cues, and a hard-mining refinement module constrained the recursive refining and reduced error propagation to fully learn boundaries of different scales and estimate depth maps in regression.</p><p>Ideally, the estimated depth values should be continuous. However, regression methods for monocular depth estimation are usually faced with more complex network structures and constraint functions. Therefore, some researchers began to discretize the depth values and introduced the classification methods to learn depth maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.2.">Classification methods.</head><p>Depth estimation and semantic segmentation are similar, and both are pixel-level predictions. Taking into account the characteristics of the scene from far to near, classification is also used to estimate monocular depth maps, as shown in Fig. <ref type="figure" target="#fig_14">17</ref>. Firstly, the continuous depth values are discretized. Then, the depth estimation network learns the corresponding classification labels for discretized depth values and regresses segmented depth maps. Finally, these segmented depth maps are combined into the final depth map.</p><p>There are several deep learning models in classification for monocular depth estimation, such as full convolutional models <ref type="bibr" target="#b10">[11]</ref>, residual models <ref type="bibr" target="#b73">[74,</ref><ref type="bibr" target="#b116">116,</ref><ref type="bibr" target="#b134">134]</ref>, and ordinal classification models <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b86">87]</ref>. Fu et al. <ref type="bibr" target="#b32">[33]</ref> put forward a deep ordered classification network to estimate monocular depth maps. It performed linear sampling on the depth value in logarithmic space, and arranged all categories in descending order according to the distance relationship, where the discrete depth values were used for ordered regression network training. Experiments proved that treating depth estimation as a regression problem might lead to larger errors in areas too far or too close to the camera, while treating as a classification problem could effectively avoid a relatively large error for predicting a larger depth value.</p><p>Summary. Single-task learning methods for monocular depth estimation mainly include regression and classification methods, where the regression methods directly returns continuous depth values, and the classification methods discretize the depth values firstly and then regress those in piecewise. However, the network and constraint functions of the regression are becoming more and more complex, and it is easy to cause local minima; and the classification method has a strong dependence on the discretization form and weight setting, otherwise the loss will be increased.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Multi-task learning methods</head><p>In order to make full use of the complementarity of the depth and other features, researchers have proposed to design a unified framework for joint multi-task training, and the features extracted from different tasks are projected to each other to enhance the final depth map. This section introduces the depth estimation methods combined with semantic segmentation in monocular images and the methods combined with visual odometry, optical flow estimation, and others in monocular videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.2.2.1.</head><p>Combined with semantic segmentation. Scene perception includes many aspects, where depth information describes the geometric relationship in space, and the semantic information represents the entity meaning of different parts in the scene <ref type="bibr" target="#b89">[90,</ref><ref type="bibr" target="#b106">106,</ref><ref type="bibr" target="#b172">171]</ref>. These tasks share similar context information <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b78">79]</ref>. Many works have been proposed to combine semantic segmentation with depth estimation, processing data under the same neural network <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b98">98,</ref><ref type="bibr" target="#b113">113,</ref><ref type="bibr" target="#b187">186]</ref>.</p><p>The model for monocular depth estimation and semantic segmentation consists of one encoder network and two decoder networks for depth regression and semantic labels prediction, where these two decoder networks share weights, as shown in Fig. <ref type="figure" target="#fig_16">18</ref>.</p><p>During training, we can train only one or two-both tasks at the same time. The shared encoder learns feature maps from the input, yet two decoders with shared weights to recover depth maps and semantic segmentations, respectively. Furthermore, the whole model is constrained by the attention guidance from context information, and the predicted results will be back-propagation to update network parameters and optimize the results.</p><p>Eigen et al. <ref type="bibr" target="#b27">[28]</ref> were the first to unify the three tasks of depth, surface normal, and semantic annotation. Based on that, more and more methods have been proposed for monocular depth estimation with semantic segmentation. Atapour-Abarghouei et al. <ref type="bibr" target="#b5">[6]</ref> considered depth estimation as a supervised image-to-image translation problem with a generative network and applied adversarial learning to force the model to select a mode to overcome the multi-modal problem resulting in blurry outputs. For semantic segmentation, they applied a fully supervised generative network  trained with cross-entropy loss functions. What's more, models with self-attention <ref type="bibr" target="#b54">[55]</ref>, instance segmentation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b150">150]</ref>, multiscale learning <ref type="bibr" target="#b100">[100]</ref>, guidance manner <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b42">43]</ref>, and others <ref type="bibr" target="#b80">[81,</ref><ref type="bibr" target="#b188">187]</ref> are proposed to estimate monocular depth combined semantic segmentation. Experiments proved that the addition of semantic information did increase the accuracy of monocular depth estimation. Monocular depth estimation combined with semantic segmentation can take advantage of the context information of the scene, overcoming problems such as object boundaries blur and improving the accuracy of the predicted depth maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.2.">Combined with others.</head><p>In addition to combining with semantic segmentation tasks, depth estimation based on monocular video is often combined with other tasks, such as visual odometry <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b180">179]</ref> and optical flow estimation <ref type="bibr" target="#b170">[169,</ref><ref type="bibr" target="#b174">173]</ref>.</p><p>Visual odometry is similar to the images transformation estimation and accurate camera pose estimation contributes to image reconstruction and further helps depth estimation <ref type="bibr" target="#b169">[168,</ref><ref type="bibr" target="#b191">190,</ref><ref type="bibr" target="#b196">195]</ref>. However, most early methods only consider static scenes, which are no longer applicable in the dynamic scene actually. Because there are usually dynamic objects in real scenes, such as cars and pedestrians. In order to better estimate the depth maps of the dynamic scene, researchers have introduced optical flow estimation into monocular depth estimation. Optical flow estimation can capture motion information in the scene, which contributes to the monocular depth estimation of dynamic scenes <ref type="bibr" target="#b91">[92]</ref>.</p><p>Based on the combination with visual odometry and optical flow estimation, there are a large quantity of works dealing with dynamic objects in the scene and the problems of occlusion and motion blur <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b112">112,</ref><ref type="bibr" target="#b154">153]</ref>. The general model of monocular depth estimation with visual odometry and flow estimation is shown in Fig. <ref type="figure" target="#fig_17">19</ref>, which usually consists of multiple sub-networks and each sub-network performs a different task. All tasks are jointly trained and the estimation of each task project and promote each other.</p><p>For dynamic objects and occlusion, Godard et al. <ref type="bibr" target="#b36">[37]</ref> proposed an automatic occlusion method, Monodepth2, which minimized photometric error to reduce the artifacts at the object boundary, and improved the sharpness of the occlusion boundary. At the same time, they put forward an auto-masking method to filter out some pixels that didn't change in appearance when dynamic objects moved at the same speed as the camera in the scene. Moreover, there are some methods dealing dynamic objects with object masks <ref type="bibr" target="#b147">[147]</ref>, object motion estimation <ref type="bibr" target="#b11">[12]</ref>, flow consistency <ref type="bibr" target="#b97">[97,</ref><ref type="bibr" target="#b154">153]</ref>, displacement field <ref type="bibr" target="#b112">[112]</ref>, etc.</p><p>In addition to combining visual odometry and optical flow estimation, there are some works that combine features estimation for further pixel-level depth maps estimation <ref type="bibr" target="#b129">[129,</ref><ref type="bibr" target="#b133">133,</ref><ref type="bibr" target="#b175">174]</ref>. For example, Spencer et al. <ref type="bibr" target="#b133">[133]</ref> proposed an unsupervised network framework, DeFeat-Net, that could simultaneously learn monocular depth, dense feature representation, and self-motion. It was robust and could work in many challenging environments, such as changing weather and light conditions, with established pixelwise loss functions <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b132">132]</ref>.</p><p>Summary. Multi-task learning methods for monocular depth estimation usually predict depth maps with other tasks, such as semantic segmentation, visual odometry, and scene optical flow estimation. By capturing features related to depth information in the scene, the accuracy of depth estimation is improved and the scene understanding is enhanced. However, there are still many challenges in multi-task learning that need to be overcome, such as limited datasets with semantic labels or missing labels, motion blur and occlusion caused by dynamic objects in the scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Summary</head><p>This section mainly reviews and summarizes the deep learning methods for monocular depth estimation based on the task types, including single-task learning and multi-task learning methods. Single-task learning methods usually estimate monocular depth maps in regression or classification manner, distinguished from whether the returned depth values are continuous or discrete. Multi-task learning methods usually combine depth estimation with semantic segmentation, camera pose, and scene flow estimation, which are trained jointly and interact with each other. The summaries for different learning tasks are concluded in Table <ref type="table">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Datasets and metrics</head><p>This section introduces the datasets and evaluation metrics of deep learning models for monocular depth estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>There are a number of datasets for monocular depth estimation, with different types and depth ranges between indoor and outdoor scenes. This section introduces some common datasets in deep learning methods for monocular deep estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">KITTI</head><p>KITTI dataset <ref type="bibr" target="#b34">[35]</ref> is an outdoor dataset for monocular deep estimation and object detection and tracking based on deep learning, which is jointly developed by Karlsruhe Institute of Technol-  ogy in Germany and Toyota Institute of Technology in the United States, as shown in Fig. <ref type="figure" target="#fig_20">20(a)</ref>. KITTI dataset is captured through a car equipped with 2 high-resolution color cameras, 2 gray-scale cameras, laser scanner and global positioning system (GPS), whose maximum measuring distance is 120 m. The dataset contains a total of 93,000 RGB-D training samples, including five categories: ''Road", ''City", ''Residential", ''Campus", and ''Person", from the city of Karlsruhe, the wild area and the highway. The original image size of KITTI is 1,242 Â 375, and its ground-truth depth maps are sparse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">NYU depth V2</head><p>NYU Depth V2 dataset <ref type="bibr" target="#b130">[130]</ref> is an indoor dataset for monocular depth estimation based on deep learning, which is provided by Silbereman et al. at the New York University. NYU Depth V2 dataset contains 407,024 frames of RGB-D image pairs captured by a Red-Green-Blue (RGB) camera and the Microsoft Kinect depth camera to simultaneously collect the RGB and depth information of 464 different indoor scenes. The original image size of NYU Depth V2 is 640 Â 480 and the depth of the dataset ranges from 0.5 m to 10 m. Due to the positional deviation between the RGB and the depth camera, the original depth maps contain missing parts or noises. Therefore, authors select 1,449 images from the dataset and use the coloring algorithm <ref type="bibr" target="#b72">[73]</ref> to fill and obtain dense depth maps, which are manually labeled with the semantic information. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3.">Make3D</head><p>Make3D dataset <ref type="bibr" target="#b123">[123,</ref><ref type="bibr" target="#b124">124]</ref> is another outdoor dataset for monocular depth estimation based on deep learning, which is constructed by <ref type="bibr">Saxena</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4.">Virtual datasets</head><p>The above datasets, KITTI, NYU Depth V2, and Make3D, are all collected from real scenes. There are some virtual datasets generated by computers, such as SceneNet RGB-D dataset <ref type="bibr" target="#b95">[95]</ref>, and SYNTHIA dataset <ref type="bibr" target="#b121">[121]</ref>. These virtual datasets include various scene types under different weather, environment, and lighting conditions. The appropriate dataset should be selected according Table <ref type="table">3</ref> A summary of the single-task and multi-task learning methods for monocular estimation, where multi-task learning methods include depth estimation with semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models Descriptions Remarks Papers</head><p>Single-task Fig. <ref type="figure">7</ref> Only perform a single-task of monocular depth estimation. Predicting monocular depth maps by regression or classification. <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b86">87,</ref><ref type="bibr" target="#b189">188]</ref> Depth with semantic segmentation Fig. <ref type="figure" target="#fig_16">18</ref> Adopting the complementarity between depth information and semantic information for multi-task learning.</p><p>The accuracy of depth estimation is improved by applying context information. <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b54">55]</ref> Depth with others Fig. <ref type="figure" target="#fig_17">19</ref> Using inter-frames geometric constraints and image reconstruction to learn multi-task estimations.</p><p>No need for GT, but problems with scale blur, reprojection, dynamic blur, and occlusion. <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b133">133,</ref><ref type="bibr" target="#b170">169,</ref><ref type="bibr" target="#b195">194]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Metrics</head><p>Evaluation metrics proposed by Eigen et al. <ref type="bibr" target="#b28">[29]</ref> is adopted to evaluate and compare the performance of depth estimation methods. Evaluation metrics include error and accuracy metrics. The error metrics (smaller is better) include absolute relative error (Abs.rel), square relative error (Sq.rel), root mean square error (RMSE), and the logarithm root mean square error (log RMS); the accuracy rate metrics (the bigger the better) include d &lt; 1:25 t , where t = 1,2,3. These metrics are formulated as: ;</p><formula xml:id="formula_5">RMS : ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi 1 T X i2T jjd i À d gt i jj 2 s ð6Þ log RMS : ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi 1 T X i2T jjlog d i ð Þ À log d gt i jj 2 s<label>ð7Þ</label></formula><formula xml:id="formula_6">abs: relative : 1 T X i2T d i À d gt i d gt i<label>ð8Þ</label></formula><formula xml:id="formula_7">d gt i d i ! ¼ d &lt; thr<label>ð10Þ</label></formula><p>where d i and d gt i are the predicted and ground-truth depth respectively at the pixel indexed by i, and T is the total number of pixels in all the evaluated images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Analysis and comparisons</head><p>In order to evaluate and compare these monocular depth estimation methods based on deep learning, we adopted the publicly available pre-trained networks trained or tested on KITTI <ref type="bibr" target="#b34">[35]</ref> dataset. Table <ref type="table">4</ref> illustrates some properties of the deep learning methods, including year, supervision, main contributions, tasks, and training data. The performance comparison of various methods is listed in Table <ref type="table">5</ref>, including error metrics and accuracy metrics. We don't describe the properties and performance of all the methods mentioned above, but only summarize some representative models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Challenges and trends</head><p>Over the past several years, monocular depth estimation based on deep learning has been extensively researched and developed. However, there are still some limitations needed to be overcome.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 4</head><p>Properties of the deep learning methods for monocular depth estimation. ''Sup." is ''S" representing the supervised, ''U" representing the unsupervised, and ''Semi" representing the semi-supervised method. ''Data" is the training data, where ''RGB-D" means RGB and depth maps, ''Stereo" means stereo images, ''Mono.seq" means monocular sequences, and ''Stereo.seq" means stereo sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Papers</head><p>Year Sup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Main contributions Tasks Data</head><p>Eigen <ref type="bibr" target="#b28">[29]</ref> 2014 S Coarse-to-fine, CNN Depth RGB-D Eigen <ref type="bibr" target="#b27">[28]</ref> 2015 S Multi-scale, CNN. depth, normal, semantic annotation RGB-D, semantic labels Zoran <ref type="bibr" target="#b198">[197]</ref> 2015 S Relative dense depth, numerical optimization, residual network Depth RGB-D Laina <ref type="bibr" target="#b68">[69]</ref> 2016 S BerHu loss, residual network Depth RGB-D Li <ref type="bibr" target="#b75">[76]</ref> 2017 S Two-stream framework, depth-gradient fusion, CNN Depth RGB-D Xu <ref type="bibr" target="#b164">[163]</ref> 2018 S Cascade-CRFs, attention model Depth RGB-D Mancini <ref type="bibr" target="#b92">[93]</ref> 2017 S Convolution + LSTM Depth Mono.seq + depth Kumar <ref type="bibr" target="#b63">[64]</ref> 2018 S ConvLSTM Depth Mono.seq + depth Jung <ref type="bibr" target="#b57">[58]</ref> 2017 S GAN, global-to-local Depth RGB-D Garg <ref type="bibr" target="#b33">[34]</ref> 2016 U Image reconstruction, CNN Depth Stereo Godard <ref type="bibr" target="#b35">[36]</ref> 2017 U Left-right photometric and disparities consistency, disparity smoothness loss Depth Stereo.seq Zhou <ref type="bibr" target="#b195">[194]</ref> 2017 U Reconstruction and photometric consistency loss Depth, camera pose Mono.seq Chang <ref type="bibr" target="#b13">[14]</ref> 2018 U Spatial pyramid pooling module, 2D + 3D CNN Depth Stereo Zhou <ref type="bibr" target="#b194">[193]</ref> 2018 U Bundle adjustment, super-resolution, clip loss Depth, camera pose Mono.seq Goldman <ref type="bibr" target="#b37">[38]</ref> 2019 U Siamese network, geometric consistency Depth Stereo Guizilini <ref type="bibr" target="#b41">[42]</ref> 2020 U 3D packing, SfM-based Depth, camera pose Mono.seq Poggi <ref type="bibr" target="#b107">[107]</ref> 2020 U Depth uncertainty estimation Depth Mono.seq Zheng <ref type="bibr" target="#b193">[192]</ref> 2018 Semi Domain adaptive, GAN Depth Synthetic RGB-D Zhao <ref type="bibr" target="#b190">[189]</ref> 2019 Semi Domain adaptive, cycle consistency, GAN Depth Synthetic RGB-D Kuznietsov <ref type="bibr" target="#b64">[65]</ref> 2017 Semi LIDAR, stereo geometric constraint Depth Stereo, sparse GT Qiu <ref type="bibr" target="#b110">[110]</ref> 2019 Semi LIDAR, binary mask, attenetion map Depth, normal RGB, sparse GT Qi <ref type="bibr" target="#b109">[109]</ref> 2018 Semi Normal-to-depth, depth-normal consistency Depth, normal RGB Zhang <ref type="bibr" target="#b188">[187]</ref> 2019 Semi Cross-task, affinity learning Depth, normal, semantic segmentation RGB, semantic labels Dos <ref type="bibr" target="#b26">[27]</ref> 2019 Semi Sparse-to-Continuous, Hilbert maps <ref type="bibr" target="#b114">[114]</ref>, occupancy map Depth RGB, sparse GT Zhang <ref type="bibr" target="#b189">[188]</ref> 2018 S Progressive hard mining network, learning multi-scale boundaries Depth RGB-D Fu <ref type="bibr" target="#b32">[33]</ref> 2018 S Ordered regression Depth RGB-D Liu <ref type="bibr" target="#b86">[87]</ref> 2020 S ConvLSTM, ordinal classification Depth Mono.seq Atapour <ref type="bibr" target="#b5">[6]</ref> 2019 U Temporally consistency, depth completion, GAN Depth, flow, semantic segmentation Mono.seq Wang <ref type="bibr" target="#b150">[150]</ref> 2020 1) In order to improve the accuracy, researchers deepen the layers of the deep neural networks, which increases the memory usage and space complexity.</p><p>2) In multi-task learning, deep learning methods for monocular depth estimation always apply multiple sub-networks or submodules to process different sub-tasks, which increases the amount of calculation and memory consumption.</p><p>3) Monocular depth estimation networks usually are encodingdecoding networks. After multiple layers of information processing, the depth features are severely lost, which leads to the low-accuracy estimated depth maps and cannot meet the requirements of practical applications.</p><p>In this section, this paper summarizes the key challenges and looks at the directions for future research of monocular depth estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Integration and optimization of the network framework</head><p>In many supervised learning models, semantic segmentation will be added with depth estimation, but it is still an independent module that handles independent tasks. In the unsupervised learning methods, there are generally multiple sub-networks which are able to learn depth estimation, visual odometry, and flow estimation, respectively. However, these networks are not well connected, which leads to a large number of parameters increasing the memory requirements and calculations. How to better integrate the network is a research direction and is worth exploring in the future.</p><p>We can obtain different features at the same time by using the same deep learning network, such as semantic information, optical flow features, and depth features. In the encoding stage, different types of features are extracted and matched at the same time; in the decoding stage, they are decoded separately to meet the application requirements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Datasets construction</head><p>The quality of datasets largely determines the generalization ability and robustness of the deep learning model. In order to improve the results of depth estimation, more data, with better quality and more scene types, is needed. However, these existing datasets used for depth estimation are relatively limited, and the construction of a new dataset is time-consuming and expensive. At present, some researchers utilize computers to generate a large number of images for depth estimation, but the quality is uneven. How to construct a dataset for monocular depth estimation that meets deep learning is a future research direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Dynamic objects and occlusion problems</head><p>Realistic scenes are usually complicated, such as containing a large number of moving objects, occlusions, illumination changes, weather changes. However, most of the existing depth estimation models only consider the ideal conditions. Although some researchers have begun to deal with dynamic objects and occlusion scenes and have made some progress recently, how to better estimate the depth of complex scenes to meet practical applications is still a very challenging task, which is an important future research direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">High-resolution depth map output</head><p>Depth estimation is a fundamental step for practical applications such as augmented reality (AR) and virtual reality (VR), and it has a high demand for the accuracy and resolution of the depth map. However, the resolution of the depth predicted by most of the current depth estimation models is usually low, for the purpose of improving calculation efficiency. At present, researchers have used color image super-resolution models <ref type="bibr" target="#b76">[77,</ref><ref type="bibr" target="#b79">80,</ref><ref type="bibr" target="#b108">108]</ref> to refine the super-resolution of depth maps <ref type="bibr" target="#b104">[104,</ref><ref type="bibr" target="#b120">120,</ref><ref type="bibr" target="#b161">160]</ref>. But how to directly output the high-resolution depth map is still a direction that needs to be studied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 5</head><p>Evaluation on KITTI dataset and best result is emboldened and bolded. The slower of the error metrics, the better; and the higher of the accuracy metrics, the better. ''Sup." is ''S" representing a supervised method, ''U" representing an unsupervised method, and ''Semi" representing a semi-supervised method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Real-time performance</head><p>Image depth estimation is the basic module of SLAM, which is closely integrated with industrial applications, such as autonomous driving. Therefore, practical applications have high requirements for the real-time performance of depth estimation. However, in order to obtain higher accuracy, researchers often construct deeper networks, with more parameters and more constraints, to perform depth estimation, which requires more calculation time and thus cannot meet the real-time requirements of practical applications. Therefore, how to apply a lighter network for real-time estimation while ensuring the accuracy of prediction is a future research direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>Monocular depth estimation plays an important role in scene understanding and high-accuracy depth maps are beneficial to the realization of multiple applications. This paper introduces related deep learning models and summarizes deep learningbased monocular depth estimation algorithms, from training manners to task types. Furthermore, this paper also summarizes the properties and performance of these monocular depth estimation methods. Finally, this paper identifies the potential challenges and suggests some future research directions of the monocular depth estimation based on deep learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. The evolution of depth estimation. This paper divides the development of depth estimation into three periods: the early period, the machine learning period, and the deep learning period, where the depth estimation method of monocular image based on deep learning is mainly surveyed and summarized.</figDesc><graphic coords="2,97.12,67.92,397.05,279.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. (a) The basic structure of RNN, where S is the internal status and the memory of the cell, I is the input, O is the output, and (U; V; W) is the sharing parameters of the cell. (b) The basic structure of LSTM [50].</figDesc><graphic coords="3,42.58,67.92,241.21,305.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>3. 1</head><label>1</label><figDesc>.1.1. CNNs-based methods. Researchers have designed CNNbased monocular depth estimation networks to learn depth features layer by layer through their convolution kernels and recover depth maps by deconvolution to meet the requirements of scene understanding. This section introduces two aspects based on the absolute depth or relative depth learned from monocular images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The general GAN-based framework for supervised monocular depth estimation.</figDesc><graphic coords="4,142.40,78.65,306.37,103.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The architecture of multi-scale network for monocular depth estimation proposed by Eigen et al. [29]. The top module is the coarse network for coarse estimation and the bottom module is the fine network for refined depth map.</figDesc><graphic coords="4,97.12,227.34,397.65,200.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. The general pipeline of deep learning for monocular depth estimation. The left module is encoder network learning depth features layer-by-layer, and the decoder network in the right module recovers the depth map.</figDesc><graphic coords="4,97.12,475.48,397.12,155.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .Fig. 7 .</head><label>67</label><figDesc>Fig.6. The overall diagram of deep learning methods for monocular depth estimation. According to whether the network is trained with GT, these deep learning methods are divided into supervised, unsupervised, and semi-supervised learning models; according to the types of network prediction task, these methods are classified into single-task and multi-task learning methods.</figDesc><graphic coords="5,97.12,67.92,397.13,404.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig.8. The general model of supervised methods with CRF for monocular depth estimation, where each depth network with fixed and shared weights learns from each pair of patches.</figDesc><graphic coords="6,139.58,552.41,313.03,168.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. There are two general architectures of RNN-based methods for monocular depth estimation. In (a), the encoder is constructed by all LSTM (or ConvLSTM) layers, yet (b) is composed of convolution and LSTM (or ConvLSTM) layers.</figDesc><graphic coords="7,133.94,67.91,328.64,193.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 .Fig. 11 .</head><label>1011</label><figDesc>Fig. 10. The principle of stereo matching methods for depth estimation, where IðLÞ and IðRÞ are stereo pair-wise images taken by the left and the right cameras, respectively.</figDesc><graphic coords="8,42.52,67.92,241.32,180.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 12 .Fig. 13 .</head><label>1213</label><figDesc>Fig. 12. The general model based on unsupervised 2D with 3D CNNs for monocular depth estimation, where the weights of these two 2D CNNs are shared and the cost volume is constrained with context information to mapping the depth map.</figDesc><graphic coords="9,139.64,67.92,312.25,110.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. The general model of domain adaptive methods for monocular depth estimation combined with synthetic data, where the network in test stage is trained on synthetic data with GT in training stage.</figDesc><graphic coords="9,42.58,425.65,241.19,171.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>3. 2 . 1 . 1 .Fig. 15 .</head><label>21115</label><figDesc>Fig. 15. The general model for monocular depth estimation with LIDAR, where the sparse depth is captured by LIDAR.</figDesc><graphic coords="10,44.66,67.91,237.19,102.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 16 .</head><label>16</label><figDesc>Fig. 16. The general model for monocular depth estimation combined with surface normal estimation, where the normal-to-depth module is depended on the geometric relationship between the depth and normal.</figDesc><graphic coords="10,42.63,234.14,241.23,112.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 17</head><label>17</label><figDesc>Fig. 17. The general model of classification methods for monocular depth estimation, where the discretization module discretizes continuous depth values, and the mapping module combines the segmented depth maps into the final depth map.</figDesc><graphic coords="11,82.94,532.52,425.65,197.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>.</head><label></label><figDesc>Fig. 17. The general model of classification methods for monocular depth estimation, where the discretization module discretizes continuous depth values, and the mapping module combines the segmented depth maps into the final depth map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 18 .</head><label>18</label><figDesc>Fig. 18. The general model for monocular depth estimation combined with semantic segmentation, where the shared encoder captures the scene structure features and two separate decoders perform semantic segmentation and depth regression respectively.</figDesc><graphic coords="12,42.46,67.92,241.38,139.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 19 .</head><label>19</label><figDesc>Fig. 19. The general model for monocular depth estimation combined with visual odometry and flow estimation, which includes three sub-networks: the depth network, the flow network, and the pose network for depth, scene flow, and camera pose estimation, respectively.</figDesc><graphic coords="12,311.41,67.92,241.35,130.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>The 1 ,</head><label>1</label><figDesc>449 samples are divided into 795 training samples and 654 test samples. Some samples of NYU Depth V2 dataset are shown in Fig. 20(b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head></head><label></label><figDesc>et al. in Stanford University. Make3D dataset includes daytime city and natural scenery, with depth maps being collected by a laser scanner. The depth ranges from 5 m to 81 m, and the range larger than that is uniformly mapped to 81 m. This dataset contains a total of 534 RGB-D image pairs, 400 of which are used for training and 134 are used for testing. The original resolution of the RGB image is 2,272 Â 1,704, and the resolution of the depth map is 55 Â 305 pixels. Some samples of Make3D dataset are shown in Fig. 20(c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Fig. 20 .</head><label>20</label><figDesc>Fig. 20. Samples of monocular depth estimation datasets. (a) is KITTI dataset [35], (b) is NYU Depth V2 dataset [130], (c) is Make3D dataset [123,124], and (d) is SceneNet RGB-D dataset [95] (the left images are RGB images and the right are the ground-truth depth maps).</figDesc><graphic coords="13,125.46,203.70,340.39,245.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2</head><label>2</label><figDesc>A summary of the deep learning methods for monocular estimation in supervised, unsupervised, and semi-supervised learning manners.</figDesc><table><row><cell>Methods</cell><cell>Models</cell><cell>Descriptions</cell><cell>Remarks</cell><cell>Papers</cell></row><row><cell>Supervised</cell><cell>Fig. 7</cell><cell>GT depth maps are used as the</cell><cell>High precision, simple framework,</cell><cell>[29,33,69,163]</cell></row><row><cell></cell><cell></cell><cell>supervision signal of the deep</cell><cell>yet heavy dependence on GT.</cell><cell></cell></row><row><cell></cell><cell></cell><cell>learning network.</cell><cell></cell><cell></cell></row><row><cell>Unsupervised</cell><cell>Fig. 11</cell><cell>Using epipolar geometric constraints</cell><cell>GT is not required, but there are</cell><cell></cell></row><row><cell></cell><cell></cell><cell>instead of GT as the supervision.</cell><cell>problems such as scale blur, dynamic</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>blur, and occlusion.</cell><cell></cell></row></table><note><p><p><p><p><p><p><p><p><p><ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b37">38]</ref> </p>Semi-supervised Figs.</p><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref> </p>Relying on virtual data, sparse depth, surface normal and other auxiliary information.</p>Heavy dependence on the auxiliary information.</p>[65,</p><ref type="bibr" target="#b109">109,</ref><ref type="bibr" target="#b193">192]</ref> </p>Y. Ming, X. Meng, C. Fan et al.</p>Neurocomputing 438 (2021) 14-33</p></note></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>q The work presented in this paper was partly supported by Natural Science Foundation of China (Grant No. 62076030), Beijing Natural Science Foundation of China (Grant No. L201023, and No. L182033) and the Fundamental Research Funds for the Central Universities (2019PTB-001).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CRediT authorship contribution statement</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Declaration of Competing Interest</head><p>The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. Chunxiao Fan is currently a professor and the director of Center for information electronic and intelligence system. She served as a member of ISO/IEC JTC1/SC6 WG9, ASN.1 (since 2006) and Chinese Sensor network working group. She also was elevated to evaluation expert of Beijing Scientific and Technical Academy Awards. Her research interests include Heterogeneous media data analysis, Internet of Things, data mining, communication software and so on. In recent years, she is director of several Nation Science Foundation Project. She has published more than 30 papers in international journals and conferences, authored and edited three books and has authorized several patent for invention.</p><p>Hui Yu is a Professor with the University of Portsmouth, UK. His research interests include methods and practical development in visual computing, machine learning and AI with the applications focusing on human-machine interaction, multimedia, virtual reality and robotics as well as 4D facial expression generation, perception and analysis. He serves as an Associate Editor for IEEE Transactions on Human-Machine Systems and Neurocomputing journal.</p><p>Y. Ming, X. Meng, C. Fan et al.</p><p>Neurocomputing 438 (2021) 14-33</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Survey on deep neural networks in speech and vision systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Samad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vidyaratne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Glandon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Iftekharuddin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">417</biblScope>
			<biblScope unit="page" from="302" to="321" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ganvo: unsupervised deep monocular visual odometry and depth estimation with generative adversarial networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Almalioglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R U</forename><surname>Saputra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>De Gusmao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Markham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Trigoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5474" to="5480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Enhancing self-supervised monocular depth estimation with traditional visual odometry</title>
		<author>
			<persName><forename type="first">L</forename><surname>Andraghetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Myriokefalitakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Dovesi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Luque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pieropan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mattoccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="424" to="433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wasserstein</forename><surname>Gan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Real-time monocular depth estimation using synthetic data with domain adaptation via image style transfer</title>
		<author>
			<persName><forename type="first">A</forename><surname>Atapour-Abarghouei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2800" to="2810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Veritatem dies aperit-temporally consistent depth prediction enabled by a multi-task geometric and semantic scene understanding approach</title>
		<author>
			<persName><forename type="first">A</forename><surname>Atapour-Abarghouei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3373" to="3384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Surf: speeded up robust features</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="404" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Monocular depth estimation: a survey</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bhoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.09402</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Image classification using random forests and ferns</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bosch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Munoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 11th International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007">2007. 2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A deep domain adaption model with multi-task networks for planetary gearbox fault diagnosis</title>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">409</biblScope>
			<biblScope unit="page" from="173" to="190" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Estimating depth from monocular images as classification using deep fully convolutional residual networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="3174" to="3182" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Depth prediction without the sensors: leveraging structure for unsupervised learning from monocular videos</title>
		<author>
			<persName><forename type="first">V</forename><surname>Casser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pirk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8001" to="8008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Interpreting recurrent neural networks behaviour via excitable network attractors</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ceni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ashwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Livi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cogn. Comput</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="330" to="356" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pyramid stereo matching network</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5410" to="5418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On the over-smoothing problem of cnn based disparity estimation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8997" to="9005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Self-supervised monocular image depth learning and confidence estimation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">W</forename><surname>John</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">381</biblScope>
			<biblScope unit="page" from="272" to="281" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Driving scene perception network: real-time joint detection, depth estimation and semantic segmentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1283" to="1291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Towards scene understanding: unsupervised monocular depth estimation with semantic-aware representation</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2624" to="2632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Single-image depth perception in the wild</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="730" to="738" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning single-image depth from videos using quality assessment networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5604" to="5613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adversarial-learning-based image-toimage transformation: a survey</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">411</biblScope>
			<biblScope unit="page" from="468" to="486" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoderdecoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Universal correspondence network</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="2414" to="2422" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Cormen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Leiserson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Rivest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Introduction to algorithms</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">42</biblScope>
		</imprint>
	</monogr>
	<note>third edition thomas h. cormen, charles e. leiserson, ronald l. rivest, clifford stein</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Markov random field texture models</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Cross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="page" from="25" to="39" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Monocular depth prediction using generative adversarial networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Bhandarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Prasad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="300" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sparse-to-continuous: enhancing monocular depth estimation using occupancy maps</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">Dos</forename><surname>Santos Rosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Guizilini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Grassi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">th International Conference on Advanced Robotics (ICAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019 19. 2019</date>
			<biblScope unit="page" from="793" to="800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2650" to="2658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="2366" to="2374" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Camconvs: Camera-aware multi-scale convolutions for single-view depth</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Facil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ummenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Montesano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Civera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="11826" to="11835" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Geo-supervised visual depth prediction</title>
		<author>
			<persName><forename type="first">X</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robot. Autom. Lett</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1661" to="1668" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sganvo: unsupervised deep visual odometry and depth estimation with stacked generative adversarial networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robot. Autom. Lett</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="4431" to="4437" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2002" to="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unsupervised cnn for single view depth estimation: geometry to the rescue</title>
		<author>
			<persName><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">K</forename><surname>Bg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="740" to="756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? The kitti vision benchmark suite</title>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth estimation with left-right consistency</title>
		<author>
			<persName><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="270" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Digging into selfsupervised monocular depth estimation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3828" to="3838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learn stereo, infer mono: siamese networks for self-supervised, monocular, depth estimation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="2672" to="2680" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">How deep should be the depth of convolutional neural networks: a backyard dog case study</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gorban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Mirkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">Y</forename><surname>Tyukin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cogn. Comput</title>
		<imprint>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Draw: a recurrent neural network for image generation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.04623</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">3d packing for selfsupervised monocular depth estimation</title>
		<author>
			<persName><forename type="first">V</forename><surname>Guizilini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ambrus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Raventos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2485" to="2494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Semantically-guided representation learning for self-supervised monocular depth</title>
		<author>
			<persName><forename type="first">V</forename><surname>Guizilini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ambrus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.12319</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning monocular depth by distilling cross-domain stereo networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="484" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Generative adversarial networks for depth map estimation from rgb video</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gwn Lore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Giering</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Bernal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1177" to="1185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Ghostnet: more features from cheap operations</title>
		<author>
			<persName><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1580" to="1589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A brief survey on semantic segmentation with deep learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">406</biblScope>
			<biblScope unit="page" from="302" to="321" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Wearable depth camera: monocular depth estimation via sparse optimization under weak supervision</title>
		<author>
			<persName><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="41337" to="41345" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">An improved deep convolutional neural network with multi-scale information for bearing fault diagnosis</title>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">359</biblScope>
			<biblScope unit="page" from="77" to="92" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">3d convolutional neural networks for human action recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="221" to="231" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Look deeper into depth: monocular depth estimation with semantic booster and attention-driven loss</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="53" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">An unsupervised image segmentation method combining graph clustering and high-level feature representation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">409</biblScope>
			<biblScope unit="page" from="83" to="92" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Self-supervised monocular trained depth estimation using self-attention and discrete disparity volume</title>
		<author>
			<persName><forename type="first">A</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4756" to="4765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Depth prediction from a single image with conditional adversarial networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1717" to="1721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">End-to-end learning of geometry and context for deep stereo regression</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Martirosyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bachrach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="66" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Stereonet: Guided hierarchical refinement for real-time edge-aware depth prediction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Khamis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fanello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kowdle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="573" to="590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">1-day learning, 1-year localization: long-term lidar localization using scan context image</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robot. Autom. Lett</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1948" to="1955" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Deep monocular depth estimation via integration of global and local predictions</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="4131" to="4144" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="1097" to="1105" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Depthnet: a recurrent neural network architecture for monocular depth prediction</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Bhandarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Prasad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="283" to="291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Semi-supervised deep learning for monocular depth map prediction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kuznietsov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stuckler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6647" to="6655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Pereira</surname></persName>
		</author>
		<title level="m">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">A survey on deep learning architectures for image-based depth reconstruction</title>
		<author>
			<persName><forename type="first">H</forename><surname>Laga</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.06113</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Mast: a memory-augmented self-supervised tracker</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6479" to="6488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth international conference on 3D vision (3DV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="239" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Monocular depth estimation using relative depth maps</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">From big to small: multi-scale local planar guidance for monocular depth estimation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Suh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10326</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Staincnns: an efficient stain feature learning method</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">406</biblScope>
			<biblScope unit="page" from="267" to="273" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Colorization using optimization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2004 Papers</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="689" to="694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Single image depth estimation by dilated deep residual convolutional neural network and soft-weight-sum inference</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.00534</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Van Den Hengel, M. He, Depth and surface normal estimation from monocular images using regression on deep features and hierarchical crfs</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1119" to="1127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">A two-streamed network for estimating fine-scaled depth maps from single rgb images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3372" to="3380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Single image super-resolution incorporating examplebased gradient profile estimation and weighted adaptive p-norm</title>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">355</biblScope>
			<biblScope unit="page" from="105" to="120" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Towards binary-valued gates for robust lstm training</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4662" to="4671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<title level="m">Microsoft coco: common objects in context, in: European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Effective image super resolution via hierarchical convolutional neural network</title>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ait-Boudaoud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">374</biblScope>
			<biblScope unit="page" from="109" to="116" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Single image depth estimation from predicted semantic labels</title>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010. 2010</date>
			<biblScope unit="page" from="1253" to="1260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Neural rgb-&gt;d sensing: depth and uncertainty from a video camera</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="10986" to="10995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Deep convolutional neural fields for depth estimation from a single image</title>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="5162" to="5170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Binocular light-field: Imaging theory and occlusion-robust depth perception application</title>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1628" to="1640" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">End-to-end multi-task learning with attention</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Johns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1871" to="1880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Region based parallel hierarchy convolutional neural network for automatic facial nerve paralysis evaluation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Pham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Syst. Rehabil. Eng</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2325" to="2332" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Multi-scale spatio-temporal feature extraction and depth estimation from sequences by ordinal classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<date type="published" when="1979">2020. 1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Object recognition from local scale-invariant features</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh IEEE International Conference on Computer Vision, IEEE</title>
		<meeting>the Seventh IEEE International Conference on Computer Vision, IEEE</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="1150" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Real-time dense monocular slam with online adapted depth prediction network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="470" to="483" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Esnet: Edge-based segmentation network for realtime semantic segmentation in traffic scenes</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1855" to="1859" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from monocular video using 3d geometric constraints</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5667" to="5675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Fast robust monocular depth estimation for obstacle detection with fully convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mancini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Costante</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valigi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Ciarfuglia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="4296" to="4303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Toward domain independence for learning-based monocular depth estimation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mancini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Costante</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valigi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Ciarfuglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Delmerico</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robot. Autom. Lett</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1778" to="1785" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">A large dataset to train convolutional networks for disparity, optical flow</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">438</biblScope>
			<biblScope unit="page" from="14" to="33" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<author>
			<orgName type="collaboration">scene flow estimation</orgName>
		</author>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4040" to="4048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Scenenet rgb-d: can 5m synthetic images beat generic imagenet pre-training on indoor segmentation?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mccormac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Leutenegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2678" to="2687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">G</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Koc ˇisky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">`</forename></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mogrifier</forename><surname>Lstm</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01792</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Un-vdnet: unsupervised network for visual odometry and depth estimation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Electron. Imaging</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">63015</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Signet: Semantic instance aided unsupervised 3d geometry perception</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sunarjo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Javidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bharadia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9810" to="9820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Joint semantic segmentation and depth estimation with deep convolutional networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Košecká</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 Fourth International Conference on 3D Vision (3DV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="611" to="619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Real-time pose and shape reconstruction of two interacting hands with a single depth camera</title>
		<author>
			<persName><forename type="first">F</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Verschoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Otaduy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph. (TOG)</title>
		<imprint>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Orb-slam: a versatile and accurate monocular slam system</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mur-Artal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M M</forename><surname>Montiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Tardos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Robot</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1147" to="1163" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Adadepth: unsupervised content congruent adaptation for depth estimation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Nath Kundu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Krishna Uppala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2656" to="2665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Color-guided depth map super resolution using convolutional neural network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="26666" to="26672" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Visual odometry</title>
		<author>
			<persName><forename type="first">D</forename><surname>Nistér</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Naroditsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004">2004. 2004. 2004</date>
			<biblScope unit="page" from="964" to="971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Rdfnet: Rgb-d multi-level residual feature fusion for indoor semantic segmentation</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4980" to="4989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">On the uncertainty of self-supervised monocular depth estimation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Aleotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mattoccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3227" to="3237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Mixed-dense connection networks for image and video super-resolution</title>
		<author>
			<persName><forename type="first">K</forename><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mandal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rajagopalan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">398</biblScope>
			<biblScope unit="page" from="360" to="376" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Geonet: geometric neural network for joint depth and surface normal estimation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="283" to="291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Deeplidar: deep surface normal guided depth prediction for outdoor scene from sparse lidar data and single color image</title>
		<author>
			<persName><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3313" to="3322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Predicting sharp and accurate occlusion boundaries in monocular depth estimation using displacement fields</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ramamonjisoa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="14648" to="14657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Geometry meets semantics for semi-supervised monocular depth estimation</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Z</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mattoccia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Di</forename><surname>Stefano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="298" to="313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Hilbert maps: scalable continuous occupancy mapping with stochastic gradient descent</title>
		<author>
			<persName><forename type="first">F</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Robot. Res</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1717" to="1730" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Unsupervised pre-trained filter learning approach for efficient convolution neural network</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ur Rehman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Waqas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ur Rehman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ahmad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">365</biblScope>
			<biblScope unit="page" from="171" to="190" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Deep robust single image depth estimation neural network using scene understanding</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>El-Khamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="37" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Cognitive modelling and learning for multimedia mining and understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cogn. Comput</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="761" to="762" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Special issue on recent advances in cognitive learning and data analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cogn. Comput</title>
		<imprint>
			<biblScope unit="page" from="1" to="2" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Monocular depth estimation using multi-scale continuous crfs as sequential deep networks</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1426" to="1440" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<monogr>
		<title level="m" type="main">A deep primal-dual network for guided depth super-resolution</title>
		<author>
			<persName><forename type="first">G</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ferstl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rüther</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.08569</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">The synthia dataset: a large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3234" to="3243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Learning representations by backpropagating errors</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="1161" to="1168" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Depth estimation using monocular and stereo cues</title>
		<author>
			<persName><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schulte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="page" from="2197" to="2203" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Visual odometry [tutorial</title>
		<author>
			<persName><forename type="first">D</forename><surname>Scaramuzza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fraundorfer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robot. Autom. Mag</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="80" to="92" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<monogr>
		<title level="m" type="main">Ordered neurons: Integrating tree structures into recurrent neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.09536</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Learning from simulated and unsupervised images through adversarial training</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Webb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2107" to="2116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<monogr>
		<title level="m" type="main">Feature-metric loss for self-supervised learning of depth and egomotion</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="746" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Scale-adaptive neural dense features: learning via hierarchical context aggregation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Spencer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hadfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6200" to="6209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Defeat-net: general monocular depth via simultaneous unsupervised representation learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Spencer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hadfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="14402" to="14413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Monocular depth estimation as regression of classification using piled residual networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
		<meeting>the 27th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2161" to="2169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Two-stage deep regression enhanced depth estimation from a single rgb image</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Emerg. Top. Comput</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Stereo matching using belief propagation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="787" to="800" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Depth recovery and refinement from a single image using defocus cues</title>
		<author>
			<persName><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mod. Opt</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="441" to="448" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Objectfusion: an object detection and segmentation framework with rgb-d slam and convolutional neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">345</biblScope>
			<biblScope unit="page" from="3" to="14" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for depth prediction from images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tonioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mattoccia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Di</forename><surname>Stefano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>intelligence</note>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Real-time self-adaptive deep stereo</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tonioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mattoccia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Stefano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="195" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Block-based vanishing line and vanishing point detection for 3d scene reconstruction</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">L</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">G</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2006 International Symposium on Intelligent Signal Processing and Communications</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="586" to="589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Practical deep stereo (pds): toward applications-friendly deep stereo matching</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="5871" to="5881" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Demon: depth and motion network for learning monocular stereo</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ummenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5038" to="5047" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Depth from motion for smartphone ar</title>
		<author>
			<persName><forename type="first">J</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kowdle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wadhwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dzitsiuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schoenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Csaszar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Dryanovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph. (TOG)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="5998" to="6008" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ricco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.07804</idno>
		<title level="m">Sfmnet: learning of structure and motion from video</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">Suppressing uncertainties for largescale facial expression recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6897" to="6906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Appearance-and-relation networks for video classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1430" to="1439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">Sdc-depth: semantic divide-andconquer network for monocular depth estimation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="541" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">Towards unified depth and semantic prediction from a single image</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2800" to="2809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Y</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">438</biblScope>
			<biblScope unit="page" from="14" to="33" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">Surge: surface regularized geometry estimation from a single image</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="172" to="180" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">Recurrent neural network for (un-) supervised learning of monocular video visual odometry and depth</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Pizer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5555" to="5564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">Deepvo: towards end-to-end visual odometry with deep recurrent convolutional neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Trigoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2043" to="2050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<monogr>
		<title level="m" type="main">Self-supervised monocular depth hints</title>
		<author>
			<persName><forename type="first">J</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Turmukhambetov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2162" to="2171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">Bilateral cyclic constraint and adaptive regularization for unsupervised monocular depth prediction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5644" to="5653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">Accurate and robust eye center localization via fully convolutional networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/CAA J. Autom. Sin</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1127" to="1138" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<monogr>
		<title level="m" type="main">Joint convolutional neural pyramid for depth map super-resolution</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.00968</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b162">
	<analytic>
		<title level="a" type="main">Deep3d: fully automatic 2d-to-3d video conversion with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="842" to="857" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<analytic>
		<title level="a" type="main">Convolutional lstm network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xingjian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">C</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="802" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b164">
	<analytic>
		<title level="a" type="main">Structured attention guided convolutional neural fields for monocular depth estimation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3917" to="3925" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b165">
	<analytic>
		<title level="a" type="main">Unsupervised framework for depth estimation and camera motion prediction from video</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">385</biblScope>
			<biblScope unit="page" from="169" to="185" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b166">
	<analytic>
		<title level="a" type="main">Bayesian denet: monocular depth prediction and frame-wise fusion with synchronized uncertainty</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="2701" to="2713" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b167">
	<analytic>
		<title level="a" type="main">Reactive obstacle avoidance of monocular quadrotors with online adapted depth prediction network</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">325</biblScope>
			<biblScope unit="page" from="142" to="158" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b168">
	<monogr>
		<title level="m" type="main">Unsupervised learning of geometry with edge-aware depth-normal consistency</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.03665</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b169">
	<analytic>
		<title level="a" type="main">Drm-slam: towards dense reconstruction of monocular slam with scene depth fusion</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">396</biblScope>
			<biblScope unit="page" from="76" to="91" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b170">
	<analytic>
		<title level="a" type="main">Geonet: unsupervised learning of dense depth, optical flow and camera pose</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1983" to="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b171">
	<analytic>
		<title level="a" type="main">Stereo matching by training a convolutional neural network to compare image patches</title>
		<author>
			<persName><forename type="first">J</forename><surname>ˇbontar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="2287" to="2318" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b172">
	<analytic>
		<title level="a" type="main">Deep-reinforcementlearning-based images segmentation for quantitative analysis of gold immunochromatographic strip</title>
		<author>
			<persName><forename type="first">N</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">E</forename><surname>Alsaadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">425</biblScope>
			<biblScope unit="page" from="173" to="180" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b173">
	<analytic>
		<title level="a" type="main">Facial expression recognition via learning deep sparse autoencoders</title>
		<author>
			<persName><forename type="first">N</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dobaie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">273</biblScope>
			<biblScope unit="page" from="643" to="649" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b174">
	<analytic>
		<title level="a" type="main">Optical flow estimation using channel attention mechanism and dilated convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">El</forename><surname>Saddik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">368</biblScope>
			<biblScope unit="page" from="124" to="132" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b175">
	<analytic>
		<title level="a" type="main">Unsupervised learning of monocular depth estimation and visual odometry with deep feature reconstruction</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Saroj Weerasekera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="340" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b176">
	<analytic>
		<title level="a" type="main">Ga-net: guided aggregation net for end-to-end stereo matching</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Prisacariu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="185" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b177">
	<analytic>
		<title level="a" type="main">Exploiting temporal consistency for real-time video depth estimation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1725" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b178">
	<analytic>
		<title level="a" type="main">Stackgan: text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5907" to="5915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b179">
	<analytic>
		<title level="a" type="main">Monocular 3d vehicle detection with multiinstance depth and geometry reasoning for autonomous driving</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">403</biblScope>
			<biblScope unit="page" from="182" to="192" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b180">
	<analytic>
		<title level="a" type="main">Unsupervised depth estimation from monocular videos with hybrid geometric-refined loss and contextual attention</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">379</biblScope>
			<biblScope unit="page" from="250" to="261" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b181">
	<analytic>
		<title level="a" type="main">Stereoscopic video saliency detection based on spatiotemporal correlation and depth confidence optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">377</biblScope>
			<biblScope unit="page" from="256" to="268" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b182">
	<analytic>
		<title level="a" type="main">Shape-from-shading: a survey</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Cryer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="690" to="706" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b183">
	<analytic>
		<title level="a" type="main">Cognitive template-clustering improved linemod for efficient multi-object pose estimation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cogn. Comput</title>
		<imprint>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b184">
	<analytic>
		<title level="a" type="main">Shufflenet: an extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6848" to="6856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b185">
	<analytic>
		<title level="a" type="main">Deep depth completion of a single rgb-d image</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="175" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b186">
	<analytic>
		<title level="a" type="main">A flexible new technique for camera calibration</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1330" to="1334" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b187">
	<analytic>
		<title level="a" type="main">Joint task-recursive learning for semantic segmentation and depth estimation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="235" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b188">
	<analytic>
		<title level="a" type="main">Pattern-affinitive propagation across depth, surface normal and semantic segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4106" to="4115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b189">
	<analytic>
		<title level="a" type="main">Progressive hard-mining network for monocular depth estimation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3691" to="3702" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b190">
	<analytic>
		<title level="a" type="main">Geometry-aware symmetric domain adaptation for monocular depth estimation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9788" to="9798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b191">
	<analytic>
		<title level="a" type="main">6d object pose estimation via viewpoint relation reasoning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">389</biblScope>
			<biblScope unit="page" from="9" to="17" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b192">
	<analytic>
		<title level="a" type="main">Domain decluttering: simplifying images to mitigate synthetic-real domain shift and improve depth estimation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3330" to="3340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b193">
	<analytic>
		<title level="a" type="main">T2net: synthetic-to-realistic translation for solving single-image depth estimation tasks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Cham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="767" to="783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b194">
	<monogr>
		<title level="m" type="main">Unsupervised learning of monocular depth estimation with bundle adjustment, super-resolution and clip loss</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kaess</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.03368</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b195">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1851" to="1858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b196">
	<analytic>
		<title level="a" type="main">Unsupervised event-based learning of optical flow, depth, and egomotion</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chaney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="989" to="997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b197">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b198">
	<analytic>
		<title level="a" type="main">Learning ordinal relationships for mid-level vision</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="388" to="396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b199">
	<monogr>
		<title level="m">Yue Ming received the B.S. degree in Communication Engineering, and the M.Sc degree in Human-Computer Interaction Engineering, and Ph.D. degree in Signal and Information Processing from Beijing Jiaotong University</title>
		<meeting><address><addrLine>China; U.S., between</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006. 2008. 2013. 2010 and 2011</date>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note>Since 2013, she has been working as a faculty member at Beijing University of Posts and Telecommunications. Her research interests are in the areas of biometrics, computer vision, computer graphics. information retrieval, pattern recognition</note>
</biblStruct>

<biblStruct xml:id="b200">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Y</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">438</biblScope>
			<biblScope unit="page" from="14" to="33" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
