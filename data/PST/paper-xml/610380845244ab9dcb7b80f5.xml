<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bi-Bimodal Modality Fusion for Correlation-Controlled Multimodal Sentiment Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-07-28">28 Jul 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wei</forename><surname>Han</surname></persName>
							<email>wei_han@mymail.sutd.edu.sg</email>
						</author>
						<author>
							<persName><forename type="first">Hui</forename><surname>Chen</surname></persName>
							<email>hui_chen@mymail.sutd.edu.sg</email>
						</author>
						<author>
							<persName><forename type="first">Alexander</forename><surname>Gelbukh</surname></persName>
							<email>gelbukh@gelbukh.com</email>
						</author>
						<author>
							<persName><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
							<email>morency@cs.cmu.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ISTD</orgName>
								<orgName type="institution" key="instit2">Singapore University of Technology and Design Singapore</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">ISTD</orgName>
								<orgName type="institution" key="instit2">Singapore University of Technology and Design Singapore</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Centro de InvestigaciÃ³n en ComputaciÃ³</orgName>
								<orgName type="institution" key="instit1">Instituto PolitÃ©cnico Nacional Mexico City</orgName>
								<orgName type="institution" key="instit2">CDMX</orgName>
								<address>
									<country key="MX">Mexico</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Language Technology Institute</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Language Technology Institute</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<region>PA</region>
									<country>USA Soujanya Poria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="institution" key="instit1">ISTD</orgName>
								<orgName type="institution" key="instit2">Singapore University of Technology and Design Singapore</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Bi-Bimodal Modality Fusion for Correlation-Controlled Multimodal Sentiment Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-07-28">28 Jul 2021</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/1122445.1122456</idno>
					<idno type="arXiv">arXiv:2107.13669v1[cs.AI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>cross-modal processing</term>
					<term>multimodal fusion</term>
					<term>multimodal representations</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multimodal sentiment analysis aims to extract and integrate semantic information collected from multiple modalities to recognize the expressed emotions and sentiment in multimodal data. This research area's major concern lies in developing an extraordinary fusion scheme that can extract and integrate key information from various modalities. However, one issue that may restrict previous work to achieve a higher level is the lack of proper modeling for the dynamics of the competition between the independence and relevance among modalities, which could deteriorate fusion outcomes by causing the collapse of modality-specific feature space or introducing extra noise. To mitigate this, we propose the Bi-Bimodal Fusion Network (BBFN), a novel end-to-end network that performs fusion (relevance increment) and separation (difference increment) on pairwise modality representations. The two parts are trained simultaneously such that the combat between them is simulated. The model takes two bimodal pairs as input due to the known information imbalance among modalities. In addition, we leverage a gated control mechanism in the Transformer architecture to further improve the final output. Experimental results on three datasets (CMU-MOSI, CMU-MOSEI, and UR-FUNNY) verifies that our model significantly outperforms the SOTA. The implementation of this work is available at https://github.com/declare-lab/BBFN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>With the unprecedented prevalence of social media in recent years and the availability of phones with high-quality cameras, we witness an explosive boost of multimodal data, such as video clips posted on different social media platforms. Such multimodal data consist of three channels: visual (image), acoustic (voice), and transcribed linguistic (text) data. Different modalities in the same data segment are often complementary to each other, providing extra cues for semantic and emotional disambiguation <ref type="bibr" target="#b22">[23]</ref>. For example, the phrase "apple tree" can indicate what the blurred red fruit on the tree is in an image, and a smiling face can clarify that some seemingly impolite words are a friendly joke. On the other hand, the three modalities usually possess unique statistical properties that make them to some extent mutually independent-say, one modality can stay practically constant while the other one exhibits large changes <ref type="bibr" target="#b30">[31]</ref>. Accordingly, a crucial issue in multimodal language processing is how to integrate heterogeneous data efficiently. A good fusion scheme should extract and integrate meaningful information from multiple modalities while preserving their mutual independence.</p><p>In this paper, we focus on the problem of multimodal sentiment analysis (MSA). As Fig. <ref type="figure" target="#fig_0">1</ref> suggests, given data from multiple modality sources, the goal of MSA is to exploit fusion techniques to combine these modalities to make predictions for the labels. In the context of emotion recognition and sentiment analysis, multimodal fusion is essential since emotional cues are often spread across different modalities <ref type="bibr" target="#b1">[2]</ref>. Previous research in this field <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b42">43</ref>] mostly adopted ternary-symmetric architectures, where bidirectional relationships in every modality pair are modeled in some way. However, as it has been pointed out by several past research <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b33">34]</ref>, the task-related information is not evenly distributed between the modalities. The architectures that do not account for this difference in the relative importance of the three modalities fail to fuse them correctly, which degrades the model's performance.</p><p>To address this issue, we introduce a fusion scheme that we call Bi-Bimodal Fusion Network (BBFN) to balance the contribution of different modality pairs properly. This fusion scheme, consisting of two bi-modal fusion modules, is quite different from traditional ternary symmetric one; see Fig. <ref type="figure" target="#fig_0">1</ref>. Since it has been empirically shown that the text modality is most significant <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b33">34]</ref>, our model takes two text-related modality pairs, TV (text-visual) and TA (textacoustic), as respective inputs for its two bimodal learning modules. Then it iteratively impels modalities to complement their information through interactive learning with their corresponding counterparts. To ensure fairness in the bidirectional learning process for both modalities, the two learning networks in each model should be identical. The basic framework of our model is layers of stacked Transformers, which have been proven efficient in multimodal learning <ref type="bibr" target="#b40">[41]</ref>.</p><p>However, a new problem arises in our implementation. As fusion proceeds, the representation vectors of the fusion results involved with a modality pair tend to become closer in the hidden space; we call it feature space collapse. Moreover, the repeated structures of transformers in the stacked architecture exacerbate this trend, impairing the mutual independence between different modalities present in the multimodal data-a crucial property for the feasibility of multimodal fusion. To tackle this problem, we introduce in our BBFN the layer-wise feature space separator, as a local regularizer that divides the feature space of different modalities in order to assure mutual independence between modalities.</p><p>We evaluated our model on two subtasks of MSA-sentiment intensity prediction and humor recognition-using three datasets: CMU-MOSI, CMU-MOSEI, and UR-FUNNY. Our experimental results show that our model outperforms state-of-the-art models on almost all metrics. Moreover, ablation study and further analysis show the effectiveness of our architecture.</p><p>Our contributions can be summarized as follows:</p><p>â€¢ Bi-bimodal fusion: We introduce a novel fusion scheme for MSA, which consists of two Transformer-based bimodal learning modules, each one taking a modality sequence pair as input and performing a progressive fusion locally in its two modality complementation modules. â€¢ Regularization: To enforce modality representations to be unique and different from each other, we use a modalityspecific feature separator, which implicitly clusters homogeneous representations and splits heterogeneous ones apart in order to maintain mutual independence between modalities. â€¢ Control: We introduce a gated control mechanism to enhance the Transformer-based fusion process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this section, we briefly overview related work in MSA and multimodal fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Multimodal Sentiment Analysis</head><p>Multimodal sentiment analysis (MSA) mainly focuses on integrating multiple resources, such as acoustic, visual, and textual information, to comprehend varied human emotions <ref type="bibr" target="#b21">[22]</ref>. In the past few years, deep neural networks have been employed in learning multimodal representation in sentiment analysis, such as Long Short-Term Memory (LSTM), which is used to model long-range dependencies from low-level multimodal features <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b38">39]</ref> and SAL-CNN <ref type="bibr" target="#b37">[38]</ref>, which utilizes a select-additive learning procedure to improve the generalizability of trained neural networks. Most of the previous work in this area focuses on early or late fusion. For example, Zadeh et al. <ref type="bibr" target="#b42">[43]</ref> proposed a Tensor Fusion Network, which blends different modality representations at a deeper layer. As attention mechanism becomes more and more popular, Zadeh et al. <ref type="bibr" target="#b43">[44]</ref> modified LSTM with a novel Multi-attention Block to capture interactions between modalities through time. Also, Gu et al. <ref type="bibr" target="#b13">[14]</ref> introduced a hierarchical attention strategy with wordlevel fusion to classify utterance-level sentiment. Moreover, Akhtar et al. <ref type="bibr" target="#b0">[1]</ref> presented a deep multi-task learning framework to jointly learn sentiment polarities and emotional intensity in a multimodal background. Rahman et al. <ref type="bibr" target="#b26">[27]</ref> directly worked on BERT and designed functional gates to control the dataflow of one modality from other two modalities.</p><p>Pham et al. <ref type="bibr" target="#b24">[25]</ref> proposed a method that cyclically translates between modalities to learn robust joint representations for sentiment analysis. More recently, Hazarika et al. <ref type="bibr" target="#b15">[16]</ref> attempted to factorize modality features in joint spaces to effectively capture commonalities across different modalities and reduce their gaps. Tsai et al. <ref type="bibr" target="#b35">[36]</ref> proposed a Capsule Network-based method to dynamically adjust weights between modalities. Most of these works incorporate interactions in every modality pairs into their design. In contrast, our model only includes two pairs involving one modality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multimodal Language Learning</head><p>Correlation-based Approach. Correlation has been learned as an important metric for objects showing concurrently. There are many previous works that include this item for various purposes. Sun et al. <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref> directly optimized over a correlation-related DCCA loss to learn multimodal representations useful for downstream tasks. Mittal et al. <ref type="bibr" target="#b19">[20]</ref> instead used correlation as a selection criteria to guide multimodal data to orderly form a union representation. Although all these works took correlation into account, they ignored the importance of modality's independence and the game between the two contradictory things.</p><p>Alignment-based Approach. Alignment is the process to map signals of different sampling rates to the same frequency. Early multimodal alignment approaches <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b44">45]</ref> usually firstly chose a target frequency and then calculated the frames in each modality that need mapping to that position. Some classical loss functions like CTC <ref type="bibr" target="#b12">[13]</ref> and their variants are widely used to facilitate alignment improvement. Thanks to the advent attention mechanism, the Transformer architecture shows state-of-the-art performance in multiple disciplines in both text and visual fields <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b36">37]</ref>. Unlike traditional alignment routines, attention naturally formulates a point-to-point mapping between two modalities, which is called "soft alignment" and has been proven effective in more general cases of multimodal representation learning and feature fusion. For example, Yu et al. <ref type="bibr" target="#b40">[41]</ref> designed a Unified Multimodal Transformer to jointly model text and visual representations in the NER problem. Moreover, Tsai et al. <ref type="bibr" target="#b33">[34]</ref> employed the Transformer to model as well as align sequences from visual, textual, and acoustic sources. Our fusion architecture is built on Transformer, but performs fusion in a progressive manner with feature space regularization and fine-grained gate control.</p><p>Application in Other Tasks. Besides multimodal sentiment analysis, multimodal learning has been applied in many other language tasks, such as Machine Translation (MT) <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b39">40]</ref>, Named Entity Recognition (NER) <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b46">47]</ref>, and parsing <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b47">48]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head><p>In this section, we first briefly define the problem and then describe our BBFN model.</p><p>Task Definition. The task of MSA aims to predict sentiment intensity, polarity, or emotion label of given multimodal input (video clip). The video consists of three modalities: ğ‘¡ (text), ğ‘£ (visual) and ğ‘ (acoustic), which are 2D tensors denoted by ğ‘€ ğ‘¡ âˆˆ R ğ‘‡ ğ‘¡ Ã—ğ‘‘ ğ‘¡ , ğ‘€ ğ‘£ âˆˆ R ğ‘‡ ğ‘£ Ã—ğ‘‘ ğ‘£ , and ğ‘€ ğ‘ âˆˆ R ğ‘€ ğ‘ Ã—ğ‘‘ ğ‘ , where ğ‘‡ ğ‘š and ğ‘‘ ğ‘š represent sequence length (e.g., number of frames) and feature vector size of modality ğ‘š.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overall Description</head><p>The overview of our model is shown in Fig. <ref type="figure" target="#fig_1">2</ref>. It consists of two modality complementation modules, each accomplishing a bimodal fusion process in its two fusion pipelines. After receiving contextaware representations from the underlying modality sequence encoders, bimodal fusion proceeds iteratively through stacked complementation layers.</p><p>The feature space separator is another key idea of our model. Each modality has its own feature representations. However, in a deep neural network, when these unique unimodal representations propagate through multiple layers, their mutual independence can be compromised, i.e., they may not be as separable as they were initially; we call this feature space collapse. The separability of the unimodal representations and their mutual independence is necessary for multimodal fusion; otherwise, one modality can hardly learn something new from its counterparts through heterogeneous attention on respective hidden representations. Accordingly, we enforce these representations to preserve more modality-specific features to prevent them from collapsing into a pair of vectors with similar distributions.</p><p>Finally, the conventional heterogeneous Transformer purely uses a residual connection to combine attention results and input representations without any controlled decision made for the acceptance and rejection along the hidden dimension of these vectors. Instead, we incorporate a gated control mechanism in the multi-head attention of the Transformer network, which also couples the feature separator and transformer fusion pipelines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Modality Sequence Encoder</head><p>We encode all modality sequences to guarantee a better fusion outcome in the subsequent modality complementation modules.</p><p>Word Embedding. We use the Transformer-based pre-trained model, BERT <ref type="bibr" target="#b8">[9]</ref> as the text encoder. The raw sentence ğ‘† = (ğ‘¤ 1 , â€¢ â€¢ â€¢ , ğ‘¤ ğ‘› ) composed of word indices is firstly concatenated with two special tokens-[CLS] at the head and [SEP] at the tail and then fed into the encoder to generate contextualized word embeddings, as the input of text modality ğ‘€ ğ‘¡ = (ğ‘š 0 , ğ‘š 1 , ..., ğ‘š ğ‘›+1 ).</p><p>Sequence Encoder. The input modality sequences ğ‘€ ğ‘š , ğ‘š âˆˆ {ğ‘¡, ğ‘£, ğ‘}, are essentially time series and exhibit temporal dependency. We use a single-layer bidirectional gated recurrent unit (BiGRU) <ref type="bibr" target="#b6">[7]</ref> followed by a linear projection layer to capture their internal dependency and cast all the hidden vectors to the same length for the convenience of further processing. The resulting sequences are</p><formula xml:id="formula_0">ğ‘‹ 0 ğ‘š = (ğ‘¥ 0 ğ‘š,0 , ğ‘¥ 0 ğ‘š,1 , ..., ğ‘¥ 0 ğ‘š,ğ‘›+1 ),<label>(1)</label></formula><p>where ğ‘š âˆˆ {ğ‘¡, ğ‘£, ğ‘} denotes a modality. These outputs serve as the initial inputs to the modality complementation module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Modality Complementation Module</head><p>In the modality complementation module, the modality representation pairs exchange information with their counterparts to "complement" the missing cues when passing through the multimodal complementation layers that interconnect two fusion pipelines with layer-wise modality-specific feature separators. We further improve the attention-based fusion procedure by adding a gated control mechanism to enhance its performance and robustness. The module is built in a stacked manner to realize an iterative fusion routine.</p><p>Modality-Specific Feature Separator. To maintain mutual independence among these modalities, we leverage the regularization effect exerted by a discriminator loss, which tells how well the hidden representations can be distinguished from their counterparts in the same complementation module. A straightforward solution for a separator according to prior work <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b31">32]</ref> is to add some geometric measures to the total loss as regularization term, such as (1) euclidean distances or cosine correlation or <ref type="bibr" target="#b1">(2)</ref> distribution  similarity measures such as KL-Divergence or Wasserstein distance along the hidden dimension. However, we chose the discriminator loss because-unlike geometric measures, which directly use hidden vectors-it is a parametric method, so it is more suitable to be coalesced into the entire model. Namely, after collecting the outputs from the previous complementation layer ğ‘‹ ğ‘–âˆ’1 ğ‘š = (ğ‘¥ ğ‘–âˆ’1 ğ‘š,0 , ğ‘¥ ğ‘–âˆ’1 ğ‘š,1 , ..., ğ‘¥ ğ‘–âˆ’1 ğ‘š,ğ‘›+1 ), we encode the sequence with a bidirectonal GRU and then apply an average pooling to acquire sequence-level hidden representations:</p><formula xml:id="formula_1">I E 0 E 1 E 2 E n+1 â€¦â€¦ E n [CLS] can â€¦â€¦ hit [SEP]</formula><formula xml:id="formula_2">hğ‘– ğ‘š = avgpool(h ğ‘– ğ‘š ) = avgpool(BiGRU(ğ‘‹ ğ‘–âˆ’1 ğ‘š ; ğœƒ ğ‘– ğ‘š )).</formula><p>where ğœƒ ğ‘– ğ‘š are the parameters of the BiGRU in the ğ‘–th layer. Here we choose BiGRU as the intermediate sequence encoder because with fewer parameters, in our experiments it provided results comparable with those of BiLSTM. Note that until now we just described the data flow of a single modality. In a complementation module, at each layer ğ‘– there are always two pipelines that generate the sequences of hidden representations concurrently for two different modalities, ğ‘š 1 and ğ‘š 2 .</p><p>Next we want to separate the possibly entangled intermediate modality representations. Different from previous works that rely on explicit distance maximization, we train a classifier to discern which modality these representations come from. A straightforward approach is directly fed all of them into the classifier, but it may occur serious issue: random noises in sequence representations cause the classifier to pay worthless effort on trivial features. We introduce a simple group strategy to mitigate this issue, which applies average operation on representations in the same group to generate a smoother representation. Specifically, after setting group size as ğ¾, the representation for the ğ‘Ÿ ğ‘¡â„ (ğ‘Ÿ = 1, 2, ..., ğ‘ /ğ¾) group is:</p><formula xml:id="formula_3">hğ‘–,ğ‘Ÿ ğ‘š = 1 ğ¾ ğ¾ âˆ‘ï¸ ğ‘—=1+(ğ‘Ÿ âˆ’1)Ã—ğ¾ hğ‘–,ğ‘— ğ‘š (2) Ä‰ğ‘– ğ‘Ÿ , Ä‰ğ‘– ğ‘Ÿ +ğ‘ ğ‘ /ğ¾ = ğ· ğ‘– ( hğ‘–,ğ‘Ÿ ğ‘š 1 , hğ‘–,ğ‘Ÿ ğ‘š 2 ).<label>(3)</label></formula><p>We leave a short explanation about how this reduces noise here. Suppose vectors possessing similar property (i.e. from the same modality in context) can be fitted with a set of gaussian distribution {N (ğœ‡ 1 , ğœ 1 ), N (ğœ‡ 2 , ğœ 2 ), N (ğœ‡ 3 , ğœ 3 ), ..., N (ğœ‡ ğ‘› , ğœ ğ‘› )} and corresponding weights {ğ‘¤ 1 , ğ‘¤ 2 , ğ‘¤ 3 , ..., ğ‘¤ ğ‘› }. According to the rule for the summation of gaussian distributions, we have</p><formula xml:id="formula_4">ğ‘’ âˆ¼ N âˆ‘ï¸ ğ‘› ğ‘¤ ğ‘› ğœ‡ ğ‘› , âˆšï¸„ âˆ‘ï¸ ğ‘› ğ‘¤ 2 ğ‘› ğœ 2 ğ‘› = N (ğœ‡, ğœ)</formula><p>By introducing the grouping trick, for each group representation the new expectation term holds constant while the variance term turns to</p><formula xml:id="formula_5">ğœ ğ‘” = âˆšï¸ƒ ğ‘› ğ‘¤ 2 ğ‘› ğœ 2 ğ‘› ğ¾ = ğœ/ğ¾</formula><p>which decreases as group size increases. The discriminators are distinct in every layer because of the diverse manifestations of the same modality in the semantic space as fusion progresses which thus requires different sets of parameters to discern. We calculate the Binary Cross Entropy between predictions and their corresponding pseudo labels that are automatically generated during training time as the discriminator loss:</p><formula xml:id="formula_6">L ğ‘– ğ‘ ğ‘’ğ‘ = âˆ’ ğ¾ 2ğ‘ ğ‘ 2ğ‘ ğ‘ /ğ¾ âˆ‘ï¸ ğ‘Ÿ =1 ğ‘ ğ‘– ğ‘Ÿ log Ä‰ğ‘– ğ‘Ÿ + (1 âˆ’ ğ‘ ğ‘– ğ‘Ÿ ) log(1 âˆ’ Ä‰ğ‘– ğ‘Ÿ ) ,</formula><p>where ğ‘ ğ‘ is the batch size and ğ‘— represents the ğ‘—-th sample.</p><p>Gated Complementation Transformer (GCT). The main body of the modality complementation module is the Gated Complementation Transformer, which are stacked into two pipelines to form the symmetric structure. For convenience of explanation, we will call the modality that keeps forwarding in the same fusion pipeline inside a complementation module the main modality, denoted by ğ‘šğ‘ğ‘–ğ‘›, and the modality that joins bimodal fusion in one pipeline but comes as an external source from the other pipeline, the complementary modality, denoted by ğ‘ğ‘œğ‘šğ‘. Noted that distinguishing main and complementary modalities makes sense only in the context of a specified pipeline.</p><p>The cross-modal fusion process occurs mainly at the multi-head attention operation, which we found to show suboptimal performance due to the lack of information flow control. To improve it in a fine-grained and controllable way, we introduce two gates: the retain gate g ğ‘Ÿ , which decides how much proportion of the target modality's components to be kept forwarding, and the compound gate g ğ‘ , which decides how much proportion of compounded components to be injected to the target modality.</p><p>We generate these two gate signals from the sequential representation of the two modalities in the same layer:</p><formula xml:id="formula_7">g ğ‘– ğ‘Ÿ = ğœ (W ğ‘–,ğ‘šğ‘ğ‘–ğ‘› ğ‘Ÿ [ hğ‘– ğ‘šğ‘ğ‘–ğ‘› âˆ¥ hğ‘– ğ‘ğ‘œğ‘šğ‘ ]), g ğ‘– ğ‘ = ğœ (W ğ‘–,ğ‘šğ‘ğ‘–ğ‘› ğ‘ [ hğ‘– ğ‘šğ‘ğ‘–ğ‘› âˆ¥ hğ‘– ğ‘ğ‘œğ‘šğ‘ ]),</formula><p>where W * âˆˆ R 2ğ‘‘Ã—ğ‘‘ is the projection matrix and âˆ¥ represents concatenation. After receiving the query ğ‘„ ğ‘– = W ğ‘– ğ‘„ ğ‘‹ ğ‘– ğ‘šğ‘ğ‘–ğ‘› , key ğ¾ ğ‘– = W ğ‘– ğ¾ ğ‘‹ ğ‘– ğ‘ğ‘œğ‘šğ‘ and value ğ‘‰ ğ‘– = W ğ‘– ğ‘‰ ğ‘‹ ğ‘– ğ‘ğ‘œğ‘šğ‘ , these gates are then employed on the multi-head attention to limit the information flow of the residual block as a part of bimodal combination:</p><formula xml:id="formula_8">m ğ‘– = MH-ATT(ğ‘„ ğ‘– , ğ¾ ğ‘– , ğ‘‰ ğ‘– ), Xğ‘– ğ‘šğ‘ğ‘–ğ‘› = LN(g ğ‘– ğ‘ âŠ™ m ğ‘– + g ğ‘– ğ‘Ÿ âŠ™ ğ‘‹ ğ‘– ğ‘šğ‘ğ‘–ğ‘› )</formula><p>, where MH-ATT represents multi-head attention, âŠ™ means componentwise multiplication and LN is layer normalization. Next, the attention results pass through the feed-forward network (similar to a conventional Transformer network) to produce the final output of the current complementation layer:</p><formula xml:id="formula_9">ğ‘‹ ğ‘– ğ‘šğ‘ğ‘–ğ‘› = LN( Xğ‘– ğ‘šğ‘ğ‘–ğ‘› + FFN( Xğ‘– ğ‘šğ‘ğ‘–ğ‘› )).<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Output Layer and Training</head><p>According to (4), a given complementation layer produces the output ğ‘‹ ğ‘– ğ‘š , where, similarly to (1), ğ‘‹ ğ‘– ğ‘š = (ğ‘¥ ğ‘– ğ‘š,0 , . . . , ğ‘¥ ğ‘– ğ‘š,ğ‘› ). When speaking of a layer of a specific complementation module ğ‘€, where ğ‘€ âˆˆ {ğ‘‡ ğ´,ğ‘‡ğ‘‰ , ğ‘‰ ğ´}, we will add ğ‘€ as index: ğ‘‹ ğ‘– ğ‘€,ğ‘š = (ğ‘¥ ğ‘– ğ‘€,ğ‘š,0 , . . . , ğ‘¥ ğ‘– ğ‘€,ğ‘š,ğ‘›+1 ). The final output of the module ğ‘€ for the modality ğ‘š is ğ‘‹ ğ¿ ğ‘€,ğ‘š , where ğ¿ is the number of layers; see Fig. <ref type="figure" target="#fig_1">2</ref>.</p><p>We compute the final representation by first extracting the heads h ğ¿ ğ‘€,ğ‘š = ğ‘¥ ğ¿ ğ‘€,ğ‘š,0 from the outputs of the last layer in each module and then concatenating them. We have also tried other methods such as average pooling and LSTM or GRU and found them producing similar results, so we chose the most computationally efficient one. As there are two heads in each of the two multimodal complementation layers, concatenating all the outputs of these four heads in total gives the final representation h final âˆˆ R 4ğ‘‘ , where the dimension of each head output is ğ‘‘. Finally, the representation vector is fed to a feed-forward network to produce the final prediction Å·.</p><p>The loss function comprises two parts: the task loss and the sum of all separators' classification loss. In the case of sentiment intensity prediction, the task loss is the mean squared error (MSE), since it is a regression problem. In the case of humor detection, we used binary cross-entropy (BCE) loss to facilitate the training for this binary classification task. Separator loss is a layer-wise loss so we sum up the results that are computed in each layer and add them to the total loss. The total loss is calculated as</p><formula xml:id="formula_10">L = 1 ğ‘ ğ‘ ğ‘ ğ‘ âˆ‘ï¸ ğ‘—=1 ğœ (ğ‘¦ ğ‘— , Å·ğ‘— ) + ğœ†ğ¾ 2ğ¿ ğ¿ âˆ‘ï¸ ğ‘–=1 âˆ‘ï¸ ğ‘€ L ğ‘€,ğ‘– ğ‘ ğ‘’ğ‘ ,</formula><p>where ğœ denotes the task loss and ğœ† is a tunable parameter to control the power of regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS 4.1 Datasets</head><p>We evaluated our BBFN model on two tasks: sentiment intensity prediction and humor detection, with three datasets involved.</p><p>â€¢ CMU-MOSI. The CMU-MOSI dataset <ref type="bibr" target="#b44">[45]</ref> is a prevalent benchmark for evaluating fusion networks' performance on the sentiment intensity prediction task. The dataset is composed of many YouTube video blogs or vlogs, in which a speaker narrates their opinions on some topic. It contains 2,199 utterance-video segments sliced from 93 videos played by 89 distinct narrators. Each segment is manually annotated with a real number score ranged from âˆ’3 to +3, indicating the relative strength of negative (score below zero) or positive (score above zero) emotion. â€¢ CMU-MOSEI. The CMU-MOSEI dataset <ref type="bibr" target="#b45">[46]</ref> is an upgraded version of CMU-MOSI concerning the number of samples. It is also enriched in terms of the versatility of speakers and covers a broader scope of topics. The dataset contains 23,453 video segments, which are annotated in the same way as CMU-MOSI. These segments are extracted from 5,000 videos involving 1,000 distinct speakers and 250 different topics. â€¢ UR-FUNNY. UR-FUNNY <ref type="bibr" target="#b14">[15]</ref> is a popular humor detection dataset, as our test benchmark. This dataset contains 16,514 multimodal punchlines sampled from the TED talks. Each sample is annotated with an equal number of binary labels indicating if the protagonist in a video expresses a sort of humor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Preprocessing</head><p>To produce machine-understandable inputs for our model and ensure fair competition with other baselines, following many previous works we process data from the three modalities into typical tensors as introduced below.</p><p>Text Modality. Many previous works adopted <ref type="bibr" target="#b23">[24]</ref> as word embedding sources. But recent works including current SOTA preferred advanced pretrained language models. Therefore as we stated before in Section 3.2 to encode input raw text in all experiments.</p><p>Visual Modality. Specifically, for experiments on CMU-MOSI and CMU-MOSEI, we use Facet, an analytical tool built on the Facial Action Coding Systems (FACS) <ref type="bibr" target="#b10">[11]</ref> to extracted facial features. For experiments on UR-FUNNY we use another facial behavioral analysis tool, Openface <ref type="bibr" target="#b2">[3]</ref> to capture facial gesture variance of every speaker. The resulting vector lengths for the three datasets (MOSI, MOSEI and UR-FUNNY) are 47, 35 and 75 respectively. Acoustic Modality. Acoustic features were extracted with CO-VAREP <ref type="bibr" target="#b7">[8]</ref>, a professional acoustic analysis framework.</p><p>Modality Alignment. The input signals in our experiments were word-aligned. Following many previous works <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b43">44]</ref>, we used P2FA <ref type="bibr" target="#b41">[42]</ref> to align visual and acoustic signals to the same resolution of text. The tool automatically separates numerous frames into several groups and match each group with a token by averaging their representation vectors to a new one. We used BERT-base-uncased as the text embedding source for all models in all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baselines and Metrics</head><p>We compared our results with several advanced multimodal fusion frameworks:</p><p>â€¢ DFF-ATMF <ref type="bibr" target="#b4">[5]</ref>: It is the first bimodal model which first learns individual modality features then executes attentionbased modality fusion.</p><p>â€¢ Low-rank Matrix Fusion (LMF) <ref type="bibr" target="#b17">[18]</ref>: It decomposes highorder tensors into many low-rank factors then performs efficient fusion based on these factors. â€¢ Tensor Fusion Network (TFN) <ref type="bibr" target="#b42">[43]</ref>: This approach models intra-modality and inter-modality dynamics concurrently with local feature extraction network and 3-fold Cartesian product. â€¢ Multimodal Factorization Model (MFM) <ref type="bibr" target="#b34">[35]</ref>: To enhance the robustness of the model of capturing intra-and intermodality dynamics, MFM is a cycle style generative-discriminative model. â€¢ Interaction Canonical Correlation Network (ICCN) <ref type="bibr" target="#b31">[32]</ref>:</p><p>Correlation between modalities is a latent trend to be excavated under the fusion process. ICCN purely relies on mathematical measure to accomplish the fusion process. â€¢ MulT <ref type="bibr" target="#b33">[34]</ref>: To alleviate the drawback of hard temporal alignment for multimodal signals, MulT utilizes stacked transformer networks to perform soft alignment to extend the number of positions on the time axis that each frame of signal can interact with. â€¢ MISA <ref type="bibr" target="#b15">[16]</ref>: Motivated by previous work in domain separation task, this work regards signals from different modalities as data in different domains and similarly constructs two kinds of feature spaces to finish the fusion process.</p><p>We used five different metrics to evaluate the performance on CMU-MOSI and CMU-MOSEI: mean absolute error (MAE), which directly calculates the error between predictions and real-number labels; seven-class accuracy (Acc-7), positive/negative (excluding zero labels) accuracy (Acc-2) and F1 score, which coarsely estimate the model's performance by comparing with quantified values; and Pearson correlation (Corr) with human-annotated truth, which measures standard deviation. As for humor recognition on UR-FUNNY, it is a binary classification problem and we only report the binary classification accuracy (Acc-2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS AND ANALYSIS 5.1 Summary of Results</head><p>We list the results with baselines on all the three datasets in Table <ref type="table" target="#tab_1">1</ref>.</p><p>From these results, it can be found that our BBFN outperforms other models on almost all metrics (except for the correlation coefficient on CMU-MOSI). We attained an improvement of around 1% over the state-of-the-art approaches in terms of binary classification accuracy and more than 2.5% in terms of 7-class accuracy. We attribute the difference partly to the insensitivity of coarse metrics to the variation in the model's predictions. The best performance boost, more than 4%, was on the MAE of CMU-MOSEI.</p><p>To better illustrate how BBFN beats the SOTA (MISA), we compute the absolute errors of all the predictions on the test set of CMU-MOSEI and paint their distributions in Fig. <ref type="figure" target="#fig_3">4</ref>. It can be observed that in the low error part (error&lt; 0.25) the curve of BBFN has more peaks than MISA, which demonstrates the higher precision that our model can reach. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ablation Study</head><p>To examine the functionality of the overall architecture and the components introduced in this work, we conducted an ablation study on CMU-MOSEI dataset; see 3.</p><p>In five experiments we verified the effect of the bimodal fusion architecture. Specifically, we (1) only used one pair as input; <ref type="bibr" target="#b1">(2)</ref> replaced input text-related modality pairs (TV, TA) with visual/acoustic-related ones; and (3) added a visual-acoustic complementation module to make a ternary symmetric model. In all cases, separators and gates were used.</p><p>The models of type (2) outperformed those of type (1) on MAE and Acc-7 (the most accurate measures), which indicates that all three modalities are important. Moreover, the performance of visualfocused input (TV+VA) is close to that of text-focused input (our TV+TA), i.e., our architecture can operate on these modality pairs, too.</p><p>On the other hand, the performance degrades on type (3), when visual-acoustic input pairs are added. That is, even after including all modalities in the input, redundant network architecture can cause harmful effects bringing in malicious noise, which damages collected useful information and confuses the model.</p><p>We also explored the benefits of the feature-space separator and gated control by removing the separator, the two gates, or both from our BBFN. The outcome shows some degradation in all metrics except the correlation. This proves that including gated control and modality separator improves the model's performance, though the greatest improvement over the baselines shown in Table <ref type="table" target="#tab_3">3</ref> comes from our overall bi-bimodal architecture. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Further Analysis</head><p>To study how the gates affect the information flow, we visualized the weights in all the gates per dimension; see Fig. <ref type="figure" target="#fig_4">5</ref>. We hypothesize that the discrepancy in weight distributions reflects the relative importance of modalities. Specifically, for the two gates associated with one modality in the same module, if the weights in the compound gate are greater than those in the retain gate, it implies that the model enforces the corresponding modality to learn much from its counterpart in the module and the modality thus is less important. Conversely, if the weights of the retain gate are greater, then the modality is more important than its counterpart. Fig. <ref type="figure" target="#fig_1">2</ref> shows three typical samples, including raw data input (for visual and acoustic we only give short descriptions), predictions and truths from the test set. In case A, most cues are attained from the text modality to express sort of disappointment, while data from the visual and acoustic modalities are not so informative. Hence for the acoustic and visual, the model makes the corresponding modalities A and V pay more attention to the heterogeneous attention results instead of themselves, which is indicated by the larger weights in the compound gates of the two modalities (TA-A-c &amp; TV-V-c).  In case B, the V and A modalities are seen to be providing key information along with T. Therefore, the weight difference is indiscernible, and two paths of information flow achieve a balance. Surprisingly, Fig. <ref type="figure" target="#fig_4">5</ref> shows that for the text modality in the TV complementation module, the weights in the compound gate are slightly higher than those in the retain gate. This implies that the textual modality can be complemented by the information attained from the visual modality.</p><p>In case C, no single modality can provide clear evidence for the neutral sentiment, but each of them serves as a favorable supplementary to others. Therefore, in Fig. <ref type="figure" target="#fig_4">5</ref> we find the weights in the gates of each modality are comparable, indicating the similar dependency of bimodal fusion results on both modalities.</p><p>We also compared our BBFN with MISA for the final prediction in the two cases. As shown in Table <ref type="table" target="#tab_2">2</ref>, BBFN's outputs are closer to the ground truth, owing to the fine-grained control offered by these gates, whereas MISA makes opposite (case A) or conservative (case B) predictions because, as a result of ternary-symmetric architectures, it is distracted by insignificant modalities, which add disturbing factors and dilute the pertinent features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We have presented Bi-Bimodal Fusion Network (BBFN), a fusion architecture for multimodal sentiment analysis that focuses on bimodal fusion process. Pairwise fusion process proceeds progressively through stacked complementation layers in each learning module. To alleviate the issue of feature space collapse and lack of control at fusion time, we introduced into our model the structure of modality-specific feature space separator and gated control mechanism, respectively. Comprehensive experiments and analysis show that our model outperforms the current state-of-the-art approaches. Despite good performance of our model, we plan to explore more advanced fusion methods and architectures. Also besides sentiment analysis, in multimodal research there are many other important tasks, for which we can combine task-specific techniques with appropriate fusion schemes. Accordingly, we plan to improve the fusion quality of multimodal data as well as the coordination of fusion and task solving modules.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Task formulation of the multimodal sentiment analysis and two types of fusion schemes. The displayed data are sampled from CMU-MOSI dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overview of our Bi-Bimodal Fusion Network (BBFN). It learns two text-related pairs of representations, TA and TV, by causing each pair of modalities to complement mutually. Finally, the four (two pairs) head representations are concatenated to generate the final prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1 ğ‘–Figure 3 :</head><label>13</label><figDesc>Figure 3: A single complementation layer: two identical pipelines (left and right) propagate the main modality and fuse that with complementary modality with regularization and gated control.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Distribution of absolute error when testing BBFN and MISA on CMU-MOSEI dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Visualization of eight gated control signals in the second layer of our BBFN for two case study samples. "XY-X/Y-c/r" denotes the compound/retain gate in the transformer pipeline for X/Y modality in XY complementation module.</figDesc><graphic url="image-13.png" coords="7,317.04,223.43,242.08,176.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Results on the test set of CMU-MOSI and CMU-MOSEI dataset. Notation: â–³ indicates results in the corresponding line are excerpted from previous papers; â€  means the results are reproduced with publicly visible source code and applicable hyperparameter setting; â€¡ shows the results have experienced paired t-test with ğ‘ &lt; 0.05 and demonstrate significant improvement over MISA, the state-of-the-art model. But, I mean, if you're going to watch a movie like that, go see Saw again or something, because this movie is really not good at all. So if you're looking for something it's sort of lighthearted.</figDesc><table><row><cell></cell><cell>Models</cell><cell></cell><cell cols="2">CMU-MOSI</cell><cell></cell><cell></cell><cell></cell><cell cols="2">CMU-MOSEI</cell><cell></cell><cell cols="2">UR-FUNNY</cell></row><row><cell></cell><cell></cell><cell>MAE</cell><cell cols="3">Corr Acc-7 Acc-2</cell><cell>F1</cell><cell>MAE</cell><cell cols="3">Corr Acc-7 Acc-2</cell><cell>F1</cell><cell>Acc-2</cell></row><row><cell></cell><cell>DFF-ATMF â–³</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>80.9</cell><cell>81.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>77.1</cell><cell>78.3</cell><cell>-</cell></row><row><cell></cell><cell>LMF â–³</cell><cell cols="2">0.917 0.695</cell><cell>33.2</cell><cell>82.5</cell><cell cols="3">82.4 0.623 0.677</cell><cell>48.0</cell><cell>82.0</cell><cell>82.1</cell><cell>67.53</cell></row><row><cell></cell><cell>TFN â–³</cell><cell cols="2">0.901 0.698</cell><cell>34.9</cell><cell>80.8</cell><cell cols="3">80.7 0.593 0.700</cell><cell>50.2</cell><cell>82.5</cell><cell>82.1</cell><cell>68.57</cell></row><row><cell></cell><cell>MFM â–³</cell><cell cols="2">0.877 0.706</cell><cell>35.4</cell><cell>81.7</cell><cell cols="3">81.6 0.568 0.717</cell><cell>51.3</cell><cell>84.4</cell><cell>84.3</cell><cell>-</cell></row><row><cell></cell><cell>ICCN â–³</cell><cell cols="2">0.862 0.714</cell><cell>39.0</cell><cell>83.0</cell><cell cols="3">83.0 0.565 0.713</cell><cell>51.6</cell><cell>84.2</cell><cell>84.2</cell><cell>-</cell></row><row><cell></cell><cell>MulT  â€ </cell><cell cols="2">0.832 0.745</cell><cell>40.1</cell><cell>83.3</cell><cell cols="3">82.9 0.570 0.758</cell><cell>51.1</cell><cell>84.5</cell><cell>84.5</cell><cell>70.55</cell></row><row><cell></cell><cell>MISA  â€ </cell><cell cols="2">0.817 0.748</cell><cell>41.4</cell><cell>82.1</cell><cell>82</cell><cell cols="2">0.557 0.748</cell><cell>51.7</cell><cell>84.9</cell><cell>84.8</cell><cell>70.61</cell></row><row><cell></cell><cell>BBFN  â€¡ (Ours)</cell><cell cols="2">0.776 0.755</cell><cell>45.0</cell><cell>84.3</cell><cell cols="3">84.3 0.529 0.767</cell><cell>54.8</cell><cell>86.2</cell><cell>86.1</cell><cell>71.68</cell></row><row><cell>Case</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Input by Modality</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Prediction &amp; Truth</cell></row><row><cell></cell><cell></cell><cell>Text</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Visual</cell><cell cols="2">Acoustic</cell><cell cols="2">Prediction Truth ABS Error</cell></row><row><cell>A</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Widely opened eyes</cell><cell cols="2">Pause and Stress</cell><cell>âˆ’1.973</cell><cell>âˆ’2.000</cell><cell>0.027</cell></row><row><cell>B</cell><cell cols="5">Plot to it than that the action scenes were my favorite parts though it's.</cell><cell></cell><cell cols="2">Smiling face Relaxed wink</cell><cell cols="2">Stress Pitch variation</cell><cell>+1.638</cell><cell>+1.666</cell><cell>0.028</cell></row><row><cell>C</cell><cell cols="8">(umm) No expression</cell><cell cols="2">Normal Voice Peaceful tone</cell><cell></cell><cell>0.000</cell><cell>0.016</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Input and predictions of two samples in our case study.</figDesc><table><row><cell>Description</cell><cell>MAE</cell><cell cols="3">Corr Acc-7 Acc-2</cell><cell>F1</cell></row><row><cell>TV only</cell><cell cols="2">0.546 0.761</cell><cell>51.8</cell><cell>85.6</cell><cell>85.6</cell></row><row><cell>TA only</cell><cell cols="2">0.548 0.759</cell><cell>51.7</cell><cell>85.5</cell><cell>85.5</cell></row><row><cell>VA only</cell><cell cols="2">0.816 0.261</cell><cell>41.1</cell><cell>71.1</cell><cell>64.5</cell></row><row><cell>VA+TA</cell><cell cols="2">0.533 0.773</cell><cell>54.1</cell><cell>84.8</cell><cell>84.9</cell></row><row><cell>TV+VA</cell><cell cols="2">0.531 0.775</cell><cell>54.5</cell><cell>85.7</cell><cell>85.7</cell></row><row><cell>TV+TA (BBFN)</cell><cell cols="2">0.529 0.767</cell><cell>54.8</cell><cell>86.2</cell><cell>86.1</cell></row><row><cell cols="3">w/o separator 0.533 0.766</cell><cell>54.1</cell><cell>85.7</cell><cell>85.4</cell></row><row><cell>w/o gates</cell><cell cols="2">0.531 0.768</cell><cell>53.9</cell><cell>85.8</cell><cell>85.7</cell></row><row><cell>w/o both</cell><cell cols="2">0.540 0.763</cell><cell>53.0</cell><cell>85.1</cell><cell>85.0</cell></row><row><cell>TV+TA+VA</cell><cell cols="2">0.547 0.768</cell><cell>52.8</cell><cell>84.3</cell><cell>84.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>An ablation study of BBFN's architecture and functional components on the test set of CMU-MOSEI.</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multi-task Learning for Multi-modal Emotion Recognition and Sentiment Analysis</title>
		<author>
			<persName><forename type="first">Shad</forename><surname>Md</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dushyant</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepanway</forename><surname>Chauhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soujanya</forename><surname>Ghosal</surname></persName>
		</author>
		<author>
			<persName><surname>Poria</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/N19-1034.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="370" to="379" />
		</imprint>
	</monogr>
	<note>Asif Ekbal, and Pushpak Bhattacharyya</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Multimodal fusion for multimedia analysis: a survey. Multimedia systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pradeep K Atrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdulmotaleb</forename><forename type="middle">El</forename><surname>Anwar Hossain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohan</forename><forename type="middle">S</forename><surname>Saddik</surname></persName>
		</author>
		<author>
			<persName><surname>Kankanhalli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="345" to="379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Openface: an open facial behavior analysis toolkit</title>
		<author>
			<persName><forename type="first">Tadas</forename><surname>BaltruÅ¡aitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multimodal learning analytics and education data mining: Using computational technologies to measure complex learning tasks</title>
		<author>
			<persName><forename type="first">Paulo</forename><surname>Blikstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcelo</forename><surname>Worsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Learning Analytics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="220" to="238" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Feiyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziqian</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dengfeng</forename><surname>Ke</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08138</idno>
		<ptr target="http://ceur-ws.org/Vol-2614/AffCon20_session1_complementary.pdf" />
		<title level="m">Complementary Fusion of Multi-Features and Multi-Modalities in Sentiment Analysis</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multimodal sentiment analysis with word-level fusion and reinforcement learning</title>
		<author>
			<persName><forename type="first">Minghai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tadas</forename><surname>BaltruÅ¡aitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<idno type="DOI">10.1145/3136755.3136801</idno>
		<ptr target="https://dl.acm.org/doi/pdf/10.1145/3136755.3136801" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM International Conference on Multimodal Interaction</title>
				<meeting>the 19th ACM International Conference on Multimodal Interaction</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="163" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1412.3555.pdf" />
	</analytic>
	<monogr>
		<title level="m">NIPS 2014 Workshop on Deep Learning</title>
				<imprint>
			<date type="published" when="2014-12">2014. December 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">COVAREP-A collaborative voice analysis repository for speech technologies</title>
		<author>
			<persName><forename type="first">Gilles</forename><surname>Degottex</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Kane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Drugman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tuomo</forename><surname>Raitio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Scherer</surname></persName>
		</author>
		<ptr target="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6853739" />
	</analytic>
	<monogr>
		<title level="m">2014 ieee international conference on acoustics, speech and signal processing (icassp)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="960" to="964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/N19-1423.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<ptr target="https://arxiv.org/pdf/2010.11929.pdf" />
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">What the face reveals: Basic and applied studies of spontaneous expression using the Facial Action Coding System (FACS)</title>
		<author>
			<persName><surname>Rosenberg Ekman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>Oxford University Press</publisher>
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adversarial evaluation of multimodal machine translation</title>
		<author>
			<persName><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/D18-1329.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2974" to="2978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santiago</forename><surname>FernÃ¡ndez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faustino</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">JÃ¼rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
				<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multimodal Affective Analysis Using Hierarchical Attention Strategy with Word-Level Alignment</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kangning</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Marsic</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/P18-1207.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2225" to="2235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">UR-FUNNY: A Multimodal Language Dataset for Understanding Humor</title>
		<author>
			<persName><forename type="first">Md</forename><surname>Kamrul Hasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wasifur</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amirali</forename><surname>Bagher Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyuan</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Md</forename><surname>Iftekhar Tanveer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><forename type="middle">Ehsan</forename><surname>Hoque</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/D19-1211.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP-IJCNLP</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2046" to="2056" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">MISA: Modality-Invariant and-Specific Representations for Multimodal Sentiment Analysis</title>
		<author>
			<persName><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<idno type="DOI">10.1145/3394171.3413678</idno>
		<ptr target="https://dl.acm.org/doi/pdf/10.1145/3394171.3413678" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia. 1122-1131</title>
				<meeting>the 28th ACM International Conference on Multimedia. 1122-1131</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Unsupervised multimodal neural machine translation with pseudo visual pivoting</title>
		<author>
			<persName><forename type="first">Po-Yao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Hauptmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.03119</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Efficient Low-rank Multimodal Fusion With Modality-Specific Factors</title>
		<author>
			<persName><forename type="first">Zhun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Bharadhwaj Lakshminarasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amirali</forename><surname>Bagher Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/P18-1209.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2247" to="2256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Visual attention model for name tagging in multimodal social media</title>
		<author>
			<persName><forename type="first">Di</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonardo</forename><surname>Neves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitor</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/P18-1185.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1990" to="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">M3er: Multiplicative multimodal emotion recognition using facial, textual, and speech cues</title>
		<author>
			<persName><forename type="first">Trisha</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uttaran</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Chandra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1359" to="1367" />
		</imprint>
	</monogr>
	<note>Aniket Bera, and Dinesh Manocha</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multimodal Named Entity Recognition for Short Social Media Posts</title>
		<author>
			<persName><forename type="first">Seungwhan</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonardo</forename><surname>Neves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitor</forename><surname>Carvalho</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/N18-1078.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="852" to="860" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Towards multimodal sentiment analysis: Harvesting opinions from the web</title>
		<author>
			<persName><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Payal</forename><surname>Doshi</surname></persName>
		</author>
		<idno type="DOI">10.1145/2070481.2070509</idno>
		<ptr target="https://dl.acm.org/doi/pdf/10.1145/2070481.2070509" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th international conference on multimodal interfaces</title>
				<meeting>the 13th international conference on multimodal interfaces</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="169" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multimodal deep learning</title>
		<author>
			<persName><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juhan</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<ptr target="https://icml.cc/2011/papers/399_icmlpaper.pdf" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="689" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
				<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Found in translation: Learning robust joint representations by cyclic translations between modalities</title>
		<author>
			<persName><forename type="first">Hai</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Manzini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">BarnabÃ¡s</forename><surname>PÃ³czos</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1812.07809.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6892" to="6899" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Improving Zero-shot Translation with Language-Independent Constraints</title>
		<author>
			<persName><forename type="first">Ngoc-Quan</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thanh-Le</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Waibel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08584</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Integrating multimodal information in large pretrained transformers</title>
		<author>
			<persName><forename type="first">Wasifur</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Md</forename><surname>Kamrul Hasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sangwu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengfeng</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Hoque</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference</title>
				<meeting>the conference</meeting>
		<imprint>
			<publisher>NIH Public Access</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page">2359</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Visually Grounded Neural Syntax Acquisition</title>
		<author>
			<persName><forename type="first">Haoyue</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayuan</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/P19-1180.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. 1842-1861</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics. 1842-1861</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning Visually-Grounded Semantics from Contrastive Adversarial Samples</title>
		<author>
			<persName><forename type="first">Haoyue</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayuan</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/C18-1315.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
				<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3715" to="3727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A shared task on multimodal machine translation and crosslingual image description</title>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khalil</forename><surname>Sima'an</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/W16-2346.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation</title>
				<meeting>the First Conference on Machine Translation</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="543" to="553" />
		</imprint>
	</monogr>
	<note>Shared Task Papers</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multimodal learning with deep Boltzmann machines</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<ptr target="https://www.jmlr.org/papers/volume15/srivastava14b/srivastava14b.pdf" />
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="2949" to="2980" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning relationships between text, audio, and video via deep canonical correlation for multimodal language analysis</title>
		<author>
			<persName><forename type="first">Zhongkai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prathusha</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Sethares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<ptr target="https://ojs.aaai.org//index.php/AAAI/article/view/6431" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8992" to="8999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Multi-modal sentiment analysis using deep canonical correlation analysis</title>
		<author>
			<persName><forename type="first">Zhongkai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Prathusha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><forename type="middle">P</forename><surname>Sethares</surname></persName>
		</author>
		<author>
			<persName><surname>Bucy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.08696</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multimodal Transformer for Unaligned Multimodal Language Sequences</title>
		<author>
			<persName><forename type="first">Yao-Hung Hubert</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaojie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zico</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/P19-1656.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6558" to="6569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning Factorized Multimodal Representations</title>
		<author>
			<persName><forename type="first">Yao-Hung Hubert</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<ptr target="https://openreview.net/pdf?id=rygqqsA9KX" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Representation Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multimodal Routing: Improving Local and Global Interpretability of Multimodal Language Analysis</title>
		<author>
			<persName><forename type="first">Yao-Hung Hubert</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muqiao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/2020.emnlp-main.143.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1823" to="1833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Åukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<ptr target="https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems. 5998-6008</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Select-additive learning: Improving generalization in multimodal sentiment analysis</title>
		<author>
			<persName><forename type="first">Haohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaksha</forename><surname>Meghawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Multimedia and Expo (ICME)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">YouTube movie reviews: Sentiment analysis in an audio-visual context</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>WÃ¶llmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Weninger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Knaup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">BjÃ¶rn</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Congkai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Sagae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<ptr target="https://ieeexplore.ieee.org/iel7/9670/5196652/06487473.pdf" />
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="46" to="53" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multimodal transformer for multimodal machine translation</title>
		<author>
			<persName><forename type="first">Shaowei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4346" to="4350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Improving multimodal named entity recognition via entity span detection with unified multimodal transformer</title>
		<author>
			<persName><forename type="first">Jianfei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Xia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Association Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Speaker identification on the SCOTUS corpus</title>
		<author>
			<persName><forename type="first">Jiahong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Liberman</surname></persName>
		</author>
		<ptr target="https://www.ling.upenn.edu/~jiahong/publications/c09.pdf" />
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="page" from="3878" to="3878" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Tensor Fusion Network for Multimodal Sentiment Analysis</title>
		<author>
			<persName><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minghai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/D17-1115.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1103" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Multi-attention recurrent network for human communication comprehension</title>
		<author>
			<persName><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prateek</forename><surname>Vij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1802.00923.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Multimodal sentiment intensity analysis in videos: Facial gestures and verbal messages</title>
		<author>
			<persName><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Pincus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<ptr target="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7742221" />
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="82" to="88" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Multimodal language analysis in the wild: CMU-MOSEI dataset and interpretable dynamic fusion graph</title>
		<author>
			<persName><forename type="first">Amirali</forename><surname>Bagher Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/P18-1208.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2236" to="2246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Adaptive co-attention network for named entity recognition in tweets</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinlan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<ptr target="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16432/16127" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
				<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Visually Grounded Compound PCFGs</title>
		<author>
			<persName><forename type="first">Yanpeng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/2020.emnlp-main.354.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4369" to="4379" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
