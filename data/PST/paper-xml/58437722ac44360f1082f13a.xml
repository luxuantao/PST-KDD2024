<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TEMPORAL ENSEMBLING FOR SEMI-SUPERVISED LEARNING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-03-15">15 Mar 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
							<email>slaine@nvidia.com</email>
						</author>
						<author>
							<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
							<email>taila@nvidia.com</email>
						</author>
						<title level="a" type="main">TEMPORAL ENSEMBLING FOR SEMI-SUPERVISED LEARNING</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-03-15">15 Mar 2017</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1610.02242v3[cs.NE]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present a simple and efficient method for training deep neural networks in a semi-supervised setting where only a small portion of training data is labeled. We introduce self-ensembling, where we form a consensus prediction of the unknown labels using the outputs of the network-in-training on different epochs, and most importantly, under different regularization and input augmentation conditions. This ensemble prediction can be expected to be a better predictor for the unknown labels than the output of the network at the most recent training epoch, and can thus be used as a target for training. Using our method, we set new records for two standard semi-supervised learning benchmarks, reducing the (non-augmented) classification error rate from 18.44% to 7.05% in SVHN with 500 labels and from 18.63% to 16.55% in CIFAR-10 with 4000 labels, and further to 5.12% and 12.16% by enabling the standard augmentations. We additionally obtain a clear improvement in CIFAR-100 classification accuracy by using random images from the Tiny Images dataset as unlabeled extra inputs during training. Finally, we demonstrate good tolerance to incorrect labels.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>It has long been known that an ensemble of multiple neural networks generally yields better predictions than a single network in the ensemble. This effect has also been indirectly exploited when training a single network through dropout <ref type="bibr">(Srivastava et al., 2014)</ref>, dropconnect <ref type="bibr">(Wan et al., 2013)</ref>, or stochastic depth <ref type="bibr" target="#b7">(Huang et al., 2016)</ref> regularization methods, and in swapout networks <ref type="bibr" target="#b24">(Singh et al., 2016)</ref>, where training always focuses on a particular subset of the network, and thus the complete network can be seen as an implicit ensemble of such trained sub-networks. We extend this idea by forming ensemble predictions during training, using the outputs of a single network on different training epochs and under different regularization and input augmentation conditions. Our training still operates on a single network, but the predictions made on different epochs correspond to an ensemble prediction of a large number of individual sub-networks because of dropout regularization.</p><p>This ensemble prediction can be exploited for semi-supervised learning where only a small portion of training data is labeled. If we compare the ensemble prediction to the current output of the network being trained, the ensemble prediction is likely to be closer to the correct, unknown labels of the unlabeled inputs. Therefore the labels inferred this way can be used as training targets for the unlabeled inputs. Our method relies heavily on dropout regularization and versatile input augmentation. Indeed, without neither, there would be much less reason to place confidence in whatever labels are inferred for the unlabeled training data.</p><p>We describe two ways to implement self-ensembling, Π-model and temporal ensembling. Both approaches surpass prior state-of-the-art results in semi-supervised learning by a considerable margin. We furthermore observe that self-ensembling improves the classification accuracy in fully labeled cases as well, and provides tolerance against incorrect labels.</p><p>The recently introduced transform/stability loss of <ref type="bibr" target="#b20">Sajjadi et al. (2016b)</ref> is based on the same principle as our work, and the Π-model can be seen as a special case of it. The Π-model can also be seen as a simplification of the Γ-model of the ladder network by <ref type="bibr" target="#b17">Rasmus et al. (2015)</ref>, a previously presented network architecture for semi-supervised learning. Our temporal ensembling method has connections to the bootstrapping method of <ref type="bibr" target="#b18">Reed et al. (2014)</ref> targeted for training with noisy labels. </p><formula xml:id="formula_0">z i∈B ← f θ (g(x i∈B )) evaluate network outputs for augmented inputs zi∈B ← f θ (g(x i∈B ))</formula><p>again, with different dropout and augmentation</p><formula xml:id="formula_1">loss ← − 1 |B| i∈(B∩L) log z i [y i ] supervised loss component + w(t) 1 C|B| i∈B ||z i − zi || 2</formula><p>unsupervised loss component update θ using, e.g., ADAM update network parameters end for end for return θ</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">SELF-ENSEMBLING DURING TRAINING</head><p>We present two implementations of self-ensembling during training. The first one, Π-model, encourages consistent network output between two realizations of the same input stimulus, under two different dropout conditions. The second method, temporal ensembling, simplifies and extends this by taking into account the network predictions over multiple previous training epochs.</p><p>We shall describe our methods in the context of traditional image classification networks. Let the training data consist of total of N inputs, out of which M are labeled. The input stimuli, available for all training data, are denoted x i , where i ∈ {1 . . . N }. Let set L contain the indices of the labeled inputs, |L| = M . For every i ∈ L, we have a known correct label y i ∈ {1 . . . C}, where C is the number of different classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Π-MODEL</head><p>The structure of Π-model is shown in Figure <ref type="figure" target="#fig_0">1</ref> (top), and the pseudocode in Algorithm 1. During training, we evaluate the network for each training input x i twice, resulting in prediction vectors z i and zi . Our loss function consists of two components. The first component is the standard crossentropy loss, evaluated for labeled inputs only. The second component, evaluated for all inputs, penalizes different predictions for the same training input x i by taking the mean square difference between the prediction vectors z i and zi .<ref type="foot" target="#foot_0">1</ref> To combine the supervised and unsupervised loss terms, we scale the latter by time-dependent weighting function w(t). By comparing the entire output vectors z i and zi , we effectively ask the "dark knowledge" <ref type="bibr" target="#b6">(Hinton et al., 2015)</ref> between the two evaluations to be close, which is a much stronger requirement compared to asking that only the final classification remains the same, which is what happens in traditional training.</p><p>It is important to notice that, because of dropout regularization, the network output during training is a stochastic variable. Thus two evaluations of the same input x i under same network weights θ yield different results. In addition, Gaussian noise and augmentations such as random translation are evaluated twice, resulting in additional variation. The combination of these effects explains the difference between the prediction vectors z i and zi . This difference can be seen as an error in classification, given that the original input x i was the same, and thus minimizing it is a reasonable goal.</p><p>In our implementation, the unsupervised loss weighting function w(t) ramps up, starting from zero, along a Gaussian curve during the first 80 training epochs. See Appendix A for further details about this and other training parameters. In the beginning the total loss and the learning gradients are thus dominated by the supervised loss component, i.e., the labeled data only. We have found it to be very important that the ramp-up of the unsupervised loss component is slow enough-otherwise, the network gets easily stuck in a degenerate solution where no meaningful classification of the data is obtained.</p><p>Our approach is somewhat similar to the Γ-model of the ladder network by <ref type="bibr" target="#b17">Rasmus et al. (2015)</ref>, but conceptually simpler. In the Π-model, the comparison is done directly on network outputs, i.e., after softmax activation, and there is no auxiliary mapping between the two branches such as the learned denoising functions in the ladder network architecture. Furthermore, instead of having one "clean" and one "corrupted" branch as in Γ-model, we apply equal augmentation and noise to the inputs for both branches.</p><p>As shown in Section 3, the Π-model combined with a good convolutional network architecture provides a significant improvement over prior art in classification accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">TEMPORAL ENSEMBLING</head><p>Analyzing how the Π-model works, we could equally well split the evaluation of the two branches in two separate phases: first classifying the training set once without updating the weights θ, and then training the network on the same inputs under different augmentations and dropout, using the just obtained predictions as targets for the unsupervised loss component. As the training targets obtained this way are based on a single evaluation of the network, they can be expected to be noisy. Temporal ensembling alleviates this by aggregating the predictions of multiple previous network evaluations into an ensemble prediction. It also lets us evaluate the network only once during training, gaining an approximate 2x speedup over the Π-model.</p><p>The structure of our temporal ensembling method is shown in Figure <ref type="figure" target="#fig_0">1</ref> (bottom), and the pseudocode in Algorithm 2. The main difference to the Π-model is that the network and augmentations are evaluated only once per input per epoch, and the target vectors z for the unsupervised loss component are based on prior network evaluations instead of a second evaluation of the network.</p><p>After every training epoch, the network outputs z i are accumulated into ensemble outputs Z i by updating Z i ← αZ i + (1 − α)z i , where α is a momentum term that controls how far the ensemble reaches into training history. Because of dropout regularization and stochastic augmentation, Z thus contains a weighted average of the outputs of an ensemble of networks f from previous training epochs, with recent epochs having larger weight than distant epochs. For generating the training targets z, we need to correct for the startup bias in Z by dividing by factor (1 − α t ). A similar bias correction has been used in, e.g., Adam (Kingma &amp; Ba, 2014) and mean-only batch normalization <ref type="bibr" target="#b21">(Salimans &amp; Kingma, 2016)</ref>. On the first training epoch, Z and z are zero as no data from previous epochs is available. For this reason, we specify the unsupervised weight ramp-up function w(t) to also be zero on the first training epoch.</p><p>Algorithm 2 Temporal ensembling pseudocode. Note that the updates of Z and z could equally well be done inside the minibatch loop; in this pseudocode they occur between epochs for clarity. Require: x i = training stimuli Require: L = set of training input indices with known labels Require: y i = labels for labeled inputs i ∈ L Require: α = ensembling momentum, 0 ≤ α &lt; 1 Require: w(t) = unsupervised weight ramp-up function Require: f θ (x) = stochastic neural network with trainable parameters θ Require: g(x) = stochastic input augmentation function</p><formula xml:id="formula_2">Z ← 0 [N ×C] initialize ensemble predictions z ← 0 [N ×C] initialize target vectors for t in [1, num epochs] do for each minibatch B do z i∈B ← f θ (g(x i∈B , t)) evaluate network outputs for augmented inputs loss ← − 1 |B| i∈(B∩L) log z i [y i ] supervised loss component + w(t) 1 C|B| i∈B ||z i − zi || 2</formula><p>unsupervised loss component update θ using, e.g., ADAM update network parameters end for</p><formula xml:id="formula_3">Z ← αZ + (1 − α)z accumulate ensemble predictions z ← Z/(1 − α t )</formula><p>construct target vectors by bias correction end for return θ</p><p>The benefits of temporal ensembling compared to Π-model are twofold. First, the training is faster because the network is evaluated only once per input on each epoch. Second, the training targets z can be expected to be less noisy than with Π-model. As shown in Section 3, we indeed obtain somewhat better results with temporal ensembling than with Π-model in the same number of training epochs. The downside compared to Π-model is the need to store auxiliary data across epochs, and the new hyperparameter α. While the matrix Z can be fairly large when the dataset contains a large number of items and categories, its elements are accessed relatively infrequently. Thus it can be stored, e.g., in a memory mapped file.</p><p>An intriguing additional possibility of temporal ensembling is collecting other statistics from the network predictions z i besides the mean. For example, by tracking the second raw moment of the network outputs, we can estimate the variance of each output component z i,j . This makes it possible to reason about the uncertainty of network outputs in a principled way <ref type="bibr" target="#b3">(Gal &amp; Ghahramani, 2016)</ref>. Based on this information, we could, e.g., place more weight on more certain predictions vs. uncertain ones in the unsupervised loss term. However, we leave the exploration of these avenues as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RESULTS</head><p>Our network structure is given in Table <ref type="table" target="#tab_7">5</ref>, and the test setup and all training parameters are detailed in Appendix A. We test the Π-model and temporal ensembling in two image classification tasks, CIFAR-10 and SVHN, and report the mean and standard deviation of 10 runs using different random seeds.</p><p>Although it is rarely stated explicitly, we believe that our comparison methods do not use input augmentation, i.e., are limited to dropout and other forms of permutation-invariant noise. Therefore we report the error rates without augmentation, unless explicitly stated otherwise. Given that the ability of an algorithm to extract benefit from augmentation is also an important property, we report the classification accuracy using a standard set of augmentations as well. In purely supervised training the de facto standard way of augmenting the CIFAR-10 dataset includes horizontal flips and random translations, while SVHN is limited to random translations. By using these same augmentations we can compare against the best fully supervised results as well. After all, the fully supervised results should indicate the upper bound of obtainable accuracy. 36.02 ± 0.10 Virtual Adversarial <ref type="bibr" target="#b14">(Miyato et al., 2016)</ref> 24.63 ADGM <ref type="bibr" target="#b12">(Maaløe et al., 2016)</ref> 22.86 SDGM <ref type="bibr" target="#b12">(Maaløe et al., 2016)</ref> 16.61 ± 0.24 GAN of <ref type="bibr">Salimans et al. (2016)</ref> 18 Enabling the standard set of augmentations further reduces the error rate by 4.2 percentage points to 12.36%. Temporal ensembling is slightly better still at 12.16%, while being twice as fast to train. This small improvement conceals the subtle fact that random horizontal flips need to be done independently for each epoch in temporal ensembling, while Π-model can randomize once per a pair of evaluations, which according to our measurements is ∼0.5 percentage points better than independent flips.</p><p>A principled comparison with <ref type="bibr" target="#b20">Sajjadi et al. (2016b)</ref> is difficult due to several reasons. They provide results only for a fairly extreme set of augmentations (translations, flipping, rotations, stretching, and shearing) on top of fractional max pooling <ref type="bibr" target="#b4">(Graham, 2014)</ref>, which introduces random, local stretching inside the network, and is known to improve classification results substantially. They quote an error rate of only 13.60% for supervised-only training with 4000 labels, while our corresponding baseline is 34.85%. This gap indicates a huge benefit from versatile augmentations and fractional max pooling-in fact, their baseline result is already better than any previous semisupervised results. By enabling semi-supervised learning they achieve a 17% drop in classification error rate (from 13.60% to 11.29%), while we see a much larger relative drop of 65% (from 34.85% to 12.16%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">SVHN</head><p>The street view house numbers (SVHN) dataset consists of 32 × 32 pixel RGB images of real-world house numbers, and the task is to classify the centermost digit. In SVHN we chose to use only the Table <ref type="table" target="#tab_1">2</ref> compares our method to the previous state-of-the-art. With the most commonly used 1000 labels we observe an improvement of 2.7 percentage points, from 8.11% to 5.43% without augmentation, and further to 4.42% with standard augmentations.</p><p>We also investigated the behavior with 500 labels, where we obtained an error rate less than half of <ref type="bibr">Salimans et al. (2016)</ref> without augmentations, with a significantly lower standard deviation as well. When augmentations were enabled, temporal ensembling further reduced the error rate to 5.12%. In this test the difference between Π-model and temporal ensembling was quite significant at 1.5 percentage points.</p><p>In <ref type="bibr">SVHN Sajjadi et al. (2016b)</ref> provide results without augmentation, with the caveat that they use fractional max pooling, which is a very augmentation-like technique due to the random, local stretching it introduces inside the network. It leads to a superb error rate of 2.28% in supervisedonly training, while our corresponding baseline is 3.05% (or 2.88% with translations). Given that in a separate experiment our network matched the best published result for non-augmented SVHN when extra data is used (1.69% from <ref type="bibr" target="#b11">Lee et al. (2015)</ref>), this gap is quite surprising, and leads us to conclude that fractional max pooling leads to a powerful augmentation of the dataset, well beyond what simple translations can achieve. Our temporal ensembling technique obtains better error rates for both 500 and 1000 labels (5.12% and 4.42%, respectively) compared to the 6.03% reported by Sajjadi et al. for 732 labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">CIFAR-100 AND TINY IMAGES</head><p>The CIFAR-100 dataset consists of 32 × 32 pixel RGB images from a hundred classes. We are not aware of previous semi-supervised results in this dataset, and chose 10000 labels for our experiments. Table <ref type="table" target="#tab_3">3</ref> shows error rates of 43.43% and 38.65% without and with augmentation, respectively. These correspond to 7.8 and 5.9 percentage point improvements compared to supervised learning with labeled inputs only.</p><p>We ran two additional tests using unlabeled extra data from Tiny Images dataset <ref type="bibr">(Torralba et al., 2008)</ref>: one with randomly selected 500k extra images, most not corresponding to any of the CIFAR-100 categories, and another with a restricted set of 237k images from the categories that correspond to those found in the CIFAR-100 dataset (see appendix A for details). The results are shown in  when even a small portion of the labels give disinformation, and the situation worsens quickly as the portion of randomized labels increases to 50% or more. On the other hand, temporal ensembling (right) shows almost perfect resistance to disinformation when half of the labels are random, and retains over ninety percent classification accuracy even when 80% of the labels are random.</p><p>the classification accuracy further. This indicates that in order to train a better classifier by adding extra data as unlabeled inputs, it is enough to have the extra data roughly in the same space as the actual inputs-in our case, natural images. We hypothesize that it may even be possible to use properly crafted synthetic data as unlabeled inputs to obtain improved classifiers.</p><p>In order to keep the training times tolerable, we limited the number of unlabeled inputs to 50k per epoch in these tests, i.e., on every epoch we trained using all 50k labeled inputs from CIFAR-100 and 50k additional unlabeled inputs from Tiny Images. The 50k unlabeled inputs were chosen randomly on each epoch from the 500k or 237k extra inputs. In temporal ensembling, after each epoch we updated only the rows of Z that corresponded to inputs used on that epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">SUPERVISED LEARNING</head><p>When all labels are used for traditional supervised training, our network approximately matches the state-of-the-art error rate for a single model in CIFAR-10 with augmentation <ref type="bibr" target="#b11">(Lee et al., 2015;</ref><ref type="bibr">Mishkin &amp; Matas, 2016</ref>) at 6.05%, and without augmentation <ref type="bibr" target="#b21">(Salimans &amp; Kingma, 2016)</ref> at 7.33%. The same is probably true for SVHN as well, but there the best published results rely on extra data that we chose not to use.</p><p>Given this premise, it is perhaps somewhat surprising that our methods reduce the error rate also when all labels are used (Tables <ref type="table" target="#tab_1">1 and 2</ref>). We believe that this is an indication that the consistency requirement adds a degree of resistance to ambiguous labels that are fairly common in many classification tasks, and that it encourages features to be more invariant to stochastic sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">TOLERANCE TO INCORRECT LABELS</head><p>In a further test we studied the hypothesis that our methods add tolerance to incorrect labels by assigning a random label to a certain percentage of the training set before starting to train. Figure <ref type="figure" target="#fig_1">2</ref> shows the classification error graphs for standard supervised training and temporal ensembling.</p><p>Clearly our methods provide considerable resistance to wrong labels, and we believe this is because the unsupervised loss term encourages the mapping function implemented by the network to be flat in the vicinity of all input data points, whereas the supervised loss term enforces the mapping function to have a specific value in the vicinity of the labeled input data points. This means that even the wrongly labeled inputs play a role in shaping the mapping function-the unsupervised loss term smooths the mapping function and thus also the decision boundaries, effectively fusing the inputs into coherent clusters, whereas the excess of correct labels in each class is sufficient for locking the clusters to the right output vectors through the supervised loss term. The difference to classical regularizers is that we induce smoothness only on the manifold of likely inputs instead of over the entire input domain. For further analysis about the importance of the gradient of the mapping function, see <ref type="bibr" target="#b23">Simard et al. (1998)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RELATED WORK</head><p>There is a large body of previous work on semi-supervised learning <ref type="bibr">(Zhu, 2005)</ref>. In here we will concentrate on the ones that are most directly connected to our work.</p><p>Γ-model is a subset of a ladder network <ref type="bibr" target="#b17">(Rasmus et al., 2015)</ref> that introduces lateral connections into an encoder-decoder type network architecture, targeted at semi-supervised learning. In Γ-model, all but the highest lateral connections in the ladder network are removed, and after pruning the unnecessary stages, the remaining network consists of two parallel, identical branches. One of the branches takes the original training inputs, whereas the other branch is given the same input corrupted with noise. The unsupervised loss term is computed as the squared difference between the (pre-activation) output of the clean branch and a denoised (pre-activation) output of the corrupted branch. The denoised estimate is computed from the output of the corrupted branch using a parametric nonlinearity that has 10 auxiliary trainable parameters per unit. Our Π-model differs from the Γ-model in removing the parametric nonlinearity and denoising, having two corrupted paths, and comparing the outputs of the network instead of pre-activation data of the final layer. <ref type="bibr" target="#b20">Sajjadi et al. (2016b)</ref> recently introduced a new loss function for semi-supervised learning, so called transform/stability loss, which is founded on the same principle as our work. During training, they run augmentation and network evaluation n times for each minibatch, and then compute an unsupervised loss term as the sum of all pairwise squared distances between the obtained n network outputs. As such, their technique follows the general pseudo-ensemble agreement (PEA) regularization framework of <ref type="bibr" target="#b0">Bachman et al. (2014)</ref>. In addition, they employ a mutual exclusivity loss term <ref type="bibr" target="#b19">(Sajjadi et al., 2016a</ref>) that we do not use. Our Π-model can be seen as a special case of the transform/stability loss obtained by setting n = 2. The computational cost of training with transform/stability loss increases linearly as a function of n, whereas the efficiency of our temporal ensembling technique remains constant regardless of how large effective ensemble we obtain via the averaging of previous epochs' predictions.</p><p>In bootstrap aggregating, or bagging, multiple networks are trained independently based on subsets of training data <ref type="bibr" target="#b1">(Breiman, 1996)</ref>. This results in an ensemble that is more stable and accurate than the individual networks. Our approach can be seen as pulling the predictions from an implicit ensemble that is based on a single network, and the variability is a result of evaluating it under different dropout and augmentation conditions instead of training on different subsets of data. In work parallel to ours, <ref type="bibr" target="#b8">Huang et al. (2017)</ref>   <ref type="bibr">&amp; Ghahramani, 2002)</ref> infer labels for unlabeled training data by comparing the associated inputs to labeled training inputs using a suitable distance metric. Our approach differs from this in two important ways. Firstly, we never compare training inputs against each other, but instead only rely on the unknown labels remaining constant, and secondly, we let the network produce the likely classifications for the unlabeled inputs instead of providing them through an outside process.</p><p>In addition to partially labeled data, considerable amount of effort has been put into dealing with densely but inaccurately labeled data. This can be seen as a semi-supervised learning task where part of the training process is to identify the labels that are not to be trusted. For recent work in this area, see, e.g., <ref type="bibr">Sukhbaatar et al. (2014)</ref> and <ref type="bibr" target="#b16">Patrini et al. (2016)</ref>. In this context of noisy labels, <ref type="bibr" target="#b18">Reed et al. (2014)</ref> presented a simple bootstrapping method that trains a classifier with the target composed of a convex combination of the previous epoch output and the known but potentially noisy labels. Our temporal ensembling differs from this by taking into account the evaluations over multiple epochs.</p><p>Generative Adversarial Networks (GAN) have been recently used for semi-supervised learning with promising results <ref type="bibr" target="#b12">(Maaløe et al., 2016;</ref><ref type="bibr" target="#b25">Springenberg, 2016;</ref><ref type="bibr" target="#b15">Odena, 2016;</ref><ref type="bibr">Salimans et al., 2016)</ref>. It Additive Gaussian noise σ = 0.15 conv1a 128 filters, 3 × 3, pad = 'same', LReLU (α = 0.1) conv1b 128 filters, 3 × 3, pad = 'same', LReLU (α = 0.1) conv1c 128 filters, 3 × 3, pad = 'same', LReLU (α = 0.1) pool1</p><p>Maxpool 2 × 2 pixels drop1</p><p>Dropout, p = 0.5 conv2a 256 filters, 3 × 3, pad = 'same', LReLU (α = 0.1) conv2b 256 filters, 3 × 3, pad = 'same', LReLU (α = 0.1) conv2c 256 filters, 3 × 3, pad = 'same', LReLU (α = 0.1) pool2</p><p>Maxpool 2 × 2 pixels drop2</p><p>Dropout, p = 0.5 conv3a 512 filters, 3 × 3, pad = 'valid', LReLU (α = 0.1) conv3b 256 filters, 1 × 1, LReLU (α = 0.1) conv3c 128 filters, 1 × 1, LReLU (α = 0.1) pool3</p><p>Global average pool (6 × 6 → 1×1 pixels) dense Fully connected 128 → 10 output Softmax could be an interesting avenue for future work to incorporate a generative component to our solution.</p><p>We also envision that our methods could be applied to regression-type learning tasks.  <ref type="table" target="#tab_4">4</ref>). Note that the extra input images were supplied as unlabeled data for our networks, and the labels were used only for narrowing down the full set of 79 million images.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Structure of the training pass in our methods. Top: Π-model. Bottom: temporal ensembling. Labels y i are available only for the labeled inputs, and the associated cross-entropy loss component is evaluated only for those.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Percentage of correct SVHN classifications as a function of training epoch when a part of the labels is randomized. With standard supervised training (left) the classification accuracy sufferswhen even a small portion of the labels give disinformation, and the situation worsens quickly as the portion of randomized labels increases to 50% or more. On the other hand, temporal ensembling (right) shows almost perfect resistance to disinformation when half of the labels are random, and retains over ninety percent classification accuracy even when 80% of the labels are random.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>CIFAR-10 results with 4000 labels, averages of 10 runs (4 runs for all labels).</figDesc><table><row><cell cols="2">Error rate (%) with # labels</cell></row><row><cell>4000</cell><cell>All (50000)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>SVHN results for 500 and 1000 labels, averages of 10 runs (4 runs for all labels).</figDesc><table><row><cell>Model</cell><cell cols="3">Error rate (%) with # labels 500 1000 All (73257)</cell></row><row><cell>Supervised-only</cell><cell>35.18 ± 5.61</cell><cell>20.47 ± 2.64</cell><cell>3.05 ± 0.07</cell></row><row><cell>with augmentation</cell><cell>31.59 ± 3.60</cell><cell>19.30 ± 3.89</cell><cell>2.88 ± 0.03</cell></row><row><cell>DGN (Kingma et al., 2014)</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>CIFAR-100 results with 10000 labels, averages of 10 runs (4 runs for all labels).</figDesc><table><row><cell cols="2">Error rate (%) with # labels</cell></row><row><cell>10000</cell><cell>All (50000)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>CIFAR-100 + Tiny Images results, averages of 10 runs.</figDesc><table><row><cell cols="2">Error rate (%) with # unlabeled</cell></row><row><cell cols="2">auxiliary inputs from Tiny Images</cell></row><row><cell>Random 500k</cell><cell>Restricted 237k</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc></figDesc><table /><note>. The addition of randomly selected, unlabeled extra images improved the error rate by 2.7 percentage points (from 26.30% to 23.63%), indicating a desirable ability to learn from random natural images. Temporal ensembling benefited much more from the extra data than the Π-model. Interestingly, restricting the extra data to categories that are present in CIFAR-100 did not improve</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>store multiple snapshots of the network during training, hopefully corresponding to different local minima, and use them as an explicit ensemble. The general technique of inferring new labels from partially labeled data is often referred to as bootstrapping or self-training, and it was first proposed by Yarowsky (1995) in the context of linguistic analysis. Whitney &amp; Sarkar (2012) analyze Yarowsky's algorithm and propose a novel graph-based label propagation approach. Similarly, label propagation methods (Zhu</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>The network architecture used in all of our tests.</figDesc><table><row><cell>NAME</cell><cell>DESCRIPTION</cell></row><row><cell>input</cell><cell>32 × 32 RGB image</cell></row><row><cell>noise</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>The Tiny Images(Torralba et al., 2008)  labels and image counts used in the CIFAR-100 plus restricted extra data tests (rightmost column of Table</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Squared difference gave slightly but consistently better results than cross-entropy loss in our tests.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">ACKNOWLEDGEMENTS</head><p>We thank the anonymous reviewers, Tero Karras, Pekka Jänis, Tim Salimans, Ian Goodfellow, as well as Harri Valpola and his colleagues at Curious AI for valuable suggestions that helped to improve this article.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.</p><p>Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning <ref type="bibr">Research, 15:1929</ref><ref type="bibr">-1958</ref><ref type="bibr">, 2014. Sainbayar Sukhbaatar, Joan Bruna, Manohar Paluri, Lubomir Bourdev, and</ref>   <ref type="bibr" target="#b21">Salimans &amp; Kingma (2016)</ref>. All data layers were initialized following <ref type="bibr" target="#b5">He et al. (2015)</ref>, and we applied weight normalization and mean-only batch normalization <ref type="bibr" target="#b21">(Salimans &amp; Kingma, 2016)</ref> with momentum 0.999 to all of them. We used leaky ReLU <ref type="bibr" target="#b13">(Maas et al., 2013)</ref> with α = 0.1 as the non-linearity, and chose to use max pooling instead of strided convolutions because it gave consistently better results in our experiments.</p><p>All networks were trained using Adam (Kingma &amp; Ba, 2014) with a maximum learning rate of λ max = 0.003, except for temporal ensembling in the SVHN case where a maximum learning rate of λ max = 0.001 worked better. Adam momentum parameters were set to β 1 = 0.9 and β 2 = 0.999 as suggested in the paper. The maximum value for the unsupervised loss component was set to w max • M/N , where M is the number of labeled inputs and N is the total number of training inputs.</p><p>For Π-model runs, we used w max = 100 in all runs except for CIFAR-100 with Tiny Images where we set w max = 300. For temporal ensembling we used w max = 30 in most runs. For the corrupted label test in Section 3.5 we used w max = 300 for 0% and 20% corruption, and w max = 3000 for corruption of 50% and higher. For basic CIFAR-100 runs we used w max = 100, and for CIFAR-100 with Tiny Images we used w max = 1000. The accumulation decay constant of temporal ensembling was set to α = 0.6 in all runs.</p><p>In all runs we ramped up both the learning rate λ and unsupervised loss component weight w during the first 80 epochs using a Gaussian ramp-up curve exp[−5(1 − T ) 2 ], where T advances linearly from zero to one during the ramp-up period. In addition to ramp-up, we annealed the learning rate λ to zero and Adam β 1 to 0.5 during the last 50 epochs, but otherwise we did not decay them during training. The ramp-down curve was similar to the ramp-up curve but time-reversed and with a scaling constant of 12.5 instead of 5. All networks were trained for 300 epochs with minibatch size of 100.</p><p>CIFAR-10 Following previous work in fully supervised learning, we pre-processed the images using ZCA and augmented the dataset using horizontal flips and random translations. The translations were drawn from [−2, 2] pixels, and were independently applied to both branches in the Π-model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SVHN</head><p>We pre-processed the input images by biasing and scaling each input image to zero mean and unit variance. We used only the 73257 items in the official training set, i.e., did not use the provided 531131 extra items. The training setups were otherwise similar to CIFAR-10 except that horizontal flips were not used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation</head><p>Our implementation is written in Python using Theano (Theano Development Team, 2016) and Lasagne <ref type="bibr" target="#b2">(Dieleman et al., 2015)</ref>, and is available at https://github.com/smlaine2/tempens.</p><p>Model convergence As discussed in Section 2.1, a slow ramp-up of the unsupervised cost is very important for getting the models to converge. Furthermore, in our very preliminary tests with 250 labels in SVHN we noticed that optimization tended to explode during the ramp-up period, and we eventually found that using a lower value for Adam β 2 parameter (e.g., 0.99 instead of 0.999) seems to help in this regard.</p><p>We do not attempt to guarantee that the occurrence of labeled inputs during training would be somehow stratified; with bad luck there might be several consecutive minibatches without any labeled inputs when the label density is very low. Some previous work has identified this as a weakness, and have solved the issue by shuffling the input sequences in such a way that stratification is guaranteed, e.g. <ref type="bibr" target="#b17">Rasmus et al. (2015)</ref> (confirmed from the authors). This kind of stratification might further improve the convergence of our methods as well.</p><p>Tiny Images, extra data from restricted categories The restricted extra data in Section 3.3 was extracted from Tiny Images by picking all images with labels corresponding to the 100 categories used in CIFAR-100. As the Tiny Images dataset does not contain CIFAR-100 categories aquarium fish and maple tree, we used images with labels fish and maple instead. The result was a total of 237 203 images that were used as unlabeled extra data. Table <ref type="table">6</ref> shows the composition of this extra data set.</p><p>It is worth noting that the CIFAR-100 dataset itself is a subset of Tiny Images, and we did not explicitly prevent overlap between this extra set and CIFAR-100. This led to approximately a third of the CIFAR-100 training and test images being present as unlabeled inputs in the extra set. The other test with 500k extra entries picked randomly out of all 79 million images had a negligible overlap with CIFAR-100.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning with pseudo-ensembles</title>
		<author>
			<persName><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ouais</forename><surname>Alsharif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27 (NIPS)</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bagging predictors</title>
		<author>
			<persName><forename type="first">Leo</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
				<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Lasagne: First release</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Sander Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eben</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Søren</forename><surname>Olson</surname></persName>
		</author>
		<author>
			<persName><surname>Kaae Sønderby</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Dropout as a bayesian approximation: Representing model uncertainty in deep learning</title>
		<author>
			<persName><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<idno>CoRR, abs/1506.02142</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Fractional max-pooling</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<idno>CoRR, abs/1412.6071</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno>CoRR, abs/1502.01852</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno>CoRR, abs/1503.02531</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno>CoRR, abs/1603.09382</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Snapshot Ensembles: Train 1, get M for free</title>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">E</forename><surname>Hopcroft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Learning Representations (ICLR)</title>
				<meeting>International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno>CoRR, abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with deep generative models</title>
		<author>
			<persName><forename type="first">Shakir</forename><surname>Diederik P Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27 (NIPS)</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Generalizing pooling functions in convolutional neural networks: Mixed, gated, and tree</title>
		<author>
			<persName><forename type="first">Chen-Yu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">W</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<idno>CoRR, abs/1509.08985</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Auxiliary deep generative models</title>
		<author>
			<persName><forename type="first">Lars</forename><surname>Maaløe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Casper</forename><forename type="middle">Kaae</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Søren</forename><surname>Kaae Sønderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ole</forename><surname>Winther</surname></persName>
		</author>
		<idno>CoRR, abs/1602.05473</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName><forename type="first">Awni</forename><forename type="middle">Y</forename><surname>Andrew L Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Learning Representations (ICLR)</title>
				<meeting>International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2013">2013. 2016</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
	<note>Proc. International Conference on Machine Learning (ICML)</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distributional smoothing with virtual adversarial training</title>
		<author>
			<persName><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shin-Ichi</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken</forename><surname>Nakae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shin</forename><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Learning Representations (ICLR)</title>
				<meeting>International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with generative adversarial networks</title>
		<author>
			<persName><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Efficient Machine Learning workshop at ICML 2016</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Making neural networks robust to label noise: a loss correction approach</title>
		<author>
			<persName><forename type="first">Giorgio</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizhen</forename><surname>Qu</surname></persName>
		</author>
		<idno>CoRR, abs/1609.03683</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semisupervised learning with ladder networks</title>
		<author>
			<persName><forename type="first">Antti</forename><surname>Rasmus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Berglund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikko</forename><surname>Honkala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tapani</forename><surname>Raiko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28 (NIPS)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>CoRR, abs/1412.6596</idno>
		<title level="m">Dumitru Erhan, and Andrew Rabinovich. Training deep neural networks on noisy labels with bootstrapping</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Mutual exclusivity loss for semi-supervised deep learning</title>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehran</forename><surname>Javanmardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tolga</forename><surname>Tasdizen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Image Processing</title>
				<imprint>
			<date type="published" when="2016">2016. 2016a</date>
			<biblScope unit="page" from="1908" to="1912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Regularization with stochastic transformations and perturbations for deep semi-supervised learning</title>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehran</forename><surname>Javanmardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tolga</forename><surname>Tasdizen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29 (NIPS)</title>
				<imprint>
			<date type="published" when="2016">2016b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Weight normalization: A simple reparameterization to accelerate training of deep neural networks</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<idno>CoRR, abs/1602.07868</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Improved techniques for training GANs</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<idno>CoRR, abs/1606.03498</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Transformation Invariance in Pattern Recognition -Tangent Distance and Tangent Propagation</title>
		<author>
			<persName><forename type="first">Patrice</forename><forename type="middle">Y</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><forename type="middle">A</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Victorri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="239" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Swapout: Learning an ensemble of deep architectures</title>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Forsyth</surname></persName>
		</author>
		<idno>CoRR, abs/1605.06465</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unsupervised and semi-supervised learning with categorical generative adversarial networks</title>
		<author>
			<persName><forename type="first">Jost</forename><surname>Tobias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Springenberg</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Learning Representations (ICLR)</title>
				<meeting>International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">camel 2121 can 2461 castle 3094 caterpillar 2382 cattle 2089 chair 2552 chimpanzee 1706 clock 2375 cloud 2390 cockroach 2318 couch 2171 crab 2735 crocodile 2712 cup 2287 dinosaur 2045 dolphin 2504 elephant 2794 fish * 3082 flatfish 1504 forest 2244 fox 2684 girl 2204 hamster 2294 house 2320 kangaroo 2563 keyboard 1948 lamp 2242 lawn mower 1929 leopard 2139 lion 3045 lizard 2130 lobster 2136 man 2248 maple * 2149 motorcycle 2168 mountain 2249 mouse 2128 mushroom 2390 oak tree 1995 orange 2650 orchid 1902 otter 2073 palm tree 2107 pear 2120 pickup truck 2478 pine tree 2341 plain 2198 plate 3109 poppy 2730 porcupine 1900 possum 2008 rabbit 2408 raccoon 2587 ray 2564 road 2862 rocket 2180 rose 2237 sea 2122 seal 2159 shark 2157 shrew 1826 skunk 2450 skyscraper 2298 snail 2369 snake 2989 spider 3024 squirrel 2374 streetcar 1905 sunflower 2761</title>
		<author>
			<persName><forename type="first">Jost</forename><surname>Tobias Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">A</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno>CoRR, abs/1412.6806</idno>
	</analytic>
	<monogr>
		<title level="m">Label # Label # Label # Label # apple 2242 baby 2771 bear 2242 beaver 2116 bed 2767 bee 2193 beetle 2173 bicycle 2599 bottle 2212 bowl 2707 boy 2234 bridge 2274 bus 3068 butterfly</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">3036</biblScope>
		</imprint>
	</monogr>
	<note>Striving for simplicity: The all convolutional net. sweet pepper 1983 table 3137 tank 1897 telephone 1889 television 2973 tiger 2603 tractor 1848 train 3020 trout 2726 tulip 2160 turtle 2438 wardrobe 2029 whale 2597 willow tree 2040 wolf 2423 woman 2446 worm 2945</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
