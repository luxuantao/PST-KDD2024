<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Chalmers University of Technology</orgName>
								<orgName type="institution" key="instit2">Göteborg University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computing Science</orgName>
								<orgName type="institution" key="instit1">Chalmers University of Technology</orgName>
								<orgName type="institution" key="instit2">Göteborg University</orgName>
								<address>
									<postCode>SE-412 96, 2003</postCode>
									<settlement>Göteborg, Göteborg, Sweden</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<idno type="ISSN">1650-3023</idno>
					</monogr>
					<idno type="MD5">6EFB8BBAF813046BB719D02D2E405611</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present an efficient and practical lock-free implementation of a concurrent priority queue that is suitable for both fully concurrent (large multi-processor) systems as well as pre-emptive (multi-process) systems. Many algorithms for concurrent priority queues are based on mutual exclusion. However, mutual exclusion causes blocking which has several drawbacks and degrades the system's overall performance. Non-blocking algorithms avoid blocking, and are either lock-free or wait-free. Previously known non-blocking algorithms of priority queues did not perform well in practice because of their complexity, and they are often based on non-available atomic synchronization primitives. Our algorithm is based on the randomized sequential list structure called Skiplist, and a real-time extension of our algorithm is also described. In our performance evaluation we compare our algorithm with some of the most efficient implementations of priority queues known. The experimental results clearly show that our lock-free implementation outperforms the other lock-based implementations in all cases for 3 threads and more, both on fully concurrent as well as on pre-emptive systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Priority queues are fundamental data structures. From the operating system level to the user application level, they are frequently used as basic components. For example, the ready-queue that is used in the scheduling of tasks in many real-time systems, can usually be implemented using a concurrent priority queue. Consequently, the design of efficient implementations of priority queues is a research area that has been extensively researched. A priority queue supports two operations, the ÁÒ× ÖØ and the Ð Ø Å Ò operation. The abstract definition of a priority queue is a set of keyvalue pairs, where the key represents a priority. The ÁÒ× ÖØ operation inserts a new key-value pair into the set, and the Ð Ø Å Ò operation removes and returns the value of the key-value pair with the lowest key (i.e. highest priority) that was in the set.</p><p>To ensure consistency of a shared data object in a concurrent environment, the most common method is to use mutual exclusion, i.e. some form of locking. Mutual exclusion degrades the system's overall performance <ref type="bibr" target="#b13">[14]</ref> as it causes blocking, i.e. other concurrent operations can not make any progress while the access to the shared resource is blocked by the lock. Using mutual exclusion can also cause deadlocks, priority inversion (which can be solved efficiently on uni-processors <ref type="bibr" target="#b12">[13]</ref> with the cost of more difficult analysis, although not as efficient on multiprocessor systems <ref type="bibr" target="#b11">[12]</ref>) and even starvation.</p><p>To address these problems, researchers have proposed non-blocking algorithms for shared data objects. Nonblocking methods do not involve mutual exclusion, and therefore do not suffer from the problems that blocking can cause. Non-blocking algorithms are either lock-free or wait-free. Lock-free implementations guarantee that regardless of the contention caused by concurrent operations and the interleaving of their sub-operations, always at least one operation will progress. However, there is a risk for starvation as the progress of other operations could cause one specific operation to never finish. This is although different from the type of starvation that could be caused by blocking, where a single operation could block every other operation forever, and cause starvation of the whole system. Wait-free <ref type="bibr" target="#b5">[6]</ref> algorithms are lock-free and moreover they avoid starvation as well, in a wait-free algorithm every operation is guaranteed to finish in a limited number of steps, regardless of the actions of the concurrent operations. Nonblocking algorithms have been shown to be of big practical importance in practical applications <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>, and recently NOBLE, which is a non-blocking inter-process communication library, has been introduced <ref type="bibr" target="#b15">[16]</ref>.</p><p>There exist several algorithms and implementations of concurrent priority queues. The majority of the algorithms are lock-based, either with a single lock on top of a sequential algorithm, or specially constructed algorithms using multiple locks, where each lock protects a small part of the shared data structure. Several different representations of the shared data structure are used, for example: Hunt et al. <ref type="bibr" target="#b6">[7]</ref> presents an implementation which is based on heap structures, Grammatikakis et al. <ref type="bibr" target="#b2">[3]</ref> compares different structures including cyclic arrays and heaps, and most recently Lotan and Shavit <ref type="bibr" target="#b8">[9]</ref> presented an implementation based on the Skiplist structure <ref type="bibr" target="#b10">[11]</ref>. The algorithm by Hunt et al. locks each node separately and uses a technique to scatter the accesses to the heap, thus reducing the contention. Its implementation is publicly available and its performance has been documented on multi-processor systems. Lotan and Shavit extend the functionality of the concurrent priority queue and assume the availability of a global high-accuracy clock. They apply a lock on each pointer, and as the multi-pointer based Skiplist structure is used, the number of locks is significantly more than the number of nodes. Its performance has previously only been documented by simulation, with very promising results.</p><p>Israeli and Rappoport have presented a wait-free algorithm for a concurrent priority queue <ref type="bibr" target="#b7">[8]</ref>. This algorithm makes use of strong atomic synchronization primitives that have not been implemented in any currently existing platform. However, there exists an attempt for a wait-free algorithm by <ref type="bibr">Barnes [2]</ref> that uses existing atomic primitives, though this algorithm does not comply with the generally accepted definition of the wait-free property. The algo-rithm is not yet implemented and the theoretical analysis predicts worse behavior than the corresponding sequential algorithm, which makes it not of practical interest.</p><p>One common problem with many algorithms for concurrent priority queues is the lack of precise defined semantics of the operations. It is also seldom that the correctness with respect to concurrency is proved, using a strong property like linearizability <ref type="bibr" target="#b4">[5]</ref>.</p><p>In this paper we present a lock-free algorithm of a concurrent priority queue that is designed for efficient use in both pre-emptive as well as in fully concurrent environments. Inspired by Lotan and Shavit <ref type="bibr" target="#b8">[9]</ref>, the algorithm is based on the randomized Skiplist <ref type="bibr" target="#b10">[11]</ref> data structure, but in contrast to <ref type="bibr" target="#b8">[9]</ref> it is lock-free. It is also implemented using common synchronization primitives that are available in modern systems. The algorithm is described in detail later in this paper, and the aspects concerning the underlying lock-free memory management are also presented. The precise semantics of the operations are defined and a proof is given that our implementation is lock-free and linearizable. We have performed experiments that compare the performance of our algorithm with some of the most efficient implementations of concurrent priority queues known, i.e. the implementation by Lotan and Shavit <ref type="bibr" target="#b8">[9]</ref> and the implementation by Hunt et al. <ref type="bibr" target="#b6">[7]</ref>. Experiments were performed on three different platforms, consisting of a multiprocessor system using different operating systems and equipped with either 2, 4 or 64 processors. Our results show that our algorithm outperforms the other lock-based implementations for 3 threads and more, in both highly pre-emptive as well as in fully concurrent environments. We also present an extended version of our algorithm that also addresses certain real-time aspects of the priority queue as introduced by Lotan and Shavit <ref type="bibr" target="#b8">[9]</ref>.</p><p>The rest of the paper is organized as follows. In Section 2 we define the properties of the systems that our implementation is aimed for. The actual algorithm is described in Section 3. In Section 4 we define the precise semantics for the operations on our implementations, as well showing correctness by proving the lock-free and linearizability property. The experimental evaluation that shows the performance of our implementation is presented in Section 5. In Section 6 we extend our algorithm with functionality that can be needed for specific real-time applications. We conclude the paper with Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Description</head><p>A typical abstraction of a shared memory multiprocessor system configuration is depicted in Figure <ref type="figure">1</ref>. Each node of the system contains a processor together with its local memory. All nodes are connected to the shared memory via an interconnection network. A set of co- operating tasks is running on the system performing their respective operations. Each task is sequentially executed on one of the processors, while each processor can serve (run) many tasks at a time. The co-operating tasks, possibly running on different processors, use shared data objects built in the shared memory to co-ordinate and communicate. Tasks synchronize their operations on the shared data objects through sub-operations on top of a cache-coherent shared memory. The shared memory may not though be uniformly accessible for all nodes in the system; processors can have different access times on different parts of the memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Algorithm</head><p>The algorithm is based on the sequential Skiplist data structure invented by Pugh <ref type="bibr" target="#b10">[11]</ref>. This structure uses randomization and has a probabilistic time complexity of Ç´ÐÓ AE µ where N is the maximum number of elements in the list. The data structure is basically an ordered list with randomly distributed short-cuts in order to improve search times, see Figure <ref type="figure">2</ref>  mum number of next pointers) of the data structure is ÐÓ AE . The height of each inserted node is randomized geometrically in the way that 50% of the nodes should have height 1, 25% of the nodes should have height 2 and so on. To use the data structure as a priority queue, the nodes are ordered in respect of priority (which has to be unique for each node), the nodes with highest priority are located first in the list. The fields of each node item are described in Figure <ref type="figure">3</ref> as it is used in this implementation. For all code examples in this paper, code that is between the " " and " " symbols are only used for the special implementation that involves timestamps (see Section 6), and are thus not included in the standard version of the implementation.</p><p>In order to make the Skiplist construction concurrent and non-blocking, we are using three of the standard atomic synchronization primitives, Test-And-Set (TAS), Fetch-And-Add (FAA) and Compare-And-Swap (CAS). Figure <ref type="figure" target="#fig_0">5</ref> describes the specification of these primitives which are available in most modern platforms.</p><p>As we are concurrently (with possible preemptions) traversing nodes that will be continuously allocated and reclaimed, we have to consider several aspects of memory management. No node should be reclaimed and then later re-allocated while some other process is traversing this node. This can be solved for example by careful reference counting. We have selected to use the lock-free memory management scheme invented by Valois <ref type="bibr" target="#b18">[19]</ref> and corrected by Michael and Scott <ref type="bibr" target="#b9">[10]</ref>, which makes use of the FAA and CAS atomic synchronization primitives.</p><p>To insert or delete a node from the list we have to change the respective set of next pointers. These have to be changed consistently, but not necessary all at once. Our solution is to have additional information on each node about its deletion (or insertion) status. This additional information will guide the concurrent processes that might traverse into one partial deleted or inserted node. When we have changed all necessary next pointers, the node is fully deleted or inserted.</p><p>One problem, that is general for non-blocking implementations that are based on the linked-list structure, arises when inserting a new node into the list. Because of the linked-list structure one has to make sure that the previous node is not about to be deleted. If we are changing the next pointer of this previous node atomically with CAS, to point to the new node, and then immediately afterwards the previous node is deleted -then the new node will be deleted as well, as illustrated in Figure <ref type="figure">4</ref>. There are several solutions to this problem. One solution is to use the CAS2 operation as it can change two pointers atomically, but this operation is not available in any existing multiprocessor system. A second solution is to insert auxiliary nodes <ref type="bibr" target="#b18">[19]</ref> between each two normal nodes, and the latest method introduced by Harris <ref type="bibr" target="#b3">[4]</ref> is to use one bit of the pointer values as a deletion mark. On most modern 32-bit systems, 32-bit values can only be located at addresses that are evenly dividable by 4, therefore bits 0 and 1 of the address are always set to zero. The method is then to use the previously unused bit 0 of the next pointer to mark that this node is about to be deleted, using CAS. Any concurrent ÁÒ× ÖØ operation will then be notified about the deletion, when its CAS operation will fail.</p><p>One memory management issue is how to de-reference pointers safely. If we simply de-reference the pointer, it might be that the corresponding node has been reclaimed before we could access it. It can also be that bit 0 of the pointer was set, thus marking that the node is deleted, and therefore the pointer is not valid. The following functions are defined for safe handling of the memory management: function READ_NODE(address:pointer to pointer to Node):pointer to Node /* De-reference the pointer and increase the reference counter for the corresponding node. In case the pointer is marked, NULL is returned */  procedure RELEASE_NODE(node:pointer to Node) /* Decrement the reference counter on the corresponding given node. If the reference count reaches zero, then call RELEASE_NODE on the nodes that this node has owned pointers to, then reclaim the node */ While traversing the nodes, processes will eventually reach nodes that are marked to be deleted. As the process that invoked the corresponding Ð Ø operation might be pre-empted, this Ð Ø operation has to be helped to finish before the traversing process can continue. However, it is only necessary to help the part of the Ð Ø operation on the current level in order to be able to traverse to the next node. The function Ê AE ÜØ, see Figure <ref type="figure" target="#fig_1">6</ref>, traverses to the next node of node1 on the given level while helping (and then sets node1 to the previous node of the helped one) any marked nodes in between to finish the deletion. The function Ë ÒÃ Ý, see Figure <ref type="figure" target="#fig_1">6</ref>, traverses in several steps through the next pointers (starting from node1) at the current level until it finds a node that has the same or higher key (priority) value than the given key. It also sets node1 to be the previous node of the returned node.</p><p>The implementation of the ÁÒ× ÖØ operation, see Fig-  , starts in lines I5-I11 with a search phase to find the node after which the new node (Ò ÛAE Ó ) should be inserted. This search phase starts from the head node at the highest level and traverses down to the lowest level until the correct node is found (ÒÓ ½). When going down one level, the last node traversed on that level is remembered (× Ú AE Ó ×) for later use (this is where we should insert the new node at that level). Now it is possible that there already exists a node with the same priority as of the new node, this is checked in lines I12-I24, the value of the old node (ÒÓ ¾) is changed atomically with a CAS. Otherwise, in lines I25-I42 it starts trying to insert the new node starting with the lowest level and increasing up to the level of the new node. The next pointers of the nodes (to become previous) are changed atomically with a CAS. After the new node has been inserted at the lowest level, it is possible that it is deleted by a concurrent Ð Ø Å Ò operation before it has been inserted at all levels, and this is checked in lines I38 and I44.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The</head><p>Ð Ø Å Ò operation, see Figure <ref type="figure">8</ref>, starts from the head node and finds the first node (ÒÓ ½) in the list that does not have its deletion mark on the value set, see lines D3-D12. It tries to set this deletion mark in line D12 using the CAS primitive, and if it succeeds it also writes a valid pointer to the prev field of the node. This prev field is necessary in order to increase the performance of concurrent À ÐÔ Ð Ø operations, these operations otherwise would have to search for the previous node in order to complete the deletion. The next step is to mark the deletion bits of the next pointers in the node, starting with the lowest level and going upwards, using the CAS primitive in each step, see lines D20-D23. Afterwards in lines D24-D35 it starts the actual deletion by changing the next pointers of the previous node (ÔÖ Ú), starting at the highest level and continuing downwards. The reason for doing the deletion in decreasing order of levels, is that concurrent search operations also start at the highest level and proceed downwards, in this way the concurrent search operations will sooner avoid traversing this node. The procedure performed by the</p><formula xml:id="formula_0">Ð Ø Å Ò</formula><p>operation in order to change each next pointer of the previous node, is to first search for the previous node and then perform the CAS primitive until it succeeds.</p><p>The algorithm has been designed for pre-emptive as well as fully concurrent systems. In order to achieve the lockfree property (that at least one thread is doing progress) on pre-emptive systems, whenever a search operation finds a node that is about to be deleted, it calls the À ÐÔ Ð Ø operation and then proceeds searching from the previous node of the deleted. The À ÐÔ Ð Ø operation, see Figure <ref type="figure" target="#fig_2">9</ref>, tries to fulfill the deletion on the current level and returns when it is completed. It starts in lines H1-H4 with setting the deletion mark on all next pointers in case they have not been set. In lines H5-H6 it checks if the node given in the prev field is valid for deletion on the current level, otherwise it searches for the correct node (ÔÖ Ú) in lines H7-H10. The actual deletion of this node on the current level takes place in lines H12-H21. This operation might execute concurrently with the corresponding Ð Ø Å Ò operation, and therefore both operations synchronize with each other in lines D27, D30, D32, D34, H13, H16, H18 and H20 in order to avoid executing sub-operations that have already been performed.</p><p>In fully concurrent systems though, the helping strategy can downgrade the performance significantly. Therefore the algorithm, after a number of consecutive failed attempts to help concurrent Ð Ø Å Ò operations that hinders the progress of the current operation, puts the operation into back-off mode. When in back-off mode, the thread does nothing for a while, and in this way avoids disturbing the concurrent operations that might otherwise progress slower. The duration of the back-off is proportional to the number of threads, and for each consecutive entering of back-off mode during one operation invocation, the duration is increased exponentially.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Correctness</head><p>In this section we present the proof of our algorithm. We first prove that our algorithm is a linearizable one <ref type="bibr" target="#b4">[5]</ref> and then we prove that it is lock-free. A set of definitions that will help us to structure and shorten the proof is first explained in this section. We start by defining the sequential semantics of our operations and then introduce two definitions concerning concurrency aspects in general.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 1</head><p>We denote with Ä Ø the abstract internal state of a priority queue at the time Ø. Ä Ø is viewed as a set of pairs Ô Ú consisting of a unique priority Ô and a corresponding value Ú. The operations that can be performed on the priority queue are ÁÒ× ÖØ (Á) and Ð Ø Å Ò ( Å ). The time Ø ½ is defined as the time just before the atomic execution of the operation that we are looking at, and the time Ø ¾ is defined as the time just after the atomic execution of the same operation. The return value of ØÖÙ ¾ is returned by an ÁÒ× ÖØ operation that has succeeded to update an existing node, the return value of ØÖÙ is returned by an ÁÒ× ÖØ operation that succeeds to insert a new node. In the following expressions that defines the sequential semantics of our operations, the syntax is Ë ½ Ç ½ Ë ¾ , where Ë ½ is the conditional state before the operation Ç ½ , and Ë ¾ is the resulting state after performing the corresponding operation: </p><formula xml:id="formula_1">Ô ½ _ ¾ Ä Ø½ Á ½ ´ Ô ½ Ú ½ µ ØÖÙ Ä Ø¾ Ä Ø½ Ô ½ Ú ½<label>(1)</label></formula><formula xml:id="formula_2">Ô ½ Ú ½½ ¾ Ä Ø½ Á ½ ´ Ô ½ Ú ½¾ µ ØÖÙ ¾ Ä Ø¾ Ä Ø½ Ò Ô ½ Ú ½½ Ô ½ Ú ½¾<label>(2)</label></formula><formula xml:id="formula_3">Ô ½ Ú ½ Ñ Ò Ô Ú Ô Ú ¾ Ä Ø½ Å ½ ´µ Ô ½ Ú ½ Ä Ø¾ Ä Ø½ Ò Ô ½ Ú ½<label>(3)</label></formula><formula xml:id="formula_4">Ä Ø½ Å ½ ´µ<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 3</head><p>In order for an implementation of a shared concurrent data object to be linearizable <ref type="bibr" target="#b4">[5]</ref>, for every concurrent execution there should exist an equal (in the sense of the effect) and valid (i.e. it should respect the semantics of the shared data object) sequential execution that respects the partial order of the operations in the concurrent execution.</p><p>Next we are going to study the possible concurrent executions of our implementation. First we need to define the interpretation of the abstract internal state of our implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 4</head><p>The pair Ô Ú is present ( Ô Ú ¾ Ä) in the abstract internal state Ä of our implementation, when there is a next pointer from a present node on the lowest level of the Skiplist that points to a node that contains the pair Ô Ú , and this node is not marked as deleted with the mark on the value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 1 The definition of the abstract internal state for our implementation is consistent with all concurrent operations examining the state of the priority queue.</head><p>Proof: As the next and value pointers are changed using the CAS operation, we are sure that all threads see the same state of the Skiplist, and therefore all changes of the abstract internal state seems to be atomic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>¾ Definition 5</head><p>The decision point of an operation is defined as the atomic statement where the result of the operation is finitely decided, i.e. independent of the result of any suboperations proceeding the decision point, the operation will have the same result. We define the state-read point of an operation to be the atomic statement where a sub-state of the priority queue is read, and this sub-state is the state on which the decision point depends. We also define the statechange point as the atomic statement where the operation changes the abstract internal state of the priority queue after it has passed the corresponding decision point.</p><p>We will now show that all of these points conform to the very same statement, i.e. the linearizability point.</p><p>Lemma 2 An ÁÒ× ÖØ operation which succeeds (Á´ Ô Ú µ ØÖÙ ), takes effect atomically at one statement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof:</head><p>The decision point for an ÁÒ× ÖØ operation which succeeds (Á´ Ô Ú µ ØÖÙ ), is when the CAS suboperation in line I27 (see Figure <ref type="figure">7</ref>) succeeds, all following CAS sub-operations will eventually succeed, and the ÁÒ× ÖØ operation will finally return ØÖÙ . The state of the list (Ä Ø½ ) directly before the passing of the decision point must have been Ô _ ¾ Ä Ø½ , otherwise the CAS would have failed. The state of the list directly after passing the decision point will be Ô Ú ¾ Ä Ø¾ . ¾ Lemma 3 An ÁÒ× ÖØ operation which updates (Á´ Ô Ú µ ØÖÙ ¾ ), takes effect atomically at one statement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof:</head><p>The decision point for an ÁÒ× ÖØ operation which updates (Á´ Ô Ú µ ØÖÙ ¾ ), is when the CAS will succeed in line I14. The state of the list (Ä Ø½ ) directly before passing the decision point must have been Ô _ ¾ Ä Ø½ , otherwise the CAS would have failed. The state of the list directly after passing the decision point will be Ô Ú ¾ Ä Ø¿ . ¾ Lemma 4 A Ð Ø Å Ò operation which succeeds ( ´µ Ô Ú ), takes effect atomically at one statement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof:</head><p>The decision point for an Ð Ø Å Ò operation which succeeds ( ´µ Ô Ú ) is when the CAS suboperation in line D12 (see Figure <ref type="figure">8</ref>) succeeds. The state of the list (Ä Ø ) directly before passing of the decision point must have been Ô Ú ¾ Ä Ø , otherwise the CAS would have failed. The state of the list directly after passing the decision point will be Ô _ ¾ Ä Ø . ¾ Lemma 5 A Ð Ø Å Ò operations which fails ( ´µ ), takes effect atomically at one statement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof:</head><p>The decision point and also the state-read point for an Ð Ø Å Ò operations which fails ( ´µ ), is when the hidden read sub-operation of the Ê AE ÜØ suboperation in line D5 successfully reads the next pointer on lowest level that equals the tail node. The state of the list (Ä Ø ) directly before the passing of the state-read point must have been Ä Ø .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>¾ Definition 6</head><p>We define the relation µ as the total order and the relation µ as the direct total order between all operations in the concurrent execution. In the following formulas, ½ µ ¾ means that if ½ holds then ¾ holds as well, and ¨stands for exclusive or (i.e. ¨ means</p><formula xml:id="formula_5">( µ ´ µ): ÇÔ ½ ÇÔ ¾ ÇÔ ¿ ÇÔ ½ µ ÇÔ ¿ ÇÔ ÇÔ µ ÇÔ ¾ µ ÇÔ ½ µ ÇÔ ¾ (5) ÇÔ ½ ÇÔ ¾ µ ÇÔ ½ µ ÇÔ ¾ ¨ÇÔ ¾ µ ÇÔ ½ (6) ÇÔ ½ µ ÇÔ ¾ µ ÇÔ ½ µ ÇÔ ¾ (7) ÇÔ ½ µ ÇÔ ¾ ÇÔ ¾ µ ÇÔ ¿ µ ÇÔ ½ µ ÇÔ ¿ (8)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 6</head><p>The operations that are directly totally ordered using formula 5, form an equivalent valid sequential execution.</p><p>Proof: If the operations are assigned their direct total order (ÇÔ ½ µ ÇÔ ¾ ) by formula 5 then also the decision, stateread and the state-change points of ÇÔ ½ is executed before the respective points of ÇÔ ¾ . In this case the operations semantics behave the same as in the sequential case, and therefore all possible executions will then be equivalent to one of the possible sequential executions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>¾ Lemma 7</head><p>The operations that are directly totally ordered using formula 6 can be ordered unique and consistent, and form an equivalent valid sequential execution.</p><p>Proof: Assume we order the overlapping operations according to their decision points. As the state before as well as after the decision points is identical to the corresponding state defined in the semantics of the respective sequential operations in formulas 1 to 4, we can view the operations as occurring at the decision point. As the decision points consist of atomic operations and are therefore ordered in time, no decision point can occur at the very same time as any other decision point, therefore giving a unique and consistent ordering of the overlapping operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>¾ Lemma 8 With respect to the retries caused by synchronization, one operation will always do progress regardless of the actions by the other concurrent operations.</head><p>Proof: We now examine the possible execution paths of our implementation. There are several potentially unbounded loops that can delay the termination of the operations. We call these loops retry-loops. If we omit the conditions that are because of the operations semantics (i.e. searching for the correct position etc.), the retry-loops take place when sub-operations detect that a shared variable has changed value. This is detected either by a subsequent read suboperation or a failed CAS. These shared variables are only changed concurrently by other CAS sub-operations. According to the definition of CAS, for any number of concurrent CAS sub-operations, exactly one will succeed. This means that for any subsequent retry, there must be one CAS that succeeded. As this succeeding CAS will cause its retry loop to exit, and our implementation does not contain any cyclic dependencies between retry-loops that exit with CAS, this means that the corresponding ÁÒ× ÖØ or Ð Ø Å Ò operation will progress. Consequently, independent of any number of concurrent operations, one operation will always progress.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>¾ Theorem 1 The algorithm implements a lock-free and linearizable priority queue.</head><p>Proof: Following from Lemmas 6 and 7 and using the direct total order we can create an identical (with the same semantics) sequential execution that preserves the partial order of the operations in a concurrent execution. Following from Definition 3, the implementation is therefore linearizable. As the semantics of the operations are basically the same as in the Skiplist <ref type="bibr" target="#b10">[11]</ref>, we could use the corresponding proof of termination. This together with Lemma 8 and that the state is only changed at one atomic statement (Lemmas 1,2,3,4,5), gives that our implementation is lock-free.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>¾ 5 Experiments</head><p>In our experiments each concurrent thread performs 10000 sequential operations, whereof the first 100 or 1000 operations are ÁÒ× ÖØ operations, and the remaining operations are randomly chosen with a distribution of 50% ÁÒ× ÖØ operations versus 50%</p><p>Ð Ø Å Ò operations. The key values of the inserted nodes are randomly chosen between ¼ and ½¼¼¼¼¼¼ £ Ò, where n is the number of threads.</p><p>Each experiment is repeated 50 times, and an average execution time for each experiment is estimated. Exactly the same sequential operations are performed for all different implementations compared. Besides our implementation, we also performed the same experiment with two lockbased implementations. These are; 1) the implementation using multiple locks and Skiplists by Lotan et al. <ref type="bibr" target="#b8">[9]</ref> which is the most recently claimed to be one of the most efficient concurrent priority queues existing, and 2) the heap-based implementation using multiple locks by Hunt et al. <ref type="bibr" target="#b6">[7]</ref>. All lock-based implementations are based on simple spin-locks using the TAS atomic primitive. A clean-cache operation is performed just before each sub-experiment. All implementations are written in C and compiled with the highest optimization level, except from the atomic primitives, which are written in assembler.</p><p>The experiments were performed using different number of threads, varying from 1 to 30. To get a highly preemptive environment, we performed our experiments on a Compaq dual-processor Pentium II 450 MHz PC running Linux. A set of experiments was also performed on a Sun Solaris system with 4 processors. In order to evaluate our algorithm with full concurrency we also used a SGI Origin 2000 195 MHz system running Irix with 64 processors. The results from these experiments are shown in Figure <ref type="figure" target="#fig_3">10</ref> together with a close-up view of the Sun experiment. The average execution time is drawn as a function of the number of threads.</p><p>From the results we can conclude that all of the implementations scale similarly with respect to the average size of the queue. The implementation by Lotan and Shavit <ref type="bibr" target="#b8">[9]</ref> scales linearly with respect to increasing number of threads when having full concurrency, although when exposed to pre-emption its performance decreases very rapidly; already with 4 threads the performance decreased with over 20 times. We must point out here that the implementation by Lotan and Shavit is designed for a large (i.e. 256) number of processors. The implementation by Hunt et al. <ref type="bibr" target="#b6">[7]</ref> shows better but similar behavior. Because of this behavior we decided to run the experiments for these two implementations only up to a certain number of threads to avoid getting timeouts. Our lock-free implementation scales best compared to all other involved implementations, having best performance already with 3 threads, independently if the system is fully concurrent or involves pre-emptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Extended Algorithm</head><p>When we have concurrent ÁÒ× ÖØ and Ð Ø Å Ò operations we might want to have certain real-time properties of the semantics of the Ð Ø Å Ò operation, as expressed in <ref type="bibr" target="#b8">[9]</ref>. The Ð Ø Å Ò operation should only return items that have been inserted by an ÁÒ× ÖØ operation that finished before the Ð Ø Å Ò operation started. To ensure this we are adding timestamps to each node. When the node is fully inserted its timestamp is set to the current time. Whenever the Ð Ø Å Ò operation is invoked it first checks the current time, and then discards all nodes that have a timestamp that is after this time. In the code of the implementation (see <ref type="bibr">Figures 6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr">8 and 9)</ref>, the additional statements that involve timestamps are marked within the " " and " " symbols. The function ØAE ÜØÌ Ñ ËØ ÑÔ, see Figure <ref type="figure" target="#fig_7">14</ref>, creates a new timestamp. The function ÓÑÔ Ö Ì Ñ ËØ ÑÔ, see Figure <ref type="figure" target="#fig_7">14</ref>, compares if the first timestamp is less, equal or higher than the second one and returns the values -1,0 or 1, respectively.</p><p>As we are only using the timestamps for relative comparisons, we do not need real absolute time, only that the timestamps are monotonically increasing. Therefore we can implement the time functionality with a shared counter, the synchronization of the counter is handled using CAS. However, the shared counter usually has a limited size (i.e. 32 bits) and will eventually overflow. Therefore the values of the timestamps have to be recycled. We will do this by exploiting information that are available in real-time systems, with a similar approach as in <ref type="bibr" target="#b14">[15]</ref>.</p><p>We assume that we have Ò periodic tasks in the system, indexed ½ Ò . For each task we will use the standard notations Ì , , Ê and to denote the period (i.e. Ñ Ò period for sporadic tasks), worst case execution time, worst case response time and deadline, respectively. The deadline of a task is less or equal to its period.</p><p>For a system to be safe, no task should miss its deadlines, i.e. Ê . For a system scheduled with fixed priority, the response time for a task in the initial system can be calculated using the standard response time analysis techniques <ref type="bibr" target="#b0">[1]</ref>. If we with denote the blocking time (the time the task can be delayed by lower priority tasks) and with Ô´ µ denote the set of tasks with higher priority than task , the response time Ê for task can be formulated as:</p><formula xml:id="formula_6">t i T i T i T i R i LT v = increment</formula><formula xml:id="formula_7">Ê • • ¾ Ô´ µ Ê Ì<label>(9)</label></formula><p>The summand in the above formula gives the time that task may be delayed by higher priority tasks. For systems scheduled with dynamic priorities, there are other ways to calculate the response times <ref type="bibr" target="#b0">[1]</ref>. Now we examine some properties of the timestamps that can exist in the system. Assume that all tasks call either the ÁÒ× ÖØ or Ð Ø Å Ò operation only once per iteration. As each call to ØAE ÜØÌ Ñ ËØ ÑÔ will introduce a new timestamp in the system, we can assume that every task invocation will introduce one new timestamp. This new timestamp has a value that is the previously highest known value plus one. We assume that the tasks always execute within their response times Ê with arbitrary many interruptions, and that the execution time is comparably small. This means that the increment of highest timestamp respective the write to a node with the current timestamp can occur anytime within the interval for the response time. The maximum time for an ÁÒ× ÖØ operation to finish is the same as the response time Ê for its task . The minimum time between two index increments is when the first increment is executed at the end of the first interval and the next increment is executed at the very beginning of the second interval, i.e. Ì Ê . The minimum time between the subsequent increments will then be the period Ì . If we denote with ÄÌ Ú the maximum life-time that the timestamp with value Ú exists in the system, the worst case scenario in respect of growth of timestamps is shown in Figure <ref type="figure">11</ref>.</p><p>The formula for estimating the maximum difference in value between two existing timestamps in any execution becomes as follows:</p><formula xml:id="formula_8">Å ÜÌ Ò ¼ Ñ Ü Ú¾ ¼ ½ ÄÌ Ú Ì • ½ (10)</formula><p>Now we have to bound the value of Ñ Ü Ú¾ ¼ ½ ÄÌ Ú . When comparing timestamps, the absolute value of these are not important, only the relative values. Our method is that we continuously traverse the nodes and replace outdated timestamps with a newer timestamp that has the same comparison result. We traverse and check the nodes at the rate of one step to the right for every invocation of an ÁÒ× ÖØ or Ð Ø Å Ò operation. With outdated timestamps we define timestamps that are older (i.e. lower) than any timestamp value that is in use by any running Ð Ø Å Ò operation. We denote with Ò ÒØÎ Ð the maximum difference that we allow between the highest known timestamp value and the timestamp value of a node, before we call this timestamp outdated.</p><formula xml:id="formula_9">Ò ÒØÎ Ð Ò ¼ Ñ Ü Ê Ì<label>(11)</label></formula><p>If we denote with Ø ancient the maximum time it takes for a timestamp value to be outdated counted from its first occurrence in the system, we get the following relation:</p><formula xml:id="formula_10">Ò ÒØÎ Ð Ò ¼ Ø ancient Ì Ò ¼ Ø ancient Ì Ò<label>(12)</label></formula><p>Ø ancient</p><formula xml:id="formula_11">Ò ÒØÎ Ð • Ò Ò ¼ ½ Ì<label>(13)</label></formula><p>Now we denote with Ø traverse the maximum time it takes to traverse through the whole list from one position and getting back, assuming the list has the maximum size AE .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AE</head><formula xml:id="formula_12">Ò ¼ Ø traverse Ì Ò ¼ Ø traverse Ì Ò<label>(14)</label></formula><p>Ø traverse</p><formula xml:id="formula_13">AE • Ò Ò ¼ ½ Ì<label>(15)</label></formula><p>The worst-case scenario is that directly after the timestamp of one node gets traversed, it gets outdated. Therefore we get:</p><formula xml:id="formula_14">Ñ Ü Ú¾ ¼ ½ ÄÌ Ú Ø ancient • Ø traverse<label>(16)</label></formula><p>Putting all together we get:  </p><formula xml:id="formula_15">Å ÜÌ Ò ¼ ¼ ¾ AE • ¾Ò • Ò ¼ Ñ Ü Ê Ì Ì Ò Ð ¼ ½ Ì Ð ¿ • ½ ½<label>(17)</label></formula><p>The above equation gives us a bound on the length of the "window" of active timestamps for any task in any possible execution. In the unbounded construction the tasks, by producing larger timestamps every time they slide this window on the ¼ ½℄ axis, always to the right. The approach now is instead of sliding this window on the set ¼ ½℄ from left to right, to cyclically slide it on a ¼ ℄ set of consecutive natural numbers, see figure 12. Now at the same time we have to give a way to the tasks to identify the order of the different timestamps because the order of the physical numbers is not enough since we are re-using timestamps. The idea is to use the bound that we have calculated for the span of different active timestamps. Let us then take a task that has observed Ú as the lowest timestamp at some invocation . When this task runs again as ¼ , it can conclude that the active timestamps are going to be between Ú and ´Ú •Å ÜÌ µ ÑÓ . On the other hand we should make sure that in this interval Ú ´Ú • Å ÜÌ µ ÑÓ ℄ there are no old timestamps. By looking closer to equation 10 we can conclude that all the other tasks have written values to their registers with timestamps that are at most Å ÜÌ less than Ú at the time that wrote the value Ú . Consequently if we use an interval that has double the Therefore we can use a timestamp field with double the size of the maximum possible value of the timestamp.</p><formula xml:id="formula_16">Ì Ð Ë Þ Å ÜÌ £ ¾ Ì Ð Ø× ÐÓ ¾ Ì Ð Ë Þ</formula><p>In this way ¼ will be able to identify that Ú ½ Ú ¾ Ú ¿ Ú (see figure <ref type="figure">13</ref>) are all new values if ¾ • ¿ Å ÜÌ and can also conclude that:</p><formula xml:id="formula_17">Ú ¿ Ú Ú ½ Ú ¾</formula><p>The mechanism that will generate new timestamps in a cyclical order and also compare timestamps is presented in Figure <ref type="figure" target="#fig_7">14</ref> together with the code for traversing the nodes. Note that the extra properties of the priority queue that are achieved by using timestamps are not complete with respect to the ÁÒ× ÖØ operations that finishes with an update. These update operations will behave the same as for the standard version of the implementation.</p><p>Besides from real-time systems, the presented technique can also be useful in non real-time systems as well. For example, consider a system of Ò ½¼ threads, where the minimum time between two invocations would be Ì ½¼ ns, and the maximum response time Ê ½¼¼¼¼¼¼¼¼¼ ns (i.e. after 1 s we would expect the thread to have crashed). Assuming a maximum size of the list AE ½¼¼¼¼, we will have a maximum timestamp difference Å ÜÌ ½¼¼¼¼½¼¼¿¼, thus needing 31 bits. Given that most systems have 32-bit integers and that many modern systems handle 64 bits as well, it implies that this technique is practical for also non real-time systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>We have presented a lock-free algorithmic implementation of a concurrent priority queue. The implementation is based on the sequential Skiplist data structure and builds on top of it to support concurrency and lock-freedom in an efficient and practical way. Compared to the previous attempts to use Skiplists for building concurrent priority queues our algorithm is lock-free and avoids the performance penalties that come with the use of locks. Compared to the previous lock-free/wait-free concurrent priority queue algorithms, our algorithm inherits and carefully retains the basic design characteristic that makes Skiplists practical: simplicity. Previous lock-free/wait-free algorithms did not perform well because of their complexity, furthermore they were often based on atomic primitives that are not available in today's systems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. The Test-And-Set (TAS), Fetch-And-Add (FAA) and Compare-And-Swap (CAS) atomic primitives.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Functions for traversing the nodes in the Skiplist data structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. HelpDelete</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. Experiment with priority queues and high contention, initialized with 100 respective 1000 nodes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>highest known timestamp value by 1 Figure 11 .</head><label>111</label><figDesc>Figure 11. Maximum timestamp increasement estimation -worst case scenario</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 12 Figure 13 .</head><label>1213</label><figDesc>Figure 12. Timestamp value recycling</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 14 .</head><label>14</label><figDesc>Figure 14. Creation, comparison, traversing and updating of bounded timestamps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>The Skiplist data structure with 5 nodes inserted.</head><label></label><figDesc></figDesc><table><row><cell cols="2">Local Memory</cell><cell cols="2">Local Memory</cell><cell>. . .</cell><cell></cell><cell>Local Memory</cell></row><row><cell>Processor 1</cell><cell></cell><cell cols="2">Processor 2</cell><cell></cell><cell></cell><cell>Processor n</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Interconnection Network</cell></row><row><cell></cell><cell></cell><cell cols="3">Shared Memory</cell><cell></cell><cell></cell></row><row><cell cols="7">Figure 1. Shared Memory Multiprocessor Sys-</cell></row><row><cell cols="2">tem Structure</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>H</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>T</cell></row><row><cell>Figure 2.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>pointer to Node Figure 3. The Node structure.</head><label></label><figDesc>. The maximum height (i.e. the maxi-</figDesc><table><row><cell>structure Node key,level,validLevel ,timeInsert : integer value : pointer to word next[level],prev : 1 2 4 3 Inserted node Deleted node</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Figure 4. Concurrent insert and delete opera- tion can delete both nodes.</head><label></label><figDesc></figDesc><table><row><cell>function TAS(value:pointer to word):boolean</cell></row><row><cell>atomic do</cell></row><row><cell>if *value=0 then</cell></row><row><cell>*value:=1;</cell></row><row><cell>return true;</cell></row><row><cell>else return false;</cell></row><row><cell>procedure FAA(address:pointer to word, number:integer)</cell></row><row><cell>atomic do</cell></row><row><cell>*address := *address + number;</cell></row><row><cell>function CAS(address:pointer to word, oldvalue:word,</cell></row><row><cell>newvalue:word):boolean</cell></row><row><cell>atomic do</cell></row><row><cell>if *address = oldvalue then</cell></row><row><cell>*address := newvalue;</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>return true; else return false;</head><label></label><figDesc></figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We compared our algorithm with some of the most efficient implementations of priority queues known. Experiments show that our implementation scales well, and with 3 threads or more our implementation outperforms the corresponding lock-based implementations, for all cases on both fully concurrent systems as well as with pre-emption.</p><p>We believe that our implementation is of highly practical interest for multi-threaded applications. We are currently incorporating it into the NOBLE <ref type="bibr" target="#b15">[16]</ref> library.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fixed Priority Pre-emptive Scheduling: An Historical Perspective</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Audsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">I</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Tin-Dell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Wellings</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Real-Time Systems</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2/3</biblScope>
			<biblScope unit="page" from="129" to="154" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Wait-Free Algorithms for Heaps</title>
		<author>
			<persName><forename type="first">G</forename><surname>Barnes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992-02">Feb. 1992</date>
			<publisher>University of Washington</publisher>
		</imprint>
		<respStmt>
			<orgName>Computer Science and Engineering</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Priority queues and sorting for parallel simulation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Grammatikakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liesche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Software Engineering, SE</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="401" to="422" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A Pragmatic Implementation of Non-Blocking Linked Lists</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Harris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th International Symposium of Distributed Computing</title>
		<meeting>the 15th International Symposium of Distributed Computing</meeting>
		<imprint>
			<date type="published" when="2001-10">Oct. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Linearizability: a Correctness Condition for Concurrent Objects</title>
		<author>
			<persName><forename type="first">M</forename><surname>Herlihy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Programming Languages and Systems</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="463" to="492" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Wait-Free Synchronization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Herlihy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TOPLAS</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="124" to="149" />
			<date type="published" when="1991-01">Jan. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An Efficient Algorithm for Concurrent Priority Queue Heaps</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Parthasarathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing Letters</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="151" to="157" />
			<date type="published" when="1996-11">Nov. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Efficient wait-free implementation of a concurrent priority queue. 7th Intl Workshop on Distributed Algorithms &apos;93</title>
		<author>
			<persName><forename type="first">A</forename><surname>Israeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rappaport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">725</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="1993-09">Sept. 1993</date>
			<publisher>Springer Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Skiplist-Based Concurrent Priority Queues</title>
		<author>
			<persName><forename type="first">I</forename><surname>Lotan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shavit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Parallel and Distributed Processing Symposium</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Correction of a Memory Management Method for Lock-Free Data Structures</title>
		<author>
			<persName><forename type="first">M</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Scott</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
		<respStmt>
			<orgName>Computer Science Dept., University of Rochester</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Skip lists: a probabilistic alternative to balanced trees</title>
		<author>
			<persName><forename type="first">W</forename><surname>Pugh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="668" to="676" />
			<date type="published" when="1990-06">June 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Real-Time Synchronization Protocols for Shared Memory Multiprocessors</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rajkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">th International Conference on Distributed Computing Systems</title>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="116" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Priority Inheritance Protocols: An Approach to Real-Time Synchronization</title>
		<author>
			<persName><forename type="first">L</forename><surname>Sha And R. Rajkumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehoczky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1175" to="1185" />
			<date type="published" when="1990-09">Sep. 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Operating System Concepts</title>
		<author>
			<persName><forename type="first">A</forename><surname>Silberschatz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Galvin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>Addison Wesley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Space Efficient Wait-Free Buffer Sharing in Multiprocessor Real-Time Systems Based on Timing Information</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tsigas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Real-Time Computing Systems and Applicatons (RTCSA 2000)</title>
		<meeting>the 7th International Conference on Real-Time Computing Systems and Applicatons (RTCSA 2000)</meeting>
		<imprint>
			<publisher>IEEE press</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="433" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">NOBLE: A Non-Blocking Inter-Process Communication Library</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tsigas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Workshop on Languages, Compilers and Runtime Systems for Scalable Computers (LCR&apos;02)</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting>the 6th Workshop on Languages, Compilers and Runtime Systems for Scalable Computers (LCR&apos;02)</meeting>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Evaluating the performance of non-blocking synchronization on shared-memory multiprocessors</title>
		<author>
			<persName><forename type="first">P</forename><surname>Tsigas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the international conference on Measurement and modeling of computer systems (SIGMETRICS 2001)</title>
		<meeting>the international conference on Measurement and modeling of computer systems (SIGMETRICS 2001)</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="320" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Integrating Non-blocking Synchronisation in Parallel Applications: Performance Advantages and Methodologies</title>
		<author>
			<persName><forename type="first">P</forename><surname>Tsigas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd ACM Workshop on Software and Performance (WOSP &apos;02)</title>
		<meeting>the 3rd ACM Workshop on Software and Performance (WOSP &apos;02)</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Lock-Free Data Structures</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Valois</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Rensselaer Polytechnic Institute</publisher>
			<pubPlace>Troy, New York</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">PhD. Thesis</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
