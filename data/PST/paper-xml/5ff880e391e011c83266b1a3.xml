<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An In-Depth Comparative Analysis of Cloud Block Storage Workloads: Findings and Implications *</title>
				<funder>
					<orgName type="full">Tencent Cloud Block Storage (TencentCloud)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jinhong</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<settlement>Kong ? Alibaba Group</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qiuping</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<settlement>Kong ? Alibaba Group</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Patrick</forename><forename type="middle">P C</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<settlement>Kong ? Alibaba Group</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chao</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<settlement>Kong ? Alibaba Group</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An In-Depth Comparative Analysis of Cloud Block Storage Workloads: Findings and Implications *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Cloud block storage systems support diverse types of applications in modern cloud services. Characterizing their I/O activities is critical for guiding better system designs and optimizations. In this paper, we present an in-depth comparative analysis of production cloud block storage workloads through the block-level I/O traces of billions of I/O requests collected from two production systems, Alibaba Cloud and Tencent Cloud Block Storage. We study their characteristics of load intensities, spatial patterns, and temporal patterns. We also compare the cloud block storage workloads with the notable public block-level I/O workloads from the enterprise data centers at Microsoft Research Cambridge, and identify the commonalities and differences of the three sources of traces. To this end, we provide 6 findings through the high-level analysis and 16 findings through the detailed analysis on load intensity, spatial patterns, and temporal patterns. We discuss the implications of our findings on load balancing, cache efficiency, and storage cluster management in cloud block storage systems.</p><p>* An earlier version of this article appeared in <ref type="bibr" target="#b21">[22]</ref>. In this extended version, we further include the workload traces from Tencent Cloud Block Storage [52] in our analysis. We extend our findings to show the commonalities and differences between the cloud block storage workloads from Alibaba Cloud and Tencent Cloud Block Storage. motivate the need of collecting and analyzing comprehensive block-level I/O traces from real-world cloud block storage systems in large-scale production.</p><p>In this paper, we present an in-depth comparative study on the block-level I/O traces from two production cloud block storage systems. The first set of traces, which we refer to as AliCloud, is collected by ourselves from a production cloud block storage system deployed at Alibaba Cloud <ref type="bibr" target="#b21">[22]</ref> and covers one-month I/O activities of 1,000 volumes. The second set of traces, which we refer to as TencentCloud, is collected from the Tencent Cloud Block Storage by Zhang et al. <ref type="bibr" target="#b51">[52]</ref> and covers I/O activities of 4,995 volumes over around nine days. Both sets of traces feature a large data scale, totaling billions of I/O requests and hundreds of terabytes of I/O traffic. We compare the cloud block storage workload characteristics of both AliCloud and TencentCloud traces with those of the MSRC traces, and identify the commonalities and differences of the three sources of traces. To this end, we provide 6 findings through the high-level analysis on the basic I/O characteristics of the traces, and further provide 16 findings through the detailed analysis on the I/O behaviors in terms of the load intensities, spatial patterns, and temporal patterns. We provide insights into load balancing, cache efficiency, and storage cluster management in cloud block storage systems. Note that Zhang et al. <ref type="bibr" target="#b51">[52]</ref> mainly use the TencentCloud traces for designing efficient cache allocation schemes in cloud block storage systems, but do not provide an in-depth analysis on the TencentCloud traces. To the best of our knowledge, compared with prior measurement studies on block-level I/O traces <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b53">54]</ref> (see details in ?5), our trace analysis is one of the largest measurement studies on block-level I/O traces reported in the literature. We make the source code of all our analysis scripts available at: http://adslab.cse.cuhk.edu.hk/software/blockanalysis.</p><p>We highlight some major findings of our trace analysis. From the high-level analysis, small I/O requests dominate in all traces. Both AliCloud and TencentCloud are write-dominant, while MSRC is read-dominant. For load intensities, all traces show similar amounts of I/O traffic, while AliCloud and TencentCloud show more diverse burstiness across volumes and have higher activeness than MSRC. For spatial patterns, all traces show aggregations of reads and writes in small working sets. In particular, TencentCloud shows the highest level of aggregations of reads, implying a more skewed access pattern in reads. All traces also show high fractions of random I/Os and varying patterns in the update coverage across volumes. For temporal patterns, all traces have varying temporal update patterns across volumes and different access tendencies for the written blocks. For example, each written block in AliCloud and TencentCloud is likely to be followed by a write, while that in MSRC is about equally likely to be followed by either a read or a write.</p><p>The rest of the paper proceeds as follows. In ?2, we present our cloud block storage architecture and its design considerations. In ?3, we introduce the traces for our analysis, and present 6 findings via our high-level analysis. In ?4, we conduct an in-depth analysis and provide 16 findings on load intensities, spatial patterns, and temporal patterns. We emphasize the commonalities and differences of the findings between AliCloud and TencentCloud. We further discuss the implications of our findings in terms of load balancing, cache efficiency, and storage cluster management in cloud block storage systems. In ?5, we review related work. In ?6, we conclude the paper.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Traditional desktop and server applications, such as virtual desktops, operating systems, web services, relational databases, and key-value stores, are now moving to the cloud. Cloud block storage systems <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53]</ref> form an infrastructure that allows cloud service providers to manage large-scale physical storage clusters. They also provide virtual disks, referred to as volumes, for clients to host various types of applications. To allow performance optimizations and efficient resource provisioning of cloud block storage systems, it is critical to characterize and understand the I/O behaviors of the applications in production environments.</p><p>Several field studies have analyzed the I/O behaviors of various architectures via the collection and characterization of block-level I/O traces <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b53">54]</ref>. In particular, the public block-level I/O traces released by Microsoft Research Cambridge <ref type="bibr" target="#b32">[33]</ref> have received wide attention from researchers and practitioners. The traces, which we refer to as MSRC, have been extensively analyzed to motivate storage system designs and optimizations, such as I/O scheduling <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b32">33]</ref>, caching <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>, erasure-coded storage <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b52">53]</ref>, as well as cloud block storage <ref type="bibr" target="#b20">[21]</ref>.</p><p>However, the MSRC traces, which were collected from enterprise data centers more than a decade ago, may not necessarily reflect the actual I/O behaviors of today's cloud block storage systems. Modern cloud environments often host much more diverse types of applications, some of which feature unique characteristics (e.g., short-lived tasks <ref type="bibr" target="#b31">[32]</ref>) that are not commonly found in traditional data center environments. Also, the workloads in MSRC are generally read-dominant <ref type="bibr" target="#b32">[33]</ref>, while the workloads in cloud environments are often write-dominant due to the heavy use of read caches in cloud applications <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b45">46]</ref>. Such mismatches</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Methodology</head><p>We introduce the cloud block storage architecture considered in the paper ( ?2.1). We further elaborate on how our trace analysis should characterize the I/O activities in response to the design considerations for cloud block storage ( ?2.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Cloud Block Storage</head><p>Figure <ref type="figure" target="#fig_0">1</ref> depicts the architecture of a cloud block storage system considered in the paper. The cloud block storage system serves as a middleware layer that bridges: (i) the virtual disks (referred to as volumes) that are perceived by upper-layer applications, and (ii) the storage clusters that provide physical storage space owned by cloud service providers. Each application is allocated with a dedicated volume. It issues read or write requests through the dedicated volume to the storage clusters. Each volume is typically replicated across multiple storage clusters for fault tolerance. For performance and reliability, today's storage cluster are often backed by flash-based solid-state drives (SSDs) instead of hard disk drives (HDDs) <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b48">49]</ref>.</p><p>In production, a cloud block storage system may manage diverse types of upper-layer cloud applications (Figure <ref type="figure" target="#fig_0">1</ref>). The I/O characteristics of such applications are often largely different, as we show in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Analysis Methodology</head><p>Cloud block storage systems should maintain quality-of-services guarantees (e.g., low-latency requests and fairness) and efficient resource utilizations (e.g., long device lifetime). We highlight three design considerations for cloud block storage systems, namely load balancing, cache efficiency, and storage cluster management. In the following, we explain how each of the design considerations can be addressed in our trace analysis of I/O activities in cloud block storage. Load balancing. Maintaining load balancing across storage devices is important for availability and performance. If load imbalance exists, some storage devices may be overloaded by a large number of I/O requests and cannot serve incoming requests in a timely manner, thereby increasing the overall I/O latencies. In addition, the overloading of I/O requests may aggravate flash wearing <ref type="bibr" target="#b47">[48]</ref>, leading to reduced endurance. Since load balancing addresses the performance differences due to the uneven distribution of I/O traffic, our trace analysis should examine the load intensities of I/O traffic. Cache efficiency. To speed up I/O performance, storage systems typically cache frequently accessed data based on efficient resource allocation schemes and admission policies <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b43">44]</ref>. However, the high variations of I/O characteristics may introduce improper cache space allocation and cache management policies, which degrade hit ratios and increase the overall I/O latencies. To investigate how the caching design can leverage workload characteristics, our trace analysis should address the spatial and temporal aggregations of I/O traffic. Storage cluster management. Enterprise storage clusters increasingly move to flash-based storage, which is sensitive to varying workload patterns in both performance and endurance. In particular, the update patterns can determine the effectiveness of garbage collection and wear-leveling in flash <ref type="bibr" target="#b16">[17]</ref>. Storage cluster management should address the variations of workload patterns, so as to maintain high performance and endurance of the underlying flash devices. Thus, our trace analysis should focus on the spatial and temporal patterns for update requests. Also, as small and random I/Os can degrade the performance and endurance of flash storage <ref type="bibr" target="#b30">[31]</ref>, our trace analysis should also examine the randomness of I/Os.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Traces</head><p>We describe the three sets of traces used in our analysis and state the limitations of our trace analysis ( ?3.1). We then present a high-level analysis on the basic statistics as well as the commonalities and differences on all three traces ( ?3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Trace Overview</head><p>Our trace analysis is based on three sets of block-level I/O traces collected from different production environments. For brevity, we refer to the traces as AliCloud, TencentCloud, and MSRC in short in the following discussion. AliCloud. The traces were collected by ourselves from a cloud block storage system deployed at Alibaba Cloud over a one-month period in January 2020. The traces are now released at <ref type="bibr" target="#b1">[2]</ref>. They comprise block-level I/O requests collected from 1,000 volumes, each of which has a raw capacity from 40 GiB to 5,000 GiB. The workloads span diverse types of cloud applications ( ?2.1). Each collected I/O request specifies the volume number, request type, request offset, request size, and timestamp (in units of microseconds). TencentCloud. The traces were collected from the cloud block storage system at Tencent Cloud Block Storage <ref type="bibr" target="#b51">[52]</ref> from 12:00 AM on October 1, 2018 to 1:00 AM on October 10, 2018 in the GMT+8 time zone (i.e., 9.04 days in total); note that the requests are missing between 1:00 AM and 2:00 AM on October 8, 2018. The traces can be downloaded from the SNIA IOTTA repository <ref type="bibr" target="#b39">[40]</ref>. They comprise block-level I/O requests collected from 4,995 volumes. The workloads are based on a mixture of cloud applications, including applications dominated by random accesses and applications with large amount of I/O activity <ref type="bibr" target="#b51">[52]</ref>. Each collected I/O request contains the volume number, request type, request offset, request size, and timestamp as in the AliCloud traces, except that the timestamp in the TencentCloud traces is in units of seconds. However, the TencentCloud traces do not provide the raw capacities of individual volumes. MSRC <ref type="bibr" target="#b32">[33]</ref>. The traces were collected by Microsoft Research Cambridge from a data center of Microsoft Windows servers over a one-week period in February 2007 and can be downloaded from the SNIA IOTTA repository <ref type="bibr" target="#b29">[30]</ref>. They comprise block-level I/O requests from 36 volumes over 179 disks in 13 servers. The workloads span a variety of applications, including home directories, project directories, web services, source control, media services, etc. Each collected I/O request includes the volume number, request type, request offset, request size, and timestamp as in the AliCloud and TencentCloud traces; it also includes the response time of the request. Both the timestamp and the response time are specified in units of 100 ns, based on the Windows Filetime timestamp format used by Microsoft Windows servers. Limitations of our trace analysis. Our trace analysis has several limitations. First, both the AliCloud and TencentCloud traces do not record the response times of the I/O requests as in MSRC, so we cannot conduct latency analysis on I/O requests in actual deployment. In particular, the timestamp field in the TencentCloud traces is in units of seconds, so it is difficult to conduct fine-grained analysis on the inter-arrival times of requests in the TencentCloud traces. Also, both traces do not indicate the specific applications running atop individual volumes, so we cannot investigate the relationship between specific application workloads and their I/O patterns. Furthermore, all three traces do not include the caching policies, cache hit/miss ratios (i.e., we cannot study the impact of caching in actual deployment) and the age or usage of each volume (i.e., we cannot study the relationships between I/O patterns and volume ages). Finally, all three traces do not capture the information of physical storage devices (e.g., data placement and failure statistics), so we cannot study the performance and reliability correlations in physical storage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">High-level Comparative Analysis</head><p>We now present a high-level comparative analysis on AliCloud, TencentCloud, and MSRC by collectively analyzing the I/O requests of all volumes in each set of traces and presenting the overall basic statistics. Our goal is to examine the basic properties of the cloud block storage workloads in AliCloud and TencentCloud as well as the classical enterprise data center workloads in MSRC. To this end, we identify the commonalities and differences of all three traces. Table <ref type="table" target="#tab_0">1</ref> summarizes different categories of basic statistics in AliCloud, TencentCloud, and MSRC, including: (i) the numbers of reads and writes, (ii) the total amounts of data read, written, and updated, as well as (iii) the working set sizes (WSSs) of reads, writes, and updates (an update request refers to a write  request to a block that has been written at least once). We define the total read, write, and update WSSs as the numbers of unique logical addresses being accessed (i.e., read, written, and updated, respectively) by all I/O requests of the traces multiplied by the block size 4 KiB. Table <ref type="table" target="#tab_1">2</ref> further summarizes the key properties of all three traces observed in our high-level analysis.</p><p>In terms of scale, both AliCloud and TencentCloud have much larger scale than MSRC in various aspects, including the number of volumes, the trace duration, the total number of I/O requests, and the total I/O traffic size. For example, AliCloud contains 20.2 billion I/O requests, 46.6? the total number of I/O requests in MSRC. It also has more volumes (27.8?), a larger total I/O traffic size (54.1?), and a larger WSS (10.3?), compared with those in MSRC. TencentCloud has an even larger scale than AliCloud in terms of the number of volumes (5.0?), the total number of I/O requests (1.7?), and the total I/O traffic size (1.8?), except that its duration only lasts for 9.04 days. In the following, we elaborate the I/O characteristics of the three traces. Finding A.1: Reads span a small proportion of working sets in both AliCloud and TencentCloud.</p><p>Referring to Table <ref type="table" target="#tab_0">1</ref>, reads in AliCloud and TencentCloud only occupy 34.3% and 37.6% of the total WSS, respectively, while reads in MSRC occupy a much larger proportion (98.4%) of the total WSS. On the other hand, writes in AliCloud and TencentCloud occupy 89.4% and 85.2% of the total WSS, respectively. The results indicate that a substantial amount of written data is never read again in both AliCloud and TencentCloud. One possible reason is that some applications tend to only write data but rarely read data (e.g., backups or journaling), although we cannot identify the specific applications running on individual volumes ( ?3.1). Finding A.2: Small-size I/Os dominate in all AliCloud, TencentCloud, and MSRC.</p><p>Figure <ref type="figure" target="#fig_1">2</ref>(a) shows the cumulative distributions of request sizes across all I/O requests in all three traces. We see that all traces feature small-size I/O requests (less than 100 KiB). Specifically, in AliCloud, 75% of reads and writes are no larger than 12 KiB and 16 KiB, respectively, while in TencentCloud, 75% of reads  and writes are no larger than 32 KiB and 12 KiB, respectively. In MSRC, 75% of reads and writes are no larger than 64 KiB and 20 KiB, respectively.</p><p>The dominance of small-size I/O requests also holds in individual volumes. We compute the average request size for each volume. Figure <ref type="figure" target="#fig_1">2</ref>(b) shows the cumulative distributions of the average request sizes of all volumes in all three traces. We see that 75% of the average read and write sizes in AliCloud are less than 37.0 KiB and 26.8 KiB, respectively, while 75% of the average read and write sizes in TencentCloud are less than 49.8 KiB and 19.0 KiB, respectively. For MSRC, 75% of the average read and write sizes are less than 48.4 KiB and 16.4 KiB, respectively. Small I/Os are also commonly found in enterprise and desktop file system workloads <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b33">34]</ref>.</p><p>To ensure that our observations are not biased by outliers, we further measure the cumulative distributions of the medians and 75th percentiles of request sizes across all volumes, as shown in Figures <ref type="figure" target="#fig_1">2(c</ref>) and 2(d), respectively. In Figure <ref type="figure" target="#fig_1">2</ref>(c), we see that the median read and write sizes in 75% of volumes in all three traces are no more than 64 KiB and 4 KiB, respectively. Similarly, in Figure <ref type="figure" target="#fig_1">2(d)</ref>, we see that the 75th percentiles of read and write sizes in 75% of volumes in all three traces are no more than 64 KiB and 12 KiB, respectively. The results indicate that small-size I/O requests dominate even if we consider medians and 75th percentiles of (instead of average) request sizes. Finding A.3: A non-negligible fraction of volumes in AliCloud are active in short time periods, but it is not the case in TencentCloud and MSRC.</p><p>We study the activeness of individual volumes. Here, we measure the number of active days for each volume, in which a volume is said to be active in a day if it receives at least one I/O request (i.e., up to 31, 9, and 7 active days in AliCloud, TencentCloud, and MSRC, respectively). Figure <ref type="figure" target="#fig_2">3</ref> depicts the cumulative distributions of numbers of active days across all volumes in all three traces. In AliCloud, 15.7% of volumes (i.e., 157 volumes) are active for only one day. We find that 147 out of the 157 volumes are active in only four hours, and the total WSS and I/O traffic of the 157 volumes account for only 1.4% and 0.07% of all 1,000 volumes, respectively. One possible reason for the short active periods in such volumes in AliCloud is   the presence of short-lived tasks in cloud applications <ref type="bibr" target="#b31">[32]</ref>. On the other hand, in TencentCloud, only 1.7% of volumes are active for only one day, while 90.1% of volumes are active for all nine days in the entire trace duration. Also, all volumes in MSRC are active for all seven days in the entire trace duration. Finding A.4: Most volumes in AliCloud and TencentCloud are write-dominant.</p><p>Referring to Table <ref type="table" target="#tab_0">1</ref>, the overall write-to-read ratio (i.e., the ratio between the number of writes and the number of reads) in AliCloud is 3:1, and that in TencentCloud is 2.35:1. However, the write-to-read ratio in MSRC is 0.42:1 only. We further analyze the write-to-read ratios on a per-volume basis. Figure <ref type="figure" target="#fig_3">4</ref> shows the cumulative distributions of write-to-read ratios across all volumes in all three traces. In AliCloud and TencentCloud, 91.5% (i.e., 915 out of 1,000) and 92.3% (i.e., 4,608 out of 4,995) of the volumes are write-dominant (i.e., the write-to-read ratios are larger than 1). Also, 42.4% and 36.5% of the volumes in AliCloud and TencentCloud even have very high write-to-read ratios that are larger than 100, respectively. On the other hand, MSRC has an opposite pattern, in which only 52.8% (19 out of 36) of volumes are write-dominant. Note that prior work <ref type="bibr" target="#b36">[37]</ref> also shows the existence of write-dominant workloads in MSRC, especially in files such as mail boxes, search indexes, registry files, and file system metadata files.</p><p>Figure <ref type="figure" target="#fig_4">5</ref> further analyzes the cumulative distributions of WSSs and I/O traffic sizes across all volumes in all three traces. Figures <ref type="figure" target="#fig_4">5(a</ref>)-5(c) show the cumulative distributions of the total WSSs, read WSSs, and write WSSs across all volumes. The write WSSs of both AliCloud and TencentCloud are significantly larger than the read WSSs and are close to the total WSSs (Figures <ref type="figure" target="#fig_4">5(a</ref>) and 5(b)). This implies that the total WSSs of both traces are mainly determined by writes. In contrast, the read WSSs of MSRC are close to the total WSSs (Figure <ref type="figure" target="#fig_4">5(c)</ref>). We also make similar observations in the I/O traffic sizes, in which AliCloud and TencentCloud are write-dominant (Figures <ref type="figure" target="#fig_4">5(d</ref>)-5(f)). One possible reason of the write dominance in both AliCloud and TencentCloud is the wide use of application-level read caches in cloud storage, in which reads are mostly absorbed in the application layer without being issued to the storage layer <ref type="bibr" target="#b45">[46]</ref>. Finding A.5: In AliCloud, larger volumes tend to have larger percentages of the total WSS over the raw capacity.</p><p>We examine the relationship between the total WSS and the raw capacity of a volume. Our analysis here focuses on the AliCloud traces, which provide the capacity information of individual volumes. Specifically, we divide the 1,000 volumes in AliCloud into four groups by volume capacities, including 40-49 GiB, 50-99 GiB, 100-199 GiB, and 200-5,000 GiB (note that each volume capacity is represented as an integer GiB). They account for 444, 179, 170, and 207 volumes, respectively. For each volume, we calculate the WSS-to-capacity percentage (i.e., the percentage of the total WSS over the raw capacity of the volume). We then plot the cumulative distributions of the WSS-to-capacity percentages for the four groups.</p><p>Figure <ref type="figure" target="#fig_5">6</ref> shows the results. For small volumes, the WSS-to-capacity percentages tend to be low (i.e., the usage of disk space is low). Specifically, 80% of the volumes in the 40-49 GiB, 50-99 GiB, and 100-199 GiB groups have WSS-to-capacity percentages of less than 7.3%, 16.1%, and 9.7%, respectively. On the other hand, in the 200-5,000 GiB group, half of the volumes have a WSS-to-capacity percentage of more than 16.7%, and 35.7% of the volumes have WSS-to-capacity percentages of more than 50%. In particular, the largest volume (i.e., with a raw capacity of 5,000 GiB) has a WSS-to-capacity percentage of 66.9%. Finding A.6: In AliCloud, larger volumes tend to have larger write request sizes, but have similar read request sizes compared to smaller volumes.</p><p>We examine the average request sizes of individual volumes with respect to their raw capacities. We again divide the volumes by raw capacities into four groups (i.e., 40-49 GiB, 50-99 GiB, 100-199 GiB, and 200-5,000 GiB) as above.</p><p>Figure <ref type="figure" target="#fig_7">7</ref> shows the results. Overall, the average request sizes in the 200-5,000 GiB group are larger than in the other three groups with smaller raw capacities. Specifically, the median of average request sizes in the 200-5,000 GiB group is 36.2 KiB, while the medians of average request sizes in the 40-49 GiB, 50-99 GiB, and 100-199 GiB groups are 11.9 KiB, 8.1 KiB, and 9.5 KiB, respectively. A possible reason is that larger volumes are often related to the workloads with the larger-size sequential data accesses.</p><p>We further examine the average read and write request sizes of individual volumes to understand where the larger requests in larger volumes come from. Figure <ref type="figure" target="#fig_8">8</ref> shows the results. We find that the average read sizes are close in four groups; the medians of average read request sizes in the 40-49 GiB, 50-99 GiB, 100-199 GiB, and 200-5,000 GiB groups are 29.3 KiB, 23.0 KiB, 24.9 KiB, and 24.4 KiB, respectively (Figure <ref type="figure" target="#fig_8">8(a)</ref>). In contrast, the median of average write request sizes in the 200-5000 GiB group is 36.5 KiB, while the medians of average write request sizes in the 40-49 GiB, 50-99 GiB, and 100-199 GiB groups are 9.1 KiB, 7.0 KiB, and 9.2 KiB, respectively (Figure <ref type="figure" target="#fig_8">8(b)</ref>). Summary. AliCloud, TencentCloud, and MSRC have some common aspects, such as the dominance of smallsize I/O requests, yet they also have many differences. In particular, both AliCloud and TencentCloud are write-dominant, while MSRC is read-dominant. A unique aspect for AliCloud, as opposed to TencentCloud and MSRC, is that it has a non-negligible fraction of volumes with short active periods. Also, large volumes in AliCloud tend to have larger WSS-to-capacity percentages and larger request sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Detailed Analysis</head><p>In this section, we conduct an in-depth comparative analysis on AliCloud, TencentCloud, and MSRC in three aspects: load intensities, spatial patterns, and temporal patterns. We report 16 findings from our analysis.    Table <ref type="table" target="#tab_3">3</ref> summarizes and compares the key properties of all three traces observed in our detailed analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Load Intensities</head><p>We study the characteristics of load intensities in the volumes of AliCloud, TencentCloud, and MSRC through a number of metrics. Specifically, we examine the average and peak load intensities <ref type="bibr" target="#b32">[33]</ref> and the distribution of inter-arrival times of requests <ref type="bibr" target="#b41">[42]</ref>. We also examine the activeness of volumes through the number of active volumes <ref type="bibr" target="#b32">[33]</ref> and the active period of each volume. Finding B.1: AliCloud, TencentCloud, and MSRC have similar average load intensities of volumes, while the peak load intensities of AliCloud and TencentCloud are generally lower than that of MSRC. We measure the load intensities of individual volumes, in terms of the number of requests per second (req/s), in two aspects. We first measure the average intensity of a volume, defined as the total number of requests divided by the time elapsed between the first and last requests of the volume. Note that for TencentCloud, if the missing hour of requests lies between the first and last requests ( ?3.1), we subtract the elapsed time by one hour. We also measure the peak intensity of a volume, in which we divide the whole duration of requests of the volume into one-minute intervals and find the peak intensity as the maximum number of requests (per second) across all intervals; we use one-minute intervals instead of one-second intervals since one-minute intervals are long enough to accumulate sufficient bursty requests.</p><p>Figure <ref type="figure" target="#fig_9">9</ref> shows the average and peak intensities of volumes in AliCloud, TencentCloud, and MSRC, sorted by the average intensities of volumes in descending order. We observe similar trends of average intensities in all three traces, but different patterns of peak intensities in TencentCloud. In AliCloud, TencentCloud, and MSRC, only 1.90%, 1.16%, and 2.78% of volumes have average intensities above 100 req/s, and the percentages of volumes with average intensities lower than 10 req/s are 81.6%, 85.7%, and 72.2%, respectively. Furthermore, their medians of average intensities are 2.55 req/s, 3.27 req/s, and 3.36 req/s, respectively. A possible reason of having similar average intensities in all three traces is that more applications are moving to  the cloud <ref type="bibr" target="#b20">[21]</ref>, so the average intensities are similar in both cloud and traditional data center environments. However, as for the peak intensities, their 90th percentiles of peak intensities are 578.7 req/s, 498.9 req/s, and 1,612.1 req/s, respectively, in which the peak intensity of MSRC is much higher than those of other two traces.</p><p>Finding B.2: AliCloud, TencentCloud, and MSRC have high burstiness in a non-negligible fraction of volumes, but their overall burstiness is mild.</p><p>We examine the burstiness of all three traces by measuring the burstiness ratio of a volume, defined as the ratio between the peak intensity and the average intensity of the volume. Figure <ref type="figure" target="#fig_10">10</ref> shows the cumulative distributions of burstiness ratios across all volumes in AliCloud, TencentCloud, and MSRC. We see that a non-negligible fraction of volumes (20.7%, 10.9%, and 38.9% in AliCloud, TencentCloud, and MSRC,    <ref type="table" target="#tab_5">4</ref>). Compared with both AliCloud and MSRC, TencentCloud has a lower percentage of volumes with high burstiness ratios, as well as a lower overall burstiness. This shows that the burstiness level is mild from the whole-system's perspective, but is significant for some of the volumes. Finding B.3: AliCloud and TencentCloud have more diverse burstiness across volumes than MSRC.</p><p>The volumes in both AliCloud and TencentCloud span a wider range of burstiness than those in MSRC. Referring to Figure <ref type="figure" target="#fig_10">10</ref>, for the volumes with low burstiness, 25.8% and 37.0% of volumes in AliCloud and TencentCloud have burstiness ratios less than 10, while the corresponding percentage is only 2.78% in MSRC. On the other hand, for the volumes with high burstiness, 2.60% and 0.80% of volumes in AliCloud and TencentCloud have burstiness ratios larger than 1,000, respectively, while there are no such volumes in MSRC. The higher diversities of burstiness in AliCloud and TencentCloud suggest larger variations in workload characteristics among different volumes in cloud block storage. Finding B.4: Both AliCloud and MSRC have high short-term burstiness from the perspective of inter-arrival times of requests.</p><p>We measure the inter-arrival times of I/O requests (i.e., the elapsed time between two adjacent requests) for each volume. We first examine the cumulative distributions of inter-arrival times across all volumes. Figure <ref type="figure" target="#fig_11">11</ref>(a) shows the results. Note that we do not consider TencentCloud, as its timestamps are in units of seconds and we cannot accurately measure the inter-arrival times at finer-grained granularities (e.g., at the microsecond level). We find that most of the inter-arrival times are smaller than one second. For example, in AliCloud and MSRC, 50% of the inter-arrival times are smaller than 351 ?s and 142 ?s, respectively, and 99% of the inter-arrival times are smaller than 3,140 ms and 484 ms, respectively. The results indicate that short inter-arrival times are common in both traces.</p><p>We also consider five groups of percentiles of inter-arrival times for each volume, including the 25th, 50th, 75th, 90th, and 95th percentiles. We represent each group of percentile values of all volumes by boxplots. Figures <ref type="figure" target="#fig_11">11(b</ref>) and 11(c) show the results of both AliCloud and MSRC, respectively. Both AliCloud and MSRC traces have a high number of bursty requests, as indicated by large fractions of short inter-arrival times in the volumes. In particular, the medians of the groups of 25th, 50th, and 75th percentiles are lower than 1.3 ms, or equivalently over 700 req/s (i.e., 31 ?s, 145 ?s, and 735 ?s in AliCloud, and 3.5 ?s, 30.5 ?s, and 1.3 ms in MSRC, respectively). Also, the volumes in AliCloud have much higher inter-arrival times of requests than those in MSRC. For example, half of the volumes in AliCloud have 25th percentiles higher than 31 ?s (Figure <ref type="figure" target="#fig_11">11</ref>(a)), while half of the volumes in MSRC have 25th percentiles higher than 3.5 ?s (Figure <ref type="figure" target="#fig_11">11(b)</ref>). Note that prior work <ref type="bibr" target="#b18">[19]</ref> also identifies the existence of short inter-arrival times (e.g., a few Recall from Section 3.2 that we examine the activeness of volumes of all three traces on a per-day basis. We now revisit the activeness of volumes of all three traces in a more fine-grained manner. Specifically, we divide the traces into 10-minute intervals. We say that a volume is active in an interval if it has at least one request in the interval. We also say that a volume is read-active and write-active in an interval if it has at least one read request and one write request in the interval, respectively.</p><p>Figure <ref type="figure" target="#fig_1">12</ref> depicts the numbers of active, read-active, and write-active volumes throughout the trace periods in AliCloud, TencentCloud, and MSRC (recall that they have 1,000, 4,995, and 36 volumes, respectively). We find that the percentages of active volumes throughout the trace duration are always larger than 73.1%, 88.2%, and 59.4% in AliCloud, TencentCloud, and MSRC, respectively. In particular, TencentCloud has the highest fraction of active volumes throughout the trace periods. Also, the numbers of active volumes in AliCloud and TencentCloud have more stable trends compared with that in MSRC. Furthermore, the number of read-active volumes in AliCloud shows diurnal patterns by often having less than 200 read-active volumes at night and more than 300 read-active volumes at daytime, which is similar to the patterns found in object storage systems <ref type="bibr" target="#b7">[8]</ref>, enterprise virtual desktops <ref type="bibr" target="#b19">[20]</ref>, and key-value stores <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b50">51]</ref>.</p><p>We also measure the active time period of each volume, based on the number of 10-minute intervals in which the volume is active. Figure <ref type="figure" target="#fig_2">13</ref> depicts the cumulative percentages of active time periods across all volumes in all three traces. More than 72.2%, 88.2%, and 55.6% of the volumes are active during 95% of the whole trace periods in AliCloud, TencentCloud, and MSRC, respectively. This indicates that most of the volumes in all three traces have high activeness throughout the whole trace periods, and AliCloud and TencentCloud have higher activeness in general than MSRC. Also, in terms of the active time in each volume, the activeness is the highest in the TencentCloud volumes. Finding B.6: Writes are the dominant factor in determining the activeness in AliCloud, TencentCloud, and MSRC.</p><p>Referring to both Figures <ref type="figure" target="#fig_1">12</ref> and<ref type="figure" target="#fig_2">13</ref>, the curves of "Active" and "Write-active" nearly overlap with each other in all three traces. It suggests that the activeness of all three traces (in terms of the number of active volumes and the active time period of a volume) is mainly determined by the presence of writes. Thus, load balancing on writes is important for the volumes in cloud block storage. Finding B.7: Removing write requests shows drastic decreases in activeness in AliCloud, TencentCloud, and MSRC. AliCloud is less read-active than MSRC, and TencentCloud is the most read-active among all three traces.</p><p>If we remove write requests and consider only the read-active volumes, Figure <ref type="figure" target="#fig_1">12</ref> shows that the Figure <ref type="figure" target="#fig_2">13</ref> further shows that the volumes in all three traces have low read-active time, which is consistent with results of prior work <ref type="bibr" target="#b32">[33]</ref>. In AliCloud, TencentCloud, and MSRC, half of the volumes have less than 2.83 days, 5.92 days, and 2.61 days of read-active time after removing writes, corresponding to 9.1%, 65.4%, and 37.3% of their whole trace durations, respectively (Figures <ref type="figure" target="#fig_2">13(a</ref>)-13(c)). In terms of the percentage of volumes that have long read-active time, we find that 7.6%, 38.4%, 16.7% of the volumes can reach more than 30 days, 8 days, and 6 days of read-active time in AliCloud, TencentCloud, and MSRC, respectively (recall that their trace durations are 31, 9.04, and 7 days, respectively). This suggests that AliCloud is less read-active than MSRC, while TencentCloud is the most read-active among all three traces. We calculate the average number of requests and the average amount of traffic in 10-minute intervals each day from 12:00 AM to 11:59 PM. We divide one day into 144 10-minute timeslots. We collect the total number of requests and the total traffic size in each timeslot, and divide them by the number of days to obtain the averages for each timeslot. We align the timestamps of all three traces to their respective time zones: for AliCloud and TencentCloud, we align the timestamps of the requests to GMT+8, while for MSRC, we align them to GMT+0. Recall that the TencentCloud traces end at 1:00 AM on the tenth day and have a missing hour of requests at 1:00 AM-2:00 AM ( ?3.1). The average numbers of requests per day in the timeslots of 12:00 AM-1:00 AM and 1:00 AM-2:00 AM are obtained by the total number of requests in the timeslots divided by ten and eight, respectively; the average amounts of traffic are handled similarly.</p><p>Figures <ref type="figure" target="#fig_3">14</ref> and<ref type="figure" target="#fig_4">15</ref> show the distributions of the average numbers of requests and the average amounts of traffic across different timeslots for all three traces, respectively. We see that in both AliCloud and TencentCloud, their average numbers of requests and sizes of traffic are almost evenly spread across daytime (6:00 AM to 6:00 PM) and nighttime (6:00 PM to 6:00 AM). In AliCloud, the daytime has 52.6% of the total average number of I/O requests (Figure <ref type="figure" target="#fig_3">14</ref> We also find that from Figures <ref type="figure" target="#fig_4">15(a</ref>) and 15(c), AliCloud and MSRC volumes have spikes for reads at 0:00 AM-0:20 AM and 1:00 AM-2:30 AM, respectively, accounting for 13.1% and 18.8% of all read traffic, respectively; in contrast, TencentCloud does not have such spikes. For AliCloud, the reason of the spikes is that 12 out of the 1,000 volumes only have read requests during 0:00 AM-0:20 AM, and they contain significant numbers of large read requests. Each of these 12 volumes has a total of more than 50.2 GiB of average read traffic at 0:00 AM-0:20 AM per day. Their average read request sizes are larger than 360 KiB (note that most of the read requests are smaller than 100 KiB; see Finding A.2 in ?3.2). We suspect that there exist scheduled scan activities in these 12 volumes at midnight, although we cannot identify their specific applications ( ?3.1). If we exclude these 12 volumes, the overall intensities of read traffic for AliCloud at midnight will be comparable to other time intervals. For MSRC, the reason of the spikes near midnight is that a volume called src1_1 has an extremely large amount of read traffic at 1:00 AM-2:30 AM, accounting for 56.8-82.9% of the read traffic of all volumes in MSRC in the corresponding 10-minute time intervals. Note that the average read request sizes of src1_1 are smaller than 43.1 KiB during the spike period, as opposed to the large requests in AliCloud. Read spikes near midnight are common in production; for example, in prior work <ref type="bibr" target="#b18">[19]</ref>, massive read spikes are observed at about 3:30 AM in a production server due to the scheduled replication tasks in early mornings. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Spatial Patterns</head><p>We study the spatial characteristics of volumes in AliCloud, TencentCloud, and MSRC through the following metrics. First, we study the randomness of I/O requests by examining the offset differences of recent requests <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b38">39]</ref>, as random I/Os can compromise the performance and endurance of flash-based storage <ref type="bibr" target="#b30">[31]</ref>. Second, we examine the aggregations of reads and writes in working sets, so as to provide guidelines for resource allocation in caching <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b36">37]</ref>. Finally, we examine the patterns of update coverage (i.e., the percentage of WSS for updates), which is important for optimizing update performance in storage cluster management <ref type="bibr" target="#b11">[12]</ref>. Finding B.9: Random I/Os are common in AliCloud, TencentCloud, and MSRC. The volumes in AliCloud and TencentCloud see higher percentages of random I/Os than those in MSRC.</p><p>We study the randomness of I/O requests by examining the spatial relationships among adjacent requests. To quantify the randomness of a request, we measure the minimum distance between the current offset of the request and the offsets of the previous 32 requests <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b38">39]</ref>. If the minimum distance exceeds a threshold (e.g., 128 KiB, which is the read-ahead length of the surveyed drives in <ref type="bibr" target="#b38">[39]</ref>), we regard the request as a random request. We measure the randomness ratio of a volume, defined as the percentage of random requests over all requests.</p><p>Figure <ref type="figure" target="#fig_15">16</ref>(a) shows the cumulative distributions of randomness ratios of volumes in AliCloud, Tencent-Cloud, and MSRC. We find that random I/Os are common in all three traces. Half of the volumes have at least 33.5%, 42.1%, and 29.4% of random I/Os in AliCloud, TencentCloud, and MSRC, respectively. Also, AliCloud and TencentCloud in general show higher randomness ratios than MSRC. In particular, all volumes in MSRC have less than 46% of random requests, while 20.4% and 35.8% of volumes in AliCloud and TencentCloud have more than 50% of random requests, respectively. The existence of randomness may come from file system workloads as witnessed in <ref type="bibr" target="#b16">[17]</ref>.</p><p>We further examine the randomness ratios of the top-10 volumes that have the most I/O traffic in each trace. Figure <ref type="figure" target="#fig_15">16(b)</ref> shows the relationships between the randomness ratios and the total I/O traffic sizes of the top-10 volumes. We see that the volumes with large amounts of I/O traffic have high randomness ratios in general. The randomness ratios of the top-10 volumes in AliCloud, TencentCloud, and MSRC are 14.0-83.4%, 54.0-91.7%, and 11.4-40.8%, respectively, and their I/O traffic sizes are 20.0-52.8 TiB, 11.7-36.8 TiB, and 0.17-2.26 TiB, respectively. We also examine the Spearman correlation coefficients <ref type="bibr" target="#b37">[38]</ref> between the I/O traffic sizes and the randomness ratios in the top-10 volumes of all three traces. We find that the coefficients are 0.079, 0.164, and 0.333 in AliCloud, TencentCloud, and MSRC, respectively, implying that there exist positive correlations between the two metrics in the top-10 volumes. The results indicate that random I/Os are common in traffic-intensive volumes.</p><p>Combining with the observation that small-size I/O requests dominate in all three traces ( ?3.2), we see that random and small I/Os are common in all three traces, especially in AliCloud and TencentCloud. Such access patterns can compromise the performance and endurance of flash-based storage <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b30">31]</ref>. Finding B.10: Reads and writes are aggregated in small working sets in non-negligible fractions of volumes in AliCloud, TencentCloud, and MSRC, while TencentCloud has the highest aggregation of reads among all three traces. Writes are more aggregated than reads.</p><p>We study how reads and writes are aggregated in the working sets of each volume. Specifically, in the read (or write) working sets, we focus on the top-1% and top-10% of unique blocks that receive the most read (or write) traffic. We examine the percentage of the read (or write) traffic size of each such block over the total read (or write) traffic size; a higher percentage implies that the I/O traffic is more aggregated in a block.</p><p>Figure <ref type="figure" target="#fig_16">17</ref> shows the boxplots of percentages of traffic sizes for the top-1% and top-10% blocks across all volumes in AliCloud, TencentCloud, and MSRC. We first focus on read traffic. We see that read traffic is aggregated in the top-1% and top-10% blocks in non-negligible fractions of volumes. In AliCloud, 75% of volumes have at least 2.5% and 13.6% of read traffic in the top-1% and top-10% read blocks, respectively (Figure <ref type="figure" target="#fig_16">17(a)</ref>). In TencentCloud, the corresponding percentages are 7.2% and 28.8%, respectively (Figure <ref type="figure" target="#fig_16">17(b)</ref>), and in MSRC, the corresponding percentages are 3.1% and 19.6%, respectively (Figure <ref type="figure" target="#fig_16">17(c)</ref>). In particular, the aggregation for reads is the highest in TencentCloud among all three traces.</p><p>In AliCloud, the boxplots indicate 147 volumes as outliers in the top-1% read blocks (Figure <ref type="figure" target="#fig_16">17</ref>(a)). Such outlier volumes have more than 21.3% of read traffic in their top-1% read blocks. It implies that a small read cache can absorb a substantial amount of read traffic for such volumes.</p><p>Compared with reads, writes are more aggregated. In AliCloud, the 25th percentiles of read traffic in the top-1% and top-10% read blocks are 2.5% and 13.6%, respectively, while the 25th percentiles of write traffic in the top-1% and top-10% written blocks increase to 13.0% and 31.2%, respectively (Figure <ref type="figure" target="#fig_16">17(a)</ref>). Similar observations hold in TencentCloud and MSRC. For example, in TencentCloud, the 25th percentiles of read and write traffic in the top-10% read and written blocks are 28.8% and 53.5%, respectively (Figure <ref type="figure" target="#fig_16">17(b)</ref>), and in MSRC, the corresponding 25th percentiles are 19.6% and 44.0%, respectively (Figure <ref type="figure" target="#fig_16">17(c)</ref>). Note that the spatial clustering of writes is common in desktop applications and is related to files such as mail boxes, search indexes, and file system metadata <ref type="bibr" target="#b36">[37]</ref>. Finding B.11: Reads and writes tend to be aggregated in read-mostly and write-mostly blocks, respectively, in AliCloud and TencentCloud.</p><p>We further classify the blocks into different types as in <ref type="bibr" target="#b22">[23]</ref> and examine the aggregation of reads and writes. Specifically, we classify a block as read-mostly (or write-mostly) if its read (or write) traffic occupies more than 95% of its total I/O traffic. We examine the percentage of all read (or write) traffic in the whole trace duration that goes to read-mostly (or write-mostly) blocks.</p><p>Table <ref type="table" target="#tab_6">5</ref> shows the overall percentages of all read and write traffic that goes to read-mostly and writemostly blocks in AliCloud, TencentCloud, and MSRC. In AliCloud and TencentCloud, the majority of read traffic (59.1% and 78.5%, respectively) and write traffic (80.7% and 90.8%, respectively) goes to read-mostly blocks and write-mostly blocks, respectively. In MSRC, 72.1% of read traffic goes to read-mostly blocks;  however, only 32.4% of write traffic goes to write-mostly blocks. Overall, both AliCloud and TencentCloud show prominent aggregations of reads and writes in read-mostly and write-mostly blocks, respectively, but it is not the case in MSRC. Note that the limited aggregation of writes in write-mostly blocks in MSRC is inconsistent with the prior finding in <ref type="bibr" target="#b22">[23]</ref>. The reason is that the study in <ref type="bibr" target="#b22">[23]</ref> considers only 12 out of 36 volumes in MSRC, while we consider all 36 volumes. Figure <ref type="figure" target="#fig_17">18</ref> shows the cumulative distributions of percentages of read and write traffic that goes to readmostly and write-mostly blocks, respectively, across all volumes in AliCloud, TencentCloud, and MSRC. Most of the volumes in all three traces have high percentages of all read and write traffic aggregated in read-mostly and write-mostly blocks, respectively. In AliCloud, half of the volumes have more than 82.6% of reads going to read-mostly blocks and more than 99.2% writes going to write-mostly blocks (Figure <ref type="figure" target="#fig_17">18(a)</ref>); in TencentCloud, the corresponding percentages are 73.4% and 98.0%, respectively (Figure <ref type="figure" target="#fig_17">18(b)</ref>); in MSRC, the corresponding percentages are 89.4% and 78.3%, respectively (Figure <ref type="figure" target="#fig_17">18(c)</ref>). Finding B.12: AliCloud and TencentCloud generally have higher update coverages and higher percentages of update traffic than MSRC. The update coverage also varies across volumes.</p><p>Recall that Table <ref type="table" target="#tab_0">1</ref> (Section 3.2) shows the overall WSSs (working set sizes) for reads, writes, and updates. We now examine the spatial characteristics of updates. We focus on the update working set, which covers the blocks that are written more than once. We measure the update coverage of a volume, defined as the ratio between the update WSS and the total WSS of the volume <ref type="bibr" target="#b11">[12]</ref>. In addition, we measure the percentages of update traffic over the total amount of traffic across all volumes.</p><p>Table <ref type="table" target="#tab_7">6</ref> shows the averages, medians, and 90th percentiles of update coverages of all volumes in all three traces. In general, AliCloud and TencentCloud have higher update coverages than MSRC. In AliCloud and TencentCloud, half of the volumes have update coverages of more than 61.2% and 56.7%, respectively, while in MSRC, the corresponding percentage is 9.4% only. In addition, Table <ref type="table">7</ref> shows that AliCloud and TencentCloud have higher percentages of update traffic than MSRC in terms of the averages, medians, and 90th percentiles. This suggests that AliCloud and TencentCloud are more update-intensive than MSRC.</p><p>Figure <ref type="figure" target="#fig_18">19</ref>(a) shows the cumulative distributions of update coverages across all volumes in AliCloud, TencentCloud, and MSRC. In AliCloud and TencentCloud, 45.2% and 37.2% of volumes have update coverages larger than 65%, respectively, and their update coverages are more diverse than MSRC. On the other hand, in MSRC, 33 out of 36 volumes have update coverages below 65%. Figure <ref type="figure" target="#fig_18">19(b)</ref> shows the cumulative distributions of percentages of update traffic. AliCloud and TencentCloud show higher percentages  of update traffic than MSRC, and TencentCloud generally has a higher percentage than AliCloud.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Temporal Patterns</head><p>We study the temporal characteristics of volumes in AliCloud, TencentCloud, and MSRC by examining the temporal relationships of adjacent I/O requests. We first examine the time elapsed between adjacent requests to the same block with respect to different combinations of read and write requests for workload-aware caching designs <ref type="bibr" target="#b36">[37]</ref>. We also study the update interval (i.e., the time interval between two consecutive writes to the same block), which facilitates flash-based storage management <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b23">24]</ref>. Finally, we study the miss ratios under least recently used (LRU) caching, which reflects the temporal aggregation of traffic for caching efficiency <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b46">47]</ref>. Recall that the TencentCloud traces have a missing hour of requests at 1:00 AM-2:00 AM on the eighth day ( ?3.1). Thus, in our following analysis for TencentCloud, we discard the adjacent requests that span across the missing hour. Finding B.13: The read-after-write (RAW) times in AliCloud, TencentCloud, and MSRC are generally larger than the write-after-write (WAW) times. Also, TencentCloud generally has smaller RAW times than AliCloud and MSRC, while AliCloud generally has larger WAW times than TencentCloud and MSRC. Furthermore, AliCloud and TencentCloud have significantly more WAW requests than RAW requests.</p><p>We first examine two types of adjacent requests <ref type="bibr" target="#b36">[37]</ref>: (i) a read-after-write (RAW) request, which refers to the read following immediately the write to the same block; and (ii) a write-after-write (WAW) request, which refers to the write following immediately the write to the same block. We measure the time of a RAW (resp. WAW) request as the elapsed time between the adjacent read and write (resp. the two adjacent writes) to the same block.</p><p>Figures <ref type="figure" target="#fig_19">20(a</ref>)-20(c) show the cumulative distributions of RAW and WAW times across all RAW and WAW requests, respectively, in all three traces. All three traces generally have larger RAW times than WAW times. Specifically, the 50th percentiles of the RAW time in AliCloud, TencentCloud, and MSRC are 3.0 hours, 4.9 minutes, and 16.1 hours, respectively, while the 50th percentiles of the WAW time are only 1.3 hours, 0.7 minutes, and 1.0 minute, respectively. Such findings are consistent with those in prior studies <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b36">37]</ref>. As for the possible reasons, the smaller WAW times are likely to appear in desktop workloads <ref type="bibr" target="#b36">[37]</ref>, while the larger RAW times are possibly related to the large OS-level buffer caches <ref type="bibr" target="#b36">[37]</ref>. Also, all three traces have different percentages of large and small RAW and WAW times. To aid our analysis, we treat the times smaller than 1.0 minute and larger than 15.0 minutes as small and large, respectively, as also used in <ref type="bibr" target="#b36">[37]</ref>. For RAW times, TencentCloud has the largest percentage of RAW times smaller than 1.0 minute among all three traces, while most of the RAW times in AliCloud and MSRC are larger than 15.0 minutes. Specifically, 1.4%, 29.8%, and 10.9% of the RAW times are smaller than 1.0 minute in AliCloud, TencentCloud, and MSRC, respectively, while 87.8%, 42.5%, and 67.8% of the RAW times are larger than 15.0 minutes in the three traces, respectively. On the other hand, most of the WAW times are smaller than 1.0 minute in TencentCloud and MSRC, while most of the WAW times are larger than 15.0 minutes in AliCloud. Specifically, 22.7%, 57.7%, and 51.5% of WAW times are smaller than 1.0 minute in AliCloud, TencentCloud, and MSRC, respectively, while 58.5%, 16.4%, and 24.3% of WAW times are larger than 15.0 minutes in the three traces, respectively.</p><p>Table <ref type="table" target="#tab_8">8</ref> shows the numbers of RAW and WAW requests in AliCloud, TencentCloud, and MSRC. We observe a large difference in the numbers of RAW and WAW requests in both AliCloud and TencentCloud, but a small difference in MSRC. Specifically, in AliCloud, the numbers of RAW and WAW requests are 12.4 billion and 103.7 billion, respectively; the number of WAW requests is 8.3? that of RAW requests. In TencentCloud, the numbers of RAW and WAW requests are 8.8 billion and 204.9 billion, respectively, and the number of WAW requests is even 23.3? that of RAW requests. In MSRC, those numbers are 297.2 million and 289.2 million, respectively, and are close to each other. We suspect that the larger number of WAW requests in AliCloud and TencentCloud may be related to the metadata and log files in the workloads; for example, the fraction of WAW requests drops significantly after the metadata and log files are excluded <ref type="bibr" target="#b17">[18]</ref>. Finding B.14: TencentCloud has the highest fractions of small RAR and WAR times and the smallest fractions of large RAR and WAR times in all three traces. There exist extremely small RAR and WAR times in MSRC. In all three traces, the WAR time is much larger than the RAR time, and there are much more RAR requests than WAR requests.</p><p>We further examine two types of adjacent requests: (i) a read-after-read (RAR) request, which refers to the read following immediately the read to the same block; and (ii) a write-after-read (WAR) request, which refers to the write following immediately the read to the same block.</p><p>Figures <ref type="figure" target="#fig_19">20(d</ref>)-20(f) show the cumulative distributions RAR and WAR times across all RAR and WAR requests, respectively, in all three traces. We again treat the times smaller than 1.0 minute and larger than 15.0 minutes as small and large, respectively, as above. TencentCloud has the highest fractions of RAR and WAR times smaller than 1.0 minute, and the lowest fractions of RAR and WAR times larger than 15.0 minutes. Specifically, 28.5%, 53.4%, and 35.6% of the RAR times are smaller than 1.0 minute in AliCloud, TencentCloud, and MSRC, respectively, while 30.0%, 21.2%, and 35.2% of the RAR times are larger than 15.0 minutes, respectively. On the other hand, 2.8%, 47.6%, and 29.2% of the WAR times are smaller than 1 minute in AliCloud, TencentCloud, and MSRC, respectively, while 93.8%, 31.1%, and 69.7% of the WAR times are larger than 15 minutes, respectively. In particular, in MSRC, there exist non-negligible fractions of extremely small RAR and WAR times (18.5% and 25.4%, respectively) that are smaller than 1 second, which is not the case for AliCloud and TencentCloud.</p><p>Overall, in all three traces, the WAR time is much larger than the RAR time. In AliCloud, the 50th percentiles of RAR and WAR times are 2.0 minutes and 18.2 hours, respectively, and 21.0% and 88.8% of RAR and WAR times are larger than 1 hour, respectively (Figure <ref type="figure" target="#fig_19">20(d)</ref>). In TencentCloud, the 50th percentiles of RAR and WAR time are 49 seconds and 78 seconds, respectively, and 11.1% and 25.2% of RAR and WAR times are larger than 1 hour, respectively (Figure <ref type="figure" target="#fig_19">20</ref>(e)). In MSRC, the 50th percentiles of RAR and WAR times are 5.2 minutes and 5.4 hours, respectively, and 33.6% and 66.7% of RAR and WAR times are larger than 1 hour, respectively (Figure <ref type="figure" target="#fig_19">20</ref>(f)). The results indicate that a block being read is likely read again soon.</p><p>We also examine the numbers of RAR and WAR requests in AliCloud, TencentCloud, and MSRC, as shown in Table <ref type="table" target="#tab_8">8</ref>. In AliCloud, TencentCloud, and MSRC, the numbers of RAR requests are 2.54?, 8.07?, and 4.19? those of WAR requests, respectively. Finding B.15: Written blocks have varying update intervals.</p><p>We measure the update interval of a block, defined as the elapsed time between two consecutive writes to the same block. Note that the update interval differs from the WAW time, as the former allows reads between two writes. Each block may be written more than once, so it may be associated with multiple update intervals (e.g., a block that is written M times has M -1 update intervals). The update interval of a block describes the lifetime of the block data.</p><p>Table <ref type="table" target="#tab_9">9</ref> shows different percentiles of update intervals across all volumes in all three traces. In AliCloud, the update intervals generally have long durations, while in TencentCloud and MSRC, the update intervals are generally small, especially in TencentCloud. In AliCloud, 50% of update intervals are larger than 95.2 minutes (1.6 hours), and the 90th percentile is 3,017.4 minutes (50.3 hours). In TencentCloud, the 25th, 50th, and 75th percentiles are only several seconds or minutes (0.23 minutes, 0.67 minutes, and 5.4 minutes, respectively), implying that the majority of updated blocks have extremely high update frequencies. However, some updated blocks still have high update intervals, as the 90th and 95th percentiles are 120.0 minutes (2.0 hours) and 973.1 minutes (16.2 hours), respectively. In MSRC, the update intervals have a bimodal pattern, in which 50% of update intervals are smaller than 1.25 minutes, while 25% of update intervals are larger than 1,438.9 minutes (24.0 hours). The reason of such a bimodal pattern in MSRC is that a volume is responsible for source control (i.e., src1_0) and updates data blocks daily. If we exclude the daily updates,  most of the written blocks in MSRC have very short update intervals.</p><p>Figure <ref type="figure" target="#fig_1">21</ref> shows the boxplots of update intervals of different groups of percentiles across all volumes in AliCloud, TencentCloud, and MSRC. We see that the distributions of update intervals have high variations across volumes in all three traces. For example, in AliCloud, the 50th percentiles of update intervals of all volumes range from 1 second to 17.8 days (Figure <ref type="figure" target="#fig_1">21(a)</ref>); in TencentCloud, the 50th percentiles vary between 1 second and 4.14 days (Figure <ref type="figure" target="#fig_1">21(b)</ref>); in MSRC, the 50th percentiles of update intervals of all volumes range from 1 second to 24 hours.</p><p>Many volumes have non-negligible proportions of short update intervals in their update requests. To further examine the distributions of update intervals in individual volumes, we divide the update intervals into four groups: (i) less than 5 minutes, (ii) 5-30 minutes, (iii) 30-240 minutes, and (iv) more than 240 minutes. We calculate the proportions for the four groups of update intervals for each volume, and represent the proportions across all volumes by boxplots.</p><p>Figure <ref type="figure" target="#fig_1">22</ref> shows the boxplots of proportions for the four groups of update intervals across all volumes in all three traces. All three traces have large proportions of either very small or very large update intervals. In AliCloud, half of the volumes have more than 35.2% and 38.2% of update intervals in less than 5 minutes and in more than 240 minutes, respectively (Figure <ref type="figure" target="#fig_1">22(a)</ref>). In TencentCloud, the corresponding percentages are 50.8% and 17.0%, respectively (Figure <ref type="figure" target="#fig_1">22</ref>(b)), while in MSRC, the corresponding percentages are 47.2% and 18.9%, respectively (Figure <ref type="figure" target="#fig_1">22(c)</ref>). Thus, a substantial amount of data is either updated frequently or not updated for long. Finding B.16. Many volumes in TencentCloud have low miss ratios even under a small cache size, while there are fewer such volumes with low miss ratios in AliCloud and MSRC. Also, when the cache size increases, AliCloud and TencentCloud show the highest absolute reductions in read and write miss ratios, respectively, in all three traces. Finally, we study the impact of caching with respect to the temporal patterns of the volumes. For each volume, we simulate a fixed-size cache for both reads and writes using the LRU policy, and evaluate the corresponding cache miss ratios for reads and writes. Here, we select 1% and 10% of the WSS of a volume as the cache size.</p><p>Figure <ref type="figure" target="#fig_21">23</ref> shows the boxplots of miss ratios across all volumes in all three traces. Some volumes show low miss ratios (i.e., LRU-based caching is effective). For the cache size of 10% of WSS, the 25th percentiles of the miss ratios for reads and writes are 59.4% and 30.7% in AliCloud, respectively (Figure <ref type="figure" target="#fig_21">23(a)</ref>), while the corresponding miss ratios are 17.0% and 17.9% in TencentCloud, respectively (Figure <ref type="figure" target="#fig_21">23</ref>(b)), and 64.1% and 32.0% in MSRC, respectively (Figure <ref type="figure" target="#fig_21">23(c)</ref>). Also, some volumes in TencentCloud can have very low miss ratios when the cache size is only 1% of WSS, implying that the access patterns of such volumes have high temporal locality, while AliCloud and MSRC have fewer such volumes. The low miss ratios in TencentCloud suggest that we can potentially improve the read and write performance using a small-size cache. Such findings are also consistent with those in <ref type="bibr" target="#b51">[52]</ref>, which shows low miss ratios of some selected volumes with small-size cache.</p><p>AliCloud has the highest absolute reductions in read miss ratios when the cache size increases from 1% to 10% of WSS among all three traces; for write miss ratios, TencentCloud shows the highest absolute reductions for increased cache sizes. In AliCloud, the 25th percentiles of the miss ratios for reads and writes reduce from 96.1% to 59.4% and from 52.8% to 30.7% (i.e., 36.7% and 22.1% of absolute reduction), respectively (Figure <ref type="figure" target="#fig_21">23(a)</ref>). In TencentCloud, the 25th percentiles of the miss ratios for reads and writes reduce from 39.4% to 17.0% and from 49.3% to 17.9% (i.e., 22.4% and 31.4% of absolute reduction, Figure <ref type="figure" target="#fig_21">23(b)</ref>), while in MSRC, the 25th percentiles of the miss ratios for reads and writes reduce from 86.9% to 64.1% and from 46.2% to 32.1% (i.e., 22.8% and 14.1% of absolute reduction), respectively (Figure <ref type="figure" target="#fig_21">23(c)</ref>). In short, in all three traces, AliCloud and TencentCloud have the highest reductions in read and write miss ratios, respectively, implying the significance of enlarging the cache size in cloud block storage workloads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Similarities and Differences Between Two Cloud Block Storage Traces</head><p>We highlight the major similarities and differences between the two cloud block storage traces, AliCloud and TencentCloud. Load intensities. AliCloud and TencentCloud show similar intensities of volumes (Finding B.1), but different observations in burstiness and activeness.</p><p>? In terms of burstiness, TencentCloud has lower overall burstiness than AliCloud, and also has a lower fraction of volumes with high burstiness ratios (Finding B.2). Nevertheless, both AliCloud and TencentCloud have more diverse burstiness across volumes than MSRC (Finding B.3). ? In terms of activeness, TencentCloud has higher activeness than AliCloud, in both the number of active volumes and the active time in each volume (Finding B.5). While writes are the dominant factor in the activeness of both cloud block storage traces (Finding B.6), TencentCloud is more read-active than AliCloud (Finding B.7). ? In terms of the distribution of traffic per day, unlike MSRC, the traffic of both AliCloud and TencentCloud is almost evenly spread across daytime and nighttime. While AliCloud shows substantial read traffic at midnight, TencentCloud does not (Finding B.8). Spatial patterns. While AliCloud and TencentCloud show higher randomness than MSRC, TencentCloud generally has higher randomness in I/O requests and higher levels of traffic aggregation in blocks than AliCloud.</p><p>? In terms of the randomness in I/Os, the volumes in both AliCloud and TencentCloud have higher percentages of random I/Os than those in MSRC. Compared with AliCloud, TencentCloud generally shows a higher percentage of random I/Os. For example, TencentCloud has a higher fraction of random requests in half of the volumes than AliCloud (42.1% versus 33.5%) (Finding B.9). ? In terms of the aggregation of traffic, TencentCloud shows more traffic in top-1% and top-10% of the most access-intensive blocks than AliCloud, indicating that TencentCloud has a higher degree of traffic aggregation in a small set of blocks (Finding B.10). Also, while both AliCloud and TencentCloud have higher percentages of writes in write-mostly blocks than MSRC, TencentCloud has higher read and write traffic aggregations than AliCloud in read-mostly and write-mostly blocks, respectively (Finding B.11). Furthermore, while both AliCloud and TencentCloud have higher fractions of update-intensive volumes (i.e., the volumes with high percentages of update traffic) than MSRC, TencentCloud has a higher percentage of update-intensive volumes than AliCloud (Finding B.12). Temporal patterns. TencentCloud generally has lower access intervals on blocks in reads, writes, and updates. It also has lower miss ratios than AliCloud under the same cache configurations. ? In terms of the time intervals in accessing the same blocks, both AliCloud and TencentCloud generally have smaller WAW times than RAW times and smaller RAR times than WAR times. However, TencentCloud generally has lower RAW, WAW, RAR, and WAR times than AliCloud (Findings B. <ref type="bibr">13 and B.14)</ref>. It indicates that TencentCloud has more access-intensive workloads than AliCloud. ? In terms of the update intervals, TencentCloud has 90% of its update intervals smaller than 2.0 hours, and its update intervals are generally smaller than AliCloud (Finding B.15). ? In terms of miss ratios, TencentCloud has lower miss ratios under the same cache configurations. It has lower absolute reductions in read miss ratios than AliCloud when the cache size increases from 1% to 10% of WSS. On the contrary, it has higher absolute reductions in write miss ratios than AliCloud when the cache size increases from 1% to 10% of WSS (Finding B.16).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Summary of Findings</head><p>Finally, we discuss the implications of our findings of the trace analysis in AliCloud, TencentCloud, and MSRC. We show how the findings address the design considerations for cloud block storage, including load balancing, cache efficiency, and storage cluster management ( ?2.2).</p><p>Load balancing. We focus on the average and peak intensities as well as the activeness of volumes. From Finding B.1, we observe that while many applications are hosted in the cloud, the volumes in cloud block storage (i.e., AliCloud and TencentCloud) have similar average load intensities to those in traditional data centers (i.e., MSRC) which were monitored more than a decade ago; however, the peak intensities are generally smaller. From Findings B.2-B.4, we observe the existence of burstiness in a non-negligible fraction of volumes. While the overall burstiness remains low, the burstiness can be severe in individual volumes across many types of applications <ref type="bibr" target="#b33">[34]</ref>, indicating that these volumes may be provisioned for high peak intensities but most of the bandwidth resources remain unused <ref type="bibr" target="#b32">[33]</ref>. The high burstiness may hence lead to performance degradations if load balancing is not properly maintained. Furthermore, the higher diversity of workload burstiness makes load balancing in cloud block storage more challenging than in traditional data centers. Failing to deal with load imbalance and the diversity of workloads may cause problems to the physical devices in cloud block storage, such as higher flash failure rates <ref type="bibr" target="#b47">[48]</ref>. Applying shared logs or distributed caches can ease the load imbalance among volumes <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b47">48]</ref>. Also, the burstiness, as shown by the short inter-arrival times, suggests that I/O requests tend to arrive in groups and can be further exploited to improve I/O performance <ref type="bibr" target="#b17">[18]</ref>.</p><p>From Findings B.5-B.7, we observe that writes are the dominant factor of activeness in all three traces. In particular, most volumes in cloud block storage (i.e., AliCloud and TencentCloud) are write-dominant ( ?3.2). The differences of activeness in reads and writes indicate that removing writes can produce a high level of idle periods, so it is possible to offload writes (e.g., by redirecting writes to other storage locations) to create idle periods in cloud block storage workloads for power savings <ref type="bibr" target="#b32">[33]</ref>.</p><p>From Finding B.8, I/O traffic is evenly spread across both daytime and nighttime in AliCloud and TencentCloud. This challenges background task scheduling (e.g., garbage collection, defragmentation, and flushing caches <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b33">34]</ref>) in cloud block storage. For example, performing background tasks only at nighttime may be ineffective to reduce the interference with foreground I/Os. A careful design of I/O scheduling for background and foreground activities becomes necessary for cloud block storage. In particular, we observe large read spikes near midnight on a daily basis in some of the volumes in AliCloud. How to prevent such I/O spikes from interfering with cloud block storage system as a whole needs careful attention.</p><p>Concerning the design of load balancing, the data placement strategies should be aware of the diversity of workloads, the burstiness of individual volumes, and the traffic distribution over time. The log-structured design <ref type="bibr" target="#b34">[35]</ref> is proven useful for balancing the write traffic in cloud-scale flash-based storage <ref type="bibr" target="#b47">[48]</ref>. Cache efficiency. We study the spatial and temporal characteristics of volumes, which provide guidelines for motivating new caching designs for cloud block storage.</p><p>From Findings B.10 and B.16, we observe the patterns of both spatial and temporal traffic aggregations in a small fraction of blocks, especially for writes. TencentCloud shows a stronger traffic aggregation compared with AliCloud and MSRC. Many volumes in cloud block storage show high aggregations of reads and writes, implying that it is viable to allocate limited cache resources for absorbing substantial amounts of reads and writes.</p><p>From Finding B.11, we observe that many volumes in cloud block storage have reads and writes aggregated in read-mostly and write-mostly blocks, respectively. Thus, one possible caching admission policy is to identify the read-mostly and write-mostly blocks in workloads, as such blocks can absorb a substantial amount of I/O traffic. Also, the read-mostly and write-mostly blocks can be put into different devices to separately reduce the read and write latencies <ref type="bibr" target="#b22">[23]</ref>.</p><p>From Findings B.13 and B.14, the blocks that have been written tend to be rewritten again. In contrast, the blocks that have been read tend to receive another write after a long period of time. Thus, if our goal is to absorb writes with caching, a possible strategy is to favor the caching of the blocks that have been written rather than those that have been read, as the latter may unlikely generate write hits. Also, cloud block storage can benefit from disk-based write caching <ref type="bibr" target="#b36">[37]</ref>, due to the limited reads from the disk-based cache. Storage cluster management. Characterizing the spatial and temporal characteristics of volumes is also critical for storage cluster management. Here, we focus on flash-based storage ( ?2.1).</p><p>From Finding B.9, we observe that upper-layer applications in cloud block storage issue a high fraction of small and random I/Os, which are known to hurt both the performance and endurance of flash-based storage <ref type="bibr" target="#b30">[31]</ref>. The log-structured storage design <ref type="bibr" target="#b34">[35]</ref> and I/O clustering <ref type="bibr" target="#b30">[31]</ref> can help mitigate the overhead of small and random I/Os.</p><p>From Findings B.12 and B.15, updates are common and have high variations across volumes, both spatially and temporally. The varying update coverage across different volumes requires the underlying caches to use adaptive caching methods to absorb update traffic. Also, the varying update patterns can harm the effectiveness of garbage collection and wear leveling in flash <ref type="bibr" target="#b16">[17]</ref>. Thus, cloud block storage systems should take into account the varying patterns when optimizing update workloads for flash-based storage. A possible direction is to maintain the flash-translation layer (FTL) at the system level <ref type="bibr" target="#b12">[13]</ref> to flexibly coordinate the I/Os issued to flash.</p><p>From Finding B.13, a larger number of WAW requests than RAW requests in AliCloud and TencentCloud indicates that the next issued requests to newly written blocks tend to be writes instead of reads. If these written blocks are replicated across different nodes, we may choose to update only one copy and invalidate other copies, instead of updating all copies, in order to save the update overhead since the written data is likely to be rewritten again <ref type="bibr" target="#b17">[18]</ref>. Unexpected results. Finally, we highlight some unexpected results reported in our findings.</p><p>From Finding B.6, we observe that the activeness of MSRC is dominated by writes, yet the MSRC is read-dominant ( ?3.2). The results indicate that MSRC is write-dominant from the perspective of activeness, but is read-dominant from the perspective of the amount of I/O traffic.</p><p>From Finding B.8, we observe read spikes near midnight in both AliCloud and MSRC traces. In the corresponding read requests of the spike period, the average read request size in AliCloud is larger than 360 KiB, while that in MSRC is smaller than 43.1 KiB. We suspect that there exist scheduled scan activities in the corresponding volumes of AliCloud.</p><p>From Finding B.11, the limited aggregation of writes in write-mostly blocks in MSRC is inconsistent with prior work <ref type="bibr" target="#b22">[23]</ref>, which emphasizes that most of the write requests access write-mostly blocks. The reason is that the previous study <ref type="bibr" target="#b22">[23]</ref> considers 12 volumes, while we consider all 36 volumes instead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>We review related work on the field studies on storage workloads and how they inspire storage system designs. Characterization of storage workloads. Several field studies characterize storage workloads using blocklevel I/O traces in various architectures, such as consumer electronics <ref type="bibr" target="#b33">[34]</ref>, virtual machines <ref type="bibr" target="#b0">[1]</ref>, Windows servers <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b32">33]</ref>, smartphone applications <ref type="bibr" target="#b53">[54]</ref>, containerized applications <ref type="bibr" target="#b15">[16]</ref>, and virtual desktop infrastructures <ref type="bibr" target="#b19">[20]</ref>. Yadgar et al. <ref type="bibr" target="#b48">[49]</ref> perform I/O workload analysis and study the performance implications (e.g., read/write amplifications and flash read costs) for SSD-based storage. In contrast, our field study focuses on cloud block storage that supports a diverse set of cloud applications in large-scale production. In particular, we provide findings and insights on performance optimizations for load balancing, caching efficiency, and storage cluster management.</p><p>Table <ref type="table" target="#tab_10">10</ref> summarizes the traces used in the existing block-level trace studies <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b53">54]</ref> in the literature, in terms of the number of traces (or volumes in our case), the trace durations, the number of read and write requests, and the total data traffic of reads and writes. Our trace analysis has the largest scale compared with the existing block-level trace studies at the time of the writing. Ahmad et al. <ref type="bibr" target="#b0">[1]</ref> do not show the overall statistics but mention that the total I/O size is at most 10 GiB. Harter et al. <ref type="bibr" target="#b15">[16]</ref> only focus on reads, and their analysis comprises of 57 docker images, each of which has the average read traffic of 27 MiB. Zhang et al. <ref type="bibr" target="#b51">[52]</ref> collected the TencentCloud traces, but they mainly focus on the cache allocation design based on trace-driven evaluation instead of providing detailed trace analysis. Inspirations from load intensity. Some designs are inspired by the characteristics of load intensity in storage workloads. Narayanan et al. <ref type="bibr" target="#b32">[33]</ref> offload writes to reduce power consumptions with the observation that some volumes are idle in reads, thereby removing writes in those volumes can increase the idle periods for power saving. SRCMap <ref type="bibr" target="#b40">[41]</ref> reduces power consumptions using sampling and replication, based on the observation on the I/O size and intensity of active data sets. Ursa <ref type="bibr" target="#b20">[21]</ref> adopts the log-structured design, based on the observation that small writes dominate in real-world workloads. Inspirations from spatial patterns. Some designs exploit the spatial characteristics of storage workloads. BORG <ref type="bibr" target="#b8">[9]</ref> organizes frequently written data in a small dedicated disk partition to reduce the I/O seek time.  <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b53">54]</ref>.</p><p>FlashTier <ref type="bibr" target="#b35">[36]</ref> manages sparse address mappings in flash caching, as storage I/Os are often aggregated in a small number of blocks. Desnoyers <ref type="bibr" target="#b13">[14]</ref> proposes an analytical model for cleaning algorithms in flash devices and analyzes the aggregation of written blocks in specific working sets. ACGR <ref type="bibr" target="#b22">[23]</ref> regulates I/O accesses for flash storage, based on the observation of read and write aggregations in read-only and write-only blocks, respectively. To improve the update performance in erasure-coded storage, CodFS <ref type="bibr" target="#b11">[12]</ref> proposes dynamic reserved space management for parity updates to address the varying working sets of updates across storage workloads, while PBS <ref type="bibr" target="#b52">[53]</ref> exploits the large fractions of overwrites to mitigate parity update overhead.</p><p>Inspirations from temporal patterns. Some designs exploit the temporal characteristics of storage workloads. Griffin <ref type="bibr" target="#b36">[37]</ref> leverages the large time intervals between writes and the subsequent reads to the same block to build an HDD-based write cache for improving the SSD lifetime. Arteaga et al. <ref type="bibr" target="#b5">[6]</ref> propose a cache-optimized RAID technique to minimize the RAID overhead in cloud storage, based on the comparisons between write-back caching and write-through caching on a set of block I/O traces from production servers in the cloud. CloudCache <ref type="bibr" target="#b4">[5]</ref> chooses the window size of the model based on the hit ratio analysis on two-week traces in the cloud. Some studies leverage the characteristics of update intervals in storage workloads for improving write performance <ref type="bibr" target="#b23">[24]</ref>, lifetime <ref type="bibr" target="#b9">[10]</ref>, garbage collection modeling, and data reduction <ref type="bibr" target="#b49">[50]</ref> in SSDs. Counter Stacks <ref type="bibr" target="#b46">[47]</ref>, SHARDS <ref type="bibr" target="#b42">[43]</ref>, and OSCA <ref type="bibr" target="#b51">[52]</ref> consider the reuse distance (i.e., the number of distinct items accessed between two accesses to the same item) to improve caching efficiency.</p><p>Cloud block storage systems. Several cloud block storage designs are proposed in the literature. Parallax <ref type="bibr" target="#b27">[28]</ref> provides storage virtualization for virtual machines atop shared block storage. Blizzard <ref type="bibr" target="#b28">[29]</ref> manages POSIX applications atop cloud block storage. Ursa <ref type="bibr" target="#b20">[21]</ref> is a hybrid block storage system that combines HDDs and SSDs for cloud-scale virtual disks. PBS <ref type="bibr" target="#b52">[53]</ref> supports erasure-coded cloud block storage with efficient updates. Our recent work SepBIT <ref type="bibr" target="#b44">[45]</ref> is a data placement scheme that mitigates the write amplification of garbage collection in log-structured cloud block storage, and its design and evaluation are based on the AliCloud and TencentCloud traces. In this work, we conduct an in-depth trace analysis that provides suggestions for improving the cloud block storage design.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Architecture of a cloud block storage system. It comprises multiple volumes that host a mix of cloud applications (e.g., virtual desktops, operating systems, web services, relational databases, and key-value stores).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Finding A.2: Cumulative distributions of I/O request sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Finding A.3: Cumulative distributions of numbers of active days across all volumes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Finding A.4: Cumulative distributions of write-to-read ratios across all volumes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Finding A.4: Cumulative distributions of WSSs (figures (a)-(c)) and I/O traffic sizes (figures (d)-(f)) across all volumes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Finding A.5: Cumulative distributions of WSS-to-capacity percentages across all volumes of different groups of raw capacities in AliCloud.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Finding A.6: Cumulative distributions of average request sizes across all volumes of different groups of raw capacities in AliCloud.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Finding A.6: Cumulative distributions of average read and write request sizes across all volumes of different groups of raw capacities in AliCloud.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Figure9: Finding B.1: Average and peak intensities of volumes. Note that we sort the average and peak intensities of volumes in descending order in their respective curves to make the distribution of each type of intensities clearly shown, so the curves are different from the ones in our conference version<ref type="bibr" target="#b21">[22]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Findings B.2-B.3: Cumulative distribution of burstiness ratios of volumes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Finding B.4: Inter-arrival times of requests. Figure (a) shows the cumulative distributions of inter-arrival times across all volumes. In figures (b) and (c), each boxplot represents the distribution of all the values collected in each volume according to the corresponding percentile.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 12 :Figure 13 :Figure 14 :</head><label>121314</label><figDesc>Figure 12: Findings B.5-B.7: Numbers of active, read-active, and write-active volumes. Note that the "Active" and "Write-active" curves almost overlap with each other.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Finding B. 8 :Figure 15 :</head><label>815</label><figDesc>Figure 15: Finding B.8: Average amount of traffic in 10-minute intervals each day from 12:00 AM to 11:59 PM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>(a)) and 52.0% of total average traffic in all timeslots (Figure 15(a)), while in TencentCloud, the daytime has 50.3% of average I/O requests (Figure 14(b)) and 49.6% of average traffic (Figure 15(b)). However, in MSRC, the daytime only has 33.7% of average I/O requests and 23.3% of average traffic, and the I/O requests and traffic mainly dominate in nighttime (Figures 14(c) and 15(c)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 16 :</head><label>16</label><figDesc>Figure 16: Finding B.9: Cumulative distributions of randomness ratios of volumes (figure (a)) and the relationship between the randomness ratios and total traffic sizes in top-10 traffic-intensive volumes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 17 :</head><label>17</label><figDesc>Figure 17: Finding B.10: Boxplots of percentages of traffic sizes for the top-1% and top-10% read and write blocks across all volumes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 18 :</head><label>18</label><figDesc>Figure 18: Finding B.11: Cumulative distributions of all percentages of read and write traffic going to read-mostly and write-mostly blocks, respectively, across all volumes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 19 :</head><label>19</label><figDesc>Figure 19: Finding B.12: Cumulative distributions of update coverages and percentages of update traffic across all volumes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 20 :</head><label>20</label><figDesc>Figure 20: Findings B.13-B.14: Cumulative distributions of RAW, WAW, RAR, and WAR times across all RAW, WAW, RAR, and WAR requests, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 21 :Figure 22 :</head><label>2122</label><figDesc>Figure 21: Finding B.15: Boxplots of percentiles of update intervals across all volumes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 23 :</head><label>23</label><figDesc>Figure 23: Finding B.16: Boxplots of miss ratios for reads and writes across all volumes, under the cache sizes of 1% and 10% of the WSS of a volume.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Basic statistics of AliCloud, TencentCloud, and MSRC.</figDesc><table><row><cell cols="5">AliCloud TencentCloud MSRC</cell></row><row><cell>#Volumes</cell><cell>1,000</cell><cell></cell><cell>4,995</cell><cell>36</cell></row><row><cell>Duration (days)</cell><cell>31</cell><cell></cell><cell>9.04</cell><cell>7</cell></row><row><cell>#Reads (millions)</cell><cell cols="2">5,058.6</cell><cell>10,030.2</cell><cell>304.9</cell></row><row><cell>#Writes (millions)</cell><cell cols="2">15,174.4</cell><cell>23,592.0</cell><cell>128.9</cell></row><row><cell>Read Traffic (TiB)</cell><cell>161.6</cell><cell></cell><cell>282.3</cell><cell>9.04</cell></row><row><cell>Write Traffic (TiB)</cell><cell>455.5</cell><cell></cell><cell>837.2</cell><cell>2.39</cell></row><row><cell>Update Traffic (TiB)</cell><cell>429.2</cell><cell></cell><cell>804.2</cell><cell>2.01</cell></row><row><cell>Total WSS (TiB)</cell><cell>29.5</cell><cell></cell><cell>38.7</cell><cell>2.87</cell></row><row><cell>Read WSS (TiB)</cell><cell>10.1</cell><cell></cell><cell>14.6</cell><cell>2.82</cell></row><row><cell>Write WSS (TiB)</cell><cell>26.3</cell><cell></cell><cell>33.0</cell><cell>0.38</cell></row><row><cell>Update WSS (TiB)</cell><cell>18.6</cell><cell></cell><cell>21.2</cell><cell>0.17</cell></row><row><cell></cell><cell></cell><cell></cell><cell>AliCloud</cell><cell>TencentCloud</cell><cell>MSRC</cell></row><row><cell>Read WSS over total WSS (A.1)</cell><cell></cell><cell></cell><cell>34.3%</cell><cell>37.6%</cell><cell>98.4%</cell></row><row><cell cols="2">75th percentiles of read/write sizes (A.2)</cell><cell cols="3">12 KiB/16 KiB 32 KiB/12 KiB 64 KiB/20 KiB</cell></row><row><cell>Volumes with short active periods (A.3)</cell><cell></cell><cell></cell><cell>15.7%</cell><cell>1.7%</cell><cell>0.0%</cell></row><row><cell>Write-dominant volumes (A.4)</cell><cell></cell><cell></cell><cell>91.5%</cell><cell>92.3%</cell><cell>52.8%</cell></row><row><cell cols="2">Higher fractions of WSS in larger volumes (A.5)</cell><cell></cell><cell>Yes</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Larger requests in larger volumes (A.6)</cell><cell></cell><cell>Yes</cell><cell>-</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note><p>Summary of the key properties of AliCloud, TencentCloud, and MSRC in Findings A.1-A.6.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell>Property</cell><cell cols="3">AliCloud TencentCloud MSRC</cell></row><row><cell></cell><cell>Average intensities (B.1)</cell><cell></cell><cell>Similar</cell><cell></cell></row><row><cell></cell><cell>Peak intensities (B.1)</cell><cell>Low</cell><cell>Low</cell><cell>High</cell></row><row><cell></cell><cell>Burstiness in volumes (B.2)</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell></row><row><cell>Load intensity</cell><cell>Diversity of burstiness (B.3) Inter-arrival times of requests (B.4) Activeness (B.5)</cell><cell>High Short High</cell><cell>High -Highest</cell><cell>Low Short Low</cell></row><row><cell></cell><cell>Activeness dominated by writes (B.6)</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell></row><row><cell></cell><cell>Activeness of reads (B.7)</cell><cell>Low</cell><cell>Highest</cell><cell>High</cell></row><row><cell></cell><cell>I/O traffic in daytime (B.8)</cell><cell>52.0%</cell><cell>49.6%</cell><cell>23.3%</cell></row><row><cell></cell><cell>Fractions of random I/Os (B.9)</cell><cell>High</cell><cell>Highest</cell><cell>Low</cell></row><row><cell>Spatial</cell><cell>Spatial aggregations of reads (B.10 and B.11)</cell><cell>High</cell><cell>Highest</cell><cell>High</cell></row><row><cell>patterns</cell><cell>Spatial aggregations of writes (B.10 and B.11)</cell><cell>High</cell><cell>High</cell><cell>Low</cell></row><row><cell></cell><cell>Update coverages (B.12)</cell><cell>High</cell><cell>High</cell><cell>Low</cell></row><row><cell></cell><cell>Read-after-write (RAW) times (B.13)</cell><cell>Large</cell><cell>Small</cell><cell>Large</cell></row><row><cell></cell><cell>Write-after-write (WAW) times (B.13)</cell><cell>Large</cell><cell>Small</cell><cell>Small</cell></row><row><cell></cell><cell>More WAW requests than RAW requests (B.13)</cell><cell>Yes</cell><cell>Yes</cell><cell>No</cell></row><row><cell>Temporal</cell><cell>Read-after-read (RAR) times (B.14)</cell><cell>Large</cell><cell>Small</cell><cell>Large</cell></row><row><cell>patterns</cell><cell>Write-after-read (WAR) times (B.14)</cell><cell>Large</cell><cell>Small</cell><cell>Large</cell></row><row><cell></cell><cell>More RAR requests than WAR requests (B.14)</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell></row><row><cell></cell><cell>Varying update intervals (B.15)</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell></row><row><cell></cell><cell>Miss ratios (B.16)</cell><cell>High</cell><cell>Low</cell><cell>High</cell></row></table><note><p>Summary of the key properties of AliCloud, TencentCloud, and MSRC in Findings B.1-B.16.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Finding B.2: Overall peak and average intensities as well as burstiness ratios. respectively) have burstiness ratios higher than 100. Also, 74.2%, 63.0%, and 97.2% of the volumes in AliCloud, TencentCloud, and MSRC have burstiness ratios higher than 10, respectively. This implies that burstiness is common, and such bursty volumes can observe load imbalance at some time. Note that prior work<ref type="bibr" target="#b33">[34]</ref> also shows that burstiness exists across various types of applications, such as enterprise systems, desktops, consumer electronics, web servers, and file systems. On the other hand, if we examine overall burstiness level by aggregating all volumes of the whole traces, the burstiness ratios are mild, with 2.11 in AliCloud, 1.65 in TencentCloud, and 7.39 in MSRC (see Table</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Finding B.11: Percentages of all read and write traffic going to read-mostly and write-mostly blocks, respectively.</figDesc><table><row><cell>Traces</cell><cell cols="3">AliCloud TencentCloud MSRC</cell></row><row><cell>Reads to read-mostly blocks (%)</cell><cell>59.2</cell><cell>78.5</cell><cell>75.9</cell></row><row><cell>Writes to write-mostly blocks (%)</cell><cell>80.7</cell><cell>90.8</cell><cell>33.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Finding B.12: Means, medians and 90th percentiles of update coverages of all volumes.</figDesc><table><row><cell>Traces</cell><cell cols="3">AliCloud TencentCloud MSRC</cell><cell>Traces</cell><cell cols="3">AliCloud TencentCloud MSRC</cell></row><row><cell>Mean (%) Median (%) 90th PCTL (%)</cell><cell>52.7 61.2 92.1</cell><cell>56.2 56.7 91.9</cell><cell>24.1 9.4 63.0</cell><cell>Mean (%) Median (%) 90th PCTL (%)</cell><cell>62.5 76.0 96.4</cell><cell>72.5 81.9 96.1</cell><cell>38.7 32.7 84.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Table 7: Finding B.12: Means, medians, and 90th</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">percentiles of the percentages of update traffic of</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>all volumes.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Findings B.13-B.14: Numbers of RAW, WAW, RAR, and WAR requests (in millions).</figDesc><table><row><cell>Traces</cell><cell cols="4">RAW (M) WAW (M) RAR (M) WAR (M)</cell></row><row><cell>AliCloud</cell><cell>12,432.7</cell><cell>103,708.4</cell><cell>29,845.0</cell><cell>11,760.6</cell></row><row><cell>TencentCloud</cell><cell>8,796.0</cell><cell>204,856.2</cell><cell>63,990.4</cell><cell>7,930.3</cell></row><row><cell>MSRC</cell><cell>297.2</cell><cell>289.2</cell><cell>1,382.4</cell><cell>330.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Finding B.15: Overall percentiles of update intervals across all volumes.</figDesc><table><row><cell></cell><cell></cell><cell cols="4">Percentiles (minutes) 25th 50th</cell><cell>75th</cell><cell>90th</cell><cell></cell><cell>95th</cell></row><row><cell></cell><cell></cell><cell cols="2">AliCloud</cell><cell></cell><cell>1.86 95.2</cell><cell>926.3</cell><cell cols="3">3,017.4 7,200.5</cell></row><row><cell></cell><cell></cell><cell cols="2">TencentCloud</cell><cell></cell><cell>0.23 0.67</cell><cell>5.4</cell><cell>120.0</cell><cell></cell><cell>973.1</cell></row><row><cell></cell><cell></cell><cell>MSRC</cell><cell></cell><cell></cell><cell cols="5">0.73 1.25 1,438.9 1,440.5 1,444.1</cell></row><row><cell>Update intervals (s)</cell><cell>1 10 2 10 4 10 6 10 8</cell><cell>25th 50th 75th 90th 95th</cell><cell>Update intervals (s)</cell><cell>1 10 2 10 4 10 6 10 8</cell><cell cols="2">25th 50th 75th 90th 95th</cell><cell>Update intervals (s)</cell><cell>1 10 2 10 4 10 6 10 8</cell><cell>25th 50th 75th 90th 95th</cell></row><row><cell></cell><cell></cell><cell>Percentiles</cell><cell></cell><cell></cell><cell cols="2">Percentiles</cell><cell></cell><cell></cell><cell>Percentiles</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Statistics of existing block-level trace studies</figDesc><table><row><cell></cell><cell>MSRC [33]</cell><cell>MS-Prod [19]</cell><cell>Zhou et al. [54]</cell><cell>Lee et al. [20]</cell><cell>Tencent -Cloud [52]</cell><cell>SSDTrace [49]</cell><cell>AliCloud</cell></row><row><cell>#Volumes</cell><cell>36</cell><cell>43</cell><cell>25</cell><cell>321</cell><cell>4,995</cell><cell>1</cell><cell>1,000</cell></row><row><cell>Duration (days)</cell><cell>7</cell><cell>0.003-1</cell><cell>&lt; 0.34</cell><cell>28</cell><cell>9.04</cell><cell>0.40</cell><cell>31</cell></row><row><cell>#Reads (millions)</cell><cell>304.9</cell><cell>126.2</cell><cell>0.04</cell><cell>2455.4</cell><cell>10,030.2</cell><cell>342.9</cell><cell>5,058.6</cell></row><row><cell>#Writes (millions)</cell><cell>128.9</cell><cell>87.3</cell><cell>0.13</cell><cell>898.3</cell><cell>23,592.0</cell><cell>9.69</cell><cell>15,174.4</cell></row><row><cell>Read Traffic (TiB)</cell><cell>9.04</cell><cell>2.98</cell><cell>0.002</cell><cell>64.8</cell><cell>282.3</cell><cell>2.94</cell><cell>161.6</cell></row><row><cell>Write Traffic (TiB)</cell><cell>2.39</cell><cell>1.70</cell><cell>0.006</cell><cell>15.0</cell><cell>837.2</cell><cell>6.38</cell><cell>455.5</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6">Conclusion</head><p>We present an in-depth comparative trace analysis on the production block-level I/O traces at <rs type="institution">Alibaba Cloud (AliCloud)</rs>, <rs type="funder">Tencent Cloud Block Storage (TencentCloud)</rs>, and <rs type="institution">Microsoft Research Cambridge (MSRC)</rs>; the AliCloud and TencentCloud traces are from cloud block storage systems, while the MSRC trace is collected from enterprise data centers. We reveal the commonalities and differences of the three sources of traces. We first identify 6 findings through the high-level analysis on the basic I/O statistics. We also identify 16 findings through the detailed analysis, based on which we further discuss the implications on three practical design considerations for cloud block storage, including load balancing, cache efficiency, and storage cluster management.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Easy and efficient disk I/O workload characterization in VMware ESX server</title>
		<author>
			<persName><forename type="first">I</forename><surname>Ahmad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 IEEE International Symposium on Workload Characterization (IISWC&apos;07)</title>
		<meeting>the 2007 IEEE International Symposium on Workload Characterization (IISWC&apos;07)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="149" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Alibaba Block Traces</title>
		<author>
			<persName><surname>Alibaba</surname></persName>
		</author>
		<ptr target="https://github.com/alibaba/block-traces" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Alibaba Cloud Block Storage</title>
		<author>
			<persName><surname>Alibaba</surname></persName>
		</author>
		<ptr target="https://www.alibabacloud.com/help/doc-detail/63136.htm" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title/>
		<ptr target="https://aws.amazon.com/ebs/" />
	</analytic>
	<monogr>
		<title level="j">Amazon. Amazon EBS</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On-demand flash cache management for cloud computing</title>
		<author>
			<persName><forename type="first">D</forename><surname>Arteaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cabrera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sundararaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><surname>Cloudcache</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th USENIX Conference on File and Storage Technologies (FAST&apos;16)</title>
		<meeting>the 14th USENIX Conference on File and Storage Technologies (FAST&apos;16)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="355" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Client-side flash caching for cloud systems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Arteaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th ACM International Systems and Storage Conference (SYSTOR&apos;14)</title>
		<meeting>the 7th ACM International Systems and Storage Conference (SYSTOR&apos;14)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Workload analysis of a large-scale key-value store</title>
		<author>
			<persName><forename type="first">B</forename><surname>Atikoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Frachtenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paleczny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGMETRICS</title>
		<meeting>ACM SIGMETRICS</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="53" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Finding a needle in Haystack: Facebook&apos;s photo storage</title>
		<author>
			<persName><forename type="first">D</forename><surname>Beaver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sobel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vajgel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th USENIX Symposium on Operating Systems Design and Implementation (OSDI&apos;10)</title>
		<meeting>the 9th USENIX Symposium on Operating Systems Design and Implementation (OSDI&apos;10)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="47" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">BORG: Block-reORGanization for self-optimizing storage systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bhadkamkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guerra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Useche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Burnett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liptak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rangaswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Hristidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th USENIX Conference on File and Storage Technologies (FAST&apos;09)</title>
		<meeting>the 7th USENIX Conference on File and Storage Technologies (FAST&apos;09)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="183" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Data retention in MLC NAND flash memory: Characterization, optimization, and recovery</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">F</forename><surname>Haratsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st IEEE International Symposium on High Performance Computer Architecture (HPCA&apos;15)</title>
		<meeting>the 21st IEEE International Symposium on High Performance Computer Architecture (HPCA&apos;15)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="551" to="563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Characterizing, modeling, and benchmarking RocksDB key-value workloads at Facebook</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vemuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H C</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th USENIX Conference on File and Storage Technologies (FAST&apos;20)</title>
		<meeting>the 18th USENIX Conference on File and Storage Technologies (FAST&apos;20)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="209" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Parity logging with reserved space: Towards efficient updates and recovery in erasure-coded clustered storage</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H W</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th USENIX Conference on File and Storage Technologies (FAST&apos;14)</title>
		<meeting>the 12th USENIX Conference on File and Storage Technologies (FAST&apos;14)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="163" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Software orchestrated flash array</title>
		<author>
			<persName><forename type="first">T.-C</forename><surname>Chiueh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Tsao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-F</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-N</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-D</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th ACM International Systems and Storage Conference (SYSTOR&apos;14)</title>
		<meeting>the 7th ACM International Systems and Storage Conference (SYSTOR&apos;14)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Analytic modeling of SSD write performance</title>
		<author>
			<persName><forename type="first">P</forename><surname>Desnoyers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th ACM International Systems and Storage Conference (SYSTOR&apos;12)</title>
		<meeting>the 5th ACM International Systems and Storage Conference (SYSTOR&apos;12)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An in-depth study of correlated failures in production SSD-based data centers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th USENIX Conference on File and Storage Technologies (FAST&apos;21)</title>
		<meeting>the 19th USENIX Conference on File and Storage Technologies (FAST&apos;21)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="417" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Slacker: Fast distribution with lazy docker containers</title>
		<author>
			<persName><forename type="first">T</forename><surname>Harter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Salmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Arpaci-Dusseau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Arpaci-Dusseau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th USENIX Conference on File and Storage Technologies (FAST&apos;16)</title>
		<meeting>the 14th USENIX Conference on File and Storage Technologies (FAST&apos;16)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="181" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The unwritten contract of solid state drives</title>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Arpaci-Dusseau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Arpaci-Dusseau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM European Conference on Computer Systems (EuroSys&apos;17)</title>
		<meeting>the 12th ACM European Conference on Computer Systems (EuroSys&apos;17)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="127" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Characteristics of I/O traffic in personal computer and server workloads</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM System Journal</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="347" to="372" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Characterization of storage workload traces from production windows servers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kavalanekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Worthington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sharda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 IEEE International Symposium on Workload Characterization (IISWC&apos;08)</title>
		<meeting>the 2008 IEEE International Symposium on Workload Characterization (IISWC&apos;08)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="119" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Understanding storage traffic characteristics on enterprise virtual desktop infrastructure</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kumano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Matsuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Endo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Fukumoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sugawara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM International Systems and Storage Conference (SYSTOR&apos;17)</title>
		<meeting>the 10th ACM International Systems and Storage Conference (SYSTOR&apos;17)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">URSA: Hybrid block storage for cloud-scale virtual disks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM European Conference on Computer Systems (EuroSys&apos;19)</title>
		<meeting>the 14th ACM European Conference on Computer Systems (EuroSys&apos;19)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An in-depth analysis of cloud block storage workloads in large scale production</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 IEEE International Symposium on Workload Characterization (IISWC&apos;20)</title>
		<meeting>the 2020 IEEE International Symposium on Workload Characterization (IISWC&apos;20)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="37" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Access characteristic guided read and write cost regulation for performance improvement on flash memory</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhuge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-M</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th USENIX Conference on File and Storage Technologies (FAST&apos;16)</title>
		<meeting>the 14th USENIX Conference on File and Storage Technologies (FAST&apos;16)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="125" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Optimizing NAND flash-based SSDs via retention relaxation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th USENIX Conference on File and Storage Technologies (FAST&apos;12)</title>
		<meeting>the 10th USENIX Conference on File and Storage Technologies (FAST&apos;12)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Analysis of and optimization for write-dominated hybrid storage nodes in cloud</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM Symposium on Cloud Computing 2019 (SoCC&apos;19)</title>
		<meeting>ACM Symposium on Cloud Computing 2019 (SoCC&apos;19)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="403" to="415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">DistCache: provable load balancing for large-scale storage systems with distributed caching</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Braverman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th USENIX Conference on File and Storage Technologies (FAST&apos;19)</title>
		<meeting>the 17th USENIX Conference on File and Storage Technologies (FAST&apos;19)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="143" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A study of SSD reliability in large scale enterprise storage deployments</title>
		<author>
			<persName><forename type="first">S</forename><surname>Maneas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mahdaviani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Emami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schroeder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th USENIX Conference on File and Storage Technologies (FAST&apos;20)</title>
		<meeting>the 18th USENIX Conference on File and Storage Technologies (FAST&apos;20)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="137" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Parallax: Virtual disks for virtual machines</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">T</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cully</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lefebvre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Feeley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Warfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd ACM European Conference on Computer Systems (EuroSys&apos;08)</title>
		<meeting>the 3rd ACM European Conference on Computer Systems (EuroSys&apos;08)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="41" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Blizzard: Fast, cloud-scale block storage for cloud-oblivious applications</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mickens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Nightingale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Elson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nareddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kadav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chidambaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th USENIX Symposium on Networked Systems Design and Implementation (NSDI&apos;14)</title>
		<meeting>the 11th USENIX Symposium on Networked Systems Design and Implementation (NSDI&apos;14)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="257" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">MSR Cambridge Traces</title>
		<ptr target="http://iotta.snia.org/traces/388" />
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>Microsoft</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">SFS: Random write considered harmful in solid state drives</title>
		<author>
			<persName><forename type="first">C</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">I</forename><surname>Eom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th USENIX Conference on File and Storage Technologies (FAST&apos;12)</title>
		<meeting>the 10th USENIX Conference on File and Storage Technologies (FAST&apos;12)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Towards characterizing cloud backend workloads: Insights from google compute clusters</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Hellerstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cirne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGMETRICS</title>
		<meeting>ACM SIGMETRICS</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="34" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Write Off-Loading: Practical power management for enterprise storage</title>
		<author>
			<persName><forename type="first">D</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Donnelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rowstron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th USENIX Conference on File and Storage Technologies (FAST&apos;08)</title>
		<meeting>the 6th USENIX Conference on File and Storage Technologies (FAST&apos;08)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="253" to="267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Disk drive level workload characterization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Riska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 USENIX Annual Technical Conference (USENIX ATC&apos;06)</title>
		<meeting>the 2006 USENIX Annual Technical Conference (USENIX ATC&apos;06)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="97" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The design and implementation of a log-structured file system</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rosenblum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Ousterhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer Systems</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="26" to="52" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">FlashTier: a lightweight, consistent and durable storage cache</title>
		<author>
			<persName><forename type="first">M</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Swift</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th ACM European Conference on Computer Systems (EuroSys&apos;12)</title>
		<meeting>the 7th ACM European Conference on Computer Systems (EuroSys&apos;12)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="267" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Extending SSD lifetimes with diskbased write caches</title>
		<author>
			<persName><forename type="first">G</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Prabhakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wobber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th USENIX Conference on File and Storage Technologies (FAST&apos;10)</title>
		<meeting>the 8th USENIX Conference on File and Storage Technologies (FAST&apos;10)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="101" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The proof and measurement of association between two things</title>
		<author>
			<persName><forename type="first">C</forename><surname>Spearman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American Journal of Psychology</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">3/4</biblScope>
			<biblScope unit="page" from="441" to="471" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">DiskAccel: Accelerating disk-based experiments by representative sampling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tarihi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Asadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sarbazi-Azad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGMETRICS</title>
		<meeting>ACM SIGMETRICS</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="297" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Tencent Block Storage</title>
		<author>
			<persName><surname>Tencent</surname></persName>
		</author>
		<ptr target="http://iotta.snia.org/traces/27917" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">SRCMap: Energy proportional storage using dynamic consolidation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Useche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rangaswami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th USENIX Conference on File and Storage Technologies (FAST&apos;10)</title>
		<meeting>the 8th USENIX Conference on File and Storage Technologies (FAST&apos;10)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="267" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Distribution fitting and performance modeling for storage traces</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wajahat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Estro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zadok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th IEEE International Symposium on the Modeling, Analysis, and Simulation of Computer and Telecommunication Systems (MASCOTS&apos;19)</title>
		<meeting>the 27th IEEE International Symposium on the Modeling, Analysis, and Simulation of Computer and Telecommunication Systems (MASCOTS&apos;19)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="138" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Efficient MRC construction with SHARDS</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Waldspurger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garthwaite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ahmad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th USENIX Conference on File and Storage Technologies (FAST&apos;15)</title>
		<meeting>the 13th USENIX Conference on File and Storage Technologies (FAST&apos;15)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="95" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Efficient SSD caching by avoiding unnecessary writes using machine learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 47th ACM International Conference on Parallel Processing (ICPP&apos;18)</title>
		<meeting>the 47th ACM International Conference on Parallel Processing (ICPP&apos;18)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Separating data via block invalidation time inference for write amplification reduction in log-structured storage</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th USENIX Conference on File and Storage Technologies (FAST&apos;22)</title>
		<meeting>the 20th USENIX Conference on File and Storage Technologies (FAST&apos;22)</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="429" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">BCW: Buffer-controlled writes to HDDs for SSD-HDD hybrid storage server</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th USENIX Conference on File and Storage Technologies (FAST&apos;20)</title>
		<meeting>the 18th USENIX Conference on File and Storage Technologies (FAST&apos;20)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="253" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Characterizing storage workloads with counter stacks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ingram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Drudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J A</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Warfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th USENIX Symposium on Operating Systems Design and Implementation (OSDI&apos;14)</title>
		<meeting>the 11th USENIX Symposium on Operating Systems Design and Implementation (OSDI&apos;14)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="335" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Lessons and actions: What we learned from 10k SSD-related storage system failures</title>
		<author>
			<persName><forename type="first">E</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 USENIX Annual Technical Conference (USENIX ATC&apos;19)</title>
		<meeting>the 2019 USENIX Annual Technical Conference (USENIX ATC&apos;19)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="961" to="976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">SSD-based workload characteristics and their performance implications</title>
		<author>
			<persName><forename type="first">G</forename><surname>Yadgar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gabel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jaffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schroeder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Storage</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">WARCIP: Write amplification reduction by clustering I/O pages</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM International Systems and Storage Conference (SYSTOR&apos;19)</title>
		<meeting>the 12th ACM International Systems and Storage Conference (SYSTOR&apos;19)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="155" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A large scale analysis of hundreds of in-memory cache clusters at Twitter</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">V</forename><surname>Rashmi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th USENIX Symposium on Operating Systems Design and Implementation (OSDI&apos;20)</title>
		<meeting>the 14th USENIX Symposium on Operating Systems Design and Implementation (OSDI&apos;20)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="191" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">OSCA: An online-model based cache allocation scheme in cloud block storage systems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of USENIX Annual Technical Conference (USENIX ATC&apos;20)</title>
		<meeting>USENIX Annual Technical Conference (USENIX ATC&apos;20)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="785" to="798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">PBS: An efficient erasure-coded block storage system based on speculative partial writes</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Storage</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">I/O characteristics of smartphone applications and their implications for eMMC design</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 IEEE International Symposium on Workload Characterization (IISWC&apos;15)</title>
		<meeting>the 2015 IEEE International Symposium on Workload Characterization (IISWC&apos;15)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="12" to="21" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
