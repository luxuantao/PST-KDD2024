<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hierarchical retinal blood vessel segmentation based on feature and ensemble learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shuangling</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Shandong University</orgName>
								<address>
									<postCode>250101</postCode>
									<settlement>Jinan</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Yilong</forename><surname>Yin</surname></persName>
							<email>ylyin@sdu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Shandong University</orgName>
								<address>
									<postCode>250101</postCode>
									<settlement>Jinan</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guibao</forename><surname>Cao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Shandong University</orgName>
								<address>
									<postCode>250101</postCode>
									<settlement>Jinan</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Benzheng</forename><surname>Wei</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">College of Science and Technology</orgName>
								<orgName type="institution">Shandong University of Traditional Chinese Medicine</orgName>
								<address>
									<postCode>250355</postCode>
									<settlement>Jinan</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuanjie</forename><surname>Zheng</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution">University of Pennsylvania</orgName>
								<address>
									<postCode>19104</postCode>
									<settlement>Philadelphia</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gongping</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Shandong University</orgName>
								<address>
									<postCode>250101</postCode>
									<settlement>Jinan</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
						</author>
						<title level="a" type="main">Hierarchical retinal blood vessel segmentation based on feature and ensemble learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D20022684787B13E75EC90D8714676D8</idno>
					<idno type="DOI">10.1016/j.neucom.2014.07.059</idno>
					<note type="submission">Received 5 April 2014 Received in revised form 25 June 2014 Accepted 28 July 2014</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Convolutional neural network Ensemble learning Feature learning Random forest Retinal blood vessel segmentation</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Segmentation of retinal blood vessels is of substantial clinical importance for diagnoses of many diseases, such as diabetic retinopathy, hypertension and cardiovascular diseases. In this paper, the supervised method is presented to tackle the problem of retinal blood vessel segmentation, which combines two superior classifiers: Convolutional Neural Network (CNN) and Random Forest (RF). In this method, CNN performs as a trainable hierarchical feature extractor and ensemble RFs work as a trainable classifier. By integrating the merits of feature learning and traditional classifier, the proposed method is able to automatically learn features from the raw images and predict the patterns. Extensive experiments have been conducted on two public retinal images databases (DRIVE and STARE), and comparisons with other major studies on the same database demonstrate the promising performance and effectiveness of the proposed method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Retinal blood vessel segmentation has been widely used in various scenarios. For example, change of the retinal blood vessel appearance is an important indicator for various ophthalmologic and cardiovascular diseases, such as diabetes, hypertension, and arteriosclerosis <ref type="bibr" target="#b0">[1]</ref>, therefore, automatic segmentation and analysis of the retinal vasculature play an extremely vital role in the implementation of screening programs for diabetic retinopathy, the evaluation of retinopathy of prematurity, foveal avascular region detection, arteriolar narrowing detection, the diagnosis of cardiovascular diseases and hypertension, and computer-assisted laser surgery <ref type="bibr" target="#b1">[2]</ref>. Moreover, the generation of retinal maps and detection of branch points have been utilized for temporal or multimodal image registration, retinal image mosaic synthesis, optic disc identification, fovea localization and biometric identification <ref type="bibr" target="#b1">[2]</ref>.</p><p>Both manual delineation and automatic algorithms have been used in retinal vessel segmentation. However, they have not gained wide acceptance due to several challenges. Manual delineation is skill demanding, tedious, time-consuming, and infeasible if given a large volume of fundus image databases. Accuracy of the automatic segmentation algorithms (to be reviewed in Section 2) is limited due to low blood vessel contrast, irregular shaped bright and dark lesions (in the form of hemorrhages, exudates, drusen and the optic disc boundary), intricate vessel topology (including vessel crossing and branching, as well as variation of vessel diameter and vessel grey levels) and nonuniform illumination of images as well as image deformation of scaling, skewing and other distortions.</p><p>In this paper, we present the hybrid method based upon convolutional neural network (CNN) <ref type="bibr" target="#b2">[3]</ref> and ensemble random forests (RFs) <ref type="bibr" target="#b3">[4]</ref> for automatic retinal blood vessel segmentation. We first employed a set of preprocessing steps to correct the nonuniform illumination of retinal images and to improve vessel contrast. We then used CNN to extract a set of hierarchical features which are not only invariant to image translation, scaling, skewing and other distortions, but also contain image based multi-scale information of the geometric structure of retina. We finally trained ensemble RFs to obtain a vessel classifier. The whole pipeline of the proposed method is trainable and automatic. Moreover, our method can effectively deal with the challenges of retinal vessel segmentation, as shown by our evaluations conducted using two publicly available databases (the DRIVE <ref type="bibr" target="#b4">[5]</ref> and STARE <ref type="bibr" target="#b0">[1]</ref>) and comparisons with state-of-the-art. Experimental results show that our approach is competitive with state-of-the-art by achieving sensitivity/specificity/accuracy/AUC values of 0.8173/0.9733/0.9767/0.9475 for the DRIVE database and of 0.8104/0.9791/0.9813/0.9751 for the STARE database. In contrast, previous methods of Fraz <ref type="bibr" target="#b5">[6]</ref> produced values of 0.7406/0.9807/0.9480/0.9747 for the DRIVE database and of 0.7548/0.9763/0.9534/0.9768 for the STARE database.</p><p>The rest of this paper is organized as follows. Section 2 gives a review of related work reported in the latest literature. Section 3 provides the description of CNN and RF. Section 4 gives a detailed description of the proposed method. Section 5 evaluates the proposed method. Further, the concluding remarks are included in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>There are a number of methods available in the literature for retinal blood vessel segmentation, as reviewed by Fraz et al. <ref type="bibr" target="#b1">[2]</ref>. These methods can be broadly divided into two categories: unsupervised and supervised <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Unsupervised methods</head><p>The unsupervised methods can be further classified into five main subcategories: matched filtering, morphological processing, vessel tracking, multiscale analysis, and model-based algorithms. The matched filter based methodology convolves a kernel based on the Gaussian or its derivatives with the retinal image to enhance the vessel features <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7]</ref>. In order to design the matched filter kernel, this methodology exploits the piece-wise linear approximation, the decrease in vessel diameter along vascular length, and the Gaussian-like intensity profile of retinal blood vessels. Mathematical morphology coupling curvature evaluation <ref type="bibr" target="#b7">[8]</ref> and matched filtering for centerline detection <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> is also deployed for retinal vessel segmentation. Vessel tracking based methodology <ref type="bibr" target="#b10">[11]</ref> segments a vessel using local information and works at the level of a single vessel rather than the entire vasculature. The multiscale approaches are based upon scalespace analysis. The multiscale second-order local structure of an image (Hessian) is examined, and a vesselness measure is obtained on the basis of eigenvalue analysis of the Hessian <ref type="bibr" target="#b11">[12]</ref>. The model based approaches include the vessel profile models <ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref>, active contour models <ref type="bibr" target="#b16">[17]</ref>, and level sets based geometric models <ref type="bibr" target="#b17">[18]</ref>. The first approach is based on hand-designed feature extraction, where each pixel is projected into a set of features predefined with prior knowledge before classification. Neimeijer et al. <ref type="bibr" target="#b18">[19]</ref> applied the Gaussian and its derivatives at multiple scales to form a feature vector for each pixel, augmented with the green channel of the RGB image. Stall et al. <ref type="bibr" target="#b4">[5]</ref> used ridge profiles to compute a feature vector for each pixel, then performed feature selection. In <ref type="bibr" target="#b19">[20]</ref>, a feature vector is composed of the pixel intensity and twodimensional Gabor wavelet transform responses taken at multiple scales. Ricci and Perfetti <ref type="bibr" target="#b20">[21]</ref> deployed line operators to form a feature vector. Lupascu et al. <ref type="bibr" target="#b21">[22]</ref> used a feature vector at different spatial scales for each pixel. In <ref type="bibr" target="#b22">[23]</ref>, a feature vector is obtained by combination of moment-invariant and gray-level features. In <ref type="bibr" target="#b5">[6]</ref>, the method utilized a feature vector based on the orientation analysis of gradient vector field, morphological transformation, line strength measures, and Gabor filter responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Supervised methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supervised</head><p>The second approach deploys Artificial neural network (ANN) or its variants to segment retinal blood vessels, where feature extraction and classification are integrated into one pipeline: feature extraction itself is directly learned from raw image and not enforced by designers, then followed in the ANN process is a generation of predictions. Nekovei and Sun <ref type="bibr" target="#b23">[24]</ref> presented a method using a back-propagation network for the detection of blood vessels. In this method, neural network is directly applied to the image pixels without any prior feature extraction. For each pixel being classified, raw pixel values in a square window centered on it are fed to the network as input.</p><p>Although these supervised methods have achieved satisfactory segmentation results in some scenarios, there are still some issues. In hand-designed feature extraction approaches, the features must be very carefully predefined before classification, making feature detection very time-consuming and tedious. Most importantly, hand-designed features have to be redesigned for datasets with different characteristics. In contrast, the approaches based upon feature learning can extract features automatically from the raw images. Moreover, CNN is supervised feature learner able to learn complex invariances such as scale and rotational invariance. However, the classification mechanism in ANN or its variants is fairly simple: usually nonlinear activation functions are employed in the last output layer to predict patterns, which results in the low performance of ANN. Considering all of above, we propose to combine two well-known classifiers: Convolutional Neural Network (CNN) and Random Forest (RF). In this paper, CNN works as a trainable hierarchical feature extractor, and then ensemble RFs perform as the trainable classifier supplied with hierarchical features learned from CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>The proposed method leverages trainable hierarchical features extracted using the CNN algorithm <ref type="bibr" target="#b2">[3]</ref>  based on the random forest (RF) technique <ref type="bibr" target="#b3">[4]</ref>. In this section, we will briefly review the CNN and RF techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Trainable feature extractor: convolutional neural network</head><p>CNN was neurobiologically motivated by the findings of locally sensitive and orientation-selective nerve cells in the visual cortex, as introduced by LeCun and Bengio in 1995 <ref type="bibr" target="#b2">[3]</ref>. The authors of CNN also designed a classic network structure that implicitly extracts relevant features from raw image, named LeNet-5 <ref type="bibr" target="#b24">[25]</ref>. Convolutional Neural Network is a special kind of multi-layer neural networks, with the following characteristics: CNN is a feedforward network that can extract topological properties from an image. Like almost every other neural network, it is trained with a version of the back-propagation algorithm. CNN is designed to recognize visual patterns directly from pixel images with minimal preprocessing. They can recognize patterns with extreme variability, such as handwritten characters.</p><p>CNN typically consists of a convolution layer, a subsampling layer, and a fully connected layer. In CNN, successive layers of convolution and subsampling are typically alternated. Schematic diagram of CNN is shown in Fig. <ref type="figure" target="#fig_1">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Convolution layer (C)</head><p>Convolution layer works as a feature extraction layer. All neurons in a feature map share the same set of weights and the same bias. In this way, all neurons in a feature map detect the same feature at different positions on the input. The other feature maps in this layer use different sets of weights and biases, so different types of local features are extracted, and weight sharing reduces the number of free parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Subsampling layer (S)</head><p>Subsampling layer works as a feature selection layer. The subsampling layers perform a local averaging and a subsampling to reduce the spatial resolution of each feature map. A certain degree of shift and distortion invariance can be achieved by reducing the spatial resolution of the feature map. In addition, the weight sharing is applied in subsampling layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3.">Fully connected layer (FC)</head><p>This is the standard layer of a multi-layer network. It performs a linear multiplication of the input vector by a weight matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Trainable traditional classifier: random forest</head><p>Random forest (RF) is a widely used "ensemble learning" method for classification and regression, which was proposed by <ref type="bibr">Breiman [4]</ref>. This method has the following desirable characteristics:</p><p>1) It is simple, easily parallelized and user-friendly (only two parameters).</p><p>2) It can work efficiently on large data sets.</p><p>3) It is relatively robust even in the presence of many noisy features, Random Forest performs well, making it unnecessary to perform feature selection procedure. 4) Its accuracy is as good as Adaboost and sometimes even better. 5) It is faster than bagging or boosting.</p><p>Random forest consists of tree predictors, where each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest <ref type="bibr" target="#b3">[4]</ref>. Each tree is grown as follows: First, the training set used to grow each tree is a bootstrap sample of the observations, i.e., a sample of size N drawn with replacement from the original sample of N observations. Some observations are represented multiple times, while others are left out. The left-out observations are called "out-ofbag" and these out-of-bag samples can be used to calculate an unbiased error rate and variable importance, which eliminates the need for a test set or cross-validation. Second, the best split at each node is selected from among a random subset of the predictor variables. Different variables are used at each split in different trees. Third, each tree is grown to its large extent possible, and there is no pruning. The random forests final prediction is the mean prediction (regression) or class with maximum votes (classification). Breiman <ref type="bibr" target="#b3">[4]</ref> showed that the prediction error converges to a limiting value as the number of trees tends to infinity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Proposed method</head><p>In the proposed method, CNN performs as a trainable hierarchical feature extractor and ensemble RFs work as a classifier. It is noteworthy that features utilized in the proposed method learned from not only the last layer output of CNN but also the intermediate layers outputs. Features learned from the same layer of CNN are fed into one RF classifier. Finally, winner-takes-all is employed to ensemble the predictions of each RF. For training procedure, raw pixel values from a square sub-window centered on representative pixel sample are fed into CNN, when CNN is well trained, several RFs are trained with learned hierarchical features extracted from CNN. Finally, ensemble method of winner-takes-all is employed to predict whether the center pixel of a sub-window belongs to a blood vessel or not. For testing procedure, sliding subwindows sampling from the testing image are directly fed into CNN to extract the learned hierarchical features, then winner classifier supplied with these features is used to predict the result. As shown in Fig. <ref type="figure">2</ref>, our method consists of three basic steps of preprocessing, hierarchical feature extraction and ensemble classification. In preprocessing, image enhancement and superpixel based sample selection are carried out.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Preprocessing</head><p>For colored images in RGB space, the green channel shows the best vessel-background contrast, while the red and blue channels show low contrast and are very noisy <ref type="bibr" target="#b19">[20]</ref>. Hence, we just employ the green channel of the RGB colored retinal image in the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">Image enhancement</head><p>First of all, images were preprocessed with histogram equalization and Gaussian filtering, for the purpose of reducing noise, making the intensity more uniform and improving the contrast of the vessels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">Superpixel based sample selection method</head><p>For large volumes of digital fundus retinal images, training pixel classifier is large time consuming and big storage space demanding, therefore, we introduce an automatic superpixel based sample selection method, which is part of our previous work <ref type="bibr" target="#b25">[26]</ref>. Superpixel, generally obtained by image over-segmentation, is local nearby pixel grouping. Advantages of using superpixels lie in the preserve natural image boundaries while capturing redundancy in the data as well as enforcing local consistency <ref type="bibr" target="#b26">[27]</ref>. In this paper, we apply Simple Linear Iterative Clustering (SLIC) <ref type="bibr" target="#b27">[28]</ref> for superpixel generation, which is an adaptation of the k-means algorithm. In order to reduce redundancy among pixel samples and pick out representative samples, pixels inside the same superpixel can be regarded as homogenous, then we randomly select several pixels inside a superpixel to represent all the pixels inside this superpixel. For simplicity and efficiency, only one pixel is randomly selected for one superpixel in this method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Trainable hierarchical feature extraction</head><p>In this method, CNN is utilized as a trainable hierarchical feature extractor. For each target pixel, raw pixel values from a square subwindow centered on it are directly fed as input to the CNN. With local receptive fields, neurons in CNN can detect elementary visual features such as oriented edges, end-points, corners. These features are then combined by the successive layers in order to capture higher-order features <ref type="bibr" target="#b24">[25]</ref>. We take full advantage of these hierarchical level features learned from different layers of CNN to train the ensemble classifier. In the proposed method, convolutional layer feature maps are indexed by C1, C3, and C5 and subsampling layer feature maps are indexed by S2 and S4. In order to learn more about the characteristics of features learned from different layers, the learned hierarchical features from S2, S4 and C5 are partly selected to be visualized using Matlab in Fig. <ref type="figure" target="#fig_2">3</ref>. We can see from the figure that previous layer in CNN extracts elementary features with a high resolution while successive layers capture higher-order statistics with a low resolution.</p><p>As to the inputs and the size of CNN, two trade-offs should be considered. First, when the dimensionality of the input space increases, the data becomes increasingly sparse, making ANN difficult to learn a decision boundary accurately <ref type="bibr" target="#b28">[29]</ref>. Therefore inputs to CNN should be small while keeping large enough to describe the data. Considering the average width and the average area of bright and dark lesions, a square window with size of 25 Â 25 is adopted as input to the CNN. Second, the training time tends to scale with the amount of training data and size of the network, and therefore it is generally preferable to train smaller networks <ref type="bibr" target="#b28">[29]</ref>. For the deep network, usually five or six layers are enough. We adopted the classic LeNet-5 network, with 4 Â 4 convolution matrix, subsampling by factor 2, as shown in Table <ref type="table" target="#tab_1">1</ref>. Bold fonts indicate layers that are used to train ensemble classifier. Different maps are for detecting different features while same map detects features from different locations. Learned features in the same feature map can be seen as contextual information. Since there is no need to perform feature selection in random forest, we utilize all learned features from different maps as well as same map in one layer. Take the layer S2 for example, 11 Â 11 neurons in one map can learn 121 features which characterize the contextual information. At the same time, there are 12 maps for 12 different features. Therefore, the learned features' dimension is 11 Â 11 Â 12 ¼1452, with one neuron corresponding to one dimensional feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ensemble classification</head><p>Ensemble classification is a process in which multiple classifiers are strategically generated and combined to solve the same problem <ref type="bibr" target="#b29">[30]</ref>. Ensemble learning utilizes multiple classifiers to get better predictive performance by combining the decisions from multiple weak learners into one powerful ensemble classifier. In this paper, we apply random forest as the base classifier and winner-takes-all is deployed as the ensemble model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">Base classifier: random forest</head><p>Learned features are extracted from layer S2, S4, C5 respectively, and then features obtained from the same layer are fed into a RF classifier individually, see Fig. <ref type="figure">2</ref>. Thereby, three layers correspond to three RF classifiers, donated as RF-1, RF-2, RF-3, respectively. The independent classifiers work in parallel. Giver an instance, they all give their decisions which are then combined by winner-takes-all to make the final decision. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">Ensemble method: winner-takes-all</head><p>The Winner-takes-all classifier is simple but effective. Its final class output is determined by the base classifier which wins the best classification performance among all the classifiers fused. We incorporate it in our method due to two reasons: 1) In winner-takes-all, base classifiers compete with each other, thus making it easy to select the best classifier while majority voting or averaging ensemble method can avoid selection of the worst classifier by obtaining a compromise between fused classifiers. Moreover, experiment is conducted to verify winnertakes-all is the best choice, and experimental details are shown in Section 5. 2) Intuitively, higher-order features learned from successive layers of CNN are more abstract and therefore more suitable for object or image classification rather than pixel level classification, while elementary features implicitly extracted from the previous layer of CNN are suitable for pixel level classification.</p><p>In out pipeline, we choose base RF classifier trained with features from S2 (RF-1) as the winner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Materials</head><p>Similar to most of the retinal vessel segmentation methods, the proposed method is evaluated on two well established public databases: the DRIVE database and the STARE database.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1.">DRIVE database</head><p>The photographs for the DRIVE (Digital Retinal Images for Vessel Extraction) database <ref type="bibr" target="#b4">[5]</ref> were obtained from a diabetic retinopathy screening program in the Netherlands. The screening population consisted of 400 diabetic subjects between 25 and 90 years of age. Forty photographs have been randomly selected, 33 do not show any sign of diabetic retinopathy and 7 show signs of mild early diabetic retinopathy. Each image has been JPEG compressed.</p><p>The images were acquired using a Canon CR5 non-mydriatic 3CCD camera with a 451 field of view (FOV). Each image was captured using 8 bits per color plane at 768 by 584 pixels. The FOV of each image is circular with a diameter of approximately 540 pixels. For this database, the images have been cropped around the FOV. For each image, a mask image is provided that delineates the FOV.</p><p>The set of 40 images has been divided into a training set and a test set evenly and each set contains 20 images. For the training images, a single manual segmentation of the vasculature is   available. For the test cases, two manual segmentations are available; one is used as the gold standard, the other one can be used to compare computer generated segmentations with those of independent human observer. All human observers were instructed and trained by an ophthalmologist on how to segment the vasculature manually. They were asked to mark all vessel pixels with a confidence of at least 70%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2.">STARE database</head><p>The STARE (Structured Analysis of the Retina) database [1] contains 20 images for blood vessel segmentation, out of which ten images contain pathology. The digitized images are obtained by a Topcon TRV-50 fundus camera at 351 field of view. The images were digitized to 605 Â 700 pixels, 8 bits per color channel. The diameter of the FOV is approximately 650 Â 700 pixels. All the images are manually segmented by two observers. The second observer pays many more attention to the thinner vessels than the first one and segmentation of the first observer is usually treated as the ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Performance measurements</head><p>In the retinal vessel segmentation process, every pixel is classified either as vessel or surrounding tissue. Consequently, there are four possible results for each segmentation: two classifications and two misclassifications. The true positive (TP) means a pixel is classified as vessel in both in the ground truth and segmented image, and the true negative (TN) means a pixel is recognized as a non-vessel in both the ground truth and segmented image. As to the two misclassifications, the false negative (FN) means a pixel is identified as non-vessel in the segmented image but as a vessel pixel in the ground truth image, and the false positive (FP) means a pixel is classified as vessel in the segmented image but non-vessel in the ground truth image.</p><p>The true positive rate (TPR) is the fraction of pixels correctly detected as vessel pixels. The false positive rate (FPR) represents the fraction of pixels incorrectly detected as vessel pixels. Sensitivity (SN) is the ability of an algorithm to identify the vessel pixels, and it is equal to TPR. Specificity (SP) reflects the ability to detect non-vessel pixels, also denoted as (1-FPR). The accuracy (Acc) represents the ratio of the total number of correctly classified pixels to the number of pixels in the image FOV.</p><p>A receive operating characteristic (ROC) curve is a plot of TPR versus FPR by varying the threshold on the probability map image. The performance metrics also include the area under ROC curve (AUC). In this paper, all the performance metrics are computed considering only pixels inside the FOV over all test images. These metrics is summarized in Table <ref type="table" target="#tab_2">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Experiments setting</head><p>In this study, all the experiments are implemented in Matlab and conducted on a high performance computer with four Intel Xeon CPUs at 2.00 GHz and 256 GB RAM. In all the experiments, networks are trained 100 epochs and each of Random Forest is trained by default parameters.  (about 2% of the entire database), therefore, the performance is evaluated on the whole set of 20 images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Performance of the proposed method</head><p>The proposed method was evaluated on DRIVE and STARE database images with available ground truth images. Since FOV masks are not provided for STARE images, they were automatically generated using source code downloaded from site by Hoover <ref type="bibr" target="#b0">[1]</ref> (available at http://www.parl.clemson.edu/ $ ahoover/stare/). Performance results are shown in Tables <ref type="table" target="#tab_3">3</ref> and<ref type="table" target="#tab_4">4</ref>.</p><p>Vessel segmentation results: The binary vessel segmentation image is obtained from the probability map image based on OSTU threshold <ref type="bibr" target="#b30">[31]</ref>. The performance metrics are calculated by taking the first human observer as the ground truth. The best case accuracy, sensitivity, specificity, AUC for the DRIVE database are 0.988368, 0.903219, 0.986776, 0.960497, respectively, and the worst case measures are 0.960937, 0.667402, 0.955739 and 0.938519, respectively. The best case vessel segmentation result for the STARE database has an accuracy of 0.994903; sensitivity, specificity, AUC are 0.94471, 0.99432, and 0.988388, respectively. The worst case accuracy is 0.953499; sensitivity, specificity, and AUC are 0.548304,0.948144, and 0.962269, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Comparison with the second human observer</head><p>The average performance measures for the DRIVE, STARE databases are tabulated in Table <ref type="table" target="#tab_5">5</ref>. From Table <ref type="table" target="#tab_5">5</ref>, we can see that the average accuracy rates and average specificity rates obtained by the algorithm are higher than the second human observers for the DRIVE and STARE databases.</p><p>The segmented images with the best and worst accuracies from the DRIVE and STARE databases are illustrated in Figs. <ref type="figure" target="#fig_3">4</ref> and<ref type="figure" target="#fig_4">5</ref>, respectively.</p><p>From Figs. <ref type="figure" target="#fig_3">4</ref> and<ref type="figure" target="#fig_4">5</ref>, we can see that the proposed method fail to capture some fine vessels. However, fine vessels are not important for clinical importance <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b22">23]</ref>. It is also worth mentioning that many clinical investigations take measurements only on major ones in a limited region around the optic disc, without using fine vessels <ref type="bibr" target="#b31">[32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Comparison of individual classifier and ensemble classifier</head><p>In this experiment, we compare the average segmentation performance of individual classifier and ensemble classifier in terms of SN, SP, Acc, and AUC. Ensemble is performed on probability map image digitalized by continuous output of RF. For each target pixel, each individual classifier gives its probability values,  and then these probability values are combined by the certain ensemble method. For weighted ensemble, weight vector for weighted ensemble one is 0.5, 0.3, 0.2, and for weighted ensemble two 0.7, 0.2, 0.1 respectively. Experimental results are shown in Tables <ref type="table" target="#tab_6">6</ref> and<ref type="table">7</ref>.</p><p>From Tables <ref type="table" target="#tab_6">6</ref> and<ref type="table">7</ref>, following conclusions can be drawn:</p><p>1) Each individual RF performs better than pure CNN, which verified that the proposed hybrid method combine the merits of feature learning and traditional classifier. 2) RF-1 obtains the better performance than RF-2 and RF-3. This is because learned features fed into RF-1 are more elementary while learned features fed into RF-2 and RF-3 are more abstract with higher-order statistics. 3) For ensemble method, using a winner-takes-all method results in the best overall performance. Winner-takes-all can get the best classifier while average or weighted ensemble allows avoiding the selection of the worst classifier, but fail to outperform the best individual classifier of the ensemble. 4) Results on STARE database images demonstrate the flexibility of the proposed method on different type of images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7.">Comparison with published methods</head><p>In Tables <ref type="table" target="#tab_7">8</ref> and<ref type="table" target="#tab_8">9</ref>, we compare our approach with the most recent methods in terms of sensitivity, specificity, accuracy, and AUC on the DRIVE and STARE databases, respectively. Tables <ref type="table" target="#tab_7">8</ref> and<ref type="table" target="#tab_8">9</ref> show us that the proposed method achieves state of the art SN and Acc on both DRIVE and STARE database images.</p><p>Following conclusions can be drawn from Tables <ref type="table" target="#tab_7">8</ref> and<ref type="table" target="#tab_8">9</ref>:</p><p>1) Square windows directly sample on raw image and raw image intensities are utilized directly as inputs. 2) Learned features are general, not problem specific. Therefore, an important practical advantage of square windows is not necessary to design features with a priori knowledge of the problem. 3) Individual classifier can be easily parallel. 4) By winner-takes-all ensemble method, ensemble can maintain advantages while get the best classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8.">Cross-training</head><p>In order to test the proposed method is robust for training set, the following cross-training experiment has been carried out.</p><p>Specifically, both feature extractor and classifier are trained on one same database while tested on another. Cross-training results on DRIVE and STARE in terms of sensitivity, specificity, accuracy, and AUC are shown in Table <ref type="table" target="#tab_9">10</ref>. Performance comparison results with cross-training in terms of average accuracy are shown in Table <ref type="table" target="#tab_1">11</ref>. Results in Tables <ref type="table" target="#tab_9">10</ref> and<ref type="table" target="#tab_1">11</ref> further prove that the proposed method is independent of training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we proposed a hierarchical retinal blood vessel segmentation method based on feature and ensemble learning. The proposed method has several unique characteristics. First, our features are extracted using not only the last layer output but also the intermediate output of CNN and therefore contain multiscale information of the geometric structure of the retina. Second, we are the first to introduce random forest into retinal blood vessel segmentation, and employ winner-takes-all as the classifier ensemble method. Third, the whole pipeline of the proposed method is automatic and trainable, which is accomplished by a combination of feature learning and ensemble learning. Fourth, our method was validated using two publically available databases and shown to outperform state-of-the-art. Finally, our method was shown to better handle the challenges in retinal vessel segmentation. This is because CNN is able to extract scale and rotational invariant features and RF is well-known for strong generalization capability.</p><p>Training time for one epoch is approximately 120 min for CNN. Since CNN is trained for 100 epochs, the total training time is about 8 days. Our future work would first focus on improving the computational time for training of CNN. To achieve this goal, we plan to deploy Genetic Algorithm (GA) for the parametric optimization of CNN. Moreover, we are interested in trying the maxpooling CNN <ref type="bibr" target="#b35">[36]</ref> as the hierarchical feature extractor and comparing with CNN. We also have a plan to apply the proposed method to other biomedical images, e.g. membrane segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 11</head><p>Performance comparison of results with cross training in terms of average accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Drive (trained on STARE) STARE (trained on DRIVE)</p><p>Soares <ref type="bibr" target="#b19">[20]</ref> 0.9397 0.9327 Ricci <ref type="bibr" target="#b20">[21]</ref> 0.9266 0.9464 Marin <ref type="bibr" target="#b22">[23]</ref> 0.9448 0.9528 Fraz <ref type="bibr" target="#b5">[6]</ref> 0.9456 0.9493 Proposed Method 0.9803 0.9710 </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>methods utilizing ground truth data usually consist of two stages: (1) feature extraction, (2) classification. During feature extraction stage, the features can be obtained by means of two ways: hand-crafted features and learning based features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Schematic diagram of convolutional neural network.</figDesc><graphic coords="2,112.46,599.21,350.21,124.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Visualization of learned hierarchical level features. (a) Original image. (b) Green channel of RGB colored image. (c) Enhanced green channel of RGB colored image. (d)-(h) Learned features from S2. (i)-(m) Learned features from S4. (n)-(r) Learned features from C5.</figDesc><graphic coords="4,82.56,343.21,420.48,380.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Segmentation results for the DRIVE database.</figDesc><graphic coords="6,333.08,306.41,208.62,418.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Segmentation results for the STARE database.</figDesc><graphic coords="7,73.46,375.89,209.09,351.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="3,94.52,483.95,398.23,239.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Inputi mage C1 S2 C3 S4 C5 output Result 1 Result 2 Result 3 Ensemble Final Result Random Forest Random Forest Random Forest Feature Vector1 Feature Vector2 Feature Vector3</head><label></label><figDesc></figDesc><table /><note><p>Fig. 2. Structure of the proposed method. Convolutional layer feature maps are indexed by C1, C3, and C5. Subsampling layer feature maps are indexed by S2 and S4.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 6</head><label>1</label><figDesc>-layer architecture for network (not counting the input), window¼ 25 Â 25).</figDesc><table><row><cell cols="2">Layer Type</cell><cell>Maps and neurons</cell><cell>Kernel</cell><cell>Learned feature</cell></row><row><cell></cell><cell></cell><cell></cell><cell>size</cell><cell>dimension</cell></row><row><cell>0</cell><cell>input</cell><cell>1 map of 25 Â 25</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>neurons</cell><cell></cell><cell></cell></row><row><cell>1</cell><cell>convolution</cell><cell>12 maps of 22 Â 22</cell><cell>4 Â 4</cell><cell>5808</cell></row><row><cell></cell><cell></cell><cell>neurons</cell><cell></cell><cell></cell></row><row><cell>2</cell><cell cols="2">subsampling 12 maps of 11 Â 11</cell><cell>2 Â 2</cell><cell>1452</cell></row><row><cell></cell><cell></cell><cell>neurons</cell><cell></cell><cell></cell></row><row><cell>3</cell><cell>convolution</cell><cell>12 maps of 8 Â 8</cell><cell>4 Â 4</cell><cell>7 6 8</cell></row><row><cell></cell><cell></cell><cell>neurons</cell><cell></cell><cell></cell></row><row><cell>4</cell><cell cols="2">subsampling 12 maps of 4 Â 4</cell><cell>2 Â 2</cell><cell>1 9 2</cell></row><row><cell></cell><cell></cell><cell>neurons</cell><cell></cell><cell></cell></row><row><cell>5</cell><cell cols="2">convolution 100 neurons</cell><cell>4 Â 4</cell><cell>1 0 0</cell></row><row><cell>6</cell><cell>output</cell><cell>1 neuron</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>Performance metrics for retinal vessel segmentation.</figDesc><table><row><cell>Measure</cell><cell>Description</cell></row><row><cell>TPR</cell><cell>TP/vessel pixel count</cell></row><row><cell>FPR</cell><cell>FP/non-vessel pixel count</cell></row><row><cell>Sensitivity(SN)</cell><cell>TPR or TP/(TP þFN)</cell></row><row><cell>Specificity(SP)</cell><cell>1-FPR or TN/(TN þ FP)</cell></row><row><cell>Accuracy(Acc)</cell><cell>(TPþ TN)/FOV pixel count</cell></row><row><cell>AUC</cell><cell>Area under the ROC curve</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc>Performance results on DRIVE Database images.</figDesc><table><row><cell>Image</cell><cell>SN</cell><cell>SP</cell><cell>Acc</cell><cell>AUC</cell></row><row><cell>1</cell><cell>0.8787</cell><cell>0.9818</cell><cell>0.9842</cell><cell>0.9637</cell></row><row><cell>2</cell><cell>0.8808</cell><cell>0.9793</cell><cell>0.9824</cell><cell>0.9651</cell></row><row><cell>3</cell><cell>0.8206</cell><cell>0.9694</cell><cell>0.9739</cell><cell>0.9352</cell></row><row><cell>4</cell><cell>0.8495</cell><cell>0.9769</cell><cell>0.9800</cell><cell>0.9505</cell></row><row><cell>5</cell><cell>0.7962</cell><cell>0.9681</cell><cell>0.9724</cell><cell>0.9347</cell></row><row><cell>6</cell><cell>0.7449</cell><cell>0.9581</cell><cell>0.9640</cell><cell>0.9430</cell></row><row><cell>7</cell><cell>0.8168</cell><cell>0.9721</cell><cell>0.9758</cell><cell>0.9382</cell></row><row><cell>8</cell><cell>0.8000</cell><cell>0.9715</cell><cell>0.9751</cell><cell>0.9415</cell></row><row><cell>9</cell><cell>0.6674</cell><cell>0.9557</cell><cell>0.9609</cell><cell>0.9385</cell></row><row><cell>10</cell><cell>0.8213</cell><cell>0.9758</cell><cell>0.9787</cell><cell>0.9434</cell></row><row><cell>11</cell><cell>0.8467</cell><cell>0.9772</cell><cell>0.9801</cell><cell>0.9367</cell></row><row><cell>12</cell><cell>0.8493</cell><cell>0.9784</cell><cell>0.9811</cell><cell>0.9601</cell></row><row><cell>13</cell><cell>0.7556</cell><cell>0.9597</cell><cell>0.9654</cell><cell>0.9432</cell></row><row><cell>14</cell><cell>0.8835</cell><cell>0.9846</cell><cell>0.9864</cell><cell>0.9626</cell></row><row><cell>15</cell><cell>0.8503</cell><cell>0.9827</cell><cell>0.9845</cell><cell>0.9301</cell></row><row><cell>16</cell><cell>0.7747</cell><cell>0.9661</cell><cell>0.9705</cell><cell>0.9489</cell></row><row><cell>17</cell><cell>0.7129</cell><cell>0.9596</cell><cell>0.9646</cell><cell>0.9276</cell></row><row><cell>18</cell><cell>0.8116</cell><cell>0.9755</cell><cell>0.9784</cell><cell>0.9545</cell></row><row><cell>19</cell><cell>0.9032</cell><cell>0.9868</cell><cell>0.9884</cell><cell>0.9605</cell></row><row><cell>20</cell><cell>0.8821</cell><cell>0.9859</cell><cell>0.9874</cell><cell>0.9709</cell></row><row><cell>Average</cell><cell>0.8173</cell><cell>0.9733</cell><cell>0.9767</cell><cell>0.9475</cell></row></table><note><p>a Bold fonts denote worst case accuracy, best case accuracy, and average result, respectively.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc>Performance results on STARE Database images.</figDesc><table><row><cell>Image</cell><cell>SN</cell><cell>SP</cell><cell>Acc</cell><cell>AUC</cell></row><row><cell>1</cell><cell>0.7985</cell><cell>0.9754</cell><cell>0.9781</cell><cell>0.9695</cell></row><row><cell>2</cell><cell>0.7515</cell><cell>0.9749</cell><cell>0.9772</cell><cell>0.9338</cell></row><row><cell>3</cell><cell>0.8502</cell><cell>0.9867</cell><cell>0.9878</cell><cell>0.9687</cell></row><row><cell>4</cell><cell>0.5483</cell><cell>0.9481</cell><cell>0.9535</cell><cell>0.9623</cell></row><row><cell>5</cell><cell>0.8246</cell><cell>0.9751</cell><cell>0.9782</cell><cell>0.9611</cell></row><row><cell>6</cell><cell>0.8833</cell><cell>0.9887</cell><cell>0.9897</cell><cell>0.9839</cell></row><row><cell>7</cell><cell>0.9416</cell><cell>0.9937</cell><cell>0.9944</cell><cell>0.9896</cell></row><row><cell>8</cell><cell>0.9447</cell><cell>0.9943</cell><cell>0.9949</cell><cell>0.9884</cell></row><row><cell>9</cell><cell>0.8955</cell><cell>0.9879</cell><cell>0.9892</cell><cell>0.9872</cell></row><row><cell>10</cell><cell>0.8879</cell><cell>0.9866</cell><cell>0.9880</cell><cell>0.9713</cell></row><row><cell>11</cell><cell>0.8918</cell><cell>0.9884</cell><cell>0.9895</cell><cell>0.9762</cell></row><row><cell>12</cell><cell>0.9107</cell><cell>0.9896</cell><cell>0.9907</cell><cell>0.9874</cell></row><row><cell>13</cell><cell>0.8789</cell><cell>0.9841</cell><cell>0.9860</cell><cell>0.9862</cell></row><row><cell>14</cell><cell>0.8592</cell><cell>0.9802</cell><cell>0.9827</cell><cell>0.9849</cell></row><row><cell>15</cell><cell>0.7851</cell><cell>0.9718</cell><cell>0.9751</cell><cell>0.9753</cell></row><row><cell>16</cell><cell>0.7387</cell><cell>0.9573</cell><cell>0.9633</cell><cell>0.9766</cell></row><row><cell>17</cell><cell>0.8889</cell><cell>0.9847</cell><cell>0.9866</cell><cell>0.9799</cell></row><row><cell>18</cell><cell>0.6451</cell><cell>0.9730</cell><cell>0.9749</cell><cell>0.9645</cell></row><row><cell>19</cell><cell>0.6311</cell><cell>0.9770</cell><cell>0.9783</cell><cell>0.9780</cell></row><row><cell>20</cell><cell>0.6523</cell><cell>0.9645</cell><cell>0.9678</cell><cell>0.9765</cell></row><row><cell>Average</cell><cell>0.8104</cell><cell>0.9791</cell><cell>0.9813</cell><cell>0.9751</cell></row></table><note><p>a Bold fonts denote worst case accuracy, best case accuracy, and average result, respectively.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5</head><label>5</label><figDesc>Performance measures on DRIVE and STARE.</figDesc><table><row><cell>Database</cell><cell>Segmentation</cell><cell>SN</cell><cell>SP</cell><cell>Acc</cell><cell>AUC</cell></row><row><cell>DRIVE</cell><cell>2nd human observer</cell><cell>0.7796</cell><cell>0.9717</cell><cell>0.9464</cell><cell>-</cell></row><row><cell></cell><cell>Proposed Method</cell><cell>0.8173</cell><cell>0.9733</cell><cell>0.9767</cell><cell>0.9475</cell></row><row><cell>STARE</cell><cell>2nd human observer</cell><cell>0.8955</cell><cell>0.9382</cell><cell>0.9347</cell><cell>-</cell></row><row><cell></cell><cell>Proposed Method</cell><cell>0.8104</cell><cell>0.9791</cell><cell>0.9813</cell><cell>0.9751</cell></row><row><cell></cell><cell>Best case accuracy</cell><cell></cell><cell cols="2">Worst case accuracy</cell><cell></cell></row><row><cell>Original image</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Probability map image</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Segmentation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>First human observer</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6</head><label>6</label><figDesc>Comparison of segmentation results for individual and ensemble classifier on DRIVE images.</figDesc><table><row><cell>Method</cell><cell>SN</cell><cell>SP</cell><cell>Acc</cell><cell>AUC</cell></row><row><cell>CNN</cell><cell>0.6425</cell><cell>0.9478</cell><cell>0.9545</cell><cell>0.9378</cell></row><row><cell>RF-1</cell><cell>0.8173</cell><cell>0.9733</cell><cell>0.9767</cell><cell>0.9475</cell></row><row><cell>RF-2</cell><cell>0.7899</cell><cell>0.9692</cell><cell>0.9731</cell><cell>0.9476</cell></row><row><cell>RF-3</cell><cell>0.7576</cell><cell>0.9644</cell><cell>0.9690</cell><cell>0.9490</cell></row><row><cell>Average ensemble</cell><cell>0.7787</cell><cell>0.9676</cell><cell>0.9718</cell><cell>0.9521</cell></row><row><cell>Weighted ensemble one</cell><cell>0.7983</cell><cell>0.9704</cell><cell>0.9742</cell><cell>0.9514</cell></row><row><cell>Weighted ensemble two</cell><cell>0.8074</cell><cell>0.9717</cell><cell>0.9754</cell><cell>0.9502</cell></row><row><cell>Median ensemble</cell><cell>0.7959</cell><cell>0.9700</cell><cell>0.9739</cell><cell>0.9506</cell></row><row><cell>Winner-takes-all</cell><cell>0.8173</cell><cell>0.9733</cell><cell>0.9767</cell><cell>0.9475</cell></row><row><cell>Table 7</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Comparison of segmentation results for individual and ensemble classifier on</cell></row><row><cell>STARE images.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>SN</cell><cell>SP</cell><cell>Acc</cell><cell>AUC</cell></row><row><cell>CNN</cell><cell>0.6648</cell><cell>0.9626</cell><cell>0.9666</cell><cell>0.9364</cell></row><row><cell>RF-1</cell><cell>0.8104</cell><cell>0.9791</cell><cell>0.9813</cell><cell>0.9751</cell></row><row><cell>RF-2</cell><cell>0.7986</cell><cell>0.9778</cell><cell>0.9801</cell><cell>0.9768</cell></row><row><cell>RF-3</cell><cell>0.7714</cell><cell>0.9746</cell><cell>0.9773</cell><cell>0.9706</cell></row><row><cell>Average ensemble</cell><cell>0.7974</cell><cell>0.9777</cell><cell>0.9800</cell><cell>0.9777</cell></row><row><cell>Weighted ensemble one</cell><cell>0.8036</cell><cell>0.9783</cell><cell>0.9806</cell><cell>0.9771</cell></row><row><cell>Weighted ensemble two</cell><cell>0.8076</cell><cell>0.9788</cell><cell>0.9810</cell><cell>0.9777</cell></row><row><cell>Median ensemble</cell><cell>0.8009</cell><cell>0.9775</cell><cell>0.9804</cell><cell>0.9781</cell></row><row><cell>Winner-takes-all</cell><cell>0.8104</cell><cell>0.9791</cell><cell>0.9813</cell><cell>0.9751</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8</head><label>8</label><figDesc>Performance comparison of vessel segmentation methods on DRIVE images.</figDesc><table><row><cell>No</cell><cell>Type</cell><cell>Methods</cell><cell>Year</cell><cell>SN</cell><cell>SP</cell><cell>Acc</cell><cell>AUC</cell></row><row><cell>1.</cell><cell></cell><cell>2nd human observer</cell><cell>-</cell><cell>0.7796</cell><cell>0.9717</cell><cell>0.9470</cell><cell>N.A</cell></row><row><cell>2.</cell><cell>Unsupervised methods</cell><cell>Zana [8]</cell><cell>2001</cell><cell>0.6971</cell><cell>N.A</cell><cell>0.9377</cell><cell>0.8984</cell></row><row><cell>3.</cell><cell></cell><cell>Jiang [33]</cell><cell>2003</cell><cell>N.A</cell><cell>N.A</cell><cell>0.9212</cell><cell>0.9114</cell></row><row><cell>4.</cell><cell></cell><cell>Mendonca [9]</cell><cell>2006</cell><cell>0.7344</cell><cell>0.9764</cell><cell>0.9452</cell><cell>N.A</cell></row><row><cell>5.</cell><cell></cell><cell>Al-Diri [17]</cell><cell>2009</cell><cell>0.7282</cell><cell>0.9551</cell><cell>N.A</cell><cell>N.A</cell></row><row><cell>6.</cell><cell></cell><cell>Lam [15]</cell><cell>2010</cell><cell>N.A</cell><cell>N.A</cell><cell>0.9472</cell><cell>0.9614</cell></row><row><cell>7.</cell><cell></cell><cell>Miri [34]</cell><cell>2011</cell><cell>0.7352</cell><cell>0.9795</cell><cell>0.9458</cell><cell>N.A</cell></row><row><cell>8.</cell><cell></cell><cell>Fraz [10]</cell><cell>2011</cell><cell>0.7152</cell><cell>0.9759</cell><cell>0.9430</cell><cell>N.A</cell></row><row><cell>9.</cell><cell></cell><cell>You [35]</cell><cell>2011</cell><cell>0.7410</cell><cell>0.9751</cell><cell>0.9434</cell><cell>N.A</cell></row><row><cell>10.</cell><cell>Supervised methods</cell><cell>Niemeijer [19]</cell><cell>2004</cell><cell>N.A</cell><cell>N.A</cell><cell>0.9416</cell><cell>0.9294</cell></row><row><cell>11.</cell><cell></cell><cell>Soares [20]</cell><cell>2006</cell><cell>0.7332</cell><cell>0.9782</cell><cell>0.9461</cell><cell>0.9614</cell></row><row><cell>12.</cell><cell></cell><cell>Staal [5]</cell><cell>2004</cell><cell>N.A</cell><cell>N.A</cell><cell>0.9441</cell><cell>0.9520</cell></row><row><cell>13.</cell><cell></cell><cell>Ricci [21]</cell><cell>2007</cell><cell>N.A</cell><cell>N.A</cell><cell>0.9595</cell><cell>0.9558</cell></row><row><cell>14.</cell><cell></cell><cell>Lupascu [22]</cell><cell>2010</cell><cell>0.7200</cell><cell>N.A</cell><cell>0.9597</cell><cell>0.9561</cell></row><row><cell>15.</cell><cell></cell><cell>Marin [23]</cell><cell>2011</cell><cell>0.7067</cell><cell>0.9801</cell><cell>0.9452</cell><cell>0.9588</cell></row><row><cell>16.</cell><cell></cell><cell>Fraz [6]</cell><cell>2012</cell><cell>0.7406</cell><cell>0.9807</cell><cell>0.9480</cell><cell>0.9747</cell></row><row><cell>17.</cell><cell></cell><cell>Proposed Method</cell><cell>2013</cell><cell>0.8173</cell><cell>0.9733</cell><cell>0.9767</cell><cell>0.9475</cell></row></table><note><p>a N.A¼ Not Available.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9</head><label>9</label><figDesc>Performance comparison of vessel segmentation methods on stare images.</figDesc><table><row><cell>No</cell><cell>Type</cell><cell>Methods</cell><cell>Year</cell><cell>SN</cell><cell>SP</cell><cell>Acc</cell><cell>AUC</cell></row><row><cell>1.</cell><cell></cell><cell>2nd human observer</cell><cell>-</cell><cell>0.8951</cell><cell>0.9384</cell><cell>0.9348</cell><cell>-</cell></row><row><cell>2.</cell><cell>Unsupervised methods</cell><cell>Hoover [1]</cell><cell>2000</cell><cell>0.6747</cell><cell>0.9565</cell><cell>0.9264</cell><cell>N.A</cell></row><row><cell>3.</cell><cell></cell><cell>Jiang [33]</cell><cell>2003</cell><cell>N.A</cell><cell>N.A</cell><cell>0.9009</cell><cell>N.A</cell></row><row><cell>4.</cell><cell></cell><cell>Mendonca [9]</cell><cell>2006</cell><cell>0.6996</cell><cell>0.9730</cell><cell>0.9440</cell><cell>N.A</cell></row><row><cell>5.</cell><cell></cell><cell>Lam [14]</cell><cell>2008</cell><cell>N.A</cell><cell>N.A</cell><cell>0.9474</cell><cell>0.9392</cell></row><row><cell>6.</cell><cell></cell><cell>Al-Diri [17]</cell><cell>2009</cell><cell>0.7521</cell><cell>0.9681</cell><cell>N.A</cell><cell>N.A</cell></row><row><cell>7.</cell><cell></cell><cell>Lam [15]</cell><cell>2010</cell><cell>N.A</cell><cell>N.A</cell><cell>0.9567</cell><cell>0.9739</cell></row><row><cell>8.</cell><cell></cell><cell>Fraz [10]</cell><cell>2011</cell><cell>0.7311</cell><cell>0.9680</cell><cell>0.9442</cell><cell>N.A</cell></row><row><cell>9.</cell><cell></cell><cell>You [35]</cell><cell>2011</cell><cell>0.7260</cell><cell>0.9756</cell><cell>0.9497</cell><cell>N.A</cell></row><row><cell>10.</cell><cell>Supervised methods</cell><cell>Staal [5]</cell><cell>2004</cell><cell>N.A</cell><cell>N.A</cell><cell>0.9516</cell><cell>0.9614</cell></row><row><cell>11.</cell><cell></cell><cell>Soares[20]</cell><cell>2006</cell><cell>0.7207</cell><cell>0.9747</cell><cell>0.9479</cell><cell>0.9671</cell></row><row><cell>12.</cell><cell></cell><cell>Ricci [21]</cell><cell>2007</cell><cell>N.A</cell><cell>N.A</cell><cell>0.9584</cell><cell>0.9602</cell></row><row><cell>13.</cell><cell></cell><cell>Marin [23]</cell><cell>2011</cell><cell>0.6944</cell><cell>0.9819</cell><cell>0.9526</cell><cell>0.9769</cell></row><row><cell>14.</cell><cell></cell><cell>Fraz [6]</cell><cell>2012</cell><cell>0.7548</cell><cell>0.9763</cell><cell>0.9534</cell><cell>0.9768</cell></row><row><cell>15.</cell><cell></cell><cell>Proposed Method</cell><cell>2013</cell><cell>0.8104</cell><cell>0.9791</cell><cell>0.9813</cell><cell>0.9751</cell></row></table><note><p>a N.A¼ Not Available.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10</head><label>10</label><figDesc>Cross-training results on drive and stare.</figDesc><table><row><cell></cell><cell>SN</cell><cell>SP</cell><cell>Acc</cell><cell>AUC</cell></row><row><cell>Drive (trained on STARE)</cell><cell>0.8431</cell><cell>0.9774</cell><cell>0.9803</cell><cell>0.9271</cell></row><row><cell>STARE(trained on DRIVE)</cell><cell>0.7116</cell><cell>0.9675</cell><cell>0.9710</cell><cell>0.9391</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Please cite this article as: S. Wang, et al., Hierarchical retinal blood vessel segmentation based on feature and ensemble learning, Neurocomputing (2014), http://dx.doi.org/10.1016/j.neucom.2014.07.059i</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would particularly like to thank the anonymous reviewers for their helpful suggestions. This work is supported by NSFC Joint Fund with Guangdong under Key Project U1201258, Program for New Century Excellent Talents in University of Ministry of Education of China (NCET-11-0315) and Shandong Natural Science Funds for Distinguished Young Scholar under Grant no. JQ201316.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Locating blood vessels in retinal images by piecewise threshold probing of a matched filter response</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hoover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kouznetsova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Goldbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="203" to="210" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Blood vessel segmentation methodologies in retinal images-A survey</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Fraz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Remagnino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Uyyanonvara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Rudnicka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Owen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Barman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Methods Progr. Biomed</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="407" to="433" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Convolutional networks for images, speech, and time series</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">3361</biblScope>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
	<note>The Handbook of Brain Theory and</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Random forests</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Ridgebased vessel segmentation in color images of the retina</title>
		<author>
			<persName><forename type="first">J</forename><surname>Staal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Abràmoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Niemeijer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Viergever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="501" to="509" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An ensemble classification-based approach applied to retinal blood vessel segmentation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Fraz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Remagnino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Uyyanonvara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Rudnicka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Owen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Barman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="2538" to="2548" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Detection and measurement of retinal vessels in fundus images using amplitude modified second-order Gaussian filter</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Chutatape</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="168" to="172" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Segmentation of vessel-like patterns using mathematical morphology and curvature evaluation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-C</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1010" to="1019" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Segmentation of retinal blood vessels by combining the detection of centerlines and morphological reconstruction</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Mendonca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Campilho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1200" to="1213" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An approach to localize the retinal blood vessels using bit planes and centerline detection</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Fraz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Barman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Remagnino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Basit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Uyyanonvara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Rudnicka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Owen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Methods Progr. Biomed</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="600" to="616" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Vessel extraction in medical images by wave-propagation and traceback</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">K</forename><surname>Quek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kirbas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="117" to="131" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multiscale vessel enhancement filtering</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Niessen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Vincken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Viergever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Computing and Computer-Assisted Interventation-MICCAI</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page" from="130" to="137" />
			<date type="published" when="1998">1998</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Analysis of retinal vasculature using a multiresolution hermite model</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bhalerao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="137" to="152" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A novel vessel segmentation algorithm for pathological retina images based on the divergence of vector fields</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="237" to="246" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">General retinal vessel segmentation using regularization-based multiconcavity modeling</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-C</forename><surname>Liew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1369" to="1381" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automatic identification of retinal arteries and veins from dual-wavelength images using structural and functional features</title>
		<author>
			<persName><forename type="first">H</forename><surname>Narasimha-Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Beach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Khoobehi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Roysam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="1427" to="1435" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An active contour model for segmenting and measuring retinal vessels</title>
		<author>
			<persName><forename type="first">B</forename><surname>Al-Diri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hunter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Steel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1488" to="1497" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Vessel extraction under non-uniform illumination: a level set approach</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Y</forename><surname>Cheung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="358" to="360" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Comparative study of retinal vessel segmentation methods on a new publicly available database</title>
		<author>
			<persName><forename type="first">M</forename><surname>Niemeijer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Staal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Loog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Abramoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Imaging</title>
		<imprint>
			<biblScope unit="page" from="648" to="656" />
			<date type="published" when="2004">2004</date>
			<publisher>International Society for Optics and Photonics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Retinal vessel segmentation using the 2-D Gabor wavelet and supervised classification</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Leandro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Cesar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">F</forename><surname>Jelinek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Cree</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1214" to="1222" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Retinal blood vessel segmentation using line operators and support vector classification</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Perfetti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1357" to="1365" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">FABC: retinal vessel segmentation using AdaBoost</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Lupascu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tegolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Trucco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Technol. Biomed</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1267" to="1274" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A new supervised method for blood vessel segmentation in retinal images by using gray-level and moment invariants-based features</title>
		<author>
			<persName><forename type="first">D</forename><surname>Marín</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aquino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Gegúndez-Arias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Bravo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="146" to="158" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Back-propagation network and its configuration for blood vessel detection in angiograms</title>
		<author>
			<persName><forename type="first">R</forename><surname>Nekovei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="64" to="72" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hierarchical level features based trainable segmentation for electron microscopy images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomed. Eng. Online</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">59</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A fully automated approach to segmentation of irregularly shaped cellular structures in EM images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Computing and Computer-Assisted Intervention-MICCAI</title>
		<imprint>
			<biblScope unit="volume">2010</biblScope>
			<biblScope unit="page" from="463" to="471" />
			<date type="published" when="2010">2010</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">SLIC superpixels compared to state-of-the-art superpixel methods</title>
		<author>
			<persName><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="2274" to="2282" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Detection of neuron membranes in electron microscopy images using a serial neural network architecture</title>
		<author>
			<persName><forename type="first">E</forename><surname>Jurrus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Paiva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Whitaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Jorgensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tasdizen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="770" to="783" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Ensemble based systems in decision making</title>
		<author>
			<persName><forename type="first">R</forename><surname>Polikar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Circuits Syst. Mag</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="21" to="45" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<author>
			<persName><forename type="first">N</forename><surname>Otsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A threshold selection method from gray-level histograms</title>
		<imprint>
			<date type="published" when="1975">1975</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="23" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Retinal vessel segmentation using multiwavelet kernels and multiscale hierarchical decomposition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Trucco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="2117" to="2133" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Adaptive local thresholding by verification-based multithreshold probing with application to vessel detection in retinal images</title>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mojon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="131" to="137" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Retinal image analysis using curvelet transform and multistructure elements morphology by reconstruction</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Miri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mahloojifar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="1183" to="1192" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Segmentation of retinal blood vessels using the radial projection and semi-supervised approach</title>
		<author>
			<persName><forename type="first">X</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>-M. Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="2314" to="2324" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Max-pooling convolutional neural networks for vision-based hand gesture recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Nagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ducatelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Di Caro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Giusti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Gambardella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Signal and Image Processing Applications (ICSIPA)</title>
		<meeting>the IEEE International Conference on Signal and Image Processing Applications (ICSIPA)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="page" from="342" to="347" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
