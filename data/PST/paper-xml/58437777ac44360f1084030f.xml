<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards perspective-free object counting with deep learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Oñoro-Rubio</surname></persName>
							<email>daniel.onoro@edu.uah.es</email>
							<affiliation key="aff0">
								<orgName type="department">GRAM</orgName>
								<orgName type="institution">University of Alcalá</orgName>
								<address>
									<settlement>Alcalá de Henares</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Roberto</forename><forename type="middle">J</forename><surname>López-Sastre</surname></persName>
							<email>robertoj.lopez@uah.es</email>
							<affiliation key="aff0">
								<orgName type="department">GRAM</orgName>
								<orgName type="institution">University of Alcalá</orgName>
								<address>
									<settlement>Alcalá de Henares</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Towards perspective-free object counting with deep learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we address the problem of counting objects instances in images. Our models are able to precisely estimate the number of vehicles in a traffic congestion, or to count the humans in a very crowded scene. Our first contribution is the proposal of a novel convolutional neural network solution, named Counting CNN (CCNN). Essentially, the CCNN is formulated as a regression model where the network learns how to map the appearance of the image patches to their corresponding object density maps. Our second contribution consists in a scale-aware counting model, the Hydra CNN, able to estimate object densities in different very crowded scenarios where no geometric information of the scene can be provided. Hydra CNN learns a multiscale non-linear regression model which uses a pyramid of image patches extracted at multiple scales to perform the final density prediction. We report an extensive experimental evaluation, using up to three different object counting benchmarks, where we show how our solutions achieve a state-of-the-art performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Take an image of a crowded scene, or of a traffic jam. We address here the hard problem of accurately counting the objects instances in these scenarios. To develop this type of ideas makes possible to build applications that span from solutions to improve security in stadiums, to systems that precisely monitor how the traffic congestions evolve.</p><p>Note that the covered applications define the typical scenarios where individual object detectors (e.g. <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>) do not work reliably. The reasons are: the extreme overlap of objects, the size of the instances, scene perspective, etc. Thus, approaches modeling the counting problem as one of object density estimation have been systematically defining the state-of-the-art <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref>. For this reason, we propose here two deep learning models for object density map estimation.</p><p>As illustrated in Figure <ref type="figure" target="#fig_0">1</ref>, we tackle the counting problem proposing deep learning architectures able to learn the regression function that projects the image appearance into an object density map. This allows the derivation of an estimated object density map for unseen images.</p><p>The main contributions of this work are as follows. First, in Section 3.2, we propose a novel deep network architecture, named Counting CNN (CCNN), which is an efficient fully-convolutional neural network able to perform an accurate regression of object density maps from image patches. Second, we show that object densities can be estimated without the need of any perspective map or other geometric information of the scene, in contrast to most of the stateof-the-art methods <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref>, which require this information. Thus, we introduce in Section 3.3 the Hydra CNN architecture, a scale-aware model, which works learning a multiscale regressor for mapping the appearance of a pyramid of multiple scale patches to an object density map. Like the mythological Hydra creature, each head of our Hydra learns the feature representation for a particular scale of the pyramid. Then, all these head features are concatenated and passed through a set of fully-connected layers, forming the body of the Hydra, which is in charge of learning the high-dimensional representation which performs the final density estimation. Third, in Section 4, we report a thorough experimental validation of the proposed models. Three publicly available datasets are used, two for crowd counting <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9]</ref> and one for vehicle counting <ref type="bibr" target="#b9">[10]</ref>. We show how our solutions report state-of-the-art results in all these heterogeneous scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Significant progress has been made to count objects in images. We refer the reader to the survey of Loy et al. <ref type="bibr" target="#b7">[8]</ref>. Following the taxonomy introduced in <ref type="bibr" target="#b7">[8]</ref>, the algorithms can be classified into three groups: counting by detection <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref>, counting by clustering <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>, and counting by regression <ref type="bibr">[3-7, 19, 20]</ref>.</p><p>Here we focus the review of the literature on the counting by regression models, because our approaches belong to this group too. But also because these approaches have so far been more accurate and faster, compared to the other groups, defining the state-of-the-art results in most of the benchmarks. Essentially, these methods work defining a mapping from the input image features to the object count. A special attention deserves the learning-to-count model of Lempitsky et al. <ref type="bibr" target="#b5">[6]</ref>. They introduce a counting approach, which works by learning a linear mapping from local image features to object density maps. With a successful learning, one can provide the object count by simply integrating over regions in the estimated density map. This strategy is followed also in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b19">20]</ref> where a structured learning framework is applied to the random forests so as to obtain the object density map estimations. In <ref type="bibr" target="#b2">[3]</ref>, the authors propose an interactive counting system, which simplifies the costly learning-to-count approach <ref type="bibr" target="#b5">[6]</ref>, proposing the use of a simple ridge regressor.</p><p>Our models also treat the counting problem as an object density estimation task, but they are deep learning based approaches which significantly differ from these previous works. To the best of our knowledge, only two works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b20">21]</ref> have addressed the object counting problem with deep learning architectures. In <ref type="bibr" target="#b20">[21]</ref> a multi-column CNN is proposed, which stacks the features maps generated by filters of different sizes and combine them to generate the final prediction for the count. Zhang et al. <ref type="bibr" target="#b6">[7]</ref> propose a CNN architecture to predict density maps, which needs to be trained following a switchable learning process that uses two different loss functions. Moreover, for the crowd counting problem they do not use the direct density estimation of the network. Instead, they use the output of the network as features to fit a ridge regressor that actually performs the final density estimation. Our models are different. First, the network architectures do not coincide. And second, we do not need to either integrate two losses or to use an extra regressor: the object density map is the direct output of our networks, which are trained with a single regression loss.</p><p>3 Deep learning to count objects</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Counting objects model</head><p>Let us first formalize our notation and counting objects methodology. In this work, we model the counting problem as one of object density estimation <ref type="bibr" target="#b5">[6]</ref>.</p><p>Our solutions require a set of annotated images, where all the objects are marked by dots. In this scenario, the ground truth density map D I , for an image I, is defined as a sum of Gaussian functions centered on each dot annotation,</p><formula xml:id="formula_0">D I (p) = µ∈A I N (p; µ, Σ) ,<label>(1)</label></formula><p>where A I is the set of 2D points annotated for the image I, and N (p; µ, Σ) represents the evaluation of a normalized 2D Gaussian function, with mean µ and isotropic covariance matrix Σ, evaluated at pixel position defined by p. With this density map D I , the total object count N I can be directly obtained by integrating the density map values in D I over the entire image, as follows,</p><formula xml:id="formula_1">N I = p∈I D I (p). (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>Note that all the Gaussian are summed, so the total object count is preserved even when there is overlap between objects.</p><p>Given this object counting model, the main objective of our work is to design deep learning architectures able to learn the non-linear regression function R that takes an image patch P as an input, and returns an object density map prediction D (P ) pred ,</p><formula xml:id="formula_3">D (P ) pred = R(P |Ω) , (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>where Ω is the set of parameters of the CNN model. For the image patch P ∈ R h×w×c , h,w and c correspond to the height, width and number of channels of the patch, respectively. In the density prediction</p><formula xml:id="formula_5">D (P )</formula><p>pred ∈ R h ×w , h and w represent the height and width of the predicted map. Thus, given an unseen test image, our model densely extracts image patches from it, and generates their corresponding object density maps, which are aggregated into a density map for the whole test image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Counting CNN</head><p>We introduce in this section our first deep learning architecture, the Counting CNN (CCNN). It is shown in Figure <ref type="figure" target="#fig_1">2</ref>. Let us dissection it.</p><p>The architecture consists of 6 convolutional layers. Conv1 and Conv2 layers have filters of size 7x7 with a depth of 32, and they are followed by a max-pooling layer, with a 2x2 kernel size. The Conv3 layer has 5x5 filters with a depth of 64, and it is also followed by a max-pooling layer with another 2x2 kernel. Conv4 and Conv5 layers are made of 1x1 filters with a depth of 1000 and 400, respectively. Note that we do not integrate any fully-connected layer in the model. With these Conv4 and Conv5 layers, we propose a fully convolutional architecture <ref type="bibr" target="#b21">[22]</ref>. All the previous layers are followed by rectified linear units (ReLU). Finally, Conv6 is another 1x1 filter with a depth of 1. Conv6 is in charge of returning the density map estimation D (P ) pred for the input patch P . Like we specify in Equation ( <ref type="formula" target="#formula_3">3</ref>), we want our deep network to learn a nonlinear mapping from the appearance of an image patch to an object density map. Thus, our CCNN has to be trained to solve such a regression problem. For doing so, we connect to the Conv6 layer the following Euclidean regression loss,</p><formula xml:id="formula_6">l(Ω) = 1 2N N n=1 R(P n |Ω) − D (Pn) gt 2 2 ,<label>(4)</label></formula><p>where N corresponds to the number of patches in the training batch, and D (Pn) gt represents the ground-truth density for the associated training patch P n . Recall that Ω encodes the network parameters. We have implemented our network design using the excellent Caffe <ref type="bibr" target="#b22">[23]</ref> framework, and we make use of the popular stochastic gradient descent algorithm to fit the parameters of our models. How do we implement the prediction stage? Given a test image, we first densely extract image patches. As illustrated in Figure <ref type="figure" target="#fig_1">2</ref>, we feed the CCNN with image patches scaled to a fixed size of 72x72 pixels. These input patches are passed through our CCNN model, which produces a density map estimation for each of them. Note that due to the two max-pooling layers, the size of the output object density map estimation is 1/4 of the size of the input image patch, i.e. 18x18 pixels. Therefore, all the predicted object density maps D P pred = R(P |Ω) are rescaled in order to fit the original input patch size. Note that this rescaling generates a density map DP pred whose associated count does not necessarily match with the original count before the rescaling. Therefore, this new resized density map must be normalized as follows,</p><formula xml:id="formula_7">DP pred = ∀p D P pred (p) ∀p DP pred (p) DP pred .<label>(5)</label></formula><p>The last step of the prediction stage consists in the assembly of all the predicted density maps for the patches. In order to generate the final object density map estimation D It , for the given test image I t , we simply aggregate all the predictions obtained for all the extracted patches into a unique density map of the size of the test image (see Figure <ref type="figure" target="#fig_0">1</ref>). Note that due to the dense extraction of patches, the predictions will overlap, so each position of the final density map must be normalized by the number of patches that cast a prediction in it.</p><p>Like we have previously mentioned, we are not the first ones proposing a deep learning model for object counting. Zhang et al. <ref type="bibr" target="#b6">[7]</ref> introduce the novel Crowd CNN architecture. In a detailed comparison of both the CCNN and the Crowd CNN, we can discover the following differences. First, the network designs are different. For instance, instead of using fully-connected layers, in our CCNN we have incorporated the fully convolutional 1x1 layers Conv4, Conv5 and Conv6. This speeds up both the training a forwards pass <ref type="bibr" target="#b21">[22]</ref>. Second, their learning strategy is more complex. The Crowd CNN model needs to incorporate two different loss functions (one for the density maps and one for the total count of the patches). During the optimization, they implement an iterative switching process to alternatively optimize with one loss or the other. In contrast, our CCNN only uses one loss. And third, our model is more compact. For the problem of crowd counting, Zhang et al. <ref type="bibr" target="#b6">[7]</ref> do not use the direct estimation of the Crowd CNN network to obtain the final object density estimation. Instead, they report the results feeding a ridge regressor with the output features of their Crowd CNN network. On the contrary, we do not need any extra regressor, our novel CCNN is learned in an end-to-end manner to directly predict the object density maps. Finally, our experiments (see Section 4.2) reveal that the CCNN improves the results of the Crowd CNN in three of four subsets of the UCSD dataset <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The Hydra CNN</head><p>In a typical pipeline of a counting by regression model, a geometric correction of the input features, using an annotated perspective map of the scene, for instance, results fundamental to report accurate results. This phenomenon has been described in several works, reporting state-of-the-art results (e.g. <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7]</ref>). Technically, the perspective distortion exhibited by an image, causes that features extracted from the same object but at different scene depths would have a huge difference in values. As a consequence, erroneous results are expected by models which uses a single regression function.</p><p>With the Hydra CNN model, we want to solve this problem. That is, Hydra CNN must be a scale-aware architecture, which is not allowed to use any previous geometric correction of the scene. Our architecture should be able to learn a nonlinear regression mapping, able to integrate the information from multiple scales simultaneously, in order to cast a precise object density map estimation. This aspect brings a fundamental benefit: Hydra CNN can work in scenarios and datasets which consider not only a single calibrated scene. For instance, a single Hydra CNN model should be able to accurately predict the number of objects for a variety of unseen scenes, exhibiting different perspectives, and generalizing well to real-world scenarios.</p><p>We attack this problem with the idea shown in Figure <ref type="figure" target="#fig_2">3</ref>. Our Hydra CNN has several heads and a body, remembering the ancient serpentine water monster called the Hydra in Greek and Roman mythology. Each head is in charge of learning the representation for a particular scale s i from the input pyramid of image patches. Therefore, during learning we feed each head with image patches extracted at a particular scale. We have to understand the output of the heads as a set of features describing the images at different scales. Then, all these features are concatenated to feed the body, which is made of fully-connected layers. Notice, that the heads are not necessarily restricted to the same architecture, so their features may have different dimensions, hence the use of fully convolutional layers in the body may not be suitable. Therefore, we use fully-connected layer in order to provide to the net full access to all the head features for the different scales. Essentially, the body learns the high-dimensional representation that merges the multiscale information provided by the heads, and it is in charge of performing the final object density map estimation.</p><p>Technically, as illustrated in Figure <ref type="figure" target="#fig_2">3</ref>, for each head of the Hydra CNN, we propose to use a CCNN model (CCNN s0, . . . , CCNN sn). Note that we simply exclude in each CCNN model for the heads, its final Conv6 layer. Then, the outputs of the different heads are concatenated and passed to the body, where we use two fully-connected layers, with 512 neurons each one. These are the layers Fc6 and Fc7 in Figure <ref type="figure" target="#fig_2">3</ref>, which are followed by a ReLu and a dropout layer. We end the architecture with the fully-connected layer Fc8, with 324 neurons, whose output is the object density map. To train this Hydra CNN model we use the same loss function defined in Equation ( <ref type="formula" target="#formula_6">4</ref>). Again the Caffe <ref type="bibr" target="#b22">[23]</ref> library is used, following for the optimization the stochastic gradient descent algorithm. Finally, given a test image, we follow the same procedure described for the CCNN model to produce the final object density map estimation.</p><p>The network design of the novel Hydra CNN is inspired by the work of Li et al. <ref type="bibr" target="#b23">[24]</ref> for visual saliency estimation. In <ref type="bibr" target="#b23">[24]</ref>, they propose a different network architecture but using a multiple input strategy, which combines the features of different views of the whole input image in order to return a visual saliency map.</p><p>In our Hydra CNN model, we adapt this idea to use the multi-scale pyramid set of image patches to feed our network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We have evaluated our solutions using three challenging benchmarks. Two have been proposed for the crowd counting problem: the UCSD pedestrian <ref type="bibr" target="#b3">[4]</ref> and the UCF CC 50 <ref type="bibr" target="#b8">[9]</ref> datasets. The third one is the TRANCOS dataset <ref type="bibr" target="#b9">[10]</ref>, which has been designed for vehicle counting in traffic jam scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">TRANCOS dataset</head><p>Experimental setup TRANCOS is a publicly available dataset, which provides a collection of 1244 images of different traffic scenes, obtained from real video surveillance cameras, with a total of 46796 annotated vehicles. The objects have been manually annotated using dots. It also provides a region of interest (ROI) per image, defining the region considered for the evaluation. This database provides images from very different scenarios, which have not been parameterized. Moreover, the cameras can move in the same scene, and no perspective maps are provided.</p><p>We strictly follow the experimental setup proposed in <ref type="bibr" target="#b9">[10]</ref>, using only the training and validation sets for learning our models. In each training image, we randomly extract 800 patches of 115x115 pixels. We also perform a data augmentation strategy by flipping each patch, having in total 1600 patches per training image. These patches are then resized to 72x72 to feed our networks. We generate the ground truth object density maps with the code provided in <ref type="bibr" target="#b9">[10]</ref>, which places a Gaussian Kernel (with a covariance matrix of Σ = 15 • 1 2x2 ) in the center of each annotated object.</p><p>For the CCNN model, we perform a cross-validation to adjust the standard deviation values of the Gaussian noise that is necessary to initialize the weights of each layer of the deep network. The Xavier initialization method <ref type="bibr" target="#b24">[25]</ref> was used to, but with it, our CCNN models are not able to converge in our experiments.</p><p>To train the Hydra CNN, we follow the same patch extraction procedure as for the CCNN model. The only difference is that from each patch we build its corresponding pyramid of s different scales, being s the number of heads of our Hydra CNN. Therefore, the first level of the pyramid contains the original patch. For the rest of levels we build centered and scaled crops, of size 1/s, of the original patch. For example, in the case of a Hydra CNN with two heads, the first level of the pyramid corresponds to the original input patch, and the second level contains a crop of size 50% of the original size. When three heads are used, the second and third levels of the pyramid contain a crop of size 66% and 33% of the original size, respectively.</p><p>To initialize the heads of the Hydra CNN model, we use the same parameters discovered by the cross-validation for the CCNN. Then we perform a crossvalidation to adjust the standard deviation for the layers Fc6 and Fc7.</p><p>The test is performed by densely scanning the input image with a stride of 10 pixels, and assembling all the patches as it is described in Section 3.2.</p><p>The TRANCOS benchmark comes with an evaluation metric to be used: the Grid Average Mean absolute Error (GAME) <ref type="bibr" target="#b9">[10]</ref>. This GAME is computed as follows,</p><formula xml:id="formula_8">GAM E(L) = 1 N N n=1 ( 4 L l=1 D l In − D l I gt n ) ,<label>(6)</label></formula><p>where N is the total number of images, D l In corresponds to the estimated object density map count for the image n and region l, and D l I g n t is the corresponding ground truth density map. For a specific level L, the GAME(L) subdivides the image using a grid of 4 L non-overlapping regions, and the error is computed as the sum of the mean absolute errors in each of these subregions. This metric provides a spatial measurement of the error. Note that a GAME(0) is equivalent to the mean absolute error (MAE) metric. Vehicle counting results Table <ref type="table" target="#tab_0">1</ref> shows a detailed comparison of our models with the state-of-the-art methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> reported in <ref type="bibr" target="#b9">[10]</ref>. First, note how all our models outperform the state-the-art. The more simple architecture of CCNN already improves the results of the previously reported models <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. Hydra CNN should be able to report the best results in TRAN-COS, given the high level of variability in terms of perspective and variety of scenes that the images of this dataset exhibits. Table <ref type="table" target="#tab_0">1</ref> shows that a Hydra CNN with just 2 scales improves the results with respect to the CCNN for a GAME(0), while for GAME <ref type="bibr" target="#b0">(1)</ref> to GAME(3) the performance is very similar. If we go further, and train a Hydra CNN with 3 heads, we are now able to report the best results for this dataset for all the GAMES. Note how the error for the higher levels of the GAME, where this metric is more restrictive, drastically decreases. This reveals that the Hydra CNN is more precise not only predicting the object density maps, but also localizing the densities within them. If we continue increasing the number of heads of Hydra CNN, this does not guarantee an increment of the performance. On the contrary, we have experimentally observed that the model saturates for 4 heads (see last row of Table <ref type="table" target="#tab_0">1</ref>), while the complexity dramatically increases.</p><formula xml:id="formula_9">Method GAME 0 GAME 1 GAME 2 GAME</formula><p>Overall, these results lead us to two conclusions. First, the object density maps can be accurately and efficiently estimated using the CCNN model, which works remarkably well. Second, the Hydra CNN idea of having a pyramid of scales as input, to learn a non-linear regression model for the prediction of object density maps, seems to be more accurate, defining the novel state-of-the-art in this benchmark.</p><p>Figure <ref type="figure" target="#fig_3">4</ref> shows an additional analysis of our models using the MAE (GAME(0)). We perform the comparison sorting all the test images by the number of annotated vehicles they contain. We divide them in 10 subsets, and plot in this figure the MAE of our CCNN and Hydra CNN 3s models. Interestingly, CCNN reports a slightly lower error for the subsets of images with less objects. But its error quickly rises when more vehicles appear in the scene. The Hydra CNN model is clearly the winner, reporting a very stable error along the different subsets.</p><p>Finally, Figure <ref type="figure" target="#fig_4">5</ref> shows some of the qualitative results obtained. The first three images present the results where our Hydra 3s model obtains a good performance, and the last two images correspond to those for which we get the maximum error. In the supplementary material, we provide more qualitative results produced by our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">UCSD dataset</head><p>Experimental setup Here we evaluate our models in the crowd counting problem. For doing so, we use the popular UCSD pedestrian benchmark <ref type="bibr" target="#b3">[4]</ref>. It is a 2000-frames video dataset from a surveillance camera of a single scene. The images have been annotated with a dot on each pedestrian. It also includes a ROI and the perspective map of the scene. In our experiments, we report results when our models use and not use this perspective map. The evaluation metric proposed in <ref type="bibr" target="#b3">[4]</ref> is the MAE.</p><p>We follow exactly the same experimental setup that is used in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b6">7]</ref>. Hence, we split the data into four different subsets: 1) "maximal": train with frames 600:5:1400; 2) "downscale": train with frames 1205:5:1600; 3) "upscale": train with frames 805:5:1100; 4) "minimal": train with frames 640:80:1360. All the frames out of the defined training ranges are used for testing. In order to train our CCNN model, for each image we collect 800 patches, of 72x72 pixels, randomly extracted all over the image, and their corresponding ground truth density maps. We perform a data augmentation by flipping each patch. Therefore, in total, we have 1600 training samples per image. As usual, when the perspective map is used, the ground truth object density maps are built scaling the covariance of the 2D Gaussian kernels, where we fix a base Σ = 8 • 1 2x2 , as it is described in <ref type="bibr" target="#b5">[6]</ref>.</p><p>To train the Hydra CNN models, we follow the same patch extraction detailed for the TRANCOS dataset. This time, 800 random patches of 72x72 pixels are extracted per training image. The pyramid of scaled versions of the patches is built using the same procedure explained before. We initialize both the CCNN and the Hydra CNN models following the procedures previously explained for the TRANCOS dataset. Finally, to perform the test we fix a stride of 10 pixels and then we proceed as it is described in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Crowd counting results</head><p>We start analyzing the performance of the CCNN model. Table <ref type="table" target="#tab_1">2</ref> shows a comparison with all the state-of-the-art methods. Our CCNN, trained using the perspective map provided, like all the competing approaches, obtains the best results for the "upscale" subset. If we compare the performance of the two deep learning models, i.e. CCNN vs. the Crowd CNN of Zhang et al. <ref type="bibr" target="#b6">[7]</ref>, our model gets a better performance in 3 of the 4 subsets.</p><p>Figure <ref type="figure" target="#fig_5">6</ref> shows some qualitative results. We have chosen five frames that best represent the object density differences in the dataset. The last two frames correspond with the maximal error produced by our CCNN model. In the supplementary material, we provide videos with all the qualitative results.</p><p>We now proceed to analyze the results obtained by the Hydra CNN models in this benchmark. Even though this dataset offers images of a fixed scene, providing its perspective map, where the objects appear at similar scales, we have decided to conduct this extra experiment with the Hydra CNN approach, Method 'maximal' 'downscale' 'upscale' 'minimal' <ref type="bibr" target="#b5">[6]</ref> 1.70  to evaluate its performance with the state-of-the-art models. Table <ref type="table" target="#tab_2">3</ref> shows the MAE results for our Hydra with two and three heads. Recall that we do not use the perspective information. We can observe two things. The first one is that both architectures report a good performance, even if they do not improve the stateof-the-art. To support this conclusion, Figure <ref type="figure" target="#fig_6">7</ref> shows a comparison between the ground truth, the CCNN model (trained using the perspective map), and the estimation generated by our Hydra with two and three heads, which does not use the perspective information. Hydra CNN models are able to closely follow both the CCNN and the GT. We belive that Hydra CNN does not outperform CCNN due to the small variability and the low perspective distortion exhibited by this dataset. In this situation, adding more scales does not seem to provide really useful information. Hence, the use of Hydra CNN does not offer here a clear advantage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">UCF CC 50 dataset</head><p>Experimental setup The UCF CC 50 dataset <ref type="bibr" target="#b8">[9]</ref> consists of 50 pictures, collected from publicly available web images. The counts of persons range between 94 and 4543, with an average of 1280 individuals per image. People have been annotated by dots, and no perspective maps are provided. The images contain very crowded scenes, which belong to diverse set of events: concerts, protests, stadiums, marathons, and pilgrimages. This dataset proposes a challenging problem, especially due to the reduced number of training images, and the variability between the scenarios covered. We have followed the same experimental setup described in <ref type="bibr" target="#b8">[9]</ref>. We randomly split the dataset into 5 subsets and perform a 5-fold cross-validation. To report the results the MAE and the Mean Standard Deviation (MSD) are used.</p><p>For training our models, we scale the images in order to make the largest size equal to 800 pixels. We follow the same experimental setup described in Section 4.1. We now randomly extract 1200 image patches of 150x150 pixels with their corresponding ground truth. We also augment the training data by flipping each sample. Finally, the covariance matrix for the ground truth density map generation with the Gaussian functions is fixed to Σ = 15 • 1 2x2 . For the initialization of the CCNN and the Hydra CNN models, we follow the crossvalidation procedure already described for the other datasets. To do the test, we densely scan the image with a stride of 10 pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Crowd counting results</head><p>Table <ref type="table" target="#tab_3">4</ref> shows a comparison of our models with the state-of-the-art approaches. In this dataset, the best performance is given by our Hydra CNN 2s, which is able to drastically reduce the MAE. Hydra CNN with 3 scales outperforms 3 of 5 models previously published. The CCNN approach only improves the results reported in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b5">6]</ref>. Analyzing the results, we find that the performance of the CCNN decreases especially in those images with the highest number of humans and where the perspective really matters. In Figure <ref type="figure" target="#fig_9">9</ref> we include some qualitative examples of the CCNN model where this can be appreciated. This issue and the results provided, confirm the advantages of the scale-aware Hydra model for the very crowded scenes of the UCF CC 50 dataset. Figure <ref type="figure" target="#fig_7">8</ref> shows some of the qualitative results that are obtained by our Hydra CNN model with two heads. The first three columns correspond with results where our network reports a good performance, while the last two columns show the maximum errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we have introduced two novel deep learning approaches to count objects in images. To the best of our knowledge, only two methods have previously explored similar ideas <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b20">21]</ref>. Therefore, our research affords novel insights into the problem of object counting with deep learning.</p><p>With our first architecture, the CCNN model, we show that object density maps can be accurately and efficiently estimated, letting the network learn the mapping which transforms the appearance of image patches into object density maps. We are able to match and improve the counting accuracy of much more  Method MAE MSD <ref type="bibr" target="#b18">[19]</ref> 655.7 697.8 <ref type="bibr" target="#b5">[6]</ref> 493.4 487.1 <ref type="bibr" target="#b6">[7]</ref> 467.0 498.5 <ref type="bibr" target="#b8">[9]</ref> 419.5 541.6 <ref type="bibr" target="#b20">[21]</ref> 377.  complex models, such as <ref type="bibr" target="#b6">[7]</ref>, where multiple loss functions and extra regressors are used in conjunction with the deep model.</p><p>Our second model, Hydra CNN, goes one step further, and provides a scaleaware solution, which is designed to learn a non-linear regressor to generate the object density maps from a pyramid of image patches at multiple scales. The experimental validation reveals that Hydra not only improves the results of its predecessor, our CCNN, but also that it is able to improve the state-of-the-art of those benchmarks that propose to count object in different scenes, showing very crowded situations, and where no geometric information for the scene, like its perspective map, is provided.</p><p>By making our software and pre-trained models available<ref type="foot" target="#foot_0">1</ref> , we make it effortless for future researches to reproduce our results and to facilitate further progress towards more accurate solutions for this challenging task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. We define the object counting task like a regression problem where a deep learning model has to learn how to map image patches to object densities.</figDesc><graphic url="image-29.png" coords="2,239.76,53.68,64.12,66.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Our novel CCNN model. The input image patch is passed forward our deep network, which estimates its corresponding density map.</figDesc><graphic url="image-53.png" coords="4,126.53,58.08,186.07,70.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Hydra CNN. The network uses a pyramid of input patches (they are cropped and rescaled to a size of 72x72). Each level of the pyramid, representing a different scale, feeds a particular head of the Hydra. All the head outputs are concatenated and passed to a fully-connected bank of layers, which form the body of the hydra.</figDesc><graphic url="image-58.png" coords="6,164.54,48.84,157.26,89.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Comparison of CCNN and Hydra CNN in the TRANCOS dataset when the number of objects</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Qualitative results of our Hydra model in the TRANCOS dataset. The first row corresponds to the target image with the ground truth. The second row shows the predicted object density maps. We show the total object count above each image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.6. CCNN qualitative results for the UCSD dataset. The first row shows the target image with its ground truth. The second row shows the predicted object density map. We show the total object count above each image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Comparison of ground truth, CCNN and Hydra CNN of two and three heads in the UCSD benchmark.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. UCF CC 50 dataset qualitative results for Hydra CNN with two scales. First row corresponds to the target image with the GT. Second row shows the predicted object density maps. We show the total object count above each image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Qualitative results of the CCNN in the UCF CC 50 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>TRANCOS dataset. Comparison with the of state-of-the-art models.</figDesc><table><row><cell>3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Mean absolute error. Comparison with the state-of-the-art methods for the UCSD pedestrian dataset.</figDesc><table><row><cell></cell><cell></cell><cell>1.28</cell><cell>1.59</cell><cell>2.02</cell></row><row><cell>[5]</cell><cell>1.70</cell><cell>2.16</cell><cell>1.61</cell><cell>2.20</cell></row><row><cell>[20]</cell><cell>1.43</cell><cell>1.30</cell><cell>1.59</cell><cell>1.62</cell></row><row><cell>[3]</cell><cell>1.24</cell><cell>1.31</cell><cell>1.69</cell><cell>1.49</cell></row><row><cell>[7]</cell><cell>1.70</cell><cell>1.26</cell><cell>1.59</cell><cell>1.52</cell></row><row><cell>Our CCNN</cell><cell>1.65</cell><cell>1.79</cell><cell>1.11</cell><cell>1.50</cell></row><row><cell cols="5">Method 'maximal' 'downscale' 'upscale' 'minimal'</cell></row><row><cell>Hydra 2s</cell><cell>2.22</cell><cell>1.93</cell><cell>1.37</cell><cell>2.38</cell></row><row><cell>Hydra 3s</cell><cell>2.17</cell><cell>2.99</cell><cell>1.44</cell><cell>1.92</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>MAE comparison of our Hydra 2s and Hydra 3s models trained without perspective information in the UCSD dataset.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>MAE and MSD comparison for the UCF CC 50 dataset.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://github.com/gramuah/ccnn</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work is supported by the projects of the DGT with references SPIP2014-1468 and SPIP2015-01809, and the project of the MINECO TEC2013-45183-R.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Interactive object counting</title>
		<author>
			<persName><forename type="first">C</forename><surname>Arteta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Noble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Privacy preserving crowd monitoring: Counting people without people models or tracking</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">S J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Learning to count with regression forest and structured labels</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fiaschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Kthe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Hamprecht</surname></persName>
		</author>
		<editor>ICPR.</editor>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning to count objects in images</title>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Cross-scene crowd counting via deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2015-06">June 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Crowd counting and profiling: Methodology and evaluation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Modeling, Simulation and Visual Analysis of Crowds</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Multi-source multi-scale counting in extremely dense crowd images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Saleemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Seibert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Extremely overlapping vehicle counting</title>
		<author>
			<persName><forename type="first">R</forename><surname>Guerrero-Gómez-Olmedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Torre-Jiménez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>López-Sastre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maldonado-Bascón</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Oñoro Rubio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Iberian Conference on Pattern Recognition and Image Analysis (IbPRIA)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Person count localization in videos from noisy foreground and detections</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Pedestrian detection in crowded scenes</title>
		<author>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Seemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Estimating the number of people in crowded scenes by mid based foreground segmentation and head-shoulder detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<editor>ICPR.</editor>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Counting people in crowded environments by fusion of shape and motion information</title>
		<author>
			<persName><forename type="first">M</forename><surname>Patzold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Evangelio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sikora</surname></persName>
		</author>
		<editor>AVSS.</editor>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Robust real-time face detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Automatic adaptation of a generic pedestrian detector to a specific traffic scene</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Counting crowded moving objects</title>
		<author>
			<persName><forename type="first">V</forename><surname>Rabaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Unified crowd segmentation</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Doretto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Krahnstoever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rittscher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Density-aware person detection and tracking in crowds</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Audibert</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">COUNT forest: CO-voting uncertain number of targets using random forest for crowd density estimation</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">Q</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kozakaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Okada</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Single-image crowd counting via multi-column convolutional neural network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016-06">June 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Caffe: Convolutional architecture for fast feature embedding</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Visual saliency based on multiscale deep features</title>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<editor>AISTATS.</editor>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Crowd counting using multiple local features</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Denman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fookes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
		<editor>DICTA.</editor>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
