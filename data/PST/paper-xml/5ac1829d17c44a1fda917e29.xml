<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">L 2 -NONEXPANSIVE NEURAL NETWORKS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Haifeng</forename><surname>Qian</surname></persName>
							<email>qianhaifeng@us.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research Yorktown Heights</orgName>
								<address>
									<postCode>10598</postCode>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mark</forename><forename type="middle">N</forename><surname>Wegman</surname></persName>
							<email>wegman@us.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research Yorktown Heights</orgName>
								<address>
									<postCode>10598</postCode>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">L 2 -NONEXPANSIVE NEURAL NETWORKS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper proposes a class of well-conditioned neural networks in which a unit amount of change in the inputs causes at most a unit amount of change in the outputs or any of the internal layers. We develop the known methodology of controlling Lipschitz constants to realize its full potential in maximizing robustness, with a new regularization scheme for linear layers, new ways to adapt nonlinearities and a new loss function. With MNIST and CIFAR-10 classifiers, we demonstrate a number of advantages. Without needing any adversarial training, the proposed classifiers exceed the state of the art in robustness against white-box L 2 -bounded adversarial attacks. They generalize better than ordinary networks from noisy data with partially random labels. Their outputs are quantitatively meaningful and indicate levels of confidence and generalization, among other desirable properties.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Artificial neural networks are often ill-conditioned systems in that a small change in the inputs can cause significant changes in the outputs <ref type="bibr" target="#b28">(Szegedy et al., 2014)</ref>. This results in poor robustness and vulnerability under adversarial attacks which has been reported on a variety of networks including image classification <ref type="bibr" target="#b5">(Carlini &amp; Wagner, 2017a;</ref><ref type="bibr" target="#b15">Goodfellow et al., 2014)</ref>, speech recognition <ref type="bibr" target="#b20">(Kreuk et al., 2018;</ref><ref type="bibr" target="#b1">Alzantot et al., 2018;</ref><ref type="bibr">Carlini &amp; Wagner, 2018)</ref>, image captioning <ref type="bibr" target="#b9">(Chen et al., 2017)</ref> and natural language processing <ref type="bibr" target="#b13">(Gao et al., 2018;</ref><ref type="bibr" target="#b12">Ebrahimi et al., 2017)</ref>. These issues bring up both theoretical questions of how neural networks generalize <ref type="bibr" target="#b18">(Kawaguchi et al., 2017;</ref><ref type="bibr" target="#b32">Xu &amp; Mannor, 2012)</ref> and practical concerns of security in applications <ref type="bibr" target="#b0">(Akhtar &amp; Mian, 2018)</ref>.</p><p>A number of remedies have been proposed for these issues and will be discussed in Section 4. Whitebox defense is particularly difficult and many proposals have failed. For example, <ref type="bibr" target="#b2">Athalye et al. (2018)</ref> reported that out of eight recent defense works, only <ref type="bibr" target="#b21">Madry et al. (2017)</ref> survived strong attacks. So far the mainstream and most successful remedy is that of adversarial training <ref type="bibr" target="#b21">(Madry et al., 2017)</ref>. However, as will be shown in Tables <ref type="table" target="#tab_1">1 and 2</ref>, the robustness by adversarial training diminishes when a white-box attacker <ref type="bibr" target="#b5">(Carlini &amp; Wagner, 2017a</ref>) is allowed to use more iterations. This paper explores a different approach and demonstrates that a combination of the following three conditions results in enhanced robustness: 1) the Lipschitz constant of a network from inputs to logits is no greater than 1 with respect to the L 2 -norm; 2) the loss function explicitly maximizes confidence gap, which is the difference between the largest and second largest logits of a classifier; 3) the network architecture restricts confidence gaps as little as possible. We will elaborate.</p><p>There are previous works that achieve the first condition <ref type="bibr" target="#b10">(Cisse et al., 2017;</ref><ref type="bibr" target="#b17">Hein &amp; Andriushchenko, 2017)</ref> or bound responses to input perturbations by other means <ref type="bibr" target="#b19">(Kolter &amp; Wong, 2017;</ref><ref type="bibr" target="#b24">Raghunathan et al., 2018;</ref><ref type="bibr" target="#b16">Haber &amp; Ruthotto, 2017)</ref>. For example, Parseval networks <ref type="bibr" target="#b10">(Cisse et al., 2017)</ref> bound the Lipschitz constant by requiring each linear or convolution layer be composed of orthonormal filters. However, the reported robustness and guarantees are often under weak attacks or with low noise magnitude, and none of these works has demonstrated results that are comparable to adversarial training.</p><p>In contrast, we are able to build MNIST and CIFAR-10 classifiers, without needing any adversarial training, that exceed the state of the art <ref type="bibr" target="#b21">(Madry et al., 2017)</ref> in robustness against white-box L 2bounded adversarial attacks. The defense is even stronger if adversarial training is added. We will refer to these networks as L 2 -nonexpansive neural networks (L2NNNs). Our advantage comes from a set of new techniques: our weight regularization, which is key in enforcing the first condition, allows greater degrees of freedom in parameter training than the scheme in <ref type="bibr" target="#b10">Cisse et al. (2017)</ref>; a new loss function is specially designed for the second condition; we adapt various layers in new ways for the third condition, for example norm-pooling and two-sided ReLU, which will be presented later.</p><p>Let us begin with intuitions behind the second and third conditions. Consider a multi-class classifier. Let g (x) denote its confidence gap for an input data point x. If the classifier is a single L2NNN,<ref type="foot" target="#foot_0">1</ref> we have a guarantee<ref type="foot" target="#foot_1">2</ref> that the classifier will not change its answer as long as the input x is modified by no more than an L 2 -norm of g (x) / √ 2. Therefore maximizing the average confidence gap directly boosts robustness and this motivates the second condition. To explain the third condition, let us introduce the notion of preserving distance: the distance between any pair of input vectors with two different labels ought to be preserved as much as possible at the outputs, while we do not care about the distance between a pair with the same label. Let d (x 1 , x 2 ) denote the L 2 -distance between the output logit-vectors for two input points x 1 and x 2 that have different labels and that are classified correctly. It is straightforward to verify the condition<ref type="foot" target="#foot_2">3</ref> of g</p><formula xml:id="formula_0">(x 1 ) + g (x 2 ) ≤ √ 2 • d (x 1 , x 2</formula><p>). Therefore a network that maximizes confidence gaps well must be one that preserves distance well. Ultimately some distances are preserved while others are lost, and ideally the decision of which distance to lose is made by parameter training rather than by artifacts of network architecture. Hence the third condition involves distance-preserving architecture choices that leave the decision to parameter training as much as possible, and this motivates many of our design decisions such as Sections 2.2 and 2.3.</p><p>In practice we employ the strategy of divide and conquer and build each layer as a nonexpansive map with respect to the L 2 -norm. It is straightforward to see that a feedforward network composed of nonexpansive layers must implement a nonexpansive map overall. How to adapt subtleties like recursion and splitting-reconvergence is included in the appendix.</p><p>Besides being robust against adversarial noises, L2NNNs have other desirable properties. They generalize better from noisy training labels than ordinary networks: for example, when 75% of MNIST training labels are randomized, an L2NNN still achieves 93.1% accuracy on the test set, in contrast to 75.2% from the best ordinary network. The problem of exploding gradients, which is common in training ordinary networks, is avoided because the gradient of any output with respect to any internal signal is bounded between -1 and 1. Unlike ordinary networks, the confidence gap of an L2NNN classifier is a quantitatively meaningful indication of confidence on individual data points, and the average gap is an indication of generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">L -NONEXPANSIVE NEURAL NETWORKS</head><p>This section describes how to adapt some individual operators in neural networks for L2NNNs. Discussions on splitting-reconvergence, recursion and normalization are in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">WEIGHTS</head><p>This section covers both the matrix-vector multiplication in a fully connected layer and the convolution calculation between input tensor and weight tensor in a convolution layer. The convolution calculation can be viewed as a set of vector-matrix multiplications: we make shifted copies of the input tensor and shuffle the copies into a set of small vectors such that each vector contains input entries in one tile; we reshape the weight tensor into a matrix by flattening all but the dimension of the output filters; then convolution is equivalent to multiplying each of the said small vectors with the flattened weight matrix. Therefore, in both cases, a basic operator is y = W x. To be a nonexpansive map with respect to the L 2 -norm, a necessary and sufficient condition is</p><formula xml:id="formula_1">y T y ≤ x T x =⇒ x T W T W x ≤ x T x, ∀x ∈ R N ρ W T W ≤ 1 (1)</formula><p>where ρ denotes the spectral radius of a matrix.</p><p>The exact condition of ( <ref type="formula">1</ref>) is difficult to incorporate into training. Instead we use an upper bound:</p><formula xml:id="formula_2">4 ρ W T W ≤ b (W ) min r(W T W ), r(W W T ) , where r (M ) = max i j |M i,j | (2)</formula><p>The above is where our linear and convolution layers differ from those in <ref type="bibr" target="#b10">Cisse et al. (2017)</ref>: they require W W T to be an identity matrix, and it is straightforward to see that their scheme is only one special case that makes b (W ) equal to 1. Instead of forcing filters to be orthogonal to each other, our bound of b (W ) provides parameter training with greater degrees of freedom.</p><p>One simple way to use ( <ref type="formula">2</ref>) is replacing W with W ′ = W/ b (W ) in weight multiplications, and this would enforce that the layer is strictly nonexpansive. Another method is described in the appendix.</p><p>As mentioned, convolution can be viewed as a first layer of making copies and a second layer of vector-matrix multiplications. With the above regularization, the multiplication layer is nonexpansive. Hence we only need to ensure that the copying layer is nonexpansive. For filter size of K 1 by K 2 and strides of S 1 by S 2 , we simply divide the input tensor by a factor of ⌈K 1 /S 1 ⌉ • ⌈K 2 /S 2 ⌉.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">RELU AND OTHERS</head><p>Let us turn our attention to the third condition from Section 1. ReLU, tanh and sigmoid are nonexpansive but do not preserve distance well. This section presents a method that improves ReLU and is generalizable to other nonlinearities. A different approach to improve sigmoid is in the appendix.</p><p>To understand the weakness of ReLU, let us consider two input data points A and B, and suppose that a ReLU in the network receives two different negative values for A and B and outputs zero for both. Comparing the A-B distance before and after this ReLU layer, there is a distance loss and this particular ReLU contributes to it. We use two-sided ReLU which is a function from R to R 2 and simply computes ReLU(x) and ReLU(−x). Two-sided ReLU has been studied in <ref type="bibr" target="#b26">Shang et al. (2016)</ref> in convolution layers for accuracy improvement. It is straightforward to verify that two-sided ReLU is nonexpansive with respect to any L p -norm and that it preserves distance in the above scenario. We will empirically verify its effectiveness in increasing confidence gaps in Section 3.</p><p>Two-sided ReLU is a special case of the following general technique. Let f (x) be a nonexpansive and monotonically increasing scalar function, and note that ReLU, tanh and sigmoid all fit these conditions. We can define a function from R to R 2 that computes f (x) and f (x) − x. Such a new function is nonexpansive with respect to any L p -norm<ref type="foot" target="#foot_4">5</ref> and preserves distance better than f (x) alone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">POOLING</head><p>The popular max-pooling is nonexpansive, but does not preserve distance as much as possible. Consider a scenario where the inputs to pooling are activations that represent edge detection, and consider two images A and B such that A contains an edge that passes a particular pooling window while B does not. Inside this window, A has positive values while B has all zeroes. For this window, the A-B distance before pooling is the L 2 -norm of A's values, yet if max-pooling is used, the A-B distance after pooling becomes the largest of A's values, which can be substantially smaller than the former. Thus we suffer a loss of distance between A and B while passing this pooling layer.</p><p>We replace max-pooling with norm-pooling, which was reported in <ref type="bibr" target="#b4">Boureau et al. (2010)</ref> to occasionally increase accuracy. Instead of taking the max of values inside a pooling window, we take the L 2 -norm of them. It is straightforward to verify that norm-pooling is nonexpansive<ref type="foot" target="#foot_5">6</ref> and would entirely preserve the L 2 -distance between A and B in the hypothetical scenario above. Other L p -norms can also be used. We will verify its effectiveness in increasing confidence gaps in Section 3.</p><p>If pooling windows overlap, we divide the input tensor by √ K where K is the maximum number of pooling windows in which an entry can appear, similar to convolution layers discussed earlier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">LOSS FUNCTION</head><p>For a classifier with K labels, we recommend building it as K overlapping L2NNNs, each of which outputs a single logit for one label. In an architecture with no split layers, this simply implies that these K L2NNNs share all but the last linear layer and that the last linear layer is decomposed into K single-output linear filters, one in each L2NNN. For a multi-L2NNN classifier, we have a guarantee<ref type="foot" target="#foot_6">7</ref> that the classifier will not change its answer as long as the input x is modified by no more than an L 2 -norm of g (x) /2, where again g (x) denotes the confidence gap. As mentioned in Section 1, a single-L2NNN classifier has a guarantee of g (x) / √ 2. Although this seems better on the surface, it is more difficult to achieve large confidence gaps. We will assume the multi-L2NNN approach.</p><p>We use a loss function with three terms, with trade-off hyperparameters γ and ω:</p><formula xml:id="formula_3">L = L a + γ • L b + ω • L c<label>(3)</label></formula><p>Let y 1 , y 2 , • • • , y K be outputs from the L2NNNs. The first loss term is</p><formula xml:id="formula_4">L a = softmax-cross-entropy (u 1 y 1 , u 2 y 2 , • • • , u K y K , label) (4) where u 1 , u 2 , • • • , u K are trainable parameters. The second loss term is L b = softmax-cross-entropy (vy 1 , vy 2 , • • • , vy K , label)</formula><p>(5) where v can be either a trainable parameter or a hyperparameter. Note that u 1 , u 2 , • • • , u K and v are not part of the classifier and are not used during inference. The third loss term is</p><formula xml:id="formula_5">L c = average log 1 − softmax (zy 1 , zy 2 , • • • , zy K ) label z (<label>6</label></formula><formula xml:id="formula_6">)</formula><p>where z is a hyperparameter.</p><p>The rationale for the first loss term ( <ref type="formula">4</ref>) is that it mimics cross-entropy loss of an ordinary network. If an ordinary network has been converted to L2NNNs by multiplying each layer with a small constant, its original outputs can be recovered by scaling up L2NNN outputs with certain constants, which is enabled by the formula (4). Hence this loss term is meant to guide the training process to discover any feature that an ordinary network can discover. The rationale for the second loss term ( <ref type="formula">5</ref>) is that it is directly related to the classification accuracy. Multiplying L2NNN outputs uniformly with v does not change the output label and only adapts to the value range of L2NNN outputs and drive towards better nominal accuracy. The third loss term (6) approximates average confidence gap: the log term is a soft measure of a confidence gap (for a correct prediction), and is asymptotically linear for larger gap values. The hyperparameter z controls the degree of softness, and has relatively low impact on the magnitude of loss due to the division by z; if we increase z then (6) asymptotically becomes the average of minus confidence gaps for correct predictions and zeroes for incorrect predictions. Therefore loss (6) encourages large confidence gaps and yet is smooth and differentiable.</p><p>A notable variation of ( <ref type="formula" target="#formula_3">3</ref>) is one that combines with adversarial training. Our implementation applies the technique of <ref type="bibr" target="#b21">Madry et al. (2017)</ref> on the first loss term (4): we use distorted inputs in calculating L a . The results are reported in Tables <ref type="table" target="#tab_1">1 and 2</ref> as Model 4. Another possibility is to use distorted inputs in calculating L a and L b , while L c should be based on original inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTS</head><p>Experiments are divided into three groups to study different properties of L2NNNs. Our MNIST and CIFAR-10 classifiers are available at http://researcher.watson.ibm.com/group/9298</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">ROBUSTNESS</head><p>This section evaluates robustness of L2NNN classifiers for MNIST and CIFAR-10 and compares against the state of the art <ref type="bibr" target="#b21">Madry et al. (2017)</ref>. The robustness metric is accuracy under whitebox non-targeted L 2 -bounded attacks. The attack code of <ref type="bibr" target="#b5">Carlini &amp; Wagner (2017a)</ref> is used. We  <ref type="formula">2017</ref>) and report their robustness against L 2 -bounded attacks in Tables <ref type="table" target="#tab_1">1 and 2</ref>.<ref type="foot" target="#foot_8">9</ref> Note that their defense diminishes as the attacks are allowed more iterations. Figure <ref type="figure" target="#fig_0">1</ref> illustrates one example of this effect: the first image is an attack on MNIST Model 2 (0 recognized as 5) found after 1K iterations, with noise L 2 -norm of 4.4, while the second picture is one found after 10K iterations, the same 0 recognized as 5, with noise L 2 -norm of 2.1. We hypothesize that adversarial training alone provides little absolute defense at the noise levels used in the two tables: adversarial examples still exist and are only more difficult to find. The fact that in Table <ref type="table" target="#tab_1">2</ref> Model 2 accuracy is lower in the 1000x10 row than the 10K row further supports our hypothesis. In contrast, the defense of the L2NNN models remain constant when the attacks are allowed more iterations, specifically MNIST Models beyond 10K iterations and CIFAR-10 Models beyond 1000 iterations. The reason is that L2NNN classifiers achieve their defense by creating a confidence gap between the largest logit and the rest, and that half of this gap is a lower bound of L 2 -norm of distortion to the input data in order to change the classification. Hence L2NNN's defense comes from a minimum-distortion guarantee. Although adversarial training alone may also increase the minimum distortion limit for misclassification, as suggested in <ref type="bibr" target="#b8">Carlini et al. (2017)</ref> for a small network, that limit likely does not reach the levels used in Tables <ref type="table" target="#tab_1">1 and 2</ref> and hence the defense depends on how likely the attacker can reach a lower-distortion misclassification. Consequently when the attacks are allowed to make more attempts the defense with guarantee stands while the other diminishes.</p><p>For both MNIST and CIFAR-10, adding adversarial training boosts the robustness of Model 4. We hypothesize that adversarial training lowers local Lipschitz constants in certain parts of the input space, specifically around the training images, and therefore makes local robustness guarantees larger <ref type="bibr" target="#b17">(Hein &amp; Andriushchenko, 2017)</ref>. To test this hypothesis on MNIST Models 3 and 4, we measure the average L 2 -norm of their Jacobian matrices, averaged over the first 1000 images in the test set, and the results are 1.05 for Model 3 and 0.83 for Model 4. Note that the L 2 -norm of Jacobian can be greater than 1 for multi-L2NNN classifiers. These measurements are consistent with, albeit does not prove, the hypothesis. To test the effects of various components of our method, we build models for each of which we disable a different technique during training. The results are reported in Table <ref type="table" target="#tab_2">3</ref>. To put the confidence gap values in context, our MNIST Model 3 has an average gap of 2.8. The first one is without weight regularization of Section 2.1 and it becomes an ordinary network which has little defense against adversarial attacks; its large average confidence gap is meaningless. For the second one we remove the third loss term (6) and for the third one we replace norm-pooling with regular max-pooling, both resulting in smaller average confidence gap and less defense against attacks. For the fourth one, we replace two-sided ReLU with regular ReLU, and this leads to degradation in nominal accuracy, average confidence gap and robustness. Parseval networks <ref type="bibr" target="#b10">(Cisse et al., 2017)</ref> can be viewed as models without L c term, norm-pooling or two-sided ReLU, and with a more restrictive scheme for weight matrix regularization.</p><p>Model 3 in Table <ref type="table" target="#tab_0">1</ref> and the second row of Table <ref type="table" target="#tab_2">3</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">MEANINGFUL OUTPUTS</head><p>This section discusses how to understand and utilize L2NNNs' output values. We observe strong correlation between the confidence gap of L2NNN and the magnitude of distortion needed to force it to misclassify, and images are included in appendix.</p><p>In the next experiment, we sort test data by the confidence gap of a classifier on each image. Then we divide the sorted data into 10 bins and report accuracy separately on each bin in Figure <ref type="figure" target="#fig_1">2</ref>. We repeat this experiment for Model 2 <ref type="bibr" target="#b21">(Madry et al., 2017)</ref> and our Model 3 of Tables <ref type="table" target="#tab_1">1 and 2</ref>. Note that the L2NNN model shows better correlation between confidence and robustness: for MNIST our first bin is 95% robust and second bin is 67% robust. This indicates that the L2NNN outputs are much more quantitatively meaningful than those of ordinary neural networks. It is an important property that an L2NNN has an easily accessible measurement on how robust its decisions are. Since robustness is easily measurable, it can be optimized directly, and we believe that this is the primary reason that we can demonstrate the robustness results of Tables <ref type="table" target="#tab_1">1 and 2</ref>. This can also be valuable in real-life applications where we need to quantify how reliable a decision is.</p><p>One of the other practical implications of this property is that we can form hybrid models which use L2NNN outputs when the confidence is high and a different model when the confidence of the L2NNN is low. This creates another dimension of trade-off between nominal accuracy and robustness that one can take advantage of in an application. We built such a hybrid model for MNIST with the switch threshold of 1.0 and achieved nominal accuracy of 99.3%, where only 6.9% of images were delegated to the alternative classifier. We built such a hybrid model for CIFAR-10 with the switch threshold of 0.1 and achieved nominal accuracy of 89.4%, where 25% of images were delegated. To put these threshold values in context, MNIST Model 3 has an average gap of 2.8 and CIFAR-10 Model 3 has an average gap of 0.34. In other words, if for a data point the L2NNN confidence gap is substantially below average, the classification is delegated to the alternative classifier, and this way we can recover nominal accuracy at a moderate cost of robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">GENERALIZATION VERSUS MEMORIZATION</head><p>This section studies L2NNN's generalization through a noisy-data experiment where we randomize some or all MNIST training labels. The setup is similar to <ref type="bibr" target="#b34">Zhang et al. (2017)</ref>, except that we added three scenarios where 25%, 50% and 75% of training labels are scrambled.</p><p>Table <ref type="table" target="#tab_4">5</ref> shows the comparison between L2NNNs and ordinary networks. Dropout rate and weightdecay weight are tuned for each WD/DR run, and each WD+DR+ES run uses the combined hyperparameters from its row. In early-stopping runs, 5000 training images are withheld as validation set and training stops when loss on validation set stops decreasing. The L2NNNs do not use weight decay, dropout or early stopping. L2NNNs achieve the best accuracy in all three partially-scrambled To illustrate why L2NNNs generalize better than ordinary networks from noisy data, we show in Table <ref type="table">6</ref> trade-off points between accuracy and confidence gap on the 50%-scrambled training set. These trade-off points are achieved by changing hyperparameters ω in (3) and v in (5). In a noisy training set, there exist data points that are close to each other yet have different labels. For a pair of such points, if an L2NNN is to classify both points correctly, the two confidence gaps must be small. Therefore, in order to achieve large average confidence gap, an L2NNN must misclassify some of the training data. In Table <ref type="table">6</ref>, as we adjust the loss function to favor larger average gap, the L2NNNs are forced to make more and more mistakes on the training set. The results suggest that loss is minimized when an L2NNN misclassifies some of the scrambled labels while fitting the 50% original labels with large gaps, and parameter training discovers this trade-off automatically. Hence we see in Table <ref type="table">6</ref> increasing accuracies and gaps on the test set. The above is a trade-off between memorization (training-set accuracy) and generalization (training-set average gap), and we hypothesize that L2NNN's trade-off between nominal accuracy and robustness, reported in Section 3.1, is due to the same mechanism. To be fair, dropout and early stopping are also able to sacrifice accuracy on a noisy training set, however they do so through different mechanisms that tend to be brittle, and Table <ref type="table" target="#tab_4">5</ref> suggests that L2NNN's mechanism is superior. More discussions and the trade-off tables for 25% and 75% scenarios are in the appendix.</p><p>Another interesting observation is that the average confidence gap dramatically shrinks in the last row of Table <ref type="table" target="#tab_4">5</ref> where the training is pure memorization. This is not surprising again due to training data points that are close to each other yet have different labels. The practical implication is that after an L2NNN model is trained, one can simply measure its average confidence gap to know whether and how much it has learned to generalize rather than to memorize the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RELATED WORK</head><p>Adversarial defense is a well-known difficult problem <ref type="bibr" target="#b28">(Szegedy et al., 2014;</ref><ref type="bibr" target="#b15">Goodfellow et al., 2014;</ref><ref type="bibr" target="#b5">Carlini &amp; Wagner, 2017a;</ref><ref type="bibr" target="#b2">Athalye et al., 2018;</ref><ref type="bibr" target="#b14">Gilmer et al., 2018)</ref>. There are many avenues to defense <ref type="bibr" target="#b6">(Carlini &amp; Wagner, 2017b;</ref><ref type="bibr" target="#b22">Meng &amp; Chen, 2017)</ref>, and here we will focus on defense works that fortify a neural network itself instead of introducing additional components.</p><p>The mainstream approach has been adversarial training, where examples of successful attacks on a classifier itself are used in training <ref type="bibr" target="#b29">(Tramèr et al., 2017;</ref><ref type="bibr" target="#b33">Zantedeschi et al., 2017)</ref>. The work of <ref type="bibr" target="#b21">Madry et al. (2017)</ref> has the best results to date and effectively flattens gradients around training data points, and, prior to our work, it is the only work that achieves sizable white-box defense. It has been reported in <ref type="bibr" target="#b8">Carlini et al. (2017)</ref> that, for a small network, adversarial training indeed increases the average minimum L 1 -norm and L ∞ -norm of noise needed to change its classification. However, in view of results of Tables <ref type="table" target="#tab_1">1 and 2</ref>  <ref type="formula">2018</ref>) which achieved provable guarantees against L ∞ -bounded attacks. However there exist scalability issues with respect to network depth, and the reported results so far are against relatively weak attacks or low noise magnitude. As shown in Table <ref type="table">4</ref>, we can match their measured L ∞ -bounded defense.</p><p>Controlling Lipschitz constants also regularizes a network over the entire input space. <ref type="bibr" target="#b28">Szegedy et al. (2014)</ref> is the seminal work that brings attention to this topic. <ref type="bibr" target="#b3">Bartlett et al. (2017)</ref> proposes the notion of spectrally-normalized margins as an indicator of generalization, which are strongly related to our confidence gap. <ref type="bibr" target="#b23">Pascanu et al. (2013)</ref> studies the role of the spectral radius of weight matrices in the vanishing and the exploding gradient problems. <ref type="bibr">Yoshida &amp; Miyato (2017)</ref> proposes a method to regularize the spectral radius of weight matrices and shows its effect in reducing generalization gap. The work on Parseval networks <ref type="bibr" target="#b10">(Cisse et al., 2017)</ref> shows that it is possible to control Lipschitz constants of neural networks through regularization. The core of their work is to constrain linear and convolution layer weights to be composed of Parseval tight frames, i.e., orthonormal filters, and thereby force the Lipschitz constant of these layers to be 1; they also propose to restrict aggregation operations. The reported robustness results of <ref type="bibr" target="#b10">Cisse et al. (2017)</ref>, however, are much weaker than those by adversarial training in <ref type="bibr" target="#b21">Madry et al. (2017)</ref>. We differ from Parseval networks in a number of ways. Our linear and convolution layers do not require filters to be orthogonal to each other and subsume Parseval layers as a special case, and therefore provide more freedom to parameter training. We use non-standard techniques, e.g. two-sided ReLU, to modify various network components to maximize confidence gaps while keeping the network nonexpansive, and we propose a new loss function for the same purpose. We are unable to obtain Parseval networks for a direct comparison, however it is possible to get a rough idea of what the comparison might be by looking at Table <ref type="table" target="#tab_2">3</ref> which shows the impacts of those new techniques. The work of <ref type="bibr" target="#b17">Hein &amp; Andriushchenko (2017)</ref> makes an important point regarding guarantees provided by local Lipschitz constants, which helps explain many observations in our results, including why adversarial training on L2NNNs leads to lasting robustness gains. The regularization proposed by <ref type="bibr" target="#b17">Hein &amp; Andriushchenko (2017)</ref> however is less practical and again introduces reliance on the coverage of training data points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS AND FUTURE WORK</head><p>In this work we have presented L 2 -nonexpansive neural networks which are well-conditioned systems by construction. Practical techniques are developed for building these networks. Their properties are studied through experiments and benefits demonstrated, including that our MNIST and CIFAR-10 classifiers exceed the state of the art in robustness against white-box adversarial attacks, that they are robust against partially random training labels, and that they output confidence gaps which are strongly correlated with robustness and generalization. There are a number of future directions, for example, other applications of L2NNN, L2NNN-friendly neural network architectures, and the relation between L2NNNs and interpretability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A L 2 -NONEXPANSIVE NETWORK COMPONENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 ADDITIONAL METHODS FOR WEIGHT REGULARIZATION</head><p>There are numerous ways to utilize the bound of (2). The main text describes a simple method of using W ′ = W/ b (W ) to enforce strict nonexpansiveness. The following is an alternative.</p><p>Approximate nonexpansiveness can be achieved by adding a penalty to the loss function whenever b (W ) exceeds 1, for example:</p><formula xml:id="formula_7">L W = min l(W T W ), l(W W T ) , where l (M ) = i max   j |M i,j | − 1, 0   (7)</formula><p>The sum of ( <ref type="formula">7</ref>) losses over all layers becomes a fourth term in the loss function ( <ref type="formula" target="#formula_3">3</ref>), multiplied with one additional hyperparameter. This would lead to an approximate L2NNN with trade-offs between how much its layers violate (1) with surrogate (2) versus other objectives in the loss function.</p><p>In practice, we have found that it is beneficial to begin neural network training with the regularization scheme of ( <ref type="formula">7</ref>), which allows larger learning rates, and switch to the first scheme of using W ′ , which avoids artifacts of an extra hyperparameter, when close to convergence. Of course if the goal is building approximate L2NNNs one can use (7) all the way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 SIGMOID AND OTHERS</head><p>Sigmoid is nonexpansive as is, but does not preserve distance as much as possible. A better way is to replace sigmoid with the following operator</p><formula xml:id="formula_8">s (x) = t • sigmoid 4x t<label>(8)</label></formula><p>where t &gt; 0 is a trainable parameter and each neuron has its own t. In general, the requirement for any scalar nonlinearity is that its derivative is bounded between -1 and 1. If a nonlinearity violates this condition, a shrinking multiplier can be applied. If the actual range of derivative is narrower, as in the case of sigmoid, an enlarging multiplier can be applied to preserve distance.</p><p>For further improvement, (8) can be combined with the general form of the two-sided ReLU of Section 2.2. Then the new nonlinearity is a function from R to R 2 that computes s(x) and s(x) − x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 SPLITTING AND RECONVERGENCE</head><p>There are different kinds of splitting in neural networks. Some splitting is not followed by reconvergence. For example, a classifier may have common layers followed by split layers for each label, and such an architecture can be viewed as multiple L2NNNs that overlap at the common layers and each contain one stack of split layers. In such cases, no modification is needed because there is no splitting within each individual L2NNN. Some splitting, however, is followed by reconvergence. In fact, convolution and pooling layers discussed earlier can be viewed as splitting, and reconvergence happens at the next layer. Another common example is skip-level connections such as in ResNet. Such splitting should be viewed as making two copies of a certain vector. Let the before-split vector be x 0 , and we make two copies as</p><formula xml:id="formula_9">x 1 = t • x 0 x 2 = 1 − t 2 • x 0<label>(9)</label></formula><p>where t ∈ [0, 1] is a trainable parameter.</p><p>In the case of ResNet, the reconvergence is an add operator, which should be treated as vectormatrix multiplication as in Section 2.1, but with much simplified forms. Let x 1 be the skip-level connections and f (x 2 ) be the channels of convolution outputs to be added with x 1 , we perform the addition as</p><formula xml:id="formula_10">y = t • x 1 + 1 − t 2 • f (x 2 )<label>(10)</label></formula><p>where t ∈ [0, 1] is a trainable parameter and could be a common parameter with (9).</p><p>ResNet-like reconvergence is referred to as aggregation layers in <ref type="bibr" target="#b10">Cisse et al. (2017)</ref> and a different formula was used:</p><formula xml:id="formula_11">y = α • x 1 + (1 − α) • f (x 2 )<label>(11</label></formula><p>) where α ∈ [0, 1] is a trainable parameter. Because splitting is not modified in <ref type="bibr" target="#b10">Cisse et al. (2017)</ref>, their scheme may seem approximately equivalent to ours if a common t parameter is used for ( <ref type="formula" target="#formula_9">9</ref>) and (10). However, there is a substantial difference: in many ResNet blocks, f (x 2 ) is a subset of rather than all of the output channels of convolution layers, and our scheme does not apply the shrinking factor of √ 1 − t 2 on channels that are not part of f (x 2 ) and therefore better preserve distances. In contrast, because splitting is not modified, at reconvergence the scheme of <ref type="bibr" target="#b10">Cisse et al. (2017)</ref> must apply the shrinking factor of 1 − α on all outputs of convolution layers, regardless of whether a channel is part of the aggregation or not. To state the difference in more general terms, our scheme enables splitting and reconvergence at arbitrary levels of granularity and multiplies shrinking factors to only the necessary components. We can also have a different t per channel or even per entry.</p><p>To be fair, the scheme of <ref type="bibr" target="#b10">Cisse et al. (2017)</ref> has an advantage of being nonexpansive with respect to any L p -norm. However, for L 2 -norm, it is inferior to ours in preserving distances and maximizing confidence gaps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 RECURSION</head><p>There are multiple ways to interpret recurrent neural networks (RNN) as L2NNNs. One way is to view an unrolled RNN as multiple overlapping L2NNNs where each L2NNN generates the output at one time step. Under this interpretation, nothing special is needed and recurrent inputs to a neuron are simply treated as ordinary inputs.</p><p>Another way to interpret an RNN is to view unrolled RNN as a single L2NNN that generates outputs at all time steps. Under this interpretation, recurrent connections are treated as splitting at their sources and should be handled as in (9).</p><p>A.5 NORMALIZATION Normalization operations are limited in an L2NNN. Subtracting mean is nonexpansive and allowed, and subtract-mean operation can be performed on arbitrary subsets of any layer. Subtracting batch mean is also allowed because it can be viewed as subtracting a bias parameter. However, scaling, e.g., division by standard deviation or batch standard deviation is only allowed if the multiplying factors are between -1 and 1. To satisfy this in practice, one simple method is to divide all multiplying factors in a normalization layer by the largest of their absolute values. of the images with small confidence gaps are genuinely ambiguous. It's worth noting the strong correlation between the confidence gap of L2NNN and the magnitude of distortion needed to force it to misclassify. Also note that our guarantee states that the minimum L 2 -norm of noise is half of the confidence gap, but in reality the needed noise is much stronger than the guarantee. The reason is that the true local guarantee is in fact larger due to local Lipschitz constants, as pointed out by <ref type="bibr" target="#b17">Hein &amp; Andriushchenko (2017)</ref>. The latter three all lead to misclassification as 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B MNIST IMAGES</head><p>Figure <ref type="figure" target="#fig_4">5</ref> shows additional details regarding the example in Figure <ref type="figure" target="#fig_0">1</ref>. The first image is the original image of a zero. The second image is an attack on Model 2 <ref type="bibr" target="#b21">(Madry et al., 2017)</ref> found after 1K iterations, with noise L 2 -norm of 4.4. The third is one found after 10K iterations for Model 2, with noise L 2 -norm of 2.1. The last image is the best attack on our Model 3 found after one million iterations, with noise L 2 -norm of 3.5. These illustrates the trend shown in Table <ref type="table" target="#tab_0">1</ref> that the defense by adversarial training diminishes as the attacks are allowed more iterations, while L2NNNs withstand strong attacks and it requires more noise to fool an L2NNN. It's worth noting that the slow degradation of Model 2's accuracy is an artifact of the attacker <ref type="bibr" target="#b5">(Carlini &amp; Wagner, 2017a)</ref>: when gradients are near zero in some parts of the input space, which is true for MNIST Model 2 due to adversarial training, it takes more iterations to make progress. It is conceivable that, with a more advanced attacker, Model 2 could drop quickly to 7.6%. What truly matter are the robust accuracies where we advance the state of the art from 7.6% to 24.4%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C DETAILS OF SCRAMBLED-LABEL EXPERIMENTS</head><p>For ordinary networks in Table <ref type="table" target="#tab_4">5</ref>, we use two network architectures. The first has 4 layers and is the architecture used in <ref type="bibr" target="#b21">Madry et al. (2017)</ref>. The second has 22 layers and is the architecture of Models 3 and 4 in Table <ref type="table" target="#tab_0">1</ref>, which includes norm-pooling and two-sided ReLU. Results of ordinary networks using these two architectures are in Tables <ref type="table" target="#tab_6">7 and 8</ref> respectively. The ordinary-network section of Table <ref type="table" target="#tab_4">5</ref> is entry-wise max of Tables <ref type="table" target="#tab_6">7 and 8</ref>.</p><p>In Tables <ref type="table" target="#tab_6">7 and 8</ref>, dropout rate and weight-decay weight are tuned for each WD/DR run, and each WD+DR+ES run uses the combined hyperparameters from its row. In early-stopping runs, 5000 training images are withheld as validation set and training stops when loss on validation set stops decreasing. Each ES or WD+DR+ES entry is an average over ten runs to account for randomness of the validation set. The L2NNNs do not use weight decay, dropout or early stopping.</p><p>Table <ref type="table" target="#tab_7">9</ref> shows L2NNN trade-off points between accuracy and confidence gap on the 25%-scrambled training set. Table <ref type="table" target="#tab_8">10</ref> shows L2NNN trade-off points between accuracy and confidence gap on To be fair, dropout and early stopping are also able to sacrifice accuracy on a noisy training set. For example, the DR run in the 50%-scrambled row in Table <ref type="table" target="#tab_5">7</ref> has 67.5% accuracy on the training set and 72.6% on the test set. However, the underlying mechanisms are very different from that of L2NNN. Dropout <ref type="bibr" target="#b27">(Srivastava et al., 2014)</ref> has an effect of data augmentation, and, with a noisy training set, dropout can create a situation where the effective data complexity exceeds the network capacity. Therefore, the parameter training is stalled at a lowered accuracy on the training set, and we get better performance if the model tends to fit more of original labels and less of the scrambled labels. The mechanism of early stopping is straightforward and simply stops the training when it is mostly memorizing scrambled labels. We get better performance from early stopping if the parameter training tends to fit the original labels early. These mechanisms from dropout and early stopping are both brittle and may not allow parameter training enough opportunity to learn from the useful data points with original labels. The comparison in Table <ref type="table" target="#tab_4">5</ref> suggests that they are inferior to L2NNN's trade-off mechanism as discussed in Section 3.3 and illustrated in Tables 6, 9 and 10. The L2NNNs in this paper do not use weight decay, dropout or early stopping, however it is conceivable that dropout may be complementary to L2NNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D PROOFS</head><p>Lemma 1. Let g (x) denote a single-L2NNN classifier's confidence gap for an input data point x. The classifier will not change its answer as long as the input x is modified by no more than an L 2 -norm of g (x) / √ 2.</p><p>Proof. Let y (x) = [y 1 (x) , y 2 (x) , • • • , y K (x)] denote logit vector of a single-L2NNN classifier for an input data point x. Let x 1 and x 2 be two input vectors such that the classifier outputs different labels i and j. By definitions, we have the following inequalities: Because the classifier is a single L2NNN, it must be true that:</p><formula xml:id="formula_12">y i (x 1 ) − y j (x 1 ) ≥ g (x 1 ) y i (x 2 ) − y j (x 2 ) ≤ 0 (12)</formula><p>x 2 − x 1 2 ≥ y (x 2 ) − y (x 1 ) 2 ≥ (y i (x 2 ) − y i (x 1 ))</p><p>2 + (y j (x 2 ) − y j (x 1 )) 2 = (y i (x 1 ) − y i (x 2 )) 2 + (y j (x 2 ) − y j (x 1 ))</p><p>2 ≥ (y i (x 1 ) − y i (x 2 ) + y j (x 2 ) − y j (x 1 )) 2 2 = ((y i (x 1 ) − y j (x 1 )) + (y j (x 2 ) − y i (x 2 )))</p><formula xml:id="formula_13">2 2 ≥ (g (x 1 ) + 0) 2 2 = g (x 1 ) / √ 2<label>(13)</label></formula><p>Lemma 2. Let g (x) denote a classifier's confidence gap for an input data point x. Let d (x 1 , x 2 ) denote the L 2 -distance between the output logit-vectors for two input points x 1 and x 2 that have different labels and that are classified correctly. Then this condition holds: g (x 1 ) + g (x 2 ) ≤ √ 2 • d (x 1 , x 2 ).</p><p>Proof. Let y (x) = [y 1 (x) , y 2 (x) , • • • , y K (x)] denote logit vector of a classifier for an input data point x. Let i and j be the labels for x 1 and x 2 . By definitions, we have the following inequalities: y i (x 1 ) − y j (x 1 ) ≥ g (x 1 ) y j (x 2 ) − y i (x 2 ) ≥ g (x 2 ) (14) Therefore, d (x 1 , x 2 ) y (x 2 ) − y (x 1 ) 2 ≥ (y i (x 2 ) − y i (x 1 )) 2 + (y j (x 2 ) − y j (x 1 )) 2 = (y i (x 1 ) − y i (x 2 )) 2 + (y j (x 2 ) − y j (x 1 ))</p><p>2 ≥ (y i (x 1 ) − y i (x 2 ) + y j (x 2 ) − y j (x 1 )) Proof. For any x 1 &gt; x 2 , by definition we have the following inequalities:</p><formula xml:id="formula_14">f (x 1 ) − f (x 2 ) ≥ 0 f (x 1 ) − f (x 2 ) ≤ x 1 − x 2<label>(17)</label></formula><p>For any p ≥ 1, invoking Lemma 3 with a = f (x 1 ) − f (x 2 ) and b = x 1 − x 2 − f (x 1 ) + f (x 2 ), we have:</p><formula xml:id="formula_15">((f (x 1 ) − f (x 2 )) p + (x 1 − x 2 − f (x 1 ) + f (x 2 )) p ≤ (x 1 − x 2 ) p (((f (x 1 ) − f (x 2 )) p + (x 1 − x 2 − f (x 1 ) + f (x 2 )) p ) 1/p ≤ x 1 − x 2 (|f (x 1 ) − f (x 2 )| p + |(f (x 1 ) − x 1 ) − (f (x 2 ) − x 2 )| p ) 1/p ≤ x 1 − x 2 h(x 1 ) − h(x 2 ) p ≤ x 1 − x 2<label>(18)</label></formula><p>Lemma 5. Norm-pooling within each pooling window is a nonexpansive map with respect to L 2norm.</p><p>Proof. Let x 1 and x 2 be two vectors with the size of a pooling window. By triangle inequality, we have</p><formula xml:id="formula_16">x 1 − x 2 2 + x 1 2 ≥ x 2 2 x 1 − x 2 2 + x 2 2 ≥ x 1 2 (19) Therefore, x 1 − x 2 2 ≥ x 2 2 − x 1 2 x 1 − x 2 2 ≥ x 1 2 − x 2 2 (20) Therefore, x 1 − x 2 2 ≥ | x 1 2 − x 2 2 | (21)</formula><p>Lemma 6. Let g (x) denote a multi-L2NNN classifier's confidence gap for an input data point x. The classifier will not change its answer as long as the input x is modified by no more than an L 2 -norm of g (x) /2.</p><p>Proof. Let y (x) = [y 1 (x) , y 2 (x) , • • • , y K (x)] denote logit vector of a multi-L2NNN classifier for an input data point x. Let x 1 and x 2 be two input vectors such that the classifier outputs different labels i and j. By definitions, we have the following inequalities:</p><formula xml:id="formula_17">y i (x 1 ) − y j (x 1 ) ≥ g (x 1 ) y i (x 2 ) − y j (x 2 ) ≤ 0 (22)</formula><p>For a multi-L2NNN classifier, each logit is a nonexpansive function of the input, and it must be true that:</p><formula xml:id="formula_18">x 2 − x 1 2 ≥ |y i (x 1 ) − y i (x 2 )| x 2 − x 1 2 ≥ |y j (x 2 ) − y j (x 1 )|<label>(23)</label></formula><p>Therefore,</p><p>x 2 − x 1 2 ≥ |y i (x 1 ) − y i (x 2 )| + |y j (x 2 ) − y j (x 1 )| 2 ≥ |y i (x 1 ) − y i (x 2 ) + y j (x 2 ) − y j (x 1 )| 2 = |(y i (x 1 ) − y j (x 1 )) + (y j (x 2 ) − y i (x 2 ))| 2 ≥ |g (x 1 ) + 0| 2 = g (x 1 ) /2 (24)</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Attacks on Model 2 found after 1K and 10K iterations: the same 0 recognized as 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Accuracy percentages of classifiers on test data bin-sorted by the confidence gap.</figDesc><graphic url="image-3.png" coords="7,108.00,282.35,396.08,111.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>, adversarial-training results may be susceptible to strong attacks. The works of Drucker &amp; Le Cun (1992); Ross &amp; Doshi-Velez (2017) are similar to adversarial training in aiming to flatten gradients around training data set but use different mechanisms. While the above approaches fortify a network around training data points, others aim to bound a network's responses to input perturbations over the entire input space. For example, Haber &amp; Ruthotto (2017) models ResNet as an ordinary differential equation and derive stability conditions. Other examples include Kolter &amp; Wong (2017); Raghunathan et al. (2018); Wong et al. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: Original and distorted images of MNIST digits in test set with the largest confidence gaps. Mstk denotes the misclassified labels. Dist denotes the L 2 -norm of the distortion noise.Let us begin by showing MNIST images with the largest confidence gaps in Figure3and those with the smallest confidence gaps in Figure4. They include images before and after attacks as well as Model 3's confidence gap, the misclassified label and L 2 -norm of the added noise. The images with large confidence gaps seem to be ones that are most different from other digits, while some</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Original image of 0; attack on Model 2 (Madry et al., 2017) found after 1K iterations; attack on Model 2 found after 10K iterations; attack on Model 3 (L2NNN) found after 1M iterations. The latter three all lead to misclassification as 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>i (x 1 ) − y j (x 1 )) + (y j (x 2 ) − y i (x 2 ))) For any a ≥ 0, b ≥ 0, p ≥ 1, the following inequality holds:a p + b p ≤ (a + b) p .Proof. If a and b are both zero, the inequality holds. If at least one of a and b is nonzero:a p + b p = (a + b) Let f (x)be a nonexpansive and monotonically increasing scalar function. Define a function from R to R 2 : h(x) = [f (x), f (x) − x]. Then h(x) is nonexpansive with respect to any L p -norm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell cols="5">MaxIter Model1 Model2 Model3 Model4</cell></row><row><cell>Natural</cell><cell>99.1%</cell><cell>98.5%</cell><cell>98.7%</cell><cell>98.2%</cell></row><row><cell>100</cell><cell>70.2%</cell><cell>91.7%</cell><cell>77.6%</cell><cell>75.6%</cell></row><row><cell>1000</cell><cell>0.05%</cell><cell>51.5%</cell><cell>20.3%</cell><cell>24.4%</cell></row><row><cell>10K</cell><cell>0%</cell><cell>16.0%</cell><cell>20.1%</cell><cell>24.4%</cell></row><row><cell>100K</cell><cell>0%</cell><cell>9.8%</cell><cell>20.1%</cell><cell>24.4%</cell></row><row><cell>1M</cell><cell>0%</cell><cell>7.6%</cell><cell>20.1%</cell><cell>24.4%</cell></row></table><note>Accuracies of MNIST classifiers under white-box non-targeted attacks with noise L 2 -norm limit of 3. MaxIter is the max number of iterations the attacker uses. Model 1 is an ordinarily trained model. Model 2 is the model from Madry et al. (2017). Model 3 is L2NNN without adversarial training. Model 4 is L2NNN with adversarial training.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell cols="5">MaxIter Model1 Model2 Model3 Model4</cell></row><row><cell>Natural</cell><cell>95.0%</cell><cell>87.1%</cell><cell>79.2%</cell><cell>77.2%</cell></row><row><cell>100</cell><cell>0%</cell><cell>13.9%</cell><cell>10.2%</cell><cell>20.8%</cell></row><row><cell>1000</cell><cell>0%</cell><cell>9.4%</cell><cell>10.1%</cell><cell>20.4%</cell></row><row><cell>10K</cell><cell>0%</cell><cell>9.0%</cell><cell>10.1%</cell><cell>20.4%</cell></row><row><cell>1000x10</cell><cell>0%</cell><cell>8.7%</cell><cell>10.1%</cell><cell>20.4%</cell></row><row><cell>100K</cell><cell>0%</cell><cell>NA</cell><cell>10.1%</cell><cell>20.4%</cell></row></table><note>Accuracies of CIFAR-10 classifiers under white-box non-targeted attacks with noise L 2norm limit of 1.5. MaxIter is the max number of iterations the attacker uses, and 1000x10 indicates 10 runs each with 1000 iterations. Model 1 is an ordinarily network. Model 2 is the model from<ref type="bibr" target="#b21">Madry et al. (2017)</ref>. Model 3 is L2NNN without adversarial training. Model 4 is L2NNN with adversarial training.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Ablation studies: MNIST model without weight regularization; one without L c loss; one with max-pooling instead of norm-pooling; one without two-sided ReLU; Gap is average confidence gap. R-Accu is under attacks with 1000 iterations and with noise L 2 -norm limit of 3.</figDesc><table><row><cell></cell><cell cols="2">Accu. Gap R-Accu.</cell></row><row><cell>no weight reg.</cell><cell>99.4% 68.3</cell><cell>0%</cell></row><row><cell>no L c loss no norm-pooling</cell><cell>99.2% 2.2 98.8% 1.3</cell><cell>8.9% 9.9%</cell></row><row><cell cols="2">no two-sided ReLU 98.0% 2.5</cell><cell>15.1%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>are two points along a trade-off curve that are controllable by varying hyperparameter ω in loss function (3). Other trade-off points have nominal accuracy and under-attack accuracy of (98.8%,19.1%), (98.4%,22.6%) and (97.9%,24.7%) respectively. Similar trade-offs have been reported by other robustness works including adversarial training<ref type="bibr" target="#b30">(Tsipras et al., 2019)</ref> and adversarial polytope<ref type="bibr" target="#b31">(Wong et al., 2018)</ref>. It remains an open question whether such trade-off is a necessary part of life, and please see Section 3.3 for further discussion on the L2NNN trade-off. Table4shows our results, again measured with the attack code of<ref type="bibr" target="#b5">Carlini &amp; Wagner (2017a)</ref>. The ǫ values match those used in<ref type="bibr" target="#b24">Raghunathan et al. (2018);</ref><ref type="bibr" target="#b19">Kolter &amp; Wong (2017)</ref>;<ref type="bibr" target="#b21">Madry et al. (2017)</ref>. Our MNIST L ∞ results are on par with<ref type="bibr" target="#b24">Raghunathan et al. (2018);</ref><ref type="bibr" target="#b19">Kolter &amp; Wong (2017)</ref> but not as good as<ref type="bibr" target="#b21">Madry et al. (2017)</ref>. Our CIFAR-10 Model 4 is on par with<ref type="bibr" target="#b21">Madry et al. (2017)</ref> for L ∞ defense.</figDesc><table><row><cell cols="4">Table 4: Accuracy of L2NNN classifiers under white-box non-targeted attacks with 1000 iterations</cell></row><row><cell>and with noise L ∞ -norm limit of ǫ.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>ǫ</cell><cell cols="2">Model3 Model4</cell></row><row><cell>MNIST</cell><cell>0.1</cell><cell>90.9%</cell><cell>92.4%</cell></row><row><cell>MNIST</cell><cell>0.3</cell><cell>7.0%</cell><cell>44.0%</cell></row><row><cell cols="3">CIFAR-10 8 /256 32.3%</cell><cell>42.5%</cell></row></table><note>Although we primarily focus on defending against L 2 -bounded adversarial attacks in this work, we achieve some level of robustness against L ∞ -bounded attacks as a by-product.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Accuracy comparison of MNIST classifiers that are trained on noisy data. Rand is the percentage of training labels that are randomized. WD is weight decay. DR is dropout. ES is early stopping. Gap1 is L2NNN's average confidence gap on training set and Gap2 is that on test set. it is remarkable that an L2NNN can deliver 93.1% accuracy on test set when three quarters of training labels are random. More detailed data and discussions are in the appendix.</figDesc><table><row><cell>Rand</cell><cell></cell><cell></cell><cell cols="2">Ordinary network</cell><cell></cell><cell>L2NNN</cell><cell></cell></row><row><cell></cell><cell>Vanilla</cell><cell>WD</cell><cell>DR</cell><cell>ES</cell><cell>WD+DR+ES</cell><cell cols="2">Gap1 Gap2</cell></row><row><cell>0</cell><cell cols="4">99.4% 99.0% 99.2% 99.0%</cell><cell>99.3%</cell><cell>98.7% 2.84</cell><cell>2.82</cell></row><row><cell>25%</cell><cell cols="4">90.4% 91.1% 91.8% 96.2%</cell><cell>98.0%</cell><cell>98.5% 0.64</cell><cell>0.63</cell></row><row><cell>50%</cell><cell cols="4">65.5% 67.7% 72.6% 81.0%</cell><cell>88.3%</cell><cell>96.0% 0.58</cell><cell>0.60</cell></row><row><cell>75%</cell><cell cols="4">41.5% 44.9% 41.8% 75.2%</cell><cell>66.4%</cell><cell>93.1% 0.86</cell><cell>0.89</cell></row><row><cell>100%</cell><cell>9.7%</cell><cell>9.1%</cell><cell>9.4%</cell><cell>NA</cell><cell>NA</cell><cell>11.9% 0.09</cell><cell>0.01</cell></row><row><cell cols="8">Table 6: Training-accuracy-versus-confidence-gap trade-off points of L2NNNs on 50%-scrambled</cell></row><row><cell cols="2">MNIST training labels.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">on training set</cell><cell>on test set</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Accu. Gap Accu. Gap</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">98.7% 0.17 79.0% 0.12</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">96.5% 0.21 79.3% 0.18</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">89.4% 0.22 86.3% 0.20</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">70.1% 0.36 93.4% 0.37</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">66.1% 0.45 93.7% 0.47</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">59.8% 0.58 96.0% 0.60</cell><cell></cell><cell></cell></row><row><cell>scenarios, and</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 :</head><label>7</label><figDesc>Accuracies of non-L2NNN MNIST classifiers that use a 4-layer architecture and that are trained on training data with various amounts of scrambled labels. Rand is the percentage of training labels that are randomized. WD is weight decay. DR is dropout. ES is early stopping.</figDesc><table><row><cell>Rand</cell><cell></cell><cell></cell><cell cols="2">Ordinary network</cell><cell></cell></row><row><cell></cell><cell>Vanilla</cell><cell>WD</cell><cell>DR</cell><cell>ES</cell><cell>WD+DR+ES</cell></row><row><cell>0</cell><cell cols="4">98.9% 99.0% 99.2% 99.0%</cell><cell>99.3%</cell></row><row><cell>25%</cell><cell cols="4">82.5% 91.1% 91.8% 79.1%</cell><cell>98.0%</cell></row><row><cell>50%</cell><cell cols="4">57.7% 67.7% 72.6% 66.4%</cell><cell>88.3%</cell></row><row><cell>75%</cell><cell cols="4">32.1% 44.9% 41.8% 52.7%</cell><cell>66.4%</cell></row><row><cell>100%</cell><cell>9.5%</cell><cell>8.9%</cell><cell>9.4%</cell><cell>NA</cell><cell>NA</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 :</head><label>8</label><figDesc>Accuracies of non-L2NNN MNIST classifiers that use a 22-layer architecture and that are trained on training data with various amounts of scrambled labels. Rand is the percentage of training labels that are randomized. WD is weight decay. DR is dropout. ES is early stopping.</figDesc><table><row><cell>Rand</cell><cell></cell><cell></cell><cell cols="2">Ordinary network</cell><cell></cell></row><row><cell></cell><cell>Vanilla</cell><cell>WD</cell><cell>DR</cell><cell>ES</cell><cell>WD+DR+ES</cell></row><row><cell>0</cell><cell cols="4">99.4% 99.0% 99.0% 99.0%</cell><cell>99.0%</cell></row><row><cell>25%</cell><cell cols="4">90.4% 86.5% 89.8% 96.2%</cell><cell>90.3%</cell></row><row><cell>50%</cell><cell cols="4">65.5% 62.5% 63.7% 81.0%</cell><cell>83.1%</cell></row><row><cell>75%</cell><cell cols="4">41.5% 38.2% 40.2% 75.2%</cell><cell>61.9%</cell></row><row><cell>100%</cell><cell>9.7%</cell><cell>9.1%</cell><cell>8.8%</cell><cell>NA</cell><cell>NA</cell></row></table><note>the 75%-scrambled training set. Like Table6, they demonstrate the trade-off mechanism between memorization (training-set accuracy) and generalization (training-set average gap).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 9 :</head><label>9</label><figDesc>Training-accuracy-versus-confidence-gap trade-off points of L2NNNs on 25%-scrambled MNIST training labels.</figDesc><table><row><cell>on training set</cell><cell>on test set</cell></row><row><cell cols="2">Accu. Gap Accu. Gap</cell></row><row><cell cols="2">99.6% 0.12 92.6% 0.10</cell></row><row><cell cols="2">97.6% 0.20 95.7% 0.17</cell></row><row><cell cols="2">78.6% 0.31 98.2% 0.30</cell></row><row><cell cols="2">77.2% 0.64 98.5% 0.63</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 10 :</head><label>10</label><figDesc>Training-accuracy-versus-confidence-gap trade-off points of L2NNNs on 75%-scrambled MNIST training labels.</figDesc><table><row><cell>on training set</cell><cell>on test set</cell></row><row><cell cols="2">Accu. Gap Accu. Gap</cell></row><row><cell cols="2">97.9% 0.07 49.8% 0.03</cell></row><row><cell cols="2">93.0% 0.09 59.2% 0.05</cell></row><row><cell cols="2">75.9% 0.10 70.0% 0.08</cell></row><row><cell cols="2">58.0% 0.18 80.4% 0.17</cell></row><row><cell cols="2">46.2% 0.29 86.8% 0.30</cell></row><row><cell cols="2">40.1% 0.44 89.8% 0.46</cell></row><row><cell cols="2">34.7% 0.86 93.1% 0.89</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">This is only an example and we recommend building a classifier as multiple L2NNNs, see Section</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">.4.2  See Lemma 1 in Appendix D for the proof of the guarantee.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">See Lemma 2 in Appendix D for the proof of the condition.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">The spectral radius of a matrix is no greater than its natural L∞-norm. W T W and W W T have the same non-zero eigenvalues and hence the same spectral radius.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">See Lemma 4 in Appendix D for the proof of nonexpansiveness.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5">See Lemma 5 in Appendix D for the proof of nonexpansiveness.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6">See Lemma 6 in Appendix D for the proof of the guarantee. The guarantee in either Lemma 1 or Lemma 6 is only a loose guarantee and it has been shown in<ref type="bibr" target="#b17">Hein &amp; Andriushchenko (2017)</ref> that a larger guarantee exists by analyzing local Lipschitz constants, though it is expensive to compute.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7">At github.com/MadryLab/mnist_challenge and github.com/MadryLab/cifar10_challenge. These models (Model 2's in Tables1 and 2) were built by adversarial training with L∞-bounded adversaries<ref type="bibr" target="#b21">(Madry et al., 2017)</ref>. To the best of our knowledge,<ref type="bibr" target="#b30">Tsipras et al. (2019)</ref> from the same lab is the only paper in the literature that reports on models trained with L2-bounded adversaries, and it reports that training with L2-bounded adversaries resulted in weaker L2 robustness than the L2 robustness results from training with L∞-bounded adversaries in<ref type="bibr" target="#b21">Madry et al. (2017)</ref>. Therefore we choose to compare against the best available models, even though they were trained with L∞-bounded adversaries. Note also that our own Model 4's in Tables1 and 2are trained with the same L∞-bounded</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8">adversaries.9  In reading Tables1 and 2, it is worth remembering that the norm of after-attack accuracy is zero, and for example the 7.6% on MNIST is currently the state of the art.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Threat of adversarial attacks on deep learning in computer vision: A survey</title>
		<author>
			<persName><forename type="first">Naveed</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajmal</forename><surname>Mian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.00553</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Did you hear that? Adversarial examples against automatic speech recognition</title>
		<author>
			<persName><forename type="first">Moustafa</forename><surname>Alzantot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharathan</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mani</forename><surname>Srivastava</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.00554</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples</title>
		<author>
			<persName><forename type="first">Anish</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.00420</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Spectrally-normalized margin bounds for neural networks</title>
		<author>
			<persName><forename type="first">Dylan</forename><forename type="middle">J</forename><surname>Peter L Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matus</forename><forename type="middle">J</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><surname>Telgarsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6241" to="6250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A theoretical analysis of feature pooling in visual recognition</title>
		<author>
			<persName><forename type="first">Y-Lan</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="111" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Towards evaluating the robustness of neural networks</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Symposium on Security and Privacy</title>
				<meeting>the IEEE Symposium on Security and Privacy</meeting>
		<imprint>
			<date type="published" when="2017">2017a</date>
			<biblScope unit="page" from="39" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adversarial examples are not easily detected: Bypassing ten detection methods</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security</title>
				<meeting>the 10th ACM Workshop on Artificial Intelligence and Security</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017b</date>
			<biblScope unit="page" from="3" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Audio adversarial examples: Targeted attacks on speech-totext</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.01944</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Provably minimally-distorted adversarial examples</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">L</forename><surname>Dill</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.10207</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Hongge</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.02051</idno>
		<title level="m">Show-and-fool: Crafting adversarial examples for neural image captioning</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Parseval networks: Improving robustness to adversarial examples</title>
		<author>
			<persName><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="854" to="863" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improving generalization performance using double backpropagation</title>
		<author>
			<persName><forename type="first">Harris</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Le Cun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="991" to="997" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Javid</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anyi</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Lowd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dejing</forename><surname>Dou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.06751</idno>
		<title level="m">Hotflip: White-box adversarial examples for nlp</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Black-box generation of adversarial text sequences to evade deep learning classifiers</title>
		<author>
			<persName><forename type="first">Ji</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Lanchantin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mary</forename><forename type="middle">Lou</forename><surname>Soffa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanjun</forename><surname>Qi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.04354</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fartash</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maithra</forename><surname>Samuel S Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.02774</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Adversarial spheres. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6572</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Stable architectures for deep neural networks</title>
		<author>
			<persName><forename type="first">Eldad</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Ruthotto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inverse Problems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">14004</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Formal guarantees on the robustness of a classifier against adversarial manipulation</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maksym</forename><surname>Andriushchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2263" to="2273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Kawaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leslie</forename><forename type="middle">Pack</forename><surname>Kaelbling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.05468</idno>
		<title level="m">Generalization in deep learning</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Provable defenses against adversarial examples via the convex outer adversarial polytope</title>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Zico</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00851</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Fooling end-to-end speaker verification by adversarial examples</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Kreuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yossi</forename><surname>Adi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Keshet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.03339</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Vladu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06083</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Magnet: a two-pronged defense against adversarial examples</title>
		<author>
			<persName><forename type="first">Dongyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security</title>
				<meeting>the 2017 ACM SIGSAC Conference on Computer and Communications Security</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="135" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Certified defenses against adversarial examples</title>
		<author>
			<persName><forename type="first">Aditi</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.09344</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Slavin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Finale</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.09404</idno>
		<title level="m">Improving the adversarial robustness and interpretability of deep neural networks by regularizing their input gradients</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Understanding and improving convolutional neural networks via concatenated rectified linear units</title>
		<author>
			<persName><forename type="first">Wenling</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2217" to="2225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
				<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Ensemble adversarial training: Attacks and defenses</title>
		<author>
			<persName><forename type="first">Florian</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Boneh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07204</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Robustness may be at odds with accuracy</title>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shibani</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Scaling provable adversarial defenses</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Hendrik Metzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J Zico</forename><surname>Kolter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Yuichi Yoshida and Takeru Miyato. Spectral norm regularization for improving the generalizability of deep learning</title>
		<author>
			<persName><forename type="first">Huan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shie</forename><surname>Mannor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.10941</idno>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="391" to="423" />
			<date type="published" when="2012">2012. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Robustness and generalization</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Efficient defenses against adversarial attacks</title>
		<author>
			<persName><forename type="first">Valentina</forename><surname>Zantedeschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria-Irina</forename><surname>Nicolae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ambrish</forename><surname>Rawat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security</title>
				<meeting>the 10th ACM Workshop on Artificial Intelligence and Security</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="39" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
				<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
