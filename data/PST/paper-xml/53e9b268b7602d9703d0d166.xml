<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Object Detection Using Feature Subset Selection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zehang</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Lab. Department of Computer Science</orgName>
								<orgName type="institution">University of Nevada</orgName>
								<address>
									<settlement>Reno</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">George</forename><surname>Bebis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Lab. Department of Computer Science</orgName>
								<orgName type="institution">University of Nevada</orgName>
								<address>
									<settlement>Reno</settlement>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Ronald</forename><surname>Miller</surname></persName>
							<email>rmille47@ford.com</email>
							<affiliation key="aff1">
								<orgName type="department">Vehicle Design R&amp;A Department</orgName>
								<orgName type="institution">Ford Motor Company</orgName>
								<address>
									<settlement>Dearborn</settlement>
									<region>MI</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Object Detection Using Feature Subset Selection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">398BF0B0AA12543A095CE32A83D421B0</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>feature subset selection</term>
					<term>genetic algorithms</term>
					<term>vehicle detection</term>
					<term>face detection</term>
					<term>support vector machines</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Past work on object detection has emphasized the issues of feature extraction and classification, however, relatively less attention has been given to the critical issue of feature selection. The main trend in feature extraction has been representing the data in a lower dimensional space, for example, using Principal Component Analysis (PCA). Without using an effective scheme to select an appropriate set of features in this space, however, these methods rely mostly on powerful classification algorithms to deal with redundant and irrelevant features. In this paper, we argue that feature selection is an important problem in object detection and demonstrate that Genetic Algorithms (GAs) provide a simple, general, and powerful framework for selecting good subsets of features, leading to improved detection rates. As a case study, we have considered PCA for feature extraction and Support Vector Machines (SVMs) for classification. The goal is searching the PCA space using GAs to select a subset of eigenvectors encoding important information about the target concept of interest. This is in contrast to traditional methods selecting some percentage of the top eigenvectors to represent the target concept, independently of the classification task. We have tested the proposed framework on two challenging applications: vehicle detection and face detection. Our experimental results illustrate significant performance improvements in both cases.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Introduction</head><p>The majority of real-world object detection problems involve "concepts" (e.g., face, vehicle) rather than specific objects. Usually, these "conceptual objects" have large within class variabilities. As a result, there is no easy way to come up with an analytical decision boundary to separate a certain "conceptual object" against others. One feasible approach is to learn the decision boundary from a set of training examples using supervised learning where each training instance is associated with a class label. Building an object detection system under this framework involves two main steps (1) extracting a number of features, and (2) training a classifier using the extracted features to distinguish among different class instances.</p><p>Choosing an appropriate set of features is critical when designing pattern classification systems under the framework of supervised learning. Often, a large number of features is extracted to better represent the target concept. Without employing some kind of feature selection strategy, however, many of them could be either redundant or even irrelevant to the classification task. Watanabe <ref type="bibr" target="#b0">[1]</ref> has shown that it is possible to make two arbitrary patterns similar by encoding them with a sufficiently large number of redundant features. As a result, the classifier might not be able to generalize nicely.</p><p>Ideally, we would like to use only features having high separability power while ignoring or paying less attention to the rest. For instance, in order to allow a vehicle detector to generalize nicely, it would be necessary to exclude features encoding fine details which might appear in specific vehicles only. A limited yet salient feature set can simplify both the pattern representation and the classifiers that are built on the selected representation. Consequently, the resulting classifier will be more efficient.</p><p>In most practical cases, relevant features are not known a-priori. Finding out what features to use in a classification task is referred to as feature selection. Although there has been a great deal of work in machine learning and related areas to address this issue <ref type="bibr" target="#b1">[2]</ref>[3] these results have not been fully explored or exploited in emerging computer vision applications. Only recently there has been an increased interest in deploying feature selection in applications such as face detection <ref type="bibr" target="#b3">[4]</ref> <ref type="bibr" target="#b4">[5]</ref>, gender classification <ref type="bibr" target="#b5">[6]</ref> <ref type="bibr" target="#b6">[7]</ref>, vehicle detection <ref type="bibr" target="#b3">[4]</ref> <ref type="bibr" target="#b7">[8]</ref>, image fusion for face recognition <ref type="bibr" target="#b8">[9]</ref>, target detection <ref type="bibr" target="#b9">[10]</ref>, pedestrian detection <ref type="bibr" target="#b10">[11]</ref>, tracking <ref type="bibr" target="#b11">[12]</ref>, image retrieval <ref type="bibr" target="#b12">[13]</ref>, and video categorization <ref type="bibr" target="#b13">[14]</ref>.</p><p>Most efforts in the literature have largely ignored the feature selection problem and have focused mainly on developing effective feature extraction methods <ref type="bibr" target="#b14">[15]</ref> and employing powerful classifiers (e.g., probabilistic <ref type="bibr" target="#b15">[16]</ref>, Hidden Markov Models (HMMs) <ref type="bibr" target="#b16">[17]</ref> , Neural Networks (NNs) <ref type="bibr" target="#b17">[18]</ref>, SVMs <ref type="bibr" target="#b18">[19]</ref>). The main trend in feature extraction has been representing the data in a lower dimensional space computed through a linear or non-linear transformation satisfying certain properties (e.g., PCA <ref type="bibr" target="#b19">[20]</ref>, Linear Discriminant Analysis (LDA) <ref type="bibr" target="#b20">[21]</ref>, Independent Components Analysis (ICA) <ref type="bibr" target="#b21">[22]</ref>,</p><p>Factor Analysis (FA) <ref type="bibr" target="#b22">[23]</ref>, and others <ref type="bibr" target="#b14">[15]</ref>). The goal is finding a new set of features that represent the target concept in a more compact and robust way but also providing more discriminative information. Without using effective schemes to select an appropriate subset of features in the computed subspaces, however, these methods rely mostly on classification algorithms to deal with the issues of redundant and irrelevant features. This might be problematic, especially when the number of training examples is small compared to the number of features (i.e., curse of dimensionality problem <ref type="bibr">[15][24]</ref>).</p><p>We argue and demonstrate the importance of feature selection in the context of two challenging object detection problems: vehicle detection and face detection. As a case study, we have considered the well-known method of PCA for feature extraction and SVMs for classification. Feature extraction using PCA has received considerable attention in the computer vision area <ref type="bibr" target="#b19">[20]</ref>[25] <ref type="bibr" target="#b25">[26]</ref>. It represents an image in a low dimensional space spanned by the principal components of the covariance matrix of the data. Although PCA provides a way to represent an image in an optimum way (i.e., minimizes reconstruction error), several studies have shown that not every principal eigenvectors encode useful information for classification purposes. We elaborate more on this issue in Section IV-A).</p><p>In this paper, we propose using GAs to search the space of eigenvectors with the goal of selecting a subset of eigenvectors encoding important information about the target concept of interest. This is in contrast to the typical strategy of picking a percentage of the top eigenvectors to represent the target concept, independently of the classification task.</p><p>The proposed approach has the advantage that it is simple, general, and powerful. An earlier version of this work has appeared in <ref type="bibr" target="#b3">[4]</ref> and it relates to our previous work on gender classification <ref type="bibr" target="#b5">[6]</ref>[7], however, the size of the classes considered here (e.g., object vs. non-object) are larger and in principle quite different from each other.</p><p>The rest of the paper is organized as follows: In Section II, we review the problem of feature selection, emphasizing different search and evaluation strategies. An overview of the proposed method is presented in Section III. In Section IV we discuss feature extraction using PCA. In particular, we discuss the problem of understanding the information encoded by different eigenvectors. Section V, presents our approach to choosing an appropriate subset of eigenvectors using genetic search. In particular, we discuss the issues of encoding and fitness evaluation. Section VI presents a brief review on SVMs. Our experimental results and comparisons using genetic eigenvector selection for vehicle and face detection are presented in Sections VII and VIII correspondingly. An analysis of our experimental results is presented in Section IX. Finally, Section X presents our conclusions and plans for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. Background on Feature Selection</head><p>Finding out which features to use for a particular problem is referred to as feature selection. Given a set of d features, the problem is selecting a subset of size m that leads to the smallest classification error. This is essentially an optimization problem that involves searching the space of possible feature subsets to find one that is optimal or near-optimal with respect to a certain criterion. A number of feature selection approaches have been proposed in the literature <ref type="bibr" target="#b26">[27]</ref>[2][3], <ref type="bibr" target="#b27">[28]</ref>[15] <ref type="bibr" target="#b28">[29]</ref>. There are two main components in every feature subset selection system: the search strategy used to pick the feature subsets and the evaluation method used to test their goodness based on some criteria. We review both of them below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Search strategies</head><p>Search strategies can be classified into one of the following three categories: (1) optimal, (2) heuristic, and (3) randomized. Exhaustive search is the most straightforward approach to optimal feature selection <ref type="bibr" target="#b14">[15]</ref>. However, the number of possible subsets grows combinatorially, which makes the exhaustive search impractical for even moderate size of features. The only optimal feature selection method which avoids the exhaustive search is based on the branch and bound algorithm <ref type="bibr">[27][2]</ref>. This method requires the monotonicity property of the criterion function, which most commonly used criterion function do not satisfy. Sequential Forward Selection (SFS) and Sequential Backward Selection (SBS) are two well-known heuristic feature selection schemes <ref type="bibr" target="#b29">[30]</ref>. SFS, starting with an empty feature set, selects the best single feature and then adds that feature to the feature set. SBS starts with the entire feature set and at each step drops the feature whose absence least decreases the performance. Combining SFS and SBS gives birth to the "plus l-take away r" feature selection method <ref type="bibr" target="#b30">[31]</ref>, which first enlarges the feature subset by adding l using SFS and then deletes r features using SBS. Sequential Forward Floating Search (SFFS) and Sequential Backward Floating Search (SBFS) <ref type="bibr" target="#b31">[32]</ref> are generalizations of the "plus l -take away r" method . The values of l and r are determined automatically and updated dynamically in SFFS and SBFS. Since these strategies make local decisions, they cannot be expected to find globally optimal solutions. In randomized search, probabilistic steps or a sampling process are employed. The Relief algorithm <ref type="bibr" target="#b32">[33]</ref> and several extension of it <ref type="bibr" target="#b33">[34]</ref> are the typical randomized search approaches. Based on their estimated effectiveness for classification, features are assigned weights in the relief algorithm. Then, features whose weights exceed a user-determined threshold are selected to train the classifier. Recently, GAs <ref type="bibr" target="#b34">[35]</ref> have attracted more and more attention in the feature selection area. Siedlecki et al. <ref type="bibr" target="#b35">[36]</ref> presented one of the earliest studies of GA-based feature selection in the context of a Knearest-neighbor classifiers. Yang et al. <ref type="bibr" target="#b28">[29]</ref> proposed a feature selection approach using GAs and NNs for classification.</p><p>A standard GA with rank-based selection strategy was used. The rank-based selection method depends on a predefined parameter p ∈ (0.5 1). Specifically, the probability of selecting the highest ranked individual is p and that of the kth highest ranked individual is p(1 -p) (k-1) . They tested their methods using several benchmark real-world pattern classification problems and reported improved results. However, they used the accuracy on the test set in the fitness function, which is not appropriate since it introduces bias to the final classification. Chtioui et al. <ref type="bibr" target="#b36">[37]</ref> investigated a GA approach for feature selection in a seed discrimination problem. Using standard GA operators, they selected the best feature subset from a set of 73 features. Vafaie et al. <ref type="bibr" target="#b37">[38]</ref> conducted a comparison between important score (IS)-a greedy-like feature section method and GAs. They represented the feature selection problem using binary encoding and standard GA operators. The evaluation function was solely based on classification performance. Using several real world problems, they found that GAs are more robust at the expense of more computational effort.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation strategies</head><p>Each of the evaluation strategies belongs to one of two categories: (1) filter and (2) wrapper. The distinction is made depending on whether feature subset evaluation is performed using the learning algorithm employed in the classifier design (i.e., wrapper) or not (i.e., filter). Filter approaches are computationally more efficient than wrapper approaches since they evaluate the goodness of selected features using criteria that can be tested quickly (e.g., reducing the correlation or the mutual information among features). This, however, could lead to non-optimal features, especially, when the features dependent on the classifier. As a result, classifier performance might be poor. Wrapper approaches on the other hand perform evaluation by training the classifier using the selected features and estimating the classification error using a validation set. Although this is a slower procedure, the features selected are usually more optimal for the classifier employed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. Method Overview</head><p>Traditionally, there are three main steps in building a pattern classification system using supervised learning. First, some preprocessing is applied to the input patterns (e.g., normalize the pattern with respect to size and orientation, compensate for light variations, reduce noise etc.). Second, feature extraction is applied to represent patterns by a compact set of features. The last step involves training a classifier to learn to assign input patterns to their correct category. In most cases, no explicit feature selection step takes place besides feature weighting performed implicitly by the classifier. Fig. <ref type="figure" target="#fig_1">1</ref> illustrates the main steps of the approach employed here. The main difference from the traditional approach is the inclusion of a step that performs feature selection using GAs. Feature extraction is carried out using PCA to project the data in a lower-dimensional space. The goal of feature selection is then to choose a subset of eigenvectors in this space, encoding mostly important information about the target concept of interest. We use a wrapper-based approach to evaluate the quality of the selected eigenvectors. Specifically, we use feedback from a SVM classifier to guide the GA search in selecting a good subset of eigenvectors, improving detection accuracy. The evaluation function used here contains two terms, the first based on classification accuracy on a validation set and the second on the number of eigenvectors selected. Given a set of eigenvectors, a binary encoding scheme is used to represent the presence or absence of a particular eigenvector in the solutions generated during evolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. Feature Extraction Using PCA</head><p>Eigenspace representations of images use PCA to linearly project an image in a low-dimensional space <ref type="bibr" target="#b19">[20]</ref>. This space is spanned by the principal components (i.e., eigenvectors corresponding to the largest eigenvalues ) of the distribution of the training images. After an image has been projected in the eigenspace, a feature vector containing the coefficients of the projection is used to represent the image. We summarize the main ideas below: Each image I(x, y) is represented as a N × N vector Γ i . First the average face Ψ is computed:</p><formula xml:id="formula_0">Ψ = 1 R R i=1 Γ i (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where R is the number of faces in the training set. Next, the difference Φ of each face from the average face is computed:</p><formula xml:id="formula_2">Φ i = Γ i -Ψ.</formula><p>Then the covariance matrix is estimated by:</p><formula xml:id="formula_3">C = 1 R R i=1 Φ i Φ T i = AA T ,<label>(2)</label></formula><p>where</p><formula xml:id="formula_4">, A = [Φ 1 Φ 2 . . . Φ R ].</formula><p>The eigenspace can then be defined by computing the eigenvectors µ i of C. Since C is very large (N × N ), computing its eigenvector will be very expensive. Instead, we can compute ν i , the eigenvectors of A T A, an R × R matrix. Then µ i can be computed from ν i as follows:</p><formula xml:id="formula_5">µ i = R j=1 ν ij Φ j , j = 1 . . . R. (<label>3</label></formula><formula xml:id="formula_6">)</formula><p>Usually, we only need to keep a smaller number of eigenvectors R k corresponding to the largest eigenvalues. Given a new image, Γ, we subtract the mean (Φ = Γ -Ψ) and compute the projection:</p><formula xml:id="formula_7">Φ = R k i=1 w i µ i . (<label>4</label></formula><formula xml:id="formula_8">)</formula><p>where w i = µ T i Γ are the coefficients of the projection (i.e., eigenfeatures).</p><p>The projection coefficients allow us to represent images as linear combinations of the eigenvectors. It is well known that the projection coefficients define a compact image representation and that a given image can be reconstructed from its projection coefficients and the eigenvectors (i.e., basis).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. What Information is Encoded by Different Eigenvectors?</head><p>There have been several attempts to understand what information is encoded by different eigenvectors, and the usefulness of this information with respect to various tasks <ref type="bibr" target="#b38">[39]</ref>[40][41] <ref type="bibr" target="#b41">[42]</ref>. These studies have concluded that different tasks make different demands in terms of the information that needs to be processed, and that this information is not contained in the same ranges of eigenvectors. For example, the first few eigenvectors seem to encode lighting while other eigenvectors seem to encode local features <ref type="bibr" target="#b41">[42]</ref>. We have made similar observations by analyzing the eigenvectors obtained from our data sets. Fig. <ref type="figure" target="#fig_2">2</ref>, for example, shows some of the eigenvectors computed from our vehicle detection data set. Obviously, eigenvectors 2 and 4 encode more lighting information than others, while eigenvectors 8 and 12</p><p>encode more information about some specific local features. Similar comments can be made for the eigenvectors derived from our face detection data set, as shown in Fig. <ref type="figure">3</ref>. Once again, eigenvectors 2 and 5 seem to encode mostly lighting while eigenvectors 8, 9 and 22 seem to encode mostly local information. Eigenvector 150 seems to encode mostly noise in both cases.</p><p>Obviously, the question is how to choose eigenvectors encoding important information about the target concept of interest. The common practice of choosing the eigenvectors corresponding to large eigenvalues might not be the best choice as has been illustrated by Balci et al. <ref type="bibr" target="#b42">[43]</ref>, Etemad et al. <ref type="bibr" target="#b20">[21]</ref>, and Sun et al. <ref type="bibr">[6][7]</ref>. In <ref type="bibr" target="#b42">[43]</ref>, PCA features were used with a NN classifier. Using pruning to improve classifier performance, they were also able to monitor which eigenvectors contribute to gender classification. Their experiments showed that not all of the high eigenvectors contributed to gender classification and that some of them had been discarded by the network. In <ref type="bibr" target="#b20">[21]</ref>, the discriminatory power of eigenvectors in a face recognition task was investigated. They found out that the recognition information of eigenvectors does not decrease monotonically with their corresponding eigenvalues. Many times, there were cases where an eigenvector corresponding to a small eigenvalue had higher discriminatory power than an eigenvector corresponding to a large eigenvalue. In this study, we apply feature selection using GAs to search the space of eigenvectors with the goal of selecting a subset of them encoding important information about the target concept of interest. In <ref type="bibr" target="#b5">[6]</ref>[7], the problem of selecting a subset of eigenvectors representing mostly gender information was considered. Using an approach similar to the one proposed here, it was illustrated that certain eigenvectors, not necessarily the top ones, were more important for gender classification than others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. Genetic Eigenvector Selection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. A brief review of GAs</head><p>GAs are a class of optimization procedures inspired by the biological mechanisms of reproduction. In the past, they have been used to solve various problems including target recognition <ref type="bibr" target="#b43">[44]</ref>, object recognition <ref type="bibr" target="#b44">[45]</ref>[46], face recognition <ref type="bibr" target="#b46">[47]</ref>, and face detection/verification <ref type="bibr" target="#b47">[48]</ref>. This section contains a brief summary of the fundamentals of GAs. Goldberg <ref type="bibr" target="#b34">[35]</ref> provides a great introduction to GAs and the reader is referred to this source, as well as to the survey paper of Srinivas et al. <ref type="bibr" target="#b48">[49]</ref> for further information.</p><p>GAs operate iteratively on a population of structures, each one of which represents a candidate solution to the problem at hand, properly encoded as a string of symbols (e.g., binary). A randomly generated set of such strings forms the initial population from which the GA starts its search. Three basic genetic operators guide this search: selection, crossover, and mutation. The genetic search process is iterative: evaluating, selecting, and recombining strings in the population during each iteration (generation) until reaching some termination condition. The basic algorithm, where P (t) is the population of strings at generation t, is given below: In summary, selection probabilistically filters out solutions that perform poorly, choosing high performance solutions to concentrate on or exploit. Crossover and mutation, through string operations, generate new solutions for exploration.</p><formula xml:id="formula_9">t =</formula><p>Given an initial population of elements, GAs use the feedback from the evaluation process to select fitter solutions, eventually converging to a population of high performance solutions. GAs do not guarantee a global optimum solution.</p><p>However, they have the ability to search through very large search spaces and come to nearly optimal solutions fast.</p><p>Their ability for fast convergence is explained by the schema theorem (i.e., short-length bit patterns in the chromosomes with above average fitness, get exponentially growing number of trials in subsequent generations <ref type="bibr" target="#b34">[35]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Feature Selection Encoding</head><p>We have employed a simple encoding scheme where the chromosome is a bit string whose length is determined by the number of eigenvectors. Each eigenvector, computed using PCA, is associated with one bit in the string. If the i th bit is 1, then the i th eigenvector is selected, otherwise, that component is ignored. Each chromosome thus represents a different subset of eigenvectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Feature Subset Fitness Evaluation</head><p>The goal of feature subset selection is to use less features to achieve the same or better performance. Therefore, the fitness evaluation contains two terms: (1) accuracy and (2) the number of features selected. The performance of the SVM is estimated using a validation data set (see Sections VII-A and VIII-A) which guides the GA search. Each feature subset contains a certain number of eigenvectors. If two subsets achieve the same performance, while containing different number of eigenvectors, the subset with fewer eigenvectors is preferred. Between accuracy and feature subset size, accuracy is our major concern. We used the fitness function shown below to combine the two terms:</p><formula xml:id="formula_10">f itness = 10 4 Accuracy + 0.5Zeros<label>(5)</label></formula><p>where Accuracy corresponds to the classification accuracy on a validation set for a particular subset of eigenvectors, and Zeros corresponds to the number eigenvectors not selected (i.e., zeros in the chromosome). The Accuracy term ranges roughly from 0.50 to 0.99, thus, the first term assumes values from 5000 to 9900. The Zeros term ranges from 0 to L -1 where L is the length of the chromosome, thus, the second term assumes values from 0 to 99 (L = 200).</p><p>Based on the weights that we have assigned to each term, the Accuracy term dominates the fitness value. This implies that individuals with higher accuracy will outweigh individuals with lower accuracy, no matter how many features they contain. Overall, the higher the accuracy is, the higher the fitness is. Also, the fewer the number of features is, the higher the fitness is.</p><p>Choosing the weights for the two terms of the fitness function is more objective-dependent than application-dependent.</p><p>When we build a pattern classification system, among many factors, we need to find the best balance between model compactness and performance accuracy. Under some scenarios, we prefer the best performance, no matter what the cost might be. If this is the case, the weight associated with the Accuracy term should be very high. Under different situations, we might favor more compact models over accuracy, as long as the accuracy is within a satisfactory range.</p><p>In this case, we should choose a higher weight for the Zeros term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Initial Population</head><p>In general, the initial population is generated randomly, (e.g., each bit in an individual is set by flipping a coin). This, however, would produce a population where each individual contains approximately the same number of 1's and 0's on the average. To explore subsets of different numbers of features, the number of 1's for each individual is generated randomly. Then, the 1's are randomly scattered in the chromosome. In all of our experiments, we used a population size of 2000 and 200 generations. In most cases, the GA converged in less than 200 generations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Selection</head><p>Our selection strategy was cross generational. Assuming a population of size N , the offspring double the size of the population and we select the best N individuals from the combined parent-offspring population <ref type="bibr" target="#b49">[50]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Crossover</head><p>There are three basic types of crossovers: one-point crossover, two-point crossover, and uniform crossover. For onepoint crossover, the parent chromosomes are split at a common point chosen randomly and the resulting sub-chromosomes are swapped. For two-point crossover, the chromosomes are thought of as rings with the first and last gene connected (i.e., wrap-around structure). In this case, the rings are split at two common points chosen randomly and the resulting sub-rings are swapped. Uniform crossover is different from the above two schemes. In this case, each gene of the offspring is selected randomly from the corresponding genes of the parents. Since we do not know in general how eigenvectors depend on each other, if dependent eigenvectors are far apart in the chromosome, it is very likely that traditional onepoint or two-point crossover will destroy the schemata. To avoid this problem, uniform crossover is used here. The crossover probability used in all of our experiments was 0.66.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Mutation</head><p>We use the traditional mutation operator which just flips a specific bit with a very low probability. The mutation probability used in all of our experiments was 0.04.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. Support Vector Machines</head><p>SVMs are primarily two-class classifiers that have been shown to be an attractive and more systematic approach to learn linear or non-linear decision boundaries [51] <ref type="bibr" target="#b51">[52]</ref>. Their key characteristic is their mathematical tractability and geometric interpretation. This has facilitated a rapid growth of interest in SVMs over the last few years, demonstrating remarkable success in fields as diverse as text categorization, bioinformatics, and computer vision <ref type="bibr" target="#b52">[53]</ref>. Specific applications include text classification <ref type="bibr" target="#b53">[54]</ref>, speed recognition <ref type="bibr" target="#b54">[55]</ref>, gene classification <ref type="bibr" target="#b55">[56]</ref>, and webpage classification <ref type="bibr" target="#b56">[57]</ref>.</p><p>Given a set of points, which belong to either of two classes, SVM finds the hyperplane leaving the largest possible fraction of points of the same class on the same side, while maximizing the distance of either class from the hyperplane. This is equivalent to performing structural risk minimization to achieve good generalization <ref type="bibr">[51] [52]</ref>. Assuming there are l examples from two classes</p><formula xml:id="formula_11">(x 1 , y 1 )(x 2 , y 2 )...(x l , y l ), x i ∈ R N , y i ∈ {-1, +1}<label>(6)</label></formula><p>Finding the optimal hyper-plane implies solving a constrained optimization problem using quadratic programming. The optimization criterion is the width of the margin between the classes. The discriminate hyperplane is defined as:</p><formula xml:id="formula_12">f (x) = l i=1 y i a i k(x, x i ) + b (7)</formula><p>where k(x, x i ) is a kernel function and the sign of f (x) indicates the membership of x. Constructing the optimal hyperplane is equivalent to find all the nonzero a i . Any data point x i corresponding to a nonzero a i is a support vector of the optimal hyperplane.</p><p>Suitable kernel functions can be expressed as a dot product in some space and satisfy the Mercer's condition <ref type="bibr" target="#b50">[51]</ref>.</p><p>By using different kernels, SVMs implement a variety of learning machines (e.g., a sigmoidal kernel corresponding to a two-layer sigmoidal neural network while a Gaussian kernel corresponding to a radial basis function (RBF) neural network). The Gaussian radial basis kernel is given by</p><formula xml:id="formula_13">k(x, x i ) = exp(- x -x i 2 2δ 2 ) (<label>8</label></formula><formula xml:id="formula_14">)</formula><p>The Gaussian kernel is used in this study. Our experiments have shown that the Gaussian kernel outperforms other kernels in the context of our applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. Vehicle Detection</head><p>Robust and reliable vehicle detection in images acquired by a moving vehicle (i.e., on-road vehicle detection) is an important problem with applications to driver assistance systems or autonomous, self-guided vehicles. This is a very challenging task in general. Vehicles, for example, come into view with different speeds and may vary in shape, size, and color. Also, vehicle appearance depends on its pose and is affected by nearby objects. Within-class variability, occlusion, and lighting conditions also change the overall appearance of vehicles. Landscape along the road changes continuously while the lighting conditions depend on the time of the day and the weather.</p><p>Research on vehicle detection has been quite active within the last ten years. Matthews et al. <ref type="bibr" target="#b57">[58]</ref> used PCA for feature extraction and NNs for classification. Goerick et al. <ref type="bibr" target="#b58">[59]</ref> employed Local Orientation Coding (LOC) to encode edge information and NNs to learn the characteristics of vehicles. A statistical model was investigated in <ref type="bibr" target="#b15">[16]</ref> where PCA and wavelet features were used to represent vehicle and non-vehicle appearance. A different statistical model was investigated by Weber et al. <ref type="bibr" target="#b59">[60]</ref>. They represented each vehicle image as a constellation of local features and used the Expectation Maximization(EM) algorithm <ref type="bibr" target="#b23">[24]</ref> to learn the parameters of the probability distribution of the constellations. An interest operator, followed by clustering, is used to identify a small number of local features in vehicle images. In <ref type="bibr" target="#b60">[61]</ref>, Papageorgiou et al. proposed using Haar wavelets for feature extraction and SVMs for classification.</p><p>Sun et al. <ref type="bibr" target="#b61">[62]</ref> fused Gabor and Haar wavelet features to improve detection accuracy.</p><p>Here, we consider the problem of rear-view vehicle detection from gray-scale images. The first step of any vehicle detection system is to hypothesize the locations of vehicles in an image. Then, verification is performed to test the hypotheses. Both steps are equally important and challenging. Approaches to generate the hypothetical locations of vehicles in an images use motion information, symmetry, shadows, and vertical/horizontal edges. Our emphasis here is on improving the performance of the verification step by selecting a representative feature subset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Vehicle Dataset</head><p>The images used in our experiments were collected in Dearborn, Michigan during two different sessions, one in the Summer of 2001 and one in the Fall of 2001. To ensure a good variety of data in each session, the images were caught during different times, different days, and on five different highways. The training set contains subimages of rear vehicle views and non-vehicles which were extracted manually from the Fall 2001 data set. A total of 1051 vehicle subimages and 1051 non-vehicle subimages were extracted (see Fig. <ref type="figure">4</ref>). In <ref type="bibr" target="#b60">[61]</ref>, the subimages were aligned by wrapping the bumpers to approximately the same position. We have not attempted to align the data in our case since alignment requires detecting certain features on the vehicle accurately. Moreover, we believe that some variability in the extraction of the subimages can actually improve performance. Each subimage in the training and test sets was scaled to 32 × 32 and preprocessed to account for different lighting conditions and contrast followed the method suggested in <ref type="bibr" target="#b47">[48]</ref>.</p><p>To evaluate the performance of the proposed approach, the average error (ER) was recorded using a three-fold cross-validation procedure. Specifically, we split the training dataset randomly three times by keeping 80% of the vehicle subimages and 80% of the non-vehicle subimages (i.e., 841 vehicle subimages and 841 non-vehicle subimages) for training. The rest 20% of the data was used for validation during feature selection. For testing, we used a fixed set of 231 vehicle and non-vehicle subimages which were extracted from the Summer 2001 data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Results</head><p>We have performed a number of experiments and comparisons to demonstrate the importance of feature selection for vehicle detection. First, SVMs were tested using some percentage of the top eigenvectors. We ran several experiments by varying the number of eigenvector from 50 to 200. Using the top 50, 100, 150, and 200 eigenvectors, the average error rates obtained were 18.21%, 10.89%, 10.24%, and 10.80% respectively. Next, we used GAs to select an optimum subset of eigenvectors. For comparison purposes, we also implemented the SFBS feature selection method discussed in Section II. Fig. <ref type="figure">5</ref>(a) shows the error rates for all the approaches tested here. Using eigenvector selection, the SVM achieved a 6.49% average error rate in the case of GAs, and a 9.07% average error rate in the case of SFBS. In terms of number of eigenvectors contained in the final solution, SFBS kept 87 features, which is 43.5% of the complete feature set, while GAs kept 46 features, which is 23% of the complete feature set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. Face Detection</head><p>Face detection from a single image is a difficult task due to the variability in scale, location, orientation, pose, race, facial expression, and occlusion. Rowley <ref type="bibr" target="#b17">[18]</ref> proposed an NN-based face detection method, where pre-processed image intensity values were used to train a multilayer NN to learn the face and nonface patterns from face and nonface examples. Sung et al. <ref type="bibr" target="#b62">[63]</ref> developed a system composed of two parts, (i) a distribution-based model for face/nonface representations and (ii) a multilayer NN for classification. SVMs have been applied to face detection by Osuna et al. <ref type="bibr" target="#b18">[19]</ref>.</p><p>In that work, the inputs to the SVM were pre-processed image intensity values such as those used in <ref type="bibr" target="#b17">[18]</ref>. SVMs have also been used with wavelet features for face detection in <ref type="bibr" target="#b60">[61]</ref>. Recently, Viola et al. <ref type="bibr" target="#b4">[5]</ref> developed a face detection system using wavelet-like features and the AdaBoost learning algorithm which combines increasingly more complex classifiers are combined in a cascade. The boosting process they used selects a weak classifier at each stage of the cascade which can been seen as a feature selection process. Two recent comprehensive surveys on face detection can be found in <ref type="bibr">[25][26]</ref>.</p><p>To detect faces in an image, a fixed window is usually run across the input image. Each time, the contents of the window are given to a classifier which verifies whether there is a face in the window or not. To account for differences in face size, the input image is represented at different scales and the same procedure is repeated at each scale. Alternatively, candidate face locations in an image can be found using color, texture, or motion information. Here, we concentrate on the verification step only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Face Dataset</head><p>Our training set contains 616 faces and 616 non-faces subimages which were extracted manually from a gender dataset <ref type="bibr" target="#b5">[6]</ref> and the CMU face detection dataset <ref type="bibr" target="#b17">[18]</ref>. Several examples are shown in Fig. <ref type="figure" target="#fig_4">6</ref>. For testing, we used a fixed set of 268 face and non-face subimages which were also extracted from disjoint set of images from the CMU face detection data set. Each subimage in the training and test sets was scaled to 32 × 32 and preprocessed to account for different lighting conditions and contrast <ref type="bibr" target="#b47">[48]</ref>.</p><p>To evaluate the performance of the proposed approach, we used a three-fold cross-validation procedure, splitting the training dataset randomly three times by keeping 84% of the face subimages and 84% of the non-face subimages (i.e., 516 vehicle subimages and 516 non-face subimages) for training. The rest 16% of the data was used for validation during feature selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Results</head><p>First, we tested SVMs using a percentage of the top eigenvectors as in the case of vehicle detection. We ran several experiments by varying the number of eigenvectors from 50 to 200. Using the top 50, 100, 150, and 200 eigenvectors, the average error rates obtained were 12.31%, 11.57%, 13.81%, and 14.93% respectively. Next, we used GAs to select an optimum subset of eigenvectors. As in the case of vehicle detection, we compared the results of the GA approach with the SFBS approach. Fig. <ref type="figure">5</ref>(b) shows the average error rates for all the approaches tested here. Using eigenvector selection, the SVM achieved a 8.21% average error rate in the case of GAs, and a 10.45% average error rate in the case of SFBS. In terms of number of eigenvectors contained in the final solution, SFBS kept 68 features, which is 34% of the complete feature set, while GAs kept 34 features, which is 17% of the complete feature set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IX. Discussion</head><p>To get an idea about the optimal set of eigenvectors selected by GAs (or SFBS) in the context of vehicle/face detection, we computed histograms (see Fig. <ref type="figure" target="#fig_5">7</ref>), showing the average distributions of the selected eigenvectors over the three training sets. The x-axis corresponds to the eigenvectors, ordered by their eigenvalues, and has been divided into bins of size 10. The y-axis corresponds to the average number of times an eigenvector within some bin was selected by the GA (or SFBS) approach in the final solution. For example, Fig. <ref type="figure" target="#fig_5">7</ref>(a) shows the average distribution of the eigenvectors selected by GAs for vehicle detection. For example, the first bar of each histogram indicates that, on average, 5.7 eigenvectors were selected from the top 10 eigenvectors.</p><p>Fig. <ref type="figure" target="#fig_5">7</ref> illustrates that the eigenvector subsets selected by GA approach were different from those selected by the SFBS approach. As we have discussed in Section II, different eigenvectors seems to encode different kind of information.</p><p>For visualization purposes, we have reconstructed several vehicle (Fig. <ref type="figure" target="#fig_6">8</ref>) and face (Fig. <ref type="figure" target="#fig_7">9</ref>) images using the selected eigenvectors only. For comparison purpose, we also reconstructed the same images using the top 50 eigenvectors. Several interesting comments can be made by observing these reconstructions, the experimental results presented in Sections VII and VIII, and the eigenvector distributions shown in Fig. <ref type="figure" target="#fig_5">7</ref>:</p><p>(1) The eigenvector subsets selected by the GA approach improve detection performance, both for vehicle and face detection: Feature subsets selected by GAs yielded an average error rate of 6.49% for vehicle detection, better that the 9.07% obtained using SFBS or 10.24% using a percentage of the top eigenvectors. In the context of face detection, the average error rate using GAs was 8.21%, which is better than the average error rate using a percentage of the top eigenvectors (i.e., 11.57%) or eigenvectors selected by SBFS (i.e., 10.45%).</p><p>( (3) The eigenvectors selected by the GA approach do not encode fine details: The images shown in the fourth row of Fig. <ref type="figure" target="#fig_6">8</ref> correspond to the reconstructed vehicle images using only the eigenvectors selected by GAs. It is interesting to note that they all look quite similar to each other. As we discussed before, only some general information about vehicles is desirable for vehicle detection. These features can be thought as features representing the "conceptual vehicle", but not individual vehicles. In contrast, the reconstructed images using the top 50 eigenvectors or eigenvector subsets selected by the SFBS approach reveal more vehicle identity information (i.e., more details) as can be seen from the images in the second and third rows. Similar observations can be made by observing the reconstructed face images shown in Fig. <ref type="figure" target="#fig_7">9</ref>. The reconstructed faces shown in the last row (i.e., using eigenvectors selected by the GA approach) look more blurry (i.e., have less details) than the original images or the ones reconstructed using the top eigenvectors or those selected by the SFBS approach. Identity information has not been preserved which might be the key to successful face detection.</p><p>(4) Eigenvectors encoding irrelevant or redundant information have not been favored by the GA approach: This is obvious by observing the reconstructed images in the fourth row of Fig. <ref type="figure" target="#fig_6">8</ref>. All of them seem to be normalized with respect to illumination. Of particular interest is the image shown in the fourth column which is much lighter than the rest. It appears that eigenvectors encoding illumination information have not be included in the final eigenvector subset. This result is very reasonable since illumination is not critical for vehicle detection, if not confusing. We can also notice that the reconstructed vehicle images are better framed compared to the original ones, therefore, some kind of implicit normalization has been accomplished with respect to location and size. For face detection, we can observe similar results. Fine details have been removed from the reconstructed face images as shown in Fig. <ref type="figure" target="#fig_7">9</ref>. Moreover, we can observe normalization effects with respect to size, location, and orientation. Of particular interest is the face image shown in the fifth column of Fig. <ref type="figure" target="#fig_7">9</ref> which is rotated and illuminated from the right side. These effects have been removed</p><p>from the reconstructed image shown in the last row. This implies that eigenvectors encoding lighting and rotation have not been included in the final solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>X. Conclusions</head><p>We have investigated a systematic feature subset selection framework using GAs. Specifically, the complete feature set is encoded in a chromosome and then optimized by GAs with respect both to detection accuracy and number of discarded features. To evaluate the proposed framework, we considered two challenging object detection problems:</p><p>vehicle detection and face detection. In both cases, we used PCA for feature extraction and SVMs for classification.</p><p>Our experimental results illustrate that the proposed method improves the performance of vehicle and face detection, both in terms of accuracy and complexity (i.e., number of features). Further analysis of our results indicates that the proposed method is capable of removing redundant and irrelevant features, outperforming traditional approaches.</p><p>For future work, we plan to generalize the encoding scheme to allow eigenvector fusion (i.e., using real weights) instead of pure selection (i.e., using 0/1 weights). We also plan to investigate qualitatively different types of encodings, for example, linkage learning, inversion operators, and messy encodings <ref type="bibr" target="#b34">[35]</ref>[64] <ref type="bibr" target="#b64">[65]</ref>, as well as hybrid feature selection schemes to find better solutions faster. Filter-based approaches, for example, are much faster in finding a subset of features. One idea is to run a filter-based approach first and then use the results to initialize the GA or even "inject" some of those solutions to the GA population in certain generations to improve exploration <ref type="bibr" target="#b65">[66]</ref>. For fitness evaluation, there are many more options. Since the main goal is to to use fewer features while achieving same or better accuracy, a fitness function containing the two terms used here seems to be appropriate. However, more powerful fitness functions can be formed by including more terms such as information measures (e.g., entropy) or dependence measures (e.g.,       </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>)</head><label></label><figDesc>The GA solutions found are quite compact: The final eigenvector subsets found by GAs are very compact -46 eigenvectors out of 200 for vehicle detection, and 34 eigenvectors out of 200 for face detection. The significant reduction in the number of eigenvectors kept speeds up classification substantially.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Main steps involved in building an object detection system using feature subset selection.</figDesc><graphic coords="21,187.00,68.47,224.51,86.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Eigenvectors corresponding to the vehicle detection dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .Fig. 4 .Fig. 5 .</head><label>345</label><figDesc>Fig. 3. Eigenvectors corresponding to the face detection dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Examples of face and non-face images used for training.</figDesc><graphic coords="22,185.22,439.34,112.47,91.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. The distributions of eigenvectors selected by (a) GAs for vehicle detection , (b) SFBS for vehicle detection, (c) GAs for face detection, (d) SFBS for face detection.</figDesc><graphic coords="22,301.98,564.47,112.37,89.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Reconstructed images using the selected eigenvectors. (first row): original images; (second row): using the top 50 eigenvectors; (third row): using the eigenvectors selected by SFBS; (fourth row): using the eigenvectors selected by GAs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Reconstructed images using the selected eigenvectors. (first row): original images; (second row): using the top 50 eigenvectors; (third row): using the eigenvectors selected by SFBS; (fourth row): using the eigenvectors selected by GAs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>It probabilistically removes, from the population, those points that have relatively low fitness. Mutation, as in natural systems, is a very low probability operator and just flips a specific bit. Mutation plays the role of restoring lost genetic material. Crossover in contrast is applied with high probability. It is a randomized yet structured operator that allows information exchange between points. Its goal is to preserve the fittest individuals without introducing any new value.</figDesc><table><row><cell>0</cell></row><row><cell>initialize P(t)</cell></row><row><cell>evaluate P(t)</cell></row><row><cell>while (termination condition is not satisfied) do</cell></row><row><cell>begin</cell></row><row><cell>select P(t+1) from P(t)</cell></row><row><cell>recombine P(t+1)</cell></row><row><cell>evaluate P(t+1)</cell></row><row><cell>t = t+1</cell></row></table><note><p><p>end</p>Evaluation of each string is based on a fitness function that is problem-dependent. It determines which of the candidate solutions are better. This corresponds to the environmental determination of survivability in natural selection. Selection of a string, which represents a point in the search space, depends on the string's fitness relative to those of other strings in the population.</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements: This research was supported by Ford Motor Company under grant No.2001332R, the University of Nevada, Reno under an Applied Research Initiative (ARI) grant, and in part by NSF under CRCD grant No.0088086.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>mutual information, minimum description length).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<title level="m">Pattern Recognition: Human and Mechanical</title>
		<meeting><address><addrLine>NY</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Feature selection for classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intelligent Data Analysis</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Selection of relevant features and examples in machine learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Langley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="245" to="271" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Boosting object detection using feature selection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bebis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Advanced Video and Signal Based Surveillance</title>
		<imprint>
			<date type="published" when="2003-07">July 2003</date>
			<biblScope unit="page" from="290" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rapid object detection using a boosted cascacd of simple features</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recogntion</title>
		<meeting>Computer Vision and Pattern Recogntion</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural-network-based gender classification using genetic eigen-feature extraction</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bebis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Louis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="2002-05">May 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Genetic feature subset selection for gender classification: A comparison study</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bebis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Louis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Workshop on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2002-12">December,2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Evolutionary gabor filter optimization with application to vehicle detection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bebis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Data Mining</title>
		<imprint>
			<date type="published" when="2003-11">November 2003</date>
			<biblScope unit="page" from="307" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Infrared and visible image fusion for face recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gyaourova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bebis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Pavlidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2004-05">May, 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Genetic algorithm based feature selection for target detection in sar images</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bhanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="591" to="608" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Detecting pedestrians using patterns of motion and appearance</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Snow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On-line selection of discriminative tracking features</title>
		<author>
			<persName><forename type="first">R</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Features and classification methods to locate deciduous trees in images</title>
		<author>
			<persName><forename type="first">N</forename><surname>Haering</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Da</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitoria</forename><surname>Lobo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">1/2</biblScope>
			<biblScope unit="page" from="133" to="149" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Video frame categorization using sort-merge feature selection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kender</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="72" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Statistical pattern recognition: A review</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Duin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="37" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Probabilistic modeling of local appearance and spatial relationships for object recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Schneiderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="45" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Face recognition using an embedded hhm</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nefian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hayes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="19" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neural network-based face fetection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Rowley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baluja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="22" to="38" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Training support vector machines: An application to face detection</title>
		<author>
			<persName><forename type="first">E</forename><surname>Osuna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Girosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Computer Vision and Pattern Recognition</title>
		<meeting>of Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Eigenfaces for recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Turk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Cognitive Neuroscience</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="71" to="86" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Discriminant analysis for recognition of human face images</title>
		<author>
			<persName><forename type="first">K</forename><surname>Etemad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Optical Society of America</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1724" to="1733" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Independent components of face images: a representation for face recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">4th Annual Joint Symposium on Neural Computation</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Factor analysis for background suppression</title>
		<author>
			<persName><forename type="first">K</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Draper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stork</surname></persName>
		</author>
		<title level="m">Pattern Classification</title>
		<imprint>
			<publisher>Jon-Wiley</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Detection faces in images:a survey</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="34" to="58" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Face detection:a survey</title>
		<author>
			<persName><forename type="first">E</forename><surname>Hjelmas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Low</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="236" to="274" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">On automatic feature selection</title>
		<author>
			<persName><forename type="first">W</forename><surname>Siedlecki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sklansky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Pattern Recognition and Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="197" to="220" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Feature selection: Evaluation, application, and small sample performance</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zongker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="153" to="158" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Feature subset selection using a genetic algorithm</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Honavar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A Data Mining Perspective</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Motoda</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</editor>
		<imprint>
			<publisher>Kluwer</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">On the effectiveness of receptors in recognition systems</title>
		<author>
			<persName><forename type="first">T</forename><surname>Marill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Green</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="11" to="17" />
			<date type="published" when="1963">1963</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">On selecting features for pattern classifiers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Stearns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Third International Conference of Pattern Recognition</title>
		<imprint>
			<date type="published" when="1976">1976</date>
			<biblScope unit="page" from="71" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Floating search methods in feature selection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Pudil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Novovicova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letter</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1119" to="1125" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A practical approach to feature selection</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rendell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Ninth International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Estimating attributes: Analysis and extension of relief</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wiskott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fellous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kruger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Malsburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="171" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Goldberg</surname></persName>
		</author>
		<title level="m">Genetic Algorithms in Search, Optimization, and Machine Learning</title>
		<imprint>
			<publisher>Addison Wesley</publisher>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A note on genetic algorithm for large-scale feature selection</title>
		<author>
			<persName><forename type="first">W</forename><surname>Siedlecki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sklansky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition letter</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="335" to="347" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Feature selection by a genetic algorithm. application to seed discrimination by artificial vision</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chtioui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bertrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Barba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Sci. Food Agric</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="77" to="86" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Feature selection methods: Genetic algorithms vs. greedy-like search</title>
		<author>
			<persName><forename type="first">H</forename><surname>Vafaie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Imam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Fuzzy and Intelligent Control Systems</title>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A low-dimensional representation of faces in the higher dimensions of space</title>
		<author>
			<persName><forename type="first">A</forename><surname>O'toole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adbi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Deffenbacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Valentin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the optical society of America</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="405" to="411" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">More about the difference between men and women: evidence from linear neural networks and the principal component approach</title>
		<author>
			<persName><forename type="first">H</forename><surname>Abdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Edelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>O'toole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="539" to="562" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Can a linear autoassociator recognize faces from new orientation?</title>
		<author>
			<persName><forename type="first">D</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Abdi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Optical Society of America</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="717" to="724" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Analyzing pca-based face recognition algorithms: Eigenvector selection and distance measures</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yambor</surname></persName>
			<affiliation>
				<orgName type="collaboration">2nd Workshop on Empirical Evaluation in Computer Vision</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Draper</surname></persName>
			<affiliation>
				<orgName type="collaboration">2nd Workshop on Empirical Evaluation in Computer Vision</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Beveridge</surname></persName>
			<affiliation>
				<orgName type="collaboration">2nd Workshop on Empirical Evaluation in Computer Vision</orgName>
			</affiliation>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Pca for gender estimation: Which eigenvectors contribute?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Balci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Atalay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition</title>
		<imprint>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Generating image filters for target recognition by genetic learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Thrift</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="906" to="910" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Genetic object recognition using combinations of views</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bebis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yfantis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Evolutionary Computation</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="132" to="146" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Genetic algorithms for object recognition in a complex scene</title>
		<author>
			<persName><forename type="first">D</forename><surname>Swets</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Punch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="595" to="598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Evolutionary pursuit and its application to face recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wechsler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="570" to="582" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Face detection and verification using genetic search</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bebis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Uthiram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Georgiopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Artificial Intelligence Tools</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="225" to="246" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Genetic algorithms: a survey</title>
		<author>
			<persName><forename type="first">M</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Patnaik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computers</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="17" to="26" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">The chc adaptive search algoruthm: How to have safe search when engaging in non-traditional genetic recombination</title>
		<author>
			<persName><forename type="first">L</forename><surname>Eshelman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Foundation of Genetic Algorithms Workshop</title>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">The Nature of Statistical Learning Theory</title>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Springer Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Tutorial on support vector machines for pattern recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Burges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="955" to="974" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Kernel methods: Current research and future directions</title>
		<author>
			<persName><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Burges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="5" to="9" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Support vector machine active learning with applications to text classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="45" to="66" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Speech recognition using svms</title>
		<author>
			<persName><forename type="first">N</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Becker</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<date type="published" when="2002">2002</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Support vector machine classification of microarray gene expression data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Grundy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Christianini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sugnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ares</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Haussler</surname></persName>
		</author>
		<idno>UCSC-CRL 99-09</idno>
		<imprint>
			<date type="published" when="1999">1999</date>
			<pubPlace>Santa Cruz,CA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University California</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Pebl: Positive-example based learning for web page classification using svm</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th Int. Conf. Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Vehicle detection and recognition in greyscale imagery</title>
		<author>
			<persName><forename type="first">N</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Charnley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Harris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Control Engineering Practice</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="473" to="479" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Artificial neural networks in real-time car detection and tracking applications</title>
		<author>
			<persName><forename type="first">C</forename><surname>Goerick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Detlev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Werner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="335" to="343" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Unsupervised learning of models for recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="18" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">A trainable system for object detection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Papageorgiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="15" to="33" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Improving the performance of on-road vehicle detection by combining gabor and wavelet features</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bebis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Fifth International Conference on Intelligent Transportation Systems</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-09">September, 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Example-base learning for view-based human face detection</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="51" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Messy genetic algorithms: Motivation, analysis, and first results</title>
		<author>
			<persName><forename type="first">D</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Korb</surname></persName>
		</author>
		<author>
			<persName><surname>Deb</surname></persName>
		</author>
		<idno>No. 89002</idno>
		<imprint>
			<pubPlace>Tuscaloosa</pubPlace>
		</imprint>
		<respStmt>
			<orgName>The Clearinghouse for Genetic Algorithms, University of Alabama</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report TCGA Report</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Search, polynomial complexity, and the fast messy genetic algorithm</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kargupta</surname></persName>
		</author>
		<imprint>
			<publisher>CS Dept</publisher>
		</imprint>
		<respStmt>
			<orgName>University of Illinois</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD Thesis</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Combining robot control strategies using genetic algorithms with memory</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sushil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gan</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science, Evolutionary Programming VI</title>
		<imprint>
			<biblScope unit="volume">1213</biblScope>
			<biblScope unit="page" from="431" to="442" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
