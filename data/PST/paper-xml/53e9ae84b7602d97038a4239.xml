<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dominant Orientation Templates for Real-Time Detection of Texture-Less Objects</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Stefan</forename><surname>Hinterstoisser</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">CAMP</orgName>
								<orgName type="institution" key="instit2">Technische Universität München (TUM)</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
							<email>vincent.lepetit@epfl.ch</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Computer Vision Laboratory</orgName>
								<orgName type="institution">École Polytechnique Fédérale de Lausanne (EPFL)</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
							<email>slobodan.ilic@in.tum.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">CAMP</orgName>
								<orgName type="institution" key="instit2">Technische Universität München (TUM)</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pascal</forename><surname>Fua</surname></persName>
							<email>pascal.fua@epfl.ch</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Computer Vision Laboratory</orgName>
								<orgName type="institution">École Polytechnique Fédérale de Lausanne (EPFL)</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nassir</forename><surname>Navab</surname></persName>
							<email>navab@in.tum.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">CAMP</orgName>
								<orgName type="institution" key="instit2">Technische Universität München (TUM)</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dominant Orientation Templates for Real-Time Detection of Texture-Less Objects</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FB2FDB5752CE05FD21222518C26DA568</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T05:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a method for real-time 3D object detection that does not require a time consuming training stage, and can handle untextured objects. At its core, is a novel template representation that is designed to be robust to small image transformations. This robustness based on dominant gradient orientations lets us test only a small subset of all possible pixel locations when parsing the image, and to represent a 3D object with a limited set of templates. We show that together with a binary representation that makes evaluation very fast and a branch-and-bound approach to efficiently scan the image, it can detect untextured objects in complex situations and provide their 3D pose in real-time.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Currently, the dominant approach to object recognition is to use statistical learning to build a classifier offline, and then to use it at run-time for the recognition <ref type="bibr" target="#b16">[17]</ref>. This works remarkably well but is not applicable for all scenarios, for example, a system that has to continuously learn new objects online. It is then difficult, or even impossible, to update the classifier without losing efficiency.</p><p>To overcome this problem, we propose an approach based on real-time template recognition. With such a tool at hand, it is then trivial and virtually instantaneous to learn new incoming objects by simply adding new templates to the database while simultaneously maintaining reliable realtime recognition.</p><p>However, we also wish to keep the advantages of statistical methods, as they learn how to reject unpromising image locations very quickly, which increases their real-time performance considerably. They can also be very robust, because they can generalize well from the training set. For these reasons, we also designed our template representation based on some fast to compute image statistics that provide invariance to small translations and deformations, which in turn allows us to quickly yet reliably search the image. As shown in Figure <ref type="figure" target="#fig_0">1</ref>, in this paper we propose a template representation that is invariant enough to make search in the images very fast and generalizes well. As a result, we can almost instantaneously learn new objects and recognize them in real-time without requiring much time for training or any feature point detection at runtime.</p><p>Our representation is related to the Histograms-of-Gradients (HoG) based representation <ref type="bibr" target="#b0">[1]</ref> that has proved to generalize well. Instead of local histograms, it relies on locally dominant orientations, and is made explicitly invariant to small translations. Our experiments show it is in practice at least as discriminant as HoG, while being much faster. Because it is explicitly made invariant to small translations, we can skip many locations while parsing the images without the risk of missing the targets. Moreover we developed a bit-coding method inspired by <ref type="bibr" target="#b15">[16]</ref> to evaluate an image location for the presence of a template. It mostly uses simple bit-wise operations, and is therefore very fast on modern CPUs. Our similarity measure also fulfills the requirements for recent branch-and-bound exploration techniques <ref type="bibr" target="#b9">[10]</ref>, speeding-up the search even more.</p><p>In the remainder of the paper we first discuss related work before we explain our template representation and how similarity can be evaluated very fast. We then show quantitative experiments and real world applications of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Template Matching is attractive for object detection because of its simplicity and its capability to handle different types of objects. It neither needs a large training set nor a time-consuming training stage, and can handle low-textured objects, which are, for example, difficult to detect with feature points-based methods.</p><p>An early approach to Template Matching <ref type="bibr" target="#b12">[13]</ref> and its extension <ref type="bibr" target="#b2">[3]</ref> include the use of the Chamfer distance between the template and the input image contours as a dissimilarity measure. This distance can efficiently be computed using the image Distance Transform (DT). It tends to generate many false positives, but <ref type="bibr" target="#b12">[13]</ref> shows that taking the orientations into account drastically reduces the number of false positives. <ref type="bibr" target="#b8">[9]</ref> is also based on the Distance Transform, however, it is invariant to scale changes and robust enough against perspective distortions to do real-time matching. Unfortunately, it is restricted to objects with closed contours, which are not always available.</p><p>But the main weakness of all Distance Transform-based methods is the need to extract contour points, using Canny method for example, and this stage is relatively fragile. It is sensitive to illumination changes, noise and blur. For instance, if the image contrast is lowered, contours on the object may not be detected and the detection will fail.</p><p>The method proposed in <ref type="bibr" target="#b14">[15]</ref> tries to overcome these limitations by considering the image gradients in contrast to the image contours. It relies on the dot product as a similarity measure between the template gradients and those in the image. Unfortunately, this measure rapidly declines with the distance to the object location, or when the object appearance is even slightly distorted. As a result, the similarity measure must be evaluated densely, and with many templates to handle appearance variations, making the method computationally costly. Using image pyramids provides some speed improvements, however, fine but important structures tend to be lost if one does not carefully sample the scale space.</p><p>Histogram of Gradients <ref type="bibr" target="#b0">[1]</ref> is another very popular method. It describes the local distributions of image gradients as computed on a regular grid. It has proven to give reliable results but tends to be slow due to the computational complexity.</p><p>Recently, <ref type="bibr" target="#b1">[2]</ref> proposed a learning-based method that recognizes objects via a Hough-style voting scheme with a non-rigid shape matcher on the contour image. It relies on statistical methods to learn the model from few images that are only constraint with a bounding box around the object. While giving very good classification results, the approach is neither appropriate for object tracking in real-time due to its expensive computation nor it is exact enough to return the correct pose of the object. Moreover, it holds all the disadvantages of Distance Transform based methods as mentioned previously.</p><p>Grabner and Bischof <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> developed another learning based approach that put more focus on online learning. In <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> it is shown how a classifier can be trained online in real-time, with a training set generated automatically. However, <ref type="bibr" target="#b3">[4]</ref> was demonstrated on textured objects, and <ref type="bibr" target="#b4">[5]</ref> cannot provide the object pose.</p><p>The method proposed in this paper has the strength of the similarity measure of <ref type="bibr" target="#b14">[15]</ref>, the robustness of <ref type="bibr" target="#b0">[1]</ref> and the online learning capability of <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>. In addition, by binarizing the template representation and using a recent branchand-bound method of <ref type="bibr" target="#b9">[10]</ref> our method becomes very fast, making possible the detection of untextured 3D objects in real-time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Approach</head><p>In this section, we describe our Dominant Orientation Templates, and how they can be built and used to parse images to quickly find objects. We will start by deriving our similarity measure, emphasizing the contributions of each aspect of it. We then show how to use a binary representation to compute the similarity using efficient bit-wise operations. We finally demonstrate how to use it within a branchand-bound exploration of the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Initial Similarity Measure</head><p>Our starting idea is to measure the similarity between an input image I, and a reference image O of an object centered on a location c in the image I by comparing the orientations of their gradients.</p><p>We chose to consider image gradients because they proved to be more discriminant than other forms of representations <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15]</ref> and are robust to illumination change and noise. For even more robustness to such changes, we use their magnitudes only to retain the orientations of the strongest gradients, without using their actual values for matching. Also, to correctly handle object occluding boundaries, we consider only the orientations of the gradients, by contrast with their directions (two vectors with a 180deg angle between them have the same orientation).. In this way, the measure will not be affected if the object is over a dark background, or a bright background. Moreover, as in SIFT or HoG <ref type="bibr" target="#b0">[1]</ref>, we discretize the orientations to a small number n o of integer values.</p><p>Our initial energy function E 1 counts how many orienta-tions are similar between the image and the template centered on location c, and can be formalized as:</p><formula xml:id="formula_0">E 1 (I, O, c) = r δ ori(I, c + r) = ori(O, r) ,<label>(1)</label></formula><p>where</p><p>• δ(P ) is a binary function that returns 1 if P is true, 0 otherwise;</p><p>• ori(O, r) is the discretized gradient orientation in the reference image O at location r which parses the template. Similarly, ori(I, c+r) is the discretized gradient orientation at c shifted by r in the input image I.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Robustness to Small Deformations</head><p>To make our measure tolerant to small deformations, and also to make it faster to compute, we will not consider all possible locations, and will decompose the two images into small squared regions R over a regular grid. For each region, we will consider only the dominant orientations. Such an approach is similar to the HMAX pooling mechanism <ref type="bibr" target="#b13">[14]</ref>. Our similarity measure can now be modified as:</p><formula xml:id="formula_1">E 2 (I, O, c) = R in O δ do(I, c + R) ∈ DO(O, R) , (2)</formula><p>where DO(O, R) returns the set of orientations of the strongest gradients in region R of the object reference image. In contrast, do(I, c + R) returns only one orientation, the orientation of the strongest gradient in the region R shifted by c in the input image.</p><p>The reason why we chose each region in O to be represented by the strongest gradients is that the strongest gradients are easy and fast to identify and very robust to noise and illumination change. Moreover, to describe uniform regions, we introduce the symbol ⊥ to indicate that no reliable gradient information is available for the region. The DO(.) function therefore returns either a set of discretized gradient orientations of the k strongest gradients in the range of [0, n o -1] or {⊥}, and can be formally written as:</p><formula xml:id="formula_2">DO(O, R) = S(O, R) if S(O, R) = ∅, {⊥} otherwise<label>(3)</label></formula><p>with</p><formula xml:id="formula_3">S(O, R) = {ori (O, l) : l ∈ maxmag k (R) ∧ mag(O, l) &gt; τ } (4) where • l is a pixel location in R, • ori(O, l) is the gradient orientation at l in image O,</formula><p>and mag(O, l) its magnitude, • maxmag k (R) is the set of locations for the k strongest gradients in R. In practice we take k = 7 but the choice of k does not seem critical.</p><p>• τ is a threshold on the gradient magnitudes to decide if the region is uniform or not.</p><p>The function do(I, c+R) is computed similarly in the input image I. However, to be faster at runtime, in do(I, c + R), k is restricted to 1, and therefore do(I, c + R) returns only one single element.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Invariance to Small Translation</head><p>We will now explicitly make our similarity measure invariant to small motions. In this way, we will be able to consider only a limited number of locations c when parsing an image and save a significant amount of time without increasing the chance of missing the target object. To do so, we consider a measure that returns the maximal value of E 2 when the object is slightly moved, which can be written as:</p><formula xml:id="formula_4">E 3 (I, O, c) = max M ∈M E 2 (I, w(O, M ), c) = max M ∈M R in O δ do(I, c + R) ∈ DO(w(O, M ), R) ,<label>(5)</label></formula><p>where w(O, M ) is the image O of the object warped using a transformation M . In practice, we consider for M only 2D translations as it appears sufficient to handle other small deformations, and M is the set of all (small) translations in the range</p><formula xml:id="formula_5">[-t; +t] 2 .</formula><p>There is of course a limit for the range t. A large t will result in high speed-up but also in a loss of discriminative power of the function. In practice, we found that t = 7 for 640 × 480 images is a good trade-off.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Ignoring the Dependence between Regions</head><p>Our last step is to ignore the dependence between the different regions R. This will simplify and significantly speed-up the computation of the similarity. We therefore approximate E 3 as given in Eq.( <ref type="formula" target="#formula_4">5</ref>) by:</p><formula xml:id="formula_6">E 4 (I, O, c) = R in O max M ∈M δ do(I, c + R) ∈ DO(w(O, M ), R) . (6)</formula><p>The speed-up comes from the fact that, for each region R, we can precompute a list L(O, R) of the dominant orientations in R when O is translated over M. As illustrated by Figure <ref type="figure" target="#fig_1">2</ref>, the measure E 4 can thus be written as: <ref type="bibr" target="#b6">(7)</ref> and L(O, R) can formally be written as:</p><formula xml:id="formula_7">E 4 (I, O, c) = R in O δ do(I, c + R) ∈ L(O, R) ,</formula><formula xml:id="formula_8">L(O, R) = {o : ∃M ∈ M such that o ∈ DO(w(O, M ), R)} .<label>(8)</label></formula><p>The collection of lists over all regions R in O forms the final object template.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Using Bitwise Operations</head><p>Inspired by <ref type="bibr" target="#b15">[16]</ref>, and as shown in Figure <ref type="figure">3</ref>, we efficiently compute the energy function E 4 using a binary representation of the lists L(O, R) and of the dominant orientations do(I, c + R). This allows us to compute E 4 with only a few bitwise operations.</p><p>By setting n o , the number of discretized orientations, to 7 we can represent a list L(O, R) or a dominant orientation do(I, c + R) with one byte i.e. a 8-bit integer. Each of the 7 first bits corresponds to an orientation while the last bit stands for ⊥.</p><p>More exactly, to each list L(O, R) corresponds a byte L whose i th bit with 0 ≤ i ≤ 6 is set to 1 iff i ∈ L(O, R), and whose 7 th bit is set to 1 iff ⊥ ∈ L(O, R). A byte D can be constructed similarly to represent a dominant orientation do(I, c + R). Note that only one bit of D is set to 1. Now the term δ do(I, c + R) ∈ L(O, R) in Eq.( <ref type="formula">7</ref>) can be evaluated very quickly. We have:</p><formula xml:id="formula_9">δ do(I, c + R) ∈ L(O, R) = 1 iff L ⊗ D = 0 , (<label>9</label></formula><formula xml:id="formula_10">)</formula><p>where ⊗ is the bitwise AND operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Using SSE Instructions</head><p>The computation of E 4 as formulated in Section 3.5 can be further speeded up using SSE operations. In addition to Figure <ref type="figure">3</ref>. Computing the similarity E4 using bitwise operations and a lookup table that counts how many terms δ() as in Eq.( <ref type="formula" target="#formula_9">9</ref>) are equal to 1. bitwise operations, which are already very fast, SSE technology allows to perform the same operation on 16 bytes in parallel. Thus, by using the function given in Listing 1, the similarity score for 16 regions can be computed with only 3 SSE operations and one lookup-table <ref type="table">with</ref>  This method is extremely cache friendly because only successive chunks of 128 bits are processed at a time which holds the number of cache misses low. This is very important because SSE technology is very sensitive to optimal cache alignment. This is probably why, although our energy function is slightly more computationally expensive in theory than <ref type="bibr" target="#b15">[16]</ref>, we found that our formulation performed 1.5 times faster in practice.</p><p>Another advantage of our algorithm, however, is that it is very flexible with respect to varying template sizes without loosing the capability of using the computational capacities very efficiently. In our method, the optimal processor load is reached by multiples of 16 in contrast to <ref type="bibr" target="#b15">[16]</ref> that needs multiples of 128 in a possible dynamic SSE implementation. The probability of wasting computational power is therefore much lower using our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.">Clustering for Efficient Branch and Bound</head><p>We can further improve the scalability of our method by exploiting the similarity between different templates representing different objects under different views. The general idea is to build clusters of similar templates-each of them being represented by what we will refer to as a cluster template. A cluster template is computed as a bitwise OR operation applied to all the templates belonging to the same cluster. It provides tight upper bounds and can be used in a branch and bound constrained search as in <ref type="bibr" target="#b9">[10]</ref>. By first computing the similarity measure E 4 between the image and the cluster templates at run-time, we can reject all the templates that belong to a cluster template not similar enough to the current image.</p><p>We use a bottom-up clustering method: To build a cluster, we start from a template picked randomly among the templates that do not yet belong to a cluster and iteratively search for the templates T that fulfill:</p><formula xml:id="formula_11">argmin T / ∈Clusteri max(d h (C or T, T ), d h (C or T, C)), (<label>10</label></formula><formula xml:id="formula_12">)</formula><p>where d h is the hamming distance, "or" the bitwise OR operation and C the cluster template before OR'ing. We proceed this way until the cluster has a given number of templates assigned or no templates are left. In the first case, we continue building clusters until every template is assigned to a cluster. For our approach, this clustering scheme allows faster runtime than the binary tree clustering suggested in <ref type="bibr" target="#b15">[16]</ref>, as will be shown in Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>In the experiments, we compared our approach called DOT (for Dominant Orientation Templates) to Affine Region Detectors <ref type="bibr" target="#b11">[12]</ref> (Harris-Affine, Hessian-Affine, MSER, IBR, EBR), to patch rectification methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b5">6]</ref> (Leopar, Panter, Gepard) and to the Histograms-of-Gradients (HoG) template matching approach <ref type="bibr" target="#b0">[1]</ref>.</p><p>For HoG, we used our own SSE optimized implementation. In order to detect the correct template from a large template database we replaced the Support Vector Machine mentioned in the original work of HoG by a nearest neighbor search since we want to avoid a training phase and to look for a robust representation instead.</p><p>We did the performance evaluation on the Oxford Graffiti and on the Oxford Wall image set <ref type="bibr" target="#b11">[12]</ref>. Since no video sequence is available, we synthesized a training set by scaling and rotating the first image of the dataset for changes in viewpoint angle up to 75 degrees and by adding random noise and affine illumination change.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Robustness</head><p>The matching scores of the different methods is shown in Figure <ref type="figure">4</ref>(a) for the Graffiti dataset, and in Figure <ref type="figure">4(b)</ref> for the Wall dataset. As defined in <ref type="bibr" target="#b11">[12]</ref>, this score is the ratio between the number of correct matches and the smaller number of regions detected in one of the two images.</p><p>For the affine regions, we first extract the regions using different region detectors and match them using SIFT. Two of them are said to be correctly matched if the overlap error of the normalized regions is smaller than 40%. In our case, the regions are defined as the patches warped by the retrieved transformation. For a fair comparison, we used the same numbers and appearances of templates for the DOT and HoG approaches. We also turned off the final check on the correlation for all patch rectification approaches (Leopar, Panter, Gepard) since there is no equivalent for the affine regions.</p><p>DOT and HoG clearly outperform the other approaches by delivering optimal matching results of 100% on the Graffiti image set. For the Wall image set, DOT performs optimal again with a matching rate of 100% while HoG performs worse for larger viewpoint changes.</p><p>These very good performances can be explained by the fact that DOT and HoG scan the whole image while the affine regions approach is dependent on the quality of the region extraction. As it will be shown in Section 4.3, even if it parses the whole image, our approach is fast enough to compete with affine region and patch rectification approaches in terms of computation times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Detection Accuracy</head><p>As it was done in <ref type="bibr" target="#b6">[7]</ref>, in Figure <ref type="figure">4</ref>(c), we compare the average overlap between the ground truth quadrangles and their corresponding warped versions obtained with DOT, HoG, the patch rectification methods and with the affine regions detectors. We did the experiments for overlap and accuracy on both image sets but due to the similarity of the results and the lack of space we only show the results on the Graffiti image set. Since the Affine Region Detectors deliver elliptic regions we fit quadrangles around these ellipses by aligning them to the main gradient orientation as it was done in <ref type="bibr" target="#b6">[7]</ref>.</p><p>The average overlap is very close to 100% for DOT and HoG, about 10% better than MSER and about 20% better than the other affine region detectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Speed</head><p>Although performing similar in terms of robustness and accuracy, DOT clearly outperforms HoG in terms of speed by several magnitudes. In order to compare both approaches, we trained them on the same locations and appearances on a 640 × 480 image with |R| = 121. The experiment was done on a standard notebook with an Intel Centrino Processor Core2Duo with 2.4GHz and 3GB RAM where unoptimized training of one template took 1.8ms and the clustering of about 1600 templates 0.76s. As one can see in Figure <ref type="figure">5</ref>(a), when using about 1600 templates our approach is about 310 times faster at runtime than our SSE optimized HoG implementation. The reason for this is both the robustness to small deformations that allows DOT to skip most of the pixel locations and the binary representation of our templates that enables a fast similarity evaluation.</p><p>We also compared our similarity measure to a SSE optimized version of Taylor's version <ref type="bibr" target="#b15">[16]</ref>. Our approach is constantly about 1.5 times faster than Taylor's. We believe it is due to the cache friendly formulation of E 4 where we successively use sequential chunks of 128 bits at a time while <ref type="bibr" target="#b15">[16]</ref> has to jump back and forth within 1024 bits (in case |R| = 121) for successively OR'ing pairs of 128 bit vectors and accumulating the result (for a closer explanation of Taylor's similarity measure please refer to <ref type="bibr" target="#b15">[16]</ref>) in a SSE register.</p><p>We also did experiments with respect to the different clustering schemes. We compared the approach where no clustering is used to the binary tree of <ref type="bibr" target="#b15">[16]</ref> and our clustering described in Section 3.7. Surprisingly, our clustering is twice as fast as the binary tree clustering at runtime. Although the matching should behave in O(log(N )) time, our implementation of the binary tree clustering behaves linearly up to about 1600 templates as it was also observed by <ref type="bibr" target="#b15">[16]</ref>. As the authors of <ref type="bibr" target="#b15">[16]</ref> claim, the reason for this might be that there are not enough overlapping templates to fully exploit the potential of their tree structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Occlusion</head><p>Occlusion is a very important aspect in template matching. To test our approach towards occlusion we selected 100 templates on the first image of the Oxford Graffiti image set, added small image deformation, noise and illumination changes and incrementally occluded the template in 2.5% steps from 0% to 100%. The results are displayed in Figure <ref type="figure">5(b</ref>). As expected the similarity of our method behaves linearly to the percentage of occlusion. This is a desirable property since it allows to detect partly occluded templates by setting the detection threshold with respect to the tolerated percentage of occlusion. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Region Size</head><p>The size of the region R is another important parameter. The larger the region R gets the faster the approach becomes at runtime. However, at the same time as the size of the region increases the discriminative power of the approach decreases since the number of gradients to be considered rises. Therefore, it is necessary to choose the size of the region R carefully to find a compromise between speed and robustness. In the following experiment on the Graffiti image set we tested the behavior of DOT with respect to the matching score and the size of the region R. The result is shown in Figure <ref type="figure">5(c</ref>). As the matching score is still 100% for regions of 7 × 7 pixels, one can see that the robustness decreases with increasing region size. Although dependent on the texture and on the density of strong gradients within one region R, we empirically found on many different objects that a region size of 7 × 7 gives very good results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Failure Cases</head><p>Figure <ref type="figure" target="#fig_4">6</ref> shows the limitation of our method: To obtain such optimal results as in Figure <ref type="figure">4</ref>, the templates have to exhibit strong gradients. In case of too smooth or blurry template images, HoG tends to perform better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Applications</head><p>Due to the robustness and the real-time capability of our approach, DOT is suited for many different applications including untextured object detection as shown in Figure <ref type="figure" target="#fig_6">8</ref>, and planar patches detection as shown in Figure <ref type="figure" target="#fig_7">9</ref>. Although neither a final refinement nor any final verification, by contrast with <ref type="bibr" target="#b6">[7]</ref> for example, was applied to the found 3D objects, the results are very accurate, robust and stable. Creating the templates for new objects is easy and illustrated by Figure <ref type="figure" target="#fig_5">7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We introduce a new binary template representation based on locally dominant gradient orientations that is invariant to small image deformations. It can very reliably detect untextured 3D objects using relatively few templates from many different viewpoints in real-time. We have shown that our approach performs superior to state-of-theart methods with respect to the combination of recognition rate and speed. Moreover, the template creation is fast and easy, does not require a training set, only a few exemplars, and can be done interactively.   DOT is however much more reliable as it does not rely on feature point detection, but parses the image instead. The corresponding video is available on http://campar.in.tum.de/Main/StefanHinterstoisser.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Overview. Our templates can detect non-textured objects over cluttered background in real-time without relying on feature point detection. Adding new objects is fast and easy, as it can be done online without the need for an initial training set. Only a few templates are required to cover all appearances of the objects.</figDesc><graphic coords="1,305.99,373.03,118.12,79.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Similarity measure E4. Our final energy measure E4 counts how many times a local dominant orientation for a region R in the image belongs to the corresponding precomputed list of orientations L(O, R) for the corresponding template region. Each list is made of the local dominant orientations that are in the region R when the object template is slightly translated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>int energy_function4( __m128i lhs, __m128i rhs ) { __m128i a = _mm_and_si128(lhs,rhs); __m128i b = _mm_cmpeq_epi8(a); return lookuptable[_mm_movemask_epi8(b)]; } Listing 1. C++ Energy function for 16 regions with 3 SSE instructions and one look-up in a 16-bit-table.Since in SSE there is no comparison on non-equality for unsigned 8-bit integers we have-in contrast to Figure3-to compare the AND'ed result to zero and count the "0" instead.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .Figure 5 .</head><label>45</label><figDesc>Figure 4. Methods comparisons on the Graffiti and Wall Oxford datasets. (a-b):Matching scores for Graffiti and Wall sets when increasing the viewpoint angle. Our method is referred as "DOT", and reaches a 100% score on both sets for every angle. These results are discussed in Section 4.1. (c) shows the overlaps between the retrieved and expected regions as an accuracy measure for Graffiti. These results are discussed in Section 4.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Failure Case. When the object does not exhibit strong gradients, like the blurry image on the left, our method performs worse than HoG.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Templates creation. To easily define the templates for a new object, we use DOT to detect a known object-the ICCV logo in this case-next to the object to learn in order to estimate the camera pose and to define an area in which the object to learn is located. A template for the new object is created from the first image, and we start detecting the object while moving the camera. When the detection score becomes too low, a new template is created in order to cover the different object appearances when the viewpoint changes.</figDesc><graphic coords="8,307.68,130.13,113.84,71.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Detection of different objects at about 12 fps over a cluttered background. The detections are shown by superimposing the thresholded gradient magnitudes from the object image over the input images. The corresponding video is available on http://campar.in.tum.de/Main/StefanHinterstoisser.</figDesc><graphic coords="8,56.09,503.88,113.85,71.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Patch 3D orientation estimation. Like Gepard<ref type="bibr" target="#b7">[8]</ref>, DOT can detect planar patches and provide an estimate of their orientations. DOT is however much more reliable as it does not rely on feature point detection, but parses the image instead. The corresponding video is available on http://campar.in.tum.de/Main/StefanHinterstoisser.</figDesc><graphic coords="8,56.09,640.49,113.85,71.78" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment: This project was funded by the BMBF project AVILUSplus (01IM08002).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Histograms of Oriented Gradients for Human Detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">From images to shape models for object detection</title>
		<author>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Real-time object detection for &quot;smart&quot; vehicles</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gavrila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Philomin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Tracking via Discriminative Online Learning of Local Features</title>
		<author>
			<persName><forename type="first">M</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semi-supervised on-line boosting for robust tracking</title>
		<author>
			<persName><forename type="first">M</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Simultaneous recognition and homography extraction of local patches with a simple linear classifier</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Benhimane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Online learning of patch perspective rectification for efficient object detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Benhimane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Real-time learning of accurate patch rectification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Distance transform templates for object detection and pose estimation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Holzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Beyond Sliding Windows: Object Localization by Efficient Subwindow Search</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008-06">June 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Distinctive Image Features from Scale-Invariant Keypoints</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A comparison of affine region detectors</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schaffalitzky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kadir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automatic target recognition by matching oriented edge pixels</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Olson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IP</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Realistic modeling of simple and complex cell tuning in the hmax model, and implications for invariant object recognition in cortex</title>
		<author>
			<persName><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riesenhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TR</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>MIT</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Occlusion Clutter, and Illumination Invariant Object Recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IAPRS</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multiple target localisation at over 100 fps</title>
		<author>
			<persName><forename type="first">S</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Drummond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Robust real-time object detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
