<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Two Level Bulk Preload Branch Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">James</forename><surname>Bonanno</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IBM Systems and Technology Group bonannoj</orgName>
								<address>
									<settlement>collura, dlipetz, brprasky</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Adam</forename><surname>Collura</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IBM Systems and Technology Group bonannoj</orgName>
								<address>
									<settlement>collura, dlipetz, brprasky</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Lipetz</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IBM Systems and Technology Group bonannoj</orgName>
								<address>
									<settlement>collura, dlipetz, brprasky</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ulrich</forename><surname>Mayer</surname></persName>
							<email>ulrich.mayer@de.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">IBM Systems and Technology Group bonannoj</orgName>
								<address>
									<settlement>collura, dlipetz, brprasky</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Brian</forename><surname>Prasky</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IBM Systems and Technology Group bonannoj</orgName>
								<address>
									<settlement>collura, dlipetz, brprasky</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anthony</forename><surname>Saporito</surname></persName>
							<email>saporit@us.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">IBM Systems and Technology Group bonannoj</orgName>
								<address>
									<settlement>collura, dlipetz, brprasky</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Two Level Bulk Preload Branch Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the large capacity hierarchical branch predictor in the 5.5 GHz IBM zEnterprise EC12 microprocessor. Performance analyses in a simulation model and on zEC12 hardware demonstrate the benefit of this hierarchy compared to a smaller one level predictor.</p><p>Novel structures and algorithms for two level branch prediction are presented. Prediction information about multiple branches is bulk transferred from the second level into the first upon detecting a perceived miss in the first level. The second level does not directly make branch predictions.</p><p>Access to the second level is limited when it is unlikely to be productive.</p><p>The second level is systematically searched in an order that is likely to provide hits as early as possible.</p><p>On the workloads analyzed in the simulation model, measurements show a maximum core performance benefit of 13.8%. On the two workloads analyzed on zEC12 hardware 3.4% and 5.3% system performance improvements are achieved.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Dynamic branch prediction reduces the performance degrading effect that conditional and redirecting instructions have on program flow. A highly accurate branch direction and target predictor is essential for good performance. However, performance of very large workloads is often limited by the capacity of the predictors more than the accuracy of the dynamic prediction algorithms. This paper presents a solution to the capacity problem.</p><p>The design described in this paper includes historybased predictors providing both the direction and target address of branches asynchronously, and most often ahead of instruction fetching and delivery. In steering both instruction fetch and delivery, the predictors attempt to minimize penalties from instruction cache misses, incorrect branch paths, and target redirects.</p><p>All prediction structures have a finite size, and while from a capacity and precision perspective bigger is better, real world practice shows that access latencies, silicon area, and power consumption limit designers from using overly large structures <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b14">14]</ref>.</p><p>This paper describes the two level hierarchical branch predictor implemented in the IBM zEnterprise EC12 <ref type="bibr" target="#b13">[13]</ref>. This branch prediction hierarchy achieves the performance benefit of a very large capacity predictor with minimal impact on latency and power. The design is a semi-exclusive two level structure where the large second level predictor is used to back fill the smaller and lower latency first level predictor. Predictions affecting program flow are only made from the fast first level structure which is located close to the instruction fetch and delivery pipeline. There is more flexibility in the placement of the second level structure since it is decoupled from the first level. The second level predictor is only powered up and accessed when content is perceived as missing from the main first level predictor.</p><p>The second level predictor has its own steered search and hit pipeline capable of providing history on a large number of tagged branches that correlate to the current instruction address space. When employed, the second level predictor is capable of searching either a small or large amount of address space. The small search space option is used for sporadic capacity based first level content gaps, and the large option for more wide spread gaps normally associated with cold code. An additional small BTB is used to prevent bulk second level transfers from polluting the main first level predictor. This additional structure also serves as a first level predictor victim buffer.</p><p>The performance benefits of this design, as compared to a smaller one level predictor, are explored using information gathered from both trace driven simulated models and from an actual IBM zEnterprise EC12 machine.</p><p>978-1-4673-5587-2/13/$31.00 ?2013 IEEE</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>Branch target buffers (BTBs) have been around for decades <ref type="bibr" target="#b5">[5]</ref>. Sussenguth describes a mechanism for looking up addresses and redirecting the flow of instructions to prevent sequencing issues <ref type="bibr" target="#b7">[7]</ref>. Studies have shown that a branch prediction scheme with a BTB has considerable advantages. Modern processors realize a high performance benefit from branch prediction, especially as lower cycle times have necessitated increased pipeline depths <ref type="bibr" target="#b3">[3]</ref>.</p><p>Significant advantages may be obtained when the BTB is as large as possible, with all other factors constant <ref type="bibr" target="#b4">[4]</ref>. Theoretically, to maximize performance, BTBs would be large enough to retain all the branches in a working application set, yet maintain low access latency and remain on-chip <ref type="bibr" target="#b2">[2]</ref>, near the instruction fetch and branch prediction logical structures, typically located in the timing-critical part of the processor pipeline <ref type="bibr" target="#b6">[6]</ref>. In addition to being sized to contain a large number of branches, the more complete the branch information stored per BTB entry, the better the performance, provided the content reduces aliasing <ref type="bibr" target="#b16">[16]</ref>.</p><p>A large BTB tends to be more accurate, but the increased size adds to the latency of accessing the BTB.</p><p>Slower BTB access increases instruction sequence redirection penalties <ref type="bibr" target="#b10">[10]</ref>. It is becoming increasingly difficult to complete a BTB search in one or two cycles because of decreasing cycle time and increasing storage structure size <ref type="bibr">[9]</ref>. Jimenez et. al. in <ref type="bibr" target="#b14">[14]</ref> showed that latency constraints must be considered when designing branch prediction logic. As microchip manufacturing technologies shrink, the delay to access content from the branch prediction logic potentially deteriorates, since transistors and wires may not equally scale. Attempts have been made to mask this latency. One way is to pipeline the branch predictor. Jimenez <ref type="bibr" target="#b19">[19]</ref> and Seznec et al. <ref type="bibr" target="#b25">[26]</ref> describe examples of pipelined direction predictors.</p><p>One encounters a tradeoff between a very large cache to store a large amount of branch information, or metadata, versus a small storage element that has fast access time <ref type="bibr">[9]</ref>. However, a small BTB that has a fast access time is subject to aliasing with older entries being overwritten by newer ones <ref type="bibr" target="#b10">[10]</ref>. Since the BTB typically stores only a portion of the branch's virtual address as a tag, aliasing can occur among branches within a thread and among branches in different threads <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b16">16]</ref>.</p><p>A balance must be made among the number of entries, content per entry, access latency and accuracy of the prediction on various workloads. Several variations of multiple level branch prediction mechanisms have been proposed to help alleviate the inherent shortfalls that a traditional branch prediction implementation possesses <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr">9,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b12">12]</ref>. The solutions are analogous to a memory hierarchy, where a small amount of data is accessed quickly (e.g., a L1 cache) and large amount of data is accessed slowly (L2, L3, main memory etc.). It was suggested by <ref type="bibr" target="#b16">[16]</ref> that multilevel BTBs may be able to strike a balance between the large number of entries needed in the BTB and the desire to have a large amount of metadata per entry.</p><p>Sometimes the levels are accessed in parallel and the slower, larger level can override a prediction from the smaller, faster level <ref type="bibr">[9]</ref>. Alternatively, after not finding a branch in the first level, it is looked for in the second level which can then provide a prediction <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b11">11]</ref>. Other approaches use a larger, slower second level to preload entries into the smaller, faster level <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b12">12]</ref>.</p><p>Virtualization of prediction storage structures has recently emerged as another approach to address the latency, size and cost issues of traditional predictors. Predictor virtualization was suggested in <ref type="bibr" target="#b17">[17]</ref> as a means to reduce the resource a conventional prediction system would consume while still maintaining a high degree of accuracy. Burcea et al. suggest virtualizing large prediction structures by exploiting the existing memory hierarchy in place of dedicated structures. A small physical storage structure would cache the working set of prediction data close to the timing critical hardware, while the remaining inactive metadata resides in memory.</p><p>Recognizing the potential of virtualization, Emma et al. in a patent application filed in 2003 <ref type="bibr" target="#b1">[1]</ref> describe a system to preload branch prediction data from a larger, higher latency virtual cache table into a smaller, lower latency non-virtual dedicated branch prediction structure. The perceived capacity of the branch prediction structure is increased without adding additional branch prediction structural capacity.</p><p>Burcea et al. propose a virtual BTB, called a phantom BTB <ref type="bibr" target="#b2">[2]</ref>. They also provide a solution that accommodates large workload footprints without dedicating large chip resources for a conventional BTB, expanding upon the work invented in <ref type="bibr" target="#b1">[1]</ref>. This work relies on temporal correlation within the first level BTB miss stream to guide the phantom BTB in prefetching branch information from the L2 cache.</p><p>Aasaraai et al. applied the concept of prediction virtualization to branch direction predictors <ref type="bibr" target="#b18">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Two level bulk preload branch prediction</head><p>This section describes bulk preload branch prediction developed for the IBM zEnterprise EC12. It introduces new structures and algorithms for implementing a two level hierarchical BTB. The zEC12 is a machine with big-endian 64-bit addressing. Therefore bit 0 is the most significant and bit 63 is the least significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Branch prediction hierarchy</head><p>The branch prediction hierarchy consists of several structures. These are depicted in Figure <ref type="figure">1</ref> along with arrows representing the flow of information between them. They are implemented as either SRAM arrays or register files.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1. Branch prediction hierarchy</head><p>The first level Branch Target Buffer (BTB1) is a tagged cache of branch prediction information. It is indexed and tagged with branch instruction addresses. In addition to tag information, each BTB1 entry contains a 2-bit bimodal Branch History Table (BHT) direction prediction and a target address used for predicted taken branches. The BTB1 contains 4k branches, is organized as a 1k x 4-way set associative cache, and is implemented as an SRAM array. Instruction address bits 49:58 are used to index into the array. Therefore, each row in the BTB1 covers 32 bytes of instruction space.</p><p>The Branch Target Buffer Preload Table (BTBP) is also a Branch Target Buffer (BTB). Each BTBP entry contains the same type of content as the BTB1. It is read in parallel with the BTB1 to make branch predictions. Together, the BTB1, the BTBP and some auxiliary structures mentioned below are considered the first level branch predictor. Any branch not predicted by the first level predictor is called a surprise branch and its direction (taken or not-taken) is guessed based on a tagless 32k entry one-bit BHT, its opcode and other instruction text fields. The BTBP contains 768 branches and is organized as a 128 x 6-way cache. Instruction address bits 52:58 are used to index into the array. Therefore, like the BTB1, each row in the BTBP covers 32 bytes of instruction space. The BTBP is implemented as a register file with multiple write ports to support the many sources of writes into the branch prediction hierarchy: surprise installs from statically guessed branches, branch preload instructions, BTB2 hits, and BTB1 victims. The BTBP serves as a filter for the BTB1. Branch prediction information is initially written into the BTBP from various sources. Content is moved into the BTB1 upon making a branch prediction from the BTBP. At that time the replaced BTB1 entry (the BTB1 victim) is moved into the BTBP and the second level Branch Target Buffer (BTB2).</p><p>The BTB2 is also a BTB containing the same type of information as the BTB1 and BTBP. The BTB2 is written upon surprise installs into the branch prediction hierarchy and with entries evicted from the BTB1 upon transferring BTBP predictions into the BTB1. The BTB2 contains 24k branches and is organized as a 4k x 6-way cache, and is implemented as an SRAM array. Instruction address bits 47:58 are used to index the BTB2. Therefore, like the BTB1 and BTBP, each row in the BTB2 covers 32 bytes of instruction space.</p><p>An estimate of the instruction footprint covered by each of these structures can be made. This is based on average instruction length, average number of branch instructions, and average number of ever-taken branches which get installed into the BTBs. It is estimated that each BTB entry covers 24 -30 bytes of instruction address space. Therefore, the first level predictor consisting of the BTB1 and BTBP is estimated to cover a footprint of 114 KB -142.5 KB.</p><p>Auxiliary structures called the Pattern History Table (PHT) and Changing Target Buffer (CTB) are used as part of the first level branch predictor for branches exhibiting multiple directions and targets. They are indexed based on the path taken to get to a branch and are tagged with branch instruction address bits. Information is also maintained in the BTB1 and BTBP to control whether or not the PHT and/or CTB are allowed to be used for a particular branch. They are the same size and similar configuration as in the IBM zEnterprise 196 <ref type="bibr" target="#b15">[15]</ref>. The PHT contains 4,096 entries and is indexed based on the direction of the 12 previous predicted branches and the instruction addresses of the 6 previous taken branches. The CTB contains 2,048 entries and is indexed based on the instruction addresses of the 12 previous taken branches. These predictors are similar to the tagged ppm-like predictors described by Michaud <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Branch prediction search process</head><p>The first level branch predictor is searched asynchronously from instruction fetching and decode. This type of design is called an asynchronous lookahead branch predictor. Upon a restart condition, such as a mispredicted branch, both instruction fetching and branch prediction start at the same instruction address. The branch prediction logic searches for the first branch at or after this starting search address in parallel to instruction fetch logic fetching instructions sequentially. Upon finding a branch in the first level predictor, it predicts whether the branch is taken or not-taken. If it is predicted taken, it also predicts a target address. The branch predictor then redirects itself either to the target of the predicted taken branch or sequentially after the predicted not-taken branch and looks for the next branch. The sequential redirect allows for greater PHT accuracy. Information about the predicted branch is sent to the instruction fetch and decode logic to redirect fetching to the predicted target stream in the case of a dynamically predicted taken branch, and to apply the direction prediction to the predicted instruction at the time of instruction decode. Some of the branch prediction information is also stored until completion time of the branch and used at that time to update the branch prediction structures. Until table updates take place, speculative BHT and PHT updates are applied to predictions.</p><p>Usually branch prediction operates ahead of instruction fetch and decode. It is therefore able to effectively steer instruction fetching and minimize or completely eliminate the target redirect penalty for predicted taken branches. Because the estimated instruction footprint of the first level branch predictor (114 KB -142.5 KB) is greater than the size of the first level instruction cache (64 KB), branch predictions occur for branches and their targets not in the first level instruction cache. Such predictions initiate instruction fetches that preload the necessary instructions into the first level instruction cache, often before the decode logic reaches that point. Therefore, the asynchronous lookahead branch predictor reduces or completely hides the first level instruction cache miss penalty. </p><p>The branch prediction pipeline consists of the 7 cycles described in Table <ref type="table" target="#tab_0">1</ref>. Branch prediction throughput and search rate are variable. The branch prediction pipeline is able to predict branches as fast as one prediction every cycle. This fastest case is a loop consisting of a single taken branch. When this case does not apply, branch predictions are possible every other cycle with the assistance of a 64 branch Fast Index Table (FIT) which accelerates branch prediction re-indexing on a 64 branch subset of the BTB1. When the FIT does not apply, it is possible to predict one taken branch every 3 cycles when those taken branches are in the most recently used (MRU) BTB1 column. Otherwise it is possible to predict one taken branch every 4 cycles. Not-taken predictions are possible at the fastest rate of 2 predictions every 5 cycles since each row searched in the BTB1 is allowed to make up to 2 not-taken predictions simultaneously. When this case does not apply, one not-taken prediction is possible every 4 cycles. When the search pipeline is not re-indexing itself for a predicted branch, it proceeds sequentially. If no branch predictions are found, the average search rate is 16 bytes per cycle. This is actually 3 cycles at 32 bytes per cycle followed by 3 cycles of 0 bytes per cycle, because on the 4th 5th and 6th cycles the logic is speculatively re-indexing the level one structures assuming the 1st, 2nd, and 3rd searches will find a predicted taken branch in the most recently used columns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Maximizing exclusivity between BTB1 and BTB2</head><p>The BTB1 and BTB2 are maintained in such a way to approximate an exclusive hierarchy. Ideally, from a capacity standpoint, the BTB1 and BTB2 would be truly exclusive of each other. This would maximize the number of unique entries in the tables. In practice, the design is "semi-exclusive;" there are potentially duplicates in the BTB1 and BTB2. Techniques are employed to minimize duplicate entries across the levels.</p><p>A truly exclusive BTB hierarchy would invalidate or replace BTB2 entries whenever they are written into the BTB1. It may or may not include a BTBP. Without a BTBP, BTB2 hits would be transferred directly into the BTB1. At that time the BTB1 victim would be written into the BTB2. If the victim and the hit map to the same BTB2 row, it is trivial to have the victim overwrite the hit. If however they map to different BTB2 rows, then two write operations would be necessary. The first write would invalidate the hit and the second would write the victim into the BTB2. With a BTBP, BTB2 hits would be written into the BTBP. Upon BTBP predictions they would be written into the BTB1 and the BTB1 victim would be written into the BTB2 and BTBP. At that time exclusivity would be guaranteed by either replacing the BTB2 hit with the BTB1 victim or by explicitly invalidating the BTB2 hit. This would require the BTBP to remember the BTB2 column corresponding to the BTB2 hit. There is therefore a relatively high cost to guarantee exclusivity: additional BTB2 writes and storage in the BTBP for the BTB2 column.</p><p>A truly exclusive design would also avoid duplication upon surprise installs. Upon encountering a surprise branch, it would be necessary to check whether it already exists in the BTB1. If so it would be prevented from being written into the BTB2.</p><p>Due to the high cost of perfect exclusivity, a semiexclusive design has been chosen. When an entry is copied from BTB2 to BTBP, it is made LRU in the BTB2.</p><p>Upon moving content from the BTBP to BTB1, the content that is evicted from the BTB1 is written into the LRU column in the BTB2 and made MRU. By making BTB2 entries LRU upon BTB2 hits, it is likely that they will be replaced by subsequent BTB1 evictions and/or surprise installs. This avoids an extra write to the BTB2 structure for invalidation but does require an additional write the BTB2 LRU structure. However, the LRU can be a separate, smaller structure than the BTB2 array itself. This approach also does not require the BTB2 column to be stored in each BTBP entry.</p><p>Compared to an inclusive design, an exclusive configuration has the advantage of the BTB2 holding the most recently updated branch prediction information. Upon eviction from the BTB1, any information that has been learned about that branch's behavior is written into the BTB2. In an inclusive design, as branch prediction information is used and updated, a mechanism would be needed to update the information in the corresponding BTB2 entry. Some action would also be necessary to ensure or encourage inclusivity. For example, upon using a BTB1 or BTBP entry to predict a branch, the corresponding BTB2 entry could be made most recently used. Inclusive designs would therefore either update BTB2 entries leading to higher write activity and more power usage than an exclusive design, or forgo such updates to save power at the expense of worse prediction accuracy once stale content from the BTB2 is ultimately used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Definition of a miss in the first level predictor</head><p>In an asynchronous lookahead branch predictor, a first level predictor miss, also called BTB1 miss, is detected after searching the BTB1 and BTBP for a predefined number of searches without finding any predictions. An example of detecting a BTB1 miss is shown in Table <ref type="table" target="#tab_1">2</ref>. It shows the process when the BTB1 miss limit is 3 searches, up to 96 bytes. The three search limit is easier to show in the table than the actual setting of 4 searches, 128 bytes, used in the performance studies. Simulation shows that reporting a BTB1 miss after 4 searches without predictions, up to 128 bytes, provides the best results on the studied workloads (Figure <ref type="figure" target="#fig_2">6</ref>). Defining a BTB1 miss in this manner allows the miss to be detected and reported to the BTB2 logic very early in the pipeline. It is however by its nature speculative since a lack of predictions from the first level predictor does not necessarily mean that they are being missed for capacity reasons. It could be the case that the code does not contain any branches in the address range being searched. This would be the case in a long unrolled loop.</p><p>Alternative ways of defining BTB1 misses are possible. One such way would be to define a BTB1 miss whenever an actual branch instruction is encountered in the decode stage of the pipeline without having been dynamically predicted by the first level branch predictor. Furthermore, only certain types of encountered branches might be detected. For example, only those that are statically guessed taken based on the opcode and instruction text. This alternative method of defining BTB1 misses could be used instead of, or in addition to, the initial definition of a BTB1 miss presented above.</p><p>This alternative way of defining a BTB1 miss need not occur at the decode stage of the pipeline. Misses could be triggered at any stage of the processor pipeline from decode until completion. There is a trade-off between an earlier more speculative indication of a BTB1 miss and a later less speculative indication of a BTB1 miss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Filtering BTB2 transfers based on instruction cache miss</head><p>The definition of a BTB1 miss is speculative. Therefore, it is beneficial to filter out BTB1 misses that are less likely to be actual first level BTB capacity misses. This is done by determining whether or not a detected BTB1 miss also has a corresponding first level instruction cache miss. BTB1 misses that also have corresponding instruction cache misses, in the same 4 KB block, are considered very likely to be BTB1 capacity misses. BTB1 misses that do not correspond to instruction cache misses are considered less likely to be capacity misses.</p><p>Filtered BTB1 misses can either be prevented from accessing the BTB2 or limited to a partial BTB2 search. In the implemented design, filtered BTB1 misses are limited to a 4-row BTB2 search (128 bytes), rather than the full 128-row BTB2 search (4 KB).</p><p>This approach is effective since the footprint of the first level predictors (BTB1+BTBP) is estimated to be between 114 KB and 142.5 KB which is greater than the size of the 64 KB first level instruction cache. Usually if a branch is displaced from the BTB1 and BTBP for capacity reasons it will also have been displaced from the instruction cache. Therefore, cases of a detected BTB1 miss and corresponding instruction cache hit are less likely to be due to limited BTB1 capacity and more likely to be false speculative perceived misses in code containing no branches at all.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">BTB2 search process</head><p>Three BTB2 search trackers are implemented to remember information about BTB1 misses and instruction cache misses; and to initiate read accesses to the BTB2 structure. Each tracker represents one 4 KB block of address space (instruction address bits 0:51). It stores the following information:</p><p>? Block instruction address Trackers with both a valid BTB1 miss and a valid instruction cache miss are fully active trackers. Such trackers initiate reads to all 128 BTB2 rows in the 4 KB block. All branch prediction information read from the BTB2 with matching tags are called BTB2 hits and are written into the BTBP. The order of the reads is determined by priority logic which is described in the next section.</p><p>Trackers with only a valid BTB1 miss indication and an invalid instruction cache miss indication initiate partial BTB2 searches. Specifically, the 128 byte section of code corresponding to BTB1 miss address bits 0:56 in the BTB2 (4 rows) is searched. By the time this partial search completes, if the instruction cache miss validity bit in the tracker is still invalid, the tracker is completely invalidated. Trackers with only an instruction cache miss valid indication and an invalid BTB1 miss indication do not initiate any BTB2 searches.</p><p>Upon a BTB1 miss, the fastest the BTB2 search can be started is in the b10 cycle. This is 7 cycles after the miss is detected in the b3 cycle of the search process. The BTB2 search itself takes 8 cycles. Accesses are pipelined such that one BTB2 row is searched each cycle once searching is underway. Therefore, a full 4 KB bulk transfer takes 128 + 8 = 136 cycles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.">BTB2 search steering</head><p>When transferring content from the BTB2, a 4 KB block of sequential addressing space is searched. Transferring all content sequentially from this block, even if the start point is based on the entry point into the block, is not the most efficient way to transfer branches from the BTB2 into the BTB1. Code executed in the 4 KB block is likely to encounter multiple taken branches and as such not be only sequential in nature. The goal is to transfer those sections in the 4 KB block that will be used first based on where the 4 KB block was entered. To determine an ordering of BTB2 return into the BTBP, an ordering table is tracked as a function of instruction checkpoint.</p><p>Given a 128 byte sector size, there are 32 sectors within a 4 KB block. The 4 KB block is divided into four 1 KB quartiles. Each quartile contains eight 1-bit sector markings and three markings to denote a reference to the other quartiles within the block. As a function of instruction checkpoint, sector ordering is tracked as follows. When a different 4 KB block is entered, the given quartile of entry is defined as the demand quartile. All sectors within the 4 KB block that have an instruction complete get the sector bit set to a '1'. If another quartile is entered from within the block the associated quartile bit within the demand quartile is also set to a '1'. This process continues for the given block until another block is entered. <ref type="bibr">At</ref>  In parallel to initiating a search in the BTB2, the 4 KB pattern block is looked up in the tagged ordering table array. Given a table hit, content is provided for defining the return ordering from the BTB2. First, sectors from the demand quartile marked active are searched and transferred. Second, quartiles which are denoted as referenced from the demand quartile are searched in the BTB2 given the sector bits are active. Third, sector bits that are active but not in the demand or demand referenced quartiles are transferred from the BTB2 to the BTBP.</p><p>After these transfers are performed, the same priority is repeated for those 128 byte sectors which do not have the sector bit set to a '1'. If there is not a table hit, content is returned in sequential order beginning with the demand quartile.</p><p>In the case of multiple 4 KB blocks being addressed by the BTB2 in parallel, the BTB2 will be prioritized to handle all 128 byte regions in the demand quartiles with the sector bits active. Ordering will then proceed through the remaining priorities for all of the multiple 4 KB blocks of data being requested by the BTB2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Study methodology</head><p>The performance provided by the two level bulk preload branch predictor was studied in a C++ simulation model of the zEnterprise EC12 microprocessor using traces of several large commercial workloads. It was also studied on a zEC12 machine running two workloads. Relevant characteristics of the zEC12 are shown in Table <ref type="table" target="#tab_6">5</ref>.</p><p>Part of the microprocessor design process includes developing a C++ performance model of the microprocessor. This model includes the performancerelevant aspects of the microarchitecture such as caches, execution units, pipeline depths, and bypasses. It models multiple levels of the instruction and data cache hierarchy. For the studies in this paper, finite models of the first level caches are used. The second level caches and beyond are considered infinite. Therefore, upon any first level cache miss, a second level cache hit is assumed.</p><p>Wrong path execution is modeled. Whenever the processor goes down a mispredicted path the model simulates what the hardware would encounter down this path using information in the trace about which instructions are located at the wrong path instruction addresses. In some cases, if the model is unable to determine what instructions are at the wrong path addresses, it substitutes filler instructions which are either no-ops or instructions from some previouslyencountered portion of the trace.</p><p>Traces collected from running various workloads and benchmarks are maintained. These traces are constructed to be representative of the performance of the entire workloads of interest even though they are often only portions of an entire benchmark in order to minimize trace size and therefore also simulation time.</p><p>For this study three configurations were analyzed. The configurations of the major branch prediction structures in each of the simulated configurations are shown in Table <ref type="table" target="#tab_4">3</ref>.</p><p>Simulating these configurations allows one to see the performance benefit of adding the BTB2 (configuration 2) compared to the performance potential of an unrealistically large low-latency BTB1 (configuration 3). The percent improvement in Cycles per Instruction (CPI) of configurations 2 and 3 compared to configuration 1 was calculated after running the performance studies. The results are presented in the subsequent "Results" section of this paper for the traces determined to be large footprint workloads.</p><p>The trace pool was analyzed to identify workloads with a large branch instruction footprint. More specifically, any trace with more than 5,000 unique taken branch instruction addresses is a good candidate for showing improvement from additional branch prediction capacity. The large footprint benchmarks expected to benefit from the BTB2 for which results are subsequently presented are listed in Table <ref type="table" target="#tab_5">4</ref> along with counts of all and ever-taken unique branch instructions addresses.</p><p>Traces 1 through 5 are workloads from the IBM Large System Performance Reference (LSPR) <ref type="bibr" target="#b20">[20]</ref> running on the Z/OS operating system. Trace 5 includes a mix of two of the LSPR workloads time sliced on one processor. Trace 6 is the Trade 6 workload <ref type="bibr" target="#b21">[21]</ref> on Z/OS. Trace 7 is of a workload running an airlines reservation system under the TPF operating system <ref type="bibr" target="#b22">[22]</ref>. Trace 8 is a commercial application server. Trace 9 is a commercial database server.</p><p>Traces 10 and 11 are the DayTrader benchmark <ref type="bibr" target="#b23">[23]</ref>. Traces 12 and 13 are the Informix [24] and Trade6 workloads running on the Linux operating system. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>This section presents and analyzes the results from both the simulation model and from runs on the actual hardware. The BTB2 design was analyzed against a wide variety of environments to show the variation in potential benefit. Each individual result should not be considered as representative of all workloads within that specific environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">BTB2 in zEnterprise EC12</head><p>Figure <ref type="figure">2</ref> shows the results of the performance studies in the C++ performance model. It shows the improvement in CPI from the two level branch predictor with the 24k BTB2 enabled (configuration 2) as well as the theoretical improvement possible from the unrealistically large 24k one level branch predictor (configuration 3). These improvements are with respect to the baseline configuration with the BTB2 disabled (configuration 1).</p><p>The bottom bars present the performance benefit of enabling the 24k (4k x 6-way) BTB2. The top bars present the performance benefit of an unrealistically large 24k (4k x 6-way) low-latency BTB1. The numbers on the right are the BTB2 effectiveness, which is defined as the ratio of the improvement from adding the BTB2 compared to the improvement from adding the unrealistically large BTB1.</p><p>For the traces and configurations studied, the maximum benefit of the BTB2 is 13.8% on the DayTrader DBServ trace. This compares to a benefit of 20.2% on this same trace with a large BTB1. BTB2 effectiveness compared to the large BTB1 varies from 16.6% to 83.4% with an average of 52%.  Figure <ref type="figure" target="#fig_0">3</ref> shows the performance benefit of enabling the BTB2 on zEC12 hardware. The WASDB+CBW2 workload was run on a single core. It can be seen that the system performance improvement measured in actual hardware (5.3%) on this workload is less than the performance improvement seen in the simulation model <ref type="bibr">(8.5%)</ref>. This is expected because only the first level instruction and data caches were modeled as finite in the simulation. The Web CICS/DB2 workload which was run on 4 cores is a new version of the single core CICS/DB2 trace. A 3.4% system performance improvement was measured on this workload demonstrating the benefit of the BTB2 in a multi-core environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BTB2 Performance Gain</head><p>The BTB2 reduces the penalty from surprise branches which often incur a target redirect or misprediction penalty. It also reduces the instruction cache miss penalty due to more predicted taken branches initiating and overlapping instruction fetches earlier than if they were encountered as surprise branches.  Figure <ref type="figure" target="#fig_1">4</ref> shows the effect of the BTB2 on bad branch outcomes on the z/OS DayTrader DBServ trace from the C++ simulation model. This figure provides insight into why such a large performance improvement is achieved. As previously reported, the BTB2 improves performance of this workload by 13.8%.</p><p>Bad branch outcomes are those that incur a performance penalty. Specifically they consist of dynamically mispredicted branches and surprise branches which are guessed or resolved taken. These bad surprise branches are classified as compulsory (first time that branch is seen), latency (surprise because a prediction wasn't available in time either due to prediction falling behind decode, or due to latency for writing surprise branches into the prediction tables), or capacity (branch was seen before, and not categorized as missed due to latency). Dynamically mispredicted branches are guessed taken and resolved not-taken, guessed not-taken and resolved taken, or guessed taken and resolved taken with wrong target address prediction.</p><p>Figure <ref type="figure" target="#fig_1">4</ref> shows that without the BTB2, 25.9% of all branch outcomes are classified as being bad. Most of these (21.9% of all branch outcomes) are capacity bad surprise branches. Adding the BTB2 reduces the number of capacity bad surprise branches to 8.1%. The total number of bad branch outcomes is reduced to 14.3%. In large footprint workloads such as this, a large portion of the branch penalty is due to branch prediction capacity rather than branch direction and target prediction algorithms. This is the reason why the multi-level branch predictor described in this paper is so effective at improving performance.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Other simulated configurations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 7. Various numbers of BTB2 trackers</head><p>Figure <ref type="figure">5</ref> shows the effect of varying the size of the BTB2 demonstrating the performance opportunity of a larger BTB2. Figure <ref type="figure" target="#fig_2">6</ref> shows the effect of varying the definition of a BTB1 miss. Figure <ref type="figure">7</ref> shows the effect of varying the number of BTB2 search trackers. These results support the choices made for the hardware implementation, shown in the charts with stripes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Future work</head><p>Further gains in performance and performance/watt are being addressed through capacity and efficiency focus. A multi-level BTB allows for designing the first level in high-speed SRAM and the BTB2 in a higher density memory technology.</p><p>Through optimal technology usage, the multi-level BTB design will support a greater number of predictions per square millimeter than a single level BTB designed solely in SRAM. Understanding the trade-offs between SRAM and eDRAM may be analyzed for defining an optimal design point which consists of SRAM for the BTB1 and eDRAM for the BTB2.</p><p>Improving the transfer rate will improve the efficiency of the ratio between a multi-level BTB hit rate and a large BTB1 only hit rate. The transfer rate is a function of bus utilization which is a function of the amount of address space covered per BTB2 congruence class. The current results apply an index which covers 32 bytes of instruction address space per 6-ways of associativity. This allows 6 predictions to be stored into the BTB2 for every 32 bytes of sequential virtual address space. By increasing the virtual address space per congruence class to 64 or 128 bytes, the amount of tag matching branches per search will increase at the cost of not being able to store all branches within a sequential code stream into the BTB2 because of overflowing a congruence class. The trade-off is not only an indexing bit range selection but also about which branches are to be tracked by the BTB2. An analysis may be made to determine how relative and indirect branches should be weighted for acquiring space in the BTB2. An additional analysis may be made to determine how branch target displacement ranges should prioritize which branches are stored into the BTB2.</p><p>Current work transfers only those branches within a 4 KB block. While the block size can be altered, being able to process more than a single block will increase the first level BTB hit rate. In a single BTB2 block search, there may be more than a single branch which redirects to another unique block. Without careful selection, the number of blocks to transfer can exponentially exceed the available bandwidth. An algorithm for multi-block transfers is an area of interest.</p><p>Investigating alternative ways of defining a BTB1 miss is a topic for additional study. Such research would explore the differences between detecting misses early in the pipe with high speculation as described in this paper versus later in the pipe with less speculation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions</head><p>This paper describes the design and performance results of the two level bulk preload branch predictor used in the IBM zEnterprise EC12.</p><p>The novel structures and algorithms comprising this branch prediction hierarchy are presented as a solution to improve performance of large workloads sensitive to BTB capacity.</p><p>This design approximates the performance benefit of a very large BTB without negatively affecting the BTB1's access time, throughput, area, or power consumption. Simulated performance results of 13 traces show that the design achieves a maximum performance benefit of 13.8%. On average 52% of the benefit of an unrealistically large single level predictor of comparable capacity is attained. Measurements on actual hardware of two LSPR workloads show 5.3% and 3.4% system performance improvements.</p><p>The measured benefit of this design underlines its value. The design yielding half the value of its theoretical ceiling provides an impetus for continued research, refinement, and development.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Benefit of BTB2 on zEC12 hardware</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Effect of BTB2 on bad branch outcomes from C++ model of Z/OS DayTrader DBServ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Various definitions of BTB1 missAdditional configurations were simulated in the C++ model to investigate the effects of varying parameters of the design. The figures in this section show the benefit of the BTB2 compared to the configuration without the BTB2. They are the average of the 13 traces.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 . First level branch prediction search pipeline</head><label>1</label><figDesc></figDesc><table><row><cell cols="2">Cycle Search</cell><cell>Re-index for</cell><cell>Re-index</cell></row><row><cell></cell><cell>process</cell><cell>prediction</cell><cell>sequential</cell></row><row><cell>b0</cell><cell>Index arrays</cell><cell></cell><cell></cell></row><row><cell>(x)</cell><cell>with search address x.</cell><cell></cell><cell></cell></row><row><cell>b1</cell><cell>Access</cell><cell></cell><cell>b0</cell></row><row><cell></cell><cell>arrays.</cell><cell></cell><cell>(x+1)</cell></row><row><cell>b2</cell><cell>Start hit</cell><cell>If under FIT</cell><cell>b0</cell></row><row><cell></cell><cell>detection.</cell><cell>control, re-index (b0) with</cell><cell>(x+2)</cell></row><row><cell></cell><cell></cell><cell>FIT-supplied</cell><cell></cell></row><row><cell></cell><cell></cell><cell>index for</cell><cell></cell></row><row><cell></cell><cell></cell><cell>expected branch</cell><cell></cell></row><row><cell></cell><cell></cell><cell>prediction.</cell><cell></cell></row><row><cell>b3</cell><cell>Finish hit</cell><cell>If not under FIT</cell><cell></cell></row><row><cell></cell><cell>detection.</cell><cell>control, re-</cell><cell></cell></row><row><cell></cell><cell>Select prediction information.</cell><cell>index (b0) assuming taken prediction from MRU column.</cell><cell></cell></row><row><cell>b4</cell><cell>Broadcast</cell><cell>If necessary, re-</cell><cell></cell></row><row><cell></cell><cell>prediction</cell><cell>index (b0) for</cell><cell></cell></row><row><cell></cell><cell>info for taken</cell><cell>not-taken</cell><cell></cell></row><row><cell></cell><cell>prediction</cell><cell>prediction or</cell><cell></cell></row><row><cell></cell><cell>from MRU</cell><cell>taken prediction</cell><cell></cell></row><row><cell></cell><cell>column.</cell><cell>not from MRU</cell><cell></cell></row><row><cell></cell><cell></cell><cell>column.</cell><cell></cell></row><row><cell>b5</cell><cell>Broadcast</cell><cell>If necessary, re-</cell><cell></cell></row><row><cell></cell><cell>prediction</cell><cell>index (b0) for</cell><cell></cell></row><row><cell></cell><cell>info for 1 st</cell><cell>second not-</cell><cell></cell></row><row><cell></cell><cell>not-taken</cell><cell>taken</cell><cell></cell></row><row><cell></cell><cell>prediction or</cell><cell>prediction.</cell><cell></cell></row><row><cell></cell><cell>taken</cell><cell></cell><cell></cell></row><row><cell></cell><cell>prediction not</cell><cell></cell><cell></cell></row><row><cell></cell><cell>from MRU</cell><cell></cell><cell></cell></row><row><cell></cell><cell>column.</cell><cell></cell><cell></cell></row><row><cell>b6</cell><cell>Broadcast</cell><cell></cell><cell>b0</cell></row><row><cell></cell><cell>branch</cell><cell></cell><cell></cell></row><row><cell></cell><cell>prediction</cell><cell></cell><cell></cell></row><row><cell></cell><cell>info for 2 nd</cell><cell></cell><cell></cell></row><row><cell></cell><cell>not-taken</cell><cell></cell><cell></cell></row><row><cell></cell><cell>prediction.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 . BTB1 miss detection as part of first level prediction search process</head><label>2</label><figDesc></figDesc><table><row><cell>Cycle</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell></row><row><cell>start</cell><cell>b0</cell><cell>b1</cell><cell>b2</cell><cell>b3</cell><cell></cell><cell></cell></row><row><cell>0x102</cell><cell>BTB1</cell><cell>BTB1</cell><cell>start</cell><cell>finish</cell><cell></cell><cell></cell></row><row><cell></cell><cell>BTBP</cell><cell>BTBP</cell><cell>hit</cell><cell>hit</cell><cell></cell><cell></cell></row><row><cell></cell><cell>index</cell><cell>access</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>detect</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>1 st miss</cell><cell></cell><cell></cell></row><row><cell>search+1</cell><cell></cell><cell>b0</cell><cell cols="2">b1 b2</cell><cell>b3</cell><cell></cell></row><row><cell>0x120</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2 nd miss</cell><cell></cell></row><row><cell>search+2</cell><cell></cell><cell></cell><cell cols="2">b0 b1</cell><cell>b2</cell><cell>b3</cell></row><row><cell>0x140</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>3 rd miss</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>BTB1 miss</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>reported at</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>starting search</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>address 0x102</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>this point, this information is stored and tracked in the ordering table array. The ordering table array tracks information as chunks of 128 bytes, which are called sectors. The table contains 512 entries and is 2-way set associative. Each entry represents a 4 KB block; therefore the table covers a 2 MB instruction footprint. Upon returning to the given block, information from the tagged ordering table array is retrieved and updated with any new paths which are traversed by the program code.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 . Simulated Configurations</head><label>3</label><figDesc></figDesc><table><row><cell>Name</cell><cell>BTBP</cell><cell>BTB1</cell><cell>BTB2</cell></row><row><cell>1. No BTB2</cell><cell>768</cell><cell>4k</cell><cell>0</cell></row><row><cell></cell><cell>(128 x 8)</cell><cell>(1k x 4)</cell><cell>(Disabled)</cell></row><row><cell>2. BTB2</cell><cell>768</cell><cell>4k</cell><cell>24k</cell></row><row><cell>enabled</cell><cell>(128 x 6)</cell><cell>(1k x 4)</cell><cell>(4k x 6)</cell></row><row><cell>3.</cell><cell>768</cell><cell>24k</cell><cell>0</cell></row><row><cell>Unrealistically</cell><cell>(128 x 6)</cell><cell>(4k x 6)</cell><cell>(Disabled)</cell></row><row><cell>large BTB1</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 . Large footprint traces</head><label>4</label><figDesc></figDesc><table><row><cell>Trace name</cell><cell>Number of</cell><cell>Number of unique</cell></row><row><cell></cell><cell>unique</cell><cell>taken branch</cell></row><row><cell></cell><cell>branch</cell><cell>instruction</cell></row><row><cell></cell><cell>instruction</cell><cell>addresses</cell></row><row><cell></cell><cell>addresses</cell><cell></cell></row><row><cell cols="2">1. Z/OS LSPR CB84 15,244</cell><cell>10,963</cell></row><row><cell>2. Z/OS LSPR</cell><cell>40,667</cell><cell>27,500</cell></row><row><cell>CICS/DB2</cell><cell></cell><cell></cell></row><row><cell cols="2">3. Z/OS LSPR IMS 29,692</cell><cell>19,673</cell></row><row><cell cols="2">4. Z/OS LSPR CB-L 25,622</cell><cell>16,612</cell></row><row><cell>5. Z/OS LSPR</cell><cell>114,955</cell><cell>51,371</cell></row><row><cell>WASDB+CBW2</cell><cell></cell><cell></cell></row><row><cell>6. Z/OS Trade6</cell><cell>115,509</cell><cell>56,017</cell></row><row><cell>7. TPF airline</cell><cell>11,160</cell><cell>9,317</cell></row><row><cell>reservations</cell><cell></cell><cell></cell></row><row><cell>8. Z/OS AppServ</cell><cell>26,340</cell><cell>16,980</cell></row><row><cell>benchmark</cell><cell></cell><cell></cell></row><row><cell>9. Z/OS DBServ</cell><cell>38,655</cell><cell>20,020</cell></row><row><cell>benchmark</cell><cell></cell><cell></cell></row><row><cell>10. Z/OS DayTrader</cell><cell>67,336</cell><cell>30,165</cell></row><row><cell>AppServ</cell><cell></cell><cell></cell></row><row><cell>11. Z/OS DayTrader</cell><cell>34,819</cell><cell>22,217</cell></row><row><cell>DBServ</cell><cell></cell><cell></cell></row><row><cell cols="2">12. zLinux Informix 16,810</cell><cell>11,765</cell></row><row><cell>13. zLinux Trade6</cell><cell>69,847</cell><cell>31,897</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 . zEnterprise EC12 chip configuration</head><label>5</label><figDesc></figDesc><table><row><cell>L1 Cache</cell><cell cols="2">Instruction cache 64KB (4-way)</cell></row><row><cell></cell><cell cols="2">Data cache 96KB (6-way)</cell></row><row><cell>L2 Cache</cell><cell cols="2">Instruction cache 1 Meg (8-way)</cell></row><row><cell></cell><cell cols="2">Data cache 1 Meg (8-way)</cell></row><row><cell>L3 Cache</cell><cell cols="2">48 Meg on-chip</cell></row><row><cell>L4 Cache</cell><cell cols="2">384 Meg off-chip</cell></row><row><cell>I-TLB1</cell><cell cols="2">4K &amp; 1 Meg pages: 64 x 2</cell></row><row><cell>D-TLB1</cell><cell cols="2">4K pages: 256 x 2</cell></row><row><cell></cell><cell cols="2">1M pages: 32 x 2</cell></row><row><cell></cell><cell>2G pages:</cell><cell>1 x 8</cell></row><row><cell>TLB2</cell><cell cols="2">128 x 4 CRSTE; 256 x 3 PTE /</cell></row><row><cell></cell><cell>CRSTE</cell></row><row><cell>Issue Queue</cell><cell>32 x 2</cell></row><row><cell cols="3">Completion Table 30 x 3 micro-ops</cell></row><row><cell>Physical Regs</cell><cell cols="2">80 general registers,</cell></row><row><cell></cell><cell cols="2">64 floating point</cell></row><row><cell cols="3">Issue bandwidth 7 (2 LSU, 2 FXU, 2 Branch, 1</cell></row><row><cell></cell><cell>Float)</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="8.">Acknowledgements</head><p>We would like to thank the review committee and our colleagues <rs type="person">Ioana Baldini</rs>, <rs type="person">Jane Bartik</rs>, <rs type="person">Steven Carlough</rs>, <rs type="person">Andrea Harris</rs>, <rs type="person">David Hutton</rs>, <rs type="person">Gary King</rs>, <rs type="person">Jim Mitchell</rs>, <rs type="person">Michael Rihn</rs>, <rs type="person">Eric Schwarz</rs>, <rs type="person">Kevin Shum</rs>, <rs type="person">Charles Webb</rs> for their advice and assistance in preparing this paper for publication.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Context Look Ahead Storage Structures</title>
		<author>
			<persName><forename type="first">P</forename><surname>Emma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hartstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Prasky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Puzak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">U.S. Patent</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">271</biblScope>
			<date type="published" when="2008-02">February 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Phantom-BTB: A Virtualized Branch Target Buffer Design</title>
		<author>
			<persName><forename type="first">I</forename><surname>Burcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moshovos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The Optimum Pipeline Depth for a Microprocessor</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hartstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Puzak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Annual International Symposium on Computer Architecture</title>
		<meeting>the 29th Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2002-05">May 2002</date>
			<biblScope unit="page" from="7" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Evaluation of branch-prediction method on traces from commercial applications</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Hilgendorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Heim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Rosenstiel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1999-07">July 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Branch Prediction Strategies and Branch Target Buffer Design</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K F</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1984-01">January 1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Method and Apparatus for Prefetching Branch History Information</title>
		<author>
			<persName><forename type="first">P</forename><surname>Emma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Getzlaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hartstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pflueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Puzak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">U.S. Patent</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">480</biblScope>
			<date type="published" when="2009-02">February 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Instruction Sequence Control</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Sussenguth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">U.S. Patent</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">183</biblScope>
			<date type="published" when="1971-01">January 1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pageable Branch History Table</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pomerene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Puzak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rechtschaffen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sparacio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">U.S. Patent</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">141</biblScope>
			<date type="published" when="1987-07">July 1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Two-Level Branch Prediction Cache</title>
		<author>
			<persName><forename type="first">D</forename><surname>Stiles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Favor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Van Dyke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">U.S. Patent</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">140</biblScope>
			<date type="published" when="1992-11">November 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Method and Apparatus for Branch Prediction Using First and Second Level Branch Prediction Tables</title>
		<author>
			<persName><forename type="first">T</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sharangpani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">U.S. Patent</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">488</biblScope>
			<date type="published" when="2003-04">April 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hybrid Branch Prediction Device with Two Levels of Branch Prediction Cache</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zuraski</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Roberts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">U.S. Patent</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">545</biblScope>
			<date type="published" when="2006-04">April 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Overriding a Static Prediction with a Level-Two Predictor</title>
		<author>
			<persName><forename type="first">M</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jourdan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Phelps</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">U.S. Patent</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">252</biblScope>
			<date type="published" when="2009-05">May 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Dobos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fried</surname></persName>
		</author>
		<ptr target="http://www.redbooks.ibm.com/redpieces/pdfs/sg248049.pdf" />
		<title level="m">IBM zEnterprise EC12 Technical Guide</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The Impact of Delay on the Design of Branch Predictors</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Keckler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd Annual ACM/IEEE International Symposium on Microarchitecture</title>
		<meeting>the 33rd Annual ACM/IEEE International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">IBM zEnterprise 196 microprocessor and cache subsystem</title>
		<author>
			<persName><forename type="first">F</forename><surname>Busaba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">1 / 2</biblScope>
			<date type="published" when="2012-03">Jan/Mar 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Branch Target Buffer Design and Optimization</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Perleberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Burcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moshovos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Somogyi</surname></persName>
		</author>
		<title level="m">Predictor Virtualization&quot; Proceedings of the 13th International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS XIII</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="157" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Toward Virtualizing Branch Direction Prediction&quot; Design, Automation &amp; Test in Europe Conference &amp; Exhibition (DATE)</title>
		<author>
			<persName><forename type="first">K</forename><surname>Aasaraai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moshovos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sadooghi-Alvandi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="455" to="460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Reconsidering Complex Branch Predictors</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jimenez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Symposium on High-Performance Computer Architecture</title>
		<meeting>the 9th International Symposium on High-Performance Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Large Systems Performance Reference&quot; Document Number SC</title>
		<ptr target="https://www-304.ibm.com/servers/resourcelink/lib03060.nsf/pages/lsprindex?OpenDocument" />
		<imprint>
			<date type="published" when="2011-07">July 2011</date>
			<biblScope unit="page" from="28" to="1187" />
		</imprint>
		<respStmt>
			<orgName>IBM Corporation</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Set up and run a Trade6 benchmark with DB2 UDB</title>
		<author>
			<persName><forename type="first">J</forename><surname>Coleman</surname></persName>
		</author>
		<ptr target="http://www.ibm.com/developerworks/data/tutorials/dm0506lau/" />
		<imprint>
			<date type="published" when="2005-06">June 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<ptr target="http://www-01.ibm.com/software/htp/tpf/" />
		<title level="m">/Transaction Processing Facility</title>
		<imprint/>
		<respStmt>
			<orgName>IBM Corporation</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Apache Geronimo Daytrader</title>
		<ptr target="http://www-01.ibm.com/software/data/informix/" />
	</analytic>
	<monogr>
		<title level="m">Informix product family</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
		</imprint>
		<respStmt>
			<orgName>IBM Corporation</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A ppm-like, tag-based predictor</title>
		<author>
			<persName><forename type="first">P</forename><surname>Michaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Instruction Level Parallelism</title>
		<imprint>
			<date type="published" when="2005-04">April 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Design Tradeoffs for the Alpha EV8 Conditional Branch Predictor</title>
		<author>
			<persName><forename type="first">A</forename><surname>Seznec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sazeides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29 th Annual International Symposium on Computer Architecture</title>
		<meeting>the 29 th Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
