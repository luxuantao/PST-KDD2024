<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PANIC: A High-Performance Programmable NIC for Multi-tenant Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jiaxin</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Wisconsin -Madison</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kiran</forename><surname>Patel</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Illinois at Chicago</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Brent</forename><forename type="middle">E</forename><surname>Stephens</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Illinois at Chicago</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anirudh</forename><surname>Sivaraman</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aditya</forename><surname>Akella</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">University of Wisconsin-Madison</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">PANIC: A High-Performance Programmable NIC for Multi-tenant Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>USENIX Association 14th USENIX Symposium on Operating Systems Design and Implementation 243</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Programmable NICs have diverse uses, and there is a need for a NIC platform that can offload computation from multiple co-resident applications to many different types of substrates, including hardware accelerators, embedded FPGAs, and embedded processor cores. Unfortunately, there is no existing NIC design that can simultaneously support a large number of diverse offloads while ensuring high throughput/low latency, multi-tenant isolation, flexible offload chaining, and support for offloads with variable performance.</p><p>This paper presents PANIC, a new programmable NIC. There are two new key components of the PANIC design that enable it to overcome the limitations of existing NICs: 1) A high-performance switching interconnect that scalably connects independent engines into offload chains, and 2) A new hybrid push/pull packet scheduler that provides cross-tenant performance isolation and low-latency load-balancing across parallel offload engines. From experiments performed on an 100 Gbps FPGA-based prototype, we find that this design overcomes the limitations of state-of-the-art programmable NICs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The gap between network line-rates and the rate at which a CPU can produce and consume data is widening rapidly <ref type="bibr" target="#b65">[71,</ref><ref type="bibr" target="#b60">66]</ref>. Emerging programmable ("smart") NICs can help overcome this problem <ref type="bibr" target="#b28">[32]</ref>. There are many different types of offloads that can be implemented on a programmable NIC. These offloads, which accelerate computation across all of the different layers of the network stack, can reduce load on the general purpose CPU, reduce latency, and increase throughput <ref type="bibr" target="#b28">[32,</ref><ref type="bibr" target="#b44">48,</ref><ref type="bibr" target="#b53">59,</ref><ref type="bibr" target="#b63">69,</ref><ref type="bibr" target="#b11">13]</ref>.</p><p>Many different cloud and datacenter applications and use cases have been shown to benefit from offloading computation to programmable NICs <ref type="bibr" target="#b11">[13,</ref><ref type="bibr" target="#b44">48,</ref><ref type="bibr" target="#b53">59,</ref><ref type="bibr" target="#b38">42,</ref><ref type="bibr" target="#b28">32,</ref><ref type="bibr" target="#b33">37,</ref><ref type="bibr" target="#b45">49,</ref><ref type="bibr" target="#b42">46,</ref><ref type="bibr" target="#b56">62,</ref><ref type="bibr" target="#b43">47,</ref><ref type="bibr" target="#b26">30,</ref><ref type="bibr" target="#b32">36,</ref><ref type="bibr" target="#b64">70,</ref><ref type="bibr" target="#b63">69,</ref><ref type="bibr" target="#b31">35,</ref><ref type="bibr" target="#b50">55,</ref><ref type="bibr" target="#b41">45]</ref>. However, there is no single "silver bullet" offload that can improve performance in all cases. Instead, we anticipate that different applications will specify their own chains of offloads, and that the operator will then merge these chains with infrastructure-related offloads and run them on her programmable NICs. To realize this vision, this paper presents PANIC, a new scalable and highperformance programmable NIC for multi-tenant networks that supports a wide variety of different types of offloads and composes them into isolated offload chains.</p><p>To enable cloud operators to provide NIC offload chains as a service to tenants, a programmable NIC must support: 1) Offload variety: some offloads like cryptography are best suited for hardware implementations, while an offload providing a low-latency bypass for RPCs in an application is better suited for an embedded core <ref type="bibr" target="#b47">[51]</ref>; 2) Offload chaining: to minimize wasted chip area on redundant functions, the NIC should facilitate composing independent hardware offload units into a chain as needed, with commonly-needed offloads shared across tenants; 3) Multi-tenant isolation: tenants should not be able to consume more than their allocation of a shared offload; 4) Variable-performance offloads: there are useful offloads that are not guaranteed to run at line-rate, as well as important offloads that run with low latency and at line-rate.</p><p>There exist many different programmable NICs <ref type="bibr" target="#b28">[32,</ref><ref type="bibr" target="#b10">12,</ref><ref type="bibr" target="#b69">75,</ref><ref type="bibr" target="#b27">31,</ref><ref type="bibr">58,</ref><ref type="bibr" target="#b66">72,</ref><ref type="bibr">23,</ref><ref type="bibr">24,</ref><ref type="bibr" target="#b9">11,</ref><ref type="bibr" target="#b52">57,</ref><ref type="bibr">53,</ref><ref type="bibr" target="#b49">54,</ref><ref type="bibr" target="#b48">52,</ref><ref type="bibr" target="#b70">76]</ref>, but, there is no programmable NIC that is currently able to provide all of the above properties. Existing NIC designs can be categorized as follows, with each category imposing key limitations:</p><p>• Pipeline-of-Offloads NICs place multiple offloads in a pipeline to enable packets to be processed by a chain of functions <ref type="bibr" target="#b48">[52,</ref><ref type="bibr" target="#b28">32]</ref>. Chaining can be modified in these NICs today but requires a significant amount of time and developer effort for FPGA synthesis, and slow offloads cause packet loss or head-of-line (HOL) blocking. • Manycore NICs load balance packets across many embedded CPU cores, with the CPU core then controlling the processing of packets as needed for different offloads <ref type="bibr">[23,</ref><ref type="bibr">24,</ref><ref type="bibr">53,</ref><ref type="bibr" target="#b49">54,</ref><ref type="bibr" target="#b52">57,</ref><ref type="bibr" target="#b66">72,</ref><ref type="bibr">58]</ref>. These designs suffer from performance issues because embedded CPU cores add tens of microseconds of additional latency <ref type="bibr" target="#b28">[32]</ref>. Also, no existing manycore NICs provide performant mechanisms to isolate competing tenants. Further, performance on manycore NICs can degrade significantly if the working set does not fit within the core's cache. • RMT NICs use on-NIC reconfigurable match+action (RMT) pipeline to implement NIC offloads. The types of offloads that can be supported by RMT pipelines are limited because each pipeline stage must be able to handle processing a new packet every single clock cycle.</p><p>This paper presents the design, implementation and evaluation of PANIC, a new NIC that overcomes the key limitations of existing NIC designs. PANIC draws inspiration from recent work on reconfigurable (RMT) switches <ref type="bibr" target="#b19">[21,</ref><ref type="bibr" target="#b61">67,</ref><ref type="bibr" target="#b62">68,</ref><ref type="bibr" target="#b23">27,</ref><ref type="bibr" target="#b14">16</ref>]. PANIC's design leverages three key principles:</p><p>1. Offloads should be self-contained. The set of potentially useful offloads is diverse and vast, spanning all of the layers of the network stack. As such, a programmable NIC should be able to support both hardware IP cores and embedded CPUs as offloads. 2. Packet scheduling, buffering, and load-balancing should be centralized for the best performance and efficiency because decentralized decisions and per-offload queuing can lead to poor tail response latencies and poor buffer utilization due to load imbalances. 3. Because the cost of small/medium-sized non-blocking fabrics is small relative to the NIC overall, the offloads should be connected by a non-blocking/low-oversubscribed switching fabric to enable flexible chaining of offloads.</p><p>Following these design principles, this paper makes three key contributions: 1) A novel programmable NIC design where diverse offloads are connected to a non-blocking switching fabric, with chains orchestrated by a programmable RMT pipeline, 2) A new hybrid push/pull scheduler-and-load balancer with priority-aware packet dropping, and 3) An analysis of the costs of on-NIC programmable switching and scheduling that finds them to be low relative to the NIC as a whole.</p><p>The PANIC NIC has four components: 1) an RMT switch pipeline, 2) a switching fabric, 3) a central scheduler, and 4) self-contained compute units. The RMT pipeline provides programmable chain orchestration. A high performance interconnect enables programmable chaining at line-rate. The central scheduler provides isolation, buffer management, and load-balancing. Self-contained compute units may be either hardware accelerators or embedded cores and are not required to run at line-rate.</p><p>To evaluate the feasibility of PANIC, we have performed both ASIC analysis and experiments with an FPGA prototype. Our ASIC analysis demonstrates the feasibility of the PANIC architecture and shows that the crossbar interconnect topology scales well up to 32 total attached compute units. Our FPGA prototype can perform dynamic offload chaining at 100 Gbps, and achieves nanosecond-level (&lt;0.8 µs) packet scheduling and load-balancing under a variety of chaining configurations. We empirically show that PANIC can handle multi-tenant isolation and below line-rate offloads better than a state-of-the-art pipeline-based design. Our end-to-end experiments in a small scale testbed demonstrate that PANIC can achieve dynamic bandwidth allocation and prioritized packet scheduling at 100 Gbps. In total, the components of PANIC, which includes an 8 * 8 crossbar, only consume a total of 11.27% of the total logic area (LUTs) available on the Xilinx UltraScale Plus FPGA that we used. The Verilog code for our FPGA prototype is publicly available<ref type="foot" target="#foot_0">1</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Motivation</head><p>We discuss in detail the requirements that we envision programmable NICs in multi-tenant networks ought to meet. We then explain why existing NICs designs fail to meet them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Requirements</head><p>1. Offload Variety: There are a large variety of network offloads, and different types of offloads have different needs. Not all offloads are best implemented on the same type of underlying engine. For example, a cryptography offload can provide much better performance if implemented with a hardware accelerator built from a custom IP core instead of an embedded processor core. To shed light on this, we experimented with a few different types of offloads using an Alpha Data ADM-PCIE-9V3 Programmable NIC <ref type="bibr" target="#b10">[12]</ref> to evaluate the behavior of different hardware IP cores that could be used as on-NIC accelerators, and the Rocket Chip Generator <ref type="bibr" target="#b12">[14]</ref> to perform cycle-accurate performance measurements of a RISC V CPU to understand the costs of running these offload with an on-NIC embedded processor. Our results in Table <ref type="table" target="#tab_0">1</ref> indeed show that offloads for encryption/decryption and authentication are a poor fit for embedded CPU designs and should be implemented in hardware.</p><p>In contrast, an application-specific offload to walk a hash table that is resident in main memory is better suited for an embedded processor core because a hardware offload may not provide enough flexibility <ref type="bibr" target="#b47">[51]</ref>. Thus, a programmable NIC should ideally provide support for both hardware and software offloads. 2. Dynamic Offload Chaining: In the case of hardware accelerators, it is important to be able to compose independent offload functionality into a chain/pipeline to avoid wasted area on redundant functionality. For example, using a programmable NIC to implement a secure remote memory access for a tenant may require the tenant to compose cryptography, congestion control, and RDMA offload engines.</p><p>Further, as tenants come and go, and as a given application's traffic patterns change, the on-NIC offload chains will also need to be dynamically updated. This is because different network transfers benefit from different sets of offloads. Further, not every application packet needs every offload. For example, for a key-value store that serves requests from both within-DC and WAN-distributed clients, IPSec and/or compression could be offloaded, but only the packets sent over the WAN may need IPSec authentication and/or compression.</p><p>Thus, an ideal programmable NIC should not restrict the type of offloads that may be simultaneously used, and should instead support dynamic offload chaining, i.e., switching and scheduling packets as needed between independent offloads. 3. Dynamic Isolation: Today's data center servers colocate applications from different competing tenants <ref type="bibr" target="#b46">[50,</ref><ref type="bibr" target="#b35">39,</ref><ref type="bibr" target="#b13">15,</ref><ref type="bibr" target="#b55">61,</ref><ref type="bibr" target="#b28">32]</ref>. Each tenant may have its own offload chains that may need to run on a programmable NIC, so it is necessary for a programmable NIC to provide performant low-level isolation mechanisms. For example, consider the case that two tenants A and B are running offload chains where packets are first uncompressed and then sent to an embedded CPU for further processing, and packet contents are such that the workload for tenant B runs at half the rate of that of tenant A. To support this, the NIC's mechanisms must ensure fair packet scheduling at the shared compression offload and that the slow chain does not cause head-of-line (HOL) blocking for the other chain. Further, if a third tenant C were to start, packet processing load across chains may shift. To handle this, the scheduling policy may need to be reprogrammed. 4. Support for offloads with variable and below line-rate performance: Some offloads may not run at line-rate. Of the compression, cryptography, authentication, and inference offloads that we ran on hardware, only inference was able to run at 100 Gbps (Table <ref type="table" target="#tab_0">1</ref>), and others ran well below linerate. Also, offload performance is variable and sometimes workload-dependent, incurring significant delay for certain requests; see, for example, compression and authentication, whose performance depends on packet size.</p><p>These results also show the need for an approach to loadbalancing that can accommodate offloads with variable performance. Slow offloads can be duplicated across multiple engines (e.g., 3 AES-256 engines) for line-rate operation. 5. High-Performance Interconnect: It is important for a programmable NIC to be able to provide high throughput for line-rate offloads. In the case where no offloads or only low-latency offloads are used, a programmable NIC should not incur any additional latency. Achieving high performance is complicated by bidirectional communication, multi-port NICs, and chaining. An offload that is used for TX and RX on a dual port NIC needs to operate at four times line-rate to prevent becoming a bottleneck. When offloads are chained, a single packet may traverse the on-NIC network multiple times. Effectively, the NIC must be able to emulate creating a line-rate connection between each hop in an offload chain.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Limitations of Existing Designs</head><p>We argue below that programmable NIC designs today (Figure <ref type="figure" target="#fig_1">1</ref>) lag behind these requirements (Table <ref type="table" target="#tab_2">2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Pipeline Designs</head><p>Figure <ref type="figure" target="#fig_1">1a</ref> illustrates the pipelined programmable NIC design.</p><p>In this design, the offloads are arranged in a linear sequence, i.e., a pipeline. Effectively, each offload looks as though it is an independent device attached in the middle of the wire connecting the NIC to a TOR switch. Most existing NICs with on-board FPGAs located as a "bump-in-the-wire" use this design <ref type="bibr" target="#b48">[52,</ref><ref type="bibr" target="#b28">32,</ref><ref type="bibr" target="#b27">31]</ref>, and other NICs use this design for fixed function offloads for TCP checksums and IPSec <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b34">38]</ref>.</p><p>Chaining: Chaining offloads is difficult in pipelined designs because of their static offload topology; the offloads are arranged in a line. Although packets can be recirculated through the pipeline as needed, this wastes on-NIC bandwidth and hurts line-rate performance.</p><p>Variable Performance Offloads: A slow offload that does not run at line-rate can cause HOL blocking in the pipeline of offloads if the pipeline is stalled, and packet loss if the pipeline is not. This can be avoided with routing logic to bypass offloads, but this requires additional buffer memory at each offload: packet arrivals in Ethernet are bursty <ref type="bibr" target="#b37">[41,</ref><ref type="bibr" target="#b15">17]</ref>, and it common for tens of packets to arrive back-to-back at line-rate. There would be significant packet loss if offloads that are not guaranteed to run at line-rate are not allocated buffer resources. For offloads where running at line-rate is workload or configuration dependent, the chip area allocated to per-offload buffers would be wasted under some traffic patterns.</p><p>Multi-tenant Isolation: In a pipeline, packets are forwarded through offloads that do not need to process the packet. Even if every offload runs at full line-rate, a high latency offload used by Tenant A but not by Tenant B will unnecessarily lead to increased latency for Tenant B. This can be avoided with routing logic to bypass offloads, but this also requires additional buffer memory at each offload to avoid pipeline stalls or packet drops. It is only possible to bypass an offload without stalling the pipeline if there is somewhere else to put the packets that it is currently processing. Multi-tenant isolation is more problematic if not all offloads are guaranteed to run at line-rate. In this case, if tenant A has already consumed all of the packet buffers allocated for an offload, then tenant B will experience HOL-blocking and possibly packet loss. Although per-offload scheduling  logic could be used to overcome this limitation, this has area overheads, and, as with per-offload packet buffers, this logic may be unutilized in some workloads. Offload Variety: Pipeline-of-offloads designs are typically used for programmable NICs that only support hardware offloads. The limitations of pipeline designs are best avoided with low-latency offloads that run at line-rate. Because embedded CPU cores may not run at full line-rate and can incur high processing latency, this makes them a poor fit for pipeline designs. To overcome this limitation, the Azure SmartNIC <ref type="bibr" target="#b28">[32]</ref> onloads computation from the programmable NIC to a core on the main CPU for certain tasks. This approach is costly, especially in cloud environments where servers are leased to customers on a per-core basis.</p><p>Some FPGA NICs implement all NIC functionality on an FPGA, including the Ethernet MAC and PCIe engines <ref type="bibr" target="#b10">[12,</ref><ref type="bibr" target="#b69">75,</ref><ref type="bibr" target="#b70">76,</ref><ref type="bibr" target="#b27">31]</ref>. Such NICs do not have many inherent limitations as a platform. With the right design, such NICs can meet all of our requirements, but no such design currently exists.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Manycore Designs</head><p>Figure <ref type="figure" target="#fig_1">1b</ref> illustrates a manycore programmable NIC <ref type="bibr">design [24, 53, 72, 73]</ref>. These designs implement network offloads by parallelizing flow processing across a large number of embedded processors that are arranged into a multihop on-chip tiled topology. Some manycore NICs additionally contain hardware engines for cryptography and compression <ref type="bibr" target="#b66">[72,</ref><ref type="bibr">58]</ref>. This supports chaining and a variety of different offloads, but performance and isolation are poor. Performance: Manycore NICs use an embedded CPU core to orchestrate the processing of a packet across offloaded functions <ref type="bibr" target="#b30">[34]</ref>. This is because the on-chip network cannot parse complex packet headers to determine the appropriate on-NIC addresses for the packet's destination. As a result of this design choice, manycore NICs have throughput and latency problems that prevent high-performance chaining. Further, manycore NICs even struggle to drive 100 Gbps and faster line-rates <ref type="bibr" target="#b28">[32]</ref>. Because a single embedded processor is not enough to saturate line-rate, manycore NICs require packet load-balancing and buffering to scale performance.</p><p>Manycore NICs struggle to provide high-throughput chaining because manycore interconnects typically only provide enough throughput for a received packet to be sent to one embedded core before being sent via DMA to main memory. As applications become complex, state and caching limitations can require that different offloads be implemented as microservices distributed across cores instead of parallel monoliths. Current manycore NIC designs are not able to provide high performance for such a usecase.</p><p>Similarly, involving a CPU in a manycore NIC adds significant packet processing latency that otherwise could be avoided for packets that only need to be processed by a hardware accelerator. For example, Firestone et al. <ref type="bibr" target="#b28">[32]</ref> report that processing a packet in one of the cores on a manycore NIC adds a latency of 10 µs or more. Multi-Tenant Isolation: Because manycore NICs must buffer packets and load-balance them across parallel embedded cores <ref type="bibr" target="#b44">[48]</ref>, the extent to which tenants are isolated is determined by how buffer resources are managed, and how packets are load balanced. Unfortunately, existing manycore NIC designs do not provide explicit control over packet scheduling and buffering <ref type="bibr" target="#b44">[48]</ref>. They use FIFO packet queuing and droptail buffer management; without any other isolation mechanisms, this can lead to HOL blocking, and drop-tail packet buffers can allow one tenant to unfairly consume buffers.</p><p>However, some level of isolation is possible in manycore NICs by <ref type="bibr" target="#b0">(1)</ref> statically partitioning CPU resources across different tenants <ref type="bibr" target="#b44">[48]</ref>, which is inefficient, and (2) then using NIC-provided SDN mechanisms for steering tenants' flows to different cores. Additionally, some NICs such as the Broadcom Stingray allow running an OS to provide software-based isolation through a Linux operating system <ref type="bibr" target="#b20">[22]</ref>, but this can exacerbate the NICs' performance issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Reconfigurable Match+Action (P4) Designs</head><p>Figure <ref type="figure" target="#fig_1">1c</ref> shows an RMT NIC design; these are built using an ASIC substrate with a programmable match+action (RMT) pipeline <ref type="bibr" target="#b38">[42,</ref><ref type="bibr" target="#b54">60]</ref>. In this model, incoming packets are first parsed by a programmable parser and then sent through a pipeline of M+A tables. Unfortunately, RMT NICs cannot support many interesting offloads (e.g., compression, encryption, or any offload that must wait on the completion of a DMA from main memory) because the actions that are possible at each stage of the pipeline are limited to relatively simple atoms that can execute within 1-2 clock cycles <ref type="bibr" target="#b61">[67,</ref><ref type="bibr" target="#b54">60,</ref><ref type="bibr" target="#b19">21]</ref>. However, RMT NICs do not suffer from multi-tenant performance isolation problems because each offload runs at line-rate with extremely low latency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PANIC Overview</head><p>PANIC is a new programmable NIC design that meets the aforementioned requirements (Section 2.1). The core idea be- hind the design of PANIC is that programmable NICs should be implemented as four logical components (Figure <ref type="figure" target="#fig_2">2</ref>): 1) A programmable RMT pipeline, which provides programmable offload chaining on a per-packet basis; 2) A switching fabric, which interconnects all other components in PANIC and enables dynamic chaining at line-rate; 3) A central scheduler, which achieves high-performance packet scheduling, traffic prioritization and traffic isolation; 4) Compute units, each of them running a single offload. This system architecture is shown in Figure <ref type="figure" target="#fig_2">2</ref>. We show that this design is suitable for both ASIC and FPGA implementations. Operational overview: Figure <ref type="figure" target="#fig_2">2</ref> illustrates how PANIC operates when packets are received. In this example, there are three compute units 1, 2, and 3 running services A, B, and C respectively. When packets are received in PANIC in Step 1, they are first processed by the RMT pipeline. The RMT pipeline parses the packet headers and matches on them to identify the chain of offloads that the packet should be forwarded to, and then it generates a PANIC descriptor that contains this offload chain information. In this example, the offload chain that is found will first send the packet to service B and then to service A.</p><p>Next, the packet is injected into the switching fabric. If the packet does not need to be processed by any offloads, it will be forwarded straight to the DMA engine of the NIC, which is connected to the interconnect in the same manner as all of the compute units used to implement offloads. Otherwise, it is sent to the central packet scheduler (Step 2).</p><p>The scheduler then buffers the packet and orchestrates scheduling and load-balancing the request across its offload chain. When there is no load, packets are chained with a source route that takes them from offload to offload without stopping at the packet scheduler. In this example, the scheduler first buffers this packet until Unit 2 is idle. Then, in Step 3, it steers this packet to Unit 2, and, in Step 4, the packet is directly pushed to Unit 1. Finally, in Step 5, after Unit 1 finishes the computation of service A, the source route steers this packet to the DMA engine, which is responsible for transferring packets over the PCIe bus into main memory on the host.</p><p>When load is high (not shown), the loaded unit (say Unit 1) detours a packet that was pushed to it (by Unit 2) off to the buffer in the central scheduler. From there, the packet can be pulled later for processing by either Unit 1 when it has finished processing a packet or by another parallel unit running the same logic as Unit 1 entirely.</p><p>Transmitting packets is similar to receiving packets in reverse, except that the main CPU can associate offload chains with transmit queues beforehand so that the RMT pipeline does not need to process packets before they can be sent to offloads. After the main CPU enqueues packet descriptors, they will be read by the DMA engine, forwarded through an offload chain and managed by the central packet buffer as needed, and then forwarded to the appropriate Ethernet MAC.</p><p>PANIC makes it possible to meet all of our requirements: 1. Offload Variety: Each offload in PANIC is an independent tile attached to the high-performance interconnect, and the RMT pipeline builds the packet headers necessary to enable hardware offloads to process packet streams without any additional routing or packet handling logic. This allows for a large variety of different types of computation to be performed by the offload engine, including hardware IP cores, embedded processors, and even embedded FPGAs <ref type="bibr" target="#b68">[74]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Dynamic Offload Chaining:</head><p>Installing a new chain in the RMT pipeline for received packets involves programming lookup tables, and installing a new chain for a transmit queue can be done by issuing MMIO writes from the main CPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Policies for Dynamic Multi-Tenant Isolation:</head><p>Performance isolation is provided by the central packet scheduler, which performs packet scheduling across the packets buffered for groups of parallel offloads that provide the same service. The scheduling algorithm determines both how chains competing for a service are isolated and how chains share packet buffers. Similar to prior work <ref type="bibr" target="#b62">[68,</ref><ref type="bibr" target="#b64">70]</ref>, packet scheduling policies in PANIC are programmable. Further, PANIC improves upon prior work by also providing policy-aware packet dropping to enable cross-tenant memory isolation.</p><p>PANIC supports any scheduling algorithm that can be implemented by assigning an integer priority to a packet, and this includes a wide range of different policies, including strict priority, weighted fair queuing (WFQ), least slack time first (LSTF), and leaky bucket rate limiting <ref type="bibr" target="#b51">[56,</ref><ref type="bibr" target="#b62">68]</ref>. Although strict priorities lead to starvation, this is intended-if there is enough mission-critical traffic to consume all available resources, then it is acceptable for competing best-effort traffic to starve. If starvation is undesirable, it can be avoided by using WFQ and rate-limiting. PANIC can support a hierarchical composition of different scheduling algorithms, e.g., fair sharing across tenants with prioritization of flows for each tenant, although this comes with additional hardware costs. More complex scheduling algorithms are also possible in PANIC because priorities for </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Design</head><p>This section discusses the design of the individual components of PANIC in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">RMT Pipeline</head><p>The RMT pipeline in PANIC is used to provide programmable chaining and to look up scheduling metadata as part of providing programmable scheduling. The design of the RMT pipelines is borrowed from the design used in programmable switches <ref type="bibr" target="#b19">[21,</ref><ref type="bibr" target="#b61">67]</ref>. When a packet is received by the NIC, the RMT pipeline first parses incoming packets and then processes them with a sequence of match-action tables (MA tables). Each MA table matches specified packet header fields and then performs a sequence of actions to modify or forward the packet. Via these actions, the RMT pipeline 1) performs simple, line-rate packet processing (e.g., IP checksum calculation) and 2) generates a PANIC descriptor for each packet that contains the appropriate chaining and scheduling metadata given the configuration that was programmed by the operator/user. Additionally, the RMT pipeline can maintain state on a per-traffic-class or per-flow basis if needed to support programmable scheduling or flow affinity. Figure <ref type="figure">3</ref> shows the PANIC descriptor added by the RMT pipeline. It includes the packet length, the allocated buffer address, and the service chain for this packet, which is a list of services to send the packet to along with per-service metadata from the RMT. Because multiple compute units may implement the same service (offload), this means that the RMT pipeline does not specify the exact unit a packet will be sent to in advance. This enables the scheduler to perform dynamic load balancing across multiple computation units implementing the same service in parallel.</p><p>In addition to specifying a list of services, the offload chain also contains metadata. For example, per-hop scheduling metadata like traffic class and priority allows a chain to have different priorities and weights across different services. Similarly, the descriptor may also contain service-specific metadata to allow the RMT pipeline to perform pre-processing to speed-up or simplify different compute units. Examples of this type of metadata include pointers to fields in a parsed packet and a pre-computed hash of an IP address.</p><p>The RMT pipeline directly connects to the switching fabric. To ensure low latency, the pipeline directly steers packets that are not processed by any service to the DMA engine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">High Performance Interconnect</head><p>To enable dynamic service chaining, PANIC use an on-chip interconnect network to switch packets, providing high-speed communication between the scheduler and on-NIC units.</p><p>Because it is necessary to forward packets between offloads at line-rate, it is important to build a high-performance network. PANIC utilizes a non-blocking, low latency, and high throughput crossbar interconnect network, which, for the scale of our design, still has a low area and power overhead. The crossbar can be configured to connect any input node to any output node with no intermediate stages, and each port runs at line-rate. As a result, every offload can simultaneously send and receive at line-rate, which enables line-rate dynamic chaining regardless of which offloads a chain uses.</p><p>Although economical in small configurations, crossbar interconnects unfortunately do not scale well with an increase in the number of cores. Most of these problems arise from the delay and area cost associated with long interior wires because the number of these wires increases significantly with the number of cores. Fortunately, with PANIC, we are able to choose between different interconnect topologies without having to change other parts of the design. If there is a need to scale beyond the limits of a single crossbar, we can switch to a more scalable (but higher-latency) flattened butterfly topology <ref type="bibr" target="#b39">[43]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Centralized Scheduler</head><p>The centralized scheduler buffers packets, schedules the order in which competing packets are processed by a service, and load-balances packets across the different compute units in a service. The scheduler architecture is shown in Figure <ref type="figure" target="#fig_3">4</ref>. The scheduler uses a new hybrid scheduling algorithm to support low-latency chaining while avoiding load imbalance, and it uses a new hardware-based priority queue (i.e., PIFO <ref type="bibr" target="#b62">[68]</ref>) to schedule and drop packets according to a programmable inter-tenant isolation policy.</p><p>An overview of the operation of the central scheduler is as follows: When a packet and its descriptor arrive, the scheduler writes the packet data into high-speed on-chip memory and stores the packet descriptor into the appropriate logical PIFO queue given the next destination service of this packet. Each logical PIFO queue corresponds to a service and sorts buffered descriptors by rank, which enables the scheduler to drop packets according to the same policy as they are scheduled by dropping the lowest-rank packet currently enqueued for the service if needed. Then, whenever any of the parallel compute units for a service have available "credits" at the scheduler, the credit manager (Figure <ref type="figure" target="#fig_3">4</ref>) chooses the compute unit with most credits, dequeues the head element of the corresponding logical PIFO queue, and sends the packet data and descriptor along with the packet data across the on-chip interconnect to the chosen unit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Hybrid Push/Pull Scheduling and Load Balancing</head><p>When one service cannot achieve line-rate with a single unit, PANIC uses multiple parallel units to improve bandwidth. To support load-balancing across variable performance offloads, PANIC provides load-aware steering. Specifically, PANIC introduces a new hybrid pull/push scheduler and load balancer that overcomes the limitations of either push or pull scheduling to provide both precise request scheduling and high utilization.</p><p>Pull-based scheduling provides the most precise control over scheduling because decisions are delayed until each unit is able to perform work. However, pull-based scheduling can lead to utilization inefficiencies because each unit must wait for a pull to complete before it can start work on a new packet, and busy-polling can lead to increased interconnect load. In contrast, push-based scheduling can lead to loadimbalance and increased tail latencies when packets have variable processing times. In this scenario, it is not possible to know how much work is enqueued at each unit at the time that load-balancing decisions must be made.</p><p>The hybrid scheduler used in PANIC provides the best properties of both pull and push scheduling. In this scheduler, during periods of high load, the central scheduler uses pullbased load balancing to provide effective load balancing and packet scheduling. During low load, the scheduler pushes packets to all of the units in a service chain with low latency. To accomplish this, the scheduler uses credits to monitor the load at different units. Next, we describe the two modes of operation, pull and push, and the use of credits. Credit Management: The credit manager tracks credits to measure load and dynamically switch between push-based and pull-based scheduling. The credit manager initially stores C credits for each compute unit. After sending a packet out, it decreases the credit number for that unit by one. When a compute unit is done processing a packet, it returns credit back to the credit manager.</p><p>The central scheduler operates in push mode as long as any of the parallel compute units in a service have credits available. If flow affinity is not needed, the scheduler steers packets to the unit which has the maximum number of pull credits to avoid load imbalance.</p><p>In contrast, if no unit has credit when a packet arrives, the scheduler buffers packets until credit is available. In this case, the central scheduler provides pull scheduling. Because the decision on which replica to use is made lazily, the number of packets queued at each unit will never exceed C.</p><p>By default, the number of initial credits C is set to two to avoid a stop-and-wait problem. However, it is possible to configure different credit numbers for each unit if needed. For example, ClickNP <ref type="bibr" target="#b43">[47]</ref> uses a SHA1 engine that can process 64 packets in parallel, and PANIC can support this level of parallelism by giving 64 or more credits to the SHA1 engine. Push-based Chaining: Push scheduling provides lowlatency offload chaining. When a packet needs to traverse multiple offloads (e.g., from A to B to C), the packet will be directly pushed to B when it finishes the computation in A rather than going back to the central scheduler. If B accepts the pushed packets, it will send a cancel message to the central scheduler to decrease its credit by one. In the case that there are multiple parallel units providing a service, the push destination is precalculated in the central scheduler. By pushing the packet directly to the next destination unit, PANIC reduces interconnect traversal latency and reduces on-chip network bandwidth demands. Furthermore, this reduces the load on the central scheduler as chain lengths grow. Detour Routing: Push mode chaining may cause a packet to be pushed to a busy downstream unit that has no buffer space to accept packets. In this case, we use detour routing: when local buffer is occupied, the downstream unit redirects the packet back to the central scheduler, where it is buffered until it can be scheduled to another idle unit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Packet Scheduling</head><p>To achieve priority scheduling and performance isolation, every packet stored in the packet buffer has its descriptor enqueued in a PIFO <ref type="bibr" target="#b62">[68]</ref>  scheduler to provide both performance and buffer isolation across tenants. Isolation Policy and Rank Computation: When a packet arrives, the central scheduler uses stateful atoms (ALUs) <ref type="bibr" target="#b61">[67]</ref> to take metadata about the packet, look it up in the RMT pipeline, and compute an integer priority that is used to enqueue a descriptor for the packet into a PIFO block. This enables PANIC to provide multi-tenant isolation, ensuring traffic from high-priority tenants has low latency. Additionally, if multiple PIFO blocks are used inside the scheduler, it is possible to support hierarchical policies. Because the onchip network ports are bidirectional, there is enough network throughput to forward incoming packets back out regardless of which logical queue the packets use. Prioritized Dropping: PANIC's PIFO scheduler performs prioritized packet dropping. Specifically, PANIC ensures that when the NIC is overloaded, the lowest priority packets will be dropped. To achieve prioritized dropping, PANIC reuses the priority-sorted descriptor queue already used for scheduling in the PIFO. When the free space in the packet buffer for a logical PIFO is smaller than a threshold, the scheduler will remove the least-priority descriptor from the logical PIFO and drop this packet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Performance Provisioning:</head><p>It is important to ensure that the central scheduler does not become a performance bottleneck and can forward packets across chains at full line-rate. To ensure that the scheduler has sufficient throughput, PANIC uses multiple ports to attach the scheduler to the on-chip network. Because the switching fabric is designed to forward between arbitrary ports at linerate, increasing the number of ports used by the scheduler is sufficient to scale the network performance of the scheduler.</p><p>The speed of the PIFO block used to schedule packets can also become a performance bottleneck. The PIFO block that we use can schedule one packet per cycle, e.g., 1000Mpps when operating at a 1GHz frequency when implemented in an ASIC design. Although this is sufficient to schedule packets in both transmit and receive directions in our current design, in the case that this number is greater than the performance of a single PIFO block, multiple parallel PIFO blocks need to be used to scale up performance. Provisioning for Compute Unit Performance: The design of the on-chip network and the scheduler can also ensure that offloads may be fully utilized despite complications from chaining. Specifically, when an offload O1 in a chain (Chain A=O1-O2-O3) runs at a slower rate than the rest of the offloads (O2-O3), it will become a bottleneck and cause O2-O3 to be not fully utilized. However, this does not lead to resource stranding. A second chain B that does not use O1 can still use O2-O3 and benefit from the remaining capacity of these offloads. The scheduler can ensure that the contending chains fairly share capacity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Compute Unit</head><p>To support offload variety, PANIC utilizes compute units to attach offloads to the switching fabric. These compute units are self-contained, meaning that hardware offloads can be designed without needing to understand the packet switching fabric and without having to issue pull requests to the hybrid scheduler. The interfacing with the switching fabric is handled by the traffic manager (Figures <ref type="figure">5 and 6</ref>). This both reduces offload complexity by avoiding duplicating packet processing functionality and reduces packet processing latency by avoiding incurring the overheads of processing a packet with a CPU.</p><p>An offload engine in PANIC can either be a hardware accelerator or a core, and Figure <ref type="figure">5</ref> and Figure <ref type="figure">6</ref> presents the design of an ASIC accelerator-based and CPU-based compute unit in PANIC, respectively. In both of these designs, the offload functionality is encapsulated as an offload engine. Both perform packet processing by reading a packet once it arrives from the network and has been placed in a local scratchpad buffer. The traffic manager is responsible for communicating with the switching fabric. This component includes logic for sending and receiving packets as well as logic for updating PANIC descriptors as needed for push-based chaining. The compute unit's local credit manager interfaces with the central scheduler and is responsible for returning credits (when a packet's processing is done) and sending cancel messages that decrement credits (when accepting a pushed packet).</p><p>The primary difference between an accelerator-based design and CPU-based design is that there is additional logic in the CPU-based design that is used to interface with the memory subsystem of the embedded CPU core. As Figure <ref type="figure">6</ref> shows, the compute unit utilizes memory-mapped I/O (MMIO) to connect an embedded CPU core as follows: 1) The traffic manager (TM) writes network data directly to a pinned region of the per-core memory. 2) Then the TM writes to an input doorbell register to notify the core that data is ready.</p><p>3) After the core finishes processing, it writes data back to the pinned memory region if needed and then writes to an output doorbell register that is used to notify the TM of a new outgoing packet. To make sure the packet data is written back to memory, the core needs to flush the cache lines for the pinned memory region. 4) The TM collects the output data and sends it back to the switching fabric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">ASIC Analysis</head><p>We expect an eventual implementation of PANIC to use an application-specific integrated circuit (ASIC), although we have also prototyped PANIC in the context of an FPGA platform for expediency. This is because relative to an FPGA, an ASIC provides higher performance, consumes less power and area, and is cheaper when produced in large volumes <ref type="bibr" target="#b40">[44]</ref>. While an ASIC implementation is beyond the scope of this work, in this section we briefly discuss the feasibility of implementing different components of PANIC in an ASIC. RMT: The implementation of an RMT pipeline in ASIC has already been proven feasible <ref type="bibr" target="#b19">[21]</ref>. The Barefoot Tofino chip <ref type="bibr" target="#b14">[16]</ref> is a concrete realization of the RMT architecture. PIFO: PANIC uses a hardware priority queue to provide programmable scheduling. Our current design was borrowed from the ASIC-based flow scheduler design of the PIFO paper <ref type="bibr" target="#b62">[68]</ref>. While this design is conceptually simple and easy to implement because it maintains a priority queue as a sorted array, it is less scalable relative to other priority queue designs, e.g., PIEO <ref type="bibr" target="#b58">[64]</ref>, which uses two levels of memory or pHeap <ref type="bibr" target="#b18">[20]</ref>, which uses a pipelined heap. For better scalability, we can replace our current design with such scalable hardware designs of priority queues at the expense of greater design and verification effort. Interconnect: One of the biggest potential scalability limitations of a PANIC implementation is the on-chip switching fabric. While crossbar interconnects are conceptually simple, the sheer number of wires in a crossbar might become a physical design and routing bottleneck, causing both an increase in area as well as an inability to meet timing beyond a certain scale. Fortunately, prior work has already demonstrated that it is feasible to build crossbars on an ASIC that are larger than are needed in PANIC. Specifically, Chole et al. built a 32 * 32 crossbar with a bit width of 640 bits <ref type="bibr">[28,</ref> Appendix C] at a 1 GHz clock rate. As another data point, the Swizzle-Switch supports a 64 * 64 crossbar with a bit width of 128 bits using specialized circuit design techniques at 559 MHz in a relatively old 45 nm technology node <ref type="bibr" target="#b57">[63]</ref>. For comparison, to provide 32 compute units each a 128 Gbps connection to the switching interconnect, PANIC only needs a 32 * 32 crossbar with a bit width of 128 bits and 256 bits at 1 GHz and 500 MHz clock frequencies, respectively.</p><p>At the same time, we anticipate a crossbar becoming no longer viable at some point as the number of offloads continues to increase. At this point, we anticipate switching to other more scalable topologies such as a flattened butterfly at the cost of increased latency and reduced bisection bandwidth. Compute Units: The PANIC offload engine can be a hardware accelerator or a CPU core. There are several ASIC-based RISC-V implementations which can used as a CPU core for the offload engine <ref type="bibr" target="#b21">[25,</ref><ref type="bibr" target="#b22">26,</ref><ref type="bibr" target="#b7">8]</ref>. Several functions important to networking, such as compression, encryption, and checksums are available as hardware accelerators, which can be reused for PANIC. Our own AES and SHA implementations (Section 6) are based on open-source hardware accelerator blocks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">FPGA Prototype</head><p>We implement an FPGA prototype of PANIC in ∼6K lines of Verilog code, including a single-stage RMT pipeline, the central scheduler, the crossbar, the packet buffer, and compute units. Also, we built a NIC driver, DMA Engine, Ethernet MAC, and physical layer (PHY) using Corundum <ref type="bibr" target="#b29">[33]</ref>. Although the PANIC architecture supports both the sending path and receiving path, in our current implementation, we mainly focus on the receive path. RMT pipeline: We implemented a single-stage RMT pipeline in our FPGA prototype. We configure the RMT pipeline in our prototype by programming the FPGA. The RMT pipeline maintains a flow table, in which each flow is assigned an offload chain and scheduling metadata. In the match stage, the RMT module matches the flow table with the IP address fields and port fields in the packet header. In the action stage, the RMT module calculates scheduling metadata and generates the PANIC descriptor (Figure <ref type="figure">3</ref>). The frequency of the RMT pipeline is 250 MHz. FPGA-based Crossbar: We have implemented an 8 * 8 fully connected crossbar in our FPGA prototype. The frequency for this crossbar is 250 MHz, and the data width is 512 bits. This leads to a per-port throughput of 128 Gbps. Central Scheduler and Packet Buffer: The architecture of the scheduler is shown in Figure <ref type="figure" target="#fig_3">4</ref>. The scheduler is connected with two crossbar ports to ensure a sufficiently high throughput connection to the on-chip network. In our implementation, the PIFO block runs at 125 MHz frequency with a queue size of 256 packets; all other components in the scheduler run at 250 MHz, with a 512 bit data width, and we add a cross-domain clocking module between other components and the PIFO. We use lower frequency for the PIFO because it suffers from a scalability issue when implemented on the FPGA (we explain this further in Section 7.5). The packet buffer is implemented with dual-channel highspeed BRAM, where each BRAM channel supports concurrent reads and writes. The packet buffer size in our implementation is 256 KB with a 512 bit data width and a 250 MHz frequency. For ease of implementation, our current prototype uses a separate physical PIFO for each logical PIFO at the cost of increasing the relative resource consumption of PIFOs. Compute Units: As Figure <ref type="figure">5</ref> shows, our implementation of a compute unit includes a traffic manager, a credit manager, and a scratchpad packet buffer. We choose the AXI4-Stream interface <ref type="bibr" target="#b1">[2]</ref> as the common interface between the offload engine and scratchpad buffer. We have included two types of accelerator-based offload engines in our FPGA prototype. One is the AES-256-CTR encryption engine <ref type="bibr" target="#b0">[1]</ref>, and the other is the SHA-3-512 hash engine <ref type="bibr" target="#b6">[7]</ref>.</p><p>We have also implemented a RISC-V core engine based on the open-source CPU core generator <ref type="bibr">[9]</ref>. Figure <ref type="figure">6</ref> shows how the RISC-V core is connected to PANIC's traffic manager, credit manager, and per-core memory. The RV32I RISC-V core we use has a five-stage pipeline with a single level of cache. The data cache and instruction cache are 2 KB each, and the local memory size is 32 KB. The frequency for this CPU is 250 MHz, and the per-core memory data width is 512 bits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Evaluation</head><p>This section evaluates our FPGA prototype to show that it meets the design requirements put forth in Section 2.1. In Section 7.2, we use microbenchmarks to show that PANIC achieves high throughput and low latency under different offload chaining models. In Section 7.3, we compare PANIC's performance with a pipeline-of-offloads NIC. Section 7.4 measures the I/O performance of a RISC-V core in PANIC, and Section 7.5 measures the hardware resource usage of our FPGA prototype. Finally, in Section 7.6, we implement different offload engines, and test PANIC end-to-end; these results demonstrate that PANIC can isolate and prioritize traffic efficiently under multi-tenant settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Testbed and methodology</head><p>For our microbenchmarks, we implemented our FPGA prototype in the ADM-PCIE-9V3 network accelerator <ref type="bibr" target="#b10">[12]</ref>, which contains a Xilinx Virtex UltraScale Plus VU3P-2 FPGA. For this evaluation, we also implemented a delay unit, a packet generator, and a packet capture agent on the FPGA. The delay unit emulates various compute units by delaying packets in a programmable fashion, which allows us to flexibly control per-packet service time and chaining models. This enables us to run microbenchmarks that systematically study PANIC's performance limits. The packet generator generates traffic of various packet sizes at different rates. The packet capture agent receives packets and calculates different flows' throughput and latency. We calculate packet processing latency by embedding a send timestamp in every generated packet.</p><p>For our end-to-end experiments, we evaluate PANIC in a small testbed of 2 Dell PowerEdge R640 servers. One server is equipped with a Mellanox Connectx-5 NIC, and the other server is equipped with the ADM-PCIE-9V3 network accelerator carrying our PANIC prototype. The Mellanox NIC and ADM-PCIE-9V3 card are directly connected. We program the VU3P-2 FPGA on the network accelerator using our Verilog implementation of PANIC. We use DPDK to send customized network packets and use PANIC to receive packets and run offloads. Because of a performance bottleneck in the kernelbased FPGA NIC driver <ref type="bibr" target="#b29">[33]</ref>, we use the packet capture agent on the FPGA to report PANIC's receive throughput instead of capturing packets on the host machine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">PANIC System Microbenchmarks</head><p>We microbenchmark PANIC's performance using the different chaining models shown in Figure <ref type="figure">7</ref>. Our results demonstrate that PANIC can both achieve high throughput and low latency for various common offload chaining models. Model 1 ("Pipelined Chain"): In model 1 (Figure <ref type="figure">7a</ref>), we attach N delay units in sequence. Each unit emulates a different service, and all of them process packets at X Gbps with fixed delay. We then configure a service chain that sends packets through all N units in numerical order.</p><p>First, we measure the throughput and latency overhead of PANIC when N = 3 and X = 100. Figure <ref type="figure">8a</ref> shows the overall throughput under different packet sizes. With MTUsized packets, PANIC can schedule packets at full line-rate. When the initial number of credits in PANIC is small, we see a nonlinear performance downgrade with small packets. This is because throughput for small packets is bounded by the scheduling round trip time in PANIC, which is 14 clock cycles. If we increase the initial credit number for each unit, we see a performance increase for small packets. When the credit number is greater than 8, the small packet performance is no longer bounded by the scheduling round trip, instead, it is bounded by the small packet performance of our delay unit. These results also demonstrate the benefits of PANIC's flexibility in per-unit credit allocation. Setting different credit numbers for each unit can improve performance.</p><p>Next, Figure <ref type="figure">8b</ref> shows the latency of different packet sizes in the same experiment. In this pipelined chain, packets can be scheduled through three units within 0.5 microseconds. This low latency performance also arises from PANIC's push scheduling which helps PANIC avoid extra packet traversals between units and the scheduler.</p><p>Next, Figure <ref type="figure">9</ref> shows PANIC's throughput as a function of the chain length when push scheduling is disabled. Without push scheduling, the packet needs to go back to the scheduler at every hop, and the total traffic that goes into the scheduler is the ingress traffic from the RMT pipeline plus the detoured traffic from units, which is: <ref type="figure">9</ref> shows, when T total exceeds the dual-ported scheduler's maximum bandwidth (250 MHz * 512 bits * 2 ports = 256 Gbps), the chaining throughput downgrades. For example, when N = 3, X = 70, T total = 210 Gbps &lt; 256 Gbps, thus PANIC can schedule this chain at full speed even when push scheduling is disabled. When N = 3, X = 90, T total = 270 Gbps &gt; 256 Gbps, the chaining throughput downgrades since the scheduler bandwidth becomes the bottleneck. Model 2 ("Parallelized Chain"): In model 2 (Figure <ref type="figure">7b</ref>), we attach three delay units running in parallel. These three units run the same service, and each unit has an average 34 Gbps throughput but variable latency. PANIC load-balances packets across these units. Figure <ref type="figure">8c</ref> shows the throughput under different packet sizes and service time variance (the service time follows uniform distribution). We see that even when the processing latency variance increases from 20% to 40%, PANIC can still efficiently load-balance packets between the parallel units without impacting throughput. In this experiment, small packet performance is better than model 1 because model 2 has multiple units running in parallel. Overall throughput is no longer bounded by the delay of a single unit. Figure <ref type="figure">8d</ref> shows PANIC scheduling latency under different loads with MTU sized packets and 40% service time variance. The error bars in this figure represent 5%-ile and 99%-ile latency. Scheduling latency reveals how long the incoming packets wait before being processed by an idle unit; we calculate it by subtracting the unit processing time from the total latency. When the NIC load is much smaller than 1, scheduling tail latency grows slowly, and is under 0.4 µs. When the load approaches 1, queueing occurs in the packet buffer, which causes tail latency to grow, but it still stays &lt; 0.8 µs. This shows that our credit-based scheme keeps latency low even at high load, and most latency is due to queueing. Model 3 ("Hybrid Chain"): Model 3 (Figure <ref type="figure">7c</ref>) is a hybrid chaining model where packets are not only load-balanced between parallel units but also go through multiple services.  Packets need to be processed first by service A and then by service B, and both services have multiple parallel compute units. Each compute unit for service A has an average throughput of 52 Gbps, while each compute unit for service B has an average throughput of 50 Gbps. Compute units for both service A and B have variable latency.</p><formula xml:id="formula_0">T total = T RMT + T detour = T RMT + (N − 1) * X = N * X. As Figure</formula><p>Figure <ref type="figure">8e</ref> shows the throughput and detour rate in this hybrid model. When the packet size is bigger than 256 bytes, the detour rate is high. This is because the downstream B units have lower throughput than the upstream A units. As a result, the B units are always busy because they are the throughput bottleneck in this system. Busy units are likely to have no space to accept pushed packets: if A unit tries to push packets to a busy downstream B unit, then the B units will more often than not detour the pushed packets back to the central scheduler.</p><p>Figure <ref type="figure">8e</ref> also shows that detour routing does not degrade throughput. This is because the maximum bandwidth of our dual-ported scheduler is 256 Gbps, and in this hybrid model, the ingress traffic from the RMT pipeline will take up 100 Gbps bandwidth in the scheduler, thus there is more than 100 Gbps bandwidth left for the detoured traffic.</p><p>However, detour routing can increase packet latency. In order to mitigate this, the central scheduler increases the priority for each detoured packet, to help them get rescheduled first. Thus, the latency incurred by detoured packets is the RTT between the compute unit and the scheduler, which is &lt; 0.5 µs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>USENIX Association</head><p>14th USENIX Symposium on Operating Systems Design and Implementation 253</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Comparison with the Pipeline Design</head><p>To demonstrate that PANIC handles multi-tenant isolation and below line-rate offloads better than state-of-the-art, we build and compared against the pipeline-of-offloads NIC as our baseline. We choose model 4 (Figure <ref type="figure">7c</ref>) as the offload chain for this comparison. The difference between model 4 and model 1 is that the delay unit emulates a below-line-rate offload in model 4. We assume two flows are competing: Flow 1 has a higher priority, and takes up 30% of the total traffic. Flow 2 has a lower priority, and takes up 70% of the total traffic. We implemented a pipeline-of-offloads NIC in the ADM-PCIE-9V3 network accelerator. In this NIC, all incoming packets are first buffered in a FIFO (First-In-First-Out) queue before entering unit A. Unit A and unit B are directly connected using the AXI4-stream interface <ref type="bibr" target="#b1">[2]</ref>. We configured the pipeline-of-offloads NIC and PANIC to have the same buffer size (64 KB), same frequency (250 MHz), and same bit-width (512 bits).</p><p>Figure <ref type="figure">8f</ref> presents a comparison of the latency of both the pipeline design and PANIC in this experiment. When the NIC load is low, Unit A is not the bottleneck, and both NICs have low latency. The pipeline design has slightly better latency since units are directly connected in it, while scheduling packets in PANIC has some overhead. When load increases, Unit A becomes the bottleneck, and both NICs start to buffer and drop packets. With high load, Flows 1 and 2 have the same latency in the pipeline design, since packets are scheduled in First-Come-First-Served order and can experience HOL blocking. In PANIC, the high priority packets have fixed low latency due to the central scheduler sorting buffered packet descriptors and serving high priority packets first.</p><p>We compare the throughput between the pipeline design and PANIC in Table <ref type="table" target="#tab_5">3</ref>. The total throughput is bounded by Unit A (60 Gbps). In the pipeline design, the low priority flow 2 has a higher throughput than flow 1, because the highvolume flow 2 steals on-chip bandwidth by taking up most of the on-chip buffer. PANIC preferably allocates buffer to high priority packets and always drops the lowest priority ones. Thus flow 1 can always achieve full throughput in PANIC.</p><p>Overall, PANIC achieves good isolation: 1) PANIC achieves comparable throughput and latency with the pipeline design when there is no HOL blocking. 2) When HOL blocking occurs, PANIC ensures that the high priority flows have a fixed low latency. 3) PANIC allocates bandwidth according to a flow's priority.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">RISC-V Core Performance</head><p>To investigate the I/O overhead of using an embedded NIC core to send/receive network packets from PANIC, we performed experiments with a single RISC-V CPU core as the only offload engine in a chain. We measure the system throughput and per-packet latency using four example C programs: No-Touch: After receiving the packet from PANIC, this program will send the packet back to PANIC immediately. This program does not make any changes to the packet data. Memcpy: This program will copy the received packet to another memory address and then send the copied packet back to PANIC. NAT: The Network Address Translation (NAT) program uses the embedded CPU core to lookup a &lt;Translated IP, Port&gt;pair for a given 5-tuple, and then replace the IP address and port header fields using the lookup results. The lookup table is stored in the local memory inside the offload engine. The RMT pipeline will pre-calculate the hash value for each packet, and the hash value is stored as per-service metadata in the PANIC descriptor. Thus the CPU core can directly read the pre-calculated hash value from the descriptor. Swap OvS: This program swaps the Ethernet and IP source and destination addresses.</p><p>Figure <ref type="figure" target="#fig_6">10</ref> shows the RISC-V core throughput and perpacket latency with MTU sized packets. We breakdown the latency number into three different parts: 1) Network I/O: the time that is spent on pulling/writing the input/output doorbell register, 2) Flush Cache: the time spent on flushing the L1 cache, 3) Computation: the time spent on computation, including the data exchange time between the L1 cache and the per-core memory. The results of this experiment show that the overhead of the NIC to CPU core interface is low, and, for those low-throughput applications, the I/O time introduced by PANIC is negligible.</p><p>For example, the throughput of the No-Touch program is 61.4 Gbps, and all the time is spent in network I/O. The throughput of the NAT and Swap OvS programs is 21.3 Gbps and 20.2 Gbps, respectively. ∼20% of the time is spent in flushing the cache, ∼27% in network I/O, and ∼50% in computation. Cache flushing is costly in our current prototype: to synchronize the data between the cache and memory, the whole L1 cache is flushed before processing the next packet. If needed, this performance could be improved by modifying the CPU to support an instruction that only flushes the cache lines for the pinned memory region used by the packet.</p><p>The throughput of Memcpy is only 1.2 Gbps, and 97% of the time is spent in computation. This is due to the limitations of the performance of the FPGA based RISC-V core. With a faster core and a higher clock frequency, this performance can be improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Hardware Resource Usage</head><p>Our UltraScale VU3P-2 FPGA has 3 MB BRAM, and 394k LUTs in total. The crossbar and PIFO occupy most of the on-chip logic resources in PANIC. When the crossbar uses 8 ports, it costs around 5.5% logic area, and for 16 ports, the logic area cost is 13.64%. When the total PIFO size is 256, it will cost 4.9% logic area, and when the size is 512, it will cost 9.42%. PIFO suffers from high logic area cost because its hardware design does not access BRAM at all; it only uses the logic unit to compare and shift elements. This design causes PIFO to be less scalable in the FPGA since it cannot benefit from the FPGA's memory hierarchy to efficiently distribute storage and processing across SRAM and LUTs. Recent advancements <ref type="bibr" target="#b58">[64]</ref> can be used to address this (Section 5). Overall, we find that PANIC can easily fit on any middle-end FPGA without utilization or timing issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.6">End-to-End Performance</head><p>In this section, we measure PANIC's end-to-end performance in our cluster. Because of the performance bottleneck of the kernel-based FPGA NIC driver, we use hardware counters to measure PANIC's receiving throughput. We implement two FPGA-based offload engines in PANIC: a SHA-3-512 engine and an AES-256 engine. Our end-to-end experiment demonstrates that: 1) PANIC can schedule network traffic at full line-rate, 2) PANIC can precisely prioritize traffic when different flows are competing for computation resources at the offloads, and 3) PANIC can support different isolation policies, including strict priority and weighted fair queueing.</p><p>The AES-256-CTR encryption engine <ref type="bibr" target="#b25">[29]</ref> encrypts input plain text into ciphered text or decrypts ciphered text to yield plain text. The fully pipelined AES-256 engine can accept 128-bit input per cycle, and it can run at 250 MHz frequency with 32 Gbps throughput. The SHA-3-512 engine <ref type="bibr" target="#b17">[19]</ref> performs SHA-3, a newest cryptographic hash which uses permutation as a building block <ref type="bibr" target="#b16">[18]</ref>. The FPGA-based SHA-3-512 engine that we use runs at 150 MHz with 6 Gbps throughput.</p><p>Since the throughput of a single SHA engine is low, we put 4 SHA engines into a single hash unit, and set the initial credit number for the hash unit to 4. Thus, the hash unit can use 4 SHA engines to process these packets in parallel. We connect two decryption units and two hash units with PANIC. Thus, the bandwidth of hash computation is (6 * 4) * 2 = 24 * 2 = 48 Gbps, and that for decryption is 32 * 2 = 64 Gbps.</p><p>In our experiment, we assume there are two types of traffic   <ref type="figure" target="#fig_1">11a</ref>.</p><p>competing for the computation resource in PANIC. One is high-volume multimedia traffic, which uses AES offload to decrypt video streams. Another is low-volume IPSec traffic, which first uses SHA to ensure the integrity of the data and then uses AES to decrypt IP payload. The IPSec traffic has higher priority than video stream traffic, and each of these traffic streams contains multiple flows. Also, we add background traffic that does not need to be processed by any compute unit.</p><p>The offload chains are shown in Figure <ref type="figure" target="#fig_2">12</ref>.</p><p>In the first experiment, we use the strict priority policy, which means all the IPSec packets have higher priority than the video packets. Figure <ref type="figure" target="#fig_1">11a</ref> shows different traffic's receiving throughput under different traffic patterns. In phase 1, the sending throughput is 30 Gbps for IPSec and is 50 Gbps for video. We can see the receiving throughput for IPSec is 30 Gbps, which is the same as the sending throughput. The receiving throughput for the video stream is only 34 Gbps. This is because IPSec and video stream share the AES offload. However, the available peak bandwidth for the AES offload is only 64 Gbps. Thus, PANIC will first satisfy the high priority IPSec traffic requirement, which only leaves 34 Gbps (64 -30) of bandwidth for the video stream. Table <ref type="table" target="#tab_8">5</ref> shows the dropping rate under phase 1. Due to prioritized packet dropping, PANIC only drops low priority video packets. Overall, when a below-line-rate offload becomes the bottleneck, PANIC always first satisfies high priority traffic's bandwidth demands.</p><p>In phase 2, the DPDK sender switches to the next traffic pattern, in which the IPSec traffic sending rate drops to 10 Gbps, and video traffic sending rate grows to 50 Gbps. Since the IPSec sending rate drops, the video stream can get more bandwidth, but it will still lose some bandwidth and experience packet drops because of the AES bottleneck. In phase 3, the DPDK sender switches to the last pattern, in which the AES offload is no longer the bottleneck; no packet drops occur, and the total throughput can reach 100 Gbps. Another noteworthy aspect in Figure <ref type="figure" target="#fig_1">11a</ref> is that no matter what computation happens, background traffic performance is unaffected.</p><p>In the second experiment, we use a weighted fair queueing (WFQ) scheduling policy where the AES offload's capacity is divided across IPSec traffic and video traffic in 2 : 1 ratio. Figure <ref type="figure" target="#fig_1">11b</ref> shows the throughput of the different traffic types under the WFQ policy for different traffic patterns. In phase 1, the sending throughput for IPSec is 70 Gbps, and for the video stream is 30 Gbps. We can see the receiving throughput for IPSec is exactly twice of the video stream, and the total throughput is 64 Gbps. If the IPSec sending rate drops to 50 Gbps (phase2), the receiving throughput remains unchanged. This result proves PANIC can shape the traffic USENIX Association 14th USENIX Symposium on Operating Systems Design and Implementation 255  In phases 1 and 2, the scheduler switches to pull-based scheduling since the AES offload is always congested. As a result, the egress packet of the SHA offload goes directly back to the scheduler instead of the congested AES offload. The scheduler then shapes the video traffic and the detoured IPSec traffic into a desired rate using WFQ.</p><p>In phase 3, the AES offload is no longer the bottleneck. Thus, the central scheduler operates in push mode: the egress packet of the SHA offload can bypass the scheduler and be directly pushed to the AES offload. As shown in Figure <ref type="figure" target="#fig_1">11b</ref>, both IPSec and video's receiving throughput can reach the sending rate, which is 30 Gbps. Overall, this shows that PANIC can shape the traffic precisely with the WFQ policy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Related Work</head><p>Several projects introduce new offloads that utilize programmable NICs and new frameworks for deploying these offloads <ref type="bibr" target="#b11">[13,</ref><ref type="bibr" target="#b44">48,</ref><ref type="bibr" target="#b53">59,</ref><ref type="bibr" target="#b38">42,</ref><ref type="bibr" target="#b28">32,</ref><ref type="bibr" target="#b33">37,</ref><ref type="bibr" target="#b45">49,</ref><ref type="bibr" target="#b59">65,</ref><ref type="bibr" target="#b42">46,</ref><ref type="bibr" target="#b56">62,</ref><ref type="bibr" target="#b43">47,</ref><ref type="bibr" target="#b36">40,</ref><ref type="bibr" target="#b26">30,</ref><ref type="bibr" target="#b32">36,</ref><ref type="bibr" target="#b64">70,</ref><ref type="bibr" target="#b63">69,</ref><ref type="bibr" target="#b31">35,</ref><ref type="bibr" target="#b50">55,</ref><ref type="bibr" target="#b41">45]</ref>. PANIC is orthogonal to these projects.</p><p>The Pensando DSC-100 NIC [58] is similar to PANIC in that it has an RMT pipeline and supports both hardware and software offloads. However, the DSC-100 requires cores to achieve offload chaining instead of a hardware scheduler.</p><p>The Fungible Data Processing Unit (DPU) is a NIC design that was recently announced in August 2020 <ref type="bibr" target="#b2">[3]</ref>. Based on publicly available documents <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, it has a hardware architecture that shares a few similarities with PANIC (e.g., processing cores, accelerators, a hardware work scheduler, and a customized on-chip network). A head-to-head compari-son of PANIC to the Fungible DPU would be an interesting avenue for future work once the DPU is generally available.</p><p>PANIC is also similar to FairNIC <ref type="bibr" target="#b30">[34]</ref>, which improves fairness between competing applications running on a commodity manycore NIC. However, PANIC provides features not possible in FairNIC like chaining without involving a CPU. Further, FairNIC helps motivate the need for PANIC detailing the non-trivial costs of isolation on manycore NICs. Adopting PANIC's scheduler and non-blocking crossbar interconnect can solve these fundamental problems with manycore NICs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusions</head><p>Programmable NICs are an enticing option for bridging the widening gap between network speeds and CPU performance in multi-tenant datacenters. But, existing designs fall short of supporting the rich and high-performance offload needs of co-resident applications. To address this need, we presented the design, implementation, and evaluation of PANIC, a new programmable NIC. PANIC synthesizes a variety of high-performance hardware blocks and data structures within a simple architecture, and couples them with novel scheduling and load balancing algorithms. Our analysis shows that PANIC is amenable to an ASIC design. We also built a 100G PANIC prototype on an FPGA, and conducted detailed experiments that show that PANIC can isolate tenants effectively, ensure high throughput and low latency, and support flexible and dynamic chaining.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>A NIC with RMT pipeline<ref type="bibr" target="#b38">[42]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustrations of existing programmable NIC architectures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: PANIC Architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Architecture of the multi-ported central scheduler</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Figure 5: Accelerator-Based Compute Unit Design</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>4 Figure 7 :Figure 8 :</head><label>478</label><figDesc>Figure 7: The different chaining models used in experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 :</head><label>10</label><figDesc>Figure 9: Model 1 throughput when push is disabled.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 :Figure 12 :</head><label>1112</label><figDesc>Figure 11: Receiving throughput with different traffic patterns. Figure a uses strict priority policy: all the IPSec packets have higher priority than the video packets. Figure b uses WFQ policy: the offload capacity is divided across IPSec traffic and video in the ratio 2:1. The table in Figure a and Figure b shows how the sending traffic pattern changes with time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-1.png" coords="1,-9.00,-10.01,630.00,255.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-2.png" coords="1,-9.00,543.00,630.00,259.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>A breakdown of the performance of different offloads when implemented in either hardware or software.</figDesc><table><row><cell>Tput</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Programmable NIC designs compared w.r.t. the requirements in Section 2.1.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>The central packet scheduler supports offloads that have variable performance. Packets for slow offloads will be buffered at the central scheduler. As loads shift, packet buffers can be dynamically allocated to different offload groups. PANIC's hybrid push/pull load balancing scheme outlined in the example above load-balances packets across parallel offloads, ensuring precise load control, low tail latency, and minimal and efficient on-chip network use. Similar to the packet scheduler, the load balancer is also programmable.</figDesc><table><row><cell>PK_LEN</cell><cell>BUF_ADDR</cell><cell cols="2">CHAIN_LEN</cell><cell>SERVICE_CHAIN</cell></row><row><cell>0</cell><cell>16</cell><cell>32</cell><cell>36</cell><cell>variable</cell></row><row><cell></cell><cell cols="4">Figure 3: PANIC Descriptor</cell></row><row><cell cols="5">later services in a chain can be dynamically computed by</cell></row><row><cell cols="5">an earlier chain stage. Similarly, each group of offloads that</cell></row><row><cell cols="5">form a service can have its own custom scheduling algorithm,</cell></row><row><cell cols="5">which is useful when different chains start with different</cell></row><row><cell cols="5">offloads and then converge and share the same service.</cell></row><row><cell cols="5">4. Support for offloads with variable and below line-rate</cell></row><row><cell>performance:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>USENIX Association14th USENIX Symposium on Operating Systems Design and Implementation 247 5. High Performance: PANIC uses an on-chip network inspired by network routers to provide a high-performance interconnect between different offload tiles and the tiles for DMA and the Ethernet MACs. PANIC uses non-blocking high-bisection topologies like the crossbar making it possible to guarantee line-rate performance even if every offload in a chain sends/receives at line-rate over the on-chip network.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Throughput of the pipeline design and PANIC.</figDesc><table><row><cell></cell><cell>Throughput</cell><cell>Total</cell></row><row><cell>Flow 1 (Pipeline Design) Flow 2 (Pipeline Design)</cell><cell>18.7 Gbps 41.9 Gbps</cell><cell>60.6 Gbps</cell></row><row><cell>Flow 1 (PANIC) Flow 2 (PANIC)</cell><cell>30.7 Gbps 29.1 Gbps</cell><cell>59.8 Gbps</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Table 4  shows different components' resource usage under different settings. In our end-to-end experiments (Section 7.6), the crossbar has 8 ports, total queue size in the PIFO array is 256 packets, and packet buffer size is 256 KB. Under this setting, we find that PANIC's design will only FPGA resource usage for different components.</figDesc><table><row><cell>Module</cell><cell>Setting</cell><cell>LUTs(%)</cell><cell>BRAM(%)</cell></row><row><cell>Crossbar</cell><cell>8 ports</cell><cell>5.5</cell><cell>0.00</cell></row><row><cell></cell><cell>16 ports</cell><cell>13.64</cell><cell>0.00</cell></row><row><cell>Scheduler (PIFO)</cell><cell>PIFO = 256</cell><cell>5.18 (4.9)</cell><cell>0.07 (0.01)</cell></row><row><cell></cell><cell>PIFO = 512</cell><cell>9.95 (9.42)</cell><cell>0.07 (0.01)</cell></row><row><cell>Packet Buffer</cell><cell>256 KB</cell><cell>0.16</cell><cell>8.94</cell></row><row><cell>Simple RMT</cell><cell>/</cell><cell>0.27</cell><cell>0.00</cell></row><row><cell cols="4">cost 11.27% logic area (LUTs) in our middle-end FPGA.</cell></row><row><cell cols="4">Total BRAM usage is 8.94% due to the limited BRAM in our</cell></row><row><cell>FPGA.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Packet dropping rate in phase 1 in Figure</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">PANIC artifact: https://bitbucket.org/uw-madisonnetworking-research/panic_osdi20_artifact</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements: We thank our shepherd, Costin Raiciu, and the anonymous OSDI reviewers for their feedback that significantly improved the paper. We thank Suvinay Subramanian and Tushar Krishna for discussions on crossbar designs and Tao Wang for his assistance with the artifact evaluation. Brent E. Stephens and Kiran Patel were funded by a Google Faculty Research Award and NSF Award CNS-1942686. Aditya Akella and Jiaxin Lin were funded by NSF Awards CNS-1717039 and CNS-1838733 and a gift from Google.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Artifact Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Abstract</head><p>This artifact contains the source code and test benches for PANIC's 100Gbps FPGA-based prototype. Our FPGA prototype is implemented in pure Verilog. Features of the prototype include: the hybrid push/pull packet scheduler, the highperformance switching interconnect, self-contained compute units, and the lightweight RMT pipeline.</p><p>This artifact provides two test benches to reproduce the results in Figure <ref type="figure">8c</ref> and Figure <ref type="figure">11a</ref> in the Vivado HDL simulator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Artifact check-list</head><p>• Compilation: Running this artifact requires Vivado Design Suite <ref type="bibr" target="#b8">[10]</ref>. Vivado v2019.x and v2020.1 WebPack are verified.</p><p>• Hardware: This artifact does not requires any specific hardware.</p><p>• Metrics: This artifact measures PANIC's receiving throughput under different chaining models and traffic patterns.</p><p>• Output: The result will be printed to the console and log files.</p><p>• Experiments: This artifact includes testbenches and running scripts to replay Figure <ref type="figure">8c</ref> and Figure <ref type="figure">11a</ref>.</p><p>• Public link: https://bitbucket.org/uwmadison-networking-research/panic_ osdi20_artifact</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.1 How to access</head><p>This artifact is publicly available at https://bitbucket. org/uw-madison-networking-research/panic_ osdi20_artifact.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.2 Software dependencies</head><p>Running this artifact requires Vivado <ref type="bibr" target="#b8">[10]</ref>. Vivado WebPack version is license-free, and it has simulation capabilities to recreate our results. Since installing the Vivado WebPack requires plenty of disk space (&gt;20GB), you can choose to instance an FPGA Developer AMI in AWS (https://aws.amazon.com/marketplace/ pp/B06VVYBLZZ) to run this artifact. The FPGA Developer AMI has pre-installed the required Vivado toolchain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Experiment workflow</head><p>1. Check Vivado is Installed Correctly $ v i v a d o −mode t c l / / E n t e r t h e Vivado Command P a l e t t e Vivado% v e r s i o n / / v2019 . x and v2020 . 1 i s v e r i f i e d Vivado% q u i t 2. Clone the Repo and Make Run $ g i t c l o n e [ A r t i f a c t R e p o ] $ cd p a n i c o s d i 2 0 a r t i f a c t $ make t e s t p a r a l l e l $ make t e s t s h a a e s The make command first compiles the source code, then runs the simulation tasks in Vivado. The test parallel test replays Figure <ref type="figure">8c</ref> and the test shaaes test replays Figure <ref type="figure">11a</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Evaluation and expected result</head><p>The result will be printed to the console. The output will also be logged in ./build/export sim/xsim/simulate.log. For the expected output and analysis please reference Figure <ref type="figure">8c</ref> and Figure <ref type="figure">11a</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 Notes</head><p>For more details about the code structure, please reference https://bitbucket.org/uw-madison-networkingresearch/panic_osdi20_artifact/src/master/ README.md</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7 AE Methodology</head><p>Submission, reviewing and badging methodology:</p><p>• https://www.usenix.org/conference/osdi20/ call-for-artifacts</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="https://opencores.org/projects/tiny_aes" />
		<title level="m">AES Hardware Accelerator</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<ptr target="https://www.xilinx.com/support/documentation/ip_documentation/ug761_axi_reference_guide.pdf" />
		<title level="m">Axi reference guide</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Dpu</forename><surname>Fungible</surname></persName>
		</author>
		<ptr target="https://www.fungible.com/news/fungible-dpu-a-new-class-of-microprocessor-powering-next-generation-data-center-infrastructure/" />
		<title level="m">A New Class of Microprocessor Powering Next Generation Data Center Infrastructure</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<ptr target="https://www.fungible.com/wp-content/uploads/2020/08/PB0028.01.02020820" />
		<title level="m">-Fungible-F1-Data-Processing-Unit</title>
				<imprint/>
	</monogr>
	<note>Fungible F1 Data Processing Unit. pdf</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<ptr target="https://www.fungible.com/wp-content/uploads/2020/08/PB0029.00.02020811" />
		<title level="m">-Fungible-S1-Data-Processing-Unit</title>
				<imprint/>
	</monogr>
	<note>Fungible S1 Data Processing Unit. pdf</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<ptr target="https://www.intel.com/content/dam/www/public/us/en/documents/datasheets/ethernet-multi-host-controller-fm10000-family-datasheet.pdf" />
		<title level="m">Intel ethernet switch fm10000 datasheet</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<ptr target="https://opencores.org/projects/sha3" />
		<title level="m">SHA-3 Hardware Accelerator</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<ptr target="https://www.sifive.com" />
		<title level="m">Silicon at the speed of software</title>
				<imprint>
			<date type="published" when="2020-05-25">2020-05-25</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<ptr target="https://www.xilinx.com/products/design-tools/vivado.html" />
		<title level="m">Vivado design suite</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title/>
		<ptr target="https://accoladetechnology.com/whitepapers/ANIC-Features-Overview.pdf" />
	</analytic>
	<monogr>
		<title level="j">ACCOLADE TECHNOLOGY. Accolade ANIC</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<ptr target="https://www.alpha-data.com/pdfs/adm-pcie-9v3.pdf" />
		<title level="m">ADM-PCIE-9V3 -High-Performance Network Accelerator</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Enabling programmable transport protocols in high-speed NICs</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Arashloo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lavrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghobadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rexford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wentzlaff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Networked Systems Design and Implementation (NSDI)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The rocket chip generator</title>
		<author>
			<persName><forename type="first">K</forename><surname>Asanovi Ć</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Avizienis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bachrach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Beamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Biancolin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Celio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dabbelt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Izraelevitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karandikar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Koenig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Love</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Magyar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Moreto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Richards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Twigg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Waterman</surname></persName>
		</author>
		<idno>UCB/EECS-2016-17</idno>
		<imprint>
			<date type="published" when="2016-04">Apr 2016</date>
			<pubPlace>Berkeley</pubPlace>
		</imprint>
		<respStmt>
			<orgName>EECS Department, University of California</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Towards predictable datacenter networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ballani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Karagiannis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rowstron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Special Interest Group on Data Communication (SIGCOMM)</title>
				<meeting>the ACM Special Interest Group on Data Communication (SIGCOMM)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName><forename type="middle">Barefoot</forename><surname>Barefoot</surname></persName>
		</author>
		<author>
			<persName><surname>Tofino</surname></persName>
		</author>
		<ptr target="https://www.barefootnetworks.com/technology/#tofino" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Network traffic characteristics of data centers in the wild</title>
		<author>
			<persName><forename type="first">T</forename><surname>Benson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Akella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Maltz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IMC</title>
				<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The keccak reference, version 3.0. NIST SHA3 Submission Document</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bertoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Daemen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Peeters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Assche</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011-01">January 2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Keccak sponge function family main document</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bertoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Daemen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Peeters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Van Assche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Submission to NIST</title>
				<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fast and scalable priority queue architecture for high-speed network switches</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bhagwan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings IEEE INFOCOM 2000. Conference on Computer Communications. Nineteenth Annual Joint Conference of the IEEE Computer and Communications Societies</title>
				<meeting>IEEE INFOCOM 2000. Conference on Computer Communications. Nineteenth Annual Joint Conference of the IEEE Computer and Communications Societies</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Forwarding metamorphosis: fast programmable match-action processing in hardware for SDN</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bosshart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gibb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varghese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Izzard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Mujica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Horowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Special Interest Group on Data Communication (SIGCOMM)</title>
				<meeting>the ACM Special Interest Group on Data Communication (SIGCOMM)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><surname>Broadcom</surname></persName>
		</author>
		<author>
			<persName><surname>Stingray</surname></persName>
		</author>
		<ptr target="https://www.broadcom.com/products/ethernet-connectivity/smartnic" />
		<title level="m">SmartNIC Adapters and IC</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Broom: an open-source out-of-order processor with resilient low-voltage operation in 28-nm cmos</title>
		<author>
			<persName><forename type="first">C</forename><surname>Celio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-F</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Asanovi Ć</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nikoli Ć</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Patter-Son</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>IEEE</publisher>
			<pubPlace>Micro</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">C</forename><surname>Celio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-F</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nikolic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Asanovic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">Boom v2, 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">AND EDSALL, T. dRMT: Disaggregated programmable switching</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fingerhut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sivaraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vargaftik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mendelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-T</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Keslassy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Orda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Special Interest Group on Data Communication (SIGCOMM</title>
				<meeting>the ACM Special Interest Group on Data Communication (SIGCOMM</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Chole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fingerhut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sivaraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vargaftik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mendelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-T</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Keslassy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Orda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Edsall</surname></persName>
		</author>
		<author>
			<persName><surname>Drmt</surname></persName>
		</author>
		<ptr target="https://cs.nyu.edu/˜anirudh/sigcomm17_drmt_extended.pdf" />
		<title level="m">Disaggregated programmable switching -extended version</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">The design of Rijndael: AES-the advanced encryption standard</title>
		<author>
			<persName><forename type="first">J</forename><surname>Daemen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rijmen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">FaRM: Fast remote memory</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dragojevi Ć</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Hodson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Castro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Networked Systems Design and Implementation (NSDI)</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Exablaze</forename><surname>Exanic</surname></persName>
		</author>
		<ptr target="https://exablaze.com//exanic-v5p" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Azure accelerated networking: SmartNICs in the public cloud</title>
		<author>
			<persName><forename type="first">D</forename><surname>Firestone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Putnam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mundkur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dabagh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Andrewartha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Angepat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bhanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Caulfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">K</forename><surname>Chandrappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chaturmohta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Humphrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lavier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ovtcharov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Padhye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Popuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Raindel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sapre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sivakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zuhair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vaid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Maltz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Greenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Networked Systems Design and Implementation (NSDI)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Corundum: An open-source 100-Gbps NIC</title>
		<author>
			<persName><forename type="first">A</forename><surname>Forencich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Snoeren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Porter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">28th IEEE International Symposium on Field-Programmable Custom Computing Machines</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Smart-NIC performance isolation with FairNIC: Programmable networking for the cloud</title>
		<author>
			<persName><forename type="first">S</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yelam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Snoeren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Special Interest Group on Data Communication (SIGCOMM)</title>
				<meeting>the ACM Special Interest Group on Data Communication (SIGCOMM)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">RDMA over commodity Ethernet at scale</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Soni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Padhye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lipshteyn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Proceedings of the ACM Special Interest Group on Data Communication</title>
		<imprint>
			<biblScope unit="issue">SIGCOMM</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Mind the Gap: A case for informed request scheduling at the NIC</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Humphries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kaffes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mazi Ères</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Workshop on Hot Topics in Networks</title>
				<imprint>
			<publisher>ACM HotNets</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The case for a network fast path to the CPU</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ibanez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shahbaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Workshop on Hot Topics in Networks</title>
				<imprint>
			<publisher>ACM HotNets</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<ptr target="http://www.intel.com/content/dam/www/public/us/en/documents/datasheets/82599-10-gbe-controller-datasheet.pdf" />
		<title level="m">INTEL. Intel 82599 10 GbE controller datasheet</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Silo: Predictable message latency in the cloud</title>
		<author>
			<persName><forename type="first">K</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ballani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moncaster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Special Interest Group on Data Communication (SIGCOMM)</title>
				<meeting>the ACM Special Interest Group on Data Communication (SIGCOMM)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Datacenter rpcs can be general and fast</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kalia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kaminsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Andersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Networked Systems Design and Implementation (NSDI)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Bullet trains: A study of NIC burst behavior at microsecond timescales</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Snoeren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Voelker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Emerging Networking Experiments and Technologies CoNEXT</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">High performance packet processing with FlexNIC</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kaufmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krishnamurthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Flattened butterfly: a costefficient topology for high-radix networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Abts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th annual International Symposium on Computer Architecture (ISCA</title>
				<meeting>the 34th annual International Symposium on Computer Architecture (ISCA</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Measuring the gap between fpgas and asics</title>
		<author>
			<persName><forename type="first">I</forename><surname>Kuon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 ACM/SIGDA 14th International Symposium on Field Programmable Gate Arrays</title>
				<meeting>the 2006 ACM/SIGDA 14th International Symposium on Field Programmable Gate Arrays</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">UNO: Uniflying host and smart NIC offload for flexible packet processing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Akella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Swift</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">V</forename><surname>Lakshman</surname></persName>
		</author>
		<editor>SoCC</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">KV-Direct: High-performance in-memory key-value store with programmable NIC</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Putnam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOSP</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Highly flexible and high-performance network processing with reconfigurable hardware</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><surname>Clicknp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Proceedings of the ACM Special Interest Group on Data Communication</title>
		<imprint>
			<biblScope unit="issue">SIGCOMM</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Offloading distributed applications onto smartnics using IPipe</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schuh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Special Interest Group on Data Communication (SIGCOMM)</title>
				<meeting>the ACM Special Interest Group on Data Communication (SIGCOMM)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Energy-efficient microservices on SmartNIC-accelerated servers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Phothilimthana</surname></persName>
		</author>
		<author>
			<persName><surname>E3</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Usenix Annual Technical Conference (ATC)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Heracles: Improving resource efficiency at scale</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Govindaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Computer Architecture (ISCA)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Snap: a microkernel approach to host networking</title>
		<author>
			<persName><forename type="first">M</forename><surname>Marty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Kruijf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Adriaens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Alfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Contavalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dalton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dukkipati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">C</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gribble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kidd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kononov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Musick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Olson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rubow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Springborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Valancius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGOPS</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<ptr target="http://www.mellanox.com/related-docs/prod_adapter_cards/PB_BlueField_Smart_NIC.pdf" />
	</analytic>
	<monogr>
		<title level="m">MELLANOX TECHNOLOGIES. Innova -2 Flex Programmable Network Adapter</title>
		<title level="s">MELLANOX TECHNOLOGIES. Mellanox BlueField Smart-NIC</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">NVIDIA Mellanox BlueField-2 DPU</title>
		<author>
			<persName><forename type="first">Technologies</forename><surname>Mellanox</surname></persName>
		</author>
		<ptr target="https://www.mellanox.com/products/bluefield2-overview" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Expanding across time to deliver bandwidth efficiency and low latency</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Mellette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mcguinness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Sno-Eren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Networked Systems Design and Implementation (NSDI)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Universal packet scheduling</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ratnasamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Networked Systems Design and Implementation</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<ptr target="https://netronome.com/product/nfp-6xxx/" />
		<title level="m">NETRONOME. NFP-6xxx flow processor</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Floem: A programming system for NIC-accelerated network applications</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Phothilimthana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kaufmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bodik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Operating Systems Design and Implementation (OSDI)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Flow-Blaze: Stateful packet processing in hardware</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pontarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bifulco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bonola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cascone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Spaziani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bruschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sanvito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Siracusano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Capone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Honda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Siracusano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Networked Systems Design and Implementation (NSDI)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Sharing the network in cloud computing</title>
		<author>
			<persName><forename type="first">L</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ratnasamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName><surname>Faircloud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Proceedings of the ACM Special Interest Group on Data Communication</title>
		<imprint>
			<biblScope unit="issue">SIGCOMM</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">SENIC: Scalable NIC for end-host rate limiting</title>
		<author>
			<persName><forename type="first">S</forename><surname>Radhakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Jeyakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kabbani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Porter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Networked Systems Design and Implementation (NSDI)</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Swizzle-switch networks for manycore systems</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sewell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Dreslinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Manville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pinckney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cieslak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sylvester</surname></persName>
		</author>
		<author>
			<persName><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal on Emerging and Selected Topics in Circuits and Systems</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="278" to="294" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Fast, scalable, and programmable packet scheduler in hardware</title>
		<author>
			<persName><forename type="first">V</forename><surname>Shrivastav</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Special Interest Group on Data Communication (SIGCOMM)</title>
				<meeting>the ACM Special Interest Group on Data Communication (SIGCOMM)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Direct universal access: Making data center resources available to FPGA</title>
		<author>
			<persName><forename type="first">R</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moscibroda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Networked Systems Design and Implementation (NSDI)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Jupiter rising: A decade of clos topologies and centralized control in google&apos;s datacenter network</title>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Armistead</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bannon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Boving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Felderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ger-Mano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kanagala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Provost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Simmons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wanderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Ölzle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stuart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Proceedings of the ACM Special Interest Group on Data Communication</title>
		<imprint>
			<biblScope unit="issue">SIGCOMM</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Packet transactions: High-level programming for linerate switches</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sivaraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Budiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varghese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Licking</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Proceedings of the ACM Special Interest Group on Data Communication</title>
		<imprint>
			<biblScope unit="issue">SIGCOMM</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Programmable packet scheduling at line rate</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sivaraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-T</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Edsall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Katti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Proceedings of the ACM Special Interest Group on Data Communication</title>
		<imprint>
			<biblScope unit="issue">SIGCOMM</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Your programmable NIC should be a programmable switch</title>
		<author>
			<persName><forename type="first">B</forename><surname>Stephens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Akella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Swift</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Workshop on Hot Topics in Networks (ACM HotNets)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Loom: Flexible and efficient nic packet scheduling</title>
		<author>
			<persName><forename type="first">B</forename><surname>Stephens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Akella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Swift</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Networked Systems Design and Implementation (NSDI)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Dark packets and the end of network scaling</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mcguinness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Voelker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ANCS</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<ptr target="http://www.mellanox.com/repository/solutions/tile-scm/docs/UG130-ArchOverview-TILE-Gx.pdf" />
		<title level="m">TILERA. Tile Processor Architecture Overview For the TILE-GX Series</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">On-chip interconnection architecture of the tile processor</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wentzlaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ramey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Brown Iii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="2007-09">Sept. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">A synthesizable datapath-oriented embedded FPGA fabric</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J E</forename><surname>Wilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H W</forename><surname>Leong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Luk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Quinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 ACM/SIGDA 15th International Symposium on Field Programmable Gate Arrays (FPGA)</title>
				<meeting>the 2007 ACM/SIGDA 15th International Symposium on Field Programmable Gate Arrays (FPGA)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<author>
			<persName><forename type="first">Xilinx</forename><forename type="middle">Xilinx</forename><surname>Alveo</surname></persName>
		</author>
		<ptr target="https://www.xilinx.com/products/boards-and-kits/alveo.html" />
		<title level="m">Adaptable Accelerator Cards for Data Center Workloads</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">N</forename><surname>Zilberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Audzevich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Covington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sume</forename><surname>Netfpga</surname></persName>
		</author>
		<imprint>
			<publisher>Toward 100 Gbps as research commodity</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
