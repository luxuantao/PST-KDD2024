<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Medical Entity Relation Verification with Large-scale Machine Reading Comprehension</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yuan</forename><surname>Xia</surname></persName>
							<email>xiayuan@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
							<email>wangchunyu03@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhenhui</forename><surname>Shi</surname></persName>
							<email>shizhenhui@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jingbo</forename><surname>Zhou</surname></persName>
							<email>zhoujingbo@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chao</forename><surname>Lu</surname></persName>
							<email>luchao@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haifeng</forename><surname>Huang</surname></persName>
							<email>huanghaifeng@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hui</forename><surname>Xiong</surname></persName>
							<email>hxiong@rutgers.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Rutgers University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Medical Entity Relation Verification with Large-scale Machine Reading Comprehension</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3447548.3467144</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Fact Verification</term>
					<term>Relation Extraction</term>
					<term>Clinical Decision Support</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Medical entity relation verification is a crucial step to build a practical and enterprise medical knowledge graph (MKG) because highprecision medical entity relation is a key requirement for many MKG-based applications. Existing relation verification approaches for general knowledge graphs are not designed for considering medical domain knowledge, although it is central to achieve highquality entity relation verification for MKG. To this end, in this paper, we introduce a system for medical entity relation verification with large-scale machine reading comprehension. The proposed system is tailored to overcome the unique challenges of medical relation verification including high variants of medical terms, the high difficulty of evidence searching in complex medical documents, and the lack of evidence labels for supervision. To deal with the problem of variants of medical terms, we introduce a synonym-aware retrieve model to retrieve the potential evidence implicitly verifying the given claim. To better utilize the medical domain knowledge, a relation-aware evidence detector and a medical ontology-enhanced aggregator are developed to improve the performance of the relation verification module. Moreover, to overcome the challenge of providing high-quality evidence due to the lack of labels, we introduce an interactive collaborative-training method to iteratively improve the evidence accuracy. Finally, we conduct extensive experiments to demonstrate that the performance of our proposed system is superior to all comparable models. We also demonstrate that our system can significantly reduce the annotation time by medical experts in real-world verification tasks. It can help to improve the efficiency by nearly 300%. In particular, our system has been embedded into the Baidu Clinical Decision Support System.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>â€¢ Information systems â†’ Information extraction; Chemical and biochemical retrieval; â€¢ Applied computing â†’ Health informatics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>With the rapid development of data-driven medical knowledge graph (MKG) construction, an evidence-based and effective medical relation verification framework is demanding. Information extraction (IE) has notably facilitated the construction of MKG which makes it possible to extract a large number of medical entity relations from medical documents such as online medical websites and Electronic Medical Records (EMRs). However, most of the relations extracted from such medical documents are unverified since the data-driven IE method usually cannot provide the evidence to support the claim (i.e. the relation between two entities). Such unverified relations are always untrustworthy and even unacceptable in medical domain applications. In real-world medical applications, such as Baidu Clinical Decision Support System (CDSS) <ref type="foot" target="#foot_0">1</ref> , the interpretable and evidence-based result is necessary for assisting the doctor to make a diagnosis <ref type="bibr" target="#b27">[27]</ref>. The verification system should provide concrete evidence to verify the relation is correct, not just the probability. In industrial applications, companies often hire a large number of medical domain experts to annotate and verify every extracted medical entity relations from the knowledge graph. This process is labor-intensive, time-consuming and expensive. Therefore, how to automatically verify the extracted medical relations as well as to improve the efficiency of the verification process becomes a vital problem for building a practical and enterprise MKG. Recently, many research efforts have been devoted to Fact Verification (FV) <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b32">32]</ref>, which aims to verify given claims with the evidence retrieved from plain text. However, most of the existing methods are general frameworks for fact verification, without considering the special properties of medical documents and handling the unique challenges in medical domain, they are less effective when dealing with medical entity relation verification.</p><p>There are mainly three challenges when dealing with medical fact verification in industrial applications. The first challenge is that there is a lot of variants for medical terms, especially for medical synonyms. For instance, medical term abdominal pain can also refer to as stomachache, collywobbles, etc. It becomes more difficult to handle such synonym variations for fact verification in medical documents. The sentence retrieval from medical documents needs to be aware of that the candidate sentence implicitly contains the target entity or the synonym of the target entity. As shown in Table <ref type="table">1</ref>, let's say when we are checking the medical claim "stomachache is the symptom of gastritis ", which can be represented with the knowledge graph (KG) triplet form (gastritis, stomachache, symptom), and assuming there is a sentence in the clinical textbook which said "Acute gastritis can have pain or an uncomfortable feeling in their upper abdomen". The sentence selection module in general fact verification systems fails to retrieve this sentence as evidence because they are unable to realize the synonym of the target entity is implicitly contained in this sentence.</p><p>The second challenge is how to better utilize medical domain knowledge for medical entity relation verification. When verifying the medical entity relation, the experts with medical domain knowledge can quickly focus on the right part of the paragraphs or sentences and can infer the medical relation with medical ontology knowledge. For instance, when checking the medical relation between pneumonia and chest radiograph, the experts will focus on the paragraph under the title of laboratory inspection, while checking the relation between pneumonia and cough, the experts will locate at the paragraph under the title of clinical manifestation. If the cough is the symptom of lobar pneumonia, the experts can infer the cough is also the symptom of pneumonia. However, to the best of our knowledge, the general verification models are unable to perceive the above domain knowledge in the medical domain.</p><p>The third challenge is how to provide high-quality evidence due to the lack of evidence labels for supervision. In the medical domain, it is difficult to improve the accuracy of the retrieved evidence since we do not have a huge amount of the labeled data which indicates the evidence is correct or not. In the general fact verification framework, the label of the retrieved evidence can be annotated by crowdsourcing, such as Amazon Mechanical Turk. While it becomes very difficult due to the high domain knowledge requirement for medical domain.</p><p>Aiming to improve the accuracy and efficiency of medical relation checking, we introduce a framework for automatic medical entity relation verification with large-scale machine reading comprehension. The overall architecture of the system is illustrated in Figure <ref type="figure" target="#fig_0">1</ref>. We develop a three-stage pipeline system: (1) Document Retrieval, a module to narrow our search place and focus on the relevant clinical documents, (2) Synonym-Aware Sentence Selection, a module to select the evidence from the retrieved clinical document, and (3) Machine Reading Comprehension-based Semantic Relation Verification (MSRV), a module to check the medical relations via machine reading comprehension of clinical materials. Additionally, we introduce an interactive collaborative-training method to iteratively improve the accuracy of retrieved evidence.</p><p>At first, to handle the first challenge of high variants of medical terms, we propose a synonym-aware sentence selection module that is aware of synonyms of the target entity and can perceive the implicit relations between the target entity and sentences. We first construct the candidate synonym pairs with rule-based and synthetic-based methods, then we construct a synonym prediction sub-task to fine-tune a pre-trained language model which is the ERNIE model <ref type="bibr" target="#b23">[23]</ref> in this paper. As ERNIE is a continual pre-training framework for language understanding, after this synonym prediction sub-task, our ERNIE-based sentence selection module can have the ability to tackle the synonym problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Claim</head><p>(gastritis, stomachache, symptom)</p><p>Stomachache is the symptom of gastritis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evidence</head><p>Acute gastritis can have pain or an uncomfortable feeling in their upper abdomen.</p><p>Table <ref type="table">1</ref>: Example of a Claim-Evidence Pair.</p><p>To tackle the second challenge, we first utilize the fine-tuned ERNIE model as an encoder to get the embedding representations of the claim, the evidence, and the evidence metadata. Then, different from previous work like <ref type="bibr" target="#b32">[32]</ref>, which computes the attention by the claim and the evidence alone, we add a relation-aware matrix to let the evidence detector to sense the correlation between the target entity relation and the evidence metadata. It can realize the global structure of the document and better focus on the context information. After that, we adopt a medical ontology-enhanced aggregator for relation verification.</p><p>To overcome the third challenge, we introduce an Interactive Collaborative-Training (ICT) method to iteratively improve the accuracy of retrieved evidence. At each iteration, we train an evidence discriminator to score the retrieved evidence and make predictions on unlabeled data set. Then, we assign the most informative evidence to medical experts for annotation and assign high confidence evidence with refined labels. In the next iteration, we retrain the evidence discriminator with labels annotated by medical experts and train our verification system with unlabeled and labeled evidence dataset. We iteratively process the above procedure, until the accuracy of the evidence meets the requirement.</p><p>We summarize our contributions as follows:</p><p>â€¢ We propose a framework for the automatic verification of the medical entity relation with large-scale machine reading comprehension, which incorporates the authoritative clinical materials for the relation verification.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Fact verification (FV) is a challenging task that requires retrieving relevant evidence from plain text and utilize the evidence to verify given claims. Existing methods usually formulate FV as a natural language inference (NLI) <ref type="bibr" target="#b0">[1]</ref> task. Thorne et al. <ref type="bibr" target="#b24">[24]</ref> mainly feed the concatenated evidence and the given claim into the NLI model. Another solution is to utilize the decomposable attention model (DAM) <ref type="bibr" target="#b18">[18]</ref> to predict each claim evidence pair individually and then all predictions are aggregated for final verification <ref type="bibr" target="#b15">[15]</ref>. There are also a few of studies <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b29">29]</ref> that adopt the enhanced sequential inference model (ESIM) <ref type="bibr" target="#b4">[5]</ref> to infer the relationship between evidence and claims, which achieves better performance. Zhou et al. <ref type="bibr" target="#b32">[32]</ref> propose the graph-based evidence aggregating and reasoning (GEAR) model for evidence claim prediction. Liu et al. <ref type="bibr" target="#b14">[14]</ref> introduce kernel graph attention network (KGAT) model for fine-grained fact verification with kernel-based attentions. However, the literature mentioned above are general frameworks for fact verification, which are less effective in the medical domain verification. MedTruth <ref type="bibr" target="#b6">[6]</ref> proposes a truth discovery method for medical knowledge condition discovery from the multi-source, but this method is mainly to consider relation discovery rather than relation automatic verification. Pre-trained language models like ERNIE <ref type="bibr" target="#b23">[23]</ref>, BERT <ref type="bibr" target="#b7">[7]</ref>, XLNet <ref type="bibr" target="#b28">[28]</ref> and OpenAI GPT <ref type="bibr" target="#b19">[19]</ref> can achieve huge gains on many Natural Language Processing (NLP) tasks, such as GLUE <ref type="bibr" target="#b26">[26]</ref> benchmark, by pre-training on unlabeled corpus and fine-tuning on labeled ones. ERNIE employs transformer encoder and pre-training tasks to fuse bidirectional context information. In our experiments, we utilize the fine-tuned ERNIE model for several subtasks in order to automatically verify medical entity relationship.</p><p>In general machine reading comprehension, a benchmark dataset is the Stanford Question Answering Dataset (SQuAD) consisting of over 10k questions posed by crowd workers on Wikipedia articles <ref type="bibr" target="#b20">[20]</ref>. Chen et al. <ref type="bibr" target="#b2">[3]</ref> propose to tackle open-domain question answering using Wikipedia as the unique knowledge source. Reading and understanding text in the clinical medicine domain is also being a major research problem in the field of NLP. For example, a reading comprehension model SeaReader for question-answering task on clinical medicine is proposed <ref type="bibr" target="#b30">[30]</ref>. Chen et al. <ref type="bibr">[4]</ref> devise a knowledge abstract matching method to retrieve relevant evidence from medical knowledge base to support medical question answering.</p><p>There is also another study to utilize a contextual self-attention multi-scale sentence embedding (CAMSE) model and two scoring strategies to exploit semantic similarity and association between a given question and the corresponding evidence document <ref type="bibr" target="#b10">[10]</ref>. Fei et al. <ref type="bibr" target="#b8">[8]</ref> propose a hierarchical multi-task word embedding model to learn more representative medical entity embeddings and apply them to medical synonym prediction. Moreover, the authors of <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b30">30]</ref> are mainly focus on the medical multiple choices questionanswering, Fei et al. <ref type="bibr" target="#b8">[8]</ref> mainly focus on synonym prediction, which is different from the medical entity relation verification. Niu et al. <ref type="bibr" target="#b17">[17]</ref> present an iterative method to generate soft evidence labels for improving the performance of machine reading comprehension, while the accuracy of the generated evidence cannot be guaranteed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>We propose a three-stage pipeline system to automatically verify the medical entity relations via large-scale machine reading comprehension. Additionally, we introduce an interactive collaborativetraining (ICT) method to improve the evidence accuracy. We first describe the process of constructing the document retrieval module with medical structured data. Second, we introduce our synonymaware sentence selection module, which selects the evidence from the retrieved clinical documents. Finally, we introduce our MRCbased semantic relation verification model (MSRV), which checks the medical entity relations via comprehension of authoritative clinical materials. The full pipeline of our proposed framework for medical entity relation verification is illustrated in Figure <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Document Retrieval</head><p>Given a claim, we first retrieve a set of evidence that is likely to be relevant using a document retrieval module. The document retrieval is built upon ElasticSearch, which is a search engine based on the Lucence library followed by BM25 <ref type="bibr" target="#b21">[21]</ref>. We parse dozens of clinical textbooks and medical encyclopedia data into 4-dimensional structure (Disease, Title, Path, Paragraph) and it serves as the data source for our module. In this paper, we define Title and Path as Evidence Metadata. The details of the clinical materials are described in section 4.1.1. We process all clinical materials into the same data structure as shown in the Table <ref type="table" target="#tab_1">2</ref>. We use entities in the given claim as search queries to find the topK relevant documents. The retrieved documents are then feed into our synonym-aware sentence selection module.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Item</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Synonym-Aware Sentence Selection</head><p>In the domain of medical relation verification, the sentence retrieval becomes harder when there are a lot of variations for the medical terms, especially for the case of medical synonyms. As the medical entity normalization is not perfect, the coverage of the available evidence retrieved by ElasticSearch is limited. To solve this problem, we propose a synonym-aware sentence selection module. As the computational complexity for the ERNIE model is expensive, before the ERNIE prediction, we construct a simple semantic similarity model to calculate the relevance score between the candidate sentence and target entities to filter out the irrelevant sentence. In this semantic similarity model, we split the candidate sentence into words, and use the average of word embedding to represent the sentence embedding. The word embedding is trained on medical EMRs with FastText <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">13]</ref>. Then we calculate the similarity of the target entity and sentence using the cosine distance. We filter out the sentences with similarity scores which are below the threshold ğœ ğ‘¤ , the remained sentences are the candidate set for our synonym-aware ERNIE selection model. The overall workflow of the synonym-aware sentence selection mainly has following two steps: synonym pairs construction and synonymous entity-sentence pair prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Synonym Pairs Construction.</head><p>Due to the limited coverage of the available synonym pairs, we construct additional medical synonym pairs from the existing corpus. Inspired by <ref type="bibr" target="#b8">[8]</ref>, we use two methods for the synonym pairs construction: a rule-based method for synonym pairs extraction and a synthetic-based method for synonym pairs generation.</p><p>For the rule-based method, we define several rule-based templates to extract synonym pairs. For instance, the templates can be abbreviation for, referred to as, commonly known as, short for, etc. Then we traverse the corpus with the templates to generate synonym pairs. Additionally, we also parse the medical term encyclopedia in the property field such as alias attribute.</p><p>For the synthetic-based method, we extract medical synonym pairs based on the existing symptom vocabulary list V ğ‘  and a corresponding attribute vocabulary list V ğ‘ . V ğ‘ gives a more specific description of the symptom (such as frequency, intensity, color,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Entity</head><p>Candidate Sentence</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rulebased Dental fluorosis</head><p>Dental fluorosis is also called mottling of teeth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Synthetic Cephalalgia</head><p>Paroxysmal headache for one month, the left frontotemporal area is heavy, â€¢ â€¢ â€¢ duration, location, etc.). The candidate synonym pairs can be constructed by splicing or combining with the symptom and its attributes. For example, by combining the symptom cough and attribute word persistent, we can get a medical synonym pair (cough, persistent cough). Note that, although there is a small portion of the synthetic synonym pairs which are not conformed to the actual grammatical rules, the overall influence on the final verification model is insignificant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Synonymous</head><p>Entity-Sentence Pair Prediction. In this section, we construct a synonym prediction task to fine-tune the ERNIE model. The objective of the task is to predict whether the target entity or the synonym of the target entity is contained in the given sentence. The detail of corpus construction is described in the section 4.1.2. An example of synonymous entity-sentence pair is shown in table <ref type="table" target="#tab_2">3</ref>. First, we feed target entity and sentence that contains the synonym of the target into the ERNIE model to obtain the embedding representation.</p><formula xml:id="formula_0">ğ‘» ğ‘  = ERNIE(ğ‘’ ğ‘¡ , ğ‘ )<label>(1)</label></formula><p>where ğ‘’ ğ‘¡ represents the target entity and ğ‘  represents the candidate sentence contains the synonym entity, ğ‘» ğ‘  is the output representation of the ERNIE model. Then, we feed the ERNIE embedding representation to a dense layer and a softmax layer to get the final synonym-aware prediction.</p><formula xml:id="formula_1">Score (i) s = exp(ğ‘¾ T ğ‘  ğ‘» (ğ‘–) ğ‘  ) ğ¶ ğ‘—=1 exp(ğ‘¾ T ğ‘  ğ‘» ( ğ‘—) ğ‘  )<label>(2)</label></formula><p>where ğ‘¾ ğ‘  âˆˆ R ğ¶Ã—ğ¹ is the output weight matrix, ğ¹ is the number of hidden dimensions of ERNIE, ğ¶ is the number of prediction labels (here, ğ¶ = 2), and Score s is the normalize output probability using the softmax function. Similarly, we set a threshold ğœ ğ‘  to control the trade-off between quality and quantity of the selected evidence with the predicted probabilistic value. Finally, the evidence retrieved by sentence selection module is fed into our MRC-based semantic relation verification model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">MRC-based Semantic Relation Verification</head><p>Given a medical relation triplet (Disease Entity, Target Entity, Relation Type), and ğ‘ pieces of retrieved evidence (ğ‘’ 1 , ğ‘’ 2 , â€¢ â€¢ â€¢ , ğ‘’ ğ‘ ), the goal of our machine reading comprehension based semantic relation verification model (MSRV) model is to verify the given claim with retrieved evidence. In this paper, the medical relation R can be one of five types: Symptom, Operation, Radiographic Examination, Laboratory Examination, and Others. Our MSRV model is comprised of two parts: a Relation-Aware Evidence Detector for precisely locating the right part of the evidence, and a Medical Ontology-Enhanced Evidence Aggregator for final textual entailment based on the medical ontology graph and retrieved evidence. </p><formula xml:id="formula_2">ğ‘ = concat(ğ‘’ ğ‘‘ , ğ‘’ ğ‘¡ , ğ‘Ÿ )<label>(3</label></formula><p>) Next, we feed the claim into the ERNIE model to get the claim representation ğ‘» ğ’„ . We also feed the evidence metadata ğ‘’ ğ‘š into the ERNIE model to get the evidence metadata representation ğ‘» ğ’ .</p><formula xml:id="formula_3">ğ‘» ğ‘ = ERNIE(ğ‘)<label>(4)</label></formula><formula xml:id="formula_4">ğ‘» ğ‘š = ERNIE(ğ‘’ ğ‘š )<label>(5)</label></formula><p>We then add a relation-aware matrix to let the evidence detector sense the correlation between the target entity relation and the evidence metadata.</p><formula xml:id="formula_5">ğ‘“ ğ‘— = ğ‘» T ğ‘ ğ‘¾ ğ‘“ ğ‘» ( ğ‘—) ğ‘š<label>(6)</label></formula><p>where ğ‘» ğ’„ , ğ‘»</p><p>ğ‘š are the embedding representation of the claim and the ğ‘—-th evidence metadata, respectively. ğ‘¾ ğ‘“ is ğ¹ Ã—ğ¹ relation-aware matrix, which enables the detector to focus on the right location.</p><formula xml:id="formula_7">ğ›¼ ğ‘— = softmax(ğ‘“ ğ‘— ) = exp(ğ‘» T ğ‘ ğ‘¾ ğ‘“ ğ‘» ( ğ‘—) ğ‘š ) ğ‘ ğ‘˜=1 exp(ğ‘» T ğ‘ ğ‘¾ ğ‘“ ğ‘» (ğ‘˜) ğ‘š )<label>(7)</label></formula><p>Finally, we calculate relation-aware attention coefficient ğ›¼ ğ‘— for each evidence using the softmax function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Medical</head><p>Ontology-Enhanced Evidence Aggregator. When the evidence detector focuses on the accurate evidence, we then use a medical ontology-enhanced evidence aggregator to check the claim via reading comprehension of the multiple evidence. The objective of the evidence aggregator is to estimate the probability that the entity relation holds true given the retrieved evidence.</p><p>We first feed each claim-evidence pair (ğ‘, ğ‘’ ğ‘— ) into ERNIE to get the representation of each claim-evidence pair ğ‘» ğ‘‘ . The final hidden state representation ğ‘» ğ‘œ is obtained by gathering the multiple evidence information. The relation-aware attention coefficient ğœ¶ is learned by the evidence detector in Eq. <ref type="bibr" target="#b7">(7)</ref>.</p><formula xml:id="formula_8">ğ‘» ğ’ = ğ‘ ğ‘˜=1 ğ›¼ ğ‘˜ ğ‘» (ğ‘˜) ğ‘£ (10)</formula><p>The final prediction Score msrv is calculated as follow:</p><formula xml:id="formula_9">Score (i) msrv = exp(ğ‘¾ T ğ‘œğ‘¢ğ‘¡ ğ‘» (ğ‘–) ğ‘œ ) ğ¶ ğ‘—=1 exp(ğ‘¾ T ğ‘œğ‘¢ğ‘¡ ğ‘» ( ğ‘—) ğ‘œ )<label>(11)</label></formula><p>where ğ‘¾ ğ‘œğ‘¢ğ‘¡ âˆˆ R ğ¶Ã—ğ¹ denotes weight matrix, as before, ğ¹ is the number of hidden dimensions of ERNIE, ğ¶ is the number of prediction labels (here, ğ¶ = 2). The final loss function is obtained as follows:</p><formula xml:id="formula_10">L = âˆ’ 1 ğ‘š ğ‘– Å· (ğ‘–) log(Score (i) msrv ) + ğœ†||ğœƒ || 2 (<label>12</label></formula><formula xml:id="formula_11">)</formula><p>where ğœƒ denotes all trainable parameters, ğ‘š is the number of training examples, Å· (ğ‘–) is the ground truth relation label for ğ‘–-th example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Interactive Collaborative-Training</head><p>In order to make the results of medical relation verification more explainable and better assist doctors to make a diagnosis, our deployed system is also aiming to improve the accuracy of the retrieved the evidence. However, it is a challenge to guarantee the accuracy of evidence due to the lack of evidence labels. Therefore, we introduce a variant of active learning method <ref type="bibr" target="#b22">[22]</ref>, i.e. interactive collaborative-training (ICT), to iteratively improve the accuracy of retrieved evidence through interactions with the MSRV model, evidence discriminator and medical experts. In our active learning settings, the informative instances should satisfy two properties: uncertainty and importance. The uncertainty means that the evidence discriminator cannot make confident predictions, and the importance means the evidence itself is important to MSRV for relation verification. The ICT is developed according to the above principle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Evidence Discriminator.</head><p>In order to improve the accuracy of retrieved evidence, we develop an evidence discriminator ğ· ğœ™ to score the retrieved evidence and then use it to filter out the wrong evidence. The evidence discriminator outputs a single scalar, which represents the confidence probability that whether the retrieved evidence can support the claim or not. If the evidence cannot support the claim, then it will be filtered and not involved the training of the MSRV model.</p><p>Score e = sigmoid(ğ· ğœ™ (concat(ğ‘, ğ‘’)))</p><p>where ğ‘ is the claim and ğ‘’ is the corresponding evidence, the evidence discriminator ğ· ğœ™ (â€¢) is built based on an ERNIE model. on ğ‘ˆ and ğ¿. After training, the ğ· ğœ™ makes evidence predictions on unlabeled instances. On the one hand, we first add the uncertain instances to ğ¿ â€² and sort them with relation-aware coefficients calculated by Eq. ( <ref type="formula" target="#formula_7">7</ref>), and then ask the medical experts to annotate the top-ğ‘› evidence labels with the help of our system. On the other hand, we select the instances with high confidence, and we refine their evidence labels according to the output score of ğ· ğœ™ . After that, we get the updated ğ‘ˆ and ğ¿, which are used to update the ğ· ğœ™ and ğ‘€ ğœƒ in the next iteration. The evidence with negative label is filtered and is not involved the training of the MSRV model ğ‘€ ğœƒ in the next iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Collaborative</head><p>In the first iteration (iteration 0), the initial labeled set ğ¿ is a small set of instances already annotated by medical experts. The initial ğ· ğœ™ is trained on ğ¿. The initial ğ‘ˆ is the same as the experimental dataset for relation verification. The evidence of ğ‘ˆ is first acquired by document retriever and sentence selection module. The initial MSRV model ğ‘€ ğœƒ is trained on ğ‘ˆ with relation labels. We iteratively process the above procedure, as the accuracy of the evidence meets the requirement (i.e. â‰¥ 0.8). The procedure of one iteration of ICT is described in Algorithm 1. The Entropy(â€¢) means the entropy of probability distribution, the ğœ–, ğ›¿ are two thresholds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENT 4.1 Data Description</head><p>4.1.1 Clinical Materials. We have collected a Chinese corpus containing 60 clinical medical textbooks (e.g. Internal Medicine, Respiratory disease, Neurology, Pediatrics, Gastroenterology, etc.) and medical encyclopedia data. In total, the corpus includes 8.72 million sentences and 23,722 diseases. The clinical materials are processed to the structured format, and then indexed by the ElasticSearch with the metadata. The metadata includes paths and titles (e.g. clinical manifestation, examination, diagnosis, treatment, differential diagnosis, etiology, prevention, prognosis, etc.). An example of a structured document is shown in Table <ref type="table" target="#tab_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Construct Corpus for Sentence Selection.</head><p>In the synonymaware sentence selection section, we first construct synonym pairs from de-identified Electronic Medical Records (EMRs) and the medical encyclopedia<ref type="foot" target="#foot_1">2</ref> according to the rule-based and the syntheticbased method. We construct 10k synonym pairs. Then we filter out the low-quality candidate pairs with pre-defined rules. We generate a synonymous entity-sentence set according to the selected synonym pairs. We retrieve the sentence containing the target entity or the synonym of the target entity as positive samples. As for negative samples, we randomly select a sentence in the same document at which the target entity locate. We take each entity-sentence pair as input and corresponding label as the prediction target. Our sentence selection module is then trained with the constructed corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Medical Ontology Knowledge. Our medical ontology graph is built based on the Baidu Medical Knowledge Graph (Baidu MKG).</head><p>Considering the efficiency in real-world applications, we transform the hierarchical structure data format into a flat adjacent matrix format and only keep the relations from nearest parent nodes or child nodes. It can avoid computing the complex hierarchical relations in real-time. The processed medical ontology graph G has 27,764 disease ontology relations and contains 16,492 disease entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Experimental</head><p>Dataset for Relation Verification. Our relation verification dataset contains 32,823 disease-target entity relation pairs. The medical entity pairs are first extracted from de-identified Electronic Medical Records (EMRs) and online websites, and then annotated by the medical experts. As the main focus of this paper is to verify the medical entity relation, the disease-target entity relation extraction is regarded as the prepossessing step. To be more specific, we first utilize a constructed medical dictionary to extract medical entity mentions from the raw texts. Then we map the entity mentions to specific entity types, and the relation between the disease and target entity is inferred from the entity types. At last, we match entity pairs in the same text to possible knowledge triplets (i.e claim), the evidence of the claim is constructed by the sentence selection module. An example of the claim-evidence pair is shown in Table <ref type="table">1</ref>. We split the dataset into 3 parts: training, validation, and testing for our medical entity relation verification system. In Figure <ref type="figure" target="#fig_5">2</ref>, we provide summary statistics of our experimental dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Settings</head><p>For the word vector model, we set the length ğ‘™ ğ‘¤ of word vector to 200, initial learning rate ğ›¼ ğ‘¤ to 0.001, neighboring window size ğ¶ ğ‘¤ to 5. For the document retrieval module, we retrieve the top ğ‘˜ relevant document, where we set ğ‘˜ is 10. For the sentence selection module, threshold ğœ ğ‘¤ of semantic similarity model and the threshold ğœ ğ‘  of synonym-aware model are set to 0.8. For both synonym-aware sentence selection module and MRC-based semantic relation verification module, we use the same network structure ğ¸ğ‘…ğ‘ ğ¼ ğ¸ ğ‘ğ‘ğ‘ ğ‘’ in all ERNIE fine-tuning tasks. The ğ¸ğ‘…ğ‘ ğ¼ğ¸ ğ‘ğ‘ğ‘ ğ‘’ model has 12 layers, the hidden state dimension ğ¹ is set to 768, the number of heads is set to 12. The learning rate ğ›¼ ğ‘’ is set to 2e-5. For the synonym-aware sentence selection model, the maximum sequence length ğ¿ ğ‘  ğ‘šğ‘ğ‘¥ is 128. For our MSRV model, we set the maximum sequence length   ğ¿ ğ‘£ ğ‘šğ‘ğ‘¥ to 256. The batch size is set to 32. The L2 regularization parameter ğœ† is set to 0.1. For ICT training, the network structure of the evidence discriminator ğ· ğœ™ is the same as ğ¸ğ‘…ğ‘ ğ¼ğ¸ ğ‘ğ‘ğ‘ ğ‘’ , the maximum sequence length ğ¿ ğ‘‘ ğ‘šğ‘ğ‘¥ is set to 256. The log base of the entropy function is set to ğ‘’, the ğœ– and ğ›¿ are set to 0.65 and 0.5, respectively. The number of assigned evidence ğ‘› for each claim is set to 3, and we process the ICT for 2 iterations. We train our system with the paddlepaddle 3 deep learning framework. For the GEAR <ref type="bibr" target="#b32">[32]</ref> and KGAT <ref type="bibr" target="#b14">[14]</ref>, we use the public source code and default parameter settings for the evaluation of the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Model Comparison</head><p>In this section, we compare our model with several baselines to verify the effectiveness of our approach.</p><p>â€¢ FastText. We obtain semantic similarity features via Fast-Text <ref type="bibr" target="#b1">[2]</ref>, and only use the cosine distance as the probability of the relation prediction. â€¢ Synonym Model. The synonym model is only using the result of the synonym-aware selection module as output. If any of the sentences are retrieved, the system will predict the claim being true. â€¢ GEAR. We utilize the GEAR <ref type="bibr" target="#b32">[32]</ref> model as one of the competitive baseline models, which achieves very promising performance in the general fact verification task.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">ONLINE EVALUATION</head><p>In this section, we show how our deployed medical verification system to solve the real-world medical verification task. We demonstrate the usability of our system by human evaluation from two perspectives: 1) the effectiveness to verify the relations and improve the accuracy of the retrieved evidence; 2) the efficiency to reduce human efforts.</p><p>Effectiveness. We randomly select 600 medical entity pairs from our medical knowledge graph (which has about 400k disease-target entity pairs) whose relations have been automatically verified by our system. Then we filter out the non-standard medical entities and finally obtain 546 candidate medical entity relation pairs for human evaluation. We employ three medical experts to manually evaluate each relation and the corresponding evidence, and label the correctness of relations and evidence by majority voting. Table <ref type="table" target="#tab_6">5</ref> shows system performance on the online experiment. Our MSRV (ED + EA + SA) + ICT model can achieve the precision of 0.963 and the recall of 0.762. As shown in Figure <ref type="figure" target="#fig_6">3</ref>, without ICT training, the initial evidence accuracy is 65.5%. After two iterations of ICT training, we get evidence accuracy of 82.5%, the improvement of evidence accuracy is significant. The row of Human in table <ref type="table" target="#tab_6">5</ref> indicates the medical expert's performance evaluated by another senior expert, which is the upper bound for this verification task. The significance of our verification system is that we can automatically verify a huge number of medical entity relations with high precision, which can significantly reduce the time for medical experts. Efficiency. Our system can also significantly reduce the time cost for manual verification of the medical knowledge graph. In some medical applications, human evaluation for every medical entity relations is necessary since high precision is required. For example, if the required precision is higher than the precision of our system (i.e. â‰¥ 0.98), human evaluation is necessary to manually verify every relation. We showcase the efficiency of our system to reduce the human effort by the following experiment.  The 546 candidate medical entity pairs are then assigned to two groups of medical experts for relation verification. Each group has 10 medical experts. Each medical expert is required to verify the medical entity relation through authoritative clinical materials and should at least retrieve one evidence to support the claim. For comparison, we only provide the system results to one expert group. The system results include the predicted probability and retrieved evidence. We then record the average time for experts to verify the candidate medical relations. As shown in Table <ref type="table" target="#tab_9">6</ref>, the average time for the expert assisted by our verification system is 36 seconds. In comparison, the average time for the other expert group is 106 seconds. Through this medical verification task, we can see that our system can significantly reduce the time of annotation by medical experts, which improves the efficiency of nearly 300%.</p><p>Additionally, in order to prove that our proposed ICT method can improve the efficiency of evidence annotation, we compare ICT with the Random Sampling strategy.</p><p>â€¢ MSRV + RS. At each iteration, we randomly select a number of instances and ask medical experts for annotation. â€¢ MSRV + ICT. At each iteration, we select the most informative instances according to Algorithm 1 and ask the medical experts for annotation.</p><p>The annotation efficiency comparison between ICT and Random Sampling is shown in Figure <ref type="figure" target="#fig_6">3</ref>. After initial training of our system, when adding the same number of labeled instances (i.e. 800), the ICT can achieve a much higher evidence accuracy (82.5% v.s 72.6%) compared to RS. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DEPLOYMENT</head><p>Figure <ref type="figure" target="#fig_8">5</ref> in Appendix Section A.2 gives an overview that how our proposed medical entity relation verification system applied on Baidu CDSS, 4 which functions as a real-time professional assistant to doctors to guide them through standard diagnosis and treatment procedures, alerting them to potential errors and recommending suitable therapeutic plans. In this setting, the claim consists of the doctor's diagnosis, the medical entity (such as symptoms, signs, diagnosis, etc.), and the relation between the diagnosis and the medical entity. As shown on the left side of Figure <ref type="figure" target="#fig_8">5</ref>, the doctor's diagnosis is acute upper respiratory infection, and below is the symptom entities which are automatically verified by our medical relation verification system. As shown on the right side of Figure <ref type="figure" target="#fig_8">5</ref>, the disease entity below the acute upper respiratory infection is the differential diagnosis needed to be distinguished by the doctor, and the text shown in the orange box is the evidence retrieved by the synonym-aware sentence selection module and then evaluated by the evidence discriminator. With the assistance of our medical verification system, the doctor in the hospital can make a more reliable diagnosis with evidence retrieved from the authoritative clinical materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this paper, we introduce a complete description of the implementation of our automatic medical entity relation verification system with large-scale machine reading comprehension. Our system is comprised of three modules: a document retrieval module, a synonym-aware sentence selection module, and an MRC-based semantic verification module. In addition, we introduce an interactive collaborative-training method to improve the evidence accuracy. The proposed synonym-aware sentence retrieval model retrieves the potential evidence that implicitly verifies the given claim. The MRC-based model contains a relation-aware evidence detector and a medical ontology-enhanced evidence aggretator to improve the precision of the relation verification module. We conduct extensive experiments on the offline dataset and applied our system for realworld medical entity relation verification tasks. The experiment results show that the performance of the proposed framework is superior to the other comparable models, and the verification system can significantly reduce the time for the medical expert verification task. To the best of our knowledge, it is the first deployed system to apply automatic medical relation verification techniques into real-world applications. In section 4.4, we provide a quantitative analysis of the experiment results. In this section, to help better understand that our MSRV model can better utilize the medical domain knowledge to achieve higher precision, we provide two cases from the test set.</p><p>Figure <ref type="figure" target="#fig_7">4</ref> shows two cases for our evaluation 5 . The first case is to verified the relation between pulmonary cryptococcosis and vomit via our verification system. As shown on the upside of Figure <ref type="figure" target="#fig_7">4</ref>, two pieces of evidence are retrieved at the final stage. We compare the prediction of the MSRV (EA only) with MSRV (ED + EA). We found that the former model infers that two pieces of evidence are relevant with probability 0.924 and 0.903, respectively. However, the evidence is located at treatment paragraph, which actually said the side-effect of a drug, thus cannot support the relation between pulmonary cryptococcosis and vomit. The ED calculates the attention coefficient between treatment and the claim is 0.0775, and the final probability for evidence 1 is 0.027, for evidence 2 is 0.025, which correctly predicts the claim is not true.</p><p>The second case is to verify the relation between pneumonia and wet rales. As shown on the downside of Figure <ref type="figure" target="#fig_7">4</ref>, without the relation-aware detector, the model retrieved two pieces of evidence. However, evidence 2 is not appropriate for verifying the given claim. 5 The text in Figure <ref type="figure" target="#fig_7">4</ref> is translated from Chinese to English.</p><p>The reason is that the retrieved evidence is under the paragraph of differential diagnosis, which actually describes the relation between the target entity and other disease entities. Our relation-aware evidence detector computes the attention coefficient score between differential diagnosis and the claim, and the final MSRV model will give little attention to this evidence.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 System Interface</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of our proposed framework for medical entity relation automatic verification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Cardiology â†’ [Chapter]Heart Failure â†’ [Section]Clinical Manifestations of Heart Failure Paragraph Paroxysmal dyspnea often occurs at night, and patients often wake up suddenly during deep sleep, with extreme anxiety and choking â€¢ â€¢ â€¢</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>3. 3 . 1</head><label>31</label><figDesc>Relation-Aware Evidence Detector. When verifying the medical entity relation, the experts with medical domain knowledge can quickly focus on the right part of the paragraphs or sentences. However, the general evidence aggregator and verification model are unable to perceive the above medical domain knowledge. We propose a relation-aware evidence detector to focus on the right part of the paragraphs when looking for the evidence.First, we generate the claim ğ‘ by concatenating the disease entity ğ‘’ ğ‘‘ , target entity ğ‘’ ğ‘¡ and relation type ğ‘Ÿ (ğ‘Ÿ âˆˆ R).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>= ERNIE(ğ‘, ğ‘’ ğ‘— ) (8) Then, we incorporate the medical ontology graph G to get representation ğ‘» ( ğ‘—) ğ‘£ , which can enhance the inference ability for textual entailment by utilizing medical ontology knowledge. The construction of G is described in 4.1.3. ğ‘» ( ğ‘—) ğ‘£ = ğ‘˜ âˆˆN ğ‘— ğ‘” ğ‘—ğ‘˜ ğ‘» (ğ‘˜) ğ‘£ (9) where ğ‘” ğ‘—ğ‘˜ âˆˆ G indicates the ontology relation between the disease entity ğ‘’ ( ğ‘—) ğ‘‘ and ğ‘’ (ğ‘˜) ğ‘‘ , N ğ‘— is set of neighbor nodes of ğ‘’ ( ğ‘—)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>3 https://github.com/PaddlePaddle/Paddle</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Summary Statistics of the Experimental Dataset</figDesc><graphic url="image-6.png" coords="7,317.96,86.61,242.10,190.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Annotation efficiency comparisons of Interactive Collaborative-Training and Random Sampling.</figDesc><graphic url="image-7.png" coords="8,348.69,499.79,176.54,123.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Visualization of the results on our proposed verification framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5</head><label>5</label><figDesc>Figure 5 is a translated English version of Baidu CDSS. The original Chinese version of Baidu CDSS is shown in Figure 6. All medical entity relations displayed in the Baidu CDSS are validated by our verification system. The displayed evidence is first retrieved by synonym-aware sentence selection module and then evaluated by the evidence discriminator. With the assistance of our medical verification system, the doctor in the hospital can make a more reliable diagnosis with evidence retrieved from the authoritative clinical materials.</figDesc><graphic url="image-8.png" coords="10,320.07,293.70,114.67,231.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Medical Entity Verification Deployment on Baidu Clinical Decision Support System (CDSS).</figDesc><graphic url="image-9.png" coords="10,447.96,295.68,114.34,108.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Medical Entity Verification Deployment on Baidu Clinical Decision Support System (Original Chinese Version)</figDesc><graphic url="image-10.png" coords="10,317.96,490.03,247.17,130.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Data Structure for Document Retrieval.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Example of Synonymous Entity-Sentence Set. The text in italic is the synonym of the target entity.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>One iteration of Interactive Collaborative-Training Input: training sets ğ‘ˆ , ğ¿; evidence discriminator ğ· ğœ™ ; base MSRV model ğ‘€ ğœƒ ; thresholds ğœ–, ğ›¿; number of assigned evidence ğ‘›; Output: trained evidence discriminator ğ· * ğœ™ ; trained base model ğ‘€ * ğœƒ ; updated training set ğ‘ˆ , ğ¿ 1: Train ğ· ğœ™ on ğ¿; Train ğ‘€ ğœƒ on ğ‘ˆ , ğ¿; 2: Initialize ğ¿ â€² = âˆ…; 3: for each (claim, evidence) âˆˆ ğ‘ˆ do ğ¿ â€² = sort(ğ¿ â€² , ğ‘›); // sort ğ¿ â€² and select top ğ‘› ğ¿ = ğ¿ âˆª ğ¿ â€² , ğ‘ˆ = ğ‘ˆ \ğ¿ â€² 16: return ğ‘€</figDesc><table><row><cell>4:</cell><cell>Acquire evidence score s e via Eq. (13);</cell></row><row><cell>5:</cell><cell>Acquire relation-aware coefficient ğ›¼ via Eq. (7);</cell></row><row><cell>6: 7:</cell><cell>if Entropy(s e ) â‰¥ ğœ– then Add the (claim, evidence, ğ›¼) to ğ¿ â€²</cell></row><row><cell>8:</cell><cell></cell></row><row><cell>9:</cell><cell>Assign ğ¿ â€² to medical experts for annotation;</cell></row><row><cell>10:</cell><cell>end if</cell></row><row><cell>11:</cell><cell>if Entropy(s e ) â‰¤ ğ›¿ then</cell></row><row><cell>12:</cell><cell>Refine the evidence label of (claim, evidence) in ğ‘ˆ ;</cell></row><row><cell>13:</cell><cell>end if</cell></row><row><cell cols="2">14: end for</cell></row><row><cell>15:</cell><cell></cell></row></table><note>-Training Process. During training, two data pools are maintained and denoted as ğ‘ˆ (unlabeled data) and ğ¿ (labeled data). Note that both ğ‘ˆ and ğ¿ have golden labels for relation verification, while only ğ¿ has golden labels for evidence. At each iteration, ğ· ğœ™ is trained on ğ¿, and the MSRV model ğ‘€ ğœƒ is trained Algorithm 1 * ğœƒ , ğ· * ğœ™ , ğ‘ˆ , ğ¿</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Performance comparisons with the different baseline models on offline dataset. (threshold ğœ ğ‘  &gt; 0.8).</figDesc><table><row><cell>Method</cell><cell>Precision</cell><cell>Recall</cell><cell>F1-Score</cell></row><row><cell>MSRV (ED + EA + SA)</cell><cell>0.947</cell><cell>0.768</cell><cell>0.848</cell></row><row><cell>MSRV (ED + EA + SA) + ICT</cell><cell>0.963</cell><cell>0.762</cell><cell>0.851</cell></row><row><cell>Human</cell><cell>0.989</cell><cell>0.937</cell><cell>0.962</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Performance comparisons of Online Evaluation.</figDesc><table /><note>(threshold ğœ ğ‘  &gt; 0.8).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4</head><label>4</label><figDesc>shows the performance comparison with different methods on the test dataset. We observe that the FastText performs the worst, which is reasonable as it does not utilize any evidence with medical domain knowledge. The performance of the synonym model is better than FastText since it captures the synonym and implicit relation in sentences. The threshold of the synonym model ğœ ğ‘  is set to 0.8. The KGAT achieves better performance at a precision of 0.792 and a recall of 0.714 compared to GEAR. The GEAR and KGAT are very promising general frameworks for fact verification, however, due to without considering the practical challenges in the medical domain, both of them perform less effectively in this medical entity relation verification task. For our MSRV model, we set up two sets of comparative experiments. The first two models do not use synonymaware sentence selection module. From table4, we can see that the precision of the MSRV model with both relation-aware evidence detector (ED) and medical ontology-enhanced evidence aggregator (EA) reaches 0.841, which is better than using an evidence aggregator alone. It means that our relation-aware evidence detector is useful for improving precision. As for the latter two experiments, MSRV (EA only + SA) utilizes a synonym-aware sentence selection module to achieve the recall of 0.822, which is higher than MSRV (EA only). It indicates that the SA module can help improve the recall. We observe that the MSRV (ED + EA + SA) performs the best precision of 0.847 and the best F1-score of 0.831, which proves that our proposed method is superior to other methods for medical relation verification. After the training of MSRV (ED + EA + SA), we adopt two iterations of ICT to improve the accuracy of retrieved evidence. The ICT method not only can improve evidence accuracy but also can improve the overall performance of the relation verification. Noting that for a fair comparison, we only compare the MSRV (ED + EA + SA) with other baseline models, as the procedure of ICT involved extra evidence label information. Two case study examples are shown in the appendix due to the space limit.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Performance comparisons on the efficiency of medical relation annotation.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://01.baidu.com/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">http://www.a-hospital.com/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>Our work is supported by the National Key Research and Development Program of China No.2020AAA0109400.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Naturalli: Natural logic inference for common sense reasoning</title>
		<author>
			<persName><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="534" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Enriching Word Vectors with Subword Information</title>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="135" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Reading Wikipedia to Answer Open-Domain Questions</title>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1870" to="1879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Knowledge abstraction matching for medical question answering</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingbo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenhui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengliang</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BIBM. IEEE</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="342" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Enhanced LSTM for Natural Language Inference</title>
		<author>
			<persName><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Inkpen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1657" to="1668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The text in Figure 5 is translated from Chinese to English. An original Chinese version of Baidu CDSS is also shown in Appendix</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">MedTruth: A Semi-supervised Approach to Discovering Knowledge Condition Information from Multi-Source Medical Data</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="719" to="728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hierarchical multi-task word embedding learning for synonym prediction</title>
		<author>
			<persName><forename type="first">Hongliang</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shulong</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="834" to="842" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">UKP-Athene: Multi-Sentence Textual Entailment for Claim Verification</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Hanselowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zile</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniil</forename><surname>Sorokin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Schiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudia</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fact Extraction and VERification Workshop</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="103" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Exploiting Sentence Embedding for Medical Question Answering</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xien</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Lv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="938" to="945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">DeSePtion: Dual Sequence Prediction and Adversarial Examples for Improved Fact-Checking</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hidey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tuhin</forename><surname>Chakrabarty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tariq</forename><surname>Alhindi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Varia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kriste</forename><surname>Krstovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Smaranda</forename><surname>Muresan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8593" to="8606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Team SWEEPer: Joint Sentence Extraction and Fact Checking with Pointer Networks</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hidey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fact Extraction and VERification (FEVER) Workshop</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="150" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">HÃ©rve</forename><surname>JÃ©gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03651</idno>
		<title level="m">Fasttext. zip: Compressing text classification models</title>
				<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Finegrained fact verification with kernel graph attention network</title>
		<author>
			<persName><forename type="first">Zhenghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7342" to="7351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">QED: A fact verification system for the FEVER shared task</title>
		<author>
			<persName><forename type="first">Jackson</forename><surname>Luken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanjiang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fact Extraction and VERification (FEVER) Workshop</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="156" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Combining fact extraction and verification with neural semantic matching networks</title>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haonan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6859" to="6866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A Self-Training Method for Machine Reading Comprehension with Soft Evidence Extraction</title>
		<author>
			<persName><forename type="first">Yilin</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangkai</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mantong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3916" to="3927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A Decomposable Attention Model for Natural Language Inference</title>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oscar</forename><surname>TÃ¤ckstrÃ¶m</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<meeting><address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2249" to="2255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ Questions for Machine Comprehension of Text</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Simple BM25 extension to multiple weighted fields</title>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Zaragoza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Active learning literature survey</title>
		<author>
			<persName><forename type="first">Burr</forename><surname>Settles</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>University of Wisconsin-Madison Department of Computer Sciences</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">ERNIE 2.0: A Continual Pre-Training Framework for Language Understanding</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shikun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Hao Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<biblScope unit="page" from="8968" to="8975" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">FEVER: a Large-scale Dataset for Fact Extraction and VERification</title>
		<author>
			<persName><forename type="first">James</forename><surname>Thorne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Christodoulopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arpit</forename><surname>Mittal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="809" to="819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The Fact Extraction and VERification Shared Task</title>
		<author>
			<persName><forename type="first">James</forename><surname>Thorne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oana</forename><surname>Cocarascu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Christodoulopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arpit</forename><surname>Mittal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fact Extraction and VERification Workshop</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP Workshop</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="353" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Generative Adversarial Regularized Mutual Information Policy Gradient Framework for Automatic Diagnosis</title>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingbo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenhui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AAAI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1062" to="1069" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5754" to="5764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Ucl machine reading group: Four factor framework for fact finding</title>
		<author>
			<persName><forename type="first">Takuma</forename><surname>Yoneda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fact Extraction and VERification (FEVER) Workshop</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="97" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Medical exam question answering with large-scale reading comprehension</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xien</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5706" to="5713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A collaborative learning framework to tag refinement for points of interest</title>
		<author>
			<persName><forename type="first">Jingbo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shan</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renjun</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Airong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1752" to="1761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">GEAR: Graph-based Evidence Aggregating and Reasoning for Fact Verification</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changcheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="892" to="901" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
