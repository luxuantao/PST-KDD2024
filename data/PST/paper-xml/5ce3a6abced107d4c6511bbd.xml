<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unified rational protein engineering with sequence-only deep representation learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ethan</forename><forename type="middle">C</forename><surname>Alley</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Wyss Institute for Biologically Inspired Engineering</orgName>
								<orgName type="institution" key="instit2">Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Grigory</forename><surname>Khimulya</surname></persName>
							<affiliation key="aff1">
								<address>
									<postCode>02138</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Surojit</forename><surname>Biswas</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Wyss Institute for Biologically Inspired Engineering</orgName>
								<orgName type="institution" key="instit2">Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mohammed</forename><surname>Alquraishi</surname></persName>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Department of Systems Biology</orgName>
								<orgName type="department" key="dep2">Harvard Medical School</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">George</forename><forename type="middle">M</forename><surname>Church</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Wyss Institute for Biologically Inspired Engineering</orgName>
								<orgName type="institution" key="instit2">Harvard University</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Department of Genetics</orgName>
								<orgName type="institution">Harvard Medical School</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Unified rational protein engineering with sequence-only deep representation learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Rational protein engineering requires a holistic understanding of protein function. Here, we apply deep learning to unlabelled amino acid sequences to distill the fundamental features of a protein into a statistical representation that is semantically rich and structurally, evolutionarily, and biophysically grounded. We show that the simplest models built on top of this uni fied rep resentation (UniRep) are broadly applicable and generalize to unseen regions of sequence space. Our data-driven approach reaches near state-of-the-art or superior performance predicting stability of natural and de novo designed proteins as well as quantitative function of molecularly diverse mutants. UniRep further enables two orders of magnitude cost savings in a protein engineering task. We conclude UniRep is a versatile protein summary that can be applied across protein engineering informatics.</p><p>Protein engineering has the potential to transform synthetic biology, medicine, and nanotechnology. Traditional approaches to protein engineering rely on random variation and screening/selection without modelling the relationship between sequence and function 1,2 . In contrast, rational engineering approaches seek to build quantitative models of protein properties, and use these models to more efficiently traverse the fitness landscape to overcome the challenges of directed evolution <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref> . Such rational design requires a holistic and predictive understanding of structural stability and quantitative molecular function that has not been consolidated in a generalizable framework to date.</p><p>Although the set of engineering-relevant properties might be large, proteins share a smaller set of fundamental features that underpin their function. Current quantitative protein modeling approaches aim to approximate one or a small subset of them. For example, structural approaches, which include biophysical modeling 10 , statistical analysis of crystal structures 10 , and molecular dynamics simulations 11 , largely operate on the basis of free energy and thermostability in order to predict protein function. More data-driven co-evolutionary approaches rely on fundamental evolutionary properties to estimate the statistical likelihood of protein stability or function. While successful in their respective domains, these methods' tailored nature, by way of the features they approximate, limit their universal application. Structural approaches are limited by the relative scarcity of structural data (Supp. Fig. <ref type="figure">1</ref>), computational tractability, or difficulty with function-relevant spatio-temporal dynamics, which are particularly important for engineering 12 , 13,14  . Co-evolutionary methods operate poorly in underexplored regions of protein space (such as low-diversity viral proteins 15 ) and are not suitable for de novo designs. Unlike these approaches, a method that scalably approximates a wider set of fundamental protein features could be deployed in a domain independent manner, bringing a more holistic understanding to bear on rational design.</p><p>Deep learning is a flexible machine learning paradigm that can learn rich data representations from raw inputs. Recently, this flexibility was demonstrated in protein structure prediction, replacing complex informatics pipelines with models that can predict structure directly from sequence <ref type="bibr" target="#b15">16</ref> . Additionally, deep learning has shown success in sub-problems of protein informatics; for example: variant effect prediction 15 , function annotation <ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18</ref> , semantic search 18 , and model-guided protein engineering <ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4</ref> . While exciting advances, these methods are domain-specific or constrained by data scarcity due to the high cost of protein characterization.</p><p>On the other hand, there is a plethora of publicly available raw protein sequence data. The number of such sequences is growing exponentially 19 , leaving most of them uncharacterized (Supp. Fig. <ref type="figure">1</ref>) and thus difficult to use in the modeling paradigms described above. Nevertheless, these are sequences from extant proteins that are putatively functional, and therefore may contain valuable information about stability, function, and other engineering-relevant properties. Indeed, previous works have attempted to learn raw sequence-based representations for subsequences 20 , and full-length "Doc2Vec" protein representations specifically for protein characteristic prediction 21 . However, these methods have neither been used to learn general representations at scale nor been evaluated on a comprehensive collection of protein informatics problems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Here, we use a recurrent neural network to learn statistical representations of proteins from ~24 million UniRef50 22 sequences (Fig. <ref type="figure" target="#fig_0">1a</ref>). Without structural or evolutionary data, this uni fied rep resentation (UniRep) summarizes arbitrary protein sequences into fixed-length vectors approximating fundamental protein features (Fig. <ref type="figure" target="#fig_0">1b</ref>). This method scalably leverages underutilized raw sequences to alleviate the data scarcity constraining protein informatics to date, and achieves generalizable, superior performance in critical engineering tasks from stability, to function, to design. amino acid sequences. The model was trained to perform next amino acid prediction (minimizing cross-entropy loss), and in so doing, was forced to learn how to internally represent proteins. b. During application, the trained model is used to generate a single fixed-length vector representation of the input sequence by globally averaging intermediate mLSTM numerical summaries (the hidden states). A top model (e.g. a sparse linear regression or random forest) trained on top of the representation, which acts as a featurization of the input sequence, enables supervised learning on diverse protein informatics tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>An mLSTM learns semantically rich representations from a massive sequence dataset Multiplicative Long-Short-Term-Memory (mLSTM) Recurrent Neural Networks (RNNs) can learn rich representations for natural language, which enable state-of-the-art performance on critical tasks <ref type="bibr" target="#b22">23</ref> . This architecture learns by going through a sequence of characters in order, trying to predict the next one based on the model's dynamic internal "summary" of the sequence it has seen so far (its "hidden state"). During training, the model gradually revises the way it constructs its hidden state in order to maximize the accuracy of its predictions, resulting in a progressively better statistical summary, or representation , of the sequence.</p><p>We trained a 1900-hidden unit mLSTM with amino acid character embeddings on ~24 million UniRef50 amino acid sequences for ~3 weeks on 4 Nvidia K80 GPUs (Methods). To examine what it learned, we interrogated the model from the amino acid to the proteome level and examined its internal states.</p><p>We found that the amino-acid embeddings (Methods) learned by UniRep contained physicochemically meaningful clusters (Fig. <ref type="figure" target="#fig_1">2a</ref>). A 2D t-Distributed Stochastic Neighbor Embedding <ref type="bibr" target="#b23">24</ref> (t-SNE) projection of average UniRep representations for 53 model organism proteomes (Supp. Table <ref type="table">1</ref>) showed meaningful organism clusters at different phylogenetic levels (Fig. <ref type="figure" target="#fig_1">2b</ref>), and these organism relationships were maintained at the individual protein level (Supp. Fig. <ref type="figure" target="#fig_1">2</ref>).</p><p>To assess how semantically related proteins are represented by UniRep, we examined its ability to partition structurally similar sequences that share little sequence identity, and enable unsupervised clustering of homologous sequences.</p><p>UniRep separated proteins from various Structural Classification of Proteins (SCOP) classes derived from crystallographic data (Fig. <ref type="figure" target="#fig_1">2c</ref>, Methods). More quantitatively, a simple Random Forest Model trained on UniRep could accurately group unseen proteins into SCOP superfamily and fold classes (Supp. Table <ref type="table">2</ref>, Methods).</p><p>We next sourced two expertly labeled datasets of protein families compiled on the basis of functional, evolutionary, and structural similarity: HOMSTRAD <ref type="bibr" target="#b24">25</ref> (3450 proteins in 1031 families) and OXBench <ref type="bibr" target="#b25">26</ref> (811 proteins in 180 families). Using Euclidean distances between UniRep vectors, we performed unsupervised hierarchical clustering of proteins in these families, and found good agreement with expert assignments according to three standard clustering metrics. We compared to baselines which include global sequence alignment distance computed with the Levenshtein algorithm, which is equivalent to the standard Needleman-Wunsch with equal penalties <ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28</ref> (Fig. <ref type="figure" target="#fig_1">2d</ref>, Supp. Figs. <ref type="figure" target="#fig_2">3-5</ref>, Methods).</p><p>We finally examined correlations of the internal hidden states with protein secondary structure on a number of datasets (Methods). Surprisingly, we found a single neuron that discriminated beta sheets from alpha helices, positively correlating with alpha-helix annotations (Pearson's r = .33, p &lt; 1e-5), and negatively correlating with beta-sheet annotations (Pearson's r = -0.35, p &lt; 2e-6). Examination of its activation pattern on the Lac repressor structure visually confirmed these statistics (Fig. <ref type="figure" target="#fig_1">2e</ref>). A larger-scale spatial analysis performed across many helices and sheets from different proteins revealed an activation pattern of the helix-sheet neuron that indicated it encodes features of both secondary structure units, going beyond individual amino acids (Fig. <ref type="figure" target="#fig_1">2f</ref>). We found other neuron correlations with biophysical parameters including solvent accessibility (Supp. Fig. <ref type="figure">6</ref>). Taken together, we conclude the UniRep vector space is semantically rich, and encodes structural, evolutionary, and functional information.  <ref type="bibr">21</ref> , a deep structural method from AlQuraishi (2018) <ref type="bibr" target="#b15">16</ref> , Levenshtein (global sequence alignment) distance, and the best of a suite of machine learning baselines (Methods). Scores how well each approach reconstitutes expert-labeled family groupings from OXBench and HOMSTRAD. All metrics vary between 0 and 1, with 0 being a random assignment and 1 being a perfect clustering (Methods). e. Activation pattern of the helix-sheet secondary structure neuron colored on the structure of the Lac repressor LacI (PDB:2PE5, right). f. Average helix-sheet neuron (as visualized in f) activation as a function of relative position along a secondary structure unit (Methods).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UniRep enables stability prediction and generalizes to de novo designed proteins</head><p>Protein stability is a fundamental determinant of protein function and a critical engineering endpoint that affects the production yields <ref type="bibr" target="#b29">30</ref> , reaction rates <ref type="bibr" target="#b30">31</ref> , and shelf-life <ref type="bibr" target="#b31">32</ref> of protein catalysts, sensors, and therapeutics. Therefore, we next sought to evaluate whether UniRep could provide a suitable basis for stability prediction.</p><p>Toward this end, we analyzed a large dataset of stability measurements for de novo designed mini proteins <ref type="bibr" target="#b4">5</ref> . For proper model comparison we withheld a random test set never seen during training, even for model selection (Methods). We compared simple sparse linear models trained on top of UniRep (Fig. <ref type="figure" target="#fig_0">1b</ref>) to those trained on a suite of baseline representations selected to encompass simple controls, standard models known to generalize well, and published state of the art. Among others, they included standard machine learning methods like bag-of-words, the state-of-the-art Doc2Vec representation from Yang et al.</p><p>(2018) <ref type="bibr">21</ref> , and a deep structural representation from the Recurrent Geometric Network (RGN) <ref type="bibr" target="#b15">16</ref> (Supp. Fig. <ref type="figure">7</ref>, Methods). For this analysis, we also generated "UniRep Fusion" by concatenating the UniRep representation with other internal states of the mLSTM to obtain an expanded version of the representation (Methods, Supp. Table <ref type="table" target="#tab_1">3</ref>).</p><p>We also benchmarked against Rosetta, an established structural stability prediction method, using published Rosetta total energy scores for a subset of proteins in this dataset <ref type="bibr" target="#b4">5</ref> . Despite lacking the explicit physical knowledge and structural data that Rosetta relies on, UniRep Fusion with a top model trained on experimental stability data significantly outperformed Rosetta on rank-order correlation with measured stability on the held-out test set (Spearman's œÅ = 0.59 vs. 0.42, Fig. <ref type="figure" target="#fig_2">3a</ref>, Methods). Unlike UniRep, Rosetta does not provide a mechanism to incorporate our experimental stability data, which we recognize is a limitation of this comparison. UniRep Fusion additionally outperformed all baselines in our suite on the test dataset (Supp. Table <ref type="table" target="#tab_3">4</ref>-5, Methods). Due to Rosetta's large computational requirements <ref type="bibr" target="#b32">33</ref> we did not extend this baseline to further analyses.</p><p>This result was surprising because de novo designed proteins constitute a miniscule proportion (1e-7) of the UniRep training data <ref type="bibr" target="#b33">34</ref> . Thus, we further evaluated UniRep's performance on de novo designs compared directly to natural proteins by using 17 distinct deep mutational scanning (DMS) datasets, which provide uniform measurements of stability of 3 natural and 14 de novo designed proteins <ref type="bibr" target="#b4">5</ref> . These datasets consist of single-residue mutants, which presents an additional challenge for UniRep trained exclusively on sequences with &lt;50% similarity. As before, a random test set was withheld for each protein.</p><p>We found UniRep Fusion-based models outperformed all baselines on both natural and de novo designed protein test sets. Surprisingly, the 3 proteins with the best test performance (EEHEE_rd3_0037, HHH_rd2_0134, HHH_0142 measured by Pearson's r ) were de novo designed (Fig. <ref type="figure" target="#fig_2">3b</ref>, validation in Supp. Fig. <ref type="figure">8</ref>). We confirmed these findings with a pooled analysis to train and predict on all subsets (Supp. Table 4-5). Further, we found that the same features of the representation were consistently identified by the linear model (Fig. <ref type="figure" target="#fig_2">3c</ref>), suggesting a learned basis that is shared between de novo designed and naturally occurring proteins. Strikingly, the helix-sheet neuron, evaluated earlier on natural proteins (Fig. <ref type="figure" target="#fig_1">2f</ref>), detected alpha helices on the de novo designed protein HHH_0142 (PDB: 5UOI) (Fig. <ref type="figure" target="#fig_2">3d</ref>). Together, these results suggest that UniRep approximates a set of fundamental biophysical features shared between all proteins.</p><p>UniRep enables prediction of the functional effects of single mutations for eight diverse proteins with distinct functions Because UniRep enables prediction of stability, we hypothesized it could be a basis for the prediction of protein function directly from sequence. To test this hypothesis, we first sourced 9 diverse quantitative function prediction DMS datasets, incorporating data from 8 different proteins <ref type="bibr" target="#b34">35</ref> . Each of these datasets only included single point mutants of the wild-type protein (&gt;99% similarity), which were characterized with molecular assays specific to each protein function <ref type="bibr" target="#b34">35</ref> . For each protein dataset, we asked if a simple sparse linear model trained on UniRep representations could predict the normalized (to wildtype) quantitative function of held out mutants.</p><p>On all 9 DMS datasets, UniRep Fusion-based models achieved superior test set performance, outperforming a comprehensive suite of baselines including a state-of-the-art Doc2Vec representation (Fig. <ref type="figure" target="#fig_2">3e</ref>, Supp. Table <ref type="table" target="#tab_5">4-5</ref>). This is surprising given that these proteins share little sequence similarity (Fig. <ref type="figure" target="#fig_2">3f</ref>), are derived from 6 different organisms, range in size (264 aa -724 aa), vary from near-universal (hsp90) to organism-specific (Gb1), and take part in diverse biological processes (e.g. catalysis, DNA binding, molecular sensing, protein chaperoning) <ref type="bibr" target="#b34">35</ref> . UniRep's consistently superior performance, despite each protein's unique biology and measurement assay, suggests UniRep is not only robust, but also must encompass features that underpin the function of all of these proteins. UniRep performance compared to a suite of baselines across 17 proteins in the DMS stability prediction task (Pearson's r ). UniRep Fusion achieved significantly higher Pearson's r on all subsets ( p &lt; 0.006; Welch's t-test on bootstrap replicates). c. Average magnitude neuron activations for de novo designed and natural protein stability prediction show significant co-activation (p &lt; 0.01; permutation test). d. Activations of the helix-sheet neuron colored onto the de novo designed protein HHH_0142 (PDB: 5UOI). e. UniRep Fusion achieves statistically lower mean squared error than a suite of baselines across a set of 8 proteins with 9 diverse functions in the DMS function prediction task (p&lt;0.0002 on 8/9 and p&lt;0.009 on 1/9; Welch's t-test on bootstrap replicates). f . UniRep performance across orders of scale: from distant proteins, to variants of the same protein one mutation apart. Scale bar illustrates the average distance between proteins in the DMS function prediction dataset. The scale of one mutation (10x magnification from the average distance bar) is shown to contrast the small mutational range UniRep is asked to model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UniRep enables generalization through accurate approximation of the fitness landscape</head><p>A core challenge of rational protein engineering is building models which generalize from local data to distant regions of sequence space where more functional variants exist. Deep learning models often have difficulty generalizing outside of their training domain <ref type="bibr" target="#b35">36</ref> . Unlike most models, which are only trained with data specific to the task at hand, UniRep was trained in an unsupervised manner on a wide sampling of proteins. Combined with its relative compactness (Supp. Fig. <ref type="figure">S9</ref>), we therefore hypothesized UniRep might capture general features of protein fitness landscapes which extend beyond task-specific training data.</p><p>To test this, we focused on fluorescent proteins, which have previously measured fitness landscapes <ref type="bibr" target="#b36">37</ref> , and are useful for in vivo and in situ imaging, calcium and transmembrane voltage sensing, and optogenetic actuation <ref type="bibr" target="#b37">38</ref> . We tested UniRep's ability to accurately predict the phenotype of distant functional variants of Aequorea victoria green fluorescent protein (avGFP) given only local phenotype data from a narrow sampling of the avGFP fitness landscape <ref type="bibr" target="#b36">37</ref> .</p><p>We considered what sized region of sequence space would make the best training data for UniRep. Training on a broad sequence corpus, like UniRef50, captures global determinants of protein function, but sacrifices fine-grained local context. On the other hand, training on a local region of extant sequences near the engineering target provides more nuanced data about the target, but neglects global features. Therefore, we hypothesized that an effective strategy may be to start with the globally trained UniRep and then fine-tune it to the evolutionary context of the task (Fig. <ref type="figure">4a</ref>). To perform evo lutionary fine-tuning (which we call "evotuning"), we ran ~13k unsupervised weight updates of the UniRep mLSTM (Evotuned UniRep) performing the same next-character prediction task on a set of ~25k likely evolutionarily related sequences obtained via JackHMMER search (Methods). We compared this to the untuned, global UniRep as well as a randomly initialized UniRep architecture trained only on local evolutionary data (Evotuned Random; Fig. <ref type="figure">4a, Methods</ref>).</p><p>Using these trained unsupervised models we generated representations for the avGFP variant sequences from Sarkisyan et al. (2016) <ref type="bibr" target="#b36">37</ref> and trained simple sparse linear regression top models on each to predict avGFP brightness. We then predicted the brightness of 27 functional homologs and engineered variants of avGFP sourced from the FPbase database <ref type="bibr" target="#b38">39</ref> using each of the models. The sequences in this generalization set were 2 to 19 mutations removed from avGFP, and were not present in the set of sequences used for evotuning.</p><p>All three representation-based models correctly predicted most of the sequences in the generalization set to be bright (Fig. <ref type="figure">4b</ref>), with the Evotuned UniRep based model having the best classification accuracy (26/27 correctly classified). As a control, we predicted the brightness of 64,800 variants, each of which harbored 1-12 mutations relative to a generalization set member. We assumed variants with 6-12 mutations were non-functional based on empirical observation of the avGFP fitness landscape <ref type="bibr" target="#b36">37</ref> . Indeed, variants with 6-12 mutations were predicted to be non-functional on average, confirming that these representation + avGFP top models were both sensitive and specific even on a distant test set (Fig. <ref type="figure">4b</ref>). Figure <ref type="figure">4c</ref> further illustrates that, on average, the representation + avGFP top models predicted decreasing brightness under increasing mutational burden. In particular, the Evotuned UniRep-based predictions for generalization set members closely matched the empirical brightness-vs-mutation curve for that of avGFP 37 (Pearson's r=.98). These observations imply that each model predicts generalization set members sit at a local optima in the fitness landscape. Furthermore, Evotuned UniRep, unlike the Evotuned Random, predicted a non-linear decline in brightness indicative of predicted epistasis, which is consistent with previous theoretical and empirical work on epistasis in protein fitness landscapes <ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41</ref> . These data suggest that UniRep-based models generalize by building good approximations of the distant fitness landscape using local measurements alone.</p><p>We confirmed these findings with additional analysis of generalization for distinct prediction tasks (Supp. Fig. <ref type="figure" target="#fig_0">10</ref>). We show that UniRep does not require evotuning to enable generalizable prediction of stability (Supp. Fig 10c <ref type="figure">)</ref>, even if forced to extrapolate from nearby proteins to more distant ones (Supp. Fig <ref type="figure" target="#fig_0">10d</ref>). We also provide data from a variant effect prediction task, which enumerates the requirements --such as standardized phenotype measurement --of an appropriate generalization task for a sequence-only model like UniRep (Supp. Fig. <ref type="figure" target="#fig_0">10b</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>To better understand how</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evotuning of UniRep increases efficiency and decreases cost in a fluorescent protein engineering problem</head><p>We hypothesized that UniRep's generalizable function prediction could enable the discovery of functional diversity and function optimizationthe ultimate goals of any protein engineering effort. We constructed a sequence prioritization problem in which the representation + avGFP top models were tasked with prioritizing the 27 truly bright homologs from the generalization set over the 32,400 likely non-functional sequences containing 6-12 mutations relative to a member of the generalization set. We tested each model's ability to prioritize the brightest GFP in the dataset under differently sized testing budgets, and its ability to recover all bright members with high purity. "Brightest sequence recovery" measures a model's utility for function optimization. "Bright diversity recovery," as measured by statistical recall, measures the ability of each model to capture functional diversity, which could represent engineering endpoints or substrates for directed evolution. As a lower-bound model, we defined a null model that orders sequences randomly, and as a upper-bound model, we defined an ideal model that prioritizes sequences perfectly according to ground truth. As a baseline, we trained avGFP top models on Doc2Vec representations 21 (Methods).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figures 4d,e illustrate each representation + avGFP top model performance in terms of recall and prioritization</head><p>of sequences by brightness. All models outperformed the null model in terms of recall and brightest sequence recovery. While the best performing Doc2Vec baseline outperformed untuned UniRep in both tasks, the various Doc2Vec baselines were unreliable in performance (Supp. Fig. <ref type="figure" target="#fig_0">11</ref>) in a manner that was not explainable by their expressivity, architecture, or training paradigm. Evotuned UniRep demonstrated superior performance in both the function diversity and function optimization tasks, having near ideal recall for small (&lt;30 sequence) testing budgets and quickly recovering the brightest sequence in the generalization set of &gt;33,000 sequences with fewer than 100 sequences tested (Fig. <ref type="figure">4d, red</ref>). The global information captured by UniRep was critical for this gain as evotuning from a random initialization of the same architecture yielded inferior performance (Fig. <ref type="figure">4d</ref>, yellow). More concretely, for small plate-scale testing budgets of 10-96 sequences, Evotuned Unirep achieved a ~3-5x improvement in recall and a ~1.2-2.2x improvement in maximum brightness captured over the best Doc2Vec baseline at the same budget. Conditioning on a desired performance level, Evotuned UniRep achieved 80% recall within the first ~60 sequences tested, which would cost approximately $3,000 to synthesize. By contrast, the next best approach examined here would be 100x more expensive (Supp. Fig. <ref type="figure" target="#fig_1">12</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 4.</head><p>UniRep, fine-tuned to a local evolutionary context, facilitates protein engineering by enabling generalization to distant peaks in the sequence landscape. a. We hypothesize UniRep trades-off nuance for universality in a theoretical protein engineering task. By unsupervised training on a subspace of sequences related to the engineering target --"evotuning" --UniRep representations are honed to the task at hand. b. Predicted brightness of 27 homologs and engineered variants of avGFP under various representations + sparse linear regression models trained only on local avGFP data. Box and whisker plots indicate predicted distribution of dark negative controls. Green region above dotted line is predicted bright, below is predicted dark. On the left in gray is the training distribution from local mutants of avGFP. c. Predicted brightness-vs-mutation curves for each of the 27 avGFP homologs and engineered variants (the generalization set). Each grey line depicts the average predicted brightness of one of the 27 generalization set members as an increasing number of random mutations is introduced. Red line shows the average empirical brightness-vs-mutation curve for avGFP <ref type="bibr" target="#b36">37</ref> . d. Recall vs. sequence testing budget curves for each representation + sparse linear regression top model (bottom). Efficiency gain over random sampling (top) depicted as the ratio of a method's recall divided by the recall of the null model as a function of testing budget. e. Maximum brightness observed vs. sequence testing budget curves for each representation + sparse linear regression top model (bottom). Efficiency gain over random sampling analogously defined for recall but instead with normalized maximum brightness (top).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>In this work we used abundant unlabeled protein sequence data to learn a broadly applicable statistical vector representation of proteins. This representation enabled near state-of-the-art or superior performance on more than 20 stability and function prediction tasks (Supp. Table <ref type="table">6</ref>) that reflect the challenges faced by protein engineers. Importantly, these results (Fig. <ref type="figure" target="#fig_2">3</ref>, Supp. Table <ref type="table" target="#tab_5">4-5</ref>) were obtained using the same set of UniRep-parameterized features, which were superior to a suite of non-trivial baseline representations. Given additionally the simplicity of the top-models used and the compactness of UniRep (Supp. Fig. <ref type="figure">9</ref>), this provides strong, albeit indirect, evidence that UniRep must approximate fundamental protein features that underpin stability and a broad array of protein-specific functions.</p><p>A more direct interrogation of UniRep features at the amino acid to whole proteome level revealed that these features at least embody a subset of known characteristics of proteins (Fig. <ref type="figure" target="#fig_1">2</ref>); however, we note the possibility of UniRep representing more. Because UniRep is learned from raw data, it is unconstrained by existing mental models for understanding proteins, and may therefore approximate currently unknown engineering-relevant features. Taken together, these results suggest UniRep is likely to be a rich and robust basis for protein engineering prediction tasks beyond those examined here.</p><p>Remarkably, UniRep achieves these results from "scratch", using only raw sequences as training information. This is done without experimentally determining, or computationally folding, a structural intermediate -a necessary input for alternative methods <ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b41">42</ref> . By enabling rapid generalization to distant, unseen regions of the fitness landscape, UniRep may improve protein engineering workflows, or in the best case, enable the discovery of sequence variants inaccessible to purely experimental or structural approaches. Although the utility of the representation is limited by sampling biases in the sequence data <ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44</ref> length of training, the size <ref type="bibr" target="#b44">45</ref> and coverage <ref type="bibr" target="#b42">43</ref> of sequence databases as well as deep-learning specific computational hardware <ref type="bibr" target="#b45">46</ref> are improving exponentially. Coupled with the continued proliferation of cheap DNA synthesis/ assembly technologies <ref type="bibr" target="#b46">47</ref> , and methods for digitized and multiplexed phenotyping, UniRep-guided protein design promises to accelerate the pace with which we can build biosensors <ref type="bibr" target="#b12">13</ref> , protein <ref type="bibr" target="#b47">48</ref> and DNA binders <ref type="bibr" target="#b48">49</ref> , and genome editing enzymes <ref type="bibr" target="#b49">50</ref> .</p><p>Finally, there are many exciting, natural extensions of UniRep. It can already be used generatively (Supp. Fig. <ref type="figure" target="#fig_2">13</ref>), evoking deep protein design, similar to previous work with small molecules <ref type="bibr" target="#b50">51</ref> . Beyond engineering, our results (Fig. <ref type="figure" target="#fig_1">2b-e</ref>, Supp. Fig. <ref type="figure" target="#fig_1">2</ref>) suggest UniRep distance might facilitate vector-parallelized semantic protein comparisons at any evolutionary depth. Among many straightforward data augmentations (Supp. Table. 7), UniRep might advance ab-initio structure prediction by incorporating untapped sequence information with the RGN <ref type="bibr" target="#b51">52</ref> via joint training <ref type="bibr" target="#b51">52</ref> . Most importantly, UniRep provides a new perspective on these and other established problems. By learning a new basis directly from ground-truth sequences, UniRep challenges protein informatics to go directly from sequence to design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models and training details:</head><p>We approached representation learning for proteins via Recurrent Neural Networks (RNNs). Unlike other approaches to representing proteins, namely as one-hot-encoded matrices as in Biswas et. al 2018 <ref type="bibr" target="#b2">3</ref> , RNNs produce fixed-length representations for arbitrary-length proteins by extracting the hidden state passed forward along a sequence. While padding to the maximum sequence length can in principle mitigate the problem of variable length sequences in a one hot encoding, it is ad-hoc, can add artifacts to training, wastes computation processing padding characters, and provides no additional information to a top model besides the naive sequence. Furthermore, even very large representations, like the 1900 dimensional UniRep, are more compact than average protein length 1-hot encodings (Supp. Fig. <ref type="figure">9</ref>), reducing the potential for overfitting. While Doc2Vec methods produce fixed-length vectors, they have been empirically outperformed by more expressive architectures like RNNs in recent work on representation learning in natural language <ref type="bibr" target="#b22">23</ref> .</p><p>We considered the mLSTM <ref type="bibr" target="#b52">53</ref> , LSTM <ref type="bibr" target="#b53">54</ref> , and Gated Recurrent Unit (GRU) <ref type="bibr" target="#b54">55</ref> for candidate RNN representation learners. After manual explorations comparing these classes, and considering previous work demonstrating the success of the mLSTM for a similar task in natural language <ref type="bibr" target="#b22">23</ref> , we decided to use the mLSTM. Specifically, the architectures selected for large-scale training runs were a 1900-dimensional single layer multiplicative LSTM <ref type="bibr" target="#b52">53</ref> ( ~18.2 million parameters) as described elsewhere <ref type="bibr" target="#b22">23</ref> , a 4-layer stacked mLSTM of 256 dimensions per layer (~1.8 million parameters), and a 4-layer stacked mLSTM with 64 dimensions per layer (~.15 million parameters), all regularized with weight normalization <ref type="bibr" target="#b55">56</ref> . As a matter of definition, we note that because all of these networks are recurrent, even the single hidden layer mLSTM-1900 is considered "deep" because the network is unrolled in the timestep dimension as a composition of hidden layers through time.</p><p>We followed a heuristic that assumes, for large data sets like ours, more expressive models will learn richer representations. Thus, we selected 1900 dimensions in the large single-layer mLSTM because it was approximately the largest dimensionality that could fit in GPU memory after some experimentation. We tried smaller widths (the 256 and 64 dimensions) in case the large number of latent dimensions in the 1900-unit mLSTM led to overfitting on prediction tasks. Our comparison with the mLSTM-1900 suggested this was almost never the case (Supp. Data 3).</p><p>Sequences of amino acids were one-hot encoded and passed through a 10 dimensional amino-acid character embedding before being input to the mLSTM layer. For the smaller stacked networks, both standard recurrent and residual recurrent connections, in which the hidden states of each layer are summed, were evaluated. For these stacked networks, dropout probability was selected from {0, .5}. Hyperparameters were tuned manually on a small number of weight updates and final parameters were selected based on the rate and stability of generalization loss decrease. We found that dropout and residual connections both increased validation set error. We hypothesized that residual connections, which should improve gradient flow to earlier layers, were not advantageous here given the small number of layers tested. We further hypothesized that these networks did not require dropout or other regularization outside of weight normalization because of the high ratio of observations to model parameters.</p><p>All models were trained with the Adam optimizer using truncated-backpropagation through time with initial states initialized to zero at the beginning of sequences and persistent across updates to simulate full backpropagation as described previously <ref type="bibr" target="#b22">23</ref> . Batch sizes and truncation windows were selected to fit into GPU memory and were, respectively, 256 and 128 (mLSTM-1900), 512 and 384 (4x-mLSTM-256), 1028 and 384 (4x-mLSTM-64). Training was performed using data parallelism on 4 Nvidia K-80 GPUs (mLSTM-1900) or 2 Nvidia K-40s (4x-mLSTM-256, 4x-mLSTM-64). The mLSTM-1900 model was trained for ~770K weight updates, or ~3.5 weeks wall clock time, corresponding to ~1 epoch. The 4x-mLSTM-256 and 4x-mLSTM-64 were trained for ~90K weight updates, ~2 days wall clock time, ~3 epochs and 220k weight updates, ~2 days wall clock time, 14 epochs, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Computing vector representations</head><p>The mLSTM architecture has two internal states that encode information about the sequence it is processing, the hidden state and the cell state <ref type="bibr" target="#b52">53</ref> . One hidden state and one cell state are produced for every amino acid in a forward pass over a sequence. Previous work in Natural Language has used the final hidden state (corresponding to the residue at the C terminus in our case) as the sequence representation <ref type="bibr" target="#b22">23</ref> .</p><p>Compared to natural language, we hypothesized that the complexity of protein folding would generate more long-range and higher-order dependencies between amino acids. Therefore, we elected to construct the UniRep representation as the average of the 1900-unit model's hidden states, integrating information across distant amino-acids. We hoped this would better represent long-term dependencies critical to protein function prediction.</p><p>For convenience we named this vector representation --the Average Hidden state of mLSTM-1900 --simply "UniRep" everywhere in the main text.</p><p>We compared the performance of UniRep as defined above with other possibilities for the representation state. We extracted final hidden state produced by the model when predicting the last amino acid in a protein sequence (Final Hidden) and the last internal cell state (Final Cell).</p><p>Curious whether these vectors contained complementary information, we also constructed a concatenation of all 3 representation possibilities (Average Hidden, Final Hidden and Final Cell) from the 1900-unit mLSTM. For convenience, we named this 1900 x 3 dimensional representation "UniRep Fusion" everywhere in the main text. We built various other concatenations of these representations (Supp. Table <ref type="table" target="#tab_1">3</ref>), denoted as "Fusions", to be evaluated in supervised stability and quantitative function prediction.</p><p>We consider UniRep to be the most information-dense representation evaluated here, while UniRep Fusion is the most complete. UniRep, by virtue of its smaller dimensionality, should be deployed where computational resources are constrained. We present UniRep Fusion only for supervised prediction tasks (of protein stability, biophysical characteristics and function).</p><p>For completeness we evaluated the influence of network size in representation performance. There is evidence that larger neural networks learn better representations <ref type="bibr" target="#b22">23</ref> . When we compared UniRep, UniRep Fusion and the other 1900-unit representations defined above to identical representations extracted from 64 and 256-unit models with an identical architecture (training described above, Supp. Table <ref type="table" target="#tab_1">3</ref>) our results agree with this pattern except with very small datasets, which are more variable as expected (Supp. Data 1,3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supervised model benchmarking</head><p>Sourcing and processing analysis datasets: All the datasets (Supp. Table <ref type="table">6</ref>) were obtained from the supplemental information of corresponding publications. Sequence, function pairs were deduplicated, validated to exclude records containing non-standard amino acid symbols, and split randomly into 80-10-10 trainvalidation -test sets as described below. Size is reported after cleaning. When protein sequences could not be found in the published data, they were retrieved from UniProt by whatever sequence identifiers were available, using UniProt ID mapping utility ( https://www.uniprot.org/help/uploadlists ). After this, the same split and top model training analysis was done with all datasets, as described below.</p><p>Inference of RGN representations on a small number (on the order of 10s) of sequences could not be completed due to challenges constructing PSSMs using the JackHMMER program (the multiple sequence alignments were so large they would not fit in available memory). To obtain comparable scores between the performance of RGN and other representations, we dropped these sequences from the datasets used for training and evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Train-Validation-Test Split:</head><p>For most supervised tasks no test set was provided, so we made a 80%-10%-10% Train-Validation-Test split in python using the numpy package and a fixed random seed. The validation set was never used for training so that it could be used to estimate generalization performance while we were conducting experiments and building models. In order to obtain a final, truly held-out, measure of generalization performance we did not look at the test set until all models were finalized and a submission ready version of this manuscript was prepared.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baselines representations -RGN:</head><p>We used the Recurrent Geometric Network (RGN) model trained on the ProteinNet12 dataset <ref type="bibr" target="#b56">57</ref> and assessed on CASP12 structures in AlQuraishi (2018) <ref type="bibr" target="#b15">16</ref> . The model was used as is without additional training or fine-tuning. Instead of using the predicted structures of the RGN, we extracted the internal state learned by the model for each sequence + PSSM combination (3200 dimensions corresponding to the last outputs of two bidirectional LSTMs, each comprised of 800 units per direction). PSSMs were generated in a manner identical to that used for generating ProteinNet12 PSSMs <ref type="bibr" target="#b56">57</ref> .</p><p>We obtained an estimate of the standard deviation of the resulting validation/test metrics by resampling 50% of the validation/test set 30 times and computing an empirical standard deviation of the obtained metrics, and used Welch's t-test for comparisons.</p><p>We were unable to compare to the published state of the art on the 9 DMS function prediction datasets from Gray et al.</p><p>(2017) <ref type="bibr" target="#b34">35</ref> . The exploratory nature of the authors' analysis of these datasets for in-domain prediction led them to use their test set during hyperparameter selection, making their results incomparable with the best machine learning practices we followed here. This is in contrast to out-of-domain/transfer performance, which was the main objective of the Gray et al's (2017) analysis. Unlike in-domain prediction, Gray et al (2017) followed best practices for leave one protein out (LOPO) transfer analysis (see below).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplemental generalization analysis</head><p>Leave-one-protein-out transfer: We evaluated generalization (transfer) learning performance using the standard procedure based on previous foundational work, Glorot et al.  (2011) <ref type="bibr" target="#b60">61</ref> . This particular analysis was done for the quantitative function prediction (including data for 8 different proteins) and for 17 protein DMS stability prediction datasets.</p><p>Briefly, e(S, T) -the transfer error -is defined as the test/validation error achieved by a model trained on the source domain S and evaluated on the target domain T. e(T, T) is similarly defined and is called the in-domain error .</p><p>In the case of quantitative function prediction and stability prediction (natural and de novo ), we used leave-one-protein out approach, constructing a Source/Target split for each protein in the dataset with that protein as the Target and the rest of the proteins as the Source. We used the same linear model we used in in-distribution regression analysis (L1 prior, LARS algorithm). The value of regularization parameter alpha was in turn obtained through leave-one-protein-out cross-validation on the training data separately for each split.</p><p>To control for the differences in difficulty between various splits, we also evaluated the baseline in-domain error -eb(T, T) -the error achieved by a baseline representation when trained and evaluated on the Target domain. We used amino acid frequency and protein length as our baseline representation, and computed eb(T, T) for each of the target domains defined above.</p><p>Transfer ratio is the ratio of transfer error to baseline in-domain error e(S, T)/eb(T, T). We use the average of transfer ratios over all Source/Target splits for a given representation to characterize transfer performance on the given dataset. We also look at the in-domain ratio e(T, T)/eb(T, T), which reflects the in-distribution performance in comparable terms to the transfer ratio.</p><p>We obtained an estimate of the standard deviation of the resulting validation/test metrics by computing an empirical standard deviation of metrics across all the different hold-one-out splits.</p><p>In the case of 17 DMS stability datasets (Supp. Table <ref type="table">6</ref>), we also constructed an additional extrapolation Source/Target split -from central to remote proteins (Supp. Fig. <ref type="figure" target="#fig_0">10d</ref>) as follows. We computed a string median of initial sequences of 17 proteins in these datasets, and selected the 4 proteins with the largest edit distance from the median. We then computed a multidimensional scaling (MDS) 2D plot of the Levenshtein distance matrix of the 17 initial proteins, and selected 4 most peripheral proteins along each axis of the plot (Supp. Fig. <ref type="figure" target="#fig_0">10e</ref>). Together, the DMS datasets for these 8 proteins constituted the Target dataset (also known as test set shown in Red on Supp. Fig. <ref type="figure" target="#fig_0">10e</ref>) to evaluate our generalization/transfer performance. The other 9 datasets served as the Source dataset to be trained on. Once the Source/Target split was defined, the transfer analysis was performed similarly to the above except that we did not average any transfer metrics (because in this case there is only a single Source and Target).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supervised remote homology detection</head><p>Datasets -H√•ndstad fold and superfamily: We used two standard benchmarks based on the SCOP database from H√•ndstad et al. <ref type="bibr" target="#b61">62</ref> : the superfamily-level remote homology detection and the harder fold-level similarity detection.</p><p>Briefly, the superfamily benchmark sets up a binary classification problem for each of the superfamilies in the dataset: a single family in the superfamily presenting a positive test set, the other families in that superfamily serving as the positive training set, a negative test set comprising one random family from each of the other superfamilies, and the negative training set combining the rest of the families in these superfamilies.</p><p>The fold-level benchmark is analogous at the fold level, setting up a classification problem for each of the folds in the dataset: one superfamily in the fold is used as positive test set, the others in that fold serving as the positive training set, a single random superfamily from every other fold comprising a negative test set, and taking the remaining sequences as the negative training set. This structured training/ test set could not be straightforwardly subsampled, so to protect against overfitting we instead held out training and evaluation on the entire fold-level dataset until the model was trained and our Bayesian hyperparameter tuning procedure finalized on the superfamily-level benchmark dataset. Because these are standard benchmarking datasets and the task was computationally expensive, we choose to evaluate only UniRep performance. Many of the more recent published methods for remote homology detection <ref type="bibr" target="#b62">63,</ref><ref type="bibr">64</ref> use PSSMs as a source of evolutionary information, which we excluded for equal comparison to UniRep, which was trained on strictly dehomologized sequences and had no access to local evolutionary information like a PSSM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Binary classification with Random Forests:</head><p>For supervised remote homology benchmarks we used Random Forest implementation from the same scikit-learn package with 1000 estimators and a "balanced" class weight. For each of the binary classification tasks in each benchmark, we conducted a Bayesian hyperparameter optimization as implemented in the skopt package ( https://scikit-optimize.github.io/ ), picking the following hyperparameters: maximum proportion of features to look at while searching for the best split (between 0.5 and 1), function to measure the quality of a split (Gini impurity or information gain), the minimum number of samples required to be at a leaf node (between 0.1 and 0.3), and the minimum number of samples required to split an internal node (between 0.01 and 1) with 75 iterations of optimization, 3-fold cross-validation on the training data and ROC score as the scoring metric.</p><p>To make our model comparable with the scores of previously developed remote homology detection models in the literature, we used two standard metrics -ROC score (normalized area under the receiver operating characteristic curve) and ROC50 (ROC score at the point where the first 50 false positives occur).</p><p>Two out of one hundred and two superfamilies, identified in the data through the name of the family presenting the positive test set -c.2.1.3 and c.3.1.2, and one out of eighty six folds, identified in the data by the name of the superfamily representing the positive test set -c.23.12, were excluded from the consideration due to our failure to obtain a coherent positive/negative train/test split from the published data source <ref type="bibr">64</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unsupervised clustering of distant but functionally related proteins</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Agglomerative clustering of representations and hierarchical clustering confirmation:</head><p>We used basic agglomerative clustering (implemented in python with sklearn <ref type="bibr" target="#b64">65</ref> ) with the average linkage metric and Euclidean distance for all vector representations, and a precomputed Levenshtein distance matrix using the python-Levenshtein package for the sequence-only control.</p><p>To confirm our cluster assignments, we picked a small set of 3 Cytochrome Oxidase (1,2,3) families from OXBench, 8 proteins total. We used the fastcluster package to produce a linkage matrix with average linkage and Euclidean distance (for UniRep) and average linkage and Levenshtein distance, implemented in python-Levenshtein, for the sequence-based control. We visualized the resulting dendrograms using the scipy 66 package.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Goodness of clustering metrics:</head><p>We selected three standard clustering metrics to evaluate the quality of inferred cluster identities compared to the true family classification given by the expert-annotated labels. The metrics we selected were required to be bounded between 0 and 1, to be invariant to the number of families and clusters present in the data, and to be symmetric. We therefore selected Adjusted Mutual Information, Adjusted Rand Index, and Fowlkes Mallows Score for these properties. The definition, usage, and properties of these metrics are described elsewhere <ref type="bibr" target="#b66">67</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fine-tuning to generalize GFP function prediction for protein engineering</head><p>Collecting fluorescent protein homologs with EBI JackHMMer: We sourced Fluorescent Protein sequences from the literature and public databases (Interpro IPR011584, IPR009017; PFAM:PF01353,PF07474 and <ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b68">69</ref> ). We were left with 1135 sequences total after cleaning sequences longer than 1000 AAs or containing invalid letters. We the used the python-Levenshtein package to compute distances from sfGFP. Looking to source sequences of varying dissimilarities from sfGFP, we sampled &lt;100 sequence proximity-ordered subsets, first selecting the most distant subset with probability proportional to the distance from sfGFP, and then continuing iteratively until the most similar set was apportioned (this subset contained less than 100 sequences unlike the rest). We then used the EBI JackHMMer web server <ref type="bibr" target="#b69">70</ref> to batch search each subset, with no more than 20 iterations. Search was stopped after more than 100,000 sequences were discovered or the search converged. We expected this batched approach to generate unique hits within each subset, but we found that after cleaning to remove long (&gt;500 AAs) or invalid seqs, and dropping duplicates with preference to the subset nearest sfGFP, almost all the sequences were discovered by the most nearby subset search. We continued with these 32,225 sequences.</p><p>With the goal of establishing a validation set which was measuring something closer to extrapolation, we took these sequences and recomputed distance with sfGFP. We selected a distance-biased 10% "out of domain" validation set by sampling with probability proportional to the 4th power of the distance (strongly weighting distant examples). We also selected a 10% "in-domain" validation set uniformly randomly from the remaining sequences. This left 25,781 training sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model finetuning with extant GFP sequences:</head><p>We loaded the weights learned by UniRep in the exact same architecture as before, but replacing the final layer, which previously predicted the next character, with a randomly initialized feed-forward layer with a single output and no non-linearity. As a control, we initialized the same randomly. We trained both models with exactly the same procedure: low learning rate (.00001), 128 batch size, and only partially feeding forward along the sequence, stopping prediction after 280 amino acids, with full back propagation rather than truncated as during the UniRef50 training process. This was determined by the computational constraint to fit the unrolled recurrent computational graph into GPU memory; however, we expected this was sufficiently long to capture the context of GFP because the vast majority of well-characterized fluorescent proteins are shorter than 280 AA. Both models trained for ~13,000 weight updates, corresponding to ~65 epochs, and ~1.5 days wall clock time on 1 Nvidia Volta GPU. Stopping was determined by computational constraints, not overfitting as measured by increasing validation set loss, which never occurred.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training LASSO regression on representation featurized sequences from the fitness landscape of avGFP:</head><p>Sequences from Sarkisyan et. al (2016) <ref type="bibr" target="#b36">37</ref> were featurized using a given representation (e.g. UniRep). A sparse LASSO linear regression model was trained using this representation as input. A range of 20 L1 penalties, log-spaced over 6 orders of magnitude, was scanned using 10-fold cross validation. The selected level of regularization was set to be the strongest (most regularizing) penalty that had statistically equal out-of-sample error to the penalty with lowest out-of-sample error across the 10 folds.</p><p>Baseline selection for retrospective fluorescent protein sequence discovery task: Doc2Vec is a standard tool for numerically representing text documents in natural language processing. Yang et al. (2018) use this simple approach to represent protein sequences <ref type="bibr">21</ref> and achieve good supervised function prediction performance with simple models based on their representation. We also found these Doc2Vec representations to be especially appropriate baselines as they were originally validated, in part, by reaching state-of-the-art performance on a rhodopsin absorption wavelength prediction task, which bears similarity to fluorescent prediction task at hand. We did not consider Rosetta an appropriate baseline here for three reasons: 1) Rosetta provides measures of stability, which does not completely define function, 2) the computational requirements to mutate and relax a reference structure for &gt;32,000 sequences, let alone de novo fold, are impractical, and 3) in general, we would not expect to have structures for sequences we have yet to discover, meaning we would need to rely on the distant avGFP as a template.</p><p>We additionally could not apply simple linear regression or Naive Bayes approaches here as those methods require fixed length inputs whereas the protein length sequences examined here were variable length.</p><p>Processing well-characterized GFP sequences from FPBase: A raw collection of 452 fluorescent proteins was sourced from FPbase.org 39 (Collection "FP database" available at https://www.fpbase.org/collection/13/ as of Nov 2, 2018). This was then filtered as follows:</p><p>1) Sequences shorter than 200 amino acids or longer than 280 amino acids were removed.</p><p>2) Remaining sequences were then multiply aligned using ClustalW (gap open penalty=10, gap extension penalty=0.1) <ref type="bibr" target="#b70">71</ref> . BLOSUM62 was used as the scoring matrix. 3) Using these aligned sequences, we computed their pairwise minimum edit distance (insertions/deletions counting as one edit). Sequences that were more than 50 mutations away from their nearest neighbor were removed. These were enriched for circularly permuted FPs and tandem FPs. 4) Remaining that had an average edit distance &gt;180 mutations ( length of avGFP = 238) to all other sequences were also removed. 5) Where possible, His tags and likely linker sequences were manually removed, and start Methionine amino acids were added to all sequences that did not have one. 6) At this point, the major phylogenetic clusters of Anthozoan (corals/anemones) and Hydrozoan (Jellyfish) FPs remained including engineered variants. Note that hydrozoan and anthozoans are almost entirely different, often sharing as little as 30% sequence similarity. The hydrozoan clade was almost entirely composed of FPs found in Aequorea victoria or engineered versions thereof. The set of sequences for the retrospective sequence discovery analysis was therefore set to be all natural or engineered green (emission wavelength between 492 nm and 577 nm) fluorescent proteins of Aequorean descent.</p><p>This set of sequences included well known engineered variants of avGFP including EGFP, mVenus, superfolder GFP, mCitrine, and Clover.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Exploratory analysis and data visualization</head><p>PCA of amino acid embeddings: We extracted embedding vectors for each amino acid from the trained UniRep model. We performed PCA as implemented in the sklearn library and used the three first principal components for the visualization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>t-SNE of organism proteomes:</head><p>We obtained 53 reference proteomes (Supp. Table <ref type="table">1</ref>) of model organisms from UniProt Reference Proteomes. We used the UniRep trained model to obtain representations for each of the proteins in each proteome. We then averaged all the proteins in each proteome to obtain the representation for the "average protein" for each of the organisms. We used t-SNE <ref type="bibr" target="#b23">24</ref> -a common technique for visualizing high-dimensional datasets (as implemented in the sklearn library, with perplexity=12) to obtain a 2D projection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PCA of conserved protein organism relationships:</head><p>To allow embedding of new points into the projected space, we computed a PCA using a subset of better characterized reference proteomes (as labelled in Supp. Fig. <ref type="figure" target="#fig_1">S2</ref>). We manually sourced 5 conserved sets of proteins from Humans, S. cerevisiae, and D. rerio using the OrthoDB <ref type="bibr" target="#b71">72</ref> as well as manual inspection and UniProt characterization data. These proteins were: dihydrofolate reductase (DHFR), methylenetetrahydrofolate reductase (MTHFR), nucleotide excision repair protein (ERCC2), elongation factor (EFTU), Heat-Shock protein 70 (HSP701a). We projected the representations for each of these variants into the space given by the first two Principal Components from the model organism PCA and drew the vector from S. cerevisiae to Human, which we translated so the base sits on the S. cerevisiae variant for each protein.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Correlation of representation with structural features:</head><p>We computed the full sequence of hidden states at every position of 3720 single-domain proteins from the SCOP database <ref type="bibr" target="#b28">29</ref> (for which we had secondary structure annotation for each of the positions obtained from PDB <ref type="bibr" target="#b72">73</ref> ). We proceeded to calculate correlations between elements of these hidden sequences (neurons) and secondary structure. Neuron 341 was a strong helix and beta-sheet detector, obtaining 0.33 correlation with alpha-helicity and -0.35 correlation with beta-sheets.</p><p>We then cut out all continuous helices and beta-sheets with surrounding context (including [30% * the length of the helix/sheet] context amino acids on each side) from the proteins in the database to look at the average activation of the neuron at each relative position in the helix/sheet, obtaining ~14000 alpha helices and 20,000 beta-sheets.</p><p>We then similarly computed all hidden states for 1,500 randomly sampled proteins with available structures from the Protein Data Bank (PDB). We attempted to compute DSSP <ref type="bibr" target="#b73">74</ref> annotations for each structure, keeping only those sequences and corresponding structures such that the DSSP calculation executed without error and DSSP secondary structure amino acid indices lined up with the primary amino acid indices. These calculations succeeded for 448/1,500 structures. After exploratory correlational analysis of various structural features, we decided to focus on solvent accessibility. Without explicit regularization on the hidden state of the LSTM, we felt it was likely that the representation was entangled, with multiple neurons possibly encoding the same biophysically relevant features. We therefore also learned, in a supervised manner, simple and sparse linear combination of neurons that were predictive of solvent accessibility. To train this model we used a position by hidden-sequence dimension matrix as the feature matrix, and solvent accessibility as a response. These were both input to LASSO and the strength of L1 regularization was selected using 10 fold cross validation.</p><p>Single neuron activations or linear combinations thereof were visualized on protein structures using the NGLview Python package <ref type="bibr" target="#b74">75</ref> . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Azotobacter vinelandii</head><note type="other">Bacteria</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Neurospora crassa Eukarya Fungi</head><p>Oryzias latipes Eukarya Animalia Supplemental Figure <ref type="figure" target="#fig_1">2</ref>. Single-protein vector arithmetic in UniRep representation space. We suspected our organism vector clustering success may be explained by learning a measure of proteome content (e.g. abundance of various protein types). Surprisingly, after sourcing 5 proteins conserved across 3 model organisms, we identify a common direction of variance, from Baker's Yeast to Human in the PCA projection space, which corresponds to the vector from Yeast to Human proteome representations, suggesting that organisms may have an arithmetic relationship in the representation space similar to that observed in Word Vectors <ref type="bibr" target="#b75">76</ref> . Note that the direction of the vectors is invariant from the PCA in the upper right to the bottom left, but the length of the vector is meaningless in the bottom left. PC1 is the x-axis of both plots, PC2 is the y axis of both plots.</p><p>Supplemental Figure <ref type="figure">6</ref>. Learned from a collection of PDB secondary structures, a linear combination of hidden state neurons identifies solvent accessible regions in the structure of Bovine Rhodopsin GPCR (PDB:1F88, left) oriented with the extracellular domain upwards (Methods). Can predict solvent accessibility with a Pearson correlation of 0.38.  <ref type="table" target="#tab_5">5</ref>. This table includes an extension of our analysis to 4 small datasets compiled previously for protein phenotype prediction using Doc2Vec representations (first 4 columns) <ref type="bibr">21</ref> . We observed widely variable results and statistically insignificant results (caused by underpowered validation and test set), with UniRep or one of the baselines we developed outperforming previous state-of-the-art <ref type="bibr">21</ref> (here and in Supp. Table <ref type="table" target="#tab_5">S5</ref>), which underscored the importance of adequate data size for accurate estimation of performance. Supp. Table <ref type="table">8</ref> All the models we evaluated, including the baseline suite used for the majority of analyses in the manuscript. We additionally used Levenshtein distance (Needleman-Wunsch where all penalties are equal) for analysis in Fig. <ref type="figure" target="#fig_1">2d</ref> and Rosetta total energy and NPSA measures for Fig. <ref type="figure" target="#fig_2">3a</ref> (as described in Methods under "Stability Ranking Task")</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Representations being concatenated</head><note type="other">UniRep</note></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Workflow to learn and apply deep protein representations. a. UniRep model was trained on 24 million UniRef50 primary</figDesc><graphic url="image-1.png" coords="3,37.50,205.50,535.50,253.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. UniRep encodes amino acid physicochemistry, organism level information, secondary structure, evolutionary and functional information, and higher order structural features . a. PCA (Principal Component Analysis) of amino acid embeddings learned by UniRep. b. t-SNE of the proteome-average UniRep vector of 53 model organisms c. Low dimensional t-SNE visualization of UniRep represented sequences from SCOP colored by ground-truth structural classes, which were assigned after crystallization 29 . d. Agglomerative distance-based clustering of UniRep, a Doc2Vec representation method from Yang et al. (2018)21  , a deep structural method from AlQuraishi (2018)<ref type="bibr" target="#b15">16</ref> , Levenshtein (global sequence alignment) distance, and the best of a suite of machine learning baselines (Methods). Scores how well each approach reconstitutes expert-labeled family groupings from OXBench and HOMSTRAD. All metrics vary between 0 and 1, with 0 being a random assignment and 1 being a perfect clustering (Methods). e. Activation pattern of the helix-sheet secondary structure neuron colored on the structure of the Lac repressor LacI (PDB:2PE5, right). f. Average helix-sheet neuron (as visualized in f) activation as a function of relative position along a secondary structure unit (Methods).</figDesc><graphic url="image-2.png" coords="5,37.50,123.00,540.00,306.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. UniRep predicts structural and functional properties of proteins. a. Spearman correlation with true measured stability rankings of de-novo designed mini proteins UniRep Fusion-based model predictions and two alternative approaches: negative Rosetta total energy and non-polar surface area (Methods). UniRep Fusion outperforms both alternatives ( p &lt; 0.001; Welch's t-test on bootstrap replicates). b.UniRep performance compared to a suite of baselines across 17 proteins in the DMS stability prediction task (Pearson's r ). UniRep Fusion achieved significantly higher Pearson's r on all subsets ( p &lt; 0.006; Welch's t-test on bootstrap replicates). c. Average magnitude neuron activations for de novo designed and natural protein stability prediction show significant co-activation (p &lt; 0.01; permutation test). d. Activations of the helix-sheet neuron colored onto the de novo designed protein HHH_0142 (PDB: 5UOI). e. UniRep Fusion achieves statistically lower mean squared error than a suite of baselines across a set of 8 proteins with 9 diverse functions in the DMS function prediction task (p&lt;0.0002 on 8/9 and p&lt;0.009 on 1/9; Welch's t-test on bootstrap replicates). f . UniRep performance across orders of scale: from distant proteins, to variants of the same protein one mutation apart. Scale bar illustrates the average distance between proteins in the DMS function prediction dataset. The scale of one mutation (10x magnification from the average distance bar) is shown to contrast the small mutational range UniRep is asked to model.</figDesc><graphic url="image-3.png" coords="8,114.00,73.50,383.25,339.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>UniRep enables extrapolation, we visually examined the spatial pattern of training and test data in sequence space (Supp. Fig 10e), Doc2Vec space (Supp. Fig 10g), and UniRep space (Supp. Fig.10f,h). Interestingly, despite not overlapping in sequence space, training and test points were co-localized in UniRep space suggesting that UniRep discovered commonalities between training and test proteins that effectively transformed the problem of extrapolation into one of interpolation, providing a plausible mechanism for our performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-5.png" coords="32,37.50,121.50,459.75,330.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-6.png" coords="35,37.50,73.50,384.00,403.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-9.png" coords="36,37.50,379.50,468.00,168.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-10.png" coords="37,37.50,73.50,436.50,291.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-11.png" coords="37,37.50,387.75,440.25,294.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-13.png" coords="39,94.50,73.50,422.25,419.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-14.png" coords="43,37.50,-78.75,344.25,344.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-15.png" coords="43,37.50,305.25,431.25,280.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-19.png" coords="49,37.50,296.25,402.75,267.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Bacteria</figDesc><table><row><cell cols="2">Bacillus subtilis (168)</cell><cell>Bacteria</cell><cell>Bacteria</cell></row><row><cell cols="2">Cyanothece (PCC7822)</cell><cell>Bacteria</cell><cell>Cyanobacteria</cell></row><row><cell cols="2">Mycoplasma genitalium</cell><cell>Bacteria</cell><cell>Bacteria</cell></row><row><cell cols="2">Mycobacterium tuberculosis</cell><cell>Bacteria</cell><cell>Bacteria</cell></row><row><cell cols="2">Prochlorococcus marinus</cell><cell>Bacteria</cell><cell>Cyanobacteria</cell></row><row><cell cols="2">Streptomyces coelicolor (A32)</cell><cell>Bacteria</cell><cell>Bacteria</cell></row><row><cell cols="2">Synechocysti s (PCC 6803 Kazusa)</cell><cell>Bacteria</cell><cell>Cyanobacteria</cell></row><row><cell cols="2">Caenorhabditis elegans</cell><cell>Eukarya</cell><cell>Animalia</cell></row><row><cell cols="2">Drosophila melanogaster (fruit fly)</cell><cell>Eukarya</cell><cell>Animalia</cell></row><row><cell cols="2">Homo sapiens (human)</cell><cell>Eukarya</cell><cell>Mammalia</cell></row><row><cell cols="2">Mus musculus (mouse)</cell><cell>Eukarya</cell><cell>Mammalia</cell></row><row><cell cols="2">Saccharomyces cerevisiae (yeast)</cell><cell>Eukarya</cell><cell>Fungi</cell></row><row><cell cols="2">Anolis carolinensis</cell><cell>Eukarya</cell><cell>Animalia</cell></row><row><cell cols="2">Aspergillus nidulans</cell><cell>Eukarya</cell><cell>Fungi</cell></row><row><cell cols="2">Arabidopsis thaliana</cell><cell>Eukarya</cell><cell>Plantae</cell></row><row><cell>Cavia porcellus</cell><cell>(guinea pig)</cell><cell>Eukarya</cell><cell>Mammalia</cell></row><row><cell cols="2">Gallus gallus domesticus (chicken)</cell><cell>Eukarya</cell><cell>Animalia</cell></row><row><cell cols="2">Coprinopsis cinerea</cell><cell>Eukarya</cell><cell>Fungi</cell></row><row><cell cols="2">Bos taurus (cow)</cell><cell>Eukarya</cell><cell>Mammalia</cell></row><row><cell cols="2">Chlamydomonas reinhardtii</cell><cell>Eukarya</cell><cell>Eukarya</cell></row><row><cell cols="2">Cryptococcus neoformans</cell><cell>Eukarya</cell><cell>Fungi</cell></row><row><cell cols="2">Canis familiaris (dog)</cell><cell>Eukarya</cell><cell>Mammalia</cell></row><row><cell cols="2">Emiliania huxleyi</cell><cell>Eukarya</cell><cell>Eukarya</cell></row><row><cell cols="2">Macaca mulatta (macaque)</cell><cell>Eukarya</cell><cell>Mammalia</cell></row><row><cell cols="2">Zea mays (corn)</cell><cell>Eukarya</cell><cell>Plantae</cell></row><row><cell cols="2">Heterocephalus glaber (naked mole</cell><cell></cell><cell></cell></row><row><cell>rat)</cell><cell></cell><cell>Eukarya</cell><cell>Mammalia</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Representation fusions (concatenations) analyzed.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>State</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>64-unit</cell><cell></cell><cell cols="2">256-unit</cell><cell></cell><cell>1900-unit</cell><cell></cell><cell>RGN</cell></row><row><cell>Name</cell><cell>Avg.</cell><cell>Final</cell><cell>Final</cell><cell>Avg.</cell><cell>Final</cell><cell>Avg.</cell><cell>Final</cell><cell>Final</cell></row><row><cell></cell><cell>Hidden</cell><cell>Hidden</cell><cell>Cell</cell><cell>Hidden</cell><cell>Cell</cell><cell>Hidden</cell><cell>Hidden</cell><cell>Cell</cell></row><row><cell>UniRep 64-unit Fusion</cell><cell>x</cell><cell>x</cell><cell>x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>UniRep 256-unit Fusion</cell><cell></cell><cell></cell><cell></cell><cell>x</cell><cell>x</cell><cell></cell><cell></cell><cell></cell></row><row><cell>UniRep Fusion</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>x</cell><cell>x</cell><cell>x</cell></row><row><cell>UniRep 64, 256,</cell><cell>x</cell><cell></cell><cell></cell><cell>x</cell><cell></cell><cell>x</cell><cell></cell><cell></cell></row><row><cell>1900-unit Avg. Hiddens</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>64, 256, 1900-unit Final</cell><cell></cell><cell></cell><cell>x</cell><cell></cell><cell>x</cell><cell></cell><cell></cell><cell>x</cell></row><row><cell>Cells</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RGN + UniRep</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>x</cell><cell></cell><cell></cell><cell>x</cell></row><row><cell>1900-unit Avg. Hiddens</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RGN + UniRep</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>x</cell><cell>x</cell></row><row><cell>1900-unit Final Cells</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Supplemental</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>of representations on 15 prediction tasks (Mean Squared Error) &amp; stability ranking task (Spearman Correlation)-test</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="2">Cytochrome P450 Thermostability</cell><cell cols="2">Rhodopsin Peak Absorption Wavelength</cell><cell cols="2">Epoxide Hydrolase Enantio selectivity</cell><cell>Channel rhodopsin Membrane Localization</cell><cell>TEM-1 Beta-lactamase</cell><cell>Ubiquitin (E1 Activity)</cell><cell>Protein G (IgG domain)</cell></row><row><cell cols="2">UniRep Fusion</cell><cell>15.8</cell><cell></cell><cell>499</cell><cell></cell><cell></cell><cell>189</cell><cell>1.25</cell><cell cols="2">0.0545*** 0.0421*** 0.0233***</cell></row><row><cell cols="2">Our Best Baseline</cell><cell>21.7</cell><cell></cell><cell>571</cell><cell></cell><cell></cell><cell>93.2</cell><cell>0.912</cell><cell>0.074</cell><cell>0.052</cell><cell>0.054</cell></row><row><cell cols="2">RGN</cell><cell>24.4</cell><cell></cell><cell cols="2">&gt;2000</cell><cell></cell><cell>&gt;1000</cell><cell>1.61</cell><cell>0.0904</cell><cell>0.054</cell><cell>0.0977</cell></row><row><cell cols="2">Best Doc2Vec</cell><cell>18.1</cell><cell></cell><cell>530</cell><cell></cell><cell></cell><cell>95.7</cell><cell>1</cell><cell>0.0881</cell><cell>0.0625</cell><cell>0.0724</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Spearman</cell></row><row><cell></cell><cell>HSP90</cell><cell>Amino glycosidase (Kka2)</cell><cell cols="2">Pab1 (RRM domain)</cell><cell cols="2">PSD95 (Pdz3 domain)</cell><cell>Ubiquitin</cell><cell>Yap65 (WW domain)</cell><cell>Stability: 17 DMS datasets combined</cell><cell>Stability: De Novo Designed mini proteins</cell><cell>Rank Correlation Stability: De Novo Design</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Rounds</cell></row><row><cell cols="2">UniRep Fusion 0.0258***</cell><cell>0.11**</cell><cell cols="6">0.0265*** 0.0208*** 0.0323*** 0.0415***</cell><cell>0.0304***</cell><cell>0.179***</cell><cell>ùû∫ =0.59***</cell></row><row><cell cols="2">Our Best Baseline 0.0344</cell><cell>0.115</cell><cell></cell><cell>0.0435</cell><cell>0.041</cell><cell></cell><cell>0.0515</cell><cell>0.0662</cell><cell>0.0398</cell><cell>0.201</cell><cell>-</cell></row><row><cell>RGN</cell><cell>0.0579</cell><cell>0.14</cell><cell></cell><cell>0.0596</cell><cell cols="2">0.0438</cell><cell>0.0601</cell><cell>0.0639</cell><cell>0.0338</cell><cell>0.189</cell><cell>-</cell></row><row><cell>Best Doc2Vec</cell><cell>0.0579</cell><cell>0.132</cell><cell></cell><cell>0.0495</cell><cell>0.046</cell><cell></cell><cell>0.064</cell><cell>0.0772</cell><cell>0.0473</cell><cell>0.258</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Rosetta Total Energy</cell><cell>ùû∫ =0.42</cell></row><row><cell>Supplemental</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Regression results -test set metrics, with lowest MSE model or model class compared to the 2nd lowest MSE model or model class *p &lt; 0.05, **p &lt; 0.01, ***p&lt;0.001 (Welch's t-test for significance), standard deviations obtained through 30x 50% validation/test set resampling. Validation set metrics can be found in Supp. Table</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Performance of representations on 15 prediction tasks (Mean Squared Error) &amp; stability ranking task (Spearman Correlation)-validation</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Rhodopsin</cell><cell cols="2">Epoxide</cell><cell cols="2">Channel</cell></row><row><cell></cell><cell></cell><cell>Cytochrome</cell><cell></cell><cell>Peak</cell><cell></cell><cell cols="2">Hydrolase</cell><cell cols="2">rhodopsin</cell><cell>Ubiquitin</cell><cell>Protein G</cell></row><row><cell></cell><cell></cell><cell>P450</cell><cell></cell><cell cols="2">Absorption</cell><cell cols="2">Enantio</cell><cell cols="2">Membrane</cell><cell>TEM-1 Beta-</cell><cell>(E1</cell><cell>(IgG</cell></row><row><cell></cell><cell></cell><cell cols="2">Thermostability</cell><cell cols="2">Wavelength</cell><cell cols="2">selectivity</cell><cell cols="2">Localization</cell><cell>lactamase</cell><cell>Activity)</cell><cell>domain)</cell></row><row><cell cols="2">UniRep Fusion</cell><cell>8.3</cell><cell></cell><cell>130</cell><cell></cell><cell>444</cell><cell></cell><cell></cell><cell>1.47</cell><cell>0.0471***</cell><cell>0.0274***</cell><cell>0.0307**</cell></row><row><cell cols="2">Our Best Baseline</cell><cell>8.27</cell><cell></cell><cell>79.4**</cell><cell></cell><cell cols="2">219***</cell><cell></cell><cell>1.4</cell><cell>0.0615</cell><cell>0.0365</cell><cell>0.0425</cell></row><row><cell cols="2">RGN</cell><cell>10.3</cell><cell></cell><cell>428</cell><cell></cell><cell>496</cell><cell></cell><cell></cell><cell>1.35</cell><cell>0.0724</cell><cell>0.0351</cell><cell>0.0952</cell></row><row><cell cols="2">Best Doc2Vec</cell><cell>8.68</cell><cell></cell><cell>97.9</cell><cell></cell><cell>320</cell><cell></cell><cell></cell><cell>1.4</cell><cell>0.08</cell><cell>0.0427</cell><cell>0.0629</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Ranking</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Stability:</cell></row><row><cell></cell><cell></cell><cell>Amino</cell><cell></cell><cell></cell><cell cols="2">PSD95</cell><cell></cell><cell></cell><cell>Yap65</cell><cell>Stability: 17</cell><cell>Stability: De Novo</cell><cell>De Novo</cell></row><row><cell></cell><cell></cell><cell>glycosidase</cell><cell cols="2">Pab1 (RRM</cell><cell cols="2">(Pdz3</cell><cell></cell><cell></cell><cell>(WW</cell><cell>DMS Datasets</cell><cell>Designed Mini</cell><cell>Design</cell></row><row><cell></cell><cell>HSP90</cell><cell>(Kka2)</cell><cell></cell><cell>domain)</cell><cell cols="2">domain)</cell><cell cols="2">Ubiquitin</cell><cell>domain)</cell><cell>Combined</cell><cell>Proteins</cell><cell>Rounds</cell></row><row><cell cols="2">UniRep Fusion 0.0218***</cell><cell>0.116**</cell><cell cols="4">0.0234*** 0.0183***</cell><cell cols="2">0.0521</cell><cell>0.0387</cell><cell>0.031***</cell><cell>0.185***</cell><cell>ùû∫ =0.62***</cell></row><row><cell>Our Best</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Baseline</cell><cell>0.0415</cell><cell>0.125</cell><cell></cell><cell>0.0497</cell><cell cols="2">0.0342</cell><cell cols="4">0.0403*** 0.033***</cell><cell>0.0425</cell><cell>0.208</cell><cell>-</cell></row><row><cell>RGN</cell><cell>0.0541</cell><cell>0.129</cell><cell></cell><cell>0.0586</cell><cell cols="2">0.0488</cell><cell cols="2">0.0632</cell><cell>0.0504</cell><cell>0.0351</cell><cell>0.193</cell><cell>-</cell></row><row><cell>Best Doc2Vec</cell><cell>0.0626</cell><cell>0.145</cell><cell></cell><cell>0.047</cell><cell cols="2">0.0465</cell><cell cols="2">0.0612</cell><cell>0.0408</cell><cell>0.0511</cell><cell>0.266</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Rosetta Total</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Energy</cell><cell>ùû∫ =0.42</cell></row><row><cell>Supplemental</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Regression results -validation set metrics, with lowest MSE model or model class compared to the 2nd lowest MSE model or model class *p &lt; 0.05, **p &lt; 0.01, ***p&lt;0.001 (Welch's t-test for significance), standard deviations obtained through 30x 50% validation/test set resampling. Test set metrics and explanation of the first 4 column tasks can be found in Supp. Table4. require ~$300,000 (100x more). Random sampling still commonly used in this context would require $1,848,000 to achieve the same. Similarly, Evotuned UniRep captures the brightest sequence in the generalization set within the first $3,000 spent in testing, and the best Doc2Vec baseline would require ~$540,000 (180x more) to do the same. Assuming on-target assembly rates improve and full economies of scale, multiplex gene assembly methods such as DropSynth 47 could bring the cost of synthesizing a model proposed GFP down to ~$2. At these cost rates, Evotuned Unirep would enable high purity functional diversity capture and function optimization for just a few hundred dollars. Taken together, these results suggest that Evotuning UniRep enables generalization to distant parts of the fitness landscape and thereby facilitates protein engineering by drastically minimizing the cost required to capture functional diversity and optimize function.</figDesc><table><row><cell>Group</cell><cell>Protein(s) in the</cell><cell>Size</cell><cell>Characteristic</cell><cell>Ref</cell></row><row><cell></cell><cell>task</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Small-scale</cell><cell>Cytochrome P450</cell><cell>261</cell><cell>Thermostability</cell><cell>21</cell></row><row><cell>protein prediction characteristics</cell><cell>Rhodopsin Bacterial</cell><cell>81</cell><cell>Peak Absorption Wavelength</cell><cell></cell></row><row><cell></cell><cell>Epoxide Hydrolase</cell><cell>152</cell><cell>Enantioselectivity</cell><cell></cell></row><row><cell></cell><cell>Channelrhodopsin</cell><cell>248</cell><cell>Plasma Membrane Localization</cell><cell></cell></row><row><cell>Large-scale</cell><cell>TEM1b-lactamase</cell><cell>5198</cell><cell>Function (diverse, see Fig. 3e)</cell><cell>35</cell></row><row><cell>function prediction</cell><cell>activity Ubiquitin -E1</cell><cell>1085</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Protein G (IgG</cell><cell>1045</cell><cell></cell><cell></cell></row><row><cell></cell><cell>domain)</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Pab1 (RRM</cell><cell>1188</cell><cell></cell><cell></cell></row><row><cell></cell><cell>domain)</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Ubiquitin</cell><cell>1195</cell><cell></cell><cell></cell></row><row><cell></cell><cell>PSD95 (Pdz3</cell><cell>1577</cell><cell></cell><cell></cell></row><row><cell></cell><cell>domain)</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Yap65 (WW</cell><cell>363</cell><cell></cell><cell></cell></row><row><cell></cell><cell>domain)</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Aminoglycoside</cell><cell>4234</cell><cell></cell><cell></cell></row><row><cell></cell><cell>kinase</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>Data augmentation of UniRep.</figDesc><table><row><cell>Name</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements:</head><p>We thank John Aach, Amaro Taylor-Weiner, Daniel Goodman, Pierce Ogden, Gleb Kuznetsov, Sam Sinai, Aaron Tucker, Miles Turpin, Jacob Swett, Nathaniel Thomas, Raahil Sha, Chris Bakerlee and Kyle Fish for valuable feedback and discussion. S.B. was supported by an NIH Training Grant to the Harvard Bioinformatics and Integrative Genomics program as well as an NSF GRFP Fellowship. M.A. was supported through NIGMS Grant P50GM107618 and NIH grant U54-CA225088 . E.C.A. and G.K. were supported by the Center for Effective Altruism. E.C.A. was partially supported by the Wyss Institute for Biologically Inspired Engineering. Computational resources were, in part, generously provided by the AWS Cloud Credits for Research program.</p></div>
			</div>


			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Code Availability: Code for UniRep model training and inference with trained weights along with links to all necessary data is available at https://github.com/churchlab/UniRep . Data Availability: All data are available in the main text or the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Materials and Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training the UniRep representations</head><p>Unsupervised training dataset: We expected that public protein databases, unlike many Natural Language datasets, would contain a) random deleterious mutations yet to be eliminated by selection, and b) hard-to-catch sequencing/assembly mistakes, both leading to increased noise. Therefore, we chose UniRef50 22 as a training dataset. It is "dehomologized" such that any two sequences have at most 50% identity with each other. By selecting the single highest quality sequence for each cluster of homologs <ref type="bibr" target="#b21">22</ref> , we hypothesized UniRef50 would be less noisy. It contains ~27 million protein sequences. We removed proteins longer than 2000 amino acids and records containing non-canonical amino acid symbols (X, B, Z, J), randomly selected test and validation subsets for monitoring training (1% of the overall dataset each) and used the rest of the data (~24 million sequences) in training.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Author Contributions: E.C.A. and G.K. conceived the study. E.C.A., G.K., and S.B. conceived the experiments, managed data, and performed the analysis. M.A. performed performed large-scale RGN inference and managed data and software for parts of the analysis. G.M.C. supervised the project. E.C.A., G.K. and S.B. wrote the manuscript with help from all authors.</p><p>Competing Interests: E.C.A., G.K., and S.B. are in the process of pursuing a patent on this technology. S.B. is a former consultant for Flagship Pioneering company VL57. A full list of G.M.C.'s tech transfer, advisory roles, and funding sources can be found on the lab's website: http://arep.med.harvard.edu/gmc/tech.html Baseline representations -Doc-2-Vec: We used the 4 best performing models as chosen for presentation by the authors of Yang et al. (2018)  <ref type="bibr">21</ref> . These are (using the original names): original with k=3 w=7, scrambled with k=3 w=5, random with k=3 w=7, and uniform k=4 w=1. We downloaded the trained models from http://cheme.caltech.edu/~kkyang/models/ and performed inference on all of the sequences in our datasets, adapting as much as possible the code used by the authors found at https://github.com/fhalab/embeddings_reproduction .</p><p>Baseline representations -N-gram, length, and basic biophysical: We utilized 6 baseline representationsthey constitute 3 distinct approaches to constructing a general scalable protein vector representations without using deep learning techniques:</p><p>-Amino-acid frequencies in the protein and protein length normalized to average protein length in the unsupervised training data -Amino-acid frequencies in the protein concatenated with predicted biophysical parameters of the protein: molecular weight, instability index, isoelectric point, secondary structure fraction. The annotation/prediction for biophysical parameters was done with biopython package version 1.72 ( https://biopython.org/ ) -Two n-gram representations with n=2 and n=3 (with scikit-learn 0.19.2 http://scikit-learn.org/stable/index.html ). -Two n-gram representations with n=2 and n=3 with Term Frequency -Inverse Document Frequency (TF-IDF) weighting intended to emphasize the n-grams unique to particular proteins 58 See Supp. Table <ref type="table">8</ref> for an exhaustive list of all the baseline variants used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stability Ranking Task:</head><p>To further benchmark our performance compared to typical protein engineering tools, we obtained the published Rosetta total energy estimates with the "beta_nov15" version of the energy function <ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b59">60</ref> and exposed nonpolar surface area of the designed structure for the stability prediction for de novo designed mini proteins dataset <ref type="bibr" target="#b4">5</ref> . The Rosetta calculations were not performed in <ref type="bibr" target="#b4">5</ref> for the control proteins in this dataset, so the scores were available for 1432 out of 5570 test set proteins and 1416 out of 5571 validation set proteins in our splits. Because these Rosetta Total Energy estimates do not directly correspond to measured stability values (e.g. lower energy is higher stability as defined), we used Spearman Rank Order Correlation coefficient, comparing our model predictions and the alternative scores' ability to reconstitute correct stability ranking of the sequences in the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Regression analysis with Lasso LARS:</head><p>We utilized a simple sparsifying linear model with L1 prior using the Least Angle Regression (LARS) algorithm (implementation from the scikit-learn package 0.19.2 http://scikit-learn.org/stable/index.html). The value of regularization parameter alpha for the model for each representation on each task was selected through 10-fold random cross-validation on the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>List of Supplementary materials:</head><p>Supplemental Figure <ref type="figure">1</ref>. Growth in sequence databases. Supplemental Table <ref type="table">1</ref>. Reference proteomes used in the organism analysis in Fig. <ref type="figure">2b</ref>. Supplemental Figure <ref type="figure">2</ref>. Single-protein vector arithmetic in UniRep representation space. Supplemental Table <ref type="table">2</ref>. Unirep achieves competitive results on homology detection as measured by ROC-AUC and ROC50-AUC (sorted by ROC score). Supplemental Figure <ref type="figure">3</ref>. A representative clustering of Cytochrome-oxidase family enzymes from HOMSTRAD (Fig. <ref type="figure">2d</ref>) further illustrates this result by reconstructing the correct monophyletic grouping of the true labels, where a sequence distance-based clustering fails. Supplemental Figure <ref type="figure">4</ref>. OXBench unsupervised homology detection task, all results. Supplemental Figure <ref type="figure">5</ref>. HOMSTRAD unsupervised homology detection task, all representations results. Supplemental Figure <ref type="figure">6</ref>. Learned from a collection of PDB secondary structures, a linear combination of hidden state neurons identifies solvent accessible regions in the structure of Bovine Rhodopsin GPCR (PDB:1F88, left) oriented with the extracellular domain upwards (Methods). Supplemental Figure <ref type="figure">7</ref>. Baseline representations. Supplemental Table <ref type="table">3</ref>  (H√•ndstad, 2007) on two most frequently used benchmark datasets.  <ref type="bibr">21</ref> , Recurrent Geometric Network hidden state, learned by recurrent processing of input sequences to predict crystal structure (lower right) <ref type="bibr" target="#b15">16</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplemental</head><p>Supplemental Figure <ref type="figure">8</ref>. Validation scores for main text Figure <ref type="figure">3e</ref>, 17 DMS protein stability prediction datasets.</p><p>Supplemental Figure <ref type="figure">9</ref>. Linear model on top of UniRep is simpler (has fewer parameters) than the same model using a standard One-Hot-Encoding if the sequence is longer than 95aa.</p><p>Supplemental Figure <ref type="figure">10</ref>. Variant Effect and stability generalization tasks; hypothesized mechanism for transfer performance. We used a well-established generalization scoring methodology from work in machine learning <ref type="bibr" target="#b60">61</ref> . It quantifies the error of local, "In-Domain" predictions as well as generalization error, which they call "Transfer", relative to a baseline (Methods); lower is better . Unless stated otherwise, we use the Leave One Protein Out (LOPO) procedure, withholding one protein from the set at a time, training a model on all but that one protein, evaluating it on the single withheld protein, and taking an average of these generalization errors (Methods) <ref type="bibr" target="#b34">35</ref> . a. We used the function prediction dataset from Figure <ref type="figure">3</ref> with 8 proteins with 9 distinct functions measured in separate experiments <ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b76">77</ref> . It was previously shown that some of the regions most vulnerable to deleterious single mutations are functional and highly conserved or co-conserved in evolution <ref type="bibr" target="#b77">78</ref> . Standard approaches therefore rely strongly on co-evolutionary data <ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b76">77</ref> or even structural data <ref type="bibr" target="#b34">35</ref> , which implicitly demarcate functional residues. Because it has neither of these as inputs and was trained on a corpus with at most 50% similarity between sequences, UniRep should find it challenging to identify such residues. Nevertheless, UniRep performs best in-Domain, successfully identifying functionally-important positions from some labeled mutation data for a given protein. However, as expected, UniRep does not generalize well to proteins for which it had no labeled training data, at least in this case. When we tested fusing UniRep to the RGN (RGN-Fusion), which was trained on a form of evolutionary data (PSSMs) and predicts protein structure, we see a boost in performance suggesting a good trade-off between In-Domain and Transfer Error. c. Generalization performance on the LOPO generalized DMS stability task, measured by In-Domain and Transfer ratios for test set (left) and validation set (right). Unlike variant effect, stability is a property consistent to all proteins. There were 3 natural and 14 de novo designed wildtypes. Using the same LOPO procedure as described above, we found that UniRep outperformed all baselines at generalization, and that the RGN no longer offered meaningfully complementary information, RGN-Fusion performing approximately as well as UniRep. This suggests UniRep does enable generalization of universal protein characteristics. d. Generalization performance on the extrapolation DMS stability task, where the test set is selected from the most peripheral proteins in the set (e, red), measured by In-Domain and transfer ratios for test set (left) and validation set (right). We took the same DMS dataset as c) but instead of using the LOPO procedure, selected a single test set consisting of the most distant proteins in sequence space visualized with an MDS of the Levenshtein distance matrix (see e). This most closely represents the setup in a protein engineering task, where the engineer is exploring outwords in sequence space from local knowledge. Here we see the strongest performance of UniRep over baselines, suggesting that UniRep is well suited to this protein engineering formulation.  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Methods for the directed evolution of proteins</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Rev. Genet</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="379" to="394" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Exploring protein fitness landscapes by directed evolution</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">H</forename><surname>Arnold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Rev. Mol. Cell Biol</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="866" to="876" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Toward machine-guided design of proteins</title>
		<author>
			<persName><forename type="first">S</forename><surname>Biswas</surname></persName>
		</author>
		<idno type="DOI">10.1101/337154</idno>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="volume">337154</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Machine learning to design integral membrane channelrhodopsins for efficient eukaryotic expression and plasma membrane localization</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Bedbrook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Rice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gradinaru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">H</forename><surname>Arnold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Comput. Biol</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">e1005786</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Global analysis of protein folding using massively parallel design, synthesis, and testing</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Rocklin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">357</biblScope>
			<biblScope unit="page" from="168" to="175" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The coming of age of de novo protein design</title>
		<author>
			<persName><forename type="first">P.-S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Boyken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">537</biblScope>
			<biblScope unit="page" from="320" to="327" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Computational protein design: a review</title>
		<author>
			<persName><forename type="first">I</forename><surname>Coluzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Phys. Condens. Matter</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">143001</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Navigating the protein fitness landscape with Gaussian processes</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">H</forename><surname>Arnold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Natl. Acad. Sci. U. S. A</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page" from="E193" to="201" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improving catalytic function by ProSAR-driven enzyme evolution</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Biotechnol</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page">338</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Protein Structure Prediction Using Rosetta</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Rohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E M</forename><surname>Strauss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M S</forename><surname>Misura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Numerical Computer Methods, Part D 383</title>
				<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="66" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Molecular dynamics simulations of biomolecules</title>
		<author>
			<persName><forename type="first">M</forename><surname>Karplus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Andrew Mccammon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Struct. Mol. Biol</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">646</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Programming molecular self-assembly of intrinsically disordered proteins containing sequences of low complexity</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Carroll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chilkoti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">P</forename><surname>L√≥pez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Chem</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="509" to="515" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Engineering an allosteric transcription factor to respond to new ligands</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="177" to="183" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Biosensor libraries harness large classes of binding domains for construction of allosteric transcriptional regulators</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Ju√°rez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lecube-Azpeitia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Church</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Commun</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">3101</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep generative models of genetic variation capture the effects of mutations</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Riesselman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Ingraham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Marks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="816" to="822" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">End-to-end differentiable learning of protein structure</title>
		<author>
			<persName><forename type="first">M</forename><surname>Alquraishi</surname></persName>
		</author>
		<idno type="DOI">10.1101/265231</idno>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="volume">265231</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Deep Recurrent Neural Network for Protein Function Prediction from Sequence</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep Semantic Protein Representation for Annotation, Discovery, and Engineering</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Schwartz</surname></persName>
		</author>
		<idno type="DOI">10.1101/365965</idno>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="volume">365965</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName><surname>Uniprotkb/Trembl</surname></persName>
		</author>
		<ptr target="https://www.uniprot.org/statistics/TrEMBL" />
		<imprint>
			<date type="published" when="2018">2018_10. 21st November 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Continuous Distributed Representation of Biological Sequences for Deep Proteomics and Genomics</title>
		<author>
			<persName><forename type="first">E</forename><surname>Asgari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R K</forename><surname>Mofrad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS One</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">e0141287</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learned protein embeddings for machine learning</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Bedbrook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">H</forename><surname>Arnold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="2642" to="2648" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">UniRef clusters: a comprehensive and scalable alternative for improving sequence similarity searches</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Suzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Mcgarvey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="926" to="932" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning to Generate Reviews and Discovering Sentiment</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Visualizing Data using t-SNE</title>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">HOMSTRAD: a database of protein structure alignments for homologous families</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mizuguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Deane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Overington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Protein Sci</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2469" to="2471" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">OXBench: a benchmark for evaluation of protein multiple sequence alignment accuracy</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">P S</forename><surname>Raghava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M J</forename><surname>Searle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Audley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Barber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Barton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">47</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Doan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Halevy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ives</surname></persName>
		</author>
		<author>
			<persName><surname>String</surname></persName>
		</author>
		<author>
			<persName><surname>Matching</surname></persName>
		</author>
		<title level="m">Principles of Data Integration</title>
				<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="95" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Tree Alignment Based on Needleman-Wunsch Algorithm for Sensor Selection in Smart Homes</title>
		<author>
			<persName><forename type="first">S.-L</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Foo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Structural Classification of Proteins--extended, integrating SCOP and ASTRAL data and classification of new structures</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Brenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Chandonia</surname></persName>
		</author>
		<author>
			<persName><surname>Scope</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="D304" to="309" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Relationship between thermal stability, degradation rate and expression yield of barnase variants in the periplasm of Escherichia coli</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Da Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Kellis</surname></persName>
		</author>
		<author>
			<persName><surname>Jr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Protein Eng</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1197" to="1202" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Stabilizing biocatalysts</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Bommarius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Paye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chem. Soc. Rev</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="6534" to="6565" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Stability of protein pharmaceuticals: an update</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Payne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Katayama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pharm. Res</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="544" to="575" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Large-scale determination of previously unsolved protein structures using evolutionary information</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ovchinnikov</surname></persName>
		</author>
		<idno type="DOI">10.7554/eLife.09248</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<ptr target="https://www.uniprot.org/uniref/?query=de+novo+designed+protein+AND+identity%3A0.5" />
		<title level="m">AND identity:0.5 in UniRef</title>
				<imprint>
			<date type="published" when="2018-11-02">2nd November 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Quantitative Missense Variant Effect Prediction Using Large-Scale Mutagenesis Data</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">E</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Hause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luebeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shendure</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Fowler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell Syst</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">e3</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Local fitness landscape of the green fluorescent protein</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Sarkisyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">533</biblScope>
			<biblScope unit="page" from="397" to="401" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The Growing and Glowing Toolbox of Fluorescent and Photoactive Proteins</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Rodriguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends Biochem. Sci</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="111" to="129" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Lambert</surname></persName>
		</author>
		<idno type="DOI">10.5281/ZENODO.1244328</idno>
		<title level="m">Tlambert03/Fpbase: V1.1.0</title>
				<imprint>
			<publisher>Zenodo</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A model of substitution trajectories in sequence space and long-term protein evolution</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Usmanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ferretti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Povolotskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Vlasov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Kondrashov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mol. Biol. Evol</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="542" to="554" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Epistasis as the primary factor in molecular evolution</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Breen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kemena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Vlasov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Notredame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Kondrashov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">490</biblScope>
			<biblScope unit="page" from="535" to="538" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">De novo design of a fluorescence-activating beta barrel -BB1</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dou</surname></persName>
		</author>
		<idno type="DOI">10.2210/pdb6d0t/pdb</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Insights into the phylogeny and coding potential of microbial dark matter</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rinke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">499</biblScope>
			<biblScope unit="page">431</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Recovery of nearly 8,000 metagenome-assembled genomes substantially expands the tree of life</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Parks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Microbiology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">1533</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><surname>Embl-Ebi</surname></persName>
		</author>
		<ptr target="https://www.ebi.ac.uk/uniprot/TrEMBLstats" />
		<title level="m">Current Release Statistics</title>
				<imprint>
			<date type="published" when="2018-11-01">1st November 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">In-Datacenter Performance Analysis of a Tensor Processing Unit</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th Annual International Symposium on Computer Architecture</title>
				<meeting>the 44th Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Multiplexed gene synthesis in emulsions for exploring protein functional landscapes</title>
		<author>
			<persName><forename type="first">C</forename><surname>Plesa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Sidore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">B</forename><surname>Lubock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kosuri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">359</biblScope>
			<biblScope unit="page" from="343" to="347" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Multiplex single-molecule interaction profiling of DNA-barcoded proteins</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">515</biblScope>
			<biblScope unit="page" from="554" to="557" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Direct measurement of DNA affinity landscapes on a high-throughput sequencing instrument</title>
		<author>
			<persName><forename type="first">R</forename><surname>Nutiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Biotechnol</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="659" to="664" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">The Future of Multiplexed Eukaryotic Genome Engineering</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACS Chem. Biol</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="313" to="325" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Automatic Chemical Design Using a Data-Driven Continuous Representation of Molecules</title>
		<author>
			<persName><forename type="first">R</forename><surname>G√≥mez-Bombarelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACS Cent Sci</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="268" to="276" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">An Overview of Multi-Task Learning in Deep Neural Networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Multiplicative LSTM for sequence modelling</title>
		<author>
			<persName><forename type="first">B</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Renals</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Learning to forget: continual prediction with LSTM</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cummins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="850" to="855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">On the Properties of Neural Machine Translation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Encoder-Decoder Approaches</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">ProteinNet: a standardized data set for machine learning of protein structure</title>
		<author>
			<persName><forename type="first">M</forename><surname>Alquraishi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arxiv</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Understanding inverse document frequency: on theoretical arguments for IDF</title>
		<author>
			<persName><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Documentation</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="503" to="520" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Simultaneous Optimization of Biomolecular Energy Functions on Features from Small Molecules and Macromolecules</title>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Chem. Theory Comput</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="6201" to="6212" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">The Rosetta All-Atom Energy Function for Macromolecular Modeling and Design</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Alford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Chem. Theory Comput</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="3031" to="3048" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Domain adaptation for large-scale sentiment classification: a deep learning approach</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on International Conference on Machine Learning</title>
				<meeting>the 28th International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">513</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Motif kernel generated by genetic programming improves remote homology and fold detection</title>
		<author>
			<persName><forename type="first">T</forename><surname>H√•ndstad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J H</forename><surname>Hestnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Saetrom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">23</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Protein remote homology detection based on bidirectional long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">443</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Soft Ngram Representation and Modeling for Protein Remote Homology Detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lovato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bicego</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Comput. Biol. Bioinform</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1482" to="1488" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine Learning in Python</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Oliphant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName><surname>Scipy</surname></persName>
		</author>
		<ptr target="http://www.scipy.org/" />
		<title level="m">Open Source Scientific Tools for Python</title>
				<imprint>
			<date type="published" when="2001">2001. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Clustering -scikit-learn 0.20.0 documentation</title>
		<ptr target="http://scikit-learn.org/stable/modules/clustering.html" />
		<imprint>
			<date type="published" when="2018-11-02">2nd November 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Diversity and Evolution of Coral Fluorescent Proteins</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">O</forename><surname>Alieva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS One</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">e2680</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title/>
		<idno type="DOI">10.5281/zenodo.1244328</idno>
		<editor>Lambert, T. tlambert03/FPbase.</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">H</forename><surname>Embl-Ebi</surname></persName>
		</author>
		<author>
			<persName><surname>Hmmer</surname></persName>
		</author>
		<ptr target="https://www.ebi.ac.uk/Tools/hmmer/search/jackhmmer" />
		<imprint>
			<date type="published" when="2018-11-02">2nd November 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Multiple sequence alignment using ClustalW and ClustalX</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Higgins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Curr. Protoc. Bioinformatics Chapter</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">OrthoDB v9.1: cataloging evolutionary and functional annotations for animal, fungal, plant, archaeal, bacterial and viral orthologs</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Zdobnov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="D744" to="D749" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">The Protein Data Bank</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Berman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="235" to="242" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Dictionary of protein secondary structure: pattern recognition of hydrogen-bonded and geometrical features</title>
		<author>
			<persName><forename type="first">W</forename><surname>Kabsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biopolymers: Original Research on Biomolecules</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="2577" to="2637" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">NGLview-interactive molecular graphics for Jupyter notebooks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Rose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1241" to="1242" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Linguistic Regularities in Continuous Space Word Representations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-T</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter</title>
				<meeting>the 2013 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">746</biblScope>
			<biblScope unit="page">751</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Mutation effects predicted from sequence co-variation</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Hopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Biotechnol</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="128" to="135" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Protein tolerance to random amino acid change</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Loeb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Natl. Acad. Sci. U. S. A</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="9205" to="9210" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title/>
		<ptr target="https://www.twistbioscience.com/products/genes?gclid=Cj0KCQiA28nfBRCDARIsANc5BFAYK3MMQaN1ZZelOT-X3gKuAsIUXqeXbOwUZ17nYEPD5Rw6_nM_XegaAqAUEALw_wcB" />
	</analytic>
	<monogr>
		<title level="j">Genes -Gene Synthesis | Twist Bioscience</title>
		<imprint>
			<date type="published" when="2018-11-19">19th November 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Resurrecting ancient genes: experimental analysis of extinct molecules</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Thornton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Rev. Genet</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">366</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Experimental assay of a fitness landscape on a macroevolutionary scale</title>
		<author>
			<persName><forename type="first">V</forename><surname>Pokusaeva</surname></persName>
		</author>
		<idno type="DOI">10.1101/222778</idno>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="volume">222778</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Hidden State UniRep 64 UniRep 64-unit Final Hidden State UniRep 64 UniRep 64-unit Final Cell State UniRep 64 UniRep 256-unit Avg. Hidden State UniRep 256 UniRep 256-unit Final Hidden State UniRep 256 UniRep 1900-unit Avg. Hidden State UniRep 1900 UniRep 1900-unit Final Hidden State UniRep 1900 UniRep 1900-unit Final Cell State UniRep</title>
		<idno>RGN RGN 3200 16 UniRep 64-unit Avg</idno>
		<imprint>
			<date type="published" when="1900">1900 Doc2Vec Original k=3 w=7 Doc2Vec 64</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m">Doc2Vec Scrambled k=3 w=5 Doc2Vec</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m">Doc2Vec Random k=3 w=7 Doc2Vec 64</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m">Doc2Vec Uniform k=4 w=1 Doc2Vec 64 21 UniRep 64-unit Fusion UniRep 192 UniRep 256-unit Fusion UniRep 512</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">UniRep Fusion UniRep</title>
		<imprint>
			<biblScope unit="volume">5700</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
