<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">STOCHASTIC MODEL-BASED MINIMIZATION OF WEAKLY CONVEX FUNCTIONS *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Damek</forename><surname>Davis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Operations Research and Information Engineering</orgName>
								<orgName type="institution">Cornell University</orgName>
								<address>
									<settlement>Ithaca</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dmitriy</forename><surname>Drusvyatskiy</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Mathematics</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<postCode>98195</postCode>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">STOCHASTIC MODEL-BASED MINIMIZATION OF WEAKLY CONVEX FUNCTIONS *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F22FB90E3E19C2FF5A28DDD798BA93E6</idno>
					<idno type="DOI">10.1137/18M1178244</idno>
					<note type="submission">Received by the editors March 30, 2018; accepted for publication (in revised form) September 10, 2018;</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>stochastic</term>
					<term>subgradient</term>
					<term>proximal</term>
					<term>prox-linear</term>
					<term>Moreau envelope</term>
					<term>weakly convex AMS subject classifications. 65K05</term>
					<term>65K10</term>
					<term>90C15</term>
					<term>90C30</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider a family of algorithms that successively sample and minimize simple stochastic models of the objective function. We show that under reasonable conditions on approximation quality and regularity of the models, any such algorithm drives a natural stationarity measure to zero at the rate O(k -1/4 ). As a consequence, we obtain the first complexity guarantees for the stochastic proximal point, proximal subgradient, and regularized Gauss-Newton methods for minimizing compositions of convex functions with smooth maps. The guiding principle, underlying the complexity guarantees, is that all algorithms under consideration can be interpreted as approximate descent methods on an implicit smoothing of the problem, given by the Moreau envelope. Specializing to classical circumstances, we obtain the long-sought convergence rate of the stochastic projected gradient method, without batching, for minimizing a smooth function on a closed convex set.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction.</head><p>Stochastic optimization plays a central role in the statistical sciences, underlying all aspects of learning from data. The goal of stochastic optimization in data science is to learn a decision rule from a limited data sample, which generalizes well to the entire population. Learning such a decision rule amounts to minimizing the regularized population risk: Here, ξ encodes the population data, which is assumed to follow some fixed but unknown probability distribution P . The functions f and r play qualitatively different roles. Typically, f (x, ξ) evaluates the loss of the decision rule parametrized by x on a data point ξ. In contrast, the function r : R d → R ∪ {+∞} models constraints on the parameters x or encourages x to have some low-dimensional structure, such as sparsity or low rank. Within a Bayesian framework, the regularizer r can model prior distributional information on x.</p><p>The pioneering 1951 work of Robbins and Monro <ref type="bibr" target="#b53">[53]</ref> gave the first procedure for solving (SO) in the setting when f (•, ξ) are smooth and strongly convex and r = 0, thereby inspiring decades of further research. Among such algorithms, the proximal stochastic (sub)gradient method is the most successful and widely used in practice. This method constructs a sequence of approximations x t of the minimizer of (SO) by iterating (SG) sample ξ t ∼ P, set x t+1 = prox αtr (x t -α t ∇ x f (x t , ξ t )) , where α t &gt; 0 is an appropriate control sequence and prox αr (•) is the proximal map prox αr (x) := argmin y r(y)</p><formula xml:id="formula_0">+ 1 2α y -x 2 .</formula><p>Thus, in each iteration, the method travels from x t in the direction opposite to a sampled gradient ∇ x f (x t , ξ t ), and follows this with a proximal operation.</p><p>Nonsmooth convex problems may be similarly optimized by replacing sample gradients by sample subgradients v t ∈ ∂ x f (x t , ξ t ), where ∂ x f (x, ξ) is the subdifferential in the sense of convex analysis <ref type="bibr" target="#b58">[58]</ref>. Even more broadly, when f (•, ξ) is neither smooth nor convex, the symbol ∂ x f (•, ξ) may refer to a generalized subdifferential. The formal definition of the subdifferential appears in section 2 and is standard in the optimization literature (e.g., <ref type="bibr" target="#b57">[57,</ref><ref type="bibr">Definition 8.3]</ref>). The reader should keep in mind that in practice, the functions f (•, ξ t ) are often all differentiable along the iterate sequence {x t }. Therefore from the viewpoint of implementation, one always computes the true gradients of the sampled functions, using conventional means. On the other hand, the nonsmoothness cannot be ignored in the analysis, since (i) the gradients do not vary continuously and (ii) the objective function can be nonsmooth at every limit point of the process. We will expand on these two observations shortly.</p><p>Performance of stochastic optimization methods is best judged by their sample complexity-the number of i.i.d. realizations ξ 1 , . . . , ξ N ∼ P needed to reach a desired accuracy of the decision rule. Classical results <ref type="bibr" target="#b44">[44]</ref> stipulate that for convex problems, it suffices to generate O(ε -2 ) samples to reach functional accuracy ε in expectation, and this complexity is unimprovable without making stronger assumptions. For smooth problems, the stochastic gradient method has sample complexity of O(ε -4 ) to reach a point with the gradient norm at most ε in expectation <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b68">68]</ref>.</p><p>Despite the ubiquity of the stochastic subgradient method in applications, its sample complexity is not yet known for any reasonably wide class of problems beyond those that are smooth or convex. This is somewhat concerning as the stochastic subgradient method is the simplest and most widely used optimization algorithm for large-scale problems arising in machine learning and is the core optimization subroutine in industry-backed software libraries, such as Google's TensorFlow <ref type="bibr" target="#b0">[1]</ref>.</p><p>The purpose of this work is to provide the first sample complexity bounds for a number of popular stochastic algorithms on a reasonably broad class of nonsmooth and nonconvex optimization problems. The problem class we consider captures a variety of important computational tasks in data science, as we illustrate below, while the algorithms we analyze include the proximal stochastic subgradient, proximal point, and regularized Gauss-Newton methods. Before stating the complexity guarantees, we must first explain the "stationarity measure" that we will use to judge the quality of the iterates. It is this stationarity measure that tends to zero at a controlled rate.</p><p>The search for stationary points. Convex optimization algorithms are judged by the rate at which they decrease the function value along the iterate sequence. Analysis of smooth optimization algorithms focuses instead on the magnitude of the gradients along the iterates. The situation becomes quite different for problems that are neither smooth nor convex. Downloaded 01/23/19 to 129.215. <ref type="bibr">17.190</ref>. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php As in the smooth setting, the primary goal of nonsmooth nonconvex optimization is the search for stationary points. A point x ∈ R d is called stationary for the problem (SO) if the inclusion 0 ∈ ∂ϕ(x) holds. In "primal terms," these are precisely the points where the directional derivative of ϕ is nonnegative in every direction. Indeed, under mild conditions on ϕ, equality holds <ref type="bibr" target="#b57">[57,</ref><ref type="bibr">Proposition 8.32]</ref>:</p><formula xml:id="formula_1">dist(0; ∂ϕ(x)) = -inf v: v ≤1 ϕ (x; v).</formula><p>Thus a point x, satisfying dist(0; ∂ϕ(x)) ≤ ε, approximately satisfies first-order necessary conditions for optimality.</p><p>An immediate difficulty in analyzing stochastic methods for nonsmooth and nonconvex problems is that it is not a priori clear how to measure the progress of the algorithm. Neither the functional suboptimality gap, ϕ(x t ) -min ϕ, nor the stationarity measure, dist(0; ∂ϕ(x t )), necessarily tend to zero along the iterate sequence. This difficulty persists even in the simplest setting of minimizing a smooth function on a closed convex set by the stochastic projected gradient method. Indeed, what is missing is a continuous measure of stationarity to monitor, instead of the highly discontinuous function x → dist(0; ∂ϕ(x)).</p><p>Weak convexity and the Moreau envelope. In this work, we focus on a class of problems that naturally admit a continuous measure of stationarity. We say that a function g : R d → R is ρ-weakly convex if the assignment x → g(x) + ρ 2 x 2 is convex. The class of weakly convex functions, first introduced in English in <ref type="bibr" target="#b49">[49]</ref>, is broad. It includes all convex functions and smooth functions with Lipschitz continuous gradient. More generally, any function of the form</p><formula xml:id="formula_2">g(x) = h(c(x)),</formula><p>with h convex and Lipschitz and c a smooth map with Lipschitz Jacobian, is weakly convex <ref type="bibr" target="#b28">[28,</ref><ref type="bibr">Lemma 4.2]</ref>. Notice that such composite functions need not be smooth nor convex; instead, the composite function class nicely interpolates between the smooth and convex settings. Classical literature highlights the importance of weak convexity in optimization <ref type="bibr" target="#b50">[50,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b55">55]</ref>, while recent advances in statistical learning and signal processing have further reinvigorated the problem class. Nonlinear least squares, phase retrieval <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b33">33]</ref>, minimization of the conditional value-at-risk <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b59">59]</ref>, graph synchronization <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b62">62]</ref>, covariance estimation <ref type="bibr" target="#b15">[16]</ref>, and robust principal component analysis <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14]</ref> directly lead to weakly convex formulations. For a recent discussion on the role of weak convexity in large-scale optimization, see, e.g., <ref type="bibr" target="#b25">[25]</ref>.</p><p>It has been known since Nurminskii's work <ref type="bibr" target="#b48">[48,</ref><ref type="bibr" target="#b49">49]</ref> that when the functions f (•, ξ) are ρ-weakly convex and r = 0, the stochastic subgradient method on (SO) generates an iterate sequence that subsequentially converges to a stationary point of the problem, almost surely. Nonetheless, the sample complexity of the basic method and of its proximal extension has remained elusive. Our approach to resolving this open question relies on an elementary observation: weakly convex problems naturally admit a continuous measure of stationarity through implicit smoothing. The key construction we use is the Moreau envelope <ref type="bibr" target="#b42">[42]</ref>:</p><formula xml:id="formula_3">ϕ λ (x) := min y ϕ(y) + 1 2λ y -x 2 ,</formula><p>where λ &gt; 0. Standard results (e.g., <ref type="bibr" target="#b42">[42]</ref> and <ref type="bibr" target="#b54">[54,</ref><ref type="bibr">Theorem 31.5]</ref>) show that as long as ϕ is ρ-weakly convex and λ &lt; ρ -1 , the envelope ϕ λ is C given by</p><formula xml:id="formula_4">∇ϕ λ (x) = λ -1 (x -prox λϕ (x)).</formula><p>See Figure <ref type="figure" target="#fig_2">1</ref>(a) for an illustration.  When f is C 1 -smooth with β-Lipschitz gradient and there is no regularizer r, the norm ∇ϕ 1/β (x) is proportional to the magnitude of the true gradient ∇f (x) . More generally, when f is C 1 -smooth and r is nonzero, the norm ∇ϕ 1/β (x) is proportional to the size of the proximal gradient step, commonly used to measure convergence in additive composite minimization <ref type="bibr" target="#b46">[46]</ref>. See the end of section 2.2 for a precise statement. In the broader nonsmooth setting, the norm of the gradient ∇ϕ λ (x) has an intuitive interpretation in terms of near-stationarity for the target problem min x ϕ(x). Namely, the definition of the Moreau envelope directly implies that for any point x ∈ R d , the proximal point x := prox λϕ (x) satisfies</p><formula xml:id="formula_5">     x -x = λ ∇ϕ λ (x) , ϕ(x) ≤ ϕ(x), dist(0; ∂ϕ(x)) ≤ ∇ϕ λ (x) .</formula><p>Thus a small gradient ∇ϕ λ (x) implies that x is near some point x that is nearly stationary for ϕ; see Figure <ref type="figure" target="#fig_3">1(b</ref>). In the language of numerical analysis, one can interpret algorithms that drive the gradient of the Moreau envelope to zero as being "backward-stable." For a longer discussion of the near-stationarity concept, we refer to reader to <ref type="bibr" target="#b25">[25]</ref> or <ref type="bibr">[28, section 4.1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions.</head><p>In this paper, we show that as long as the functions f (•, ξ) + r(•) are ρ-weakly convex and mild Lipschitz conditions hold, the proximal stochastic subgradient method will generate a point x satisfying E ∇ϕ 1/2ρ (x) ≤ ε after at most O(ε -4 ) iterations. This is perhaps surprising, since neither the Moreau envelope nor the proximal map of ϕ explicitly appear in the definition of the stochastic proximal subgradient method. This work appears to be the first to recognize the Moreau envelope as a useful potential function for analyzing subgradient methods.</p><p>Indeed, we will show that the worst-case complexity O(ε -4 ) holds for a much wider family of algorithms than the stochastic subgradient method. Setting the stage, recall that the stochastic subgradient method relies on sampling subgradient estimates of f , or equivalently sampling good linear models of the function. More broadly, suppose Downloaded 01/23/19 to 129.215.17.190. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php that f is an arbitrary function (not necessarily written as an expectation), and for every point x we have available a family of "models" {f x (•, ξ)} ξ∼P , indexed by a random element ξ ∼ P . The oracle concept we use assumes that the only access to f is by sampling a model f x (•, ξ) centered around any base point x. Naturally, to make use of such models we must have some control on their approximation quality. We will call the assignment (x, y, ξ) → f x (y, ξ) a stochastic one-sided model if it satisfies</p><formula xml:id="formula_6">(1.1) E ξ [f x (x, ξ)] = f (x) and E ξ [f x (y, ξ) -f (y)] ≤ τ 2 y -x 2 2 ∀x, y.</formula><p>Thus, in each expectation, each model f x (•, ξ) should lower bound f up to a quadratic error, while agreeing with f at the base-point x. See Figure <ref type="figure" target="#fig_8">2</ref> for an illustration.</p><formula xml:id="formula_7">0.5 f f x 0.5 f + (x -0.5) 2 f x Fig. 2. Illustration of a one-sided model: f (x) = |x 2 -1|, f 0.5 (y) = |1.25 -y|.</formula><p>The methods we consider then simply iterate the following steps:</p><p>(1.2) sample ξ t ∼ P,</p><formula xml:id="formula_8">set x t+1 = argmin y f xt (y, ξ t ) + r(y) + 1 2α t y -x t 2 .</formula><p>We will prove that under mild Lipschitz conditions and provided that each function</p><formula xml:id="formula_9">f x (•, ξ) + r(•) is ρ-weakly convex, Algorithm 1.2 finds a point x with E ∇ϕ 1/2ρ (x) ≤ ε after at most O(ε -4 ) iterations.</formula><p>The main principle underlying the convergence guarantees is interesting in its own right. We will show that Algorithm 1.2 can be interpreted as an approximate descent method on the Moreau envelope:</p><formula xml:id="formula_10">(1.3) E[ϕ λ (x t+1 )] ≤ E[ϕ λ (x t )] -α t c 1 E[ ∇ϕ λ (x t ) 2 ] + α 2 t c 2 ,</formula><p>where λ, c 1 , c 2 are problem dependent constants. When the models f x (•, ξ) are true under-estimators of f in expectation, meaning that (1.1) holds with τ = 0, and the functions f x (•, ξ) + r(•) are convex, one expects guarantees that are analogous to the stochastic subgradient method for convex minimization. Indeed, we will show that under these circumstances, Algorithm <ref type="bibr">(</ref> To crystallize the ideas, consider the setting of stochastic composite optimization, studied recently by Duchi and Ruan <ref type="bibr" target="#b30">[30]</ref>:</p><formula xml:id="formula_11">f (x, ξ) = h c(x, ξ), ξ ,</formula><p>where the functions h(•, ξ) are convex and the maps c(•, ξ) are smooth. Note that in the simplest setting when P is a discrete distribution on {1, . . . , m}, problem (SO) reduces to minimizing a regularized empirical average of composite functions:</p><formula xml:id="formula_12">min x∈R d ϕ(x) = f (x) + r(x), where f (x) = 1 m m i=1 h i (c i (x)).</formula><p>The following three stochastic one-sided models appear naturally:</p><formula xml:id="formula_13">f x (y, ξ) = f (x) + ∇c(x, ξ) T w(x, ξ), y -x , (1.4) f x (y, ξ) = h c(x, ξ) + ∇c(x, ξ)(y -x), ξ , (1.5) f x (y, ξ) = h c(y, ξ)), (1.6)</formula><p>where w(x, ξ) ∈ ∂h(c(x, ξ), ξ) is a subgradient selection. Each iteration of Algorithm 1.2 with model (1.4) reduces to the stochastic proximal subgradient update, already mentioned previously. When equipped with model (1.5), the method becomes the stochastic prox-linear algorithm-a close variant of Gauss-Newton. Both of these schemes were recently investigated in <ref type="bibr" target="#b30">[30]</ref>, where the authors showed that almost surely all limit points are stationary for problem (SO). Algorithm 1.2 equipped with model (1.6) is the stochastic proximal-point algorithm. This scheme was recently considered for convex minimization in <ref type="bibr" target="#b60">[60,</ref><ref type="bibr" target="#b65">65,</ref><ref type="bibr" target="#b66">66]</ref> and extended to monotone inclusions in <ref type="bibr" target="#b8">[9]</ref>. Notice that in contrast to the stochastic proximal subgradient method, the stochastic proximal point and prox-linear algorithms require solving an auxiliary subproblem. The advantage of these two schemes is that models (1.5) and (1.6) provide much finer approximation quality in that they are two-sided instead of one-sided. Indeed, empirical evidence <ref type="bibr">[30, section 4]</ref> suggests that the latter two algorithms can perform significantly better and are much more robust to the choice of the sequence α t . We also observe this phenomenon in our experiments in section 5.</p><p>The outline of the paper is as follows. We begin with section 2, which records some basic notation and results focusing on weak convexity and the Moreau envelope. This section also presents a number of illustrative applications that will be readily amenable to our algorithmic techniques. We then present three distinct convergence arguments: for the stochastic projected subgradient method in section 3.1, for the stochastic proximal subgradient method in section 3.2, and for algorithms based on general stochastic one-sided models in section 4. Each argument has its own virtues. In particular, our guarantees for the stochastic projected subgradient method place no restriction on the parameters α t to be used, in contrast to our latter results. The argument for the stochastic proximal subgradient method generalizes verbatim to the setting when f is C 1 -smooth and the stochastic gradient estimator has bounded variance, instead of a bounded second moment that we assume elsewhere. Section 4 applies to the most general classes of algorithms including stochastic proximal subgradient, prox-linear, and proximal-point methods.</p><p>Context and related literature. The convergence guarantees we develop for the proximal stochastic subgradient method are new even in simplified cases. Two Downloaded 01/23/19 to 129.215.17.190. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php such settings are (i) when f (•, ξ) are smooth and r is the indicator function of a closed convex set, and (ii) when f is nonsmooth, we have explicit access to the exact subgradients of f , and r = 0.</p><p>Analogous convergence guarantees when r is an indicator function of a closed convex set were recently established for a different algorithm in <ref type="bibr" target="#b23">[23]</ref>. This scheme proceeds by directly applying the gradient descent method to the Moreau envelope ϕ λ , with each proximal point prox λϕ (x) approximately evaluated by a convex subgradient method. In contrast, we show here that the basic stochastic subgradient method in the fully proximal setting, and without any modification or parameter tuning, already satisfies the desired convergence guarantees.</p><p>Our work also improves in two fundamental ways on the results in the seminal papers on the stochastic proximal gradient method for smooth functions <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b68">68]</ref>: first, we allow f (•, ξ) to be nonsmooth, and second, even when f (•, ξ) are smooth, we do not require the variance of our stochastic estimator for ∇f (x t ) to decrease as a function of t. The second contribution removes the well-known "mini-batching" requirements common to <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b68">68]</ref>, while the first significantly expands the class of functions for which the rate of convergence of the stochastic proximal subgradient method is known. It is worthwhile to mention that our techniques rely on weak convexity of the regularizer r, while no such assumption is made in <ref type="bibr" target="#b68">[68]</ref>.</p><p>The results in this paper are orthogonal to the recent line of work on accelerated rates of convergence for smooth nonconvex finite sum minimization problems, e.g., <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b52">52]</ref>. These works crucially exploit the finite sum structure and/or (higher order) smoothness of the objective functions to push beyond the O(ε -4 ) complexity. We leave it as an intriguing open question whether such improvement is possible for the nonsmooth weakly convex setting we consider here.</p><p>The unifying concept of stochastic one-sided models has not been explicitly used before. The complexity guarantees for the proximal stochastic subgradient, proxlinear, and proximal point methods (Theorem 4.3) for stochastic composite minimization are new and nicely complement the recent paper <ref type="bibr" target="#b30">[30]</ref>. There, the authors proved that almost surely all limit points of the first two methods are stationary. For a historical account of the prox-linear method, see, e.g., <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b39">39]</ref> and the references therein. For a systematic study of two-sided models (e.g., (1.5) and (1.6)) in optimization, see <ref type="bibr" target="#b26">[26]</ref>. Stochastic compositional problems have also appeared in a parallel line of work beginning with <ref type="bibr" target="#b67">[67]</ref>. There, the authors require the entire composite function to be either convex or smooth. We make no such assumptions here.</p><p>The convergence rate of Algorithm 1.2 in terms of function values in the convex setting is presented in Theorems 4.4 and 4.5, and is intriguing. Even specializing to the proximal stochastic subgradient method, Theorems 4.4 and 4.5 appear to be stronger than the state of the art. Namely, in contrast to previous work <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b31">31]</ref>, the norms of the subgradients of r do not enter the complexity bounds established in Theorem 4.4, while Theorem 4.5 extends the nonuniform averaging technique of <ref type="bibr" target="#b61">[61]</ref> for strongly convex minimization to the fully proximal setting.</p><p>The observation that Algorithm 1.2 is an approximate descent method on the Moreau envelope (1.3) is tangentially related to the recent work on "inexact firstorder oracles" in convex optimization <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b47">47]</ref> and its partial extensions to nonconvex settings <ref type="bibr" target="#b32">[32]</ref>. Expanding on the precise relationship between the techniques is an interesting open question.</p><p>2. Basic notation and preliminaries. Throughout, we consider a Euclidean space R d endowed with an inner product •, • and the induced norm x =</p><p>x, x . Downloaded 01/23/19 to 129.215.17.190. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DAMEK DAVIS AND DMITRIY DRUSVYATSKIY</head><p>For any function ϕ : R d → R ∪ {∞}, the domain and epigraph are the sets</p><formula xml:id="formula_14">dom ϕ = {x ∈ R d : ϕ(x) &lt; ∞}, epi ϕ = {(x, r) ∈ R d × R : r ≥ ϕ(x)},</formula><p>respectively. We say that ϕ :</p><formula xml:id="formula_15">R d → R ∪ {∞} is closed if epi ϕ is a closed set.</formula><p>This work focuses on algorithms for minimizing weakly convex functions.</p><formula xml:id="formula_16">1 A function ϕ : R d → R ∪ {∞} is called ρ-weakly convex if the assignment x → ϕ(x) + ρ 2</formula><p>x 2 is a convex function. In this section, we summarize some basic properties of this function class. All results we state in this section are either standard or follow quickly from analogous results for convex functions. For further details and a historical account, we refer the reader to the short note <ref type="bibr" target="#b25">[25]</ref>. Example 2.1 (robust phase retrieval). Phase retrieval is a common computational problem, with applications in diverse areas such as imaging, X-ray crystallography, and speech processing. For simplicity, we will consider the version of the problem over the reals. The (real-valued) phase retrieval problem seeks to determine a point x satisfying the magnitude conditions</p><formula xml:id="formula_17">| a i , x | ≈ b i for i = 1, . . . , m,</formula><p>where a i ∈ R d and b i ∈ R are given. Whenever gross outliers occur in the measurements b i , the following robust formulation of the problem is appealing <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b33">33]</ref>:</p><formula xml:id="formula_18">min x 1 m m i=1 | a i , x 2 -b 2 i |.</formula><p>The use of the 1 -penalty promotes strong recovery and stability properties even in the noiseless setting <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b33">33]</ref>. Numerous other nonconvex approaches to phase retrieval exist, which rely on different problem formulations; see, for example, <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b63">63]</ref>.</p><p>Example 2.2 (covariance matrix estimation). The problem of covariance estimation from quadratic measurements, introduced in <ref type="bibr" target="#b15">[16]</ref>, is a higher rank variant of phase retrieval. Let a 1 , . . . , a m ∈ R d be measurement vectors. The goal is to recover a low rank decomposition of a covariance matrix X XT , with X ∈ R d×r for a given 0 ≤ r ≤ d, from quadratic measurements</p><formula xml:id="formula_19">b i ≈ a T i X XT a i = Tr( X XT a i a T i ).</formula><p>Note that we can only recover X up to multiplication by an orthogonal matrix. This problem arises in a variety of contexts, such as covariance sketching for data streams and spectrum estimation of stochastic processes. We refer the reader to <ref type="bibr" target="#b15">[16]</ref> for details. Supposing that m is even, the authors of <ref type="bibr" target="#b15">[16]</ref> show that the following potential function has strong recovery guarantees under usual statistical assumptions:</p><p>(2.2) min</p><formula xml:id="formula_20">X∈R d×r 1 m m i=1 XX T , a 2i a T 2i -a 2i-1 a T 2i-1 -(b 2i -b 2i-1 ) .</formula><p>Example 2.3 (blind deconvolution and biconvex compressive sensing). The problem of blind deconvolution seeks to recover a pair of vectors in two low-dimensional structured spaces from their pairwise convolution. This problem occurs in a number of fields, such as astronomy and computer vision <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b38">38]</ref>. For simplicity focusing on the real-valued case, one appealing formulation of the problem reads</p><formula xml:id="formula_21">min x,y 1 m m i=1 | u i , x v i , y -b i |,</formula><p>where u i and v i are known vectors, and b i are the convolution measurements. More broadly, problems of this form fall within the area of biconvex compressive sensing <ref type="bibr" target="#b40">[40]</ref>. Similarly to the previous two examples, the use of the 1 -penalty on the residuals yields strong recovery and stability guarantees under statistical assumptions. Details will appear in a forthcoming paper.</p><p>Example 2.4 (sparse dictionary learning). The problem of sparse dictionary learning seeks to find a sparse representation of the input data as a linear combination of basic atoms, which comprise the "dictionary." This technique is routinely used in image and video processing. More formally, given a set of vectors {x 1 , . . . , x m } ⊂ R d , we wish to find a matrix D ∈ R d×n and sparse weights {r 1 , . . . , r m } ⊂ R n such that the error x i -Dr i 2 is small for all i. The following is a robust variant of the standard relaxation of the problem:</p><formula xml:id="formula_22">(2.3) min D∈R d×n , r1∈R n 1 m m i=1 x i -Dr i 2 + λ r i 1 subject to D i ≤ 1 ∀i.</formula><p>More precisely, typical formulations use the squared norm • 2 2 instead of the norm • 2 ; see, e.g., <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b64">64]</ref>. When there are outliers in the data (i.e., not all of the data vectors x i can be sparsely represented), the formulation (2.3) may be more appealing.</p><p>Example 2.5 (robust PCA). In robust principal component analysis, one seeks to identify sparse corruptions of a low-rank matrix <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14]</ref>. One typical example is image deconvolution, where the low-rank structure models the background of an image while the sparse corruption models the foreground. Formally, given an m × n matrix M , the goal is to find a decomposition M = L + S, where L is low rank and S is sparse. A common relaxation of the problem is min</p><formula xml:id="formula_23">U ∈R m×r ,V ∈R n×r U V T -M 1 ,</formula><p>where r is the target rank. As is common, the entrywise 1 -norm encourages a sparse residual U V Example 2.6 (conditional value-at-risk). As in the introduction, let f (x, ξ) be a loss of a decision rule parametrized by x on a data point ξ, where the population data follows a probability distribution ξ ∼ P . Rather than minimizing the expectation f (x) = E ξ∼P f (x, ξ), one often wishes to minimize the conditional expectation of the random variable f (x, •) over its α-tail for some fixed α ∈ (0, 1). This quantity is called the conditional value-at-risk (cVaR) and it has a distinguished history. In particular, it is well known from the seminal work <ref type="bibr" target="#b59">[59]</ref> that minimizing cVaR of the loss function can be formalized as</p><formula xml:id="formula_24">2 min γ∈R,x∈R d (1 -α)γ + E ξ∼P [(f (x, ξ) -γ) + ],</formula><p>where we use the notation r + = max{0, r}. If the loss function f (•, ξ) is ρ-weakly convex for a.e. ξ, then the entire objective function is ρ-weakly convex jointly in (γ, x). In particular, this is the case when f (•, ξ) is C 1 -smooth with Lipschitz gradient, or when the loss f (•, ξ) is convex for a.e. ξ. Notice that the terms inside the expectation (f (•, ξ) -γ) + are always nonsmooth, even if the loss function f (•, ξ) is smooth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Subdifferential and the Moreau envelope.</head><p>A key property of convex functions is that any subgradient yields a global affine under-estimator of the function. It is this availability of global under-estimators that enables convergence guarantees for nonsmooth convex optimization. An analogous property is true for weakly convex functions, where the subdifferential is meant in a broader variational analytic sense and the affine under-estimators are replaced by concave quadratic under-estimators. We now formalize this observation.</p><p>Consider a function ϕ : R d → R ∪ {∞} and a point x ∈ R d , with ϕ(x) finite. The subdifferential of ϕ at x, denoted ∂ϕ(x), consists of all vectors v satisfying ϕ(y) ≥ ϕ(x) + v, y -x + o( y -x ) as y → x.</p><p>We set ∂ϕ(x) = ∅ for all x / ∈ dom ϕ. When ϕ is C 1 -smooth, the subdifferential ∂ϕ(x) consists only of the gradient {∇ϕ(x)}, while for convex functions it reduces to the subdifferential in the sense of convex analysis. The following characterization of weak convexity is standard; we provide a short proof for completeness. Lemma 2.1 (subdifferential characterization). The following are equivalent for any lower-semicontinuous function ϕ : R d → R ∪ {∞}.</p><p>1. The function f is ρ-weakly convex.</p><p>2. The approximate secant inequality holds:</p><formula xml:id="formula_25">(2.4) ϕ(λx + (1 -λ)y) ≤ λϕ(x) + (1 -λ)ϕ(y) + ρλ(1-λ) 2 x -y 2 for all x, y ∈ R d and λ ∈ [0, 1]. 3.</formula><p>The subgradient inequality holds:</p><formula xml:id="formula_26">(2.5) ϕ(y) ≥ ϕ(x) + v, y -x - ρ 2 y -x 2 ∀x, y ∈ R d , v ∈ ∂ϕ(x).</formula><p>4. The subdifferential map is hypomonotone:</p><formula xml:id="formula_27">(2.6) v -w, x -y ≥ -ρ x -y 2</formula><p>for all x, y ∈ R d , v ∈ ∂ϕ(x), and w ∈ ∂ϕ(y).</p><p>If ϕ is C 2 -smooth, then the four properties above are all equivalent to</p><formula xml:id="formula_28">∇ 2 ϕ(x) -ρI ∀x ∈ R d .</formula><p>Proof. Algebraic manipulation shows that the usual secant inequality on the function ϕ + ρ 2 • 2 is precisely the approximate secant inequality (2.4) on ϕ. Therefore we deduce the equivalence 1 ⇔ 2. Suppose now 1 holds and define the function g(x) = ϕ(x) + ρ 2 x 2 . Note the equality ∂g(x) = ∂ϕ(x) + ρx; see, e.g., [57, Exercise 8.8]. Since g is convex, the inequality g(y) ≥ g(x) + v + ρx, y -x holds for all x, y ∈ R d and v ∈ ∂ϕ(x). Algebraic manipulations then immediately imply (2.5), and therefore 3 holds. The implication 3 ⇒ 4 follows by adding to (2.5) the analogous inequality with x and y interchanged. Finally suppose that 4 holds. Algebraic manipulations then imply that the subdifferential of ϕ + ρ 2 • 2 is a globally monotone map. Applying <ref type="bibr" target="#b57">[57,</ref><ref type="bibr">Theorem 12</ref>.17], we conclude that ϕ + ρ 2 • 2 is convex and therefore 1 holds. Finally the characterization of weak convexity when ϕ is C 2 -smooth is immediate from the second-order characterization of convexity of the function ϕ + ρ 2 • 2 . For any function ϕ : R d → R ∪ {∞} and λ &gt; 0, the Moreau envelope and the proximal map are defined by</p><formula xml:id="formula_29">ϕ λ (x) := min y ϕ(y) + 1 2λ y -x 2 , (2.7) prox λϕ (x) := argmin y ϕ(y) + 1 2λ y -x 2 ,</formula><p>respectively. Classically, the Moreau envelope of a convex function is C 1 -smooth for any λ &gt; 0; see <ref type="bibr" target="#b42">[42]</ref>. The same is true for weakly convex functions, provided λ is sufficiently small. Lemma 2.2. Consider a ρ-weakly convex function ϕ : R d → R ∪ {∞}. Then for any λ ∈ (0, ρ -1 ), the Moreau envelope ϕ λ is C 1 -smooth with gradient given by ∇ϕ λ (x) = λ -1 (x -prox λϕ (x)).</p><p>See Figure <ref type="figure" target="#fig_2">1</ref>(a) for an illustration. As mentioned in the introduction, the norm of the gradient ∇ϕ λ (x) has an intuitive interpretation in terms of near-stationarity. Namely, the optimality conditions for the minimization problem in (2.7) directly imply that for any point x ∈ R d , the proximal point x := prox λϕ (x) satisfies</p><formula xml:id="formula_30">     x -x = λ ∇ϕ λ (x) , ϕ(x) ≤ ϕ(x), dist(0; ∂ϕ(x)) ≤ ∇ϕ λ (x) .</formula><p>Thus a small gradient ∇ϕ λ (x) implies that x is near some point x that is nearly stationary for ϕ; see Figure <ref type="figure" target="#fig_3">1(b)</ref>. All of the convergence guarantees that we present will be in terms of the quantity ∇ϕ λ (x) .</p><p>It is important to keep in mind that in more classical circumstances, the size of the gradient of the Moreau envelope is proportional to more familiar quantities. To illustrate, consider the optimization problem <ref type="bibr">(2.8)</ref> min  <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b46">46]</ref> focusing on this problem class highlights the role of the prox-gradient mapping:</p><formula xml:id="formula_31">x∈R d ϕ(x) := f (x) + r(x</formula><formula xml:id="formula_32">(2.9) G λ (x) = λ -1 (x -prox λr (x -λ∇f (x))) .</formula><p>Indeed, complexity estimates are typically stated in terms of the norm G 1/ρ (x) . On the other hand, one can show that the two stationarity measures, ∇ϕ 1/2ρ (x) and G 1/ρ (x) , are proportional [27, Theorem 4.5]:</p><formula xml:id="formula_33">1 4 ∇ϕ 1/2ρ (x) ≤ G 1/ρ (x) ≤ 3 2 1 + 1 √ 2 ∇ϕ 1/2ρ (x) ∀x ∈ R d .</formula><p>Thus when specializing our results to the setting (2.8), all of the convergence guarantees can be immediately translated in terms of the prox-gradient mapping.</p><p>3. Proximal stochastic subgradient method. In this section, we analyze the proximal stochastic subgradient method for weakly convex minimization. Throughout, we consider the optimization problem <ref type="bibr">(3.1)</ref> min</p><formula xml:id="formula_34">x∈R d ϕ(x) = f (x) + r(x),</formula><p>where r : R d → R ∪ {+∞} is a closed convex function and f : R d → R is a ρ-weakly convex function. We assume that the only access to f is through a stochastic subgradient oracle.</p><p>Assumption A (stochastic subgradient oracle). Fix a probability space (Ω, F, P ) and equip R d with the Borel σ-algebra. We make the following three assumptions. Assumptions (A1), (A2), (A3) are standard in the literature on stochastic subgradient methods: assumptions (A1) and (A2) are identical to assumptions (A1) and (A2) in <ref type="bibr" target="#b43">[43]</ref>, while assumption (A3) is the same as the assumption listed in <ref type="bibr" target="#b43">[43,</ref><ref type="bibr">Equation (2.5)</ref>]. We will investigate the efficiency of the proximal stochastic subgradient method, described in Algorithm 3.1.</p><p>Algorithm 3.1 Proximal stochastic subgradient method. Input: x 0 ∈ dom r, a sequence {α t } t≥0 ⊂ R + , and iteration count T .</p><p>Step t = 0, . . . , T :</p><formula xml:id="formula_35">sample ξ t ∼ P, set x t+1 = prox αtr (x t -α t G(x t , ξ t ))</formula><p>.</p><p>Sample t * ∈ {0, . . . , T } according to P(t</p><formula xml:id="formula_36">* = t) = α t / T i=0 α i . Return x t *</formula><p>Henceforth, the symbol E t [•] will denote the expectation conditioned on all the realizations ξ 0 , ξ 1 , . . . , ξ t-1 . Downloaded 01/23/19 to 129.215.17.190. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php 3.1. Projected stochastic subgradient method. Our analysis of Algorithm 3.1 is shorter and more transparent when r is the indicator function of a closed, convex set X . This is not surprising, since projected subgradient methods are typically much easier to analyze than their proximal extensions (e.g., <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b31">31]</ref>). Note that (3.1) then reduces to the constrained problem</p><formula xml:id="formula_37">(3.2) min x∈X f (x),</formula><p>and the proximal map prox αr (•) becomes the nearest point projection proj X (•). Thus throughout section 3.1, we suppose that assumptions (A1), (A2), and (A3) hold and that r(•) is the indicator function of a closed convex set X . The following is the main result of this section.</p><p>Theorem 3.1 (stochastic projected subgradient method). Let x t * be the point returned by Algorithm 3.1. Then in terms of any constant ρ &gt; ρ, the estimate</p><formula xml:id="formula_38">(3.3) E ϕ 1/ ρ(x t+1 ) ≤ E[ϕ 1/ ρ(x t )] - α t (ρ -ρ) ρ E ∇ϕ 1/ ρ(x t ) 2 + α 2 t ρL 2<label>2</label></formula><p>holds, and therefore we have</p><formula xml:id="formula_39">(3.4) E ∇ϕ 1/ ρ(x t * ) 2 ≤ ρ ρ -ρ • (ϕ 1/ ρ(x 0 ) -min ϕ) + ρL 2 2 T t=0 α 2 t T t=0 α t .</formula><p>In particular, if Algorithm 3.1 uses the constant parameter α t = ∆ ρL 2 (T +1) for some ∆ ≥ ϕ 1/2ρ (x 0 ) -min ϕ, then the point x t * satisfies</p><formula xml:id="formula_40">(3.5) E ∇ϕ 1/2ρ (x t * ) 2 ≤ 2ρ∆L 2 T + 1 .</formula><p>Proof. Let x t denote the points generated by Algorithm 3.1. For each index t, define v t := E t [G(x t , ξ)] ∈ ∂f (x t ) and set xt := prox ϕ/ ρ(x t ). We successively deduce Next, observe that the function x → f (x) + ρ 2 x -x t 2 is strongly convex with parameter ρ -ρ, and therefore</p><formula xml:id="formula_41">E t ϕ 1/ ρ(x t+1 ) ≤ E t f (x t ) + ρ 2 xt -x t+1 2 (3.6) = f (x t ) + ρ 2 E t proj X (x t -α t G(x t , ξ t )) -proj X (x t ) 2 ≤ f (x t ) + ρ 2 E t (x t -xt ) -α t G(x t , ξ t ) 2 (3.7) ≤ f (x t ) + ρ 2 x t -xt 2 + ρα t E t [ xt -x t , G(x t , ξ t ) ] + α 2 t ρL 2 2 ≤ ϕ 1/ ρ(x t ) + ρα t xt -x t , v t + α 2 t ρL 2 2 ≤ ϕ 1/ ρ(x t ) + ρα t f (x t ) -f (x t ) + ρ 2 x t -xt 2 + α 2 t ρL 2 2 , (<label>3</label></formula><formula xml:id="formula_42">f (x t ) -f (x t ) - ρ 2 x t -xt 2 = f (x t ) + ρ 2 x t -x t 2 -f (x t ) + ρ 2 x t -xt 2 + ρ -ρ 2 x t -xt 2 ≥ (ρ -ρ) x t -xt 2 = ρ -ρ ρ2 ∇ϕ 1/ ρ(x t ) 2 ,</formula><p>where the last equality follows from Lemma 2.2. Thus we deduce</p><formula xml:id="formula_43">E t ϕ 1/ ρ(x t+1 ) ≤ ϕ 1/ ρ(x t ) - α t (ρ -ρ) ρ ∇ϕ 1/ ρ(x t ) 2 + α 2 t ρL 2 2 .</formula><p>Taking expectations of both sides with respect to ξ 0 , ξ 1 , . . . , ξ t-1 and using the law of total expectation yields the claimed inequality (3.3) Unfolding the recursion (3.3) yields</p><formula xml:id="formula_44">E ϕ 1/ ρ(x T +1 ) ≤ ϕ 1/ ρ(x 0 ) + ρL 2 2 T t=0 α 2 t - ρ -ρ ρ • T t=0 α t E ∇ϕ 1/ ρ(x t ) 2 .</formula><p>Lower-bounding the left-hand side by min ϕ and rearranging, we obtain the bound </p><formula xml:id="formula_45">1 T t=0 α t T t=0 α t E ∇ϕ 1/ ρ(x t ) 2 ≤ ρ ρ -ρ • ϕ 1/ ρ(x 0 ) -min ϕ + ρL 2</formula><formula xml:id="formula_46">α t = 2ρ∆L 2</formula><p>T +1 for all indices t = 0, 1, . . . , T . Let us translate the estimate (3.5) into a more convenient complexity bound. In particular, suppose that f is L-Lipschitz and the diameter of X is bounded by some D &gt; 0. Then we may set ∆ := min{ρD 2 , DL}, where the first term follows from the definition of the Moreau envelope and the second follows from Lipschitz continuity. Then the number of subgradient evaluations required to find a point x satisfying</p><formula xml:id="formula_47">E ∇ϕ 1/2ρ (x) ≤ ε is at most (3.9)     16 • (ρLD) 2 • min 1, L ρD ε 4     .</formula><p>Improved complexity under convexity. It is intriguing to ask if the complexity (3.9) can be improved when f is a convex function. The answer, unsurprisingly, is yes. Since f is convex, here and for the rest of the section, we will let the constant ρ &gt; 0 be arbitrary. As a first attempt, one may follow the observation of Nesterov <ref type="bibr" target="#b45">[45]</ref> for smooth minimization. The idea is that the right-hand side of the guarantee (3.4) depends on the initial gap ϕ(x 0 ) -min ϕ. We can make this quantity as small as we wish in expectation by a separate subgradient method. Namely, we may simply run a stochastic subgradient method for T iterations to decrease the expected gap ϕ(x 0 ) -min ϕ to ∆ := LD/ √ T + 1; see, for example, [36, Proposition 5.5] for this Downloaded 01/23/19 to 129.215.17.190. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php basic guarantee. Then we run another round of a stochastic subgradient method for T iterations using the optimal choice α := ∆ ρL 2 (T +1) . A quick computation shows that the resulting two-round scheme will find a point x satisfying E ∇ϕ 1/2ρ (x) ≤ ε</p><formula xml:id="formula_48">after at most O(1) • L 2 (ρD) 2/3 ε 8/3</formula><p>iterations. By following a completely different technique, introduced by Allen-Zhu <ref type="bibr" target="#b4">[5]</ref> for smooth stochastic minimization, this complexity can be even further improved to O(</p><formula xml:id="formula_49">(L 2 +ρ 2 D 2 ) log 3 ( ρD ε ) ε 2</formula><p>) by running logarithmically many rounds of the stochastic subgradient method on quadratically regularized problems. Since this procedure and its analysis is somewhat long and is independent of the rest of the material, we have placed it in an independent arXiv technical report <ref type="bibr" target="#b18">[19]</ref>.</p><p>3.2. Proximal stochastic subgradient method. We next move on to convergence guarantees of Algorithm 3.1 in full generality. An important consequence we discuss at the end of the section is a convergence guarantee for the stochastic proximal gradient method for minimizing a sum of a smooth function and a convex function, where the gradient oracle has bounded variance (instead of bounded second moment). Those not interested in this guarantee can in principle skip to section 4, which details our most general convergence result for nonsmooth minimization.</p><p>Before we proceed, note that for any x ∈ U and v ∈ ∂f (x), we have v ≤ L. To see this, observe that assumptions (A2) and (A3) directly imply that whenever f is differentiable at x ∈ U , we have</p><formula xml:id="formula_50">∇f (x) 2 = E ξ [G(x, ξ)] 2 ≤ E ξ [ G(x, ξ) 2 ] ≤ L 2 .</formula><p>Since at any point x, the subdifferential ∂f (x) is the convex hull of limits of gradients at nearby points [54, Theorem 25.6], the claim follows. We will use this estimate in the proof of Lemma 3.3. We break up the analysis of Algorithm 3.1 into two lemmas. Henceforth, fix a real ρ &gt; ρ. Let x t be the iterates produced by Algorithm 3.1 and let ξ t ∼ P be the i.i.d. realizations used. For each index t, define v t := E t [G(x t , ξ)] ∈ ∂f (x t ) and set xt := prox ϕ/ ρ(x t ). Observe that by the optimality conditions of the proximal map and the subdifferential sum rule [57, Exercise 10.10], there exists a vector vt ∈ ∂f (x t ) satisfying ρ(x t -xt ) ∈ ∂r(x t ) + vt . The following lemma realizes xt as a proximal point of r. Lemma 3.2. For each index t ≥ 0, equality holds:</p><formula xml:id="formula_51">xt = prox αtr (α t ρx t -α t vt + (1 -α t ρ)x t ) .</formula><p>Proof. By the definition of vt , we have</p><formula xml:id="formula_52">α t ρ(x t -xt ) ∈ α t ∂r(x t ) + α t vt ⇐⇒ α t ρx t -α t vt + (1 -α t ρ)x t ∈ xt + α t ∂r(x t ) ⇐⇒ xt = prox αtr (α t ρx t -α t vt + (1 -α t ρ)x t ),</formula><p>where the last equivalence follows from the optimality conditions for the proximal subproblem. This completes the proof.</p><p>The next lemma establishes a crucial descent property for the iterates. Lemma 3.3. Suppose ρ ∈ (ρ, 2ρ] and we have α t ∈ (0, 1/ρ] for all indices t ≥ 0. Then the following inequality holds: Proof. Set δ := 1 -α t ρ. We successively deduce</p><formula xml:id="formula_53">E t x t+1 -xt 2 ≤ x t -xt 2 + 4α</formula><formula xml:id="formula_54">E t x t+1 -xt 2 = E t prox αtr (x t -α t G(x t , ξ t )) -prox αtr (α t ρx t -α t vt + δ xt ) 2 ≤ E t x t -α t G(x t , ξ t ) -(α t ρx t -α t vt + δ xt ) 2 (3.10) = E t δ(x t -xt ) -α t (G(x t , ξ t ) -vt ) 2 (3.11) = δ 2 x t -xt 2 -2δα t E t [ x t -xt , G(x t , ξ t ) -vt ] + α 2 t E t G(x t , ξ t ) -vt 2 = δ 2 x t -xt 2 -2δα t x t -xt , v t -vt + 4α 2 t L 2 ≤ δ 2 x t -xt 2 + 2δα t ρ x t -xt 2 + 4α 2 t L 2 (3.12) = (1 -(2α t (ρ -ρ) + α 2 t ρ(2ρ -ρ))) x t -xt 2 + 4α 2 t L 2 ,</formula><p>where the first equation follows from Lemma 3. Theorem 3.4 (stochastic proximal subgradient method). Fix a real ρ ∈ (ρ, 2ρ] and a step-size sequence α t ∈ (0, 1/ρ]. Then the iterates x t generated by Algorithm 3.1 satisfy</p><formula xml:id="formula_55">(3.13) E ϕ 1/ ρ(x t+1 ) ≤ E[ϕ 1/ ρ(x t )] - α t (ρ -ρ) ρ E ∇ϕ 1/ ρ(x t ) 2 + α 2 t ρL 2 ,</formula><p>and the point x t * returned by Algorithm 3.1 satisfies</p><formula xml:id="formula_56">(3.14) E ∇ϕ 1/ ρ(x t * ) 2 ≤ ρ ρ -ρ • (ϕ 1/ ρ(x 0 ) -min ϕ) + 2ρL 2 T t=0 α 2 t T t=0 α t .</formula><p>In particular, if Algorithm 3.1 uses the parameter α t = 1 2 min{ 1 ρ , ∆ ρL 2 (T +1) } for some real ∆ ≥ ϕ 1/ ρ(x 0 ) -min ϕ, then the point x t * satisfies</p><formula xml:id="formula_57">(3.15) E ∇ϕ 1/2ρ (x t * ) 2 ≤ 8 • max ρ∆ T + 1 , L ρ∆ T + 1 .</formula><p>Proof. We successively observe</p><formula xml:id="formula_58">E t ϕ 1/ ρ(x t+1 ) ≤ E t ϕ(x t ) + ρ 2 xt -x t+1 2 ≤ ϕ(x t ) + ρ 2 x t -xt 2 + 4α 2 t L 2 -2α t (ρ -ρ) x t -xt 2 = ϕ 1/ ρ(x t ) + ρ 2α 2 t L 2 -α t (ρ -ρ) x t -xt 2 ,</formula><p>where the first inequality follows directly from the definition of the proximal map and the second follows from Lemma 3.3. Taking expectations with respect to ξ 0 , . . . , ξ t-1 yields the claimed inequality <ref type="bibr">(3.13)</ref>. The rest of the proof proceeds as in Theorem 3.1. Namely, unfolding the recursion (3.13) yields </p><formula xml:id="formula_59">E ϕ 1/ ρ(x T +1 ) ≤ ϕ 1/ ρ(x 0 ) + 2ρL 2 T t=0 α 2 t - ρ -ρ ρ E T t=0 α t x t -</formula><formula xml:id="formula_60">α t E ∇ϕ 1/ ρ(x t ) 2 ≤ ρ ρ -ρ • (ϕ 1/ ρ(x 0 ) -min ϕ) + 2ρL 2 T t=0 α 2 t T t=0 α t .</formula><p>Recognizing the left-hand side as E[ x t * -xt * 2 ] establishes <ref type="bibr">(3.14)</ref>.</p><p>To establish <ref type="bibr">(3.15)</ref>, set ρ := 2ρ and α t := min{ </p><formula xml:id="formula_61">E ∇ϕ 1/2ρ (x t * ) 2 ≤ 4ρ∆ T + 1 + 4L 2 ≤ 8ρ∆ T + 1 .</formula><p>Thus (3.15) is proved.</p><p>Proximal stochastic gradient for smooth minimization. We next look at the consequences of our results in the setting when f is C 1 -smooth with ρ-Lipschitz gradient. Note that then f is automatically ρ-weakly convex. In this smooth setting, it is common to replace assumption (A3) with the following finite variance condition.</p><p>(A3) There is a real σ ≥ 0 such that the inequality E ξ [ G(x, ξ) -∇f (x) 2 ] ≤ σ 2 holds for all x ∈ dom r. Henceforth, let us therefore assume that f is C 1 -smooth with ρ-Lipschitz gradient, and assumptions (A1), (A2), and (A3) hold.</p><p>All of the results in section 3.2 can be easily modified to apply to this setting. In particular, Lemma 3.2 holds verbatim, while Lemma 3.3 extends as follows.</p><p>Lemma 3.5. Fix a real ρ &gt; ρ and a sequence α t ∈ (0, 1/ρ]. Then the following inequality holds:</p><formula xml:id="formula_62">E t x t+1 -xt 2 ≤ x t -xt 2 + α 2 t σ 2 -α t (ρ -ρ) x t -xt 2 .</formula><p>Proof. By the same argument as in Lemma 3.3, we arrive at (3.11) with vt = ∇f (x t ). Set δ := 1 -α t ρ and w t := G(x t , ξ t ) -∇f (x t ). Adding and subtracting ∇f (x t ), we successively deduce We can now state the convergence guarantees of the proximal stochastic gradient method. The proof is completely analogous to that of Theorem 3.4, with Lemma 3.5 playing the role of Lemma 3.3.</p><formula xml:id="formula_63">E t x t+1 -xt 2 ≤ E t δ(x t -xt ) -α t (G(x t , ξ t ) -∇f (x t )) 2 = E t δ(x t -xt ) -α t (∇f (x t ) -∇f (x t )) -α t w t 2 = δ(x t -xt ) -α t (∇f (x t ) -∇f (x t )) 2 + α 2 t E t w t 2 (3.17) ≤ δ 2 x t -xt 2 -2δα t x t -xt , ∇f (x t ) -∇f (x t ) + α 2 t ∇f (x t ) -∇f (x t ) 2 + α 2 t σ 2 (3.18) ≤ (δ 2 + 2δα t ρ + ρ 2 α 2 t ) x t -xt 2 + α 2 t σ 2 (3.19) = x t -xt 2 + α 2 t σ 2 -α t (ρ -ρ)(2 -α t (ρ -ρ)) x t -</formula><p>Corollary 3.6 (stochastic prox-gradient method for smooth minimization). Fix a real ρ &gt; ρ and a step-size sequence α t ∈ (0, 1/ρ]. Then the iterates x t generated by Algorithm 3.1 satisfy</p><formula xml:id="formula_64">(3.20) E ϕ 1/ ρ(x t+1 ) ≤ E[ϕ 1/ ρ(x t )] - α t (ρ -ρ) 2ρ E ∇ϕ 1/ ρ(x t ) 2 + α 2 t ρσ 2 2 ,</formula><p>and the point x t * returned by Algorithm 3.1 satisfies</p><formula xml:id="formula_65">(3.21) E ∇ϕ 1/ ρ(x t * ) 2 ≤ 2ρ ρ -ρ • (ϕ 1/ ρ(x 0 ) -min ϕ) + ρσ 2 2 T t=0 α 2 t T t=0 α t .</formula><p>In particular, if Algorithm 3.1 uses the constant parameter</p><formula xml:id="formula_66">α t = min{ 1 2ρ , ∆ ρσ 2 (T +1) } for some ∆ ≥ ϕ 1/2ρ (x 0 ) -min ϕ, then the point x t * satisfies (3.22) E ∇ϕ 1/(2ρ) (x t * ) 2 ≤ 8 • max 2ρ∆ T + 1 , σ ρ∆ T + 1 .</formula><p>As mentioned at the end of section 2.2, it is immediate to translate the complexity estimate in Corollary 3.6 to an analogous estimate in terms of the size of the proxgradient mapping (2.9), thereby allowing for a direct comparison with previous results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Stochastic model-based minimization.</head><p>In the previous section, we established the complexity of O(ε -4 ) for the stochastic proximal subgradient methods. In this section, we show that the complexity O(ε -4 ) persists for a much wider class of algorithms, including the stochastic proximal point and prox-linear algorithms. Henceforth, we consider the optimization problem (4.1) min</p><formula xml:id="formula_67">x∈R d ϕ(x) := f (x) + r(x),</formula><p>where r : R d → R ∪ {∞} is a closed function (not necessarily convex) and f : R d → R is locally Lipschitz. We assume that the only access to f is through a stochastic one-sided model.</p><p>Assumption B (stochastic one-sided model). Fix a probability space (Ω, F, P ) and equip R d with the Borel σ-algebra. We assume that there exist real τ, η, L ∈ R such that the following four properties hold.</p><p>(B1) (Sampling.) It is possible to generate i.i.d. realizations ξ 1 , ξ 2 , . . . ∼ P . (B2) (One-sided accuracy.) There is an open convex set U containing dom r and a measurable function (x, y, ξ) → g x (y, ξ), defined on U × U × Ω, satisfying</p><formula xml:id="formula_68">E ξ [f x (x, ξ)] = f (x) ∀x ∈ U, and E ξ [f x (y, ξ) -f (y)] ≤ τ 2 y -x 2</formula><p>∀x, y ∈ U. Downloaded 01/23/19 to 129.215.17.190. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p><formula xml:id="formula_69">(B3) (Weak convexity.) The function f x (•, ξ) + r(•) is η-weakly convex ∀x ∈ U , a.e.</formula><p>ξ ∈ Ω. (B4) (Lipschitz property.) There exists a measurable function</p><formula xml:id="formula_70">L : Ω → R + satisfy- ing E ξ [L(ξ) 2 ] ≤ L and such that (4.2) f x (x, ξ) -f x (y, ξ) ≤ L(ξ) x -y</formula><p>for all x, y ∈ U and a.e. ξ ∼ P .</p><p>It will be useful for the reader to keep in mind the following lemma, which shows that the objective function ϕ is itself weakly convex with parameter τ + η and that f is L-Lipschitz continuous on U .</p><p>Lemma 4.1. The function ϕ is (τ + η)-weakly convex and the following inequality holds:</p><formula xml:id="formula_71">(4.3) |f (x) -f (y)| ≤ L x -y for all x, y ∈ U.</formula><p>Proof. Fix arbitrary points x, y ∈ dom r and a real λ ∈ [0, 1], and set x = λx + (1 -λ)y. Define the function f x (y) := E ξ [f x (y, ξ)]. Taking into account the equivalence of weak convexity with the approximate secant inequality (2.4), we successively deduce</p><formula xml:id="formula_72">ϕ(x) = E ξ [r(x) + f x(x, ξ)] (4.4) ≤ λE ξ [r(x) + f x(x, ξ)] + (1 -λ)E ξ [r(y) + f x(y, ξ)] + ηλ(1-λ) 2 x -y 2 (4.5) = λ(r(x) + f x(x)) + (1 -λ)(r(y) + f x(y)) + ηλ(1-λ) 2 x -y 2 ≤ λϕ(x) + (1 -λ)ϕ(y) + τ (λ 2 (1-λ)+λ(1-λ) 2 ) 2 x -y 2 + ηλ(1-λ) 2 x -y 2 (4.6) = λϕ(x) + (1 -λ)ϕ(y) + (τ +η)λ(1-λ) 2 x -y 2 ,</formula><p>where (4.4) uses (B2), inequality (4.5) uses (B3), and (4.6) uses (B2). Thus ϕ is (τ + η)-weakly convex, as claimed.</p><p>Next, taking expectations in (B2) and in (4.2) yields the estimates</p><formula xml:id="formula_73">f (x) -f x (y) ≤ L x -y and f x (y) -f (y) ≤ τ 2 x -y 2 .</formula><p>Thus for any point x ∈ U , we deduce</p><formula xml:id="formula_74">limsup y→x f (x) -f (y) x -y ≤ limsup y→x L x -y + τ 2 y -x 2 x -y = L.</formula><p>In particular, when f is differentiable at x, setting y = x -s∇f (x) with s 0, we deduce ∇f (x) ≤ L. Since f is locally Lipschitz continuous, its Lipschitz constant on U is no greater than sup y∈U { ∇f (y) : f is differentiable at y}. <ref type="foot" target="#foot_2">3</ref> We therefore deduce that f is L-Lipschitz continuous on U , as claimed.</p><p>We can now formalize the algorithm we investigate as Algorithm 4.1. The reader should note that, in contrast to the previously discussed algorithms, Algorithm 4.1 employs a nondecreasing step size β t , which is inversely proportional to α t . This notational choice will simplify the analysis and complexity guarantees that follow. Input: x 0 ∈ R d , real ρ &gt; τ + η, a sequence {β t } t≥0 ⊆ (ρ, ∞), and iteration count T .</p><p>Step t = 0, . . . , T :</p><formula xml:id="formula_75">     sample ξ t ∼ P, set x t+1 = argmin x r(x) + f xt (x, ξ t ) + β t 2 x -x t 2      .</formula><p>Sample t * ∈ {0, . . . , T } according to the discrete probability distribution</p><formula xml:id="formula_76">P(t * = t) ∝ ρ -τ -η β t -η . Return x t * .</formula><p>4.1. Analysis of the algorithm. Henceforth, let {x t } t≥0 be the iterates generated by Algorithm 4.1 and let {ξ t } t≥0 be the corresponding samples used. For each index t ≥ 0, define the proximal point xt = prox ϕ/ ρ(x t ).</p><p>As in section 3, we will use the symbol E t [•] to denote the expectation conditioned on all the realizations ξ 0 , ξ 1 , . . . , ξ t-1 . The analysis of Algorithm 4.1 relies on the following lemma, which establishes two descent-type properties. Estimate (4.7) is in the same spirit as Lemma 3.3. The estimate (4.8), in contrast, will be used at the end of the section to obtain the convergence rate of Algorithm 4.1 in function values under convexity assumptions. Lemma 4.2. For every index t ≥ 0, we have</p><formula xml:id="formula_77">(4.7) E t xt -x t+1 2 ≤ xt -x t 2 - ρ -τ -η β t -η xt -x t 2 + 4L 2 (β t -η)(β t -ρ)</formula><p>.</p><p>Moreover, for any point x ∈ dom r, the following inequality holds:</p><formula xml:id="formula_78">(4.8) E t x t+1 -x 2 ≤ β t + τ β t -η x t -x 2 - 2 β t -η E t [ϕ(x t+1 ) -ϕ(x)] + 2L 2 β t (β t -η) .</formula><p>Proof. Recall that the function</p><formula xml:id="formula_79">x → r(x) + f xt (x, ξ t ) + β t 2 x -x t 2</formula><p>is strongly convex with constant β t -η and x t+1 is its minimizer. Hence for any x ∈ dom r, the following inequality holds:</p><formula xml:id="formula_80">r(x) + f xt (x, ξ t ) + βt 2 x -x t 2 ≥ r(x t+1 )+f xt (x t+1 , ξ t ) + βt 2 x t+1 -x t 2 + βt-η<label>2</label></formula><p>x -x t+1 2 . Downloaded 01/23/19 to 129.215.17.190. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php Rearranging and taking expectations, we successively deduce</p><formula xml:id="formula_81">E t β t -η 2 x -x t+1 2 + β t 2 x t+1 -x t 2 - β t 2 x -x t 2 ≤ E t [r(x) + f xt (x, ξ t ) -r(x t+1 ) -f xt (x t+1 , ξ t )] ≤ E t [r(x) + f xt (x, ξ t ) -r(x t+1 ) -f xt (x t , ξ t ) + L(ξ) x t+1 -x t ] (4.9) ≤ r(x) + E ξ [f xt (x, ξ)] -E t [r(x t+1 )] -E ξ [f xt (x t , ξ)] (4.10) + E ξ [L(ξ) 2 ] • E t [ x t+1 -x t 2 ] ≤ r(x) + f (x) -E t [r(x t+1 )] -f (x t ) + τ 2 x -x t 2 + L E t [ x t+1 -x t 2 ] (4.11) = E t [r(x) + f (x) -r(x t+1 ) -f (x t )] + τ 2 x -x t 2 + L E t [ x t+1 -x t 2 ] ≤ E t [r(x) + f (x) -r(x t+1 ) -f (x t+1 )] + τ 2 x -x t 2<label>(4.12)</label></formula><formula xml:id="formula_82">+ LE t [ x t+1 -x t ] + L E t [ x t+1 -x t 2 ],</formula><p>where (4.9) follows from assumption (B4), inequality (4.10) follows from Cauchy-Schwartz, inequality (4.11) follows from assumption (B2), and (4.12) follows from Lemma 4.1. Define δ := E t [ x t+1 -x t 2 ] and notice that δ ≥ E t x t -x t+1 . Rearranging (4.12), we immediately deduce</p><formula xml:id="formula_83">E t β t -η 2 x -x t+1 2 ≤ E t β t + τ 2 x * -x t 2 - β t δ 2 2 + 2Lδ -E t [ϕ(x t+1 ) -ϕ(x)] ≤ E t β t + τ 2 x -x t 2 + 2L 2 β t -E t [ϕ(x t+1 ) -ϕ(x)],</formula><p>where the last inequality follows by maximizing the right-hand side in δ ∈ R. Dividing through by β-η 2 , we arrive at the claimed inequality (4.8). Next, setting x = xt in (4.12) and using the definition of the prox-point, we obtain</p><formula xml:id="formula_84">E t β t -η 2 xt -x t+1 2 + β t 2 x t+1 -x t 2 - β t 2 xt -x t 2 ≤ E t - ρ 2 xt -x t 2 + ρ 2 x t+1 -x t 2 + τ 2 xt -x t 2 + 2Lδ = τ - ρ 2 xt -x t 2 + ρ 2 • E t [ x t+1 -x t 2 ] + 2Lδ.</formula><p>Rearranging, we deduce</p><formula xml:id="formula_85">β t -η 2 • E t xt -x t+1 2 ≤ β t -ρ + τ 2 xt -x t 2 + ρ -β t 2 δ 2 + 2Lδ (4.13) ≤ β t -ρ + τ 2 xt -x t 2 + 2L 2 β t - ρ ,</formula><p>where the last inequality follows by maximizing the right-hand side of (4.13) in δ ∈ R.</p><p>After multiplying through by </p><formula xml:id="formula_86">E ϕ 1/ ρ(x t+1 ) ≤ E[ϕ 1/ ρ(x t )] - ρ -τ -η 2ρ(β t -η) E ∇ϕ 1/ ρ(x t ) 2 + 2ρL 2 (β t -η)(β t -ρ) ,</formula><p>and the point x t * returned by Algorithm 3.1 satisfies</p><formula xml:id="formula_87">(4.15) E ∇ϕ 1/ ρ(x t * ) 2 ≤ ρ(ϕ 1/ ρ(x 0 ) -min x ϕ) + 2ρ 2 L 2 • T t=0 1 (βt-η)(βt-ρ) T t=0 ρ-τ -η 2(βt-η) .</formula><p>In particular, if Algorithm 3.1 uses the constant parameter β t = ρ + 2 ρL 2 (T +1) ∆ for some real ∆ ≥ ϕ 1/ ρ(x 0 ) -min ϕ and sets ρ = 2(ρ + η), then the point x t * satisfies (4. <ref type="bibr" target="#b15">16</ref>)</p><formula xml:id="formula_88">E ∇ϕ 1/ ρ(x t * ) 2 ≤ 4ρ∆ T + 1 + 8L 2ρ∆ T + 1 .</formula><p>Proof. Using the definition of the Moreau envelope and appealing to the estimate (4.7) in Lemma 4.2, we deduce</p><formula xml:id="formula_89">E t [ϕ 1/ ρ(x t+1 )] ≤ E t ϕ(x t ) + ρ 2 x t+1 -xt 2 ≤ ϕ(x t ) + ρ 2 • E t x -xt 2 , ≤ ϕ(x t ) + ρ 2 xt -x t 2 - ρ -τ -η β t -η xt -x t 2 + 4L 2 (β t -η)(β t -ρ) = ϕ 1/ ρ(x t ) - ρ -τ -η 2ρ(β t -η) ∇ϕ 1/ ρ(x t ) 2 + 2ρL 2 (β t -η)(β t -ρ)</formula><p>.</p><p>Taking expectations with respect to ξ 0 , . . . , ξ t-1 and using the tower rule yields the claimed inequality <ref type="bibr">(4.14)</ref>. Unfolding the recursion (4.14) yields</p><formula xml:id="formula_90">E[ϕ 1/ ρ(x t+1 )] ≤ ϕ 1/ ρ(x 0 ) - T t=0 ρ -τ -η 2ρ(β t -η) E[ ∇ϕ 1/ ρ(x t ) 2 ] + 2ρL 2 • T t=0 1 (βt-η)(βt-ρ) .</formula><p>Using the inequality ϕ 1/ ρ(x t+1 ) ≥ min ϕ and rearranging yields</p><formula xml:id="formula_91">T t=0 ρ -τ -η β t -η E[ ϕ 1/ ρ(x t ) 2 ] ≤ 2ρ(ϕ 1/ ρ(x 0 ) -min ϕ) + 4L 2 ρ2 T t=0 1 (β t -η)(β t -ρ)</formula><p>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dividing through by T t=0</head><p>ρ-τ -η βt-η and recognizing the left-hand side as E[ ϕ 1/ ρ(x t * ) 2 ] yields <ref type="bibr">(4.15)</ref>. Setting ρ = 2(ρ+η) and β t = ρ+ 2ρL 2 (T + 1)/∆ in (4.15) immediately yields the final guarantee (4.16).</p><p>Next we consider the "convex setting," which is when the models E ξ f (•, ξ) globally lower bound f , without quadratic error, and the functions f x (•, ξ)+r(•) are µ-strongly convex. By analogy with the stochastic subgradient method, one would expect that Algorithm 4.1 drives the function gap E[ϕ(x t )-ϕ(•)] to zero at the rates O(1/ √ t) and O(1/µt) in the settings µ = 0 and µ &gt; 0, respectively. The following two theorems establish exactly that. Even when specializing to the stochastic proximal subgradient Downloaded 01/23/19 to 129.215.17.190. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php method, Theorems 4.4 and 4.5 improve on the state of the art. In contrast to previous work <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b31">31]</ref>, the norms of the subgradients of r do not enter the complexity bounds established in Theorem 4.4, while Theorem 4.5 extends the nonuniform averaging technique of <ref type="bibr" target="#b61">[61]</ref> for strongly convex minimization to the fully proximal setting.</p><p>Theorem 4.4 (convergence rate under convexity). Suppose that τ = 0 and the functions f x (•, ξ) + r(•) are convex. Let {x t } be the iterates generated by Algorithm 4.1 and set α t = β -1 t . Then for all T &gt; 0, we have</p><formula xml:id="formula_92">(4.17) E ϕ 1 T t=0 αt T t=0 α t x t+1 -ϕ(x * ) ≤ 1 2 x 0 -x * 2 + L 2 T t=0 α 2 t T t=0 α t</formula><p>, where x * is any minimizer of ϕ. In particular, if Algorithm 3.1 uses the constant parameter</p><formula xml:id="formula_93">α t = D L √ 2(T +1)</formula><p>, for some real D &gt; x 0 -x * , then the following estimate holds:</p><formula xml:id="formula_94">(4.18) E ϕ 1 T +1 T +1 t=1 x t -ϕ(x * ) ≤ √ 2LD √ T + 1 .</formula><p>Proof. Setting η := 0 and x := x * in the estimate (4.8) in Lemma 4.2, and taking expectations of both sides yields</p><formula xml:id="formula_95">2α t E[ϕ(x t+1 ) -ϕ(x * )] ≤ E x t -x * 2 -E x t+1 -x * 2 + 2L 2 α 2 t .</formula><p>The estimate (4.17) then follows by summing across t = 0, . . . , T , dividing through by T t=0 α t , and using convexity of ϕ. The estimate (4.18) is immediate from (4.17).</p><p>The following theorem uses the nonuniform averaging technique from <ref type="bibr" target="#b61">[61]</ref>.</p><p>Theorem 4.5 (convergence rate under strong convexity). Suppose that τ = 0 and the functions f x (•, ξ) + r(•) are µ-strongly convex for some µ &gt; 0. Then for all T &gt; 0, the iterates generated by Algorithm 4.1 with β t = µ(t+1)</p><formula xml:id="formula_96">2 satisfy E ϕ 2 (T + 2)(T + 3) -2 T +1 t=1 (t + 1)x t -ϕ(x * ) ≤ µ x 0 -x * 2 (T + 2) 2 + 8L 2 µ(T + 2)</formula><p>, where x * is any minimizer of ϕ.</p><p>Proof.</p><formula xml:id="formula_97">Define ∆ t := 1 2 E[ x -x t 2 ]</formula><p>. Setting η := -µ and x := x * in the estimate (4.8) of Lemma 4.2, taking expectations of both sides, and multiplying through by</p><formula xml:id="formula_98">(β t + µ)/2 yields E[ϕ(x t+1 ) -ϕ(x * )] ≤ β t ∆ t -(β t + µ)∆ t+1 + L 2 β t .</formula><p>Plugging in β t := µ(t+1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head><p>, multiplying through by t + 2, and summing, we get Dividing through by the sum T t=0 (t + 2) = (T +2)(T +3) 2 -1 and using convexity of ϕ, we deduce</p><formula xml:id="formula_99">T t=0 (t + 2)E[ϕ(x t+1 ) -ϕ(x * )] ≤ T t=0 µ(t+1)(t+2) 2 ∆ t -µ(t+2)(t+3) 2 ∆ t+1 + T t=0 2L 2 (t+2) µ(t+1) ≤ µ∆ 0 + 4L 2 (T +</formula><formula xml:id="formula_100">E ϕ 2 (T + 2)(T + 3) -2 T t=0 (t + 2)x t+1 -ϕ(x * ) ≤ µ x 0 -x * 2 (T + 2)(T + 3) -2 + 8L 2 (T + 1) µ((T + 2)(T + 3) -2) ≤ µ x 0 -x * 2 (T + 2) 2 + 8L 2 µ(T + 2) , (4.19)</formula><p>where <ref type="bibr">(4.19)</ref> uses the estimate (T + 2)(T + 3) -2 ≥ (T + 2) 2 . The proof is complete.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Algorithmic examples.</head><p>We next look at the consequences of Theorems 4.3 and 4.4. We begin with the algorithms briefly mentioned in the introduction: stochastic proximal point, prox-linear, and proximal subgradient. In each case, we list the standard assumptions under which the methods are applicable, and then verify properties (B1)-(B4) for some τ, η, L ≥ 0. Complexity guarantees for each method then follow immediately from Theorem 4.3. We then describe the problem of minimizing the expectation of a convex monotone composition (e.g., conditional value-at-risk), and describe a natural model-based algorithm for the problem. (E3) The function r is ρ-weakly convex, and there exist square integrable functions , γ, M : Ω → R such that for a.e. ξ ∈ Ω, the function z → h(z, ξ) is convex and (ξ)-Lipschitz, the map x → c(x, ξ) is C 1 -smooth with γ(ξ)-Lipschitz Jacobian, and the inequality ∇c(x, ξ) op ≤ M (ξ) holds for all x ∈ U and a.e. ξ ∈ Ω. The stochastic prox-linear method <ref type="bibr" target="#b30">[30]</ref> is Algorithm 4.1 with the convex models</p><formula xml:id="formula_101">f x (y, ξ) = h c(x, ξ) + ∇c(x, ξ)(y -x), ξ .</formula><p>Observe that (B1) and (B3) hold trivially with η = ρ. Assumption (B2) holds with τ</p><formula xml:id="formula_102">= E ξ [ (ξ)] 2 E ξ [γ(ξ) 2 ] by [30, Lemma 3.12]. Assumption (E3) also directly implies (B4) with L = E ξ [ (ξ)] 2 E ξ [M (ξ) 2 ].</formula><p>Expectation of convex monotone compositions. As an application of Theorem 4.4, suppose we wish to optimize the problem (4.1), where r is convex and f is given by f</p><formula xml:id="formula_103">(x) = E ξ [h(c(x, ξ), ξ)].</formula><p>Suppose that h( </p><formula xml:id="formula_104">G : U × Ω → R d satisfying G(x, ξ) ∈ ∂ x c(x, ξ)</formula><p>for all x ∈ U . (F4) There exist square integrable functions , M : Ω → R such that for a.e. ξ ∈ Ω, the function z → h(z, ξ) is (ξ)-Lipschitz and the map x → c(x, ξ) is M (ξ)-Lipschitz for a.e. ξ ∈ Ω. One reasonable class of models then reads</p><formula xml:id="formula_105">f x (y, ξ) = h(c(x, ξ) + G(x, ξ), y -x , ξ).</formula><p>Assumption (B1) is immediate from (F1). Assumption (F2) directly implies (B2) with τ = 0 and (B3) with η = 0. Finally (F4) readily implies (B2) with </p><formula xml:id="formula_106">L = E ξ [ (ξ)] 2 E ξ [M (ξ) 2 ].</formula><formula xml:id="formula_107">) = argmin γ∈R,y∈R d (1 -α)γ+ [g(x t , ξ t ) + v t , y -x t -γ] + + r(y) + β t 2 ( y -x t 2 + γ -γ t 2 ).</formula><p>5. Numerical illustrations. In this section, we illustrate our three running examples (stochastic subgradient, prox-linear, prox-point) on phase retrieval and blind deconvolution problems, outlined in section 2.1. In particular, our experiments complement the recent paper <ref type="bibr" target="#b30">[30]</ref>, which performs an extensive numerical study of the stochastic subgradient and prox-linear algorithms on the phase retrieval problem.</p><p>Our main goal in this section is to illustrate that the update rules for all three algorithms have essentially the same computational cost. Indeed, the subproblems for the stochastic prox-point and prox-linear algorithms have a closed form solution. Note that our theoretical guarantees (Theorem 4.3) imply essentially the same worst-case complexity for the stochastic subgradient, prox-linear, and proximal point algorithms. In contrast, our numerical results on both problems clearly show that the latter two algorithms are much better empirically both in terms of speed and robustness to the choice of step size. Intuitively, the reason appears to be that the models used by the latter two algorithms provide much tighter approximation. Indeed, the models are two-sided in the sense that the two-sided error |E ξ∼P [f x (y, ξ)]-f (y)| is upper-bounded by a multiple of the quadratic y -x 2 . 5.1. Phase retrieval. The experimental setup for the phase retrieval problem is as follows. We generate standard Gaussian measurements a i ∼ N (0, I d×d ) for i = 1, . . . , m; generate the target signal x and initial point x 0 uniformly on the unit sphere; and set b i = a i , x 2 for each i = 1, . . . , m. We then apply the three stochastic algorithms to the problem Then the next iterate is defined to be x + ∆. Setting γ = λ( a, x 2 -b) and ζ = 2λ a, x a, we therefore seek to solve the problem <ref type="bibr">(5.1)</ref> argmin</p><formula xml:id="formula_108">∆∈R d |γ + ζ, ∆ | + 1 2 ∆ 2 .</formula><p>An explicit solution ∆ * to this subproblem follows from a standard Lagrangian calculation, and is recorded for example in [30, section 4]:</p><p>(5. Let us compute the candidate solutions using first-order optimality conditions:</p><p>(5.4) λ -1 (x -y) ∈ 2 a, y a • sign( a, y 2 -b) if a, y 2 = b, [-1, 1] otherwise .</p><p>An easy computation shows that there are at most four points y that satisfy (5.4):</p><p>x -2λ a, x 2λ a 2 ± 1 a, x -a, x ± √ b a 2  a .</p><p>Therefore we may set the next iterate to be the candidate solution y with the lowest function value for the subproblem (5.3). We perform three sets of experiments corresponding to (d, m) = (10, 30), (50, 150), (100, 300), and record the result in Figure <ref type="figure">3</ref> (see online version for color figures). The dashed blue line indicates the initial functional error. In each set of experiments, we use 100 equally spaced step-size parameters β -1 t between 10 -4 and 1. The figures on the left record the function gap after 100m iterations, averaged over 15 rounds. The figures on the right output the number of iterations divided by m used by the stochastic prox-linear and proximal point methods to find a point achieving 10 -4 functional suboptimality, averaged over 15 rounds. It is clear from the figures that the stochastic prox-linear and proximal point algorithms perform much better and are more robust to the choice of the step-size parameter than the stochastic subgradient method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Blind deconvolution.</head><p>We next consider a problem inspired by blind deconvolution and biconvex compressive sensing <ref type="bibr" target="#b40">[40]</ref>. The experimental setup is as follows. We generate Gaussian measurements u i ∼ N (0, I d1×d1 ) and v i ∼ N (0, I d2×d2 ) for i = 1, . . . , m; generate the target signal x uniformly on the unit sphere; and set b i = u i , x v i , x for each i = 1, . . . , m. The problem formulation reads , and then obtain an explicit formula for (x, y) using (5.8).</p><p>Our numerical experiments are similar to those for phase retrieval. We perform three </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>) = f (x) + r(x), where f (x) = E ξ∼P [f (x, ξ)]. (SO)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>λ</head><label></label><figDesc>∇ϕ λ (x) ∇ϕ λ (x) ∈ ∂ϕ(x) (a) Moreau envelope of ϕ(x) = |x 2 -1|.(b) Approximate stationarity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. An illustration of the Moreau envelope.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>2. 1 .</head><label>1</label><figDesc>Examples of weakly convex functions. Weakly convex functions are widespread in applications and are typically easy to recognize. One common source is the composite function class (2.1) ϕ(x) := h(c(x)), where h : R m → R is convex and L-Lipschitz and c : R d → R m is a C 1 -smooth map with β-Lipschitz continuous Jacobian. An easy argument shows that the composite function ϕ is Lβ-weakly convex [28, Lemma 4.2]. Below, we list a few examples to illustrate how widespread this problem class is in large-scale data scientific applications. The examples are here only to set the context; the reader can safely skip this discussion during the initial reading.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(A1) It is possible to generate i.i.d. realizations ξ 1 , ξ 2 , . . . ∼ P . (A2) There is an open set U containing dom r and a measurable mapping G : U × Ω → R d satisfying E ξ [G(x, ξ)] ∈ ∂f (x) for all x ∈ U . (A3) There is a real L ≥ 0 such that the inequality E ξ [ G(x, ξ) 2 ] ≤ L 2 holds for all x ∈ dom r.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>.</head><label></label><figDesc>Notice that the left-hand side is precisely the expectation E[ ∇ϕ 1/ ρ(x t * ) 2 ]. Thus (3.4) holds, as claimed. Finally, (3.5) follows from (3.4) by setting ρ = 2ρ and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>DAMEK DAVIS AND DMITRIY DRUSVYATSKIY Algorithm 4 . 1</head><label>41</label><figDesc>Stochastic model-based minimization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Convergence guarantees in function values then follow from Theorem 4.4. Stochastic proximal point. Consider the optimization problem (4.1) under the following assumptions. (C1) It is possible to generate i.i.d. realizations ξ 1 , ξ 2 , . . . ∼ P . (C2) There is an open convex set U containing dom r and a measurable function (x, y, ξ) → f x (y, ξ) defined on U × U × Ω satisfying E ξ [f x (y, ξ)] = f (y) for all x, y ∈ U . (C3) Each function r(•) + f x (•, ξ) is ρ-weakly convex ∀x ∈ U , a.e. ξ ∈ Ω. (C4) There exists a measurable function L: Ω → R + satisfying E ξ [L(ξ) 2 ] ≤ L and such that f x (x, ξ) -f x (y, ξ) ≤ L(ξ) x -yfor all x, y ∈ U and a.e. ξ ∈ Ω. The stochastic proximal point method is Algorithm 4.1 with the models f x (y, ξ). It is immediate to see that (B1)-(B4) hold with τ = 0 and η = ρ.Stochastic proximal subgradient. We next slightly loosen assumptions (A1)-(A3) for the proximal stochastic subgradient method, by allowing r to be nonconvex, and show how these assumptions imply (B1)-(B4). Consider the optimization problem (4.1), and let us assume that the following properties are true.(D1) It is possible to generate i.i.d. realizations ξ 1 , ξ 2 , . . . ∼ P . (D2) The function f is ρ 1 -weakly convex and r is ρ 2 -weakly convex for some ρ 1 , ρ 2 ≥ 0. (D3) There is an open convex set U containing dom r and a measurable mappingG : U × Ω → R d satisfying E ξ [G(x, ξ)] ∈ ∂f (x) for all x ∈ U . (D4) There is a real L ≥ 0 such that the inequality E ξ [ G(x, ξ) 2 ] ≤ L 2holds for all x ∈ U . The stochastic subgradient method is Algorithm 4.1 with the linear models Observe that (B1) and (B3) with η = ρ 2 are immediate from the definitions; (B2) with τ = ρ 1 follows from the discussion in [21, section 2]. Assumption (B4) is also immediate from (D4). Stochastic prox-linear. Consider the optimization problem (4.1) with f (x) = E ξ∼P h c(x, ξ), ξ . We assume that there exists an open convex set U containing dom r such that the following properties are true. (E1) It is possible to generate i.i.d. realizations ξ 1 , ξ 2 , . . . ∼ P . (E2) The assignments h : R m × Ω → R and c : U × Ω → R m are measurable.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>| 2 .</head><label>2</label><figDesc>a i , x 2 -b i |.Each step of the algorithms is trivial to implement. Since the three methods only use one data point at a time, let us define the functiong(x) = | a, x 2 -b| for a fixed vector a ∈ R d and a real b ≥ 0.Stochastic subgradient. The stochastic subgradient method simply needs to evaluate an element of the subdifferential∂g(x) = 2 a, x a • sign( a, x 2 -b) if a, x 2 = b, [-1, 1] otherwise .Stochastic prox-linear. The stochastic prox-linear method needs to solve subproblems of the form argmin ∆∈R d | a, x 2 + 2 a, x a, ∆ -b| + 1 2λ ∆ Downloaded 01/23/19 to 129.215.17.190. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Stochastic proximal point. Finally, the stochastic proximal point method requires solving the problem(5.3) argmin y | a, y 2 -b| + 1 2λ y -x 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>|Fig. 3 .Fig. 4 .</head><label>34</label><figDesc>Fig. 3. Bottom to top: (d, m) = (10, 30), (50, 150), (100, 300). The dashed blue line indicates the initial functional error.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>sets of experiments corresponding to (d 1 , d 2 , m) = (10, 10, 30), (50, 50, 200), (100, 100, 400), and record the result in Figure 4 (see online version for color figures). The dashed blue line indicates the initial functional error. In each set of experiments, we use 100 equally spaced step-size parameters β -1 t between 10 -4 and 1. The figures on the left record the function gap after 100m iterations, averaged over 10 rounds. The figures on the right output the number of iterations divided by m used by the stochastic prox-linear and proximal point methods to find a point achieving 10 -4 functional suboptimality, averaged over 10 rounds. As in phase retrieval, it is clear from the figure that the stochastic prox-linear and proximal point algorithms perform much better and are more robust to the choice of the step-size parameter than the stochastic subgradient method. Downloaded 01/23/19 to 129.215.17.190. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>1 -smooth with the gradient Downloaded 01/23/19 to 129.215.17.190. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</figDesc><table><row><cell>DAMEK DAVIS AND DMITRIY DRUSVYATSKIY</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Downloaded 01/23/19 to 129.215.17.190. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</figDesc><table><row><cell>DAMEK DAVIS AND DMITRIY DRUSVYATSKIY</cell></row></table><note><p><p>T </p>-M .</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>), Downloaded 01/23/19 to 129.215.17.190. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php where f : R d → R is C 1 -smooth with ρ-Lipschitz gradient and r : R d → R ∪ {∞} is closed and convex. Much of the literature</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>.8)    where(3.6)  follows directly from the definition of the proximal map, (3.7) uses that the projection proj X (•) is 1-Lipschitz, and (3.8) follows from(2.5). Downloaded 01/23/19 to 129.215.17.190. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php DAMEK DAVIS AND DMITRIY DRUSVYATSKIY</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Downloaded 01/23/19 to 129.215.17.190. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</figDesc><table><row><cell>DAMEK DAVIS AND DMITRIY DRUSVYATSKIY</cell></row></table><note><p>2 t L 2 -2α t (ρ -ρ) x t -xt 2 .</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>xt 2 . Downloaded 01/23/19 to 129.215.17.190. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php Lower-bounding the left-hand side by min ϕ and rearranging, we obtain the bound (3.16)</figDesc><table><row><cell>1</cell><cell>T</cell></row><row><cell>T t=0 α t</cell><cell>t=0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>xt 2 , Downloaded 01/23/19 to 129.215.17.190. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php where (3.17) follows from assumption (A2), namely E t G(x t , ξ t ) = ∇f (x t ), (3.18) follows by expanding the square and using assumption (A3), and (3.19) follows from (2.6) and Lipschitz continuity of ∇f . The assumption ρ ≥ ρ guarantees 2-α t (ρ-ρ) ≥ 1. The result follows.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>2 βt-η , we arrive at the claimed estimate (4.7). We can now establish the convergence guarantees of Algorithm 4.1. Downloaded 01/23/19 to 129.215.17.190. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php Theorem 4.3 (convergence rate). Fix a real ρ &gt; τ + η and a sequence {β t } t≥0 ∈ (ρ, ∞). Then the iterates x t generated by Algorithm 4.1 satisfy (4.14)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>1) µ . Downloaded 01/23/19 to 129.215.17.190. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</figDesc><table><row><cell>DAMEK DAVIS AND DMITRIY DRUSVYATSKIY</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>Downloaded 01/23/19 to 129.215.17.190. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php under the assumption that the loss g(•, ξ) is convex. Then given an iterate (x t , γ t ), the stochastic model-based algorithm would sample ξ t ∼ P , choose a subgradient v t ∈ ∂ x g(x t , ξ t ), and perform the simple update (x t+1 , γ t+1</figDesc><table><row><cell>Thus the stochastic model-based algorithm (Algorithm 4.1)</cell></row><row><cell>enjoys the O( 1 √ t ) convergence guarantee in expected function value gap (Theorem 4.4).</cell></row><row><cell>As an illustration, consider the conditional value-at-risk problem, discussed in</cell></row><row><cell>Example 2.6,</cell></row><row><cell>min</cell></row><row><cell>γ∈R,x∈R</cell></row></table><note><p>d (1 -α)γ + E ξ∼P [(g(x, ξ) -γ) + ] + r(x),</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>To the best of our knowledge, the class of weakly convex functions was introduced in[49]. Downloaded 01/23/19 to 129.215.17.190. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>We refer the reader to<ref type="bibr" target="#b56">[56,</ref> Page 44]  and<ref type="bibr" target="#b7">[8]</ref> for a historical account of the cVaR minimization formula, and in particular its interpretation as the "optimized certainty equivalent" introduced in<ref type="bibr" target="#b6">[7]</ref>.Downloaded 01/23/19 to 129.215.17.190. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>This follows by combining the gradient formula for the Clarke subdifferential [17, Theorem 8.1] with the mean value theorem [17, Theorem 2.4]. Downloaded 01/23/19 to 129.215.17.190. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_3"><p>f x (y, ξ) = f (x) + G(x, ξ), y -x . Downloaded 01/23/19 to 129.215.17.190. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_4"><p>Downloaded 01/23/19 to 129.215.17.190. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment. The authors thank John Duchi for his careful reading and helpful feedback on the initial version of this manuscript.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The research of Drusvyatskiy was supported by the AFOSR YIP award FA9550-15-1-0237 and by the NSF grants DMS 1651851 and CCF 1740551 awards.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Stochastic proximal point. Finally, the stochastic proximal point method requires solving the problem <ref type="bibr">(5.5)</ref> argmin</p><p>x,y</p><p>| u, x v, y -b| + 1 2λ</p><p>x -x 0 2 + 1 2λ y -y 0 2 .</p><p>Let us enumerate the critical points. Writing out the optimality conditions for (x, y), there are two cases to consider. In the first case u, x v, y = b, it is straightforward to show that the possible critical points have the form (5.6)</p><p>Indeed, suppose for the moment u, x v, y &gt; b. Then optimality conditions for (5.5) imply</p><p>(5.7)</p><p>Thus if we determine v, y and u, x , we will have an explicit formula for (x, y).</p><p>Taking the dot product of the first equation with u and the second with v yields</p><p>Solving for v, y and u, x , we get</p><p>Combining these expressions with (5.7), we deduce that x and y can be expressed as in <ref type="bibr">(5.6)</ref>. The setting u, x v, y &gt; b is completely analogous. In the second case, suppose u, x v, y = b. Then the optimality condition for (5.5) implies that there exists γ such that</p><p>We must solve this system of equations for γ, η := u, x , and δ := v, y . Substituting the third equation into the first yields</p><p>Taking the dot product of the first equation with u and the second with v yields</p><p>Solving the first equation for γ, we get the expression γ = η u,x0 -η 2 b u 2</p><p>. Plugging this formula into the second equation and clearing the denominator, we arrive at the quartic polynomial 0 = η 4 v 2 -η 3 v 2 u, x 0 + bη u 2 v, y 0 -b 2 u 2 . Downloaded 01/23/19 to 129.215.17.190. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<ptr target="https://www.tensorflow.org/about/bib" />
	</analytic>
	<monogr>
		<title level="m">TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems, White paper</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Decoding binary node labels from censored edge measurements: Phase transition and efficient recovery</title>
		<author>
			<persName><forename type="first">E</forename><surname>Abbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bandeira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bracher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singer</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNSE.2014.2368716</idno>
		<ptr target="https://doi.org/10.1109/TNSE.2014.2368716" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Network Sci. Eng</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="10" to="22" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The first direct acceleration of stochastic gradient methods</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katyusha</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">STOC 2017: Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1200" to="1205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1708.08694" />
		<title level="m">Natasha 2: Faster Non-Convex Optimization than SGD</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">How to Make the Gradients Small Stochastically</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1801" />
		<imprint>
			<date type="published" when="2018">02982v1, 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On the low-rank approach for semidefinite programs arising in synchronization and community detection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bandeira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Boumal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Voroninski</surname></persName>
		</author>
		<ptr target="http://jmlr.org/proceedings/papers/v49" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Conference on Learning Theory</title>
		<meeting>the 29th Conference on Learning Theory</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="361" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Expected utility, penalty functions, and duality in stochastic nonlinear programming</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ben-Tal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Teboulle</surname></persName>
		</author>
		<idno type="DOI">10.1287/mnsc.32.11.1445</idno>
		<ptr target="https://doi.org/10.1287/mnsc.32.11.1445" />
	</analytic>
	<monogr>
		<title level="j">Management Sci</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1445" to="1466" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An old-new concept of convex risk measures: The optimized certainty equivalent</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ben-Tal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Teboulle</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1467-9965.2007.00311.x</idno>
		<ptr target="https://doi.org/10.1111/j.1467-9965.2007.00311.x" />
	</analytic>
	<monogr>
		<title level="j">Math. Finance</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="449" to="476" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ergodic convergence of a stochastic proximal point algorithm</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bianchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Optim</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="2235" to="2260" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Descent methods for composite nondifferentiable optimization problems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Burke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Program</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="260" to="279" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Robust principal component analysis?</title>
		<author>
			<persName><forename type="first">E</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Phase retrieval via Wirtinger flow: theory and algorithms</title>
		<author>
			<persName><forename type="first">E</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Soltanolkotabi</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIT.2015.2399924</idno>
		<ptr target="https://doi.org/10.1109/TIT.2015.2399924" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="page" from="1985" to="2007" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Total variation blind deconvolution</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-K</forename><surname>Wong</surname></persName>
		</author>
		<idno type="DOI">10.1109/83.661187</idno>
		<ptr target="https://doi.org/10.1109/83.661187" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="370" to="375" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rank-sparsity incoherence for matrix decomposition</title>
		<author>
			<persName><forename type="first">V</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sanghavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Parrilo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Willsky</surname></persName>
		</author>
		<idno type="DOI">10.1137/090761793</idno>
		<ptr target="https://doi.org/10.1137/090761793" />
	</analytic>
	<monogr>
		<title level="j">SIAM J. Optim</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="572" to="596" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Solving random quadratic systems of equations is nearly as easy as solving linear systems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Candès</surname></persName>
		</author>
		<idno type="DOI">10.1002/cpa.21638</idno>
		<ptr target="https://doi-org.offcampus.lib.washington.edu/10.1002/cpa.21638" />
	</analytic>
	<monogr>
		<title level="j">Comm. Pure Appl. Math</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="822" to="883" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Exact and stable covariance estimation from quadratic sampling via convex programming</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Goldsmith</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIT.2015.2429594</idno>
		<ptr target="https://doi.org/10.1109/TIT.2015.2429594" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="page" from="4034" to="4059" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">H</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Ledyaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Wolenski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nonsmooth Analysis and Control Theory, Grad. Texts in Math</title>
		<imprint>
			<biblScope unit="volume">178</biblScope>
			<date type="published" when="1998">1998</date>
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On proximal subgradient splitting method for minimizing the sum of two nonsmooth convex functions, Set-Valued Var</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cruz</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11228-016-0376-5</idno>
		<ptr target="https://doi.org/10.1007/s11228-016-0376-5" />
	</analytic>
	<monogr>
		<title level="j">Anal</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="245" to="263" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Complexity of Finding Near-Stationary Points of Convex Functions Stochastically</title>
		<author>
			<persName><forename type="first">D</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Drusvyatskiy</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1802.08556" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Drusvyatskiy</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1803.06523" />
		<title level="m">Stochastic Model-Based Minimization of Weakly Convex Functions</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Stochastic Subgradient Method Converges at the Rate O(k -1/4 ) on Weakly Convex Functions</title>
		<author>
			<persName><forename type="first">D</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Drusvyatskiy</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1802.02988" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">The Nonsmooth Landscape of Phase Retrieval</title>
		<author>
			<persName><forename type="first">D</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Drusvyatskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Paquette</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1711.03247" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Copyright © by SIAM. Unauthorized reproduction of this article is prohibited. DAMEK DAVIS AND DMITRIY DRUSVYATSKIY</title>
		<idno>Downloaded 01/23/19 to 129.215.17.190</idno>
		<ptr target="http://www.siam.org/journals/ojsa.php" />
		<imprint/>
	</monogr>
	<note>Redistribution subject to SIAM license or copyright</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Proximally Guided Stochastic Method for Nonsmooth, Nonconvex Problems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Grimmer</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1707.03505" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">First-order methods of smooth convex optimization with inexact oracle</title>
		<author>
			<persName><forename type="first">O</forename><surname>Devolder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Glineur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nesterov</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10107-013-0677-5</idno>
		<ptr target="https://doi.org/10.1007/s10107-013-0677-5" />
	</analytic>
	<monogr>
		<title level="j">Math. Program</title>
		<imprint>
			<biblScope unit="volume">146</biblScope>
			<biblScope unit="page" from="37" to="75" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Drusvyatskiy</surname></persName>
		</author>
		<title level="m">The proximal point method revisited, SIAG/OPT Views and News</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Drusvyatskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lewis</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1610.03446" />
		<title level="m">Nonsmooth Optimization Using Taylor-Like Models: Error Bounds, Convergence, and Termination Criteria</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Error bounds, quadratic growth, and linear convergence of proximal methods</title>
		<author>
			<persName><forename type="first">D</forename><surname>Drusvyatskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Lewis</surname></persName>
		</author>
		<idno type="DOI">10.1287/moor.2017.0889</idno>
		<ptr target="https://doi.org/10.1287/moor.2017.0889" />
	</analytic>
	<monogr>
		<title level="j">Math. Oper. Res</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="919" to="948" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Efficiency of minimizing compositions of convex functions and smooth maps</title>
		<author>
			<persName><forename type="first">D</forename><surname>Drusvyatskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Paquette</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10107-018-1311-3</idno>
		<ptr target="https://doi.org/10.1007/s10107-018-1311-3" />
	</analytic>
	<monogr>
		<title level="j">Math. Program</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Solving (Most) of a Set of Quadratic Equalities: Composite Optimization for Robust Phase Retrieval</title>
		<author>
			<persName><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ruan</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1705.02356" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ruan</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1703.08570" />
		<title level="m">Stochastic Methods for Composite Optimization Problems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Efficient online and batch learning using forward backward splitting</title>
		<author>
			<persName><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="2899" to="2934" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Gradient Method with Inexact Oracle for Composite Non-Convex Optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dvurechensky</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1703.09180" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Phase retrieval: Stability and recovery guarantees</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Eldar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mendelson</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.acha.2013.08.003</idno>
		<ptr target="https://doi.org/10.1016/j.acha.2013.08.003" />
	</analytic>
	<monogr>
		<title level="j">Appl. Comput. Harmon. Anal</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="473" to="494" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Stochastic first-and zeroth-order methods for nonconvex stochastic programming</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ghadimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Optim</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="2341" to="2368" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Mini-batch stochastic approximation methods for nonconvex stochastic composite optimization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ghadimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Program</title>
		<imprint>
			<biblScope unit="volume">155</biblScope>
			<biblScope unit="page" from="267" to="305" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">First order methods for nonsmooth convex large-scale optimization, I: General purpose methods</title>
		<author>
			<persName><forename type="first">A</forename><surname>Juditsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nemirovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Optimization for Machine Learning</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Sra</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Write</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="266" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Non-convex finite-sum optimization via SCSG methods</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="2345" to="2355" />
			<date type="published" when="2017">2017</date>
			<publisher>Curran Associates</publisher>
			<pubPlace>Red Hook, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Understanding blind deconvolution algorithms</title>
		<author>
			<persName><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2011.148</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2011.148" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="2354" to="2367" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A proximal method for composite minimization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wright</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10107-015-0943-9</idno>
		<ptr target="https://doi.org/10.1007/s10107-015-0943-9" />
	</analytic>
	<monogr>
		<title level="j">Math. Program</title>
		<imprint>
			<biblScope unit="volume">158</biblScope>
			<biblScope unit="page" from="501" to="546" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Self-calibration and biconvex compressive sensing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Strohmer</surname></persName>
		</author>
		<idno type="DOI">10.1088/0266-5611/31/11/115002</idno>
		<ptr target="https://doi.org/10.1088/0266-5611/31/11/115002" />
	</analytic>
	<monogr>
		<title level="j">Inverse Probl</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Supervised dictionary learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/3448-supervised-dictionary-learning.pdf" />
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1033" to="1040" />
			<date type="published" when="2009">2009</date>
			<publisher>Curran Associates</publisher>
			<pubPlace>Red Hook, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Proximité et dualité dans un espace hilbertien</title>
		<author>
			<persName><forename type="first">J.-J</forename><surname>Moreau</surname></persName>
		</author>
		<ptr target="http://www.numdam.org/item/?id=BSMF" />
	</analytic>
	<monogr>
		<title level="j">Bull. Soc. Math. France</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="273" to="299" />
			<date type="published" when="1965">1965. 1965 93 273 0</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Robust stochastic approximation approach to stochastic programming</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nemirovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Juditsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Optim</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1574" to="1609" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Problem Complexity and Method Efficiency in Optimization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nemirovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yudin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Wiley-Intersci. Ser. Discrete Math. Optim</title>
		<imprint>
			<date type="published" when="1983">1983</date>
			<publisher>John Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">How to make the gradients small</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Nesterov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OPTIMA: Math. Optim. Soc. Newsletter</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="10" to="11" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Gradient methods for minimizing composite functions</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Nesterov</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10107-012-0629-5</idno>
		<ptr target="https://doi.org/10.1007/s10107-012-0629-5" />
	</analytic>
	<monogr>
		<title level="j">Math. Program</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="page" from="125" to="161" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Universal gradient methods for convex optimization problems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Nesterov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Program</title>
		<imprint>
			<biblScope unit="volume">152</biblScope>
			<biblScope unit="page" from="381" to="404" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Minimization of nondifferentiable functions in the presence of noise</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Nurminskii</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF01071541</idno>
		<ptr target="https://doi.org/10.1007/BF01071541" />
	</analytic>
	<monogr>
		<title level="j">Cybernetics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="619" to="621" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">The quasigradient method for the solving of the nonlinear programming Downloaded 01/23/19 to 129.215.17.190. Redistribution subject to SIAM license or copyright</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Nurminskii</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF01068677</idno>
		<ptr target="https://doi.org/10.1007/BF01068677" />
	</analytic>
	<monogr>
		<title level="j">Cybernetics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="145" to="150" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Amenable functions in optimization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Poliquin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Rockafellar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Nonsmooth Optimization: Methods and Applications</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Gordon and Breach</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="338" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Prox-regular functions in variational analysis</title>
		<author>
			<persName><forename type="first">R</forename><surname>Poliquin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Rockafellar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Amer. Math. Soc</title>
		<imprint>
			<biblScope unit="volume">348</biblScope>
			<biblScope unit="page" from="1805" to="1838" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Proximal stochastic methods for nonsmooth nonconvex finite-sum optimization</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1145" to="1153" />
			<date type="published" when="2016">2016</date>
			<publisher>Curran Associates</publisher>
			<pubPlace>Red Hook, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A stochastic approximation method</title>
		<author>
			<persName><forename type="first">H</forename><surname>Robbins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Monro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Math. Statistics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="400" to="407" />
			<date type="published" when="1951">1951</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Rockafellar</surname></persName>
		</author>
		<title level="m">Convex Analysis</title>
		<meeting><address><addrLine>Princeton, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Princeton University Press</publisher>
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Favorable classes of Lipschitz-continuous functions in subgradient optiin Progress in Nondifferentiable Optimization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Rockafellar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IIASA Collaborative Proc. Ser</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="125" to="143" />
			<date type="published" when="1981">1981</date>
			<pubPlace>Laxenburg</pubPlace>
		</imprint>
	</monogr>
	<note>IIASA</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">The fundamental risk quadrangle in risk management, optimization and statistical estimation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Rockafellar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Uryasev</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.sorms.2013.03.001</idno>
		<ptr target="https://doi.org/10.1016/j.sorms.2013.03.001" />
	</analytic>
	<monogr>
		<title level="j">Surv. Oper. Res. Management Sci</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="33" to="53" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Rockafellar and R.-B. Wets, Variational Analysis</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Grundlehren Math. Wiss.</title>
		<imprint>
			<biblScope unit="volume">317</biblScope>
			<date type="published" when="1998">1998</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Convex Analysis</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Rockafellar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Princet. Math. Ser</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<date type="published" when="1970">1970</date>
			<publisher>Princeton University Press</publisher>
			<pubPlace>Princeton, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Optimization of conditional value-at-risk</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Rockafellar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Uryasev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Risk</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="21" to="41" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Stochastic Proximal Iteration: A Non-Asymptotic Improvement Upon Stochastic Gradient Descent</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">K</forename><surname>Ryu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<ptr target="www.math.ucla.edu/∼eryu/papers/spi.pdf" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">A Simpler Approach to Obtaining an O(1/t) Convergence Rate for the Projected Stochastic Subgradient Method</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1212.2002" />
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Angular synchronization by eigenvectors and semidefinite programming</title>
		<author>
			<persName><forename type="first">A</forename><surname>Singer</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.acha.2010.02.001</idno>
		<ptr target="https://doi.org/10.1016/j.acha.2010.02.001" />
	</analytic>
	<monogr>
		<title level="j">Appl. Comput. Harmon. Anal</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="20" to="36" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">A geometric analysis of phase retrieval</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Found. Comput. Math</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1131" to="1198" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Dictionary learning, IEEE Signal Process</title>
		<author>
			<persName><forename type="first">I</forename><surname>Tosic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
		<idno type="DOI">10.1109/MSP.2010.939537</idno>
		<ptr target="https://doi.org/10.1109/MSP.2010.939537" />
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="27" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Asymptotic and finite-sample properties of estimators based on stochastic gradients</title>
		<author>
			<persName><forename type="first">P</forename><surname>Toulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Airoldi</surname></persName>
		</author>
		<idno type="DOI">10.1214/16-AOS1506</idno>
		<ptr target="https://doi.org/10.1214/16-AOS1506" />
	</analytic>
	<monogr>
		<title level="j">Ann. Statist</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="1694" to="1727" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Toulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Horel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Airoldi</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1510.00967" />
		<title level="m">Stable Robbins-Monro Approximations Through Stochastic Proximal Updates</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Stochastic compositional gradient descent: Algorithms for minimizing compositions of expected-value functions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10107-016-1017-3</idno>
		<ptr target="https://doi.org/10.1007/s10107-016-1017-3" />
	</analytic>
	<monogr>
		<title level="j">Math. Program</title>
		<imprint>
			<biblScope unit="volume">161</biblScope>
			<biblScope unit="page" from="419" to="449" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Block stochastic gradient iteration for convex and nonconvex optimization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Optim</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1686" to="1716" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
