<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Enterprise Cooperation and Competition Analysis with a Sign-Oriented Preference Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Le</forename><surname>Dai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Techonology</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><surname>Yin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Techonology</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chuan</forename><surname>Qin</surname></persName>
							<email>chuanqin0426@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Techonology</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Baidu Talent Intelligence Center</orgName>
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tong</forename><surname>Xu</surname></persName>
							<email>tongxu@ustc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Techonology</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName><surname>Xiangnan He</surname></persName>
							<email>xiangnanhe@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="department">School of Data Science</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Enhong</forename><surname>Chen</surname></persName>
							<email>cheneh@ustc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Techonology</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Data Science</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hui</forename><surname>Xiong</surname></persName>
							<email>hxiong@rutgers.edu</email>
							<affiliation key="aff3">
								<orgName type="institution">Rutgers University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Enterprise Cooperation and Competition Analysis with a Sign-Oriented Preference Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3394486.3403120</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Enterprise Analysis</term>
					<term>Graph Embedding</term>
					<term>Signed Network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The development of effective cooperative and competitive strategies has been recognized as the key to the success of many companies in a globalized world. Therefore, many efforts have been made on the analysis of cooperation and competition among companies. However, existing studies either rely on labor intensive empirical analysis with specific cases or do not consider the heterogeneous company information when quantitatively measuring company relationships in a company network. More importantly, it is not clear how to generate a unified representation for cooperative and competitive strategies in a data driven way. To this end, in this paper, we provide a large-scale data driven analysis on the cooperative and competitive relationships among companies in a Sign-oriented Preference Network (SOPN). Specifically, we first exploit a Relational Graph Convolutional Network (RGCN) for generating a deep representation of the heterogeneous company features and a company relation network. Then, based on the representation, we generate two sets of preference vectors for each company by utilizing the attention mechanism to model the importance of different relations, representing their cooperative and competitive strategies respectively. Also, we design a sign constraint to model the dependency between cooperation and competition relations. Finally, we conduct extensive experiments on a real-world dataset, and verify the effectiveness of our approach. Moreover, we provide a case study to show some interesting patterns and their potential business value.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>With the accelerated process of globalization, the relations between enterprises have become closer than ever, brought with it opportunities as well as challenges. Faced with fierce competition, enterprises are devoting much attention to the selection of business partners, in order to achieve success in this globalized world <ref type="bibr" target="#b1">[2]</ref>. Thus, the analysis of cooperation and competition has been a crucial task, beneficial to both enterprises and third-party investors with helpful development guidances and insightful investment cues <ref type="bibr" target="#b25">[25]</ref>.</p><p>In the literature, large efforts have been made on this issue with various techniques. Traditionally, experts of management science usually deal with this problem via empirical study on specific cases <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14]</ref>, in which statistical methods are widely utilized. Specifically, some researches mainly focus on certain pairs of enterprises with cooperation or competition relationship <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b18">19]</ref>. In these cases, the companies will be treated as separately or rarely connected, and then evaluated via theoretical analysis over quantitative experiments. However, they can hardly provide the overall analysis in consideration of global connections among enterprises. Meanwhile, some other researchers study this issue in the perspective of enterprise network with specific relations like shareholding <ref type="bibr" target="#b2">[3]</ref>, talent flow <ref type="bibr" target="#b32">[32]</ref>, etc. Along this line, they represent the company relation network with deep learning modules to support downstream tasks. However, current solutions may fail due to the following two reasons. First, existing studies may fail to automatically analyze strategies about cooperation and competition, integrating heterogeneous enterprise information and multiple relations. It remains unclear how to generate a unified representation for cooperation and competition strategies in a data driven way. Second, since existing studies mainly weigh the connections without distinguishing their attitude (cooperation or competition), the dependency between multiple relations are always ignored. For example, one common situation is that enterprises sometimes seek cooperation with the competitors of their competition candidates <ref type="bibr" target="#b23">[23]</ref>, as suggested by the idiom "the enemy of my enemy is my friend". Therefore, a more comprehensive solution is still urgently required.</p><p>To that end, in this paper, we propose a novel Sign-oriented Preference Network (SOPN) for more effective cooperation and competition analysis. In general, the following major challenges will be addressed. First, heterogeneous enterprise information for describing the profile of enterprises and multiple relations between them should be carefully integrated. Second, a unified representation about cooperation and competition strategies should be generated for a more comprehensive analysis. Third, the mutual dependency between cooperation and competition relations should also be considered. Last but not least, during the representation of enterprise network, interpretability of cooperation and competition strategies must be ensured to support downstream applications. To deal with these challenges, SOPN generates two sets of preference vectors that represent enterprise strategies for cooperation and competition. Specifically, we first aggregate the company features and company relation graph, and employ a Relational Graph Convolutional Network (RGCN) <ref type="bibr" target="#b22">[22]</ref> to generate deep representation of the heterogeneous data. Then, with the embedding vectors derived from RGCN, we leverage the attention mechanism <ref type="bibr" target="#b0">[1]</ref> to distinguish the effects of multiple company relations on company strategies during the computation of cooperation preferences and competition preferences. Also, to model the dependency of the cooperation and competition relations, we design a sign constraint based on signed graph theory <ref type="bibr" target="#b30">[30]</ref>. At last, the model is trained through a hybrid loss function, combining task specific loss with sign constraint and graph decoding loss, to ensure both effective modeling and better interpretability. To be specific, the contributions of this paper can be summarized as follows:</p><p>• We propose a novel embedding framework on Sign-oriented Preference Network (SOPN) for generating the unified representation of cooperation and competition strategies, in which heterogeneous information and multiple relations have been effectively integrated. • The hybrid loss function has been carefully designed, which includes the sign-oriented constraint to describe the dependency among cooperation and competition relations, and the decoding loss to ensure the interpretability of strategies.</p><p>• Extensive experiments on real-world enterprise dataset have validated the effectiveness of our solution, and further revealed some interesting discoveries through case study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Generally, the related work of our study can be grouped into the following: Company Analysis and Graph Embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Company Analysis</head><p>The problem of company analysis has been widely studied by experts and researchers, especially in the business and management domains. Existing researches can be divided into the following three categories. The first is based on case studies. For example, Quintana et al. <ref type="bibr" target="#b18">[19]</ref> analyzed the effect of co-opetition (shortly for cooperation and competition) strategy on technological diversity and new product development through a panel data of European dedicated biotechnology firms. Also, Gnyawali et al. <ref type="bibr" target="#b8">[9]</ref> investigated why and how co-opetition between large firms occurs, evolves, and impacts the participating firms and the industry through a case study of two giant companies in the electronics industry. Those studies requires experts to conduct careful empirical research over specific cases, which can be very labor intensive. The second is conducted through statistic methods. For example, Deng et al. <ref type="bibr" target="#b4">[5]</ref> ranked the relative performance of competing companies with modified statistic method TOPSIS and conducted an empirical study of a real case to prove the effectiveness of their approach. And Kung et al. <ref type="bibr" target="#b13">[14]</ref> utilized the Globalization Grey Relational Analysis (GRA), to find the significant financial ratio variables and other financial indicators affecting the financial performance of venture capital enterprises in Taiwan. Although great success has been achieved, those approaches are limited to a few topics like financial performance evaluation <ref type="bibr" target="#b4">[5]</ref> and supplier selection <ref type="bibr" target="#b16">[17]</ref>. In addition, the third is based on machine learning techniques due to their superiority of capturing complex information. As an example, Zhang et al. <ref type="bibr" target="#b33">[33]</ref> incorporated information from job transition records of digital resumes to help sharpen company talent strategy. Those works have achieved great results within specific tasks.</p><p>As we can see, despite the great success achieved in enterprise analysis, most of the existing researches are still in the stage of case analysis, lack of quantitative experiments combining various company information and multiple relations. Moreover, none of existing researches obtain a unified representation about cooperation and competition strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph Embedding</head><p>To better utilize all the information of the company input, one of our main requirements is to properly generate representations for the company network, preserving heterogeneous company information and company relations.</p><p>In recent years, graph embedding methods have attracted increasing attention due to the ubiquity of networks in real world, and are proven to be efficient and effective in network representation. First, to model network relations, some proximity preserving methods have been introduced. Methods based on random walk assume that nodes with the similar network structure have similar vector representation. For example, Perozzi et al. <ref type="bibr" target="#b17">[18]</ref> sampled local network structures by random walk, and then utilized skip-gram model to learn the vectorized representation. Some other methods learn node representations based on k-order distance between nodes in network. For example, Tang et al. <ref type="bibr" target="#b24">[24]</ref> proposed LINE to explicitly define two functions for preserving first-order and second-order proximities, and minimized the combination of the two. Besides, some methods also incorporate deep learning to obtain high-order nonlinear representation of the local structural context <ref type="bibr" target="#b26">[26]</ref><ref type="bibr" target="#b27">[27]</ref><ref type="bibr" target="#b28">[28]</ref>. Second, node representations are enhanced to combine extra attribute information. As an example, Kipf et al. <ref type="bibr" target="#b12">[13]</ref> proposed Graph Convolutional Network (GCN) based on a first-order approximation of spectral convolutions on graphs to propagate attribute information. Hamilton et al. <ref type="bibr" target="#b10">[11]</ref> sampled and aggregated attribute information from neighboring nodes to iteratively generate node embeddings in GraphSage. Third, interactions in multi-relational networks are taken into consideration. For example, Schlichtkrull et al. <ref type="bibr" target="#b22">[22]</ref> adapted GCN for highly multi-relational data such as realistic knowledge bases. Zhang et al. <ref type="bibr" target="#b31">[31]</ref> proposed HetGNN which aims at representation learning in heterogeneous graphs with multiple types of nodes as well as relations. Moreover, there are also some studies focusing on two specific relations, i.e. those of a signed graph <ref type="bibr" target="#b30">[30]</ref>, where relations are labeled as either positive or negative <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b29">29]</ref>. As an example, Derr et al. <ref type="bibr" target="#b5">[6]</ref> proposed a node embedding network SGCN for signed graphs, considering different propagation rules and constraints for both signs under balance theory of sign networks.</p><p>All the previous studies focus on node embedding and applications based on it, but they do not generate representation for preferences, and cannot be directly applied to our cooperation and </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head><p>In this section, we present our method in detail. We first formulate the problem of enterprise cooperation and competition strategy analysis. Then, we give an overview of the model architecture. Afterwards, we describe all the details of the framework we propose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Definition</head><p>In this subsection, we formally introduce the cooperation and competition strategies analysis problem and clarify mathematical symbols in this paper. For facilitating illustration, Table <ref type="table" target="#tab_0">1</ref> lists some important mathematical notations used through out this paper.</p><p>In our setup, the company information and company relations are given as input in the heterogeneous form, which contains numeric and textual company information and complex connections between companies. Numeric input includes some parameters and statistics of a running company, such as establish date, market capitalization and number of employees. They can reflect a company's age, scale, and many other beneficial information. Textual input consists of company description text, usually describing the domain, main business, and some other comments on a company, which gives the company a high-level summarization.</p><p>Formally speaking, for each company c i in company set C, we denote its numeric information as its numeric feature µ i ∈ R N µ , where N µ is the dimension of numeric input, and represent its introduction and description text as a word sequence w i = {w 1 , w 2 , . . . , w T } with each w t ∈ W, where W is the set of all words. Besides, the multiple inter-company relations are denoted as company relation graph G = (V, E, R), where the set of nodes V = {1, 2, . . . , |C|} represents all companies, and the edge set E contains all the relations, where (u, r , v) ∈ E represents that there is a relation between company u and v of relation type r ∈ R.</p><p>With the symbols stated above, we define the enterprise cooperation and competition strategy analysis problem as follows: Definition 3.1. Company cooperation and competition strategy analysis. Given a company c i with heterogeneous information, namely numeric information µ i , a textual sequence w i = {w 1 , w 2 , . . . , w T } with sequence length T and a company relation graph G = (V, E, R) containing relations of companies in company set C, our goal is to generate a set of N coo preference vectors P coo i representing the cooperation strategy, and a set of N com preference vectors P com i representing the competition strategy. Through the encoding of all companies, the cooperation and competition preference vectors should be as close to the representations of its cooperation or competition candidates as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Method Overview</head><p>Figure <ref type="figure" target="#fig_0">1</ref> shows the overall architecture of Sign-Oriented Preference Network. As shown in Figure <ref type="figure" target="#fig_0">1</ref>(a), we first generate node embedding emb i for each company c i based on the heterogeneous input introduced before. We aggregate numerical input µ i and text input w i processed by a Gated Recurrent Unit (GRU) <ref type="bibr" target="#b3">[4]</ref> as the initial node feature f i . Then we employ Relational Graph Convolutional Network (RGCN) <ref type="bibr" target="#b22">[22]</ref> to encode the heterogeneous company features and company relations, and generate deep representations emb i for each company. Second, as shown in Figure <ref type="figure" target="#fig_0">1</ref>(b), to generate more representative cooperation and competition preference vectors P coo i and P com i , we incorporate the attention mechanism to distinguish different influence of different relations for each company, preserving cooperation and competition strategies respectively. Third, we train the model through a sign-oriented hybrid loss function that we propose, combining task specific loss with other constraints. To model the interaction and dependency between cooperation and competition relations, we design sign constraint L siдn according to the balance theory in signed graph theory <ref type="bibr" target="#b30">[30]</ref>. And to ensure that our approach is both effective and interpretable, we also include a decoding loss L dec in the hybrid loss function. In the following subsections, we will explain how each part of our approach works in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Company Embedding</head><p>The first step is to generate node embeddings for each company based on the heterogeneous input. The aim of doing so is to aggregate as much information about companies, and acquire unified representations of companies for further cooperation and competition strategy analysis. Thus, it is necessary to deal with the heterogeneous input properly.</p><p>As described above, the heterogeneous input is consisted of company features and a company relation graph G. Specifically, the company features contain mainly two types of input information, the numeric information (e.g. company registered capital and numbers of employees) and the textual descriptions (e.g. company introductions and detailed scope of business operations). For each company c i , its numeric information of N µ dimensions are normalized as numeric feature µ i , while its textual descriptions are in the form of a word sequence w i = {w 1 , w 2 , . . . , w T } with T words. Here, we first leverage word2vec <ref type="bibr" target="#b15">[16]</ref> to transform each word w t in the sequence into a d 0 -dimensional pre-trained word embedding vector. After the initialization, considering that the company descriptions have various lengths and can be long, we embed the textual sequence by a bi-directional variation of RNN, a bi-directional Gated Recurrent Unit (Bi-GRU), which preserves the most of contextual information of company description sentences from both forward and backward directions. Formally, given the description sequence w i = {w 1 , w 2 , . . . , w T } of company c i with sequence length T , we set the input of the first layer of Bi-GRU as</p><formula xml:id="formula_0">− → h (0) = ← − h (0) = {w 1 , w 2 , . . . , w T }. At time step t, forward hidden state − → h (l )</formula><p>t and backward hidden state</p><formula xml:id="formula_1">← − h (l )</formula><p>t are updated at each layer l based on the previous hidden states</p><formula xml:id="formula_2">− → h (l ) t −1 and ← − h (l ) t −1</formula><p>for both directions as:</p><formula xml:id="formula_3">− → h (l ) t = GRU( − → h (l ) t −1 ; − → θ GRU ),<label>(1)</label></formula><formula xml:id="formula_4">← − h (l ) t = GRU( ← − h (l ) t −1 ; ← − θ GRU ),<label>(2)</label></formula><p>where − → θ GRU and ← − θ GRU denote forward and backward GRU parameters to be learned respectively.</p><p>To extract deep context in company descriptions, the introduced GRU architecture has L 0 Bi-GRU layers. Thus, the deep linguistic information are able to be captured in the hidden states. As hidden state at each direction only contains one-side context, it is beneficial to combine them into one vector. Therefore, we obtain the company description sequence representation of company c i at the last time step T by concatenating</p><formula xml:id="formula_5">− → h (l ) t and ← − h (l )</formula><p>t :</p><formula xml:id="formula_6">τ i = [ − → h (L 0 ) T ; ← − h (L 0 ) T ].<label>(3)</label></formula><p>With normalized numeric input µ i and textual input τ i derived from Bi-GRU, now we can get the input feature f i of company c i by concatenating µ i and τ i :</p><formula xml:id="formula_7">f i = [µ i ; τ i ].<label>(4)</label></formula><p>So far, we have obtained the deep representation of company features, combining both numeric and textual information. However, each feature vector f i only describes information about company c i , but its relations with other companies are not considered yet. As discussed in Section 3.1, the various relations between all companies are represented as company relation graph G = (V, E, R). The unified representations for all companies should contain not only features of nodes, but also structural information in G. Thus, we employ a Relational Graph Convolutional Network (RGCN) to generate embedding for each node i in G due to its multiple relations. Formally, for each company c i or node i, with its input features { f i } as the initial node features, the embedding of node i is calculated through messages passed from other nodes at layer l: emb</p><formula xml:id="formula_8">(l +1) i = σ r ∈R j ∈N r i 1 c i,r W (l ) r emb (l ) j + W (l ) 0 emb (l ) i ,<label>(5)</label></formula><p>where σ is the activation function (e.g. sigmoid), N r i denotes neighbors of company c i under relation r , c i,r is normalization constant of node i and relation r that can either be learned or chosen in advance, and W (l ) r and W (l ) 0 are two weight matrices to be learned. Since a RGCN layer only aggregates information from direct neighbors, we need more layers to propagate information of the whole network and model deep structural information. With L 1 RGCN layers employed, more structural information can be preserved in the embedding vectors. Thus, the heterogeneous input is encoded as node embedding emb i = emb (L 1 ) i at each node i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Cooperation and Competition Preferences Aggregating</head><p>In this subsection, we aim at generating two sets of preference vectors for each company c i based on node embeddings {emb i } acquired, representing its cooperation and competition strategies respectively. As a company's cooperation and competition strategies have great influence on how it connects with other companies, it is intuitive to aggregate information from its neighbors. Besides, various relations between companies are of different importance to the cooperation and competition strategies. Therefore, we leverage the attention mechanism to learn the contributions of different relations when generating the set of N coo cooperation preference vectors</p><formula xml:id="formula_9">P coo i = {p coo i,k , k = 1, ..., N coo } and N com competition pref- erence vectors P com i = {p coo i,k , k = 1, .</formula><p>.., N com } for each company c i , with two hyper-parameters N coo and N com . Specifically, the attention score α coo r,k of relation r on the k-th cooperation preference vector is calculated over all company pairs with relation r as:</p><formula xml:id="formula_10">α coo r,k = exp(e coo r,k ) r ∈R exp(e coo r,k ) , (<label>6</label></formula><formula xml:id="formula_11">)</formula><formula xml:id="formula_12">e coo r,k = v coo α,k tanh W coo α,k (i,r ′ , j)∈ E if r ′ =r [emb i ; emb j ] + b coo α,k ,<label>(7)</label></formula><p>where v coo α,k , W coo α,k , b coo α,k are all the parameters to be learned during the training process.</p><p>Similarly, the attention score α com r,k of relation r on the k-th competition preference vector is computed as:</p><formula xml:id="formula_13">α com r,k = exp(e com r,k ) r ∈R exp(e com r,k ) ,<label>(8)</label></formula><formula xml:id="formula_14">e com r,k = v com α,k tanh W com α,k (i,r ′ , j)∈ E if r ′ =r [emb i ; emb j ] + b com α,k .<label>(9)</label></formula><p>After the attention scores are defined, we can aggregate information for each company from its neighboring node embeddings, with some relations emphasized and others weakened. The k-th cooperation preference p coo i,k and competition preference p com i,k of company c i are represented as:</p><formula xml:id="formula_15">p coo i,k = j ∈N i α coo r i, j ,k emb j ,<label>(10)</label></formula><formula xml:id="formula_16">p com i,k = j ∈N i α com r i, j ,k emb j ,<label>(11)</label></formula><p>where N i denotes the set of neighboring companies that are connected with company c i . Till now, we have defined the vectorized cooperation preference set and competition preference set for each company. To summarize, we first encode the heterogeneous company features and relations into embeddings by utilizing a RGCN model. Specifically, the textual features of company features are extracted with a Bi-GRU network from word sequence input. Then, we leverage the attention mechanism to model the importances of different relations, as we aggregate feature information from neighboring nodes and generate the cooperation and competition preferences. Next, we will describe in detail how to train the model with the designing of a sign-oriented hybrid loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Sign-oriented Hybrid Loss</head><p>Due to the complexity of the cooperation and competition analysis problem, we still need a well designed objective function to train the SOPN model. In this subsection, we focus on the construction of a novel sign-oriented hybrid loss function during the training process. First, a sign constraint based on the balance theory is introduced for modeling the dependency between cooperation and competition relations. Second, a decoding loss is constructed to extract meaningful information from the cooperation and competition preferences and ensure the interpretability of the strategy analysis. At last, the sign constraint and the decoding loss are combined with the main prediction task objective to train the model and learn all the parameters introduced in the previous subsection.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.1">Sign Constraint on Cooperation and Competition Preferences.</head><p>As described before, the cooperation and competition relations are not independent. One common situation is that a company can somehow seek cooperation with the competitors of its competition candidates, as the idiom suggested, "the enemy of my enemy is my friend". This gives us the insight of modeling the dependency between cooperation and competition relations into a sign constraint, with cooperation and competition considered as positive and negative links respectively. According to the balance theory in signed network, each triangle of three companies contains either two negative links or zero negative links. That means, for each company pair c i and c j with cooperative (positive) relation, the links between company c i and c j and any other company are much likely to have the same signs, positive or negative. Intuitively, as illustrated in Figure <ref type="figure" target="#fig_2">2</ref>(a), we expect the cooperation preference vectors P coo i of company c i to be closer to its cooperation candidate company c j 's cooperation preference vectors P coo j , than company c j 's competition preference vectors P com j , and vice versa. Formally, the sign constraint between two companies with cooperative relation can be formulated as:</p><formula xml:id="formula_17">l + (i, j) = k max{0, min k 1 ∥p coo i,k − p coo j,k 1 ∥ − min k 2 ∥p coo i,k − p com j,k 2 ∥} + k max{0, min k 1 ∥p com i,k − p com j,k 1 ∥ − min k 2 ∥p com i,k − p coo j,k 2 ∥}. (<label>12</label></formula><formula xml:id="formula_18">)</formula><p>On the contrary, as illustrated in Figure <ref type="figure" target="#fig_2">2</ref>(b), for two companies c i and c j with competitive relation, the cooperation preference vectors P coo i of company c i should be closer to the competition preference vectors P com j of company c j , than its cooperation preference vectors P coo j . Formally, the sign constraint between two companies with competitive relation can be formulated as:</p><formula xml:id="formula_19">l − (i, j) = k max{0, min k 1 ∥p coo i,k − p com j,k 1 ∥ − min k 2 ∥p coo i,k − p coo j,k 2 ∥} + k max{0, min k 1 ∥p com i,k − p coo j,k 1 ∥ − min k 2 ∥p com i,k − p com j,k 2 ∥}. (<label>13</label></formula><formula xml:id="formula_20">)</formula><p>Thus, the total sign constraint between cooperation and competition relations is as follows:</p><formula xml:id="formula_21">L siдn = (i, j)∈coo l + (i, j) + (i, j)∈com l − (i, j),<label>(14)</label></formula><p>where coo and com denote the set of company pairs labeled with cooperation and competition relations, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.2">Node Embeddings Decoding.</head><p>For further analysis on enterprise cooperation and competition strategies, we expect to extract meaningful information from the cooperation and competition preferences. That requires the cooperation preference vectors P coo i and competition preference vectors P com i along with the node embeddings {emb i } to have the capability of reconstructing input features and structural information of each node i. In doing so, a decoder is employed to transfer the node embeddings into input features. The objective function of the decoder is formulated as:</p><formula xml:id="formula_22">L r ec = i ∈V d(dec(emb i ), f i ),<label>(15)</label></formula><p>where d can be chosen as L2 distance.</p><p>Besides, to prove the node embeddings' capability of capturing structural information, we construct two sets P and N , both of which are consisted of sampled company pairs (i, j). The company pairs (i, j) in set P are sampled from links in company relation graph G, while company pairs (i, j) in N denote links that do not exists. The distance of pair (i, j) in P is expected to be closer than that in N . Formally, we can construct the objective function as:</p><formula xml:id="formula_23">L r el = (i, j)∈P, (u,v)∈N max 0, ∥emb i − emb j ∥ − ∥emb u − emb v ∥ . (16)</formula><p>Thus, the loss function of decoding node embeddings into input features and structural information is as follows:</p><formula xml:id="formula_24">L dec = β L r ec + (1 − β)L r el ,<label>(17)</label></formula><p>where β controls the proportion of the two terms. </p><formula xml:id="formula_25">coo α,k , W coo α,k , b coo α,k , v com α,k , W com α,k , b com α,k</formula><p>during the calculation of attention scores. In order to learn all these parameters, we design a cooperation and competition prediction task with some manually labeled cooperation and competition relations. For each company c i , as its cooperation and competition strategies are represented as cooperation and competition preference vectors, the cooperation preference set P coo i and competition preference set P com i should preserve as much information as possible about its cooperation and competition companies, respectively. Therefore, we formulate the objective function of this task as:</p><formula xml:id="formula_26">L pr ed = (i, j)∈coo min k d(emb j , p coo i,k ) + (i, j)∈com min k d(emb j , p com i,k ), (<label>18</label></formula><formula xml:id="formula_27">)</formula><p>where and com denote the labeled cooperation and competition company sets and d(x, y) is a loss function that measures distance between vectors x and y, such as L2 loss. Thus, the total loss of the training process contains three parts, namely the sign constraint loss L siдn , the decoding loss L dec from the reconstruction of input features and structural information, and the prediction loss L pr ed from the cooperation and competition prediction task:</p><formula xml:id="formula_28">L = λ 1 L siдn + λ 2 L dec + λ 3 L pr ed ,<label>(19)</label></formula><p>where λ 1 , λ 2 and λ 3 control the proportions of different objectives.  We conduct experiments on the enterprise dataset collected and hand labeled by experts. For the input information of our framework, we collect the enterprise dataset from a public data service website Tianyancha in China<ref type="foot" target="#foot_0">1</ref> , which contains a vast repository of Chinese enterprise information, including various company information and multiple relations about companies. Company information collected includes numerical features of each company (e.g. company registered capital and numbers of employees) and textual descriptions (company introductions and detailed scope of business operations). Besides, we also collect R different relations among these companies, including shareholding relation, litigation relation and so on. For cooperation and competition relations used in the training step, we manually extract those relations for a selection of companies from each company's public prospectus carefully labeled by analysts and experts. Some of the statistics are listed in Table <ref type="table" target="#tab_1">2</ref> to demonstrate the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Baselines.</head><p>In order to demonstrate the accuracy of learned preference vectors, we compare with some state-of-the-art algorithms on cooperation and competition relation prediction. The baseline methods can be divided into two categories. The first is to propose cooperation and competition candidates based on item recommendation. The second is to directly predict the possible relations of a pair of companies based on their company embeddings. The details of these baselines are illustrated as follows:</p><p>• BPR <ref type="bibr" target="#b20">[21]</ref>: It is an implicit feedback based recommendation method. In our setup, we can see observed cooperation and competition samples as positive feedback separately, and recommend companies for each relation independently.</p><p>• LibFM <ref type="bibr" target="#b19">[20]</ref>: It is a recommendation method best at contextaware prediction. The setup under our problem is similar to BPR, but with company information added as context. • GRU <ref type="bibr" target="#b3">[4]</ref>: Different from the above methods, the following methods are embedding based. This method only leverages company information and conducts company description embedding using GRU, a common variation of RNN.  <ref type="bibr" target="#b24">[24]</ref>: It learns the network embedding by preserving the first-order proximity or second-order proximity of the network structure separately. • GCN <ref type="bibr" target="#b12">[13]</ref>: This method is based on an efficient variant of convolution neural network operating directly on graphs. We also optimize node embedding in semi-supervised learning as proposed in their paper. • GraphSage <ref type="bibr" target="#b10">[11]</ref>: It is an up-to-date network embedding method for large scale graph embedding. • RGCN <ref type="bibr" target="#b22">[22]</ref>: This represents the state-of-the-art method for relational network embedding. • RGCN-T: This is an variation of RGCN, where we also incorporate text input in the network. The input features have the same preprocessing as in our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Evaluation Protocols.</head><p>In order to evaluate the accuracy of the generated cooperation and competition preference vectors, we split the hand-labeled relation into 60% training, 10% validation, and 30% testing data sets. It is also worth mentioning that, to mimic the situation where cooperation and competition companies are all unknown in reality, we filtered all the edges connect across training and testing set to avoid information leakage in the testing stage. We compare the results with two categories of evaluation metrics. The first is classification correctness for labeled pairs of companies. The metrics include Precision, Recall, F1-score and AUC. The second is ranking metrics, where for each company, we compare output scores of the candidates with the real labels on each relation. The metrics include NDCG and MAP.</p><p>4.1.4 Experimental Setup. We implement our model using Py-Torch<ref type="foot" target="#foot_1">2</ref> and DGL <ref type="foot" target="#foot_2">3</ref> . The parameters are all initialized using Xavier <ref type="bibr" target="#b7">[8]</ref> initialization. We set the number of GRU layers as 1 and number of RGCN layers as 2, and the embedding dimensions are set as 50. In the process of model training, we use the Adam optimizer <ref type="bibr" target="#b11">[12]</ref> for parameter optimization. We set learning rate as 0.01 and mini-batch size as 32. The parameters of baselines are set up similarly as our method and are all tuned to be optimal to ensure fair comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Overall Performance.</head><p>To demonstrate the effectiveness of our generated strategy vectors, we first compare our SOPN model with all the baseline methods on cooperation and competition candidate prediction problem. All the results are shown in Table <ref type="table" target="#tab_2">3</ref>.</p><p>From the results, we can get several observations. First, the performance of SOPN surpasses the baseline methods on most of the evaluation metrics. This clearly proves that our generated strategies have the ability to accurately represent all the cooperation and competition candidates. Second, our SOPN obtains much higher precision than baselines, which shows that our approach is useful  for a more thorough mining of potential candidates, and thus more suitable for real-world scenarios. Third, from comparison between GRU, GCN and RGCN, we can easily see that applying different forms of input has a different impact on the final performance, which proves the necessity of heterogeneous input embedding. Last but not least, we can see that cooperation relies more on text input, and relation information plays more important role in competition, as GRU works better on cooperation but graph embedding methods have better performance on competition prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Ablation</head><p>Study. Moreover, we conduct some ablation experiments to further show how each part of our method affects final results. We experiment with different types of inputs different loss configuration, and show the results in Table <ref type="table" target="#tab_3">4</ref>. The method SOPN-R is with randomly initialized feature as feature input, instead of numeric and textual inputs. SOPN-N is SOPN with numerical input but without text input, and SOPN-T is the opposite. We also omit different parts of loss for comparison.</p><p>From the results, we can draw the following conclusions: First, it is clear that the more information the model is fed with, the better performance it has. Second, sign loss can boost the performance alongside the prediction loss, which proves that there exists certain correlation between cooperation and competition, and our model is capable of capturing such correlation. Third, decoding loss L dec slightly reduces the performance in many cases. This is probably because the model strives to achieve two different goals at the same time. But with the slight decrease of performance, we are able to obtain more interpretability, which is a huge need for further cooperation and competition strategy analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Parameter Sensitivity.</head><p>We investigate the sensitivity of our model parameter in this section. Specifically, we mainly evaluate how the numbers of preference vectors N coo and N com affect the performance. The results are shown in Figure <ref type="figure" target="#fig_7">4</ref>.</p><p>In the first few steps, the precision keeps going up. This is because more preference vectors mean more capability of capturing different aspects of cooperation or competition strategies. But this would do harm to the recall, as shown in Figure <ref type="figure" target="#fig_7">4</ref>, since more preference vectors can cause more companies to be mistaken as candidates. In fact, it may also lead to serious over-fitting problem once the numbers is larger than 3, and even precision decreasing. Next thing we can notice is that, as the numbers of preference vectors increase, the F-1 scores gradually increase, and then decrease.</p><p>To ensure best performance, we choose the optimal value N coo = 3, N com = 2 as the number of preference vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Cooperation and Competition Analysis</head><p>To further demonstrate how to utilize the generated preference vectors and analyze enterprise cooperation and competition, we will continue our discussion with several case studies. As shown in Figure <ref type="figure" target="#fig_6">3</ref>, we first visualize the company of our concern (blue), along with its cooperation (green) and competition (red) preferences and all the related companies in two-dimensional space. All the company embeddings and preference vectors learned by SOPN are projected into two dimensional with the widely used visualization tool t-SNE <ref type="bibr" target="#b14">[15]</ref>. Then we set the pointer size in proportion to the real market capitalization of each company. As each preference vector can be seen as a cluster center of preferred companies' embeddings, we use the decoder defined in Section 3.5.3 to also restore preferred company scale for each preference vector. With size and distance representing company's actual scale and its relevance with other company in the company network respectively, we can intuitively discover how each company and preference relate to each other.</p><p>From Figure <ref type="figure" target="#fig_6">3</ref>, we can observe two different cooperation and competition strategies by two companies. The left company (labeled as 1 in the left) in Figure <ref type="figure" target="#fig_6">3</ref>(a) has a relatively small scale. From the visualization, we can infer that in order to fight against several large companies (red markers, representing companies in competition), this particular company unites with several small companies that are already closely related (green markers, representing companies in cooperation). In contrast, the right company (labeled as 1 in the right) in Figure <ref type="figure" target="#fig_6">3</ref>(b) has conducted a different strategy. With more fierce competition in the industry (implied by the red markers nearby), it has to cooperate with larger companies that are less related (as the green markers imply). Above these observations, it is also beneficial for discovering potential partners and/or competitors for an enterprise, which shows a great possibility of our method to help realize business value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we proposed a Sign-oriented Preference Network (SOPN) for the analysis of enterprise cooperation and competition strategies. Specifically, we first exploited a Relational Graph Convolutional Network (RGCN) to generate a deep representation for companies based on heterogeneous company information. Also, with the embedding vectors derived from RGCN, we designed the attention mechanism to distinguish the effects of multiple company relations on company strategies by quantitatively measuring their corresponding cooperation and competition preferences. Then, to model the dependency of the cooperation and competition relations, we designed a sign-oriented constraint based on the signed graph theory. Moreover, we trained the model through a hybrid loss function, combining task specific loss with the sign constraint and graph decoding loss, to ensure both effective modeling and better interpretability. Finally, extensive experiments on real-world enterprise dataset showed the effectiveness of our approach. Meanwhile, we provided a case study to reveal some interesting patterns as well as their business implications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The framework of Sign-Oriented Preference Network (SOPN).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Possible balanced triangles between cooperation and competition candidates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>3. 5 . 3</head><label>53</label><figDesc>Prediction Task Objective. With the sign constraint and the decoding loss taken into consideration, now we can focus on the main prediction objective for training the model. The parameters defined in the SOPN model can be divided into several parts: − → θ GRU and ← − θ GRU from the Bi-GRU, W (l ) r,k and W (l ) r,k , l = 0, . . . , L 1 from the RGCN network and v</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Visualization of two companies' cooperation and competition preferences and candidates. Marker scales are in proportion to real market capitalization of each company.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Comparison on different numbers of cooperation and competition preference vectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Mathematical notations.</figDesc><table><row><cell>Symbol</cell><cell>Description</cell></row><row><cell>C</cell><cell>The company set</cell></row><row><cell>f i</cell><cell>Concatenated features of numeric features µ i</cell></row><row><cell></cell><cell>and textual features τ i of company c i</cell></row><row><cell>G</cell><cell>The company network with node set V, labeled</cell></row><row><cell></cell><cell>edge set E and relation set R</cell></row><row><cell>N i</cell><cell>The neighboring nodes of node i</cell></row><row><cell>emb i</cell><cell>The embedded vector of company c i</cell></row><row><cell>P coo i , P com i</cell><cell>The sets of cooperation and competition prefer-ence vectors of company c i</cell></row><row><cell cols="2">N coo , N com Hyperparameters of the number of cooperation</cell></row><row><cell></cell><cell>and competition preference vectors</cell></row><row><cell cols="2">competition analysis problem. Furthermore, most of the previous</cell></row><row><cell cols="2">studies deal with single relation or single form of input, and are</cell></row><row><cell cols="2">not capable for incorporating heterogeneous input and multiple</cell></row><row><cell cols="2">relations in our problem.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Statistics of the enterprise dataset.</figDesc><table><row><cell>Name</cell><cell>Value</cell></row><row><cell>#Companies</cell><cell>3,595</cell></row><row><cell>#Labeled companies</cell><cell>300</cell></row><row><cell>#Numeric features</cell><cell>6</cell></row><row><cell>#Relation types</cell><cell>5</cell></row><row><cell>#Edges</cell><cell>3,188</cell></row><row><cell>#Labeled cooperation pairs</cell><cell>714</cell></row><row><cell cols="2">#Labeled competition pairs 1,552</cell></row><row><cell>Avg. words in description</cell><cell>106.36</cell></row><row><cell>Avg. edges per company</cell><cell>10.63</cell></row><row><cell>Avg. cooperation relations</cell><cell>2.38</cell></row><row><cell>Avg. competition relations</cell><cell>5.17</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Performance comparisons on cooperation and competition candidate prediction.</figDesc><table><row><cell>Method</cell><cell cols="2">Precision Recall</cell><cell cols="5">Cooperation F1 AUC NDCG MAP Precision Recall</cell><cell cols="2">Competition F1 AUC NDCG MAP</cell></row><row><cell>BPR</cell><cell>0.378</cell><cell cols="2">0.443 0.408 0.688</cell><cell>0.788</cell><cell>0.709</cell><cell>0.508</cell><cell cols="2">0.446 0.475 0.523</cell><cell>0.777</cell><cell>0.653</cell></row><row><cell>LibFM</cell><cell>0.396</cell><cell cols="2">0.454 0.423 0.696</cell><cell>0.786</cell><cell>0.707</cell><cell>0.514</cell><cell cols="2">0.460 0.485 0.528</cell><cell>0.776</cell><cell>0.653</cell></row><row><cell>GRU</cell><cell>0.501</cell><cell cols="2">0.534 0.517 0.701</cell><cell>0.792</cell><cell>0.752</cell><cell>0.583</cell><cell cols="2">0.362 0.447 0.561</cell><cell>0.779</cell><cell>0.670</cell></row><row><cell>Node2Vec</cell><cell>0.247</cell><cell cols="2">0.250 0.248 0.590</cell><cell>0.681</cell><cell>0.700</cell><cell>0.508</cell><cell cols="2">0.446 0.475 0.523</cell><cell>0.777</cell><cell>0.653</cell></row><row><cell>LINE</cell><cell>0.395</cell><cell cols="2">0.409 0.402 0.626</cell><cell>0.703</cell><cell>0.630</cell><cell>0.521</cell><cell cols="2">0.489 0.505 0.537</cell><cell>0.771</cell><cell>0.645</cell></row><row><cell>GCN</cell><cell>0.557</cell><cell cols="2">0.443 0.493 0.695</cell><cell>0.736</cell><cell>0.721</cell><cell>0.545</cell><cell cols="2">0.495 0.518 0.556</cell><cell>0.781</cell><cell>0.664</cell></row><row><cell>GraphSage</cell><cell>0.644</cell><cell cols="2">0.431 0.517 0.695</cell><cell>0.783</cell><cell>0.733</cell><cell>0.574</cell><cell cols="2">0.430 0.492 0.567</cell><cell>0.780</cell><cell>0.674</cell></row><row><cell>RGCN</cell><cell>0.622</cell><cell cols="2">0.431 0.510 0.703</cell><cell>0.806</cell><cell>0.742</cell><cell>0.596</cell><cell cols="2">0.446 0.510 0.583</cell><cell>0.778</cell><cell>0.666</cell></row><row><cell>RGCN-T</cell><cell>0.578</cell><cell cols="2">0.500 0.536 0.724</cell><cell>0.815</cell><cell>0.762</cell><cell>0.619</cell><cell cols="2">0.481 0.541 0.603</cell><cell>0.796 0.692</cell></row><row><cell>SOPN</cell><cell>0.765</cell><cell cols="4">0.453 0.569 0.745 0.857 0.767</cell><cell>0.630</cell><cell cols="3">0.493 0.553 0.613 0.815 0.682</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Ablation experiments demonstrating model performance with different input types and losses.</figDesc><table><row><cell>Method</cell><cell cols="2">Precision Recall</cell><cell cols="5">Cooperation F1 AUC NDCG MAP Precision Recall</cell><cell cols="2">Competition F1 AUC NDCG MAP</cell></row><row><cell>SOPN-R</cell><cell>0.555</cell><cell cols="2">0.340 0.422 0.658</cell><cell>0.809</cell><cell>0.728</cell><cell>0.572</cell><cell cols="2">0.415 0.481 0.564</cell><cell>0.788</cell><cell>0.650</cell></row><row><cell>SOPN-N</cell><cell>0.636</cell><cell cols="2">0.477 0.545 0.726</cell><cell>0.850</cell><cell>0.755</cell><cell>0.629</cell><cell cols="2">0.483 0.546 0.609</cell><cell>0.811</cell><cell>0.677</cell></row><row><cell>SOPN-T</cell><cell>0.661</cell><cell cols="2">0.488 0.562 0.733</cell><cell>0.857</cell><cell>0.767</cell><cell>0.625</cell><cell cols="2">0.479 0.543 0.607</cell><cell>0.808</cell><cell>0.675</cell></row><row><cell>only L pr ed</cell><cell>0.651</cell><cell cols="2">0.488 0.558 0.732</cell><cell>0.853</cell><cell>0.747</cell><cell>0.611</cell><cell cols="2">0.465 0.528 0.595</cell><cell>0.807</cell><cell>0.675</cell></row><row><cell>with L siдn</cell><cell>0.676</cell><cell cols="2">0.500 0.575 0.739</cell><cell cols="2">0.842 0.774</cell><cell>0.630</cell><cell cols="3">0.491 0.552 0.614 0.810</cell><cell>0.678</cell></row><row><cell>with L dec</cell><cell>0.653</cell><cell cols="2">0.386 0.485 0.683</cell><cell>0.849</cell><cell>0.755</cell><cell>0.597</cell><cell cols="2">0.423 0.495 0.579</cell><cell>0.796</cell><cell>0.657</cell></row><row><cell>SOPN (full)</cell><cell>0.765</cell><cell cols="4">0.453 0.569 0.745 0.857 0.767</cell><cell>0.630</cell><cell cols="3">0.493 0.553 0.613 0.815 0.682</cell></row><row><cell cols="5">• Node2Vec[10]: This method incorporates unsupervised net-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">work embedding based on biased truncated random walk.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>• LINE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://www.tianyancha.com</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">http://pytorch.org</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">https://github.com/dmlc/dgl</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This research was partially supported by grants from the National Key Research and Development Program of China (Grant No. 2018YF B1402600), and the National Natural Science Foundation of China (Grant No. 91746301, 61703386, U1605251).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Revisiting globalization challenges and opportunities in the development of cooperatives</title>
		<author>
			<persName><forename type="first">Ignacio</forename><surname>Bretos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carmen</forename><surname>Marcuello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Public and Cooperative Economics</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="47" to="73" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Incorporating corporation relationship via graph convolutional neural networks for stock price prediction</title>
		<author>
			<persName><forename type="first">Yingmei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongyu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Information and Knowledge Management</title>
				<meeting>the 27th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1655" to="1658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2014 Workshop on Deep Learning</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Inter-company comparison using modified TOPSIS with objective weights</title>
		<author>
			<persName><forename type="first">Hepu</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung-Hsing</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">J</forename><surname>Willis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Operations Research</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="963" to="973" />
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Signed graph convolutional networks</title>
		<author>
			<persName><forename type="first">Tyler</forename><surname>Derr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Data Mining (ICDM)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="929" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Solving link-oriented tasks in signed network via an embedding approach</title>
		<author>
			<persName><forename type="first">Dongfang</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="75" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
				<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Co-opetition between giants: Collaboration with competitors for technological innovation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Devi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byung-Jin Robert</forename><surname>Gnyawali</surname></persName>
		</author>
		<author>
			<persName><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Research policy</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="650" to="663" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations, ICLR 2015</title>
				<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07">2015. May 7-9, 2015</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Learning Representations (ICLR &apos;17)</title>
				<meeting>the 5th International Conference on Learning Representations (ICLR &apos;17)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Applying grey relational analysis and grey decision-making to evaluate the relationship between company attributes and its financial performance-a case study of venture capital enterprises in Taiwan</title>
		<author>
			<persName><forename type="first">Chaang-</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Yung</forename><surname>Kung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun-Li</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Decision support systems</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="842" to="852" />
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and Their Compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Neural Information Processing Systems (NIPS&apos;13)</title>
				<meeting>the 26th International Conference on Neural Information Processing Systems (NIPS&apos;13)<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Long term supplier selection using a combined fuzzy MCDM approach: A case study for a telecommunication company</title>
		<author>
			<persName><forename type="first">Semih</forename><surname>Önüt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Selin</forename><surname>Soner Kara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elif</forename><surname>Işik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert systems with applications</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="3887" to="3895" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cooperation, competition, and innovative capability: a panel data of European dedicated biotechnology firms</title>
		<author>
			<persName><forename type="first">Cristina</forename><surname>Quintana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><forename type="middle">A</forename><surname>Benavides-Velasco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technovation</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="927" to="938" />
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Factorization machines with libfm</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology (TIST)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeno</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">BPR: Bayesian Personalized Ranking from Implicit Feedback</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence</title>
				<meeting>the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Semantic Web Conference</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Global competition and local cooperation: success and failure in the Sinos Valley</title>
		<author>
			<persName><forename type="first">Hubert</forename><surname>Schmitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brazil. World development</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1627" to="1650" />
			<date type="published" when="1999">1999. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on world wide web</title>
				<meeting>the 24th international conference on world wide web</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Business models, business strategy and innovation</title>
		<author>
			<persName><forename type="first">Teece</forename><surname>David</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Long range planning</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="172" to="194" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Graph Attention Networks. International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Structural deep network embedding</title>
		<author>
			<persName><forename type="first">Daixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1225" to="1234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">MCNE: An End-to-End Framework for Learning Multiple Conditional Network Representations of Social Network</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Defu</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongfang</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1064" to="1072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">SNE: signed network embedding</title>
		<author>
			<persName><forename type="first">Shuhan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PAKDD</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="183" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Signed graphs</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Zaslavsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discrete Applied Mathematics</title>
		<imprint>
			<biblScope unit="page" from="47" to="74" />
			<date type="published" when="1982">1982. 1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Heterogeneous graph neural network</title>
		<author>
			<persName><forename type="first">Chuxu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongjin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitesh V</forename><surname>Chawla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="793" to="803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Large-Scale Talent Flow Embedding for Company Competitive Analysis</title>
		<author>
			<persName><forename type="first">Le</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengshu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingxin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference</title>
				<meeting>The Web Conference</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="2354" to="2364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Large-Scale Talent Flow Forecast with Dynamic Latent Factor Model</title>
		<author>
			<persName><forename type="first">Le</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengshu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2312" to="2322" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
